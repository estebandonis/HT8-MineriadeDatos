{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as smd\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.diagnostic as diag\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector #Para seleccionar de forma automática las variables numéricas y categóricas\n",
    "from sklearn.preprocessing import OneHotEncoder #Para codificar las variables categóricas usando dummies\n",
    "from sklearn.preprocessing import StandardScaler #Para normalizar las variables numéricas\n",
    "from sklearn.compose import ColumnTransformer #Modifica las columnas usando los preprocesadores\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline #Planifica una secuencia de procesos\n",
    "from sklearn import set_config #Para mostrar graficamente el pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "set_config(display='diagram')\n",
    "#Metrics\n",
    "from sklearn.metrics import make_scorer, accuracy_score,precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_df = pd.read_csv('train.csv')\n",
    "\n",
    "houses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
      "['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(houses_df.drop(['Id', 'SalePrice'], axis=1))\n",
    "categorical_columns = categorical_columns_selector(houses_df)\n",
    "\n",
    "print(numerical_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the number of categories\n",
    "num_categories = 3\n",
    "\n",
    "# Define the category names\n",
    "category_names = ['Economica', 'Intermedia', 'Cara']\n",
    "\n",
    "# Create a new column in the DataFrame for the categories\n",
    "houses_df['Precio_Categoria'] = pd.qcut(houses_df['SalePrice'], q=num_categories, labels=category_names)\n",
    "\n",
    "# Verificar la distribución de las categorías\n",
    "plt.bar(houses_df['Precio_Categoria'].value_counts().index, houses_df['Precio_Categoria'].value_counts().values)\n",
    "plt.xlabel('Categoría de Precio')\n",
    "plt.ylabel('Número de Casas')\n",
    "plt.title('Distribución de Categorías de Precio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador_categorico = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "preprocesador_numerico = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador = ColumnTransformer([\n",
    "    ('Variables Categóricas',preprocesador_categorico, categorical_columns),\n",
    "    ('Variables Numéricas',preprocesador_numerico, numerical_columns)\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Categóricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Numéricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre></pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content \"><pre>MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = make_pipeline(preprocesador,MLPClassifier(activation=\"relu\",verbose=True,hidden_layer_sizes=(30, 20),max_iter=500))\n",
    "modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = houses_df.pop('Precio_Categoria')\n",
    "data = houses_df.drop(['Id', 'SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass         0\n",
       "MSZoning           0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "Street             0\n",
       "                ... \n",
       "MiscVal            0\n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "Length: 79, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target,test_size=0.3,train_size=0.7)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07981632\n",
      "Iteration 2, loss = 0.91960346\n",
      "Iteration 3, loss = 0.79640087\n",
      "Iteration 4, loss = 0.69651808\n",
      "Iteration 5, loss = 0.61845302\n",
      "Iteration 6, loss = 0.55455327\n",
      "Iteration 7, loss = 0.50310510\n",
      "Iteration 8, loss = 0.46299990\n",
      "Iteration 9, loss = 0.43286931\n",
      "Iteration 10, loss = 0.40863566\n",
      "Iteration 11, loss = 0.39130948\n",
      "Iteration 12, loss = 0.37536414\n",
      "Iteration 13, loss = 0.36301963\n",
      "Iteration 14, loss = 0.34872180\n",
      "Iteration 15, loss = 0.33615918\n",
      "Iteration 16, loss = 0.32545948\n",
      "Iteration 17, loss = 0.31517918\n",
      "Iteration 18, loss = 0.30599923\n",
      "Iteration 19, loss = 0.29776498\n",
      "Iteration 20, loss = 0.29037233\n",
      "Iteration 21, loss = 0.28254462\n",
      "Iteration 22, loss = 0.27677343\n",
      "Iteration 23, loss = 0.26633811\n",
      "Iteration 24, loss = 0.25918825\n",
      "Iteration 25, loss = 0.26052806\n",
      "Iteration 26, loss = 0.24814003\n",
      "Iteration 27, loss = 0.24386806\n",
      "Iteration 28, loss = 0.23332306\n",
      "Iteration 29, loss = 0.23368380\n",
      "Iteration 30, loss = 0.22203855\n",
      "Iteration 31, loss = 0.22309139\n",
      "Iteration 32, loss = 0.21587611\n",
      "Iteration 33, loss = 0.21135332\n",
      "Iteration 34, loss = 0.20688776\n",
      "Iteration 35, loss = 0.19922610\n",
      "Iteration 36, loss = 0.19717261\n",
      "Iteration 37, loss = 0.18780983\n",
      "Iteration 38, loss = 0.18678471\n",
      "Iteration 39, loss = 0.17953940\n",
      "Iteration 40, loss = 0.17654612\n",
      "Iteration 41, loss = 0.17266162\n",
      "Iteration 42, loss = 0.16980380\n",
      "Iteration 43, loss = 0.16454048\n",
      "Iteration 44, loss = 0.15816893\n",
      "Iteration 45, loss = 0.15638685\n",
      "Iteration 46, loss = 0.15108324\n",
      "Iteration 47, loss = 0.15077632\n",
      "Iteration 48, loss = 0.14440067\n",
      "Iteration 49, loss = 0.14611608\n",
      "Iteration 50, loss = 0.14252121\n",
      "Iteration 51, loss = 0.13926904\n",
      "Iteration 52, loss = 0.13055456\n",
      "Iteration 53, loss = 0.12895260\n",
      "Iteration 54, loss = 0.12706097\n",
      "Iteration 55, loss = 0.12291140\n",
      "Iteration 56, loss = 0.11874069\n",
      "Iteration 57, loss = 0.11763182\n",
      "Iteration 58, loss = 0.11224183\n",
      "Iteration 59, loss = 0.11024759\n",
      "Iteration 60, loss = 0.10985219\n",
      "Iteration 61, loss = 0.10437184\n",
      "Iteration 62, loss = 0.10099931\n",
      "Iteration 63, loss = 0.09993445\n",
      "Iteration 64, loss = 0.09657690\n",
      "Iteration 65, loss = 0.09545541\n",
      "Iteration 66, loss = 0.09203225\n",
      "Iteration 67, loss = 0.09155382\n",
      "Iteration 68, loss = 0.08768137\n",
      "Iteration 69, loss = 0.08416123\n",
      "Iteration 70, loss = 0.08345971\n",
      "Iteration 71, loss = 0.07900529\n",
      "Iteration 72, loss = 0.07890634\n",
      "Iteration 73, loss = 0.07596252\n",
      "Iteration 74, loss = 0.07463365\n",
      "Iteration 75, loss = 0.07220423\n",
      "Iteration 76, loss = 0.06944633\n",
      "Iteration 77, loss = 0.06848906\n",
      "Iteration 78, loss = 0.06648688\n",
      "Iteration 79, loss = 0.06486455\n",
      "Iteration 80, loss = 0.06247404\n",
      "Iteration 81, loss = 0.06063128\n",
      "Iteration 82, loss = 0.05902911\n",
      "Iteration 83, loss = 0.05847678\n",
      "Iteration 84, loss = 0.05659366\n",
      "Iteration 85, loss = 0.05545188\n",
      "Iteration 86, loss = 0.05524900\n",
      "Iteration 87, loss = 0.05634390\n",
      "Iteration 88, loss = 0.05560299\n",
      "Iteration 89, loss = 0.05209869\n",
      "Iteration 90, loss = 0.05570004\n",
      "Iteration 91, loss = 0.05037339\n",
      "Iteration 92, loss = 0.04895191\n",
      "Iteration 93, loss = 0.04568464\n",
      "Iteration 94, loss = 0.04477502\n",
      "Iteration 95, loss = 0.04415604\n",
      "Iteration 96, loss = 0.04472591\n",
      "Iteration 97, loss = 0.04527499\n",
      "Iteration 98, loss = 0.04054443\n",
      "Iteration 99, loss = 0.03925836\n",
      "Iteration 100, loss = 0.03876821\n",
      "Iteration 101, loss = 0.03717127\n",
      "Iteration 102, loss = 0.03644862\n",
      "Iteration 103, loss = 0.03563782\n",
      "Iteration 104, loss = 0.03483825\n",
      "Iteration 105, loss = 0.03578304\n",
      "Iteration 106, loss = 0.03406169\n",
      "Iteration 107, loss = 0.03314726\n",
      "Iteration 108, loss = 0.03211671\n",
      "Iteration 109, loss = 0.03082943\n",
      "Iteration 110, loss = 0.02987331\n",
      "Iteration 111, loss = 0.03043523\n",
      "Iteration 112, loss = 0.02857051\n",
      "Iteration 113, loss = 0.02846276\n",
      "Iteration 114, loss = 0.02784096\n",
      "Iteration 115, loss = 0.02795711\n",
      "Iteration 116, loss = 0.02599624\n",
      "Iteration 117, loss = 0.02575397\n",
      "Iteration 118, loss = 0.02532107\n",
      "Iteration 119, loss = 0.02484000\n",
      "Iteration 120, loss = 0.02444935\n",
      "Iteration 121, loss = 0.02384377\n",
      "Iteration 122, loss = 0.02273505\n",
      "Iteration 123, loss = 0.02256156\n",
      "Iteration 124, loss = 0.02252866\n",
      "Iteration 125, loss = 0.02136914\n",
      "Iteration 126, loss = 0.02130305\n",
      "Iteration 127, loss = 0.02077727\n",
      "Iteration 128, loss = 0.02060606\n",
      "Iteration 129, loss = 0.01979042\n",
      "Iteration 130, loss = 0.01881628\n",
      "Iteration 131, loss = 0.01844385\n",
      "Iteration 132, loss = 0.01808758\n",
      "Iteration 133, loss = 0.01763443\n",
      "Iteration 134, loss = 0.01726782\n",
      "Iteration 135, loss = 0.01704810\n",
      "Iteration 136, loss = 0.01703958\n",
      "Iteration 137, loss = 0.01647071\n",
      "Iteration 138, loss = 0.01644186\n",
      "Iteration 139, loss = 0.01647349\n",
      "Iteration 140, loss = 0.01603279\n",
      "Iteration 141, loss = 0.01596294\n",
      "Iteration 142, loss = 0.01538829\n",
      "Iteration 143, loss = 0.01459605\n",
      "Iteration 144, loss = 0.01427945\n",
      "Iteration 145, loss = 0.01409750\n",
      "Iteration 146, loss = 0.01404485\n",
      "Iteration 147, loss = 0.01397743\n",
      "Iteration 148, loss = 0.01440491\n",
      "Iteration 149, loss = 0.01348610\n",
      "Iteration 150, loss = 0.01340450\n",
      "Iteration 151, loss = 0.01284177\n",
      "Iteration 152, loss = 0.01397280\n",
      "Iteration 153, loss = 0.01306820\n",
      "Iteration 154, loss = 0.01254062\n",
      "Iteration 155, loss = 0.01189396\n",
      "Iteration 156, loss = 0.01192467\n",
      "Iteration 157, loss = 0.01146797\n",
      "Iteration 158, loss = 0.01124294\n",
      "Iteration 159, loss = 0.01091037\n",
      "Iteration 160, loss = 0.01083367\n",
      "Iteration 161, loss = 0.01074240\n",
      "Iteration 162, loss = 0.01047801\n",
      "Iteration 163, loss = 0.01036143\n",
      "Iteration 164, loss = 0.01030111\n",
      "Iteration 165, loss = 0.01013684\n",
      "Iteration 166, loss = 0.00972746\n",
      "Iteration 167, loss = 0.00968420\n",
      "Iteration 168, loss = 0.00945112\n",
      "Iteration 169, loss = 0.01145571\n",
      "Iteration 170, loss = 0.01064460\n",
      "Iteration 171, loss = 0.01265977\n",
      "Iteration 172, loss = 0.01055876\n",
      "Iteration 173, loss = 0.00970144\n",
      "Iteration 174, loss = 0.00956753\n",
      "Iteration 175, loss = 0.00881465\n",
      "Iteration 176, loss = 0.00911330\n",
      "Iteration 177, loss = 0.00844683\n",
      "Iteration 178, loss = 0.00902560\n",
      "Iteration 179, loss = 0.00846653\n",
      "Iteration 180, loss = 0.00866250\n",
      "Iteration 181, loss = 0.00783663\n",
      "Iteration 182, loss = 0.00763652\n",
      "Iteration 183, loss = 0.00747289\n",
      "Iteration 184, loss = 0.00776100\n",
      "Iteration 185, loss = 0.00733364\n",
      "Iteration 186, loss = 0.00717458\n",
      "Iteration 187, loss = 0.00716695\n",
      "Iteration 188, loss = 0.00689425\n",
      "Iteration 189, loss = 0.00680438\n",
      "Iteration 190, loss = 0.00668341\n",
      "Iteration 191, loss = 0.00678225\n",
      "Iteration 192, loss = 0.00665817\n",
      "Iteration 193, loss = 0.00661032\n",
      "Iteration 194, loss = 0.00652486\n",
      "Iteration 195, loss = 0.00625676\n",
      "Iteration 196, loss = 0.00621493\n",
      "Iteration 197, loss = 0.00627839\n",
      "Iteration 198, loss = 0.00659049\n",
      "Iteration 199, loss = 0.00646560\n",
      "Iteration 200, loss = 0.00596921\n",
      "Iteration 201, loss = 0.00577627\n",
      "Iteration 202, loss = 0.00580989\n",
      "Iteration 203, loss = 0.00572548\n",
      "Iteration 204, loss = 0.00750784\n",
      "Iteration 205, loss = 0.00711499\n",
      "Iteration 206, loss = 0.00669531\n",
      "Iteration 207, loss = 0.00574147\n",
      "Iteration 208, loss = 0.00566118\n",
      "Iteration 209, loss = 0.00524849\n",
      "Iteration 210, loss = 0.00714534\n",
      "Iteration 211, loss = 0.00602769\n",
      "Iteration 212, loss = 0.00622031\n",
      "Iteration 213, loss = 0.00531380\n",
      "Iteration 214, loss = 0.00520854\n",
      "Iteration 215, loss = 0.00486296\n",
      "Iteration 216, loss = 0.00477705\n",
      "Iteration 217, loss = 0.00483280\n",
      "Iteration 218, loss = 0.00505262\n",
      "Iteration 219, loss = 0.00462312\n",
      "Iteration 220, loss = 0.00464778\n",
      "Iteration 221, loss = 0.00443480\n",
      "Iteration 222, loss = 0.00443281\n",
      "Iteration 223, loss = 0.00437569\n",
      "Iteration 224, loss = 0.00442947\n",
      "Iteration 225, loss = 0.00439617\n",
      "Iteration 226, loss = 0.00436521\n",
      "Iteration 227, loss = 0.00437047\n",
      "Iteration 228, loss = 0.00422183\n",
      "Iteration 229, loss = 0.00405858\n",
      "Iteration 230, loss = 0.00397717\n",
      "Iteration 231, loss = 0.00399814\n",
      "Iteration 232, loss = 0.00385450\n",
      "Iteration 233, loss = 0.00378909\n",
      "Iteration 234, loss = 0.00378878\n",
      "Iteration 235, loss = 0.00371433\n",
      "Iteration 236, loss = 0.00593281\n",
      "Iteration 237, loss = 0.00491520\n",
      "Iteration 238, loss = 0.00411395\n",
      "Iteration 239, loss = 0.00365089\n",
      "Iteration 240, loss = 0.00380444\n",
      "Iteration 241, loss = 0.00363256\n",
      "Iteration 242, loss = 0.00370020\n",
      "Iteration 243, loss = 0.00369049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(data_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelo.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      " [[136   0  18]\n",
      " [  1 127  13]\n",
      " [ 14  24 105]]\n",
      "Accuracy:  0.8401826484018264\n",
      "Precision:  0.8401826484018264\n",
      "recall:  0.8401826484018264\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(target_test,pred)\n",
    "accuracy=accuracy_score(target_test,pred)\n",
    "precision =precision_score(target_test,pred,average='micro')\n",
    "recall =  recall_score(target_test,pred,average='micro')\n",
    "f1 = f1_score(target_test,pred,average='micro')\n",
    "print('Matriz de confusión\\n',cm)\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Categóricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Numéricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content \"><pre>MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(), max_iter=300,\n",
       "              verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(activation='identity', hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo1 = make_pipeline(preprocesador,MLPClassifier(activation=\"identity\",verbose=True,hidden_layer_sizes=(), max_iter=300))\n",
    "\n",
    "modelo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.38358049\n",
      "Iteration 2, loss = 1.20794499\n",
      "Iteration 3, loss = 1.07661262\n",
      "Iteration 4, loss = 0.98180954\n",
      "Iteration 5, loss = 0.91669351\n",
      "Iteration 6, loss = 0.86489883\n",
      "Iteration 7, loss = 0.82303187\n",
      "Iteration 8, loss = 0.78486231\n",
      "Iteration 9, loss = 0.75261125\n",
      "Iteration 10, loss = 0.72501346\n",
      "Iteration 11, loss = 0.70167159\n",
      "Iteration 12, loss = 0.68091431\n",
      "Iteration 13, loss = 0.66275773\n",
      "Iteration 14, loss = 0.64739544\n",
      "Iteration 15, loss = 0.63276349\n",
      "Iteration 16, loss = 0.61940818\n",
      "Iteration 17, loss = 0.60678292\n",
      "Iteration 18, loss = 0.59588108\n",
      "Iteration 19, loss = 0.58526996\n",
      "Iteration 20, loss = 0.57560870\n",
      "Iteration 21, loss = 0.56590766\n",
      "Iteration 22, loss = 0.55764571\n",
      "Iteration 23, loss = 0.54977946\n",
      "Iteration 24, loss = 0.54209890\n",
      "Iteration 25, loss = 0.53484899\n",
      "Iteration 26, loss = 0.52801122\n",
      "Iteration 27, loss = 0.52172465\n",
      "Iteration 28, loss = 0.51602859\n",
      "Iteration 29, loss = 0.51080394\n",
      "Iteration 30, loss = 0.50494098\n",
      "Iteration 31, loss = 0.49914025\n",
      "Iteration 32, loss = 0.49405482\n",
      "Iteration 33, loss = 0.48917165\n",
      "Iteration 34, loss = 0.48437924\n",
      "Iteration 35, loss = 0.48062253\n",
      "Iteration 36, loss = 0.47569457\n",
      "Iteration 37, loss = 0.47259682\n",
      "Iteration 38, loss = 0.46856207\n",
      "Iteration 39, loss = 0.46458695\n",
      "Iteration 40, loss = 0.46142460\n",
      "Iteration 41, loss = 0.45786251\n",
      "Iteration 42, loss = 0.45473602\n",
      "Iteration 43, loss = 0.45122406\n",
      "Iteration 44, loss = 0.44817030\n",
      "Iteration 45, loss = 0.44514645\n",
      "Iteration 46, loss = 0.44239529\n",
      "Iteration 47, loss = 0.43948665\n",
      "Iteration 48, loss = 0.43673275\n",
      "Iteration 49, loss = 0.43374156\n",
      "Iteration 50, loss = 0.43183501\n",
      "Iteration 51, loss = 0.42908356\n",
      "Iteration 52, loss = 0.42604387\n",
      "Iteration 53, loss = 0.42308712\n",
      "Iteration 54, loss = 0.42075586\n",
      "Iteration 55, loss = 0.41858327\n",
      "Iteration 56, loss = 0.41613232\n",
      "Iteration 57, loss = 0.41420417\n",
      "Iteration 58, loss = 0.41196067\n",
      "Iteration 59, loss = 0.40984174\n",
      "Iteration 60, loss = 0.40753206\n",
      "Iteration 61, loss = 0.40555421\n",
      "Iteration 62, loss = 0.40364024\n",
      "Iteration 63, loss = 0.40167832\n",
      "Iteration 64, loss = 0.40017181\n",
      "Iteration 65, loss = 0.39868340\n",
      "Iteration 66, loss = 0.39678289\n",
      "Iteration 67, loss = 0.39443332\n",
      "Iteration 68, loss = 0.39298535\n",
      "Iteration 69, loss = 0.39085586\n",
      "Iteration 70, loss = 0.38984896\n",
      "Iteration 71, loss = 0.38877982\n",
      "Iteration 72, loss = 0.38685829\n",
      "Iteration 73, loss = 0.38434164\n",
      "Iteration 74, loss = 0.38251396\n",
      "Iteration 75, loss = 0.38137560\n",
      "Iteration 76, loss = 0.38026664\n",
      "Iteration 77, loss = 0.37881148\n",
      "Iteration 78, loss = 0.37690266\n",
      "Iteration 79, loss = 0.37546112\n",
      "Iteration 80, loss = 0.37410025\n",
      "Iteration 81, loss = 0.37238125\n",
      "Iteration 82, loss = 0.37089433\n",
      "Iteration 83, loss = 0.36985778\n",
      "Iteration 84, loss = 0.36877993\n",
      "Iteration 85, loss = 0.36743272\n",
      "Iteration 86, loss = 0.36623675\n",
      "Iteration 87, loss = 0.36489676\n",
      "Iteration 88, loss = 0.36403318\n",
      "Iteration 89, loss = 0.36272946\n",
      "Iteration 90, loss = 0.36192182\n",
      "Iteration 91, loss = 0.36043816\n",
      "Iteration 92, loss = 0.35884201\n",
      "Iteration 93, loss = 0.35765061\n",
      "Iteration 94, loss = 0.35642907\n",
      "Iteration 95, loss = 0.35542028\n",
      "Iteration 96, loss = 0.35432393\n",
      "Iteration 97, loss = 0.35321560\n",
      "Iteration 98, loss = 0.35289198\n",
      "Iteration 99, loss = 0.35207613\n",
      "Iteration 100, loss = 0.35050807\n",
      "Iteration 101, loss = 0.34928964\n",
      "Iteration 102, loss = 0.34830800\n",
      "Iteration 103, loss = 0.34716021\n",
      "Iteration 104, loss = 0.34616019\n",
      "Iteration 105, loss = 0.34529312\n",
      "Iteration 106, loss = 0.34424521\n",
      "Iteration 107, loss = 0.34310285\n",
      "Iteration 108, loss = 0.34226690\n",
      "Iteration 109, loss = 0.34098393\n",
      "Iteration 110, loss = 0.34067603\n",
      "Iteration 111, loss = 0.34000314\n",
      "Iteration 112, loss = 0.33892829\n",
      "Iteration 113, loss = 0.33777487\n",
      "Iteration 114, loss = 0.33702216\n",
      "Iteration 115, loss = 0.33620996\n",
      "Iteration 116, loss = 0.33531493\n",
      "Iteration 117, loss = 0.33423258\n",
      "Iteration 118, loss = 0.33377010\n",
      "Iteration 119, loss = 0.33322143\n",
      "Iteration 120, loss = 0.33193703\n",
      "Iteration 121, loss = 0.33103481\n",
      "Iteration 122, loss = 0.33030335\n",
      "Iteration 123, loss = 0.32953830\n",
      "Iteration 124, loss = 0.32866247\n",
      "Iteration 125, loss = 0.32807904\n",
      "Iteration 126, loss = 0.32730085\n",
      "Iteration 127, loss = 0.32672238\n",
      "Iteration 128, loss = 0.32608758\n",
      "Iteration 129, loss = 0.32510720\n",
      "Iteration 130, loss = 0.32392632\n",
      "Iteration 131, loss = 0.32323412\n",
      "Iteration 132, loss = 0.32261812\n",
      "Iteration 133, loss = 0.32237471\n",
      "Iteration 134, loss = 0.32172878\n",
      "Iteration 135, loss = 0.32073734\n",
      "Iteration 136, loss = 0.31992089\n",
      "Iteration 137, loss = 0.31930098\n",
      "Iteration 138, loss = 0.31854239\n",
      "Iteration 139, loss = 0.31779252\n",
      "Iteration 140, loss = 0.31712143\n",
      "Iteration 141, loss = 0.31640664\n",
      "Iteration 142, loss = 0.31567555\n",
      "Iteration 143, loss = 0.31507138\n",
      "Iteration 144, loss = 0.31423275\n",
      "Iteration 145, loss = 0.31358448\n",
      "Iteration 146, loss = 0.31307632\n",
      "Iteration 147, loss = 0.31219217\n",
      "Iteration 148, loss = 0.31171220\n",
      "Iteration 149, loss = 0.31147871\n",
      "Iteration 150, loss = 0.31119691\n",
      "Iteration 151, loss = 0.31040015\n",
      "Iteration 152, loss = 0.30947432\n",
      "Iteration 153, loss = 0.30850786\n",
      "Iteration 154, loss = 0.30814804\n",
      "Iteration 155, loss = 0.30839680\n",
      "Iteration 156, loss = 0.30773812\n",
      "Iteration 157, loss = 0.30653368\n",
      "Iteration 158, loss = 0.30566081\n",
      "Iteration 159, loss = 0.30547772\n",
      "Iteration 160, loss = 0.30485553\n",
      "Iteration 161, loss = 0.30396846\n",
      "Iteration 162, loss = 0.30363004\n",
      "Iteration 163, loss = 0.30355378\n",
      "Iteration 164, loss = 0.30258496\n",
      "Iteration 165, loss = 0.30158811\n",
      "Iteration 166, loss = 0.30178033\n",
      "Iteration 167, loss = 0.30237939\n",
      "Iteration 168, loss = 0.30185184\n",
      "Iteration 169, loss = 0.29967279\n",
      "Iteration 170, loss = 0.29893794\n",
      "Iteration 171, loss = 0.29841092\n",
      "Iteration 172, loss = 0.29800612\n",
      "Iteration 173, loss = 0.29740798\n",
      "Iteration 174, loss = 0.29686365\n",
      "Iteration 175, loss = 0.29670725\n",
      "Iteration 176, loss = 0.29640687\n",
      "Iteration 177, loss = 0.29551895\n",
      "Iteration 178, loss = 0.29494004\n",
      "Iteration 179, loss = 0.29442042\n",
      "Iteration 180, loss = 0.29378542\n",
      "Iteration 181, loss = 0.29334958\n",
      "Iteration 182, loss = 0.29304574\n",
      "Iteration 183, loss = 0.29247844\n",
      "Iteration 184, loss = 0.29260824\n",
      "Iteration 185, loss = 0.29193294\n",
      "Iteration 186, loss = 0.29078068\n",
      "Iteration 187, loss = 0.29019890\n",
      "Iteration 188, loss = 0.29005838\n",
      "Iteration 189, loss = 0.28918070\n",
      "Iteration 190, loss = 0.28929651\n",
      "Iteration 191, loss = 0.28941222\n",
      "Iteration 192, loss = 0.28883508\n",
      "Iteration 193, loss = 0.28799271\n",
      "Iteration 194, loss = 0.28700335\n",
      "Iteration 195, loss = 0.28698258\n",
      "Iteration 196, loss = 0.28648011\n",
      "Iteration 197, loss = 0.28614171\n",
      "Iteration 198, loss = 0.28568040\n",
      "Iteration 199, loss = 0.28543174\n",
      "Iteration 200, loss = 0.28455855\n",
      "Iteration 201, loss = 0.28413967\n",
      "Iteration 202, loss = 0.28416070\n",
      "Iteration 203, loss = 0.28387920\n",
      "Iteration 204, loss = 0.28282411\n",
      "Iteration 205, loss = 0.28229828\n",
      "Iteration 206, loss = 0.28202641\n",
      "Iteration 207, loss = 0.28150990\n",
      "Iteration 208, loss = 0.28133925\n",
      "Iteration 209, loss = 0.28101330\n",
      "Iteration 210, loss = 0.28031625\n",
      "Iteration 211, loss = 0.28004630\n",
      "Iteration 212, loss = 0.27947832\n",
      "Iteration 213, loss = 0.27925458\n",
      "Iteration 214, loss = 0.27874819\n",
      "Iteration 215, loss = 0.27833171\n",
      "Iteration 216, loss = 0.27826704\n",
      "Iteration 217, loss = 0.27750749\n",
      "Iteration 218, loss = 0.27667709\n",
      "Iteration 219, loss = 0.27688572\n",
      "Iteration 220, loss = 0.27724380\n",
      "Iteration 221, loss = 0.27666113\n",
      "Iteration 222, loss = 0.27557553\n",
      "Iteration 223, loss = 0.27518094\n",
      "Iteration 224, loss = 0.27471064\n",
      "Iteration 225, loss = 0.27412947\n",
      "Iteration 226, loss = 0.27364766\n",
      "Iteration 227, loss = 0.27320176\n",
      "Iteration 228, loss = 0.27297957\n",
      "Iteration 229, loss = 0.27347121\n",
      "Iteration 230, loss = 0.27336859\n",
      "Iteration 231, loss = 0.27306701\n",
      "Iteration 232, loss = 0.27228767\n",
      "Iteration 233, loss = 0.27166708\n",
      "Iteration 234, loss = 0.27102849\n",
      "Iteration 235, loss = 0.27066867\n",
      "Iteration 236, loss = 0.27029875\n",
      "Iteration 237, loss = 0.26983888\n",
      "Iteration 238, loss = 0.26976938\n",
      "Iteration 239, loss = 0.26936056\n",
      "Iteration 240, loss = 0.26878698\n",
      "Iteration 241, loss = 0.26864148\n",
      "Iteration 242, loss = 0.26830335\n",
      "Iteration 243, loss = 0.26780539\n",
      "Iteration 244, loss = 0.26744096\n",
      "Iteration 245, loss = 0.26713044\n",
      "Iteration 246, loss = 0.26692849\n",
      "Iteration 247, loss = 0.26661439\n",
      "Iteration 248, loss = 0.26646623\n",
      "Iteration 249, loss = 0.26604143\n",
      "Iteration 250, loss = 0.26553523\n",
      "Iteration 251, loss = 0.26533982\n",
      "Iteration 252, loss = 0.26490361\n",
      "Iteration 253, loss = 0.26494868\n",
      "Iteration 254, loss = 0.26446258\n",
      "Iteration 255, loss = 0.26365302\n",
      "Iteration 256, loss = 0.26296789\n",
      "Iteration 257, loss = 0.26312148\n",
      "Iteration 258, loss = 0.26276008\n",
      "Iteration 259, loss = 0.26227527\n",
      "Iteration 260, loss = 0.26189273\n",
      "Iteration 261, loss = 0.26203454\n",
      "Iteration 262, loss = 0.26197730\n",
      "Iteration 263, loss = 0.26151304\n",
      "Iteration 264, loss = 0.26096297\n",
      "Iteration 265, loss = 0.26056870\n",
      "Iteration 266, loss = 0.26059315\n",
      "Iteration 267, loss = 0.26011017\n",
      "Iteration 268, loss = 0.25976909\n",
      "Iteration 269, loss = 0.25939165\n",
      "Iteration 270, loss = 0.25895116\n",
      "Iteration 271, loss = 0.25871012\n",
      "Iteration 272, loss = 0.25842301\n",
      "Iteration 273, loss = 0.25831537\n",
      "Iteration 274, loss = 0.25770712\n",
      "Iteration 275, loss = 0.25756835\n",
      "Iteration 276, loss = 0.25695830\n",
      "Iteration 277, loss = 0.25659378\n",
      "Iteration 278, loss = 0.25683391\n",
      "Iteration 279, loss = 0.25678360\n",
      "Iteration 280, loss = 0.25579225\n",
      "Iteration 281, loss = 0.25534211\n",
      "Iteration 282, loss = 0.25573971\n",
      "Iteration 283, loss = 0.25508744\n",
      "Iteration 284, loss = 0.25443609\n",
      "Iteration 285, loss = 0.25441293\n",
      "Iteration 286, loss = 0.25416757\n",
      "Iteration 287, loss = 0.25479027\n",
      "Iteration 288, loss = 0.25440776\n",
      "Iteration 289, loss = 0.25357672\n",
      "Iteration 290, loss = 0.25269905\n",
      "Iteration 291, loss = 0.25252455\n",
      "Iteration 292, loss = 0.25226803\n",
      "Iteration 293, loss = 0.25194882\n",
      "Iteration 294, loss = 0.25180286\n",
      "Iteration 295, loss = 0.25150459\n",
      "Iteration 296, loss = 0.25110974\n",
      "Iteration 297, loss = 0.25095596\n",
      "Iteration 298, loss = 0.25045881\n",
      "Iteration 299, loss = 0.25031916\n",
      "Iteration 300, loss = 0.25006687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(), max_iter=300,\n",
       "              verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(activation='identity', hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo1.fit(data_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = modelo1.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      " [[137   0  17]\n",
      " [  1 131   9]\n",
      " [ 16  26 101]]\n",
      "Accuracy:  0.8424657534246576\n",
      "Precision:  0.8424657534246576\n",
      "recall:  0.8424657534246576\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(target_test,pred1)\n",
    "accuracy=accuracy_score(target_test,pred1)\n",
    "precision =precision_score(target_test,pred1,average='micro')\n",
    "recall =  recall_score(target_test,pred1,average='micro')\n",
    "f1 = f1_score(target_test,pred1,average='micro')\n",
    "print('Matriz de confusión\\n',cm)\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.79400782\n",
      "Iteration 2, loss = 0.42592944\n",
      "Iteration 3, loss = 0.38034711\n",
      "Iteration 4, loss = 0.36467983\n",
      "Iteration 5, loss = 0.32799651\n",
      "Iteration 6, loss = 0.30163170\n",
      "Iteration 7, loss = 0.27507806\n",
      "Iteration 8, loss = 0.26959087\n",
      "Iteration 9, loss = 0.24009409\n",
      "Iteration 10, loss = 0.22263661\n",
      "Iteration 11, loss = 0.20728146\n",
      "Iteration 12, loss = 0.19871478\n",
      "Iteration 13, loss = 0.18572094\n",
      "Iteration 14, loss = 0.18352131\n",
      "Iteration 15, loss = 0.17436275\n",
      "Iteration 1, loss = 1.01542284\n",
      "Iteration 16, loss = 0.16424657\n",
      "Iteration 2, loss = 0.55686466\n",
      "Iteration 17, loss = 0.15450167\n",
      "Iteration 3, loss = 0.43612424\n",
      "Iteration 18, loss = 0.17105437\n",
      "Iteration 4, loss = 0.37140253\n",
      "Iteration 19, loss = 0.16379573\n",
      "Iteration 5, loss = 0.34158265\n",
      "Iteration 20, loss = 0.14188118\n",
      "Iteration 6, loss = 0.30916089\n",
      "Iteration 21, loss = 0.15709427\n",
      "Iteration 7, loss = 0.28171222\n",
      "Iteration 22, loss = 0.14096867\n",
      "Iteration 8, loss = 0.26707119\n",
      "Iteration 23, loss = 0.14475308\n",
      "Iteration 9, loss = 0.25142905\n",
      "Iteration 24, loss = 0.13302305\n",
      "Iteration 10, loss = 0.23005316\n",
      "Iteration 25, loss = 0.13640647\n",
      "Iteration 11, loss = 0.21583572\n",
      "Iteration 26, loss = 0.12562117\n",
      "Iteration 12, loss = 0.20321434\n",
      "Iteration 27, loss = 0.11282538\n",
      "Iteration 13, loss = 0.19711883\n",
      "Iteration 28, loss = 0.12141272\n",
      "Iteration 14, loss = 0.18468603\n",
      "Iteration 29, loss = 0.11520730\n",
      "Iteration 15, loss = 0.19755790\n",
      "Iteration 30, loss = 0.14263751\n",
      "Iteration 16, loss = 0.18489159\n",
      "Iteration 31, loss = 0.17033064\n",
      "Iteration 17, loss = 0.16755254\n",
      "Iteration 32, loss = 0.12951886\n",
      "Iteration 18, loss = 0.14943442\n",
      "Iteration 33, loss = 0.13733211\n",
      "Iteration 19, loss = 0.14560725\n",
      "Iteration 34, loss = 0.11544782\n",
      "Iteration 20, loss = 0.16848056\n",
      "Iteration 35, loss = 0.10559724\n",
      "Iteration 21, loss = 0.20352994\n",
      "Iteration 36, loss = 0.11205499\n",
      "Iteration 22, loss = 0.19023385\n",
      "Iteration 37, loss = 0.13060212\n",
      "Iteration 23, loss = 0.17918444\n",
      "Iteration 24, loss = 0.14973402\n",
      "Iteration 38, loss = 0.14435459\n",
      "Iteration 25, loss = 0.13387356\n",
      "Iteration 39, loss = 0.16025858\n",
      "Iteration 26, loss = 0.14059883\n",
      "Iteration 40, loss = 0.12217407\n",
      "Iteration 27, loss = 0.13431876\n",
      "Iteration 41, loss = 0.10271750\n",
      "Iteration 28, loss = 0.14281261\n",
      "Iteration 42, loss = 0.08489257\n",
      "Iteration 29, loss = 0.14182531\n",
      "Iteration 43, loss = 0.08311342\n",
      "Iteration 30, loss = 0.12744897\n",
      "Iteration 44, loss = 0.07031010\n",
      "Iteration 31, loss = 0.13883182\n",
      "Iteration 45, loss = 0.06965147\n",
      "Iteration 32, loss = 0.10761898\n",
      "Iteration 46, loss = 0.06328985\n",
      "Iteration 33, loss = 0.10424916\n",
      "Iteration 47, loss = 0.06518036\n",
      "Iteration 34, loss = 0.10932033\n",
      "Iteration 48, loss = 0.06473025\n",
      "Iteration 35, loss = 0.11236576\n",
      "Iteration 49, loss = 0.05875854\n",
      "Iteration 36, loss = 0.11951089\n",
      "Iteration 50, loss = 0.07124814\n",
      "Iteration 37, loss = 0.10318002\n",
      "Iteration 51, loss = 0.08246814\n",
      "Iteration 38, loss = 0.10527656\n",
      "Iteration 52, loss = 0.08091984\n",
      "Iteration 53, loss = 0.08871087\n",
      "Iteration 39, loss = 0.08451209\n",
      "Iteration 54, loss = 0.11899678\n",
      "Iteration 40, loss = 0.07869532\n",
      "Iteration 41, loss = 0.09100970\n",
      "Iteration 55, loss = 0.09076078\n",
      "Iteration 42, loss = 0.07793319\n",
      "Iteration 56, loss = 0.11451858\n",
      "Iteration 43, loss = 0.07337442\n",
      "Iteration 57, loss = 0.08778427\n",
      "Iteration 44, loss = 0.06712842\n",
      "Iteration 58, loss = 0.07195432\n",
      "Iteration 45, loss = 0.07106216\n",
      "Iteration 59, loss = 0.06892761\n",
      "Iteration 60, loss = 0.06360056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.07062303\n",
      "Iteration 47, loss = 0.07400353\n",
      "Iteration 48, loss = 0.07244192\n",
      "Iteration 49, loss = 0.08260903\n",
      "Iteration 50, loss = 0.07275685\n",
      "Iteration 51, loss = 0.05647240\n",
      "Iteration 52, loss = 0.08833581\n",
      "Iteration 53, loss = 0.09036157\n",
      "Iteration 54, loss = 0.05717733\n",
      "Iteration 55, loss = 0.07617151\n",
      "Iteration 56, loss = 0.05165989\n",
      "Iteration 57, loss = 0.05903010\n",
      "Iteration 58, loss = 0.05618140\n",
      "Iteration 59, loss = 0.05899268\n",
      "Iteration 60, loss = 0.05000074\n",
      "Iteration 61, loss = 0.04179981\n",
      "Iteration 62, loss = 0.05177441\n",
      "Iteration 63, loss = 0.08063761\n",
      "Iteration 64, loss = 0.11262163\n",
      "Iteration 65, loss = 0.07480705\n",
      "Iteration 66, loss = 0.06468639\n",
      "Iteration 1, loss = 1.08771443\n",
      "Iteration 67, loss = 0.07621143\n",
      "Iteration 2, loss = 0.54110162\n",
      "Iteration 68, loss = 0.05945392\n",
      "Iteration 69, loss = 0.06487480\n",
      "Iteration 3, loss = 0.42693753\n",
      "Iteration 70, loss = 0.05531887\n",
      "Iteration 4, loss = 0.37386381\n",
      "Iteration 71, loss = 0.09353785\n",
      "Iteration 5, loss = 0.33825777\n",
      "Iteration 72, loss = 0.05171177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.30551539\n",
      "Iteration 7, loss = 0.29102821\n",
      "Iteration 8, loss = 0.27721874\n",
      "Iteration 9, loss = 0.26167035\n",
      "Iteration 10, loss = 0.24466845\n",
      "Iteration 11, loss = 0.23338448\n",
      "Iteration 12, loss = 0.20906016\n",
      "Iteration 13, loss = 0.20456135\n",
      "Iteration 14, loss = 0.19251517\n",
      "Iteration 15, loss = 0.17810225\n",
      "Iteration 16, loss = 0.17330667\n",
      "Iteration 17, loss = 0.16988624\n",
      "Iteration 18, loss = 0.16483653\n",
      "Iteration 19, loss = 0.17484796\n",
      "Iteration 20, loss = 0.14759365\n",
      "Iteration 21, loss = 0.13251075\n",
      "Iteration 22, loss = 0.13814814\n",
      "Iteration 23, loss = 0.12064118\n",
      "Iteration 24, loss = 0.12562170\n",
      "Iteration 25, loss = 0.12172811\n",
      "Iteration 26, loss = 0.12413796\n",
      "Iteration 27, loss = 0.11083948\n",
      "Iteration 28, loss = 0.11713412\n",
      "Iteration 29, loss = 0.10214688\n",
      "Iteration 30, loss = 0.11635806\n",
      "Iteration 31, loss = 0.10010666\n",
      "Iteration 32, loss = 0.10439342\n",
      "Iteration 33, loss = 0.09679782\n",
      "Iteration 1, loss = 1.03123222\n",
      "Iteration 34, loss = 0.10720254\n",
      "Iteration 2, loss = 0.48562929\n",
      "Iteration 35, loss = 0.11526107\n",
      "Iteration 3, loss = 0.41190296\n",
      "Iteration 36, loss = 0.12507395\n",
      "Iteration 4, loss = 0.33787257\n",
      "Iteration 37, loss = 0.16053739\n",
      "Iteration 5, loss = 0.33845947\n",
      "Iteration 38, loss = 0.14386514\n",
      "Iteration 6, loss = 0.30469920\n",
      "Iteration 39, loss = 0.15648381\n",
      "Iteration 7, loss = 0.29090282\n",
      "Iteration 40, loss = 0.14999655\n",
      "Iteration 8, loss = 0.28043792\n",
      "Iteration 41, loss = 0.16572996\n",
      "Iteration 9, loss = 0.24759493\n",
      "Iteration 42, loss = 0.11949062\n",
      "Iteration 10, loss = 0.23581194\n",
      "Iteration 43, loss = 0.11696596\n",
      "Iteration 11, loss = 0.22972170\n",
      "Iteration 44, loss = 0.10991995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.28449696\n",
      "Iteration 13, loss = 0.22153653\n",
      "Iteration 14, loss = 0.22555578\n",
      "Iteration 15, loss = 0.18847622\n",
      "Iteration 16, loss = 0.18196219\n",
      "Iteration 17, loss = 0.17480485\n",
      "Iteration 18, loss = 0.18460494\n",
      "Iteration 19, loss = 0.16664392\n",
      "Iteration 20, loss = 0.15156626\n",
      "Iteration 21, loss = 0.15137523\n",
      "Iteration 22, loss = 0.14663116\n",
      "Iteration 23, loss = 0.15404841\n",
      "Iteration 24, loss = 0.15349210\n",
      "Iteration 1, loss = 0.98111721\n",
      "Iteration 25, loss = 0.13210760\n",
      "Iteration 2, loss = 0.50817224\n",
      "Iteration 26, loss = 0.13448353\n",
      "Iteration 3, loss = 0.40678267\n",
      "Iteration 27, loss = 0.14954815\n",
      "Iteration 4, loss = 0.36401442\n",
      "Iteration 28, loss = 0.15396683\n",
      "Iteration 5, loss = 0.33045564\n",
      "Iteration 29, loss = 0.12516269\n",
      "Iteration 6, loss = 0.30454720\n",
      "Iteration 30, loss = 0.13122907\n",
      "Iteration 7, loss = 0.28180338\n",
      "Iteration 8, loss = 0.27231221\n",
      "Iteration 31, loss = 0.12530155\n",
      "Iteration 9, loss = 0.26960446\n",
      "Iteration 32, loss = 0.13190416\n",
      "Iteration 10, loss = 0.24274453\n",
      "Iteration 33, loss = 0.11972549\n",
      "Iteration 11, loss = 0.24394018\n",
      "Iteration 34, loss = 0.10023000\n",
      "Iteration 12, loss = 0.22269955\n",
      "Iteration 35, loss = 0.12975950\n",
      "Iteration 13, loss = 0.22975819\n",
      "Iteration 36, loss = 0.10579532\n",
      "Iteration 14, loss = 0.23100080\n",
      "Iteration 37, loss = 0.12474449\n",
      "Iteration 15, loss = 0.19254179\n",
      "Iteration 38, loss = 0.15207677\n",
      "Iteration 16, loss = 0.18175025\n",
      "Iteration 39, loss = 0.10509422\n",
      "Iteration 17, loss = 0.18774840\n",
      "Iteration 40, loss = 0.10356894\n",
      "Iteration 18, loss = 0.16533995\n",
      "Iteration 41, loss = 0.09828090\n",
      "Iteration 19, loss = 0.15093541\n",
      "Iteration 42, loss = 0.11477411\n",
      "Iteration 20, loss = 0.17548965\n",
      "Iteration 43, loss = 0.13817539\n",
      "Iteration 21, loss = 0.15368787\n",
      "Iteration 44, loss = 0.13361490\n",
      "Iteration 22, loss = 0.13897459\n",
      "Iteration 45, loss = 0.15165932\n",
      "Iteration 23, loss = 0.15268507\n",
      "Iteration 46, loss = 0.17382063\n",
      "Iteration 24, loss = 0.13579543\n",
      "Iteration 47, loss = 0.15254722\n",
      "Iteration 25, loss = 0.12708665\n",
      "Iteration 48, loss = 0.16786982\n",
      "Iteration 26, loss = 0.12051291\n",
      "Iteration 49, loss = 0.10669290\n",
      "Iteration 27, loss = 0.13137082\n",
      "Iteration 50, loss = 0.13661920\n",
      "Iteration 28, loss = 0.14035735\n",
      "Iteration 51, loss = 0.13795501\n",
      "Iteration 29, loss = 0.14680700\n",
      "Iteration 52, loss = 0.13321654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.15116930\n",
      "Iteration 31, loss = 0.13166327\n",
      "Iteration 32, loss = 0.16583060\n",
      "Iteration 33, loss = 0.14600539\n",
      "Iteration 34, loss = 0.12983855\n",
      "Iteration 35, loss = 0.10417326\n",
      "Iteration 36, loss = 0.09915326\n",
      "Iteration 37, loss = 0.11126104\n",
      "Iteration 38, loss = 0.09365566\n",
      "Iteration 39, loss = 0.08978172\n",
      "Iteration 40, loss = 0.08125238\n",
      "Iteration 41, loss = 0.09067273\n",
      "Iteration 42, loss = 0.08152603\n",
      "Iteration 1, loss = 0.92769366\n",
      "Iteration 43, loss = 0.08539588\n",
      "Iteration 2, loss = 0.45728287\n",
      "Iteration 44, loss = 0.08410587\n",
      "Iteration 3, loss = 0.39800297\n",
      "Iteration 45, loss = 0.07870081\n",
      "Iteration 4, loss = 0.34134793\n",
      "Iteration 46, loss = 0.07139925\n",
      "Iteration 5, loss = 0.32883562\n",
      "Iteration 47, loss = 0.10284545\n",
      "Iteration 6, loss = 0.30034687\n",
      "Iteration 48, loss = 0.10981241\n",
      "Iteration 7, loss = 0.27144052\n",
      "Iteration 49, loss = 0.12819388\n",
      "Iteration 8, loss = 0.25769130\n",
      "Iteration 50, loss = 0.11970355\n",
      "Iteration 9, loss = 0.23941794\n",
      "Iteration 51, loss = 0.13029751\n",
      "Iteration 10, loss = 0.23676947\n",
      "Iteration 52, loss = 0.10868970\n",
      "Iteration 11, loss = 0.22843416\n",
      "Iteration 53, loss = 0.07789626\n",
      "Iteration 12, loss = 0.21992768\n",
      "Iteration 54, loss = 0.07823743\n",
      "Iteration 13, loss = 0.19380028\n",
      "Iteration 55, loss = 0.10522424\n",
      "Iteration 14, loss = 0.20055248\n",
      "Iteration 56, loss = 0.07134301\n",
      "Iteration 15, loss = 0.17116339\n",
      "Iteration 57, loss = 0.08777689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.18083589\n",
      "Iteration 17, loss = 0.15867206\n",
      "Iteration 18, loss = 0.15513822\n",
      "Iteration 19, loss = 0.15546465\n",
      "Iteration 20, loss = 0.14564866\n",
      "Iteration 21, loss = 0.14284604\n",
      "Iteration 22, loss = 0.12665806\n",
      "Iteration 23, loss = 0.14716101\n",
      "Iteration 24, loss = 0.12384043\n",
      "Iteration 25, loss = 0.12892869\n",
      "Iteration 26, loss = 0.11051716\n",
      "Iteration 27, loss = 0.13171325\n",
      "Iteration 28, loss = 0.11552247\n",
      "Iteration 1, loss = 0.84853470\n",
      "Iteration 29, loss = 0.12810809\n",
      "Iteration 2, loss = 0.44974317\n",
      "Iteration 30, loss = 0.17609582\n",
      "Iteration 3, loss = 0.40238333\n",
      "Iteration 31, loss = 0.17938101\n",
      "Iteration 4, loss = 0.35537053\n",
      "Iteration 32, loss = 0.16391624\n",
      "Iteration 5, loss = 0.32670531\n",
      "Iteration 33, loss = 0.14645880\n",
      "Iteration 6, loss = 0.30035157\n",
      "Iteration 34, loss = 0.15251396\n",
      "Iteration 7, loss = 0.27402672\n",
      "Iteration 35, loss = 0.12725246\n",
      "Iteration 8, loss = 0.26751174\n",
      "Iteration 36, loss = 0.11938157\n",
      "Iteration 9, loss = 0.24883284\n",
      "Iteration 37, loss = 0.11422550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.24300153\n",
      "Iteration 11, loss = 0.22805231\n",
      "Iteration 12, loss = 0.21487000\n",
      "Iteration 13, loss = 0.21559529\n",
      "Iteration 14, loss = 0.19749558\n",
      "Iteration 15, loss = 0.19033283\n",
      "Iteration 16, loss = 0.18187552\n",
      "Iteration 17, loss = 0.16804925\n",
      "Iteration 18, loss = 0.16086887\n",
      "Iteration 19, loss = 0.16641179\n",
      "Iteration 20, loss = 0.15738076\n",
      "Iteration 21, loss = 0.15879807\n",
      "Iteration 22, loss = 0.14329769\n",
      "Iteration 23, loss = 0.14834595\n",
      "Iteration 1, loss = 1.00863013\n",
      "Iteration 24, loss = 0.15582781\n",
      "Iteration 2, loss = 0.48840105\n",
      "Iteration 25, loss = 0.16933882\n",
      "Iteration 3, loss = 0.38353977\n",
      "Iteration 26, loss = 0.14708234\n",
      "Iteration 4, loss = 0.34337709\n",
      "Iteration 27, loss = 0.21347118\n",
      "Iteration 5, loss = 0.30724674\n",
      "Iteration 28, loss = 0.19521246\n",
      "Iteration 6, loss = 0.28668816\n",
      "Iteration 29, loss = 0.17487437\n",
      "Iteration 7, loss = 0.24704917\n",
      "Iteration 30, loss = 0.20839162\n",
      "Iteration 8, loss = 0.23047495\n",
      "Iteration 31, loss = 0.22176336\n",
      "Iteration 9, loss = 0.22349495\n",
      "Iteration 32, loss = 0.16489343\n",
      "Iteration 10, loss = 0.20361025\n",
      "Iteration 33, loss = 0.13201691\n",
      "Iteration 11, loss = 0.21243412\n",
      "Iteration 34, loss = 0.12252528\n",
      "Iteration 12, loss = 0.20187345\n",
      "Iteration 35, loss = 0.14513069\n",
      "Iteration 13, loss = 0.16505000\n",
      "Iteration 36, loss = 0.11399045\n",
      "Iteration 14, loss = 0.16489112\n",
      "Iteration 37, loss = 0.12513016\n",
      "Iteration 15, loss = 0.15052829\n",
      "Iteration 38, loss = 0.11601442\n",
      "Iteration 16, loss = 0.14420316\n",
      "Iteration 39, loss = 0.10368314\n",
      "Iteration 17, loss = 0.14494703\n",
      "Iteration 40, loss = 0.10287706\n",
      "Iteration 18, loss = 0.14450436\n",
      "Iteration 41, loss = 0.10493697\n",
      "Iteration 19, loss = 0.14444445\n",
      "Iteration 42, loss = 0.10593565\n",
      "Iteration 20, loss = 0.14004414\n",
      "Iteration 43, loss = 0.08875892\n",
      "Iteration 21, loss = 0.17165618\n",
      "Iteration 44, loss = 0.12486532\n",
      "Iteration 22, loss = 0.13402530\n",
      "Iteration 45, loss = 0.09289503\n",
      "Iteration 23, loss = 0.11022454\n",
      "Iteration 46, loss = 0.11621974\n",
      "Iteration 24, loss = 0.10668195\n",
      "Iteration 47, loss = 0.11393606\n",
      "Iteration 25, loss = 0.08687070\n",
      "Iteration 48, loss = 0.09850465\n",
      "Iteration 26, loss = 0.10051217\n",
      "Iteration 49, loss = 0.11973603\n",
      "Iteration 27, loss = 0.08833126\n",
      "Iteration 50, loss = 0.09428100\n",
      "Iteration 28, loss = 0.08826694\n",
      "Iteration 51, loss = 0.13989926\n",
      "Iteration 29, loss = 0.07378747\n",
      "Iteration 52, loss = 0.12802595\n",
      "Iteration 30, loss = 0.07672900\n",
      "Iteration 53, loss = 0.10962327\n",
      "Iteration 31, loss = 0.07394397\n",
      "Iteration 54, loss = 0.10781538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.07546403\n",
      "Iteration 33, loss = 0.06438424\n",
      "Iteration 34, loss = 0.07049060\n",
      "Iteration 35, loss = 0.07021020\n",
      "Iteration 36, loss = 0.10732763\n",
      "Iteration 37, loss = 0.10881973\n",
      "Iteration 38, loss = 0.11773063\n",
      "Iteration 39, loss = 0.12956028\n",
      "Iteration 40, loss = 0.10540487\n",
      "Iteration 41, loss = 0.09883690\n",
      "Iteration 42, loss = 0.11014881\n",
      "Iteration 43, loss = 0.08950322\n",
      "Iteration 44, loss = 0.10795132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95473829\n",
      "Iteration 2, loss = 0.44173758\n",
      "Iteration 3, loss = 0.36143636\n",
      "Iteration 4, loss = 0.32029894\n",
      "Iteration 5, loss = 0.28834664\n",
      "Iteration 6, loss = 0.27056452\n",
      "Iteration 7, loss = 0.25127765\n",
      "Iteration 8, loss = 0.23521335\n",
      "Iteration 9, loss = 0.22351676\n",
      "Iteration 10, loss = 0.20501800\n",
      "Iteration 11, loss = 0.18612918\n",
      "Iteration 12, loss = 0.17952421\n",
      "Iteration 13, loss = 0.17165722\n",
      "Iteration 14, loss = 0.16217900\n",
      "Iteration 1, loss = 1.05136783\n",
      "Iteration 15, loss = 0.14673452\n",
      "Iteration 2, loss = 0.48180981\n",
      "Iteration 16, loss = 0.14353928\n",
      "Iteration 3, loss = 0.39832823\n",
      "Iteration 17, loss = 0.14516867\n",
      "Iteration 4, loss = 0.34813857\n",
      "Iteration 18, loss = 0.13404684\n",
      "Iteration 5, loss = 0.32087994\n",
      "Iteration 19, loss = 0.12540668\n",
      "Iteration 6, loss = 0.28774540\n",
      "Iteration 20, loss = 0.13016087\n",
      "Iteration 7, loss = 0.27056459\n",
      "Iteration 21, loss = 0.13546678\n",
      "Iteration 8, loss = 0.25753540\n",
      "Iteration 22, loss = 0.13506748\n",
      "Iteration 9, loss = 0.25439701\n",
      "Iteration 23, loss = 0.11698932\n",
      "Iteration 10, loss = 0.24750553\n",
      "Iteration 24, loss = 0.12362757\n",
      "Iteration 11, loss = 0.22738069\n",
      "Iteration 25, loss = 0.12373284\n",
      "Iteration 12, loss = 0.23532335\n",
      "Iteration 26, loss = 0.12937004\n",
      "Iteration 13, loss = 0.24238563\n",
      "Iteration 14, loss = 0.20310778\n",
      "Iteration 27, loss = 0.12668745\n",
      "Iteration 15, loss = 0.19338250\n",
      "Iteration 28, loss = 0.12888818\n",
      "Iteration 29, loss = 0.10950153\n",
      "Iteration 16, loss = 0.17123879\n",
      "Iteration 17, loss = 0.16269277\n",
      "Iteration 30, loss = 0.11275381\n",
      "Iteration 18, loss = 0.15800854\n",
      "Iteration 31, loss = 0.09910923\n",
      "Iteration 19, loss = 0.15009063\n",
      "Iteration 32, loss = 0.10594718\n",
      "Iteration 20, loss = 0.14514400\n",
      "Iteration 33, loss = 0.08665392\n",
      "Iteration 21, loss = 0.14287486\n",
      "Iteration 34, loss = 0.10542727\n",
      "Iteration 22, loss = 0.14718107\n",
      "Iteration 35, loss = 0.09208182\n",
      "Iteration 23, loss = 0.14056170\n",
      "Iteration 36, loss = 0.11976867\n",
      "Iteration 37, loss = 0.10151882\n",
      "Iteration 24, loss = 0.15605011\n",
      "Iteration 38, loss = 0.11301510\n",
      "Iteration 25, loss = 0.12361084\n",
      "Iteration 39, loss = 0.10781485\n",
      "Iteration 26, loss = 0.12479886\n",
      "Iteration 40, loss = 0.12933576\n",
      "Iteration 27, loss = 0.12758078\n",
      "Iteration 41, loss = 0.10772684\n",
      "Iteration 28, loss = 0.12231944\n",
      "Iteration 42, loss = 0.11129253\n",
      "Iteration 29, loss = 0.13757402\n",
      "Iteration 43, loss = 0.08190979\n",
      "Iteration 30, loss = 0.13074947\n",
      "Iteration 44, loss = 0.08008843\n",
      "Iteration 31, loss = 0.12917561\n",
      "Iteration 45, loss = 0.08875732\n",
      "Iteration 32, loss = 0.11312731\n",
      "Iteration 46, loss = 0.09152025\n",
      "Iteration 33, loss = 0.10907237\n",
      "Iteration 47, loss = 0.09800027\n",
      "Iteration 34, loss = 0.09114404\n",
      "Iteration 48, loss = 0.07969502\n",
      "Iteration 35, loss = 0.10755531\n",
      "Iteration 49, loss = 0.06363770\n",
      "Iteration 36, loss = 0.08954855\n",
      "Iteration 50, loss = 0.09458404\n",
      "Iteration 37, loss = 0.09663627\n",
      "Iteration 51, loss = 0.07508328\n",
      "Iteration 38, loss = 0.10321139\n",
      "Iteration 39, loss = 0.09441332\n",
      "Iteration 40, loss = 0.07326364\n",
      "Iteration 52, loss = 0.07204216\n",
      "Iteration 41, loss = 0.07439600\n",
      "Iteration 53, loss = 0.05512957\n",
      "Iteration 42, loss = 0.10378201\n",
      "Iteration 54, loss = 0.06216543\n",
      "Iteration 43, loss = 0.09865239\n",
      "Iteration 55, loss = 0.05966629\n",
      "Iteration 44, loss = 0.11550591\n",
      "Iteration 56, loss = 0.05739907\n",
      "Iteration 45, loss = 0.08871067\n",
      "Iteration 57, loss = 0.06061914\n",
      "Iteration 46, loss = 0.08933304\n",
      "Iteration 58, loss = 0.05438608\n",
      "Iteration 47, loss = 0.10349394\n",
      "Iteration 59, loss = 0.05546859\n",
      "Iteration 48, loss = 0.19848788\n",
      "Iteration 60, loss = 0.05798895\n",
      "Iteration 49, loss = 0.13171858\n",
      "Iteration 61, loss = 0.07157078\n",
      "Iteration 50, loss = 0.12752846\n",
      "Iteration 62, loss = 0.09532479\n",
      "Iteration 51, loss = 0.10929490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 63, loss = 0.07544480\n",
      "Iteration 64, loss = 0.07402757\n",
      "Iteration 65, loss = 0.05616449\n",
      "Iteration 66, loss = 0.06044336\n",
      "Iteration 67, loss = 0.05138610\n",
      "Iteration 68, loss = 0.04627270\n",
      "Iteration 69, loss = 0.05954154\n",
      "Iteration 70, loss = 0.05762006\n",
      "Iteration 71, loss = 0.06156908\n",
      "Iteration 72, loss = 0.05364541\n",
      "Iteration 73, loss = 0.04620562\n",
      "Iteration 74, loss = 0.05271187\n",
      "Iteration 75, loss = 0.04769925\n",
      "Iteration 1, loss = 1.19904348\n",
      "Iteration 76, loss = 0.05305137\n",
      "Iteration 2, loss = 0.56462521\n",
      "Iteration 77, loss = 0.11987984\n",
      "Iteration 3, loss = 0.46621651\n",
      "Iteration 78, loss = 0.13598729\n",
      "Iteration 4, loss = 0.40475479\n",
      "Iteration 79, loss = 0.12137471\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.35811984\n",
      "Iteration 6, loss = 0.32042242\n",
      "Iteration 7, loss = 0.29562120\n",
      "Iteration 8, loss = 0.28291397\n",
      "Iteration 9, loss = 0.27442881\n",
      "Iteration 10, loss = 0.24797473\n",
      "Iteration 11, loss = 0.23452915\n",
      "Iteration 12, loss = 0.22020082\n",
      "Iteration 13, loss = 0.21280303\n",
      "Iteration 14, loss = 0.20177128\n",
      "Iteration 15, loss = 0.19502215\n",
      "Iteration 16, loss = 0.18397639\n",
      "Iteration 17, loss = 0.17709594\n",
      "Iteration 18, loss = 0.17025994\n",
      "Iteration 1, loss = 1.10763686\n",
      "Iteration 19, loss = 0.17494177\n",
      "Iteration 2, loss = 0.52592141\n",
      "Iteration 3, loss = 0.42572394\n",
      "Iteration 20, loss = 0.16595878\n",
      "Iteration 4, loss = 0.36951562\n",
      "Iteration 21, loss = 0.15240887\n",
      "Iteration 5, loss = 0.34076724\n",
      "Iteration 22, loss = 0.14860496\n",
      "Iteration 6, loss = 0.30914112\n",
      "Iteration 23, loss = 0.14373233\n",
      "Iteration 7, loss = 0.28301128\n",
      "Iteration 24, loss = 0.12911222\n",
      "Iteration 25, loss = 0.12467555\n",
      "Iteration 8, loss = 0.26851919\n",
      "Iteration 26, loss = 0.12739582\n",
      "Iteration 9, loss = 0.24639566\n",
      "Iteration 27, loss = 0.11306968\n",
      "Iteration 10, loss = 0.24024981\n",
      "Iteration 11, loss = 0.22248568\n",
      "Iteration 28, loss = 0.11042244\n",
      "Iteration 12, loss = 0.20354778\n",
      "Iteration 29, loss = 0.10277850\n",
      "Iteration 13, loss = 0.20023251\n",
      "Iteration 30, loss = 0.11914830\n",
      "Iteration 14, loss = 0.19501912\n",
      "Iteration 31, loss = 0.13151443\n",
      "Iteration 15, loss = 0.18559352\n",
      "Iteration 32, loss = 0.14364969\n",
      "Iteration 16, loss = 0.17360322\n",
      "Iteration 33, loss = 0.12836360\n",
      "Iteration 34, loss = 0.12495005\n",
      "Iteration 17, loss = 0.18393054\n",
      "Iteration 35, loss = 0.11328214\n",
      "Iteration 18, loss = 0.16491180\n",
      "Iteration 36, loss = 0.11929525\n",
      "Iteration 19, loss = 0.17028148\n",
      "Iteration 37, loss = 0.12665999\n",
      "Iteration 20, loss = 0.19254653\n",
      "Iteration 38, loss = 0.12088260\n",
      "Iteration 21, loss = 0.19790530\n",
      "Iteration 39, loss = 0.13233208\n",
      "Iteration 22, loss = 0.20925114\n",
      "Iteration 40, loss = 0.11421116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.16892794\n",
      "Iteration 24, loss = 0.15474570\n",
      "Iteration 25, loss = 0.14545587\n",
      "Iteration 26, loss = 0.12484106\n",
      "Iteration 27, loss = 0.11895071\n",
      "Iteration 28, loss = 0.11109107\n",
      "Iteration 29, loss = 0.11644497\n",
      "Iteration 30, loss = 0.12596429\n",
      "Iteration 31, loss = 0.11440700\n",
      "Iteration 32, loss = 0.11478757\n",
      "Iteration 33, loss = 0.11316382\n",
      "Iteration 34, loss = 0.10002275\n",
      "Iteration 35, loss = 0.10425178\n",
      "Iteration 1, loss = 1.05489621\n",
      "Iteration 36, loss = 0.10438846\n",
      "Iteration 2, loss = 0.47037400\n",
      "Iteration 37, loss = 0.08729216\n",
      "Iteration 3, loss = 0.40197687\n",
      "Iteration 38, loss = 0.08101124\n",
      "Iteration 4, loss = 0.36640563\n",
      "Iteration 39, loss = 0.10770008\n",
      "Iteration 40, loss = 0.11993470\n",
      "Iteration 5, loss = 0.33539884\n",
      "Iteration 6, loss = 0.30242908\n",
      "Iteration 41, loss = 0.11242453\n",
      "Iteration 7, loss = 0.27797942\n",
      "Iteration 42, loss = 0.08101135\n",
      "Iteration 8, loss = 0.26474063\n",
      "Iteration 43, loss = 0.08793279\n",
      "Iteration 9, loss = 0.25257901\n",
      "Iteration 44, loss = 0.09011027\n",
      "Iteration 10, loss = 0.23335496\n",
      "Iteration 45, loss = 0.08655576\n",
      "Iteration 11, loss = 0.21133233\n",
      "Iteration 46, loss = 0.08000408\n",
      "Iteration 12, loss = 0.20955510\n",
      "Iteration 47, loss = 0.08216511\n",
      "Iteration 13, loss = 0.19610634\n",
      "Iteration 48, loss = 0.07765379\n",
      "Iteration 14, loss = 0.17739430\n",
      "Iteration 49, loss = 0.08185792\n",
      "Iteration 15, loss = 0.16858094\n",
      "Iteration 50, loss = 0.08276494\n",
      "Iteration 16, loss = 0.16858121\n",
      "Iteration 51, loss = 0.09824199\n",
      "Iteration 17, loss = 0.15331470\n",
      "Iteration 52, loss = 0.08289823\n",
      "Iteration 18, loss = 0.14937999\n",
      "Iteration 53, loss = 0.08823207\n",
      "Iteration 19, loss = 0.14162653\n",
      "Iteration 54, loss = 0.07720509\n",
      "Iteration 20, loss = 0.12486147\n",
      "Iteration 55, loss = 0.05629350\n",
      "Iteration 21, loss = 0.13286189\n",
      "Iteration 56, loss = 0.06823070\n",
      "Iteration 22, loss = 0.12647463\n",
      "Iteration 57, loss = 0.05566865\n",
      "Iteration 23, loss = 0.14710523\n",
      "Iteration 58, loss = 0.06880789\n",
      "Iteration 24, loss = 0.14187570\n",
      "Iteration 59, loss = 0.05444253\n",
      "Iteration 25, loss = 0.17236006\n",
      "Iteration 60, loss = 0.06034352\n",
      "Iteration 26, loss = 0.14331561\n",
      "Iteration 61, loss = 0.06354301\n",
      "Iteration 27, loss = 0.17437887\n",
      "Iteration 62, loss = 0.05593854\n",
      "Iteration 28, loss = 0.17849997\n",
      "Iteration 63, loss = 0.09659549\n",
      "Iteration 29, loss = 0.19681298\n",
      "Iteration 64, loss = 0.09215996\n",
      "Iteration 30, loss = 0.15251646\n",
      "Iteration 65, loss = 0.09562773\n",
      "Iteration 31, loss = 0.15525422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 66, loss = 0.08700232\n",
      "Iteration 67, loss = 0.08386296\n",
      "Iteration 68, loss = 0.05579930\n",
      "Iteration 69, loss = 0.09586412\n",
      "Iteration 70, loss = 0.10141782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.78546553\n",
      "Iteration 2, loss = 0.46058130\n",
      "Iteration 3, loss = 0.40420470\n",
      "Iteration 4, loss = 0.35496142\n",
      "Iteration 1, loss = 0.71085275\n",
      "Iteration 5, loss = 0.32167446\n",
      "Iteration 2, loss = 0.43445520\n",
      "Iteration 6, loss = 0.29560229\n",
      "Iteration 3, loss = 0.38385955\n",
      "Iteration 7, loss = 0.27544466\n",
      "Iteration 4, loss = 0.34439223\n",
      "Iteration 8, loss = 0.26187270\n",
      "Iteration 5, loss = 0.32220556\n",
      "Iteration 9, loss = 0.24552640\n",
      "Iteration 6, loss = 0.29394551\n",
      "Iteration 10, loss = 0.23362803\n",
      "Iteration 7, loss = 0.27789370\n",
      "Iteration 11, loss = 0.22367763\n",
      "Iteration 8, loss = 0.27943714\n",
      "Iteration 12, loss = 0.22505814\n",
      "Iteration 9, loss = 0.26362196\n",
      "Iteration 13, loss = 0.22070134\n",
      "Iteration 10, loss = 0.23006929\n",
      "Iteration 14, loss = 0.22349299\n",
      "Iteration 11, loss = 0.21546740\n",
      "Iteration 15, loss = 0.21656763\n",
      "Iteration 12, loss = 0.21118688\n",
      "Iteration 16, loss = 0.19164865\n",
      "Iteration 13, loss = 0.21038325\n",
      "Iteration 17, loss = 0.18871684\n",
      "Iteration 14, loss = 0.20105701\n",
      "Iteration 18, loss = 0.16775803\n",
      "Iteration 15, loss = 0.18590268\n",
      "Iteration 19, loss = 0.16681958\n",
      "Iteration 16, loss = 0.16882760\n",
      "Iteration 20, loss = 0.17314010\n",
      "Iteration 17, loss = 0.18736039\n",
      "Iteration 21, loss = 0.18581390\n",
      "Iteration 18, loss = 0.16973542\n",
      "Iteration 22, loss = 0.22172581\n",
      "Iteration 19, loss = 0.15210406\n",
      "Iteration 23, loss = 0.17387618\n",
      "Iteration 20, loss = 0.14428655\n",
      "Iteration 24, loss = 0.17898912\n",
      "Iteration 21, loss = 0.12838206\n",
      "Iteration 25, loss = 0.16727947\n",
      "Iteration 22, loss = 0.15410746\n",
      "Iteration 26, loss = 0.14876453\n",
      "Iteration 23, loss = 0.12845967\n",
      "Iteration 27, loss = 0.14626550\n",
      "Iteration 24, loss = 0.13632501\n",
      "Iteration 28, loss = 0.15389111\n",
      "Iteration 25, loss = 0.11349977\n",
      "Iteration 29, loss = 0.15218624\n",
      "Iteration 26, loss = 0.12921426\n",
      "Iteration 30, loss = 0.11973919\n",
      "Iteration 27, loss = 0.12106077\n",
      "Iteration 31, loss = 0.13257608\n",
      "Iteration 28, loss = 0.09612754\n",
      "Iteration 32, loss = 0.10812104\n",
      "Iteration 29, loss = 0.09260500\n",
      "Iteration 33, loss = 0.10083622\n",
      "Iteration 30, loss = 0.09770755\n",
      "Iteration 34, loss = 0.09709814\n",
      "Iteration 31, loss = 0.08538778\n",
      "Iteration 35, loss = 0.09594374\n",
      "Iteration 32, loss = 0.09388584\n",
      "Iteration 36, loss = 0.09442296\n",
      "Iteration 33, loss = 0.09768138\n",
      "Iteration 37, loss = 0.12032494\n",
      "Iteration 34, loss = 0.10294049\n",
      "Iteration 38, loss = 0.10884963\n",
      "Iteration 35, loss = 0.12554003\n",
      "Iteration 39, loss = 0.09442522\n",
      "Iteration 36, loss = 0.11316397\n",
      "Iteration 40, loss = 0.09822784\n",
      "Iteration 37, loss = 0.14664148\n",
      "Iteration 41, loss = 0.08831702\n",
      "Iteration 38, loss = 0.12870872\n",
      "Iteration 39, loss = 0.11686076\n",
      "Iteration 42, loss = 0.09860331\n",
      "Iteration 40, loss = 0.10538384\n",
      "Iteration 43, loss = 0.08747039\n",
      "Iteration 41, loss = 0.11139835\n",
      "Iteration 44, loss = 0.09094368\n",
      "Iteration 42, loss = 0.13622739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.08208261\n",
      "Iteration 46, loss = 0.12359583\n",
      "Iteration 47, loss = 0.08546341\n",
      "Iteration 48, loss = 0.09092873\n",
      "Iteration 49, loss = 0.09802408\n",
      "Iteration 50, loss = 0.09212961\n",
      "Iteration 51, loss = 0.09676021\n",
      "Iteration 52, loss = 0.19872823\n",
      "Iteration 53, loss = 0.20856826\n",
      "Iteration 54, loss = 0.16025080\n",
      "Iteration 55, loss = 0.13584490\n",
      "Iteration 56, loss = 0.11325141\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07715690\n",
      "Iteration 2, loss = 0.52798383\n",
      "Iteration 3, loss = 0.41333213\n",
      "Iteration 4, loss = 0.38464037\n",
      "Iteration 5, loss = 0.33890452\n",
      "Iteration 6, loss = 0.32317033\n",
      "Iteration 7, loss = 0.30665758\n",
      "Iteration 8, loss = 0.26817148\n",
      "Iteration 9, loss = 0.25575095\n",
      "Iteration 10, loss = 0.24567876\n",
      "Iteration 11, loss = 0.23168467\n",
      "Iteration 12, loss = 0.21324202\n",
      "Iteration 1, loss = 0.98841854\n",
      "Iteration 2, loss = 0.49867775\n",
      "Iteration 13, loss = 0.20811094\n",
      "Iteration 3, loss = 0.42983930\n",
      "Iteration 14, loss = 0.20930884\n",
      "Iteration 15, loss = 0.19588509\n",
      "Iteration 4, loss = 0.36578564\n",
      "Iteration 16, loss = 0.18128664\n",
      "Iteration 5, loss = 0.34018814\n",
      "Iteration 17, loss = 0.16840449\n",
      "Iteration 6, loss = 0.29767552\n",
      "Iteration 18, loss = 0.15187428\n",
      "Iteration 7, loss = 0.28197192\n",
      "Iteration 19, loss = 0.15252539\n",
      "Iteration 8, loss = 0.26687990\n",
      "Iteration 20, loss = 0.14649413\n",
      "Iteration 9, loss = 0.25492185\n",
      "Iteration 21, loss = 0.14442446\n",
      "Iteration 10, loss = 0.23809745\n",
      "Iteration 22, loss = 0.13619092\n",
      "Iteration 11, loss = 0.22571885\n",
      "Iteration 23, loss = 0.13564416\n",
      "Iteration 12, loss = 0.21770292\n",
      "Iteration 24, loss = 0.14130223\n",
      "Iteration 13, loss = 0.20612793\n",
      "Iteration 25, loss = 0.13741660\n",
      "Iteration 14, loss = 0.21331304\n",
      "Iteration 26, loss = 0.13869156\n",
      "Iteration 15, loss = 0.23352105\n",
      "Iteration 27, loss = 0.14830064\n",
      "Iteration 16, loss = 0.22872883\n",
      "Iteration 28, loss = 0.15458579\n",
      "Iteration 17, loss = 0.23135980\n",
      "Iteration 29, loss = 0.15416154\n",
      "Iteration 18, loss = 0.20571717\n",
      "Iteration 30, loss = 0.13500756\n",
      "Iteration 19, loss = 0.18037651\n",
      "Iteration 31, loss = 0.10412206\n",
      "Iteration 20, loss = 0.19162901\n",
      "Iteration 21, loss = 0.19123788\n",
      "Iteration 32, loss = 0.10088228\n",
      "Iteration 22, loss = 0.18294899\n",
      "Iteration 33, loss = 0.09680854\n",
      "Iteration 34, loss = 0.10019088\n",
      "Iteration 23, loss = 0.15700359\n",
      "Iteration 35, loss = 0.10129519\n",
      "Iteration 24, loss = 0.14738387\n",
      "Iteration 36, loss = 0.10215397\n",
      "Iteration 25, loss = 0.13929555\n",
      "Iteration 37, loss = 0.12471732\n",
      "Iteration 26, loss = 0.14621303\n",
      "Iteration 38, loss = 0.12691456\n",
      "Iteration 27, loss = 0.13159776\n",
      "Iteration 39, loss = 0.12466743\n",
      "Iteration 28, loss = 0.12287330\n",
      "Iteration 40, loss = 0.12310197\n",
      "Iteration 29, loss = 0.14809285\n",
      "Iteration 41, loss = 0.11769380\n",
      "Iteration 30, loss = 0.13511952\n",
      "Iteration 42, loss = 0.11119907\n",
      "Iteration 31, loss = 0.13007301\n",
      "Iteration 43, loss = 0.13845124\n",
      "Iteration 32, loss = 0.12787372\n",
      "Iteration 44, loss = 0.16581951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.11077683\n",
      "Iteration 34, loss = 0.11684125\n",
      "Iteration 35, loss = 0.10550345\n",
      "Iteration 36, loss = 0.11821296\n",
      "Iteration 37, loss = 0.11466175\n",
      "Iteration 38, loss = 0.13043116\n",
      "Iteration 39, loss = 0.15524783\n",
      "Iteration 40, loss = 0.15690495\n",
      "Iteration 41, loss = 0.11879444\n",
      "Iteration 42, loss = 0.14390533\n",
      "Iteration 43, loss = 0.10601496\n",
      "Iteration 44, loss = 0.10769943\n",
      "Iteration 45, loss = 0.10259120\n",
      "Iteration 1, loss = 0.75242638\n",
      "Iteration 46, loss = 0.11180775\n",
      "Iteration 2, loss = 0.44267879\n",
      "Iteration 47, loss = 0.11120885\n",
      "Iteration 3, loss = 0.35581339\n",
      "Iteration 48, loss = 0.08948535\n",
      "Iteration 4, loss = 0.30861519\n",
      "Iteration 49, loss = 0.10834394\n",
      "Iteration 5, loss = 0.28939771\n",
      "Iteration 50, loss = 0.09482263\n",
      "Iteration 6, loss = 0.27009688\n",
      "Iteration 51, loss = 0.10323039\n",
      "Iteration 7, loss = 0.24122217\n",
      "Iteration 52, loss = 0.11344063\n",
      "Iteration 8, loss = 0.23867826\n",
      "Iteration 53, loss = 0.09040064\n",
      "Iteration 9, loss = 0.22459166\n",
      "Iteration 54, loss = 0.07371714\n",
      "Iteration 10, loss = 0.20281218\n",
      "Iteration 55, loss = 0.07755959\n",
      "Iteration 11, loss = 0.18261252\n",
      "Iteration 56, loss = 0.10243865\n",
      "Iteration 12, loss = 0.16691070\n",
      "Iteration 57, loss = 0.10353493\n",
      "Iteration 13, loss = 0.18911882\n",
      "Iteration 58, loss = 0.07827310\n",
      "Iteration 14, loss = 0.18452369\n",
      "Iteration 59, loss = 0.10154330\n",
      "Iteration 15, loss = 0.16816870\n",
      "Iteration 60, loss = 0.11468150\n",
      "Iteration 16, loss = 0.14985891\n",
      "Iteration 61, loss = 0.13669012\n",
      "Iteration 17, loss = 0.12154814\n",
      "Iteration 62, loss = 0.11765166\n",
      "Iteration 18, loss = 0.11698913\n",
      "Iteration 63, loss = 0.10463571\n",
      "Iteration 19, loss = 0.11410825\n",
      "Iteration 64, loss = 0.08813738\n",
      "Iteration 20, loss = 0.10592550\n",
      "Iteration 65, loss = 0.10196390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.10031645\n",
      "Iteration 22, loss = 0.10641364\n",
      "Iteration 23, loss = 0.11263022\n",
      "Iteration 24, loss = 0.11298936\n",
      "Iteration 25, loss = 0.10089409\n",
      "Iteration 26, loss = 0.08992379\n",
      "Iteration 27, loss = 0.08423899\n",
      "Iteration 28, loss = 0.08328446\n",
      "Iteration 29, loss = 0.07292309\n",
      "Iteration 30, loss = 0.08254949\n",
      "Iteration 31, loss = 0.09433396\n",
      "Iteration 32, loss = 0.07598794\n",
      "Iteration 33, loss = 0.08872158\n",
      "Iteration 1, loss = 0.78299466\n",
      "Iteration 34, loss = 0.10003384\n",
      "Iteration 2, loss = 0.41422628\n",
      "Iteration 35, loss = 0.07835921\n",
      "Iteration 3, loss = 0.33788246\n",
      "Iteration 36, loss = 0.08367717\n",
      "Iteration 4, loss = 0.29831476\n",
      "Iteration 37, loss = 0.09079871\n",
      "Iteration 5, loss = 0.28147103\n",
      "Iteration 38, loss = 0.10048419\n",
      "Iteration 6, loss = 0.24823307\n",
      "Iteration 7, loss = 0.23126426\n",
      "Iteration 39, loss = 0.09020000\n",
      "Iteration 40, loss = 0.07193302\n",
      "Iteration 8, loss = 0.22137407\n",
      "Iteration 9, loss = 0.20657743\n",
      "Iteration 41, loss = 0.06761589\n",
      "Iteration 10, loss = 0.19395245\n",
      "Iteration 42, loss = 0.07434116\n",
      "Iteration 11, loss = 0.18001051\n",
      "Iteration 43, loss = 0.09480320\n",
      "Iteration 12, loss = 0.17118321\n",
      "Iteration 44, loss = 0.10938070\n",
      "Iteration 13, loss = 0.17719623\n",
      "Iteration 45, loss = 0.06380580\n",
      "Iteration 14, loss = 0.19371625\n",
      "Iteration 46, loss = 0.06341253\n",
      "Iteration 15, loss = 0.18245235\n",
      "Iteration 47, loss = 0.06081070\n",
      "Iteration 16, loss = 0.15289732\n",
      "Iteration 48, loss = 0.05929016\n",
      "Iteration 17, loss = 0.15135258\n",
      "Iteration 49, loss = 0.06575075\n",
      "Iteration 18, loss = 0.13320296\n",
      "Iteration 50, loss = 0.06197341\n",
      "Iteration 19, loss = 0.13895406\n",
      "Iteration 51, loss = 0.06647262\n",
      "Iteration 20, loss = 0.13710705\n",
      "Iteration 52, loss = 0.06353859\n",
      "Iteration 21, loss = 0.12934761\n",
      "Iteration 53, loss = 0.04495232\n",
      "Iteration 22, loss = 0.13208043\n",
      "Iteration 54, loss = 0.04591668\n",
      "Iteration 23, loss = 0.14042623\n",
      "Iteration 55, loss = 0.04202413\n",
      "Iteration 24, loss = 0.13421402\n",
      "Iteration 56, loss = 0.03596617\n",
      "Iteration 25, loss = 0.11767235\n",
      "Iteration 57, loss = 0.03374407\n",
      "Iteration 26, loss = 0.11666327\n",
      "Iteration 58, loss = 0.06070795\n",
      "Iteration 27, loss = 0.11440804\n",
      "Iteration 59, loss = 0.03704522\n",
      "Iteration 60, loss = 0.04206422\n",
      "Iteration 28, loss = 0.13322805\n",
      "Iteration 29, loss = 0.13994194\n",
      "Iteration 61, loss = 0.04255495\n",
      "Iteration 30, loss = 0.13328062\n",
      "Iteration 62, loss = 0.09426674\n",
      "Iteration 31, loss = 0.12740587\n",
      "Iteration 63, loss = 0.05527432\n",
      "Iteration 32, loss = 0.11891345\n",
      "Iteration 64, loss = 0.03474312\n",
      "Iteration 33, loss = 0.09647794\n",
      "Iteration 65, loss = 0.04659196\n",
      "Iteration 34, loss = 0.10879661\n",
      "Iteration 66, loss = 0.03795775\n",
      "Iteration 35, loss = 0.08836668\n",
      "Iteration 67, loss = 0.02785705\n",
      "Iteration 36, loss = 0.11928039\n",
      "Iteration 68, loss = 0.02597717\n",
      "Iteration 37, loss = 0.12140631\n",
      "Iteration 69, loss = 0.02281643\n",
      "Iteration 38, loss = 0.11448196\n",
      "Iteration 70, loss = 0.02203024\n",
      "Iteration 71, loss = 0.02760629Iteration 39, loss = 0.10493886\n",
      "\n",
      "Iteration 72, loss = 0.02225807\n",
      "Iteration 40, loss = 0.08679180\n",
      "Iteration 73, loss = 0.02259449\n",
      "Iteration 41, loss = 0.10044002\n",
      "Iteration 74, loss = 0.02318481\n",
      "Iteration 42, loss = 0.10627607\n",
      "Iteration 75, loss = 0.02391454\n",
      "Iteration 43, loss = 0.11701730\n",
      "Iteration 76, loss = 0.03902871\n",
      "Iteration 44, loss = 0.11574755\n",
      "Iteration 77, loss = 0.05027762\n",
      "Iteration 45, loss = 0.11698665\n",
      "Iteration 78, loss = 0.06757739\n",
      "Iteration 46, loss = 0.13075159\n",
      "Iteration 47, loss = 0.13258838\n",
      "Iteration 79, loss = 0.04370210\n",
      "Iteration 48, loss = 0.10752212\n",
      "Iteration 80, loss = 0.06704899\n",
      "Iteration 49, loss = 0.11415171\n",
      "Iteration 81, loss = 0.11275231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.09567591\n",
      "Iteration 51, loss = 0.07859152\n",
      "Iteration 52, loss = 0.07378984\n",
      "Iteration 53, loss = 0.08690913\n",
      "Iteration 54, loss = 0.07576950\n",
      "Iteration 55, loss = 0.06765952\n",
      "Iteration 56, loss = 0.07989603\n",
      "Iteration 57, loss = 0.08051565\n",
      "Iteration 58, loss = 0.06931510\n",
      "Iteration 59, loss = 0.06958419\n",
      "Iteration 60, loss = 0.06596049\n",
      "Iteration 61, loss = 0.09955658\n",
      "Iteration 62, loss = 0.06350146\n",
      "Iteration 1, loss = 0.92086478\n",
      "Iteration 63, loss = 0.05853769\n",
      "Iteration 2, loss = 0.48379768\n",
      "Iteration 64, loss = 0.05984773\n",
      "Iteration 3, loss = 0.40515365\n",
      "Iteration 65, loss = 0.04852908\n",
      "Iteration 4, loss = 0.36830310\n",
      "Iteration 66, loss = 0.04591872\n",
      "Iteration 5, loss = 0.32472288\n",
      "Iteration 67, loss = 0.04755983\n",
      "Iteration 6, loss = 0.29661170\n",
      "Iteration 68, loss = 0.04300413\n",
      "Iteration 7, loss = 0.28041234\n",
      "Iteration 69, loss = 0.04195513\n",
      "Iteration 8, loss = 0.25603245\n",
      "Iteration 70, loss = 0.04983120\n",
      "Iteration 9, loss = 0.24299243\n",
      "Iteration 71, loss = 0.04718314\n",
      "Iteration 10, loss = 0.23278650\n",
      "Iteration 72, loss = 0.05233643\n",
      "Iteration 11, loss = 0.21344424\n",
      "Iteration 73, loss = 0.04558687\n",
      "Iteration 12, loss = 0.20555776\n",
      "Iteration 74, loss = 0.04379875\n",
      "Iteration 13, loss = 0.20197333\n",
      "Iteration 75, loss = 0.07939712\n",
      "Iteration 14, loss = 0.19485750\n",
      "Iteration 76, loss = 0.04740315\n",
      "Iteration 15, loss = 0.21065864\n",
      "Iteration 77, loss = 0.04246714\n",
      "Iteration 16, loss = 0.18149363\n",
      "Iteration 78, loss = 0.05465635\n",
      "Iteration 17, loss = 0.18186515\n",
      "Iteration 79, loss = 0.04606865\n",
      "Iteration 18, loss = 0.17769913\n",
      "Iteration 80, loss = 0.05223784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.15953545\n",
      "Iteration 20, loss = 0.14508690\n",
      "Iteration 21, loss = 0.14582577\n",
      "Iteration 22, loss = 0.12178962\n",
      "Iteration 23, loss = 0.14244690\n",
      "Iteration 24, loss = 0.14825533\n",
      "Iteration 25, loss = 0.13116913\n",
      "Iteration 26, loss = 0.11900741\n",
      "Iteration 27, loss = 0.12826122\n",
      "Iteration 28, loss = 0.13575286\n",
      "Iteration 29, loss = 0.11772225\n",
      "Iteration 30, loss = 0.13148975\n",
      "Iteration 31, loss = 0.12563479\n",
      "Iteration 1, loss = 0.94722077\n",
      "Iteration 32, loss = 0.13147988\n",
      "Iteration 2, loss = 0.48858887\n",
      "Iteration 33, loss = 0.17287868\n",
      "Iteration 3, loss = 0.39293628\n",
      "Iteration 34, loss = 0.14707986\n",
      "Iteration 4, loss = 0.33415025\n",
      "Iteration 35, loss = 0.12888299\n",
      "Iteration 5, loss = 0.30713730\n",
      "Iteration 36, loss = 0.12006659\n",
      "Iteration 6, loss = 0.28531720\n",
      "Iteration 37, loss = 0.10377388\n",
      "Iteration 7, loss = 0.26282608\n",
      "Iteration 38, loss = 0.10764472\n",
      "Iteration 8, loss = 0.25184699\n",
      "Iteration 39, loss = 0.09350435\n",
      "Iteration 9, loss = 0.22851288\n",
      "Iteration 40, loss = 0.08861795\n",
      "Iteration 10, loss = 0.22422802\n",
      "Iteration 41, loss = 0.08711705\n",
      "Iteration 11, loss = 0.20808786\n",
      "Iteration 42, loss = 0.08677298\n",
      "Iteration 12, loss = 0.19917851\n",
      "Iteration 43, loss = 0.08845983\n",
      "Iteration 13, loss = 0.18860192\n",
      "Iteration 44, loss = 0.07570288\n",
      "Iteration 14, loss = 0.17566686\n",
      "Iteration 45, loss = 0.07803669\n",
      "Iteration 15, loss = 0.17058375\n",
      "Iteration 46, loss = 0.10489313\n",
      "Iteration 16, loss = 0.17380541\n",
      "Iteration 47, loss = 0.12495978\n",
      "Iteration 17, loss = 0.16862759\n",
      "Iteration 48, loss = 0.19637738\n",
      "Iteration 18, loss = 0.14340755\n",
      "Iteration 49, loss = 0.15682892\n",
      "Iteration 19, loss = 0.16072974\n",
      "Iteration 50, loss = 0.11992630\n",
      "Iteration 20, loss = 0.15292168\n",
      "Iteration 51, loss = 0.09221151\n",
      "Iteration 21, loss = 0.15596715\n",
      "Iteration 52, loss = 0.11172711\n",
      "Iteration 22, loss = 0.13090594\n",
      "Iteration 53, loss = 0.07821866\n",
      "Iteration 23, loss = 0.14513390\n",
      "Iteration 54, loss = 0.13394404\n",
      "Iteration 24, loss = 0.12895042\n",
      "Iteration 55, loss = 0.11896626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.13733569\n",
      "Iteration 26, loss = 0.12339035\n",
      "Iteration 27, loss = 0.12149452\n",
      "Iteration 28, loss = 0.16093501\n",
      "Iteration 29, loss = 0.12844908\n",
      "Iteration 30, loss = 0.11176674\n",
      "Iteration 31, loss = 0.11339941\n",
      "Iteration 32, loss = 0.12411118\n",
      "Iteration 33, loss = 0.11211381\n",
      "Iteration 34, loss = 0.10777666\n",
      "Iteration 35, loss = 0.09993164\n",
      "Iteration 36, loss = 0.10599595\n",
      "Iteration 37, loss = 0.11523637\n",
      "Iteration 1, loss = 1.15232299\n",
      "Iteration 38, loss = 0.11202799\n",
      "Iteration 2, loss = 0.52050426\n",
      "Iteration 39, loss = 0.13344831\n",
      "Iteration 3, loss = 0.47343877\n",
      "Iteration 40, loss = 0.10921130\n",
      "Iteration 4, loss = 0.39332270\n",
      "Iteration 41, loss = 0.10560273\n",
      "Iteration 5, loss = 0.34743412\n",
      "Iteration 42, loss = 0.12039661\n",
      "Iteration 6, loss = 0.31316854\n",
      "Iteration 43, loss = 0.09388171\n",
      "Iteration 7, loss = 0.28697286\n",
      "Iteration 44, loss = 0.08775310\n",
      "Iteration 8, loss = 0.27790798\n",
      "Iteration 45, loss = 0.08874685\n",
      "Iteration 9, loss = 0.25819061\n",
      "Iteration 46, loss = 0.07873021\n",
      "Iteration 10, loss = 0.24136728\n",
      "Iteration 47, loss = 0.06832644\n",
      "Iteration 48, loss = 0.07470288\n",
      "Iteration 11, loss = 0.22168725\n",
      "Iteration 49, loss = 0.05929662\n",
      "Iteration 12, loss = 0.21389193\n",
      "Iteration 13, loss = 0.20326153\n",
      "Iteration 50, loss = 0.07207425\n",
      "Iteration 51, loss = 0.09920484\n",
      "Iteration 14, loss = 0.19233401\n",
      "Iteration 52, loss = 0.12370826\n",
      "Iteration 15, loss = 0.18507695\n",
      "Iteration 53, loss = 0.09002200\n",
      "Iteration 16, loss = 0.18427663\n",
      "Iteration 54, loss = 0.11437063\n",
      "Iteration 17, loss = 0.16604483\n",
      "Iteration 55, loss = 0.15937249\n",
      "Iteration 18, loss = 0.16601986\n",
      "Iteration 56, loss = 0.23239487\n",
      "Iteration 19, loss = 0.15572227\n",
      "Iteration 57, loss = 0.15353804\n",
      "Iteration 20, loss = 0.15161825\n",
      "Iteration 58, loss = 0.13458905\n",
      "Iteration 21, loss = 0.16387418\n",
      "Iteration 59, loss = 0.10026270\n",
      "Iteration 22, loss = 0.14960289\n",
      "Iteration 60, loss = 0.11574990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.15298044\n",
      "Iteration 24, loss = 0.15953358\n",
      "Iteration 25, loss = 0.14587814\n",
      "Iteration 26, loss = 0.12349214\n",
      "Iteration 27, loss = 0.12168874\n",
      "Iteration 28, loss = 0.13564246\n",
      "Iteration 29, loss = 0.15561408\n",
      "Iteration 30, loss = 0.14691120\n",
      "Iteration 31, loss = 0.13438157\n",
      "Iteration 32, loss = 0.13602564\n",
      "Iteration 33, loss = 0.11678083\n",
      "Iteration 34, loss = 0.12445919\n",
      "Iteration 35, loss = 0.10750755\n",
      "Iteration 36, loss = 0.10781858\n",
      "Iteration 1, loss = 0.76798940\n",
      "Iteration 37, loss = 0.10695346\n",
      "Iteration 2, loss = 0.44537525\n",
      "Iteration 38, loss = 0.11727040\n",
      "Iteration 3, loss = 0.38180320\n",
      "Iteration 39, loss = 0.10229432\n",
      "Iteration 4, loss = 0.35373368\n",
      "Iteration 40, loss = 0.09133949\n",
      "Iteration 5, loss = 0.31208850\n",
      "Iteration 6, loss = 0.28479335\n",
      "Iteration 41, loss = 0.08303158\n",
      "Iteration 7, loss = 0.26357842\n",
      "Iteration 42, loss = 0.10533414\n",
      "Iteration 8, loss = 0.24832657\n",
      "Iteration 43, loss = 0.11495120\n",
      "Iteration 9, loss = 0.23099669\n",
      "Iteration 44, loss = 0.09389900\n",
      "Iteration 10, loss = 0.21469814\n",
      "Iteration 45, loss = 0.09524982\n",
      "Iteration 11, loss = 0.20739890\n",
      "Iteration 46, loss = 0.10124822\n",
      "Iteration 12, loss = 0.19825800\n",
      "Iteration 47, loss = 0.09821215\n",
      "Iteration 13, loss = 0.18176173\n",
      "Iteration 48, loss = 0.10695348\n",
      "Iteration 14, loss = 0.19037585\n",
      "Iteration 49, loss = 0.07478588\n",
      "Iteration 15, loss = 0.17426388\n",
      "Iteration 50, loss = 0.06708177\n",
      "Iteration 16, loss = 0.17291250\n",
      "Iteration 51, loss = 0.06907896\n",
      "Iteration 17, loss = 0.16983367\n",
      "Iteration 52, loss = 0.05978586\n",
      "Iteration 18, loss = 0.17679266\n",
      "Iteration 53, loss = 0.06373917\n",
      "Iteration 19, loss = 0.16414072\n",
      "Iteration 54, loss = 0.07584522\n",
      "Iteration 20, loss = 0.15465435\n",
      "Iteration 55, loss = 0.05866429\n",
      "Iteration 21, loss = 0.16723059\n",
      "Iteration 56, loss = 0.06977726\n",
      "Iteration 22, loss = 0.15359449\n",
      "Iteration 57, loss = 0.06421438\n",
      "Iteration 23, loss = 0.13068025\n",
      "Iteration 58, loss = 0.06458367\n",
      "Iteration 24, loss = 0.12529946\n",
      "Iteration 59, loss = 0.06972672\n",
      "Iteration 25, loss = 0.12372771\n",
      "Iteration 60, loss = 0.06402081\n",
      "Iteration 26, loss = 0.15165551\n",
      "Iteration 61, loss = 0.06933942\n",
      "Iteration 27, loss = 0.15701588\n",
      "Iteration 62, loss = 0.04733787\n",
      "Iteration 28, loss = 0.16555624\n",
      "Iteration 63, loss = 0.04411371\n",
      "Iteration 29, loss = 0.14983445\n",
      "Iteration 64, loss = 0.04928110\n",
      "Iteration 30, loss = 0.15960272\n",
      "Iteration 65, loss = 0.06450612\n",
      "Iteration 31, loss = 0.14018587\n",
      "Iteration 66, loss = 0.05920124\n",
      "Iteration 32, loss = 0.11224403\n",
      "Iteration 67, loss = 0.05595871\n",
      "Iteration 33, loss = 0.10288968\n",
      "Iteration 68, loss = 0.08574221\n",
      "Iteration 34, loss = 0.09839725\n",
      "Iteration 69, loss = 0.08803178\n",
      "Iteration 35, loss = 0.11107441\n",
      "Iteration 70, loss = 0.07521784\n",
      "Iteration 36, loss = 0.11274763\n",
      "Iteration 71, loss = 0.08039954\n",
      "Iteration 37, loss = 0.10369651\n",
      "Iteration 72, loss = 0.09981098\n",
      "Iteration 38, loss = 0.10866911\n",
      "Iteration 73, loss = 0.06931613\n",
      "Iteration 39, loss = 0.10054007\n",
      "Iteration 74, loss = 0.07303966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.10441554\n",
      "Iteration 41, loss = 0.09374528\n",
      "Iteration 42, loss = 0.12730700\n",
      "Iteration 43, loss = 0.08760754\n",
      "Iteration 44, loss = 0.10050320\n",
      "Iteration 45, loss = 0.07251507\n",
      "Iteration 46, loss = 0.08240566\n",
      "Iteration 47, loss = 0.06159352\n",
      "Iteration 48, loss = 0.07245694\n",
      "Iteration 49, loss = 0.06955119\n",
      "Iteration 50, loss = 0.06253698\n",
      "Iteration 51, loss = 0.06686517\n",
      "Iteration 52, loss = 0.06510580\n",
      "Iteration 1, loss = 0.85742408\n",
      "Iteration 53, loss = 0.06493940\n",
      "Iteration 2, loss = 0.44719468\n",
      "Iteration 54, loss = 0.07751635\n",
      "Iteration 3, loss = 0.36777623\n",
      "Iteration 55, loss = 0.09896009\n",
      "Iteration 4, loss = 0.33559840\n",
      "Iteration 56, loss = 0.10113018\n",
      "Iteration 5, loss = 0.30643131\n",
      "Iteration 57, loss = 0.06519935\n",
      "Iteration 6, loss = 0.27299894\n",
      "Iteration 58, loss = 0.09300839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.26744448\n",
      "Iteration 8, loss = 0.25652707\n",
      "Iteration 9, loss = 0.24225574\n",
      "Iteration 10, loss = 0.23297650\n",
      "Iteration 11, loss = 0.23295505\n",
      "Iteration 12, loss = 0.19726492\n",
      "Iteration 13, loss = 0.19035002\n",
      "Iteration 14, loss = 0.18263339\n",
      "Iteration 15, loss = 0.23208315\n",
      "Iteration 16, loss = 0.20519697\n",
      "Iteration 17, loss = 0.20596316\n",
      "Iteration 18, loss = 0.19493724\n",
      "Iteration 1, loss = 0.93600455\n",
      "Iteration 19, loss = 0.17986801\n",
      "Iteration 2, loss = 0.50665342\n",
      "Iteration 20, loss = 0.21582717\n",
      "Iteration 21, loss = 0.19678308\n",
      "Iteration 3, loss = 0.38468811\n",
      "Iteration 4, loss = 0.35083967\n",
      "Iteration 22, loss = 0.18653789\n",
      "Iteration 5, loss = 0.32581086\n",
      "Iteration 23, loss = 0.15379667\n",
      "Iteration 24, loss = 0.13962843\n",
      "Iteration 6, loss = 0.31306695\n",
      "Iteration 25, loss = 0.14205060\n",
      "Iteration 7, loss = 0.28321309\n",
      "Iteration 26, loss = 0.12386622\n",
      "Iteration 8, loss = 0.25559761\n",
      "Iteration 27, loss = 0.12516184\n",
      "Iteration 9, loss = 0.28252006\n",
      "Iteration 28, loss = 0.11750682\n",
      "Iteration 10, loss = 0.24433792\n",
      "Iteration 29, loss = 0.11518410\n",
      "Iteration 11, loss = 0.23046950\n",
      "Iteration 30, loss = 0.12684989\n",
      "Iteration 12, loss = 0.22369720\n",
      "Iteration 31, loss = 0.11542156\n",
      "Iteration 13, loss = 0.19916428\n",
      "Iteration 32, loss = 0.10395881\n",
      "Iteration 14, loss = 0.20861909\n",
      "Iteration 33, loss = 0.12887555\n",
      "Iteration 15, loss = 0.18704572\n",
      "Iteration 34, loss = 0.10602833\n",
      "Iteration 16, loss = 0.18463483\n",
      "Iteration 35, loss = 0.11800310\n",
      "Iteration 17, loss = 0.17682521\n",
      "Iteration 36, loss = 0.11606279\n",
      "Iteration 18, loss = 0.17743573\n",
      "Iteration 37, loss = 0.10468781\n",
      "Iteration 19, loss = 0.16076509\n",
      "Iteration 20, loss = 0.15857440\n",
      "Iteration 38, loss = 0.10595356\n",
      "Iteration 21, loss = 0.16409363\n",
      "Iteration 39, loss = 0.09811450\n",
      "Iteration 22, loss = 0.15099497\n",
      "Iteration 40, loss = 0.11340232\n",
      "Iteration 23, loss = 0.15785744\n",
      "Iteration 41, loss = 0.10932067\n",
      "Iteration 24, loss = 0.17156199\n",
      "Iteration 42, loss = 0.10927266\n",
      "Iteration 25, loss = 0.15821590\n",
      "Iteration 43, loss = 0.08489818\n",
      "Iteration 26, loss = 0.15061657\n",
      "Iteration 44, loss = 0.09286471\n",
      "Iteration 45, loss = 0.07447965\n",
      "Iteration 27, loss = 0.12966495\n",
      "Iteration 46, loss = 0.08553329\n",
      "Iteration 28, loss = 0.16443356\n",
      "Iteration 29, loss = 0.11511081\n",
      "Iteration 47, loss = 0.08646289\n",
      "Iteration 30, loss = 0.10714140\n",
      "Iteration 48, loss = 0.08154552\n",
      "Iteration 31, loss = 0.10389809\n",
      "Iteration 49, loss = 0.08513278\n",
      "Iteration 32, loss = 0.10027459\n",
      "Iteration 50, loss = 0.07236556\n",
      "Iteration 33, loss = 0.09867739\n",
      "Iteration 51, loss = 0.07473063\n",
      "Iteration 34, loss = 0.11253442\n",
      "Iteration 52, loss = 0.07727377\n",
      "Iteration 35, loss = 0.14727595\n",
      "Iteration 53, loss = 0.07823862\n",
      "Iteration 36, loss = 0.15921294\n",
      "Iteration 54, loss = 0.08530700\n",
      "Iteration 37, loss = 0.13268431\n",
      "Iteration 55, loss = 0.09301100\n",
      "Iteration 38, loss = 0.13197297\n",
      "Iteration 56, loss = 0.08835305\n",
      "Iteration 39, loss = 0.12301698\n",
      "Iteration 57, loss = 0.13272251\n",
      "Iteration 40, loss = 0.11889584\n",
      "Iteration 58, loss = 0.15385294\n",
      "Iteration 41, loss = 0.11712561\n",
      "Iteration 59, loss = 0.16285515\n",
      "Iteration 42, loss = 0.11951335\n",
      "Iteration 60, loss = 0.17031507\n",
      "Iteration 43, loss = 0.13532760\n",
      "Iteration 61, loss = 0.13694319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.12212028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03153896\n",
      "Iteration 2, loss = 0.48659689\n",
      "Iteration 1, loss = 1.01534377\n",
      "Iteration 3, loss = 0.40915016\n",
      "Iteration 2, loss = 0.53065801\n",
      "Iteration 4, loss = 0.36800336\n",
      "Iteration 3, loss = 0.43735172\n",
      "Iteration 5, loss = 0.33930249\n",
      "Iteration 4, loss = 0.36667674\n",
      "Iteration 6, loss = 0.32438522\n",
      "Iteration 5, loss = 0.32967260\n",
      "Iteration 7, loss = 0.28639095\n",
      "Iteration 6, loss = 0.30855554\n",
      "Iteration 8, loss = 0.26743890\n",
      "Iteration 7, loss = 0.28854324\n",
      "Iteration 9, loss = 0.24762049\n",
      "Iteration 8, loss = 0.27125125\n",
      "Iteration 10, loss = 0.23644586\n",
      "Iteration 9, loss = 0.26275451\n",
      "Iteration 11, loss = 0.23581865\n",
      "Iteration 10, loss = 0.24825871\n",
      "Iteration 12, loss = 0.24623634\n",
      "Iteration 11, loss = 0.23143368\n",
      "Iteration 13, loss = 0.25846761\n",
      "Iteration 12, loss = 0.21956849\n",
      "Iteration 14, loss = 0.25585419\n",
      "Iteration 13, loss = 0.21419712\n",
      "Iteration 15, loss = 0.21590078\n",
      "Iteration 14, loss = 0.19765807\n",
      "Iteration 16, loss = 0.21961512\n",
      "Iteration 15, loss = 0.19834169\n",
      "Iteration 17, loss = 0.23444698\n",
      "Iteration 16, loss = 0.19285516\n",
      "Iteration 18, loss = 0.20496328\n",
      "Iteration 17, loss = 0.22705705\n",
      "Iteration 19, loss = 0.18756401\n",
      "Iteration 18, loss = 0.16757846\n",
      "Iteration 20, loss = 0.17972774\n",
      "Iteration 19, loss = 0.17401270\n",
      "Iteration 21, loss = 0.18763716\n",
      "Iteration 20, loss = 0.21452505\n",
      "Iteration 22, loss = 0.16283172\n",
      "Iteration 21, loss = 0.18712359\n",
      "Iteration 23, loss = 0.15056170\n",
      "Iteration 22, loss = 0.16169078\n",
      "Iteration 24, loss = 0.14457001\n",
      "Iteration 23, loss = 0.14869236\n",
      "Iteration 25, loss = 0.14353566\n",
      "Iteration 24, loss = 0.14613791\n",
      "Iteration 26, loss = 0.13303727\n",
      "Iteration 25, loss = 0.14251585\n",
      "Iteration 27, loss = 0.12490559\n",
      "Iteration 26, loss = 0.12654468\n",
      "Iteration 28, loss = 0.12805665\n",
      "Iteration 27, loss = 0.13462950\n",
      "Iteration 29, loss = 0.13189595\n",
      "Iteration 28, loss = 0.13679898\n",
      "Iteration 30, loss = 0.12275503\n",
      "Iteration 29, loss = 0.13557305\n",
      "Iteration 31, loss = 0.12384264\n",
      "Iteration 30, loss = 0.16154413\n",
      "Iteration 32, loss = 0.11821535\n",
      "Iteration 31, loss = 0.15982856\n",
      "Iteration 33, loss = 0.12152794\n",
      "Iteration 32, loss = 0.13580104\n",
      "Iteration 34, loss = 0.11171327\n",
      "Iteration 33, loss = 0.16955218\n",
      "Iteration 35, loss = 0.10439549\n",
      "Iteration 34, loss = 0.13245990\n",
      "Iteration 36, loss = 0.10917702\n",
      "Iteration 35, loss = 0.13095658\n",
      "Iteration 37, loss = 0.10515827\n",
      "Iteration 36, loss = 0.11704355\n",
      "Iteration 38, loss = 0.10141395\n",
      "Iteration 37, loss = 0.13335190\n",
      "Iteration 39, loss = 0.11304753\n",
      "Iteration 38, loss = 0.11694033\n",
      "Iteration 40, loss = 0.10219114\n",
      "Iteration 39, loss = 0.09994778\n",
      "Iteration 41, loss = 0.10256782\n",
      "Iteration 40, loss = 0.09849487\n",
      "Iteration 42, loss = 0.12489354\n",
      "Iteration 41, loss = 0.10583246\n",
      "Iteration 43, loss = 0.10826809\n",
      "Iteration 42, loss = 0.09628434\n",
      "Iteration 44, loss = 0.08986100\n",
      "Iteration 43, loss = 0.09089410\n",
      "Iteration 45, loss = 0.09160188\n",
      "Iteration 44, loss = 0.10079399\n",
      "Iteration 46, loss = 0.10272688\n",
      "Iteration 45, loss = 0.08626786\n",
      "Iteration 47, loss = 0.10018490\n",
      "Iteration 46, loss = 0.10042742\n",
      "Iteration 48, loss = 0.11018527\n",
      "Iteration 47, loss = 0.14183559\n",
      "Iteration 49, loss = 0.08512784\n",
      "Iteration 50, loss = 0.12536296\n",
      "Iteration 48, loss = 0.13129693\n",
      "Iteration 51, loss = 0.11347391\n",
      "Iteration 49, loss = 0.15081432\n",
      "Iteration 50, loss = 0.11936610\n",
      "Iteration 52, loss = 0.09351597\n",
      "Iteration 51, loss = 0.08683683\n",
      "Iteration 53, loss = 0.13249007\n",
      "Iteration 52, loss = 0.09813925\n",
      "Iteration 54, loss = 0.09879646\n",
      "Iteration 53, loss = 0.10379038\n",
      "Iteration 55, loss = 0.08907655\n",
      "Iteration 54, loss = 0.10234583\n",
      "Iteration 56, loss = 0.08133907\n",
      "Iteration 55, loss = 0.12699155\n",
      "Iteration 57, loss = 0.07192223\n",
      "Iteration 56, loss = 0.12361143Iteration 58, loss = 0.05918880\n",
      "\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.05631635\n",
      "Iteration 60, loss = 0.05441048\n",
      "Iteration 61, loss = 0.05232398\n",
      "Iteration 62, loss = 0.04701126\n",
      "Iteration 63, loss = 0.04659359\n",
      "Iteration 64, loss = 0.04679172\n",
      "Iteration 65, loss = 0.04435602\n",
      "Iteration 66, loss = 0.04048706\n",
      "Iteration 67, loss = 0.04266308\n",
      "Iteration 68, loss = 0.04686739\n",
      "Iteration 69, loss = 0.04984076\n",
      "Iteration 70, loss = 0.05019221\n",
      "Iteration 71, loss = 0.06961760\n",
      "Iteration 1, loss = 0.95944425\n",
      "Iteration 72, loss = 0.06417664\n",
      "Iteration 2, loss = 0.46320527\n",
      "Iteration 73, loss = 0.07660663\n",
      "Iteration 3, loss = 0.37345936\n",
      "Iteration 74, loss = 0.10921692\n",
      "Iteration 4, loss = 0.32540645\n",
      "Iteration 75, loss = 0.07940101\n",
      "Iteration 5, loss = 0.29602342\n",
      "Iteration 76, loss = 0.12993153\n",
      "Iteration 6, loss = 0.28072537\n",
      "Iteration 77, loss = 0.13254904\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.25315035\n",
      "Iteration 8, loss = 0.25745869\n",
      "Iteration 9, loss = 0.23470425\n",
      "Iteration 10, loss = 0.20601027\n",
      "Iteration 11, loss = 0.19995783\n",
      "Iteration 12, loss = 0.18132690\n",
      "Iteration 13, loss = 0.18207883\n",
      "Iteration 14, loss = 0.17251058\n",
      "Iteration 15, loss = 0.18333976\n",
      "Iteration 16, loss = 0.16077620\n",
      "Iteration 17, loss = 0.15028531\n",
      "Iteration 18, loss = 0.15461935\n",
      "Iteration 19, loss = 0.14119718\n",
      "Iteration 20, loss = 0.14396467\n",
      "Iteration 1, loss = 0.99796551\n",
      "Iteration 21, loss = 0.11470610\n",
      "Iteration 2, loss = 0.47302970\n",
      "Iteration 22, loss = 0.10530183\n",
      "Iteration 3, loss = 0.38577075\n",
      "Iteration 23, loss = 0.09678853\n",
      "Iteration 4, loss = 0.33670023\n",
      "Iteration 24, loss = 0.09075964\n",
      "Iteration 5, loss = 0.30407271\n",
      "Iteration 25, loss = 0.09488394\n",
      "Iteration 6, loss = 0.29812426\n",
      "Iteration 26, loss = 0.09841176\n",
      "Iteration 7, loss = 0.27886682\n",
      "Iteration 27, loss = 0.09557075\n",
      "Iteration 8, loss = 0.24666082\n",
      "Iteration 28, loss = 0.08205496\n",
      "Iteration 9, loss = 0.23257358\n",
      "Iteration 29, loss = 0.08831990\n",
      "Iteration 10, loss = 0.21532905\n",
      "Iteration 30, loss = 0.06962092\n",
      "Iteration 11, loss = 0.20745282\n",
      "Iteration 31, loss = 0.06586871\n",
      "Iteration 12, loss = 0.19397971\n",
      "Iteration 32, loss = 0.06936447\n",
      "Iteration 13, loss = 0.18735280\n",
      "Iteration 33, loss = 0.08198491\n",
      "Iteration 14, loss = 0.18520014\n",
      "Iteration 34, loss = 0.11401279\n",
      "Iteration 15, loss = 0.16895168\n",
      "Iteration 35, loss = 0.09404202\n",
      "Iteration 16, loss = 0.15919957\n",
      "Iteration 36, loss = 0.11542188\n",
      "Iteration 17, loss = 0.16097562\n",
      "Iteration 37, loss = 0.14676367\n",
      "Iteration 18, loss = 0.17017946\n",
      "Iteration 38, loss = 0.17758509\n",
      "Iteration 19, loss = 0.16925120\n",
      "Iteration 39, loss = 0.10844646\n",
      "Iteration 20, loss = 0.13981754\n",
      "Iteration 40, loss = 0.14671560\n",
      "Iteration 21, loss = 0.13309265\n",
      "Iteration 41, loss = 0.09334376\n",
      "Iteration 22, loss = 0.12565312\n",
      "Iteration 42, loss = 0.08432870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.12337164\n",
      "Iteration 24, loss = 0.11876784\n",
      "Iteration 25, loss = 0.10949858\n",
      "Iteration 26, loss = 0.11415391\n",
      "Iteration 27, loss = 0.11422385\n",
      "Iteration 28, loss = 0.11157109\n",
      "Iteration 29, loss = 0.11217633\n",
      "Iteration 30, loss = 0.09415827\n",
      "Iteration 31, loss = 0.13162661\n",
      "Iteration 32, loss = 0.11519377\n",
      "Iteration 33, loss = 0.14606172\n",
      "Iteration 34, loss = 0.13404031\n",
      "Iteration 35, loss = 0.10275226\n",
      "Iteration 1, loss = 0.99680123\n",
      "Iteration 36, loss = 0.09055208\n",
      "Iteration 2, loss = 0.45695834\n",
      "Iteration 37, loss = 0.08619720\n",
      "Iteration 3, loss = 0.37497321\n",
      "Iteration 38, loss = 0.09989416\n",
      "Iteration 4, loss = 0.34444604\n",
      "Iteration 39, loss = 0.07804837\n",
      "Iteration 5, loss = 0.31401443\n",
      "Iteration 40, loss = 0.08931975\n",
      "Iteration 6, loss = 0.28766436\n",
      "Iteration 41, loss = 0.07543156\n",
      "Iteration 7, loss = 0.26517577\n",
      "Iteration 42, loss = 0.07946646\n",
      "Iteration 8, loss = 0.24673047\n",
      "Iteration 43, loss = 0.07431939\n",
      "Iteration 9, loss = 0.23895475\n",
      "Iteration 44, loss = 0.08210967\n",
      "Iteration 10, loss = 0.23189546\n",
      "Iteration 45, loss = 0.07308801\n",
      "Iteration 11, loss = 0.23341572\n",
      "Iteration 46, loss = 0.06346025\n",
      "Iteration 12, loss = 0.21928361\n",
      "Iteration 47, loss = 0.07511382\n",
      "Iteration 13, loss = 0.19061784\n",
      "Iteration 48, loss = 0.06040275\n",
      "Iteration 14, loss = 0.18554847\n",
      "Iteration 49, loss = 0.06411746\n",
      "Iteration 15, loss = 0.17817057\n",
      "Iteration 50, loss = 0.05787033\n",
      "Iteration 16, loss = 0.17428138\n",
      "Iteration 51, loss = 0.06427104\n",
      "Iteration 17, loss = 0.16742769\n",
      "Iteration 52, loss = 0.09553897\n",
      "Iteration 18, loss = 0.17227012\n",
      "Iteration 53, loss = 0.07617202\n",
      "Iteration 19, loss = 0.14813476\n",
      "Iteration 54, loss = 0.07552643\n",
      "Iteration 20, loss = 0.14858783\n",
      "Iteration 55, loss = 0.08010186\n",
      "Iteration 21, loss = 0.16068598\n",
      "Iteration 56, loss = 0.09527444\n",
      "Iteration 22, loss = 0.14017854\n",
      "Iteration 57, loss = 0.06455503\n",
      "Iteration 23, loss = 0.16784940\n",
      "Iteration 58, loss = 0.06592666\n",
      "Iteration 24, loss = 0.15907456\n",
      "Iteration 59, loss = 0.06647648\n",
      "Iteration 25, loss = 0.15024625\n",
      "Iteration 60, loss = 0.06051079\n",
      "Iteration 26, loss = 0.16370737\n",
      "Iteration 61, loss = 0.06540190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.17876081\n",
      "Iteration 28, loss = 0.16439918\n",
      "Iteration 29, loss = 0.14174472\n",
      "Iteration 30, loss = 0.12201596\n",
      "Iteration 31, loss = 0.10656445\n",
      "Iteration 32, loss = 0.11429980\n",
      "Iteration 33, loss = 0.10573970\n",
      "Iteration 34, loss = 0.10796951\n",
      "Iteration 35, loss = 0.11021012\n",
      "Iteration 36, loss = 0.13801393\n",
      "Iteration 37, loss = 0.11412828\n",
      "Iteration 38, loss = 0.11664681\n",
      "Iteration 39, loss = 0.10909389\n",
      "Iteration 1, loss = 0.85511636\n",
      "Iteration 40, loss = 0.10929226\n",
      "Iteration 2, loss = 0.48151769\n",
      "Iteration 41, loss = 0.12743733\n",
      "Iteration 3, loss = 0.38808422\n",
      "Iteration 42, loss = 0.09520692\n",
      "Iteration 4, loss = 0.35136915\n",
      "Iteration 43, loss = 0.09426657\n",
      "Iteration 5, loss = 0.30948304\n",
      "Iteration 6, loss = 0.30400320\n",
      "Iteration 44, loss = 0.08838485\n",
      "Iteration 7, loss = 0.30473959\n",
      "Iteration 45, loss = 0.10720745\n",
      "Iteration 8, loss = 0.26137189\n",
      "Iteration 46, loss = 0.07446245\n",
      "Iteration 9, loss = 0.24824734\n",
      "Iteration 47, loss = 0.07760037\n",
      "Iteration 10, loss = 0.23319141\n",
      "Iteration 48, loss = 0.07111826\n",
      "Iteration 11, loss = 0.23310425\n",
      "Iteration 49, loss = 0.06976922\n",
      "Iteration 12, loss = 0.20735129\n",
      "Iteration 50, loss = 0.07230093\n",
      "Iteration 13, loss = 0.19759127\n",
      "Iteration 51, loss = 0.07440906\n",
      "Iteration 14, loss = 0.18759596\n",
      "Iteration 52, loss = 0.09044953\n",
      "Iteration 15, loss = 0.18322098\n",
      "Iteration 53, loss = 0.07469358\n",
      "Iteration 16, loss = 0.16667506\n",
      "Iteration 54, loss = 0.09038700\n",
      "Iteration 17, loss = 0.17601286\n",
      "Iteration 55, loss = 0.11066539\n",
      "Iteration 18, loss = 0.15753839\n",
      "Iteration 56, loss = 0.12996759\n",
      "Iteration 19, loss = 0.16299666\n",
      "Iteration 57, loss = 0.18879022\n",
      "Iteration 20, loss = 0.16021855\n",
      "Iteration 58, loss = 0.13665041\n",
      "Iteration 21, loss = 0.15747542\n",
      "Iteration 59, loss = 0.11477067\n",
      "Iteration 22, loss = 0.14632766\n",
      "Iteration 60, loss = 0.11400983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.13738345\n",
      "Iteration 24, loss = 0.12133499\n",
      "Iteration 25, loss = 0.12923170\n",
      "Iteration 26, loss = 0.11097016\n",
      "Iteration 27, loss = 0.11547229\n",
      "Iteration 28, loss = 0.14106171\n",
      "Iteration 29, loss = 0.13034607\n",
      "Iteration 30, loss = 0.11151554\n",
      "Iteration 31, loss = 0.16076947\n",
      "Iteration 32, loss = 0.13311171\n",
      "Iteration 33, loss = 0.16482020\n",
      "Iteration 34, loss = 0.18088627\n",
      "Iteration 35, loss = 0.10614662\n",
      "Iteration 1, loss = 1.11843196\n",
      "Iteration 36, loss = 0.11612159\n",
      "Iteration 2, loss = 0.46962353\n",
      "Iteration 37, loss = 0.09626543\n",
      "Iteration 3, loss = 0.42443688\n",
      "Iteration 38, loss = 0.10108577\n",
      "Iteration 4, loss = 0.37805933\n",
      "Iteration 39, loss = 0.10295927\n",
      "Iteration 5, loss = 0.32832591\n",
      "Iteration 40, loss = 0.09916772\n",
      "Iteration 6, loss = 0.30147743\n",
      "Iteration 41, loss = 0.09037513\n",
      "Iteration 7, loss = 0.28356968\n",
      "Iteration 42, loss = 0.08522147\n",
      "Iteration 8, loss = 0.26166017\n",
      "Iteration 43, loss = 0.06773830\n",
      "Iteration 9, loss = 0.24258530\n",
      "Iteration 44, loss = 0.06741569\n",
      "Iteration 10, loss = 0.23601656\n",
      "Iteration 45, loss = 0.08821713\n",
      "Iteration 11, loss = 0.22066565\n",
      "Iteration 46, loss = 0.06801446\n",
      "Iteration 12, loss = 0.20647183\n",
      "Iteration 47, loss = 0.08952199\n",
      "Iteration 13, loss = 0.20134254\n",
      "Iteration 48, loss = 0.09268653\n",
      "Iteration 14, loss = 0.17799182\n",
      "Iteration 49, loss = 0.09869098\n",
      "Iteration 15, loss = 0.17788871\n",
      "Iteration 50, loss = 0.09872137\n",
      "Iteration 16, loss = 0.18209304\n",
      "Iteration 51, loss = 0.09269221\n",
      "Iteration 17, loss = 0.17390842\n",
      "Iteration 52, loss = 0.15895505\n",
      "Iteration 18, loss = 0.17727568\n",
      "Iteration 53, loss = 0.11369237\n",
      "Iteration 19, loss = 0.14795016\n",
      "Iteration 54, loss = 0.08669847\n",
      "Iteration 20, loss = 0.16824439\n",
      "Iteration 55, loss = 0.08114211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.14564252\n",
      "Iteration 22, loss = 0.15389680\n",
      "Iteration 23, loss = 0.12356441\n",
      "Iteration 24, loss = 0.13322468\n",
      "Iteration 25, loss = 0.15091486\n",
      "Iteration 26, loss = 0.13919424\n",
      "Iteration 27, loss = 0.15729124\n",
      "Iteration 28, loss = 0.18235412\n",
      "Iteration 29, loss = 0.12998056\n",
      "Iteration 30, loss = 0.11527699\n",
      "Iteration 31, loss = 0.10198824\n",
      "Iteration 32, loss = 0.09972120\n",
      "Iteration 33, loss = 0.09636426\n",
      "Iteration 1, loss = 0.93241925\n",
      "Iteration 34, loss = 0.10444016\n",
      "Iteration 2, loss = 0.44518847\n",
      "Iteration 35, loss = 0.09346145\n",
      "Iteration 3, loss = 0.38917784\n",
      "Iteration 36, loss = 0.09846628\n",
      "Iteration 4, loss = 0.33956819\n",
      "Iteration 37, loss = 0.10926722\n",
      "Iteration 5, loss = 0.30646280\n",
      "Iteration 38, loss = 0.09041219\n",
      "Iteration 6, loss = 0.28599991\n",
      "Iteration 39, loss = 0.10363958\n",
      "Iteration 7, loss = 0.27525733\n",
      "Iteration 40, loss = 0.08295845\n",
      "Iteration 8, loss = 0.26080754\n",
      "Iteration 41, loss = 0.08974551\n",
      "Iteration 9, loss = 0.24443867\n",
      "Iteration 42, loss = 0.07895481\n",
      "Iteration 10, loss = 0.22413844\n",
      "Iteration 43, loss = 0.08806770\n",
      "Iteration 11, loss = 0.21017272\n",
      "Iteration 44, loss = 0.07050506\n",
      "Iteration 12, loss = 0.19734011\n",
      "Iteration 45, loss = 0.07532372\n",
      "Iteration 13, loss = 0.20346370\n",
      "Iteration 46, loss = 0.07572629\n",
      "Iteration 14, loss = 0.18414895\n",
      "Iteration 47, loss = 0.06327727\n",
      "Iteration 15, loss = 0.18108324\n",
      "Iteration 48, loss = 0.06299838\n",
      "Iteration 16, loss = 0.19256971\n",
      "Iteration 49, loss = 0.07992982\n",
      "Iteration 17, loss = 0.19168253\n",
      "Iteration 50, loss = 0.08003639\n",
      "Iteration 18, loss = 0.18536466\n",
      "Iteration 51, loss = 0.08889355\n",
      "Iteration 19, loss = 0.16130505\n",
      "Iteration 52, loss = 0.10622866\n",
      "Iteration 20, loss = 0.18226033\n",
      "Iteration 53, loss = 0.08591863\n",
      "Iteration 21, loss = 0.16005424\n",
      "Iteration 22, loss = 0.15003790\n",
      "Iteration 54, loss = 0.10591645\n",
      "Iteration 23, loss = 0.13519585\n",
      "Iteration 55, loss = 0.10338179\n",
      "Iteration 24, loss = 0.12391591\n",
      "Iteration 56, loss = 0.07479656\n",
      "Iteration 25, loss = 0.11353162\n",
      "Iteration 57, loss = 0.10415584\n",
      "Iteration 26, loss = 0.10963966\n",
      "Iteration 58, loss = 0.07912471\n",
      "Iteration 27, loss = 0.10500352\n",
      "Iteration 59, loss = 0.07825662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.12657196\n",
      "Iteration 29, loss = 0.10362250\n",
      "Iteration 30, loss = 0.13354600\n",
      "Iteration 31, loss = 0.13439502\n",
      "Iteration 32, loss = 0.17002383\n",
      "Iteration 33, loss = 0.14178538\n",
      "Iteration 34, loss = 0.12314257\n",
      "Iteration 35, loss = 0.14087713\n",
      "Iteration 36, loss = 0.11852748\n",
      "Iteration 37, loss = 0.14807164\n",
      "Iteration 38, loss = 0.13745156\n",
      "Iteration 39, loss = 0.10374623\n",
      "Iteration 40, loss = 0.08551202\n",
      "Iteration 1, loss = 0.98119047\n",
      "Iteration 41, loss = 0.07685786\n",
      "Iteration 2, loss = 0.48889720\n",
      "Iteration 42, loss = 0.08396080\n",
      "Iteration 3, loss = 0.39331807\n",
      "Iteration 43, loss = 0.08183633\n",
      "Iteration 4, loss = 0.35572246\n",
      "Iteration 44, loss = 0.08307496\n",
      "Iteration 5, loss = 0.32794886\n",
      "Iteration 45, loss = 0.09635311\n",
      "Iteration 6, loss = 0.30642458\n",
      "Iteration 46, loss = 0.10010305\n",
      "Iteration 7, loss = 0.28644109\n",
      "Iteration 47, loss = 0.09487748\n",
      "Iteration 8, loss = 0.27912733\n",
      "Iteration 48, loss = 0.09397574\n",
      "Iteration 9, loss = 0.25811252\n",
      "Iteration 49, loss = 0.08846937\n",
      "Iteration 10, loss = 0.25669894\n",
      "Iteration 50, loss = 0.06477866\n",
      "Iteration 11, loss = 0.24535672\n",
      "Iteration 51, loss = 0.06675316\n",
      "Iteration 12, loss = 0.22785424\n",
      "Iteration 52, loss = 0.06951341\n",
      "Iteration 13, loss = 0.22129477\n",
      "Iteration 53, loss = 0.06755658\n",
      "Iteration 14, loss = 0.21301650\n",
      "Iteration 54, loss = 0.07499775\n",
      "Iteration 15, loss = 0.21378158\n",
      "Iteration 55, loss = 0.07448329\n",
      "Iteration 16, loss = 0.20587321\n",
      "Iteration 56, loss = 0.07094618\n",
      "Iteration 17, loss = 0.18373611\n",
      "Iteration 57, loss = 0.07180270\n",
      "Iteration 18, loss = 0.17507699\n",
      "Iteration 58, loss = 0.06849698\n",
      "Iteration 19, loss = 0.16933680\n",
      "Iteration 59, loss = 0.06866267\n",
      "Iteration 20, loss = 0.18077707\n",
      "Iteration 60, loss = 0.07941983\n",
      "Iteration 21, loss = 0.17594488\n",
      "Iteration 22, loss = 0.16866578\n",
      "Iteration 61, loss = 0.09087533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.16615587\n",
      "Iteration 24, loss = 0.15034829\n",
      "Iteration 25, loss = 0.14767179\n",
      "Iteration 26, loss = 0.15254362\n",
      "Iteration 27, loss = 0.14412295\n",
      "Iteration 28, loss = 0.12779158\n",
      "Iteration 29, loss = 0.12609994\n",
      "Iteration 30, loss = 0.12119296\n",
      "Iteration 31, loss = 0.14303598\n",
      "Iteration 32, loss = 0.13931549\n",
      "Iteration 33, loss = 0.14345811\n",
      "Iteration 34, loss = 0.12534657\n",
      "Iteration 35, loss = 0.12417618\n",
      "Iteration 1, loss = 0.84477660\n",
      "Iteration 36, loss = 0.15206391\n",
      "Iteration 2, loss = 0.45183692\n",
      "Iteration 37, loss = 0.13747723\n",
      "Iteration 3, loss = 0.38524658\n",
      "Iteration 4, loss = 0.35863796\n",
      "Iteration 38, loss = 0.12028953\n",
      "Iteration 5, loss = 0.32969784\n",
      "Iteration 39, loss = 0.10127240\n",
      "Iteration 6, loss = 0.29938279\n",
      "Iteration 40, loss = 0.11036946\n",
      "Iteration 7, loss = 0.30056275\n",
      "Iteration 41, loss = 0.10594929\n",
      "Iteration 8, loss = 0.27992949\n",
      "Iteration 42, loss = 0.10355322\n",
      "Iteration 9, loss = 0.24924974\n",
      "Iteration 43, loss = 0.09453760\n",
      "Iteration 10, loss = 0.24008081\n",
      "Iteration 44, loss = 0.09979397\n",
      "Iteration 11, loss = 0.21737037\n",
      "Iteration 45, loss = 0.09372385\n",
      "Iteration 12, loss = 0.20574442\n",
      "Iteration 46, loss = 0.09945930\n",
      "Iteration 13, loss = 0.19846530\n",
      "Iteration 47, loss = 0.09834924\n",
      "Iteration 14, loss = 0.20438408\n",
      "Iteration 48, loss = 0.08433014\n",
      "Iteration 15, loss = 0.20611185\n",
      "Iteration 49, loss = 0.09793367\n",
      "Iteration 16, loss = 0.17875915\n",
      "Iteration 50, loss = 0.10063814\n",
      "Iteration 17, loss = 0.17691144\n",
      "Iteration 51, loss = 0.09140686\n",
      "Iteration 18, loss = 0.16669691\n",
      "Iteration 52, loss = 0.12128403\n",
      "Iteration 19, loss = 0.17210319\n",
      "Iteration 53, loss = 0.09058027\n",
      "Iteration 20, loss = 0.16686427\n",
      "Iteration 54, loss = 0.10789341\n",
      "Iteration 21, loss = 0.17407894\n",
      "Iteration 55, loss = 0.15443834\n",
      "Iteration 22, loss = 0.16524980\n",
      "Iteration 56, loss = 0.15563353\n",
      "Iteration 23, loss = 0.15668531\n",
      "Iteration 57, loss = 0.22461221\n",
      "Iteration 24, loss = 0.14085011\n",
      "Iteration 58, loss = 0.27141009\n",
      "Iteration 25, loss = 0.13734085\n",
      "Iteration 59, loss = 0.25401221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.12332957\n",
      "Iteration 27, loss = 0.11873153\n",
      "Iteration 28, loss = 0.12449375\n",
      "Iteration 29, loss = 0.11653674\n",
      "Iteration 30, loss = 0.13305289\n",
      "Iteration 31, loss = 0.12785178\n",
      "Iteration 32, loss = 0.12135396\n",
      "Iteration 33, loss = 0.11701716\n",
      "Iteration 34, loss = 0.09651405\n",
      "Iteration 35, loss = 0.14009515\n",
      "Iteration 36, loss = 0.11703509\n",
      "Iteration 37, loss = 0.10662597\n",
      "Iteration 38, loss = 0.14715173\n",
      "Iteration 1, loss = 1.04776505\n",
      "Iteration 39, loss = 0.14110155\n",
      "Iteration 2, loss = 0.47961424\n",
      "Iteration 40, loss = 0.14160939\n",
      "Iteration 3, loss = 0.39071518\n",
      "Iteration 41, loss = 0.10165081\n",
      "Iteration 4, loss = 0.35098053\n",
      "Iteration 42, loss = 0.09524198\n",
      "Iteration 5, loss = 0.31991504\n",
      "Iteration 43, loss = 0.10918899\n",
      "Iteration 6, loss = 0.30070477\n",
      "Iteration 44, loss = 0.09778438\n",
      "Iteration 7, loss = 0.27506934\n",
      "Iteration 45, loss = 0.10869153\n",
      "Iteration 8, loss = 0.26123630\n",
      "Iteration 46, loss = 0.14834204\n",
      "Iteration 9, loss = 0.26313635\n",
      "Iteration 47, loss = 0.10826733\n",
      "Iteration 10, loss = 0.23550738\n",
      "Iteration 48, loss = 0.08405357\n",
      "Iteration 11, loss = 0.22390622\n",
      "Iteration 49, loss = 0.08910494\n",
      "Iteration 12, loss = 0.21987401\n",
      "Iteration 50, loss = 0.08544609\n",
      "Iteration 13, loss = 0.22893677\n",
      "Iteration 51, loss = 0.07482829\n",
      "Iteration 14, loss = 0.22472566\n",
      "Iteration 52, loss = 0.08380987\n",
      "Iteration 15, loss = 0.23714655\n",
      "Iteration 53, loss = 0.07730314\n",
      "Iteration 16, loss = 0.21266848\n",
      "Iteration 54, loss = 0.09738236\n",
      "Iteration 17, loss = 0.19076406\n",
      "Iteration 55, loss = 0.08531937\n",
      "Iteration 18, loss = 0.18171627\n",
      "Iteration 56, loss = 0.07848672\n",
      "Iteration 19, loss = 0.15677957\n",
      "Iteration 57, loss = 0.07572355\n",
      "Iteration 20, loss = 0.16102704\n",
      "Iteration 58, loss = 0.08341680\n",
      "Iteration 21, loss = 0.16029151\n",
      "Iteration 59, loss = 0.06125829\n",
      "Iteration 22, loss = 0.16500702\n",
      "Iteration 60, loss = 0.04985360\n",
      "Iteration 23, loss = 0.15107121\n",
      "Iteration 61, loss = 0.04546382\n",
      "Iteration 24, loss = 0.15200118\n",
      "Iteration 62, loss = 0.04318141\n",
      "Iteration 25, loss = 0.13127793\n",
      "Iteration 63, loss = 0.04593819\n",
      "Iteration 26, loss = 0.12924408\n",
      "Iteration 64, loss = 0.07210490\n",
      "Iteration 27, loss = 0.11961976\n",
      "Iteration 65, loss = 0.06968353\n",
      "Iteration 28, loss = 0.12215002\n",
      "Iteration 66, loss = 0.05822728\n",
      "Iteration 29, loss = 0.12593590\n",
      "Iteration 67, loss = 0.07214518\n",
      "Iteration 30, loss = 0.11997358\n",
      "Iteration 68, loss = 0.07588271\n",
      "Iteration 31, loss = 0.12296570\n",
      "Iteration 69, loss = 0.06588789\n",
      "Iteration 32, loss = 0.10766837\n",
      "Iteration 70, loss = 0.07284872\n",
      "Iteration 33, loss = 0.12347649\n",
      "Iteration 71, loss = 0.09000519\n",
      "Iteration 34, loss = 0.10198721\n",
      "Iteration 72, loss = 0.09442445\n",
      "Iteration 35, loss = 0.12282884\n",
      "Iteration 73, loss = 0.08252273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.15545568\n",
      "Iteration 37, loss = 0.13976235\n",
      "Iteration 38, loss = 0.10694313\n",
      "Iteration 39, loss = 0.08694338\n",
      "Iteration 40, loss = 0.10427212\n",
      "Iteration 41, loss = 0.09778688\n",
      "Iteration 42, loss = 0.10174472\n",
      "Iteration 43, loss = 0.09410297\n",
      "Iteration 44, loss = 0.14000392\n",
      "Iteration 45, loss = 0.12614166\n",
      "Iteration 46, loss = 0.11382787\n",
      "Iteration 47, loss = 0.12993830\n",
      "Iteration 48, loss = 0.09904379\n",
      "Iteration 1, loss = 0.92090136\n",
      "Iteration 49, loss = 0.11696465\n",
      "Iteration 2, loss = 0.55394632\n",
      "Iteration 50, loss = 0.12326363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.41930011\n",
      "Iteration 4, loss = 0.39022025\n",
      "Iteration 5, loss = 0.34392543\n",
      "Iteration 6, loss = 0.32475863\n",
      "Iteration 7, loss = 0.30634714\n",
      "Iteration 8, loss = 0.27829588\n",
      "Iteration 9, loss = 0.26278626\n",
      "Iteration 10, loss = 0.24129503\n",
      "Iteration 11, loss = 0.22586955\n",
      "Iteration 12, loss = 0.21418211\n",
      "Iteration 13, loss = 0.21141631\n",
      "Iteration 14, loss = 0.20414574\n",
      "Iteration 15, loss = 0.20776871\n",
      "Iteration 1, loss = 0.71082132\n",
      "Iteration 16, loss = 0.19712256\n",
      "Iteration 2, loss = 0.44117548\n",
      "Iteration 17, loss = 0.18496616\n",
      "Iteration 3, loss = 0.36723022\n",
      "Iteration 18, loss = 0.19333621\n",
      "Iteration 4, loss = 0.31757990\n",
      "Iteration 19, loss = 0.17713367\n",
      "Iteration 5, loss = 0.28020942\n",
      "Iteration 20, loss = 0.16940879\n",
      "Iteration 6, loss = 0.27570371\n",
      "Iteration 21, loss = 0.17140632\n",
      "Iteration 7, loss = 0.25777411\n",
      "Iteration 22, loss = 0.15596396\n",
      "Iteration 8, loss = 0.22936027\n",
      "Iteration 23, loss = 0.15782664\n",
      "Iteration 9, loss = 0.22343663\n",
      "Iteration 24, loss = 0.14979498\n",
      "Iteration 10, loss = 0.19725604\n",
      "Iteration 25, loss = 0.18455712\n",
      "Iteration 11, loss = 0.18237335\n",
      "Iteration 26, loss = 0.18772934\n",
      "Iteration 12, loss = 0.20118517\n",
      "Iteration 27, loss = 0.16662100\n",
      "Iteration 13, loss = 0.17206183\n",
      "Iteration 28, loss = 0.16225679\n",
      "Iteration 14, loss = 0.17638749\n",
      "Iteration 29, loss = 0.15921427\n",
      "Iteration 15, loss = 0.15303807\n",
      "Iteration 30, loss = 0.14794786\n",
      "Iteration 16, loss = 0.16230745\n",
      "Iteration 31, loss = 0.15696547\n",
      "Iteration 17, loss = 0.13573569\n",
      "Iteration 32, loss = 0.14104785\n",
      "Iteration 18, loss = 0.12771532\n",
      "Iteration 33, loss = 0.14998132\n",
      "Iteration 19, loss = 0.11170133\n",
      "Iteration 34, loss = 0.16646739\n",
      "Iteration 20, loss = 0.10989491\n",
      "Iteration 35, loss = 0.19262009\n",
      "Iteration 21, loss = 0.10098111\n",
      "Iteration 36, loss = 0.18398903\n",
      "Iteration 22, loss = 0.10125164\n",
      "Iteration 37, loss = 0.14569030\n",
      "Iteration 23, loss = 0.10875321\n",
      "Iteration 38, loss = 0.11594016\n",
      "Iteration 24, loss = 0.11148119\n",
      "Iteration 39, loss = 0.12901538\n",
      "Iteration 25, loss = 0.15447683\n",
      "Iteration 40, loss = 0.10404547\n",
      "Iteration 26, loss = 0.12449211\n",
      "Iteration 41, loss = 0.11101103\n",
      "Iteration 27, loss = 0.13038946\n",
      "Iteration 42, loss = 0.11199994\n",
      "Iteration 28, loss = 0.09915791\n",
      "Iteration 43, loss = 0.13475015\n",
      "Iteration 29, loss = 0.09483088\n",
      "Iteration 44, loss = 0.13716554\n",
      "Iteration 30, loss = 0.10737017\n",
      "Iteration 45, loss = 0.10735689\n",
      "Iteration 31, loss = 0.11337439\n",
      "Iteration 46, loss = 0.08873371\n",
      "Iteration 32, loss = 0.11550807\n",
      "Iteration 47, loss = 0.09205281\n",
      "Iteration 33, loss = 0.10201968\n",
      "Iteration 48, loss = 0.09144284\n",
      "Iteration 34, loss = 0.07464049\n",
      "Iteration 49, loss = 0.08837124\n",
      "Iteration 35, loss = 0.08218582\n",
      "Iteration 50, loss = 0.07967065\n",
      "Iteration 36, loss = 0.11850007\n",
      "Iteration 51, loss = 0.07739324\n",
      "Iteration 37, loss = 0.10213478\n",
      "Iteration 52, loss = 0.08715562\n",
      "Iteration 38, loss = 0.11512890\n",
      "Iteration 53, loss = 0.09552153\n",
      "Iteration 39, loss = 0.08410802\n",
      "Iteration 54, loss = 0.12502565\n",
      "Iteration 40, loss = 0.06692763\n",
      "Iteration 55, loss = 0.09908067\n",
      "Iteration 41, loss = 0.06671268\n",
      "Iteration 56, loss = 0.10315529\n",
      "Iteration 42, loss = 0.05294324\n",
      "Iteration 57, loss = 0.10861614\n",
      "Iteration 43, loss = 0.06079945\n",
      "Iteration 58, loss = 0.15747090\n",
      "Iteration 44, loss = 0.04949740\n",
      "Iteration 59, loss = 0.13511495\n",
      "Iteration 45, loss = 0.04720876\n",
      "Iteration 60, loss = 0.17912133\n",
      "Iteration 46, loss = 0.04533096\n",
      "Iteration 61, loss = 0.15000148\n",
      "Iteration 47, loss = 0.05069817\n",
      "Iteration 62, loss = 0.10385644\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.03546960\n",
      "Iteration 49, loss = 0.04009273\n",
      "Iteration 50, loss = 0.03885388\n",
      "Iteration 51, loss = 0.03760076\n",
      "Iteration 52, loss = 0.03026397\n",
      "Iteration 53, loss = 0.03778642\n",
      "Iteration 54, loss = 0.03860461\n",
      "Iteration 55, loss = 0.04128918\n",
      "Iteration 56, loss = 0.03524600\n",
      "Iteration 57, loss = 0.03409907\n",
      "Iteration 58, loss = 0.04205565\n",
      "Iteration 59, loss = 0.04098347\n",
      "Iteration 60, loss = 0.05212598\n",
      "Iteration 1, loss = 0.93434294\n",
      "Iteration 61, loss = 0.06980529\n",
      "Iteration 2, loss = 0.44781775\n",
      "Iteration 62, loss = 0.12964925\n",
      "Iteration 3, loss = 0.37553187\n",
      "Iteration 63, loss = 0.16841842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.31663410\n",
      "Iteration 5, loss = 0.28069679\n",
      "Iteration 6, loss = 0.26397736\n",
      "Iteration 7, loss = 0.25378351\n",
      "Iteration 8, loss = 0.23198835\n",
      "Iteration 9, loss = 0.22238184\n",
      "Iteration 10, loss = 0.22768282\n",
      "Iteration 11, loss = 0.20497277\n",
      "Iteration 12, loss = 0.21936525\n",
      "Iteration 13, loss = 0.19138927\n",
      "Iteration 14, loss = 0.17504941\n",
      "Iteration 15, loss = 0.17596982\n",
      "Iteration 16, loss = 0.16730087\n",
      "Iteration 1, loss = 0.88726115\n",
      "Iteration 17, loss = 0.14954459\n",
      "Iteration 2, loss = 0.48462606\n",
      "Iteration 18, loss = 0.14743862\n",
      "Iteration 3, loss = 0.40232791\n",
      "Iteration 19, loss = 0.14333003\n",
      "Iteration 4, loss = 0.35869936\n",
      "Iteration 20, loss = 0.13052912\n",
      "Iteration 5, loss = 0.33007237\n",
      "Iteration 21, loss = 0.12870491\n",
      "Iteration 22, loss = 0.12841970\n",
      "Iteration 6, loss = 0.30333115\n",
      "Iteration 23, loss = 0.12136433\n",
      "Iteration 7, loss = 0.27487182\n",
      "Iteration 24, loss = 0.10897850\n",
      "Iteration 8, loss = 0.25438430\n",
      "Iteration 25, loss = 0.11294040\n",
      "Iteration 9, loss = 0.24409372\n",
      "Iteration 26, loss = 0.12885688\n",
      "Iteration 10, loss = 0.23064791\n",
      "Iteration 27, loss = 0.10716406\n",
      "Iteration 11, loss = 0.22570061\n",
      "Iteration 28, loss = 0.09991690\n",
      "Iteration 12, loss = 0.21791501\n",
      "Iteration 29, loss = 0.13529247\n",
      "Iteration 13, loss = 0.21433013\n",
      "Iteration 30, loss = 0.11265961\n",
      "Iteration 14, loss = 0.19554619\n",
      "Iteration 15, loss = 0.19626860\n",
      "Iteration 31, loss = 0.12302361\n",
      "Iteration 32, loss = 0.11613910\n",
      "Iteration 16, loss = 0.16820368\n",
      "Iteration 33, loss = 0.13402675\n",
      "Iteration 17, loss = 0.17864934\n",
      "Iteration 34, loss = 0.10032992\n",
      "Iteration 18, loss = 0.16667142\n",
      "Iteration 35, loss = 0.08960359\n",
      "Iteration 19, loss = 0.16884245\n",
      "Iteration 36, loss = 0.11545143\n",
      "Iteration 20, loss = 0.17607995\n",
      "Iteration 37, loss = 0.07904826\n",
      "Iteration 21, loss = 0.15845553\n",
      "Iteration 38, loss = 0.08119636\n",
      "Iteration 22, loss = 0.16345508\n",
      "Iteration 39, loss = 0.07351864\n",
      "Iteration 23, loss = 0.15364108\n",
      "Iteration 40, loss = 0.08142310\n",
      "Iteration 24, loss = 0.13726117\n",
      "Iteration 41, loss = 0.08550249\n",
      "Iteration 25, loss = 0.12699637\n",
      "Iteration 42, loss = 0.10199692\n",
      "Iteration 26, loss = 0.12603088\n",
      "Iteration 43, loss = 0.09153735\n",
      "Iteration 27, loss = 0.12608448\n",
      "Iteration 44, loss = 0.09057288\n",
      "Iteration 28, loss = 0.12666521\n",
      "Iteration 45, loss = 0.09293654\n",
      "Iteration 29, loss = 0.13692188\n",
      "Iteration 46, loss = 0.07365886\n",
      "Iteration 30, loss = 0.13172229\n",
      "Iteration 47, loss = 0.07776495\n",
      "Iteration 31, loss = 0.10765815\n",
      "Iteration 48, loss = 0.06328565\n",
      "Iteration 32, loss = 0.10540245\n",
      "Iteration 49, loss = 0.07364886\n",
      "Iteration 33, loss = 0.09717916\n",
      "Iteration 50, loss = 0.09632337\n",
      "Iteration 34, loss = 0.08845610\n",
      "Iteration 51, loss = 0.10768479\n",
      "Iteration 35, loss = 0.11342042\n",
      "Iteration 52, loss = 0.08909994\n",
      "Iteration 36, loss = 0.08807002\n",
      "Iteration 53, loss = 0.09770172\n",
      "Iteration 37, loss = 0.08868200\n",
      "Iteration 54, loss = 0.10163672\n",
      "Iteration 38, loss = 0.08969617\n",
      "Iteration 55, loss = 0.09970205\n",
      "Iteration 39, loss = 0.09034810\n",
      "Iteration 56, loss = 0.09038688\n",
      "Iteration 40, loss = 0.11075082\n",
      "Iteration 57, loss = 0.09170178\n",
      "Iteration 41, loss = 0.10020809\n",
      "Iteration 58, loss = 0.07906287\n",
      "Iteration 42, loss = 0.12162332\n",
      "Iteration 59, loss = 0.08019241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.11643082\n",
      "Iteration 44, loss = 0.11279057\n",
      "Iteration 45, loss = 0.10582973\n",
      "Iteration 46, loss = 0.09419079\n",
      "Iteration 47, loss = 0.08318156\n",
      "Iteration 48, loss = 0.10560276\n",
      "Iteration 49, loss = 0.09080312\n",
      "Iteration 50, loss = 0.07903718\n",
      "Iteration 51, loss = 0.10380468\n",
      "Iteration 52, loss = 0.07477715\n",
      "Iteration 53, loss = 0.07799191\n",
      "Iteration 54, loss = 0.09332950\n",
      "Iteration 55, loss = 0.08685854\n",
      "Iteration 56, loss = 0.05356476\n",
      "Iteration 1, loss = 1.08665654\n",
      "Iteration 57, loss = 0.04923293\n",
      "Iteration 2, loss = 0.83789625\n",
      "Iteration 3, loss = 0.69332769\n",
      "Iteration 58, loss = 0.04932353\n",
      "Iteration 59, loss = 0.05219133\n",
      "Iteration 4, loss = 0.60255112\n",
      "Iteration 60, loss = 0.06202003\n",
      "Iteration 5, loss = 0.54287850\n",
      "Iteration 6, loss = 0.50229420\n",
      "Iteration 61, loss = 0.07379087\n",
      "Iteration 7, loss = 0.47057882\n",
      "Iteration 62, loss = 0.11297826\n",
      "Iteration 8, loss = 0.44459681\n",
      "Iteration 63, loss = 0.05480889\n",
      "Iteration 9, loss = 0.42445720\n",
      "Iteration 64, loss = 0.10195509\n",
      "Iteration 10, loss = 0.40485260\n",
      "Iteration 65, loss = 0.18317144\n",
      "Iteration 11, loss = 0.39135600\n",
      "Iteration 66, loss = 0.14697615\n",
      "Iteration 12, loss = 0.37735470\n",
      "Iteration 67, loss = 0.13144378\n",
      "Iteration 13, loss = 0.36495376\n",
      "Iteration 68, loss = 0.13856957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.35362853\n",
      "Iteration 15, loss = 0.34309815\n",
      "Iteration 16, loss = 0.33481266\n",
      "Iteration 17, loss = 0.32669684\n",
      "Iteration 18, loss = 0.31804227\n",
      "Iteration 19, loss = 0.31071808\n",
      "Iteration 20, loss = 0.30422726\n",
      "Iteration 21, loss = 0.29887904\n",
      "Iteration 22, loss = 0.29152718\n",
      "Iteration 23, loss = 0.28570975\n",
      "Iteration 24, loss = 0.27918882\n",
      "Iteration 25, loss = 0.27439713\n",
      "Iteration 1, loss = 1.14754194\n",
      "Iteration 26, loss = 0.26957794\n",
      "Iteration 2, loss = 0.88879486\n",
      "Iteration 27, loss = 0.26430229\n",
      "Iteration 3, loss = 0.72970263\n",
      "Iteration 28, loss = 0.25930914\n",
      "Iteration 4, loss = 0.62846040\n",
      "Iteration 29, loss = 0.25510689\n",
      "Iteration 30, loss = 0.25042024\n",
      "Iteration 5, loss = 0.56151566\n",
      "Iteration 31, loss = 0.24500574\n",
      "Iteration 6, loss = 0.51446492\n",
      "Iteration 32, loss = 0.24318915\n",
      "Iteration 7, loss = 0.47816860\n",
      "Iteration 33, loss = 0.23842637\n",
      "Iteration 8, loss = 0.45095830\n",
      "Iteration 9, loss = 0.42861851\n",
      "Iteration 34, loss = 0.23350840\n",
      "Iteration 10, loss = 0.41019995\n",
      "Iteration 35, loss = 0.23537078\n",
      "Iteration 36, loss = 0.22983846\n",
      "Iteration 11, loss = 0.39601482\n",
      "Iteration 37, loss = 0.22219369\n",
      "Iteration 12, loss = 0.38259709\n",
      "Iteration 13, loss = 0.37051393\n",
      "Iteration 38, loss = 0.22105842\n",
      "Iteration 39, loss = 0.21538486\n",
      "Iteration 14, loss = 0.36034232\n",
      "Iteration 40, loss = 0.21582246\n",
      "Iteration 15, loss = 0.35084255\n",
      "Iteration 41, loss = 0.21021618\n",
      "Iteration 16, loss = 0.34125383\n",
      "Iteration 17, loss = 0.33436029\n",
      "Iteration 42, loss = 0.21227240\n",
      "Iteration 18, loss = 0.32482801\n",
      "Iteration 43, loss = 0.20345228\n",
      "Iteration 19, loss = 0.31979416\n",
      "Iteration 44, loss = 0.20172382\n",
      "Iteration 20, loss = 0.31302894\n",
      "Iteration 45, loss = 0.19704904\n",
      "Iteration 21, loss = 0.30643379\n",
      "Iteration 46, loss = 0.19513026\n",
      "Iteration 22, loss = 0.30374149\n",
      "Iteration 47, loss = 0.19188396\n",
      "Iteration 23, loss = 0.29558731\n",
      "Iteration 48, loss = 0.18911160\n",
      "Iteration 24, loss = 0.28942565\n",
      "Iteration 49, loss = 0.18577818\n",
      "Iteration 25, loss = 0.28359305\n",
      "Iteration 50, loss = 0.18532545\n",
      "Iteration 26, loss = 0.27916481\n",
      "Iteration 51, loss = 0.18342126\n",
      "Iteration 27, loss = 0.27345934\n",
      "Iteration 52, loss = 0.17979189\n",
      "Iteration 28, loss = 0.26943083\n",
      "Iteration 53, loss = 0.17932116\n",
      "Iteration 29, loss = 0.26455693\n",
      "Iteration 54, loss = 0.17322837\n",
      "Iteration 30, loss = 0.25964705\n",
      "Iteration 55, loss = 0.17354515\n",
      "Iteration 31, loss = 0.25559210\n",
      "Iteration 56, loss = 0.17243626\n",
      "Iteration 32, loss = 0.25147828\n",
      "Iteration 57, loss = 0.16649065\n",
      "Iteration 33, loss = 0.24893780\n",
      "Iteration 58, loss = 0.16520465\n",
      "Iteration 34, loss = 0.24419576\n",
      "Iteration 59, loss = 0.16162207\n",
      "Iteration 35, loss = 0.24105813\n",
      "Iteration 60, loss = 0.16024105\n",
      "Iteration 36, loss = 0.23715923\n",
      "Iteration 61, loss = 0.15703295\n",
      "Iteration 37, loss = 0.23663337\n",
      "Iteration 62, loss = 0.15612242\n",
      "Iteration 38, loss = 0.23050381\n",
      "Iteration 63, loss = 0.15527580\n",
      "Iteration 39, loss = 0.22799949\n",
      "Iteration 64, loss = 0.15626626\n",
      "Iteration 65, loss = 0.15234036\n",
      "Iteration 40, loss = 0.22399933\n",
      "Iteration 66, loss = 0.15127491\n",
      "Iteration 41, loss = 0.22055409\n",
      "Iteration 67, loss = 0.14840269\n",
      "Iteration 42, loss = 0.21808656\n",
      "Iteration 43, loss = 0.21448831\n",
      "Iteration 68, loss = 0.14669960\n",
      "Iteration 44, loss = 0.21228076\n",
      "Iteration 69, loss = 0.14235054\n",
      "Iteration 45, loss = 0.20875942\n",
      "Iteration 70, loss = 0.14341377\n",
      "Iteration 71, loss = 0.14251020\n",
      "Iteration 46, loss = 0.20640819\n",
      "Iteration 72, loss = 0.13918483\n",
      "Iteration 47, loss = 0.20561026\n",
      "Iteration 73, loss = 0.13613551\n",
      "Iteration 48, loss = 0.20263213\n",
      "Iteration 74, loss = 0.13720194\n",
      "Iteration 49, loss = 0.20224201\n",
      "Iteration 75, loss = 0.13570405\n",
      "Iteration 50, loss = 0.19651131\n",
      "Iteration 76, loss = 0.13202419\n",
      "Iteration 51, loss = 0.19446159\n",
      "Iteration 77, loss = 0.13167368\n",
      "Iteration 52, loss = 0.19135521\n",
      "Iteration 78, loss = 0.13039256\n",
      "Iteration 53, loss = 0.18777219\n",
      "Iteration 79, loss = 0.12851006\n",
      "Iteration 54, loss = 0.18500063\n",
      "Iteration 80, loss = 0.12631540\n",
      "Iteration 55, loss = 0.18399353\n",
      "Iteration 81, loss = 0.12563266\n",
      "Iteration 56, loss = 0.18356736\n",
      "Iteration 82, loss = 0.12399416\n",
      "Iteration 57, loss = 0.17847448\n",
      "Iteration 83, loss = 0.12519812\n",
      "Iteration 58, loss = 0.17607689\n",
      "Iteration 84, loss = 0.12287275\n",
      "Iteration 59, loss = 0.17451101\n",
      "Iteration 85, loss = 0.11940459\n",
      "Iteration 60, loss = 0.17077958\n",
      "Iteration 86, loss = 0.11871236\n",
      "Iteration 61, loss = 0.16856859\n",
      "Iteration 87, loss = 0.11948382\n",
      "Iteration 62, loss = 0.16887639\n",
      "Iteration 88, loss = 0.11838811\n",
      "Iteration 63, loss = 0.16476084\n",
      "Iteration 89, loss = 0.11873544\n",
      "Iteration 64, loss = 0.16262593\n",
      "Iteration 90, loss = 0.11405242\n",
      "Iteration 65, loss = 0.16465693\n",
      "Iteration 91, loss = 0.11364465\n",
      "Iteration 66, loss = 0.15693713\n",
      "Iteration 92, loss = 0.11138205\n",
      "Iteration 67, loss = 0.15884994\n",
      "Iteration 93, loss = 0.11099580\n",
      "Iteration 68, loss = 0.15652799\n",
      "Iteration 94, loss = 0.11076823\n",
      "Iteration 69, loss = 0.15324816\n",
      "Iteration 95, loss = 0.10807934\n",
      "Iteration 70, loss = 0.15143822\n",
      "Iteration 96, loss = 0.10530699\n",
      "Iteration 71, loss = 0.14913773\n",
      "Iteration 97, loss = 0.10689867\n",
      "Iteration 72, loss = 0.15048481\n",
      "Iteration 98, loss = 0.10457176\n",
      "Iteration 73, loss = 0.14619665\n",
      "Iteration 99, loss = 0.10232409\n",
      "Iteration 74, loss = 0.14595525\n",
      "Iteration 100, loss = 0.10222080\n",
      "Iteration 75, loss = 0.14395852\n",
      "Iteration 76, loss = 0.13957858\n",
      "Iteration 77, loss = 0.13982332\n",
      "Iteration 78, loss = 0.13645034\n",
      "Iteration 79, loss = 0.13526196\n",
      "Iteration 80, loss = 0.13341459\n",
      "Iteration 81, loss = 0.13363721\n",
      "Iteration 82, loss = 0.13053737\n",
      "Iteration 83, loss = 0.12954840\n",
      "Iteration 84, loss = 0.12906006\n",
      "Iteration 85, loss = 0.12614811\n",
      "Iteration 86, loss = 0.12478442\n",
      "Iteration 87, loss = 0.12228775\n",
      "Iteration 1, loss = 1.27209213\n",
      "Iteration 88, loss = 0.12362556\n",
      "Iteration 2, loss = 0.96968949\n",
      "Iteration 89, loss = 0.12108347\n",
      "Iteration 3, loss = 0.77770851\n",
      "Iteration 90, loss = 0.11943083\n",
      "Iteration 4, loss = 0.65942381\n",
      "Iteration 91, loss = 0.12092059\n",
      "Iteration 5, loss = 0.58845681\n",
      "Iteration 92, loss = 0.11969477\n",
      "Iteration 6, loss = 0.53786761\n",
      "Iteration 93, loss = 0.11681615\n",
      "Iteration 7, loss = 0.49917829\n",
      "Iteration 94, loss = 0.11653015\n",
      "Iteration 8, loss = 0.47121582\n",
      "Iteration 95, loss = 0.11343080\n",
      "Iteration 9, loss = 0.44302729\n",
      "Iteration 96, loss = 0.11242169\n",
      "Iteration 10, loss = 0.42288563\n",
      "Iteration 97, loss = 0.11075946\n",
      "Iteration 11, loss = 0.40615335\n",
      "Iteration 98, loss = 0.10859749\n",
      "Iteration 12, loss = 0.39074784\n",
      "Iteration 99, loss = 0.10813350\n",
      "Iteration 13, loss = 0.37572950\n",
      "Iteration 100, loss = 0.10969240\n",
      "Iteration 14, loss = 0.36368979\n",
      "Iteration 15, loss = 0.35442969\n",
      "Iteration 16, loss = 0.34243708\n",
      "Iteration 17, loss = 0.33595691\n",
      "Iteration 18, loss = 0.32638134\n",
      "Iteration 19, loss = 0.31974007\n",
      "Iteration 20, loss = 0.31337740\n",
      "Iteration 21, loss = 0.30540337\n",
      "Iteration 22, loss = 0.29825527\n",
      "Iteration 23, loss = 0.29470862\n",
      "Iteration 24, loss = 0.28689227\n",
      "Iteration 25, loss = 0.28147334\n",
      "Iteration 26, loss = 0.27741918\n",
      "Iteration 1, loss = 1.00105208\n",
      "Iteration 27, loss = 0.27047571\n",
      "Iteration 2, loss = 0.82600147\n",
      "Iteration 28, loss = 0.26864028\n",
      "Iteration 3, loss = 0.69992608\n",
      "Iteration 29, loss = 0.26177508\n",
      "Iteration 4, loss = 0.61373528\n",
      "Iteration 30, loss = 0.25692705\n",
      "Iteration 5, loss = 0.55232694\n",
      "Iteration 31, loss = 0.25213676\n",
      "Iteration 6, loss = 0.50542167\n",
      "Iteration 32, loss = 0.24960000\n",
      "Iteration 7, loss = 0.47075086\n",
      "Iteration 33, loss = 0.24615427\n",
      "Iteration 8, loss = 0.44334852\n",
      "Iteration 34, loss = 0.24245685\n",
      "Iteration 9, loss = 0.41955285\n",
      "Iteration 35, loss = 0.23990136\n",
      "Iteration 10, loss = 0.40297378\n",
      "Iteration 36, loss = 0.23980350\n",
      "Iteration 11, loss = 0.38719556\n",
      "Iteration 37, loss = 0.23525426\n",
      "Iteration 12, loss = 0.37350543\n",
      "Iteration 38, loss = 0.22683618\n",
      "Iteration 13, loss = 0.36028611\n",
      "Iteration 39, loss = 0.22678155\n",
      "Iteration 14, loss = 0.35030853\n",
      "Iteration 40, loss = 0.22109700\n",
      "Iteration 15, loss = 0.34154683\n",
      "Iteration 41, loss = 0.21852505\n",
      "Iteration 16, loss = 0.33514968\n",
      "Iteration 42, loss = 0.21430703\n",
      "Iteration 17, loss = 0.32526369\n",
      "Iteration 43, loss = 0.21212004\n",
      "Iteration 18, loss = 0.31885254\n",
      "Iteration 44, loss = 0.20772195\n",
      "Iteration 19, loss = 0.31193434\n",
      "Iteration 45, loss = 0.20592587\n",
      "Iteration 20, loss = 0.30557275\n",
      "Iteration 46, loss = 0.20170847\n",
      "Iteration 21, loss = 0.29958450\n",
      "Iteration 47, loss = 0.19954209\n",
      "Iteration 22, loss = 0.29381528\n",
      "Iteration 48, loss = 0.19646232\n",
      "Iteration 23, loss = 0.28907796\n",
      "Iteration 49, loss = 0.19496173\n",
      "Iteration 24, loss = 0.28429637\n",
      "Iteration 50, loss = 0.19324966\n",
      "Iteration 25, loss = 0.27955604\n",
      "Iteration 51, loss = 0.18880064\n",
      "Iteration 26, loss = 0.27430776\n",
      "Iteration 52, loss = 0.18735590\n",
      "Iteration 27, loss = 0.26938576\n",
      "Iteration 53, loss = 0.18522619\n",
      "Iteration 28, loss = 0.26588570\n",
      "Iteration 54, loss = 0.18126139\n",
      "Iteration 29, loss = 0.26170136\n",
      "Iteration 55, loss = 0.18160809\n",
      "Iteration 30, loss = 0.25625957\n",
      "Iteration 56, loss = 0.18035035\n",
      "Iteration 31, loss = 0.25378114\n",
      "Iteration 57, loss = 0.17444114\n",
      "Iteration 32, loss = 0.24971353\n",
      "Iteration 58, loss = 0.17589832\n",
      "Iteration 33, loss = 0.24551131\n",
      "Iteration 59, loss = 0.17044560\n",
      "Iteration 34, loss = 0.24245729\n",
      "Iteration 60, loss = 0.16642103\n",
      "Iteration 35, loss = 0.23864988\n",
      "Iteration 61, loss = 0.16490763\n",
      "Iteration 36, loss = 0.23616526\n",
      "Iteration 62, loss = 0.16745204\n",
      "Iteration 37, loss = 0.23124617\n",
      "Iteration 63, loss = 0.16643817\n",
      "Iteration 38, loss = 0.23138142\n",
      "Iteration 64, loss = 0.16704655\n",
      "Iteration 39, loss = 0.22615760\n",
      "Iteration 65, loss = 0.16253380\n",
      "Iteration 40, loss = 0.22561099\n",
      "Iteration 66, loss = 0.15763365\n",
      "Iteration 41, loss = 0.22206861\n",
      "Iteration 67, loss = 0.15376641\n",
      "Iteration 42, loss = 0.22010915\n",
      "Iteration 68, loss = 0.15501323\n",
      "Iteration 43, loss = 0.21388125\n",
      "Iteration 69, loss = 0.15302075\n",
      "Iteration 44, loss = 0.21148126\n",
      "Iteration 70, loss = 0.14620833\n",
      "Iteration 45, loss = 0.21038484\n",
      "Iteration 71, loss = 0.14678029\n",
      "Iteration 46, loss = 0.20963885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.14652263\n",
      "Iteration 47, loss = 0.20248725\n",
      "Iteration 73, loss = 0.14238919\n",
      "Iteration 48, loss = 0.20113892\n",
      "Iteration 74, loss = 0.13989568\n",
      "Iteration 49, loss = 0.19996577\n",
      "Iteration 75, loss = 0.14127845\n",
      "Iteration 50, loss = 0.19521309\n",
      "Iteration 76, loss = 0.14040254\n",
      "Iteration 51, loss = 0.19616157\n",
      "Iteration 77, loss = 0.13521840\n",
      "Iteration 52, loss = 0.19048946\n",
      "Iteration 78, loss = 0.13702346\n",
      "Iteration 53, loss = 0.18929721\n",
      "Iteration 79, loss = 0.13476373\n",
      "Iteration 54, loss = 0.18818009\n",
      "Iteration 80, loss = 0.13349276\n",
      "Iteration 55, loss = 0.18337904\n",
      "Iteration 81, loss = 0.12651028\n",
      "Iteration 56, loss = 0.18365106\n",
      "Iteration 82, loss = 0.13727951\n",
      "Iteration 57, loss = 0.17825573\n",
      "Iteration 83, loss = 0.12928347\n",
      "Iteration 58, loss = 0.17850787\n",
      "Iteration 84, loss = 0.12811950\n",
      "Iteration 59, loss = 0.17455806\n",
      "Iteration 85, loss = 0.12633375\n",
      "Iteration 60, loss = 0.17450095\n",
      "Iteration 86, loss = 0.12277028\n",
      "Iteration 61, loss = 0.17416057\n",
      "Iteration 87, loss = 0.12162888\n",
      "Iteration 62, loss = 0.16985460\n",
      "Iteration 88, loss = 0.12052100\n",
      "Iteration 63, loss = 0.16618928\n",
      "Iteration 89, loss = 0.11899655\n",
      "Iteration 64, loss = 0.16564190\n",
      "Iteration 90, loss = 0.11943274\n",
      "Iteration 65, loss = 0.16259562\n",
      "Iteration 91, loss = 0.11796165\n",
      "Iteration 66, loss = 0.16005568\n",
      "Iteration 92, loss = 0.11472566\n",
      "Iteration 67, loss = 0.15695081\n",
      "Iteration 93, loss = 0.11421585\n",
      "Iteration 68, loss = 0.15750607\n",
      "Iteration 94, loss = 0.11312485\n",
      "Iteration 69, loss = 0.15428287\n",
      "Iteration 95, loss = 0.11160896\n",
      "Iteration 70, loss = 0.15176941\n",
      "Iteration 96, loss = 0.11193539\n",
      "Iteration 71, loss = 0.15126367\n",
      "Iteration 97, loss = 0.10919857\n",
      "Iteration 72, loss = 0.15026690\n",
      "Iteration 98, loss = 0.10826282\n",
      "Iteration 73, loss = 0.14808600\n",
      "Iteration 99, loss = 0.10772498\n",
      "Iteration 74, loss = 0.15170938\n",
      "Iteration 100, loss = 0.10552791\n",
      "Iteration 75, loss = 0.14439104\n",
      "Iteration 76, loss = 0.14330162\n",
      "Iteration 77, loss = 0.14169694\n",
      "Iteration 78, loss = 0.14581471\n",
      "Iteration 79, loss = 0.13882836\n",
      "Iteration 80, loss = 0.14056477\n",
      "Iteration 81, loss = 0.14090036\n",
      "Iteration 82, loss = 0.13613546\n",
      "Iteration 83, loss = 0.13478988\n",
      "Iteration 84, loss = 0.13101733\n",
      "Iteration 85, loss = 0.12987410\n",
      "Iteration 86, loss = 0.13244076\n",
      "Iteration 87, loss = 0.12858228\n",
      "Iteration 1, loss = 1.38521067\n",
      "Iteration 88, loss = 0.12684349\n",
      "Iteration 2, loss = 0.92448701\n",
      "Iteration 89, loss = 0.12515662\n",
      "Iteration 3, loss = 0.70755565\n",
      "Iteration 90, loss = 0.12338340\n",
      "Iteration 4, loss = 0.59577091\n",
      "Iteration 91, loss = 0.12412616\n",
      "Iteration 5, loss = 0.53215893\n",
      "Iteration 92, loss = 0.12018714\n",
      "Iteration 6, loss = 0.48904917\n",
      "Iteration 93, loss = 0.12072774\n",
      "Iteration 7, loss = 0.45711105\n",
      "Iteration 94, loss = 0.12533016\n",
      "Iteration 8, loss = 0.43400540\n",
      "Iteration 95, loss = 0.12716618\n",
      "Iteration 9, loss = 0.41365597\n",
      "Iteration 96, loss = 0.13537759\n",
      "Iteration 10, loss = 0.40055464\n",
      "Iteration 97, loss = 0.12195179\n",
      "Iteration 11, loss = 0.38497666\n",
      "Iteration 98, loss = 0.12013425\n",
      "Iteration 12, loss = 0.37328824\n",
      "Iteration 99, loss = 0.12084989\n",
      "Iteration 13, loss = 0.36236933\n",
      "Iteration 100, loss = 0.11221337\n",
      "Iteration 14, loss = 0.35339454\n",
      "Iteration 15, loss = 0.34387323\n",
      "Iteration 16, loss = 0.33958800\n",
      "Iteration 17, loss = 0.32834406\n",
      "Iteration 18, loss = 0.32315456\n",
      "Iteration 19, loss = 0.31539927\n",
      "Iteration 20, loss = 0.30978976\n",
      "Iteration 21, loss = 0.30469215\n",
      "Iteration 22, loss = 0.29858007\n",
      "Iteration 23, loss = 0.29396067\n",
      "Iteration 24, loss = 0.28904235\n",
      "Iteration 25, loss = 0.28462734\n",
      "Iteration 26, loss = 0.27899597\n",
      "Iteration 27, loss = 0.27672622\n",
      "Iteration 1, loss = 1.31889616\n",
      "Iteration 28, loss = 0.27122048\n",
      "Iteration 2, loss = 0.96453718\n",
      "Iteration 29, loss = 0.26736348\n",
      "Iteration 3, loss = 0.76968992\n",
      "Iteration 30, loss = 0.26424201Iteration 4, loss = 0.65803194\n",
      "\n",
      "Iteration 31, loss = 0.26084405\n",
      "Iteration 5, loss = 0.58949626\n",
      "Iteration 6, loss = 0.54018905\n",
      "Iteration 32, loss = 0.25878421\n",
      "Iteration 7, loss = 0.50419039\n",
      "Iteration 33, loss = 0.25430711\n",
      "Iteration 8, loss = 0.47567227\n",
      "Iteration 34, loss = 0.25103975\n",
      "Iteration 9, loss = 0.45352936\n",
      "Iteration 35, loss = 0.24778599\n",
      "Iteration 10, loss = 0.43377076\n",
      "Iteration 36, loss = 0.24330852\n",
      "Iteration 11, loss = 0.41602070\n",
      "Iteration 37, loss = 0.24049535\n",
      "Iteration 12, loss = 0.40306431\n",
      "Iteration 38, loss = 0.23624808\n",
      "Iteration 39, loss = 0.23415101\n",
      "Iteration 13, loss = 0.38887266\n",
      "Iteration 40, loss = 0.23091828\n",
      "Iteration 14, loss = 0.37700670\n",
      "Iteration 41, loss = 0.22998222\n",
      "Iteration 15, loss = 0.36701540\n",
      "Iteration 42, loss = 0.22550467\n",
      "Iteration 16, loss = 0.35577953\n",
      "Iteration 43, loss = 0.22352354\n",
      "Iteration 17, loss = 0.34822922\n",
      "Iteration 44, loss = 0.21976859\n",
      "Iteration 18, loss = 0.33830750\n",
      "Iteration 45, loss = 0.21909563\n",
      "Iteration 19, loss = 0.33098173\n",
      "Iteration 46, loss = 0.21491015\n",
      "Iteration 20, loss = 0.32338298\n",
      "Iteration 47, loss = 0.21175954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.31633208\n",
      "Iteration 48, loss = 0.20823457\n",
      "Iteration 22, loss = 0.31156722\n",
      "Iteration 49, loss = 0.20885776\n",
      "Iteration 23, loss = 0.30408311\n",
      "Iteration 50, loss = 0.20450578\n",
      "Iteration 51, loss = 0.20274380\n",
      "Iteration 24, loss = 0.29739822\n",
      "Iteration 52, loss = 0.20339769\n",
      "Iteration 25, loss = 0.29162041\n",
      "Iteration 53, loss = 0.19709852\n",
      "Iteration 26, loss = 0.28698051\n",
      "Iteration 54, loss = 0.19486090\n",
      "Iteration 27, loss = 0.28120236\n",
      "Iteration 55, loss = 0.19435856\n",
      "Iteration 28, loss = 0.27614954\n",
      "Iteration 56, loss = 0.18845264\n",
      "Iteration 29, loss = 0.27348927\n",
      "Iteration 57, loss = 0.18637890\n",
      "Iteration 30, loss = 0.26694282\n",
      "Iteration 58, loss = 0.18584462\n",
      "Iteration 31, loss = 0.26336009\n",
      "Iteration 59, loss = 0.18685348\n",
      "Iteration 32, loss = 0.25915713\n",
      "Iteration 60, loss = 0.18052615\n",
      "Iteration 33, loss = 0.25571421\n",
      "Iteration 61, loss = 0.17821341\n",
      "Iteration 34, loss = 0.25006914\n",
      "Iteration 62, loss = 0.17686945\n",
      "Iteration 35, loss = 0.24649119\n",
      "Iteration 63, loss = 0.17313166\n",
      "Iteration 36, loss = 0.24272457\n",
      "Iteration 64, loss = 0.17183605\n",
      "Iteration 37, loss = 0.23951425\n",
      "Iteration 65, loss = 0.16864331\n",
      "Iteration 38, loss = 0.23882125\n",
      "Iteration 66, loss = 0.16739327\n",
      "Iteration 39, loss = 0.23418736\n",
      "Iteration 67, loss = 0.16411470\n",
      "Iteration 40, loss = 0.23208560\n",
      "Iteration 68, loss = 0.16246598\n",
      "Iteration 41, loss = 0.22633613\n",
      "Iteration 69, loss = 0.16112910\n",
      "Iteration 42, loss = 0.22709273\n",
      "Iteration 70, loss = 0.16067977\n",
      "Iteration 43, loss = 0.21954168\n",
      "Iteration 71, loss = 0.15671902\n",
      "Iteration 44, loss = 0.21791279\n",
      "Iteration 72, loss = 0.15575736\n",
      "Iteration 45, loss = 0.21303111\n",
      "Iteration 73, loss = 0.15351517\n",
      "Iteration 46, loss = 0.21165698\n",
      "Iteration 74, loss = 0.15222242\n",
      "Iteration 47, loss = 0.20719035\n",
      "Iteration 75, loss = 0.14958621\n",
      "Iteration 48, loss = 0.20569254\n",
      "Iteration 76, loss = 0.14871739\n",
      "Iteration 49, loss = 0.20349973\n",
      "Iteration 77, loss = 0.14654686\n",
      "Iteration 50, loss = 0.20119366\n",
      "Iteration 78, loss = 0.14763383\n",
      "Iteration 51, loss = 0.19862610\n",
      "Iteration 79, loss = 0.14455421\n",
      "Iteration 52, loss = 0.19603832\n",
      "Iteration 80, loss = 0.14435252\n",
      "Iteration 53, loss = 0.19133337\n",
      "Iteration 54, loss = 0.18931651\n",
      "Iteration 81, loss = 0.14615243\n",
      "Iteration 55, loss = 0.18764484\n",
      "Iteration 82, loss = 0.14657588\n",
      "Iteration 56, loss = 0.18464448\n",
      "Iteration 83, loss = 0.14166714\n",
      "Iteration 57, loss = 0.18337728\n",
      "Iteration 84, loss = 0.14031413\n",
      "Iteration 58, loss = 0.17912680\n",
      "Iteration 85, loss = 0.13571960\n",
      "Iteration 59, loss = 0.17685478\n",
      "Iteration 86, loss = 0.13698277\n",
      "Iteration 87, loss = 0.13247461\n",
      "Iteration 60, loss = 0.17486484\n",
      "Iteration 88, loss = 0.13395235\n",
      "Iteration 61, loss = 0.17303551\n",
      "Iteration 89, loss = 0.13284569\n",
      "Iteration 62, loss = 0.16942551\n",
      "Iteration 90, loss = 0.13289091\n",
      "Iteration 63, loss = 0.16782323\n",
      "Iteration 91, loss = 0.12900308\n",
      "Iteration 64, loss = 0.16638840\n",
      "Iteration 92, loss = 0.12803580\n",
      "Iteration 65, loss = 0.16349683\n",
      "Iteration 93, loss = 0.12588816\n",
      "Iteration 66, loss = 0.16091491\n",
      "Iteration 94, loss = 0.12542597\n",
      "Iteration 67, loss = 0.16161070\n",
      "Iteration 95, loss = 0.12120529\n",
      "Iteration 68, loss = 0.15567498\n",
      "Iteration 96, loss = 0.12291697\n",
      "Iteration 69, loss = 0.15711000\n",
      "Iteration 97, loss = 0.12038271\n",
      "Iteration 70, loss = 0.15295562\n",
      "Iteration 98, loss = 0.12062657\n",
      "Iteration 71, loss = 0.15068739\n",
      "Iteration 99, loss = 0.11939880\n",
      "Iteration 72, loss = 0.15090287\n",
      "Iteration 100, loss = 0.11931494\n",
      "Iteration 73, loss = 0.14776788\n",
      "Iteration 74, loss = 0.14528701\n",
      "Iteration 75, loss = 0.14438124\n",
      "Iteration 76, loss = 0.14161572\n",
      "Iteration 77, loss = 0.14097019\n",
      "Iteration 78, loss = 0.13907423\n",
      "Iteration 79, loss = 0.13670287\n",
      "Iteration 80, loss = 0.13654172\n",
      "Iteration 81, loss = 0.13583143\n",
      "Iteration 82, loss = 0.13268511\n",
      "Iteration 83, loss = 0.12923898\n",
      "Iteration 84, loss = 0.13280080\n",
      "Iteration 85, loss = 0.13601450\n",
      "Iteration 1, loss = 1.27520867\n",
      "Iteration 86, loss = 0.12813730\n",
      "Iteration 2, loss = 0.90548160\n",
      "Iteration 87, loss = 0.12797484\n",
      "Iteration 3, loss = 0.72218392\n",
      "Iteration 88, loss = 0.12953211\n",
      "Iteration 4, loss = 0.61298271\n",
      "Iteration 89, loss = 0.12190829\n",
      "Iteration 5, loss = 0.54782876\n",
      "Iteration 90, loss = 0.12165152\n",
      "Iteration 91, loss = 0.12288436\n",
      "Iteration 6, loss = 0.50189299\n",
      "Iteration 7, loss = 0.46650209\n",
      "Iteration 92, loss = 0.12292057\n",
      "Iteration 8, loss = 0.44004007\n",
      "Iteration 93, loss = 0.12343947\n",
      "Iteration 9, loss = 0.41805529\n",
      "Iteration 94, loss = 0.12822547\n",
      "Iteration 10, loss = 0.39946818\n",
      "Iteration 95, loss = 0.12257972\n",
      "Iteration 11, loss = 0.38596446\n",
      "Iteration 96, loss = 0.11446908\n",
      "Iteration 12, loss = 0.37254341\n",
      "Iteration 97, loss = 0.11538020\n",
      "Iteration 13, loss = 0.36021947\n",
      "Iteration 98, loss = 0.11407721\n",
      "Iteration 14, loss = 0.34967568\n",
      "Iteration 99, loss = 0.10996681\n",
      "Iteration 15, loss = 0.34098391\n",
      "Iteration 100, loss = 0.10882650\n",
      "Iteration 16, loss = 0.33169541\n",
      "Iteration 17, loss = 0.32302184\n",
      "Iteration 18, loss = 0.31749894\n",
      "Iteration 19, loss = 0.30965302\n",
      "Iteration 20, loss = 0.30401517\n",
      "Iteration 21, loss = 0.29643668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.29163847\n",
      "Iteration 23, loss = 0.28562176\n",
      "Iteration 24, loss = 0.28104473\n",
      "Iteration 25, loss = 0.27797967\n",
      "Iteration 26, loss = 0.27240839\n",
      "Iteration 27, loss = 0.26690187\n",
      "Iteration 28, loss = 0.26258618\n",
      "Iteration 1, loss = 1.24141554\n",
      "Iteration 29, loss = 0.25914553\n",
      "Iteration 2, loss = 0.86502452\n",
      "Iteration 30, loss = 0.25566954\n",
      "Iteration 3, loss = 0.69173785\n",
      "Iteration 31, loss = 0.25248845\n",
      "Iteration 32, loss = 0.24724952\n",
      "Iteration 4, loss = 0.59047670\n",
      "Iteration 5, loss = 0.53129137\n",
      "Iteration 33, loss = 0.24507648\n",
      "Iteration 6, loss = 0.48706933\n",
      "Iteration 34, loss = 0.24038353\n",
      "Iteration 7, loss = 0.45132860\n",
      "Iteration 35, loss = 0.23569925\n",
      "Iteration 8, loss = 0.42924769\n",
      "Iteration 36, loss = 0.23325184\n",
      "Iteration 9, loss = 0.40747186\n",
      "Iteration 37, loss = 0.22910473\n",
      "Iteration 10, loss = 0.38955385\n",
      "Iteration 38, loss = 0.22661897\n",
      "Iteration 11, loss = 0.37452404\n",
      "Iteration 39, loss = 0.22344322\n",
      "Iteration 12, loss = 0.36168992\n",
      "Iteration 40, loss = 0.22138260\n",
      "Iteration 13, loss = 0.35030586\n",
      "Iteration 41, loss = 0.21689439\n",
      "Iteration 14, loss = 0.34008246\n",
      "Iteration 42, loss = 0.21715903\n",
      "Iteration 15, loss = 0.33038611\n",
      "Iteration 43, loss = 0.21160828\n",
      "Iteration 16, loss = 0.32163846\n",
      "Iteration 44, loss = 0.21032179\n",
      "Iteration 45, loss = 0.20617113\n",
      "Iteration 17, loss = 0.31456714\n",
      "Iteration 18, loss = 0.30793404\n",
      "Iteration 46, loss = 0.20316324\n",
      "Iteration 19, loss = 0.30191252\n",
      "Iteration 47, loss = 0.20115788\n",
      "Iteration 48, loss = 0.19889612\n",
      "Iteration 20, loss = 0.29508814\n",
      "Iteration 49, loss = 0.19567627\n",
      "Iteration 21, loss = 0.28840418\n",
      "Iteration 50, loss = 0.19366995\n",
      "Iteration 22, loss = 0.28042920\n",
      "Iteration 51, loss = 0.19220562\n",
      "Iteration 23, loss = 0.27696035\n",
      "Iteration 52, loss = 0.18988589\n",
      "Iteration 24, loss = 0.27048207\n",
      "Iteration 53, loss = 0.18599650\n",
      "Iteration 25, loss = 0.26492469\n",
      "Iteration 54, loss = 0.18562705\n",
      "Iteration 26, loss = 0.26045704\n",
      "Iteration 55, loss = 0.18189662\n",
      "Iteration 27, loss = 0.25443339\n",
      "Iteration 56, loss = 0.18062655\n",
      "Iteration 28, loss = 0.25076847\n",
      "Iteration 57, loss = 0.17804942\n",
      "Iteration 29, loss = 0.24452893\n",
      "Iteration 58, loss = 0.17908696\n",
      "Iteration 30, loss = 0.24119972\n",
      "Iteration 59, loss = 0.17533040\n",
      "Iteration 31, loss = 0.23666929\n",
      "Iteration 60, loss = 0.17123652\n",
      "Iteration 32, loss = 0.23418432\n",
      "Iteration 61, loss = 0.17149247\n",
      "Iteration 33, loss = 0.22723966\n",
      "Iteration 62, loss = 0.17011960\n",
      "Iteration 34, loss = 0.22595282\n",
      "Iteration 63, loss = 0.16654271\n",
      "Iteration 35, loss = 0.22324500\n",
      "Iteration 64, loss = 0.16556838\n",
      "Iteration 36, loss = 0.21535135\n",
      "Iteration 65, loss = 0.16157682\n",
      "Iteration 37, loss = 0.21528734\n",
      "Iteration 66, loss = 0.15915864\n",
      "Iteration 38, loss = 0.21000184\n",
      "Iteration 67, loss = 0.15780055\n",
      "Iteration 39, loss = 0.20704916\n",
      "Iteration 68, loss = 0.16133613\n",
      "Iteration 40, loss = 0.20290625\n",
      "Iteration 69, loss = 0.15814549\n",
      "Iteration 41, loss = 0.19904007\n",
      "Iteration 70, loss = 0.15972326\n",
      "Iteration 42, loss = 0.19614643\n",
      "Iteration 71, loss = 0.15262312\n",
      "Iteration 43, loss = 0.19075268\n",
      "Iteration 72, loss = 0.15171394\n",
      "Iteration 44, loss = 0.18878247\n",
      "Iteration 73, loss = 0.14915221\n",
      "Iteration 45, loss = 0.18507483\n",
      "Iteration 74, loss = 0.14649978\n",
      "Iteration 46, loss = 0.18285843\n",
      "Iteration 75, loss = 0.14875531\n",
      "Iteration 47, loss = 0.17949884\n",
      "Iteration 76, loss = 0.14370088\n",
      "Iteration 48, loss = 0.17622220\n",
      "Iteration 77, loss = 0.14183434\n",
      "Iteration 49, loss = 0.17303705\n",
      "Iteration 78, loss = 0.13980166\n",
      "Iteration 50, loss = 0.17125631\n",
      "Iteration 79, loss = 0.13980391\n",
      "Iteration 51, loss = 0.16844833\n",
      "Iteration 80, loss = 0.13736832\n",
      "Iteration 52, loss = 0.16566928\n",
      "Iteration 81, loss = 0.13632246\n",
      "Iteration 53, loss = 0.16337247\n",
      "Iteration 82, loss = 0.13449883\n",
      "Iteration 54, loss = 0.16139765\n",
      "Iteration 83, loss = 0.13521614\n",
      "Iteration 55, loss = 0.15554940\n",
      "Iteration 84, loss = 0.12955598\n",
      "Iteration 56, loss = 0.15381873\n",
      "Iteration 85, loss = 0.13207325\n",
      "Iteration 57, loss = 0.15036319\n",
      "Iteration 86, loss = 0.12876976\n",
      "Iteration 58, loss = 0.15177643\n",
      "Iteration 87, loss = 0.12713700\n",
      "Iteration 59, loss = 0.14935678\n",
      "Iteration 88, loss = 0.12602146\n",
      "Iteration 60, loss = 0.14310938\n",
      "Iteration 89, loss = 0.12434537\n",
      "Iteration 61, loss = 0.14477419\n",
      "Iteration 90, loss = 0.12508462\n",
      "Iteration 62, loss = 0.14075741\n",
      "Iteration 91, loss = 0.12428376\n",
      "Iteration 63, loss = 0.14157390\n",
      "Iteration 92, loss = 0.12473563\n",
      "Iteration 64, loss = 0.14047060\n",
      "Iteration 65, loss = 0.13430058\n",
      "Iteration 93, loss = 0.13974595\n",
      "Iteration 66, loss = 0.13272838\n",
      "Iteration 94, loss = 0.12525306\n",
      "Iteration 67, loss = 0.12959167\n",
      "Iteration 95, loss = 0.12805654\n",
      "Iteration 68, loss = 0.12749584\n",
      "Iteration 96, loss = 0.12832208\n",
      "Iteration 69, loss = 0.12581739\n",
      "Iteration 97, loss = 0.12163638\n",
      "Iteration 70, loss = 0.12341007\n",
      "Iteration 98, loss = 0.11601435\n",
      "Iteration 71, loss = 0.12176348\n",
      "Iteration 99, loss = 0.11591500\n",
      "Iteration 72, loss = 0.12022817\n",
      "Iteration 100, loss = 0.11635819\n",
      "Iteration 73, loss = 0.11855255\n",
      "Iteration 74, loss = 0.11656930\n",
      "Iteration 75, loss = 0.11613918\n",
      "Iteration 76, loss = 0.11159091\n",
      "Iteration 77, loss = 0.11172802\n",
      "Iteration 78, loss = 0.11036575\n",
      "Iteration 79, loss = 0.10931765\n",
      "Iteration 80, loss = 0.10700017\n",
      "Iteration 81, loss = 0.10429081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 0.10624183\n",
      "Iteration 83, loss = 0.10296776\n",
      "Iteration 84, loss = 0.10206631\n",
      "Iteration 1, loss = 1.26815900\n",
      "Iteration 85, loss = 0.10125912\n",
      "Iteration 2, loss = 0.89154097\n",
      "Iteration 86, loss = 0.09773610\n",
      "Iteration 3, loss = 0.72208612\n",
      "Iteration 87, loss = 0.09716914\n",
      "Iteration 4, loss = 0.60206885\n",
      "Iteration 88, loss = 0.09508818\n",
      "Iteration 5, loss = 0.53921376\n",
      "Iteration 89, loss = 0.09456814\n",
      "Iteration 6, loss = 0.49203852\n",
      "Iteration 90, loss = 0.09347925\n",
      "Iteration 7, loss = 0.45748563\n",
      "Iteration 91, loss = 0.09071075\n",
      "Iteration 8, loss = 0.43233521\n",
      "Iteration 92, loss = 0.08979348\n",
      "Iteration 9, loss = 0.40949641\n",
      "Iteration 93, loss = 0.08848335\n",
      "Iteration 10, loss = 0.39272826\n",
      "Iteration 94, loss = 0.08765150\n",
      "Iteration 11, loss = 0.37742350\n",
      "Iteration 95, loss = 0.08635988\n",
      "Iteration 12, loss = 0.36345233\n",
      "Iteration 96, loss = 0.08749392\n",
      "Iteration 13, loss = 0.35110013\n",
      "Iteration 97, loss = 0.08570046\n",
      "Iteration 14, loss = 0.34082915\n",
      "Iteration 98, loss = 0.08348410\n",
      "Iteration 15, loss = 0.33104272\n",
      "Iteration 99, loss = 0.08448334\n",
      "Iteration 16, loss = 0.32166331\n",
      "Iteration 100, loss = 0.08252750\n",
      "Iteration 17, loss = 0.31377281\n",
      "Iteration 18, loss = 0.30603451\n",
      "Iteration 19, loss = 0.29848016\n",
      "Iteration 20, loss = 0.29204245\n",
      "Iteration 21, loss = 0.28549854\n",
      "Iteration 22, loss = 0.28008641\n",
      "Iteration 23, loss = 0.27598403\n",
      "Iteration 24, loss = 0.26782736\n",
      "Iteration 25, loss = 0.26312500\n",
      "Iteration 26, loss = 0.25856592\n",
      "Iteration 27, loss = 0.25399444\n",
      "Iteration 28, loss = 0.24870645\n",
      "Iteration 29, loss = 0.24461431\n",
      "Iteration 1, loss = 1.11708735\n",
      "Iteration 30, loss = 0.24052291\n",
      "Iteration 2, loss = 0.82314727\n",
      "Iteration 31, loss = 0.23598634\n",
      "Iteration 3, loss = 0.70800111\n",
      "Iteration 32, loss = 0.23319234\n",
      "Iteration 4, loss = 0.62178396\n",
      "Iteration 33, loss = 0.22826117\n",
      "Iteration 5, loss = 0.56282096\n",
      "Iteration 34, loss = 0.22543647\n",
      "Iteration 6, loss = 0.51499294\n",
      "Iteration 35, loss = 0.22104394\n",
      "Iteration 7, loss = 0.48066768\n",
      "Iteration 36, loss = 0.21819413\n",
      "Iteration 8, loss = 0.45246762\n",
      "Iteration 37, loss = 0.21422017\n",
      "Iteration 9, loss = 0.42991671\n",
      "Iteration 38, loss = 0.21098156\n",
      "Iteration 10, loss = 0.41140005\n",
      "Iteration 39, loss = 0.20709522\n",
      "Iteration 11, loss = 0.39389996\n",
      "Iteration 40, loss = 0.20445243\n",
      "Iteration 12, loss = 0.38050434\n",
      "Iteration 41, loss = 0.20211715\n",
      "Iteration 13, loss = 0.36845792\n",
      "Iteration 42, loss = 0.19894303\n",
      "Iteration 14, loss = 0.35665113\n",
      "Iteration 43, loss = 0.19764199\n",
      "Iteration 15, loss = 0.34656342\n",
      "Iteration 44, loss = 0.19484259\n",
      "Iteration 16, loss = 0.33839186\n",
      "Iteration 45, loss = 0.18947213\n",
      "Iteration 17, loss = 0.33068585\n",
      "Iteration 46, loss = 0.18955210\n",
      "Iteration 18, loss = 0.32297277\n",
      "Iteration 47, loss = 0.18426758\n",
      "Iteration 19, loss = 0.31493411\n",
      "Iteration 48, loss = 0.18314857\n",
      "Iteration 20, loss = 0.30878271\n",
      "Iteration 49, loss = 0.17951049\n",
      "Iteration 21, loss = 0.30264647\n",
      "Iteration 50, loss = 0.17830690\n",
      "Iteration 22, loss = 0.29650682\n",
      "Iteration 51, loss = 0.17409595\n",
      "Iteration 23, loss = 0.29147928\n",
      "Iteration 52, loss = 0.17419472\n",
      "Iteration 24, loss = 0.28470715\n",
      "Iteration 53, loss = 0.16874235\n",
      "Iteration 25, loss = 0.28018381\n",
      "Iteration 54, loss = 0.16923935\n",
      "Iteration 26, loss = 0.27565104\n",
      "Iteration 55, loss = 0.16646379\n",
      "Iteration 27, loss = 0.27046646\n",
      "Iteration 56, loss = 0.16348472\n",
      "Iteration 28, loss = 0.26766943\n",
      "Iteration 57, loss = 0.16366800\n",
      "Iteration 29, loss = 0.26207344\n",
      "Iteration 58, loss = 0.15908078\n",
      "Iteration 30, loss = 0.25961464\n",
      "Iteration 59, loss = 0.15689641\n",
      "Iteration 31, loss = 0.25398818\n",
      "Iteration 60, loss = 0.15548333\n",
      "Iteration 32, loss = 0.25096725\n",
      "Iteration 61, loss = 0.15313734\n",
      "Iteration 33, loss = 0.24645277\n",
      "Iteration 62, loss = 0.15088665\n",
      "Iteration 34, loss = 0.24259126\n",
      "Iteration 63, loss = 0.14981243\n",
      "Iteration 35, loss = 0.23854932\n",
      "Iteration 64, loss = 0.14937019\n",
      "Iteration 36, loss = 0.23567866\n",
      "Iteration 65, loss = 0.14701115\n",
      "Iteration 37, loss = 0.23206149\n",
      "Iteration 66, loss = 0.14382979\n",
      "Iteration 38, loss = 0.22972775\n",
      "Iteration 67, loss = 0.14232395\n",
      "Iteration 39, loss = 0.22717430\n",
      "Iteration 68, loss = 0.14325969\n",
      "Iteration 40, loss = 0.22344486\n",
      "Iteration 69, loss = 0.13820574\n",
      "Iteration 41, loss = 0.21953212\n",
      "Iteration 70, loss = 0.13899493\n",
      "Iteration 42, loss = 0.22035540\n",
      "Iteration 71, loss = 0.13581311\n",
      "Iteration 43, loss = 0.21740564\n",
      "Iteration 72, loss = 0.13557010\n",
      "Iteration 44, loss = 0.21623875\n",
      "Iteration 73, loss = 0.13338389\n",
      "Iteration 45, loss = 0.20812267\n",
      "Iteration 74, loss = 0.13146097\n",
      "Iteration 46, loss = 0.20764729\n",
      "Iteration 75, loss = 0.13026174\n",
      "Iteration 47, loss = 0.20159893\n",
      "Iteration 76, loss = 0.12844485\n",
      "Iteration 48, loss = 0.19978350\n",
      "Iteration 77, loss = 0.12849091\n",
      "Iteration 49, loss = 0.19626635\n",
      "Iteration 78, loss = 0.12656429\n",
      "Iteration 50, loss = 0.19576245\n",
      "Iteration 79, loss = 0.12442232\n",
      "Iteration 80, loss = 0.12385342\n",
      "Iteration 51, loss = 0.19127914\n",
      "Iteration 81, loss = 0.12166542\n",
      "Iteration 52, loss = 0.18897360\n",
      "Iteration 82, loss = 0.12120235\n",
      "Iteration 53, loss = 0.18931056\n",
      "Iteration 83, loss = 0.11920553\n",
      "Iteration 54, loss = 0.18440160\n",
      "Iteration 84, loss = 0.11990810\n",
      "Iteration 55, loss = 0.18241844\n",
      "Iteration 85, loss = 0.11701437\n",
      "Iteration 56, loss = 0.17948465\n",
      "Iteration 86, loss = 0.11672058\n",
      "Iteration 57, loss = 0.17659471\n",
      "Iteration 87, loss = 0.11379473\n",
      "Iteration 58, loss = 0.17528265\n",
      "Iteration 88, loss = 0.11574188\n",
      "Iteration 59, loss = 0.17195305\n",
      "Iteration 89, loss = 0.11216336\n",
      "Iteration 60, loss = 0.17128988\n",
      "Iteration 90, loss = 0.11161819\n",
      "Iteration 61, loss = 0.16868696\n",
      "Iteration 91, loss = 0.11307952\n",
      "Iteration 62, loss = 0.16745967\n",
      "Iteration 92, loss = 0.10920119\n",
      "Iteration 63, loss = 0.16447193\n",
      "Iteration 93, loss = 0.10876362\n",
      "Iteration 64, loss = 0.16507165\n",
      "Iteration 94, loss = 0.10717186\n",
      "Iteration 65, loss = 0.15814229\n",
      "Iteration 95, loss = 0.10710535\n",
      "Iteration 66, loss = 0.16049199\n",
      "Iteration 96, loss = 0.10442994\n",
      "Iteration 67, loss = 0.15749385\n",
      "Iteration 97, loss = 0.10569709\n",
      "Iteration 68, loss = 0.15492376\n",
      "Iteration 98, loss = 0.10462072\n",
      "Iteration 69, loss = 0.15139995\n",
      "Iteration 99, loss = 0.10260974\n",
      "Iteration 70, loss = 0.15035283\n",
      "Iteration 100, loss = 0.10195556\n",
      "Iteration 71, loss = 0.14828224\n",
      "Iteration 72, loss = 0.14783298\n",
      "Iteration 73, loss = 0.14593765\n",
      "Iteration 74, loss = 0.14441426\n",
      "Iteration 75, loss = 0.14475273\n",
      "Iteration 76, loss = 0.13982908\n",
      "Iteration 77, loss = 0.13962196\n",
      "Iteration 78, loss = 0.13728903\n",
      "Iteration 79, loss = 0.13434572\n",
      "Iteration 80, loss = 0.13469063\n",
      "Iteration 81, loss = 0.13258739\n",
      "Iteration 82, loss = 0.13132893\n",
      "Iteration 83, loss = 0.12944145\n",
      "Iteration 1, loss = 1.63554888\n",
      "Iteration 84, loss = 0.12881054\n",
      "Iteration 2, loss = 0.99272956\n",
      "Iteration 85, loss = 0.12819539\n",
      "Iteration 3, loss = 0.80247596\n",
      "Iteration 86, loss = 0.12507044\n",
      "Iteration 4, loss = 0.69695112\n",
      "Iteration 87, loss = 0.12529354\n",
      "Iteration 5, loss = 0.60685015\n",
      "Iteration 88, loss = 0.12521527\n",
      "Iteration 6, loss = 0.55676186\n",
      "Iteration 89, loss = 0.12421240\n",
      "Iteration 7, loss = 0.52554069\n",
      "Iteration 90, loss = 0.12156497\n",
      "Iteration 8, loss = 0.49111842\n",
      "Iteration 91, loss = 0.11830704\n",
      "Iteration 9, loss = 0.46575548\n",
      "Iteration 92, loss = 0.11820584\n",
      "Iteration 10, loss = 0.44745315\n",
      "Iteration 93, loss = 0.11794406\n",
      "Iteration 11, loss = 0.42897569\n",
      "Iteration 94, loss = 0.11652135\n",
      "Iteration 12, loss = 0.41320351\n",
      "Iteration 95, loss = 0.11594812\n",
      "Iteration 13, loss = 0.40076954\n",
      "Iteration 96, loss = 0.11289546\n",
      "Iteration 14, loss = 0.38879875\n",
      "Iteration 97, loss = 0.11477550\n",
      "Iteration 15, loss = 0.37713045\n",
      "Iteration 98, loss = 0.10999193\n",
      "Iteration 99, loss = 0.11215009\n",
      "Iteration 16, loss = 0.36753304\n",
      "Iteration 100, loss = 0.10924483\n",
      "Iteration 17, loss = 0.35833339\n",
      "Iteration 18, loss = 0.35019942\n",
      "Iteration 19, loss = 0.34154843\n",
      "Iteration 20, loss = 0.33542660\n",
      "Iteration 21, loss = 0.32798628\n",
      "Iteration 22, loss = 0.32135734\n",
      "Iteration 23, loss = 0.31558945\n",
      "Iteration 24, loss = 0.31042037\n",
      "Iteration 25, loss = 0.30467044\n",
      "Iteration 26, loss = 0.29915223\n",
      "Iteration 27, loss = 0.29313703\n",
      "Iteration 28, loss = 0.29025782\n",
      "Iteration 29, loss = 0.28434997\n",
      "Iteration 1, loss = 1.14454026\n",
      "Iteration 30, loss = 0.28047338\n",
      "Iteration 2, loss = 0.84736564\n",
      "Iteration 31, loss = 0.27583498\n",
      "Iteration 3, loss = 0.67356353\n",
      "Iteration 32, loss = 0.27241957\n",
      "Iteration 4, loss = 0.58158706\n",
      "Iteration 33, loss = 0.26695420\n",
      "Iteration 5, loss = 0.51872955\n",
      "Iteration 34, loss = 0.26289030\n",
      "Iteration 6, loss = 0.47680191\n",
      "Iteration 35, loss = 0.25918294\n",
      "Iteration 7, loss = 0.44335040\n",
      "Iteration 36, loss = 0.25483966\n",
      "Iteration 8, loss = 0.42041108\n",
      "Iteration 37, loss = 0.25124157\n",
      "Iteration 9, loss = 0.39971368\n",
      "Iteration 38, loss = 0.24824094\n",
      "Iteration 10, loss = 0.38322732\n",
      "Iteration 39, loss = 0.24468813\n",
      "Iteration 11, loss = 0.37039260\n",
      "Iteration 40, loss = 0.24114436\n",
      "Iteration 12, loss = 0.35793725\n",
      "Iteration 41, loss = 0.23806061\n",
      "Iteration 13, loss = 0.34951949\n",
      "Iteration 42, loss = 0.23527462\n",
      "Iteration 14, loss = 0.33636103\n",
      "Iteration 43, loss = 0.23267107\n",
      "Iteration 15, loss = 0.32848083\n",
      "Iteration 44, loss = 0.22883597\n",
      "Iteration 16, loss = 0.32002817\n",
      "Iteration 45, loss = 0.22776042\n",
      "Iteration 17, loss = 0.31236756\n",
      "Iteration 46, loss = 0.22222513\n",
      "Iteration 18, loss = 0.30472351\n",
      "Iteration 47, loss = 0.22120465\n",
      "Iteration 19, loss = 0.30020775\n",
      "Iteration 48, loss = 0.21662098\n",
      "Iteration 20, loss = 0.29340938\n",
      "Iteration 49, loss = 0.21473560\n",
      "Iteration 21, loss = 0.28640732\n",
      "Iteration 50, loss = 0.21233380\n",
      "Iteration 22, loss = 0.28187635\n",
      "Iteration 51, loss = 0.20917645\n",
      "Iteration 23, loss = 0.27586835\n",
      "Iteration 52, loss = 0.20679372\n",
      "Iteration 53, loss = 0.20410427\n",
      "Iteration 24, loss = 0.27231274\n",
      "Iteration 54, loss = 0.20113333\n",
      "Iteration 25, loss = 0.26545075\n",
      "Iteration 55, loss = 0.19792263\n",
      "Iteration 26, loss = 0.26179057\n",
      "Iteration 56, loss = 0.19742535\n",
      "Iteration 27, loss = 0.25738221\n",
      "Iteration 57, loss = 0.19561618\n",
      "Iteration 28, loss = 0.25213385\n",
      "Iteration 58, loss = 0.19172738\n",
      "Iteration 29, loss = 0.24819346\n",
      "Iteration 30, loss = 0.24579362\n",
      "Iteration 59, loss = 0.18936174\n",
      "Iteration 31, loss = 0.24226347\n",
      "Iteration 60, loss = 0.18725265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.23633864\n",
      "Iteration 61, loss = 0.18547134\n",
      "Iteration 33, loss = 0.23422217\n",
      "Iteration 62, loss = 0.18341762\n",
      "Iteration 34, loss = 0.22932754\n",
      "Iteration 63, loss = 0.18144311\n",
      "Iteration 64, loss = 0.17871887\n",
      "Iteration 35, loss = 0.22758085\n",
      "Iteration 36, loss = 0.22286621\n",
      "Iteration 65, loss = 0.17610611\n",
      "Iteration 37, loss = 0.21913915\n",
      "Iteration 66, loss = 0.17408454\n",
      "Iteration 38, loss = 0.21589646\n",
      "Iteration 67, loss = 0.17062708\n",
      "Iteration 39, loss = 0.21310185\n",
      "Iteration 68, loss = 0.16889225\n",
      "Iteration 40, loss = 0.21078244\n",
      "Iteration 69, loss = 0.16691254\n",
      "Iteration 41, loss = 0.20694695\n",
      "Iteration 70, loss = 0.16509917\n",
      "Iteration 42, loss = 0.20420719\n",
      "Iteration 71, loss = 0.16371981\n",
      "Iteration 43, loss = 0.20245652\n",
      "Iteration 72, loss = 0.16238207\n",
      "Iteration 44, loss = 0.19858148\n",
      "Iteration 73, loss = 0.15984749\n",
      "Iteration 45, loss = 0.19573066\n",
      "Iteration 74, loss = 0.15939446\n",
      "Iteration 46, loss = 0.19313120\n",
      "Iteration 75, loss = 0.15648611\n",
      "Iteration 47, loss = 0.19200893\n",
      "Iteration 76, loss = 0.15524938\n",
      "Iteration 48, loss = 0.18689528\n",
      "Iteration 77, loss = 0.15347855\n",
      "Iteration 49, loss = 0.18457071\n",
      "Iteration 78, loss = 0.15275897\n",
      "Iteration 50, loss = 0.18420264\n",
      "Iteration 79, loss = 0.15005848\n",
      "Iteration 51, loss = 0.17942156\n",
      "Iteration 80, loss = 0.14842879\n",
      "Iteration 52, loss = 0.17833649\n",
      "Iteration 81, loss = 0.14872415\n",
      "Iteration 53, loss = 0.17431366\n",
      "Iteration 82, loss = 0.14851867\n",
      "Iteration 54, loss = 0.17310729\n",
      "Iteration 83, loss = 0.14697979\n",
      "Iteration 55, loss = 0.17114525\n",
      "Iteration 84, loss = 0.14247477\n",
      "Iteration 56, loss = 0.16775547\n",
      "Iteration 85, loss = 0.14092421\n",
      "Iteration 57, loss = 0.16670635\n",
      "Iteration 86, loss = 0.13879485\n",
      "Iteration 58, loss = 0.16291214\n",
      "Iteration 87, loss = 0.13809587\n",
      "Iteration 59, loss = 0.16171762\n",
      "Iteration 88, loss = 0.13724052\n",
      "Iteration 60, loss = 0.15860504\n",
      "Iteration 89, loss = 0.13417167\n",
      "Iteration 61, loss = 0.15719121\n",
      "Iteration 90, loss = 0.13368164\n",
      "Iteration 62, loss = 0.15597006\n",
      "Iteration 91, loss = 0.13348228\n",
      "Iteration 63, loss = 0.15547058\n",
      "Iteration 92, loss = 0.13056809\n",
      "Iteration 64, loss = 0.15033480\n",
      "Iteration 93, loss = 0.12983385\n",
      "Iteration 65, loss = 0.15035575\n",
      "Iteration 94, loss = 0.12911571\n",
      "Iteration 66, loss = 0.14939471\n",
      "Iteration 95, loss = 0.12802329\n",
      "Iteration 67, loss = 0.14733753\n",
      "Iteration 96, loss = 0.12563924\n",
      "Iteration 68, loss = 0.14353349\n",
      "Iteration 97, loss = 0.12452137\n",
      "Iteration 69, loss = 0.14277235\n",
      "Iteration 98, loss = 0.12264417\n",
      "Iteration 99, loss = 0.12237187\n",
      "Iteration 70, loss = 0.13989437\n",
      "Iteration 100, loss = 0.12104263\n",
      "Iteration 71, loss = 0.14025404\n",
      "Iteration 101, loss = 0.11943901\n",
      "Iteration 72, loss = 0.13818262\n",
      "Iteration 102, loss = 0.12048803\n",
      "Iteration 73, loss = 0.13561831\n",
      "Iteration 103, loss = 0.11772848\n",
      "Iteration 74, loss = 0.13602693\n",
      "Iteration 75, loss = 0.13340293\n",
      "Iteration 104, loss = 0.11833965\n",
      "Iteration 76, loss = 0.13136295\n",
      "Iteration 105, loss = 0.11448863\n",
      "Iteration 106, loss = 0.11635472\n",
      "Iteration 77, loss = 0.13301208\n",
      "Iteration 107, loss = 0.11273655\n",
      "Iteration 78, loss = 0.13057288\n",
      "Iteration 108, loss = 0.11317618\n",
      "Iteration 79, loss = 0.12981550\n",
      "Iteration 80, loss = 0.12427034\n",
      "Iteration 109, loss = 0.11096593\n",
      "Iteration 81, loss = 0.12398249\n",
      "Iteration 110, loss = 0.11075208\n",
      "Iteration 82, loss = 0.12372688\n",
      "Iteration 111, loss = 0.11006064\n",
      "Iteration 83, loss = 0.12524607\n",
      "Iteration 112, loss = 0.10838529\n",
      "Iteration 84, loss = 0.12076822\n",
      "Iteration 113, loss = 0.10770161\n",
      "Iteration 85, loss = 0.12056004\n",
      "Iteration 114, loss = 0.10863348\n",
      "Iteration 86, loss = 0.11722831\n",
      "Iteration 115, loss = 0.10713212\n",
      "Iteration 87, loss = 0.11836343\n",
      "Iteration 116, loss = 0.10329760\n",
      "Iteration 88, loss = 0.12113220\n",
      "Iteration 117, loss = 0.10431537\n",
      "Iteration 89, loss = 0.11450134\n",
      "Iteration 118, loss = 0.10273711\n",
      "Iteration 90, loss = 0.11243429\n",
      "Iteration 119, loss = 0.10163543\n",
      "Iteration 120, loss = 0.10303023\n",
      "Iteration 91, loss = 0.11250022\n",
      "Iteration 92, loss = 0.11372109\n",
      "Iteration 121, loss = 0.09866982\n",
      "Iteration 93, loss = 0.12034863\n",
      "Iteration 122, loss = 0.10124162\n",
      "Iteration 94, loss = 0.11068535\n",
      "Iteration 123, loss = 0.09894929\n",
      "Iteration 95, loss = 0.10675904\n",
      "Iteration 124, loss = 0.09657736\n",
      "Iteration 96, loss = 0.11350640\n",
      "Iteration 125, loss = 0.09816479\n",
      "Iteration 97, loss = 0.10627635\n",
      "Iteration 126, loss = 0.09559985\n",
      "Iteration 98, loss = 0.10733052\n",
      "Iteration 127, loss = 0.09497021\n",
      "Iteration 99, loss = 0.10236114\n",
      "Iteration 128, loss = 0.09477306\n",
      "Iteration 100, loss = 0.10647663\n",
      "Iteration 129, loss = 0.09290317\n",
      "Iteration 101, loss = 0.10488545\n",
      "Iteration 130, loss = 0.09274799\n",
      "Iteration 102, loss = 0.10483438\n",
      "Iteration 131, loss = 0.09132248\n",
      "Iteration 103, loss = 0.10225068\n",
      "Iteration 132, loss = 0.09109703\n",
      "Iteration 104, loss = 0.10219471\n",
      "Iteration 133, loss = 0.09017482\n",
      "Iteration 105, loss = 0.10020998\n",
      "Iteration 134, loss = 0.09024428\n",
      "Iteration 106, loss = 0.10190184\n",
      "Iteration 135, loss = 0.08922874\n",
      "Iteration 107, loss = 0.09721829\n",
      "Iteration 136, loss = 0.08639939\n",
      "Iteration 108, loss = 0.09815989\n",
      "Iteration 137, loss = 0.08797529\n",
      "Iteration 109, loss = 0.09374483\n",
      "Iteration 138, loss = 0.08623251\n",
      "Iteration 110, loss = 0.09242171\n",
      "Iteration 139, loss = 0.08479216\n",
      "Iteration 111, loss = 0.09192608\n",
      "Iteration 140, loss = 0.08321355\n",
      "Iteration 112, loss = 0.08843589\n",
      "Iteration 141, loss = 0.08448417\n",
      "Iteration 113, loss = 0.08933127\n",
      "Iteration 142, loss = 0.08376852\n",
      "Iteration 114, loss = 0.08898759\n",
      "Iteration 143, loss = 0.08146580\n",
      "Iteration 115, loss = 0.08604612\n",
      "Iteration 144, loss = 0.08133113\n",
      "Iteration 116, loss = 0.08606442\n",
      "Iteration 145, loss = 0.08061764\n",
      "Iteration 117, loss = 0.08507338\n",
      "Iteration 146, loss = 0.07978198\n",
      "Iteration 118, loss = 0.08567462\n",
      "Iteration 147, loss = 0.07979125\n",
      "Iteration 119, loss = 0.08564811\n",
      "Iteration 148, loss = 0.07851420\n",
      "Iteration 120, loss = 0.08288811\n",
      "Iteration 149, loss = 0.07900939\n",
      "Iteration 121, loss = 0.08340010\n",
      "Iteration 150, loss = 0.07902735\n",
      "Iteration 122, loss = 0.08289758\n",
      "Iteration 151, loss = 0.07824923\n",
      "Iteration 123, loss = 0.08119769\n",
      "Iteration 152, loss = 0.07954489\n",
      "Iteration 124, loss = 0.08109171\n",
      "Iteration 153, loss = 0.07776065\n",
      "Iteration 125, loss = 0.07812460\n",
      "Iteration 154, loss = 0.07733049\n",
      "Iteration 126, loss = 0.07860911\n",
      "Iteration 155, loss = 0.07617688\n",
      "Iteration 127, loss = 0.07961982\n",
      "Iteration 156, loss = 0.07394633\n",
      "Iteration 128, loss = 0.07781060\n",
      "Iteration 157, loss = 0.07388567\n",
      "Iteration 158, loss = 0.07480934\n",
      "Iteration 129, loss = 0.07526347\n",
      "Iteration 159, loss = 0.07781180\n",
      "Iteration 130, loss = 0.07432938\n",
      "Iteration 160, loss = 0.07156752\n",
      "Iteration 131, loss = 0.07587782\n",
      "Iteration 161, loss = 0.07124100\n",
      "Iteration 132, loss = 0.07415904\n",
      "Iteration 162, loss = 0.07167277\n",
      "Iteration 133, loss = 0.07227665\n",
      "Iteration 163, loss = 0.07050690\n",
      "Iteration 134, loss = 0.07275292\n",
      "Iteration 164, loss = 0.07015093\n",
      "Iteration 135, loss = 0.07120999\n",
      "Iteration 165, loss = 0.06760406\n",
      "Iteration 136, loss = 0.07133163\n",
      "Iteration 166, loss = 0.06947376\n",
      "Iteration 137, loss = 0.07245501\n",
      "Iteration 167, loss = 0.06838501\n",
      "Iteration 138, loss = 0.07209922\n",
      "Iteration 168, loss = 0.06958952\n",
      "Iteration 139, loss = 0.06937372\n",
      "Iteration 169, loss = 0.06806326\n",
      "Iteration 140, loss = 0.06897402\n",
      "Iteration 170, loss = 0.06752459\n",
      "Iteration 141, loss = 0.06728120\n",
      "Iteration 171, loss = 0.06643135\n",
      "Iteration 142, loss = 0.06728595\n",
      "Iteration 172, loss = 0.06530940\n",
      "Iteration 143, loss = 0.06610669\n",
      "Iteration 173, loss = 0.06274782\n",
      "Iteration 144, loss = 0.06501143\n",
      "Iteration 174, loss = 0.06232253\n",
      "Iteration 145, loss = 0.06385945\n",
      "Iteration 175, loss = 0.06265726\n",
      "Iteration 146, loss = 0.06539114\n",
      "Iteration 176, loss = 0.06332359\n",
      "Iteration 147, loss = 0.06320008\n",
      "Iteration 177, loss = 0.06073653\n",
      "Iteration 148, loss = 0.06357062\n",
      "Iteration 178, loss = 0.06107452\n",
      "Iteration 149, loss = 0.06174094\n",
      "Iteration 179, loss = 0.05971829\n",
      "Iteration 150, loss = 0.06257086\n",
      "Iteration 180, loss = 0.05909368\n",
      "Iteration 151, loss = 0.06240004\n",
      "Iteration 181, loss = 0.05918023\n",
      "Iteration 152, loss = 0.06360649\n",
      "Iteration 182, loss = 0.06015962\n",
      "Iteration 153, loss = 0.06136547\n",
      "Iteration 183, loss = 0.06248989\n",
      "Iteration 154, loss = 0.05865625\n",
      "Iteration 184, loss = 0.06576683\n",
      "Iteration 155, loss = 0.05873708\n",
      "Iteration 185, loss = 0.06401669\n",
      "Iteration 156, loss = 0.05873461\n",
      "Iteration 186, loss = 0.05668079\n",
      "Iteration 157, loss = 0.06159694\n",
      "Iteration 187, loss = 0.05856822\n",
      "Iteration 158, loss = 0.06431175\n",
      "Iteration 188, loss = 0.05707375\n",
      "Iteration 159, loss = 0.05901094\n",
      "Iteration 189, loss = 0.05606491\n",
      "Iteration 160, loss = 0.05817934\n",
      "Iteration 190, loss = 0.05485808\n",
      "Iteration 161, loss = 0.06182622\n",
      "Iteration 191, loss = 0.05694828\n",
      "Iteration 162, loss = 0.06031194\n",
      "Iteration 192, loss = 0.05345072\n",
      "Iteration 163, loss = 0.05535188\n",
      "Iteration 193, loss = 0.05380761\n",
      "Iteration 164, loss = 0.05490977\n",
      "Iteration 194, loss = 0.05304517\n",
      "Iteration 165, loss = 0.05468258\n",
      "Iteration 195, loss = 0.05278652\n",
      "Iteration 166, loss = 0.05458737\n",
      "Iteration 196, loss = 0.05080102\n",
      "Iteration 167, loss = 0.05391072\n",
      "Iteration 197, loss = 0.05048177\n",
      "Iteration 168, loss = 0.05782787\n",
      "Iteration 198, loss = 0.04962405\n",
      "Iteration 169, loss = 0.05691685\n",
      "Iteration 199, loss = 0.05117322\n",
      "Iteration 170, loss = 0.05315662\n",
      "Iteration 200, loss = 0.04952935\n",
      "Iteration 171, loss = 0.05308572\n",
      "Iteration 172, loss = 0.05240342\n",
      "Iteration 173, loss = 0.05038418\n",
      "Iteration 174, loss = 0.04891085\n",
      "Iteration 175, loss = 0.04855052\n",
      "Iteration 176, loss = 0.04766230\n",
      "Iteration 177, loss = 0.04796597\n",
      "Iteration 178, loss = 0.04642902\n",
      "Iteration 179, loss = 0.04658896\n",
      "Iteration 180, loss = 0.04509859\n",
      "Iteration 181, loss = 0.04708797\n",
      "Iteration 182, loss = 0.04560326\n",
      "Iteration 183, loss = 0.04584121\n",
      "Iteration 1, loss = 1.11024155\n",
      "Iteration 184, loss = 0.04453081\n",
      "Iteration 2, loss = 0.82356110\n",
      "Iteration 185, loss = 0.04643811\n",
      "Iteration 3, loss = 0.66343286\n",
      "Iteration 186, loss = 0.04490061\n",
      "Iteration 4, loss = 0.57719697\n",
      "Iteration 187, loss = 0.04815236\n",
      "Iteration 5, loss = 0.52185195\n",
      "Iteration 188, loss = 0.04493303\n",
      "Iteration 6, loss = 0.48039254\n",
      "Iteration 189, loss = 0.04379081\n",
      "Iteration 7, loss = 0.45231324\n",
      "Iteration 190, loss = 0.04949882\n",
      "Iteration 8, loss = 0.43097838\n",
      "Iteration 191, loss = 0.04375531\n",
      "Iteration 9, loss = 0.40908408\n",
      "Iteration 192, loss = 0.04154944\n",
      "Iteration 10, loss = 0.39301018\n",
      "Iteration 193, loss = 0.04054644\n",
      "Iteration 11, loss = 0.38285381\n",
      "Iteration 194, loss = 0.04050389\n",
      "Iteration 12, loss = 0.36620133\n",
      "Iteration 195, loss = 0.04132609\n",
      "Iteration 13, loss = 0.35851940\n",
      "Iteration 196, loss = 0.04155597\n",
      "Iteration 14, loss = 0.34384094\n",
      "Iteration 197, loss = 0.04257680\n",
      "Iteration 15, loss = 0.33737636\n",
      "Iteration 198, loss = 0.03949732\n",
      "Iteration 16, loss = 0.32691799\n",
      "Iteration 199, loss = 0.03844410\n",
      "Iteration 17, loss = 0.31951720\n",
      "Iteration 200, loss = 0.03869460\n",
      "Iteration 18, loss = 0.31098007\n",
      "Iteration 19, loss = 0.30346518\n",
      "Iteration 20, loss = 0.29761227\n",
      "Iteration 21, loss = 0.28973999\n",
      "Iteration 22, loss = 0.28461962\n",
      "Iteration 23, loss = 0.27972510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.27298152\n",
      "Iteration 25, loss = 0.26923537\n",
      "Iteration 26, loss = 0.26505781\n",
      "Iteration 27, loss = 0.25802442\n",
      "Iteration 28, loss = 0.25367036\n",
      "Iteration 29, loss = 0.24865602\n",
      "Iteration 30, loss = 0.24492899\n",
      "Iteration 31, loss = 0.23975401\n",
      "Iteration 1, loss = 1.57538633\n",
      "Iteration 32, loss = 0.23602194\n",
      "Iteration 2, loss = 0.92422217\n",
      "Iteration 33, loss = 0.23257742\n",
      "Iteration 3, loss = 0.80676255\n",
      "Iteration 34, loss = 0.23095481\n",
      "Iteration 4, loss = 0.71151961\n",
      "Iteration 35, loss = 0.22597673\n",
      "Iteration 5, loss = 0.61800526\n",
      "Iteration 36, loss = 0.22276934\n",
      "Iteration 6, loss = 0.56850286\n",
      "Iteration 37, loss = 0.21707160\n",
      "Iteration 7, loss = 0.53805076\n",
      "Iteration 38, loss = 0.21377818\n",
      "Iteration 8, loss = 0.50810936\n",
      "Iteration 39, loss = 0.21175095\n",
      "Iteration 9, loss = 0.48235176\n",
      "Iteration 40, loss = 0.20793507\n",
      "Iteration 10, loss = 0.46322569\n",
      "Iteration 41, loss = 0.20466548\n",
      "Iteration 11, loss = 0.44733620\n",
      "Iteration 42, loss = 0.20394986\n",
      "Iteration 12, loss = 0.43193606\n",
      "Iteration 43, loss = 0.20736870\n",
      "Iteration 13, loss = 0.41827703\n",
      "Iteration 44, loss = 0.20054041\n",
      "Iteration 14, loss = 0.40720076\n",
      "Iteration 45, loss = 0.19576421\n",
      "Iteration 15, loss = 0.39618926\n",
      "Iteration 46, loss = 0.19207690\n",
      "Iteration 16, loss = 0.38594870\n",
      "Iteration 47, loss = 0.18964161\n",
      "Iteration 17, loss = 0.37753736\n",
      "Iteration 48, loss = 0.18579683\n",
      "Iteration 18, loss = 0.36848295\n",
      "Iteration 49, loss = 0.18251514\n",
      "Iteration 19, loss = 0.35988972\n",
      "Iteration 50, loss = 0.17927861\n",
      "Iteration 20, loss = 0.35311611\n",
      "Iteration 51, loss = 0.17761424\n",
      "Iteration 21, loss = 0.34610441\n",
      "Iteration 52, loss = 0.17598726\n",
      "Iteration 22, loss = 0.33922902\n",
      "Iteration 53, loss = 0.17502505\n",
      "Iteration 23, loss = 0.33250731\n",
      "Iteration 54, loss = 0.17321888\n",
      "Iteration 24, loss = 0.32689855\n",
      "Iteration 55, loss = 0.16871915\n",
      "Iteration 25, loss = 0.32110667\n",
      "Iteration 56, loss = 0.16567874\n",
      "Iteration 26, loss = 0.31572406\n",
      "Iteration 57, loss = 0.16444046\n",
      "Iteration 27, loss = 0.31011376\n",
      "Iteration 58, loss = 0.16239122\n",
      "Iteration 28, loss = 0.30588967\n",
      "Iteration 59, loss = 0.16000385\n",
      "Iteration 29, loss = 0.30079718\n",
      "Iteration 60, loss = 0.15578419\n",
      "Iteration 30, loss = 0.29705348\n",
      "Iteration 61, loss = 0.15502454\n",
      "Iteration 31, loss = 0.29320366\n",
      "Iteration 62, loss = 0.15054899\n",
      "Iteration 32, loss = 0.28911354\n",
      "Iteration 63, loss = 0.14991132\n",
      "Iteration 33, loss = 0.28375018\n",
      "Iteration 64, loss = 0.14735061\n",
      "Iteration 34, loss = 0.28015597\n",
      "Iteration 65, loss = 0.14700577\n",
      "Iteration 35, loss = 0.27592075\n",
      "Iteration 66, loss = 0.14573837\n",
      "Iteration 36, loss = 0.27262188\n",
      "Iteration 67, loss = 0.14191106\n",
      "Iteration 37, loss = 0.26938194\n",
      "Iteration 68, loss = 0.14030398\n",
      "Iteration 38, loss = 0.26632020\n",
      "Iteration 69, loss = 0.13800400\n",
      "Iteration 39, loss = 0.26205495\n",
      "Iteration 70, loss = 0.13554010\n",
      "Iteration 40, loss = 0.25980205\n",
      "Iteration 71, loss = 0.13357360\n",
      "Iteration 41, loss = 0.25660634\n",
      "Iteration 72, loss = 0.13319415\n",
      "Iteration 42, loss = 0.25348256\n",
      "Iteration 73, loss = 0.13182400\n",
      "Iteration 43, loss = 0.25091614\n",
      "Iteration 74, loss = 0.12991405\n",
      "Iteration 44, loss = 0.24656866\n",
      "Iteration 75, loss = 0.12955157\n",
      "Iteration 45, loss = 0.24504572\n",
      "Iteration 76, loss = 0.12722066\n",
      "Iteration 46, loss = 0.23991931\n",
      "Iteration 77, loss = 0.12467883\n",
      "Iteration 47, loss = 0.23833757\n",
      "Iteration 78, loss = 0.12324199\n",
      "Iteration 48, loss = 0.23477681\n",
      "Iteration 79, loss = 0.12331476\n",
      "Iteration 49, loss = 0.23252715\n",
      "Iteration 80, loss = 0.12166840\n",
      "Iteration 50, loss = 0.22876225\n",
      "Iteration 81, loss = 0.11940604\n",
      "Iteration 51, loss = 0.22641427\n",
      "Iteration 82, loss = 0.12087529\n",
      "Iteration 52, loss = 0.22408763\n",
      "Iteration 83, loss = 0.11672141\n",
      "Iteration 53, loss = 0.22175781\n",
      "Iteration 84, loss = 0.11504166\n",
      "Iteration 54, loss = 0.21938148\n",
      "Iteration 85, loss = 0.11500265\n",
      "Iteration 55, loss = 0.21660517\n",
      "Iteration 86, loss = 0.11444701\n",
      "Iteration 56, loss = 0.21436982\n",
      "Iteration 57, loss = 0.21124226\n",
      "Iteration 87, loss = 0.11188305\n",
      "Iteration 58, loss = 0.20992362\n",
      "Iteration 88, loss = 0.11189268\n",
      "Iteration 59, loss = 0.20616269\n",
      "Iteration 89, loss = 0.10997812\n",
      "Iteration 60, loss = 0.20641064\n",
      "Iteration 90, loss = 0.11077152\n",
      "Iteration 61, loss = 0.20247656\n",
      "Iteration 91, loss = 0.10766340\n",
      "Iteration 92, loss = 0.10483708\n",
      "Iteration 62, loss = 0.20116154\n",
      "Iteration 63, loss = 0.19749217\n",
      "Iteration 93, loss = 0.10454854\n",
      "Iteration 64, loss = 0.19536875\n",
      "Iteration 94, loss = 0.10357568\n",
      "Iteration 65, loss = 0.19327159\n",
      "Iteration 95, loss = 0.10227748\n",
      "Iteration 66, loss = 0.19136592\n",
      "Iteration 96, loss = 0.10231024\n",
      "Iteration 67, loss = 0.19174654\n",
      "Iteration 97, loss = 0.10068845\n",
      "Iteration 68, loss = 0.18744014\n",
      "Iteration 98, loss = 0.09884444\n",
      "Iteration 69, loss = 0.18605488\n",
      "Iteration 99, loss = 0.09901771\n",
      "Iteration 70, loss = 0.18299367\n",
      "Iteration 100, loss = 0.09706862\n",
      "Iteration 71, loss = 0.18343334\n",
      "Iteration 101, loss = 0.09531459\n",
      "Iteration 72, loss = 0.17734163\n",
      "Iteration 102, loss = 0.09448980\n",
      "Iteration 103, loss = 0.09514310\n",
      "Iteration 73, loss = 0.18030414\n",
      "Iteration 104, loss = 0.09508313\n",
      "Iteration 74, loss = 0.17442518\n",
      "Iteration 105, loss = 0.09346960\n",
      "Iteration 75, loss = 0.17406994\n",
      "Iteration 76, loss = 0.17196613\n",
      "Iteration 106, loss = 0.09245058\n",
      "Iteration 107, loss = 0.09037770\n",
      "Iteration 77, loss = 0.17098297\n",
      "Iteration 108, loss = 0.09071679\n",
      "Iteration 78, loss = 0.16945714\n",
      "Iteration 109, loss = 0.09505585\n",
      "Iteration 79, loss = 0.16655537\n",
      "Iteration 80, loss = 0.16582284\n",
      "Iteration 110, loss = 0.09438359\n",
      "Iteration 81, loss = 0.16189910\n",
      "Iteration 111, loss = 0.09093204\n",
      "Iteration 82, loss = 0.16104697\n",
      "Iteration 112, loss = 0.09004470\n",
      "Iteration 83, loss = 0.15998678\n",
      "Iteration 113, loss = 0.08545236\n",
      "Iteration 84, loss = 0.15631855\n",
      "Iteration 114, loss = 0.08788941\n",
      "Iteration 85, loss = 0.15545596\n",
      "Iteration 115, loss = 0.08345186\n",
      "Iteration 86, loss = 0.15518586\n",
      "Iteration 116, loss = 0.08319371\n",
      "Iteration 87, loss = 0.15286386\n",
      "Iteration 117, loss = 0.08211249\n",
      "Iteration 88, loss = 0.15111110\n",
      "Iteration 118, loss = 0.08203834\n",
      "Iteration 89, loss = 0.14781285\n",
      "Iteration 119, loss = 0.08407372\n",
      "Iteration 90, loss = 0.14845039\n",
      "Iteration 120, loss = 0.07906258\n",
      "Iteration 91, loss = 0.14443172\n",
      "Iteration 121, loss = 0.08025339\n",
      "Iteration 92, loss = 0.14416375\n",
      "Iteration 122, loss = 0.07821721\n",
      "Iteration 93, loss = 0.14239784\n",
      "Iteration 123, loss = 0.07657856\n",
      "Iteration 94, loss = 0.14096589\n",
      "Iteration 124, loss = 0.07783506\n",
      "Iteration 95, loss = 0.13974414\n",
      "Iteration 125, loss = 0.07469398\n",
      "Iteration 96, loss = 0.13813023\n",
      "Iteration 126, loss = 0.07428683\n",
      "Iteration 97, loss = 0.13632428\n",
      "Iteration 127, loss = 0.07346175\n",
      "Iteration 98, loss = 0.13424812\n",
      "Iteration 128, loss = 0.07283169\n",
      "Iteration 99, loss = 0.13522474\n",
      "Iteration 129, loss = 0.07182636\n",
      "Iteration 100, loss = 0.13238487\n",
      "Iteration 130, loss = 0.07188450\n",
      "Iteration 101, loss = 0.13175344\n",
      "Iteration 131, loss = 0.07350317\n",
      "Iteration 102, loss = 0.12973829\n",
      "Iteration 132, loss = 0.07528193\n",
      "Iteration 133, loss = 0.07501057\n",
      "Iteration 103, loss = 0.12859443\n",
      "Iteration 104, loss = 0.12736520\n",
      "Iteration 134, loss = 0.06803107\n",
      "Iteration 105, loss = 0.12589845\n",
      "Iteration 135, loss = 0.06964565\n",
      "Iteration 106, loss = 0.12528449\n",
      "Iteration 136, loss = 0.06765456\n",
      "Iteration 107, loss = 0.12322492\n",
      "Iteration 137, loss = 0.06694127\n",
      "Iteration 108, loss = 0.12303781\n",
      "Iteration 138, loss = 0.06640892\n",
      "Iteration 109, loss = 0.12144077\n",
      "Iteration 139, loss = 0.06521991\n",
      "Iteration 110, loss = 0.12075400\n",
      "Iteration 140, loss = 0.06609575\n",
      "Iteration 111, loss = 0.11904681\n",
      "Iteration 141, loss = 0.06412832\n",
      "Iteration 112, loss = 0.11872662\n",
      "Iteration 142, loss = 0.06373976\n",
      "Iteration 113, loss = 0.11639001\n",
      "Iteration 143, loss = 0.06305081\n",
      "Iteration 114, loss = 0.11751260\n",
      "Iteration 144, loss = 0.06228795\n",
      "Iteration 145, loss = 0.06257914\n",
      "Iteration 115, loss = 0.11520991\n",
      "Iteration 146, loss = 0.06146935\n",
      "Iteration 116, loss = 0.11583127\n",
      "Iteration 147, loss = 0.06226531\n",
      "Iteration 117, loss = 0.11428236\n",
      "Iteration 148, loss = 0.06315114\n",
      "Iteration 118, loss = 0.11053835\n",
      "Iteration 149, loss = 0.06163920\n",
      "Iteration 119, loss = 0.11199018\n",
      "Iteration 150, loss = 0.05859894\n",
      "Iteration 120, loss = 0.10913148\n",
      "Iteration 151, loss = 0.05821723\n",
      "Iteration 121, loss = 0.10872893\n",
      "Iteration 152, loss = 0.05735369\n",
      "Iteration 122, loss = 0.10791633\n",
      "Iteration 153, loss = 0.05671163\n",
      "Iteration 123, loss = 0.10705517\n",
      "Iteration 154, loss = 0.05679864\n",
      "Iteration 124, loss = 0.10878237\n",
      "Iteration 155, loss = 0.05694736\n",
      "Iteration 125, loss = 0.11032253\n",
      "Iteration 156, loss = 0.05714588\n",
      "Iteration 126, loss = 0.10604812\n",
      "Iteration 157, loss = 0.05895881\n",
      "Iteration 127, loss = 0.10417092\n",
      "Iteration 158, loss = 0.06025234\n",
      "Iteration 128, loss = 0.10775288\n",
      "Iteration 159, loss = 0.05809196\n",
      "Iteration 129, loss = 0.10077578\n",
      "Iteration 160, loss = 0.05447409\n",
      "Iteration 130, loss = 0.10077687\n",
      "Iteration 161, loss = 0.05498715\n",
      "Iteration 131, loss = 0.10066161\n",
      "Iteration 162, loss = 0.05330714\n",
      "Iteration 132, loss = 0.09887175\n",
      "Iteration 163, loss = 0.05250952\n",
      "Iteration 133, loss = 0.10058751\n",
      "Iteration 164, loss = 0.05200591\n",
      "Iteration 134, loss = 0.09867674\n",
      "Iteration 165, loss = 0.05151858\n",
      "Iteration 135, loss = 0.09835613\n",
      "Iteration 166, loss = 0.05068671\n",
      "Iteration 136, loss = 0.09811141\n",
      "Iteration 167, loss = 0.05143483\n",
      "Iteration 137, loss = 0.09497174\n",
      "Iteration 168, loss = 0.05085741\n",
      "Iteration 138, loss = 0.09632130\n",
      "Iteration 169, loss = 0.05161519\n",
      "Iteration 139, loss = 0.09346696\n",
      "Iteration 170, loss = 0.05427048\n",
      "Iteration 140, loss = 0.09293247\n",
      "Iteration 171, loss = 0.05119338\n",
      "Iteration 141, loss = 0.09409653\n",
      "Iteration 172, loss = 0.05008033\n",
      "Iteration 142, loss = 0.09121834\n",
      "Iteration 173, loss = 0.04792649\n",
      "Iteration 143, loss = 0.09068089\n",
      "Iteration 174, loss = 0.04637991\n",
      "Iteration 144, loss = 0.09001060\n",
      "Iteration 175, loss = 0.04856119\n",
      "Iteration 145, loss = 0.09024495\n",
      "Iteration 176, loss = 0.05199541\n",
      "Iteration 146, loss = 0.08737738\n",
      "Iteration 177, loss = 0.05266041\n",
      "Iteration 147, loss = 0.08823487\n",
      "Iteration 178, loss = 0.05211530\n",
      "Iteration 148, loss = 0.08679115\n",
      "Iteration 179, loss = 0.05253095\n",
      "Iteration 149, loss = 0.08741609\n",
      "Iteration 180, loss = 0.04292703\n",
      "Iteration 150, loss = 0.08731481\n",
      "Iteration 181, loss = 0.04642470\n",
      "Iteration 151, loss = 0.08756920\n",
      "Iteration 182, loss = 0.04422393\n",
      "Iteration 152, loss = 0.08358798\n",
      "Iteration 183, loss = 0.04906127\n",
      "Iteration 153, loss = 0.08418862\n",
      "Iteration 184, loss = 0.04504268\n",
      "Iteration 154, loss = 0.08169130\n",
      "Iteration 185, loss = 0.04283044\n",
      "Iteration 155, loss = 0.08287236\n",
      "Iteration 186, loss = 0.04095234\n",
      "Iteration 156, loss = 0.08231785\n",
      "Iteration 187, loss = 0.04188587\n",
      "Iteration 157, loss = 0.08114361\n",
      "Iteration 188, loss = 0.04036557\n",
      "Iteration 158, loss = 0.08568164\n",
      "Iteration 189, loss = 0.04192170\n",
      "Iteration 159, loss = 0.08628167\n",
      "Iteration 190, loss = 0.03993793\n",
      "Iteration 160, loss = 0.08089643\n",
      "Iteration 191, loss = 0.04031679\n",
      "Iteration 161, loss = 0.08139442\n",
      "Iteration 192, loss = 0.03987821\n",
      "Iteration 162, loss = 0.07889071\n",
      "Iteration 193, loss = 0.04153320\n",
      "Iteration 163, loss = 0.07913954\n",
      "Iteration 194, loss = 0.03983124\n",
      "Iteration 164, loss = 0.07854398\n",
      "Iteration 195, loss = 0.03930737\n",
      "Iteration 165, loss = 0.07547468\n",
      "Iteration 196, loss = 0.04289608\n",
      "Iteration 197, loss = 0.04281589\n",
      "Iteration 166, loss = 0.07722516\n",
      "Iteration 198, loss = 0.04142710\n",
      "Iteration 167, loss = 0.07861946\n",
      "Iteration 199, loss = 0.04581212\n",
      "Iteration 168, loss = 0.07999802\n",
      "Iteration 200, loss = 0.04565565\n",
      "Iteration 169, loss = 0.08239998\n",
      "Iteration 170, loss = 0.07465287\n",
      "Iteration 171, loss = 0.07583231\n",
      "Iteration 172, loss = 0.07547768\n",
      "Iteration 173, loss = 0.07666069\n",
      "Iteration 174, loss = 0.07138160\n",
      "Iteration 175, loss = 0.07226986\n",
      "Iteration 176, loss = 0.07305824\n",
      "Iteration 177, loss = 0.06934345\n",
      "Iteration 178, loss = 0.07086774\n",
      "Iteration 179, loss = 0.06831762\n",
      "Iteration 180, loss = 0.06793405\n",
      "Iteration 181, loss = 0.06715387\n",
      "Iteration 1, loss = 1.12427599\n",
      "Iteration 182, loss = 0.06814234\n",
      "Iteration 2, loss = 0.84849336\n",
      "Iteration 183, loss = 0.06794260\n",
      "Iteration 3, loss = 0.69930496\n",
      "Iteration 184, loss = 0.06637670\n",
      "Iteration 4, loss = 0.60394973\n",
      "Iteration 185, loss = 0.06590709\n",
      "Iteration 5, loss = 0.54383210\n",
      "Iteration 186, loss = 0.06605214\n",
      "Iteration 6, loss = 0.50083934\n",
      "Iteration 187, loss = 0.06620300\n",
      "Iteration 7, loss = 0.47013177\n",
      "Iteration 188, loss = 0.06605932\n",
      "Iteration 8, loss = 0.44409359\n",
      "Iteration 189, loss = 0.06416030\n",
      "Iteration 9, loss = 0.42603221\n",
      "Iteration 190, loss = 0.06460300\n",
      "Iteration 10, loss = 0.40691567\n",
      "Iteration 191, loss = 0.06468231\n",
      "Iteration 11, loss = 0.39479781\n",
      "Iteration 192, loss = 0.06523058\n",
      "Iteration 12, loss = 0.38241541\n",
      "Iteration 193, loss = 0.06205939\n",
      "Iteration 13, loss = 0.37045998\n",
      "Iteration 194, loss = 0.06218736\n",
      "Iteration 14, loss = 0.36151026\n",
      "Iteration 195, loss = 0.06016238\n",
      "Iteration 15, loss = 0.35192526\n",
      "Iteration 196, loss = 0.05964572\n",
      "Iteration 16, loss = 0.34263839\n",
      "Iteration 197, loss = 0.05993228\n",
      "Iteration 17, loss = 0.33471570\n",
      "Iteration 198, loss = 0.05933693\n",
      "Iteration 18, loss = 0.32923411\n",
      "Iteration 199, loss = 0.06012059\n",
      "Iteration 19, loss = 0.32229500\n",
      "Iteration 200, loss = 0.06195377\n",
      "Iteration 20, loss = 0.31604148\n",
      "Iteration 21, loss = 0.30962138\n",
      "Iteration 22, loss = 0.30454999\n",
      "Iteration 23, loss = 0.30048341\n",
      "Iteration 24, loss = 0.29533744\n",
      "Iteration 25, loss = 0.28940539\n",
      "Iteration 26, loss = 0.28539837\n",
      "Iteration 27, loss = 0.28103928\n",
      "Iteration 28, loss = 0.27631991\n",
      "Iteration 29, loss = 0.27289584\n",
      "Iteration 30, loss = 0.26909245\n",
      "Iteration 31, loss = 0.26522909\n",
      "Iteration 32, loss = 0.26184773\n",
      "Iteration 1, loss = 1.57502834\n",
      "Iteration 33, loss = 0.25818321\n",
      "Iteration 2, loss = 1.08873150\n",
      "Iteration 34, loss = 0.25468405\n",
      "Iteration 3, loss = 0.81775711\n",
      "Iteration 35, loss = 0.25128929\n",
      "Iteration 4, loss = 0.67594131\n",
      "Iteration 36, loss = 0.24725607\n",
      "Iteration 5, loss = 0.59438639\n",
      "Iteration 37, loss = 0.24480708\n",
      "Iteration 6, loss = 0.54200496\n",
      "Iteration 38, loss = 0.24184550\n",
      "Iteration 7, loss = 0.50079387\n",
      "Iteration 39, loss = 0.23758022\n",
      "Iteration 8, loss = 0.47022013\n",
      "Iteration 40, loss = 0.23530663\n",
      "Iteration 9, loss = 0.44584143\n",
      "Iteration 41, loss = 0.23303801\n",
      "Iteration 10, loss = 0.42450574\n",
      "Iteration 42, loss = 0.23038279\n",
      "Iteration 11, loss = 0.40785730\n",
      "Iteration 43, loss = 0.22742243\n",
      "Iteration 12, loss = 0.39441851\n",
      "Iteration 44, loss = 0.22500188\n",
      "Iteration 13, loss = 0.38030049\n",
      "Iteration 45, loss = 0.22053415\n",
      "Iteration 14, loss = 0.36785767\n",
      "Iteration 46, loss = 0.21831857\n",
      "Iteration 15, loss = 0.35740827\n",
      "Iteration 47, loss = 0.21452636\n",
      "Iteration 16, loss = 0.35251591\n",
      "Iteration 48, loss = 0.21342084\n",
      "Iteration 17, loss = 0.34193917\n",
      "Iteration 49, loss = 0.21089373\n",
      "Iteration 18, loss = 0.33203953\n",
      "Iteration 50, loss = 0.20727006\n",
      "Iteration 19, loss = 0.32404330\n",
      "Iteration 51, loss = 0.20557848\n",
      "Iteration 20, loss = 0.31693476\n",
      "Iteration 52, loss = 0.20199901\n",
      "Iteration 21, loss = 0.31058860\n",
      "Iteration 53, loss = 0.19998262\n",
      "Iteration 22, loss = 0.30438648\n",
      "Iteration 54, loss = 0.19868554\n",
      "Iteration 23, loss = 0.30046507\n",
      "Iteration 55, loss = 0.19442060\n",
      "Iteration 24, loss = 0.29458428\n",
      "Iteration 56, loss = 0.19275047\n",
      "Iteration 25, loss = 0.28796586\n",
      "Iteration 57, loss = 0.19125650\n",
      "Iteration 26, loss = 0.28268996\n",
      "Iteration 58, loss = 0.18976325\n",
      "Iteration 27, loss = 0.27831458\n",
      "Iteration 59, loss = 0.18647864\n",
      "Iteration 28, loss = 0.27390055\n",
      "Iteration 60, loss = 0.18724469\n",
      "Iteration 29, loss = 0.26839298\n",
      "Iteration 61, loss = 0.18665330\n",
      "Iteration 30, loss = 0.26482933\n",
      "Iteration 62, loss = 0.18176120\n",
      "Iteration 31, loss = 0.25979287\n",
      "Iteration 63, loss = 0.17751627\n",
      "Iteration 32, loss = 0.25675971\n",
      "Iteration 64, loss = 0.17500288\n",
      "Iteration 33, loss = 0.25260937\n",
      "Iteration 65, loss = 0.17339674\n",
      "Iteration 34, loss = 0.24870399\n",
      "Iteration 66, loss = 0.16991287\n",
      "Iteration 35, loss = 0.24500696\n",
      "Iteration 67, loss = 0.17103917\n",
      "Iteration 36, loss = 0.24112880\n",
      "Iteration 68, loss = 0.16552272\n",
      "Iteration 37, loss = 0.23910648\n",
      "Iteration 69, loss = 0.16481345\n",
      "Iteration 38, loss = 0.23519839\n",
      "Iteration 70, loss = 0.16506645\n",
      "Iteration 39, loss = 0.23349420\n",
      "Iteration 71, loss = 0.16458073\n",
      "Iteration 40, loss = 0.22871666\n",
      "Iteration 72, loss = 0.16139688\n",
      "Iteration 41, loss = 0.22763823\n",
      "Iteration 73, loss = 0.15950580\n",
      "Iteration 42, loss = 0.22300566\n",
      "Iteration 74, loss = 0.15732843\n",
      "Iteration 43, loss = 0.22034647\n",
      "Iteration 75, loss = 0.15589272\n",
      "Iteration 44, loss = 0.21675506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.15424451\n",
      "Iteration 45, loss = 0.21423440\n",
      "Iteration 77, loss = 0.15028323\n",
      "Iteration 46, loss = 0.20887666\n",
      "Iteration 78, loss = 0.14784796\n",
      "Iteration 47, loss = 0.21159355\n",
      "Iteration 79, loss = 0.14646429\n",
      "Iteration 48, loss = 0.20348177\n",
      "Iteration 80, loss = 0.14398184\n",
      "Iteration 49, loss = 0.20466790\n",
      "Iteration 81, loss = 0.14450301\n",
      "Iteration 50, loss = 0.19844151\n",
      "Iteration 82, loss = 0.14356234\n",
      "Iteration 51, loss = 0.19872769\n",
      "Iteration 83, loss = 0.14250945\n",
      "Iteration 52, loss = 0.19260242\n",
      "Iteration 84, loss = 0.13976887\n",
      "Iteration 53, loss = 0.19191134\n",
      "Iteration 85, loss = 0.14103960\n",
      "Iteration 54, loss = 0.18841224\n",
      "Iteration 86, loss = 0.13591570\n",
      "Iteration 55, loss = 0.18894941\n",
      "Iteration 87, loss = 0.13446227\n",
      "Iteration 56, loss = 0.18413144\n",
      "Iteration 88, loss = 0.13256774\n",
      "Iteration 57, loss = 0.18271450\n",
      "Iteration 89, loss = 0.13056644\n",
      "Iteration 58, loss = 0.17944519\n",
      "Iteration 90, loss = 0.12997971\n",
      "Iteration 59, loss = 0.17876710\n",
      "Iteration 91, loss = 0.12907553\n",
      "Iteration 60, loss = 0.17754002\n",
      "Iteration 92, loss = 0.12847146\n",
      "Iteration 93, loss = 0.12607563\n",
      "Iteration 61, loss = 0.17847684\n",
      "Iteration 94, loss = 0.13099983\n",
      "Iteration 62, loss = 0.17064926\n",
      "Iteration 95, loss = 0.12465779\n",
      "Iteration 63, loss = 0.17106868\n",
      "Iteration 96, loss = 0.12381440\n",
      "Iteration 64, loss = 0.16812119\n",
      "Iteration 97, loss = 0.12210872\n",
      "Iteration 65, loss = 0.16522136\n",
      "Iteration 98, loss = 0.12037526\n",
      "Iteration 66, loss = 0.16220862\n",
      "Iteration 99, loss = 0.12191350\n",
      "Iteration 67, loss = 0.16200740\n",
      "Iteration 100, loss = 0.12300853\n",
      "Iteration 68, loss = 0.15947645\n",
      "Iteration 101, loss = 0.12098613\n",
      "Iteration 69, loss = 0.15796593\n",
      "Iteration 102, loss = 0.11558195\n",
      "Iteration 70, loss = 0.15519600\n",
      "Iteration 103, loss = 0.11465028\n",
      "Iteration 71, loss = 0.15420011\n",
      "Iteration 104, loss = 0.11410936\n",
      "Iteration 72, loss = 0.15490385\n",
      "Iteration 105, loss = 0.11273333\n",
      "Iteration 73, loss = 0.14980877\n",
      "Iteration 106, loss = 0.11048252\n",
      "Iteration 74, loss = 0.14897359\n",
      "Iteration 107, loss = 0.11031971\n",
      "Iteration 75, loss = 0.14976913\n",
      "Iteration 108, loss = 0.10837396\n",
      "Iteration 76, loss = 0.14692739\n",
      "Iteration 109, loss = 0.10838391\n",
      "Iteration 77, loss = 0.14415955\n",
      "Iteration 110, loss = 0.10650689\n",
      "Iteration 78, loss = 0.14164394\n",
      "Iteration 111, loss = 0.10678950\n",
      "Iteration 79, loss = 0.14220278\n",
      "Iteration 112, loss = 0.10623627\n",
      "Iteration 80, loss = 0.14079183\n",
      "Iteration 113, loss = 0.10374863\n",
      "Iteration 81, loss = 0.13732433\n",
      "Iteration 114, loss = 0.10327663\n",
      "Iteration 82, loss = 0.13571387\n",
      "Iteration 115, loss = 0.10417470\n",
      "Iteration 83, loss = 0.13397920\n",
      "Iteration 116, loss = 0.10200143\n",
      "Iteration 84, loss = 0.13410342\n",
      "Iteration 117, loss = 0.09989998\n",
      "Iteration 85, loss = 0.13290076\n",
      "Iteration 118, loss = 0.09898171\n",
      "Iteration 86, loss = 0.13082187\n",
      "Iteration 119, loss = 0.10380598\n",
      "Iteration 87, loss = 0.13070276\n",
      "Iteration 120, loss = 0.09928789\n",
      "Iteration 88, loss = 0.12810133\n",
      "Iteration 121, loss = 0.09752811\n",
      "Iteration 89, loss = 0.12859179\n",
      "Iteration 122, loss = 0.09609877\n",
      "Iteration 90, loss = 0.13063585\n",
      "Iteration 123, loss = 0.09541508\n",
      "Iteration 91, loss = 0.12653171\n",
      "Iteration 124, loss = 0.09535505\n",
      "Iteration 92, loss = 0.12484493\n",
      "Iteration 125, loss = 0.09424087\n",
      "Iteration 93, loss = 0.12582414\n",
      "Iteration 126, loss = 0.09573329\n",
      "Iteration 94, loss = 0.12653377\n",
      "Iteration 127, loss = 0.09864046\n",
      "Iteration 95, loss = 0.11965348\n",
      "Iteration 128, loss = 0.09240796\n",
      "Iteration 96, loss = 0.12064725\n",
      "Iteration 129, loss = 0.09168208\n",
      "Iteration 97, loss = 0.11708878\n",
      "Iteration 130, loss = 0.09381726\n",
      "Iteration 98, loss = 0.11754774\n",
      "Iteration 131, loss = 0.09376195\n",
      "Iteration 99, loss = 0.11645195\n",
      "Iteration 132, loss = 0.09460920\n",
      "Iteration 100, loss = 0.11387785\n",
      "Iteration 133, loss = 0.09131240\n",
      "Iteration 101, loss = 0.11393981\n",
      "Iteration 134, loss = 0.08555411\n",
      "Iteration 102, loss = 0.11122456\n",
      "Iteration 135, loss = 0.08714161\n",
      "Iteration 103, loss = 0.11070133\n",
      "Iteration 136, loss = 0.08612801\n",
      "Iteration 104, loss = 0.10873743\n",
      "Iteration 137, loss = 0.08382021\n",
      "Iteration 105, loss = 0.10746482\n",
      "Iteration 138, loss = 0.08221173\n",
      "Iteration 106, loss = 0.10759494\n",
      "Iteration 139, loss = 0.08361050\n",
      "Iteration 107, loss = 0.10563536\n",
      "Iteration 140, loss = 0.08255008\n",
      "Iteration 108, loss = 0.10419034\n",
      "Iteration 141, loss = 0.07957862\n",
      "Iteration 109, loss = 0.10413451\n",
      "Iteration 142, loss = 0.08058394\n",
      "Iteration 110, loss = 0.10436255\n",
      "Iteration 143, loss = 0.08046853\n",
      "Iteration 111, loss = 0.10514759\n",
      "Iteration 144, loss = 0.07949727\n",
      "Iteration 112, loss = 0.10322393\n",
      "Iteration 145, loss = 0.07700167\n",
      "Iteration 113, loss = 0.10220645\n",
      "Iteration 146, loss = 0.07794190\n",
      "Iteration 114, loss = 0.10180214\n",
      "Iteration 147, loss = 0.07571736\n",
      "Iteration 115, loss = 0.10249831\n",
      "Iteration 148, loss = 0.07433876\n",
      "Iteration 116, loss = 0.10266022\n",
      "Iteration 149, loss = 0.07495667\n",
      "Iteration 117, loss = 0.09805658\n",
      "Iteration 150, loss = 0.07562106\n",
      "Iteration 118, loss = 0.09699286\n",
      "Iteration 151, loss = 0.07356409\n",
      "Iteration 119, loss = 0.09575294\n",
      "Iteration 152, loss = 0.07211953\n",
      "Iteration 120, loss = 0.09409953\n",
      "Iteration 153, loss = 0.07135817\n",
      "Iteration 121, loss = 0.09428987\n",
      "Iteration 154, loss = 0.07143781\n",
      "Iteration 122, loss = 0.09149259\n",
      "Iteration 155, loss = 0.07468064\n",
      "Iteration 123, loss = 0.09016880\n",
      "Iteration 156, loss = 0.07136112\n",
      "Iteration 124, loss = 0.08952000\n",
      "Iteration 157, loss = 0.07056315\n",
      "Iteration 125, loss = 0.08936342\n",
      "Iteration 158, loss = 0.07044417\n",
      "Iteration 126, loss = 0.08887873\n",
      "Iteration 159, loss = 0.06961809\n",
      "Iteration 127, loss = 0.08753210\n",
      "Iteration 160, loss = 0.06868538\n",
      "Iteration 128, loss = 0.08630648\n",
      "Iteration 161, loss = 0.06758850\n",
      "Iteration 129, loss = 0.08841137\n",
      "Iteration 162, loss = 0.06816290\n",
      "Iteration 130, loss = 0.08911207\n",
      "Iteration 163, loss = 0.06446311\n",
      "Iteration 131, loss = 0.08560680\n",
      "Iteration 164, loss = 0.06550244\n",
      "Iteration 132, loss = 0.08408557\n",
      "Iteration 165, loss = 0.06422939\n",
      "Iteration 133, loss = 0.08276767\n",
      "Iteration 166, loss = 0.06302995\n",
      "Iteration 134, loss = 0.08256269\n",
      "Iteration 167, loss = 0.06422123\n",
      "Iteration 135, loss = 0.08304805\n",
      "Iteration 168, loss = 0.06201718\n",
      "Iteration 136, loss = 0.08191917\n",
      "Iteration 169, loss = 0.06086980\n",
      "Iteration 137, loss = 0.08115974\n",
      "Iteration 170, loss = 0.06271743\n",
      "Iteration 138, loss = 0.07927691\n",
      "Iteration 171, loss = 0.06134381\n",
      "Iteration 139, loss = 0.07778728\n",
      "Iteration 172, loss = 0.06144882\n",
      "Iteration 140, loss = 0.07693130\n",
      "Iteration 173, loss = 0.05956623\n",
      "Iteration 141, loss = 0.07913100\n",
      "Iteration 174, loss = 0.05840869\n",
      "Iteration 142, loss = 0.08026931\n",
      "Iteration 175, loss = 0.05916638\n",
      "Iteration 143, loss = 0.08405062\n",
      "Iteration 176, loss = 0.05865161\n",
      "Iteration 144, loss = 0.08264016\n",
      "Iteration 177, loss = 0.05778211\n",
      "Iteration 145, loss = 0.09026334\n",
      "Iteration 178, loss = 0.05764851\n",
      "Iteration 146, loss = 0.09044828\n",
      "Iteration 179, loss = 0.05723720\n",
      "Iteration 147, loss = 0.08563514\n",
      "Iteration 180, loss = 0.05705109\n",
      "Iteration 148, loss = 0.07907759\n",
      "Iteration 181, loss = 0.05442765\n",
      "Iteration 182, loss = 0.05694665\n",
      "Iteration 149, loss = 0.07571838\n",
      "Iteration 183, loss = 0.05873871\n",
      "Iteration 150, loss = 0.07733499\n",
      "Iteration 184, loss = 0.06021838\n",
      "Iteration 151, loss = 0.07354603\n",
      "Iteration 185, loss = 0.05535514\n",
      "Iteration 152, loss = 0.07137894\n",
      "Iteration 153, loss = 0.07229600\n",
      "Iteration 186, loss = 0.05472222\n",
      "Iteration 154, loss = 0.07097647\n",
      "Iteration 187, loss = 0.05314375\n",
      "Iteration 155, loss = 0.06782519\n",
      "Iteration 188, loss = 0.05173412\n",
      "Iteration 156, loss = 0.07160185\n",
      "Iteration 189, loss = 0.05062875\n",
      "Iteration 190, loss = 0.05062375\n",
      "Iteration 157, loss = 0.07212180\n",
      "Iteration 191, loss = 0.04969070\n",
      "Iteration 158, loss = 0.06954437\n",
      "Iteration 192, loss = 0.05019117\n",
      "Iteration 159, loss = 0.06590794\n",
      "Iteration 193, loss = 0.05278218\n",
      "Iteration 160, loss = 0.06674415\n",
      "Iteration 194, loss = 0.05309449\n",
      "Iteration 161, loss = 0.06514444\n",
      "Iteration 195, loss = 0.05060636\n",
      "Iteration 162, loss = 0.06566789\n",
      "Iteration 196, loss = 0.04764868\n",
      "Iteration 163, loss = 0.06324599\n",
      "Iteration 197, loss = 0.04941461\n",
      "Iteration 164, loss = 0.06371775\n",
      "Iteration 198, loss = 0.04739396\n",
      "Iteration 165, loss = 0.06368920\n",
      "Iteration 199, loss = 0.04746442\n",
      "Iteration 166, loss = 0.06122443\n",
      "Iteration 200, loss = 0.04556081\n",
      "Iteration 167, loss = 0.06342053\n",
      "Iteration 168, loss = 0.06221406\n",
      "Iteration 169, loss = 0.06736952\n",
      "Iteration 170, loss = 0.06629224\n",
      "Iteration 171, loss = 0.06759179\n",
      "Iteration 172, loss = 0.06509271\n",
      "Iteration 173, loss = 0.05968778\n",
      "Iteration 174, loss = 0.05957471\n",
      "Iteration 175, loss = 0.05886537\n",
      "Iteration 176, loss = 0.05918203\n",
      "Iteration 177, loss = 0.06019783\n",
      "Iteration 178, loss = 0.06213037\n",
      "Iteration 179, loss = 0.06014181\n",
      "Iteration 1, loss = 1.36085065\n",
      "Iteration 180, loss = 0.06043678\n",
      "Iteration 2, loss = 0.90724284\n",
      "Iteration 181, loss = 0.05775852\n",
      "Iteration 3, loss = 0.77065167\n",
      "Iteration 182, loss = 0.05700841\n",
      "Iteration 4, loss = 0.65827071\n",
      "Iteration 183, loss = 0.06288963\n",
      "Iteration 5, loss = 0.57946086\n",
      "Iteration 184, loss = 0.05939718\n",
      "Iteration 6, loss = 0.53296031\n",
      "Iteration 185, loss = 0.05196115\n",
      "Iteration 7, loss = 0.49894276\n",
      "Iteration 186, loss = 0.05244051\n",
      "Iteration 8, loss = 0.46820062\n",
      "Iteration 187, loss = 0.05634616\n",
      "Iteration 9, loss = 0.44740850\n",
      "Iteration 188, loss = 0.05567069\n",
      "Iteration 10, loss = 0.42881828\n",
      "Iteration 189, loss = 0.05851706\n",
      "Iteration 11, loss = 0.41163324\n",
      "Iteration 190, loss = 0.05518212\n",
      "Iteration 12, loss = 0.39816325\n",
      "Iteration 191, loss = 0.05529171\n",
      "Iteration 13, loss = 0.38681146\n",
      "Iteration 192, loss = 0.05417550\n",
      "Iteration 14, loss = 0.37505735\n",
      "Iteration 193, loss = 0.05453046\n",
      "Iteration 15, loss = 0.36480546\n",
      "Iteration 194, loss = 0.04911166\n",
      "Iteration 16, loss = 0.35481552\n",
      "Iteration 195, loss = 0.04897243\n",
      "Iteration 17, loss = 0.34844180\n",
      "Iteration 196, loss = 0.04892113\n",
      "Iteration 18, loss = 0.34048155\n",
      "Iteration 197, loss = 0.05002003\n",
      "Iteration 19, loss = 0.33387724\n",
      "Iteration 198, loss = 0.04670783\n",
      "Iteration 20, loss = 0.32710309\n",
      "Iteration 199, loss = 0.04611460\n",
      "Iteration 21, loss = 0.31875701\n",
      "Iteration 200, loss = 0.04817435\n",
      "Iteration 22, loss = 0.31563187\n",
      "Iteration 23, loss = 0.30792955\n",
      "Iteration 24, loss = 0.30356931\n",
      "Iteration 25, loss = 0.29749761\n",
      "Iteration 26, loss = 0.29281761\n",
      "Iteration 27, loss = 0.28785619\n",
      "Iteration 28, loss = 0.28390900\n",
      "Iteration 29, loss = 0.27957159\n",
      "Iteration 30, loss = 0.27543693\n",
      "Iteration 31, loss = 0.27124814\n",
      "Iteration 32, loss = 0.26783798\n",
      "Iteration 33, loss = 0.26425691\n",
      "Iteration 34, loss = 0.25987513\n",
      "Iteration 1, loss = 1.44137121\n",
      "Iteration 35, loss = 0.25656212\n",
      "Iteration 2, loss = 0.94748934\n",
      "Iteration 36, loss = 0.25381128\n",
      "Iteration 3, loss = 0.76249640\n",
      "Iteration 37, loss = 0.24961643\n",
      "Iteration 4, loss = 0.61666450\n",
      "Iteration 38, loss = 0.24643017\n",
      "Iteration 5, loss = 0.54830819\n",
      "Iteration 39, loss = 0.24358077\n",
      "Iteration 6, loss = 0.50867011\n",
      "Iteration 40, loss = 0.24049018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.47083924\n",
      "Iteration 41, loss = 0.23833191\n",
      "Iteration 8, loss = 0.44701649\n",
      "Iteration 42, loss = 0.23448530\n",
      "Iteration 9, loss = 0.42615719\n",
      "Iteration 43, loss = 0.23247530\n",
      "Iteration 10, loss = 0.40920829\n",
      "Iteration 44, loss = 0.22815216\n",
      "Iteration 11, loss = 0.39533463\n",
      "Iteration 45, loss = 0.22591155\n",
      "Iteration 12, loss = 0.38258885\n",
      "Iteration 46, loss = 0.22386632\n",
      "Iteration 13, loss = 0.37100399\n",
      "Iteration 47, loss = 0.22025370\n",
      "Iteration 14, loss = 0.36065057\n",
      "Iteration 48, loss = 0.21852259\n",
      "Iteration 15, loss = 0.35150528\n",
      "Iteration 49, loss = 0.21584252\n",
      "Iteration 16, loss = 0.34321439\n",
      "Iteration 50, loss = 0.21398588\n",
      "Iteration 17, loss = 0.33426299\n",
      "Iteration 51, loss = 0.21034269\n",
      "Iteration 18, loss = 0.32680334\n",
      "Iteration 52, loss = 0.20934576\n",
      "Iteration 19, loss = 0.31935160\n",
      "Iteration 53, loss = 0.20650392\n",
      "Iteration 20, loss = 0.31272844\n",
      "Iteration 54, loss = 0.20327990\n",
      "Iteration 21, loss = 0.30634036\n",
      "Iteration 55, loss = 0.20163410\n",
      "Iteration 22, loss = 0.29991182\n",
      "Iteration 56, loss = 0.19854883\n",
      "Iteration 23, loss = 0.29345088\n",
      "Iteration 57, loss = 0.19737588\n",
      "Iteration 24, loss = 0.28860985\n",
      "Iteration 58, loss = 0.19343765\n",
      "Iteration 25, loss = 0.28271232\n",
      "Iteration 59, loss = 0.19237333\n",
      "Iteration 26, loss = 0.27838197\n",
      "Iteration 60, loss = 0.18876379\n",
      "Iteration 61, loss = 0.18838802\n",
      "Iteration 27, loss = 0.27186827\n",
      "Iteration 62, loss = 0.18522027\n",
      "Iteration 28, loss = 0.26831190\n",
      "Iteration 63, loss = 0.18469738\n",
      "Iteration 29, loss = 0.26322101\n",
      "Iteration 64, loss = 0.18073743\n",
      "Iteration 30, loss = 0.25854153\n",
      "Iteration 65, loss = 0.17911875\n",
      "Iteration 31, loss = 0.25455459\n",
      "Iteration 66, loss = 0.17697995\n",
      "Iteration 32, loss = 0.24939856\n",
      "Iteration 67, loss = 0.17637534\n",
      "Iteration 33, loss = 0.25109324\n",
      "Iteration 68, loss = 0.17346123\n",
      "Iteration 34, loss = 0.24259569\n",
      "Iteration 69, loss = 0.17274010\n",
      "Iteration 35, loss = 0.23890369\n",
      "Iteration 70, loss = 0.16867922\n",
      "Iteration 36, loss = 0.23364914\n",
      "Iteration 71, loss = 0.16838853\n",
      "Iteration 37, loss = 0.23055486\n",
      "Iteration 72, loss = 0.17146072\n",
      "Iteration 38, loss = 0.22586669\n",
      "Iteration 73, loss = 0.16529165\n",
      "Iteration 39, loss = 0.22242611\n",
      "Iteration 74, loss = 0.16440466\n",
      "Iteration 40, loss = 0.21794494\n",
      "Iteration 75, loss = 0.16041625\n",
      "Iteration 41, loss = 0.21587128\n",
      "Iteration 76, loss = 0.15811219\n",
      "Iteration 42, loss = 0.21103510\n",
      "Iteration 77, loss = 0.15617385\n",
      "Iteration 43, loss = 0.20905061\n",
      "Iteration 78, loss = 0.15469468\n",
      "Iteration 44, loss = 0.20592790\n",
      "Iteration 79, loss = 0.15339313\n",
      "Iteration 45, loss = 0.20076037\n",
      "Iteration 80, loss = 0.15297688\n",
      "Iteration 46, loss = 0.19954123\n",
      "Iteration 81, loss = 0.15132726\n",
      "Iteration 47, loss = 0.19421403\n",
      "Iteration 82, loss = 0.14827471\n",
      "Iteration 48, loss = 0.19454032\n",
      "Iteration 83, loss = 0.14726632\n",
      "Iteration 49, loss = 0.18911249\n",
      "Iteration 84, loss = 0.14613063\n",
      "Iteration 50, loss = 0.18661624\n",
      "Iteration 85, loss = 0.14497581\n",
      "Iteration 51, loss = 0.18234410\n",
      "Iteration 86, loss = 0.14265096\n",
      "Iteration 52, loss = 0.18131694\n",
      "Iteration 87, loss = 0.14315865\n",
      "Iteration 53, loss = 0.17659121\n",
      "Iteration 88, loss = 0.14340107\n",
      "Iteration 54, loss = 0.17671838\n",
      "Iteration 89, loss = 0.13664088\n",
      "Iteration 55, loss = 0.17265103\n",
      "Iteration 90, loss = 0.14097354\n",
      "Iteration 56, loss = 0.17214996\n",
      "Iteration 91, loss = 0.14060646\n",
      "Iteration 57, loss = 0.17129866\n",
      "Iteration 92, loss = 0.13481985\n",
      "Iteration 58, loss = 0.16538866\n",
      "Iteration 93, loss = 0.13509798\n",
      "Iteration 59, loss = 0.16404179\n",
      "Iteration 94, loss = 0.13102463\n",
      "Iteration 60, loss = 0.15875418\n",
      "Iteration 95, loss = 0.13414994\n",
      "Iteration 61, loss = 0.15651909\n",
      "Iteration 96, loss = 0.13317282\n",
      "Iteration 62, loss = 0.15375179\n",
      "Iteration 97, loss = 0.12803401\n",
      "Iteration 63, loss = 0.15204553\n",
      "Iteration 98, loss = 0.12908137\n",
      "Iteration 64, loss = 0.14994441\n",
      "Iteration 99, loss = 0.12707181\n",
      "Iteration 65, loss = 0.14725317\n",
      "Iteration 100, loss = 0.12655999\n",
      "Iteration 66, loss = 0.14548263\n",
      "Iteration 101, loss = 0.12561654\n",
      "Iteration 67, loss = 0.14124137\n",
      "Iteration 102, loss = 0.12514178\n",
      "Iteration 68, loss = 0.14110013\n",
      "Iteration 103, loss = 0.12255026\n",
      "Iteration 69, loss = 0.13795448\n",
      "Iteration 104, loss = 0.12137996\n",
      "Iteration 70, loss = 0.13596730\n",
      "Iteration 105, loss = 0.11935032\n",
      "Iteration 71, loss = 0.13440906\n",
      "Iteration 106, loss = 0.11818840\n",
      "Iteration 72, loss = 0.13042538\n",
      "Iteration 107, loss = 0.11603578\n",
      "Iteration 73, loss = 0.13066793\n",
      "Iteration 108, loss = 0.11558260\n",
      "Iteration 74, loss = 0.12727279\n",
      "Iteration 109, loss = 0.11440742\n",
      "Iteration 75, loss = 0.12700437\n",
      "Iteration 110, loss = 0.11299577\n",
      "Iteration 76, loss = 0.12475363\n",
      "Iteration 111, loss = 0.11247125\n",
      "Iteration 77, loss = 0.12275356\n",
      "Iteration 112, loss = 0.11302626\n",
      "Iteration 78, loss = 0.12267086\n",
      "Iteration 113, loss = 0.11020424\n",
      "Iteration 79, loss = 0.11954586\n",
      "Iteration 114, loss = 0.10995149\n",
      "Iteration 80, loss = 0.12116642\n",
      "Iteration 115, loss = 0.11040542\n",
      "Iteration 81, loss = 0.12383545\n",
      "Iteration 116, loss = 0.10742586\n",
      "Iteration 82, loss = 0.11439938\n",
      "Iteration 117, loss = 0.10708808\n",
      "Iteration 83, loss = 0.11641134\n",
      "Iteration 118, loss = 0.10805625\n",
      "Iteration 84, loss = 0.11340410\n",
      "Iteration 119, loss = 0.11217787\n",
      "Iteration 85, loss = 0.11334135\n",
      "Iteration 120, loss = 0.11038971\n",
      "Iteration 86, loss = 0.10818787\n",
      "Iteration 121, loss = 0.10878496\n",
      "Iteration 87, loss = 0.10958898\n",
      "Iteration 122, loss = 0.10603036\n",
      "Iteration 88, loss = 0.10898983\n",
      "Iteration 123, loss = 0.10288103\n",
      "Iteration 89, loss = 0.10545436\n",
      "Iteration 124, loss = 0.10253173\n",
      "Iteration 90, loss = 0.10564148\n",
      "Iteration 125, loss = 0.10612265\n",
      "Iteration 91, loss = 0.10285143\n",
      "Iteration 126, loss = 0.10706527\n",
      "Iteration 92, loss = 0.10367484\n",
      "Iteration 127, loss = 0.10034636\n",
      "Iteration 93, loss = 0.10153909\n",
      "Iteration 128, loss = 0.10219821\n",
      "Iteration 94, loss = 0.09927292\n",
      "Iteration 129, loss = 0.10140074\n",
      "Iteration 95, loss = 0.09641921\n",
      "Iteration 130, loss = 0.09757482\n",
      "Iteration 96, loss = 0.09998028\n",
      "Iteration 131, loss = 0.09778638\n",
      "Iteration 97, loss = 0.09477236\n",
      "Iteration 132, loss = 0.09453954\n",
      "Iteration 98, loss = 0.09636366\n",
      "Iteration 133, loss = 0.09484780\n",
      "Iteration 99, loss = 0.09279649\n",
      "Iteration 134, loss = 0.09239843\n",
      "Iteration 100, loss = 0.09162874\n",
      "Iteration 135, loss = 0.09514180\n",
      "Iteration 101, loss = 0.09009382\n",
      "Iteration 136, loss = 0.09061240\n",
      "Iteration 102, loss = 0.08837898\n",
      "Iteration 137, loss = 0.09372028\n",
      "Iteration 103, loss = 0.08851678\n",
      "Iteration 138, loss = 0.09305458\n",
      "Iteration 104, loss = 0.08738632\n",
      "Iteration 139, loss = 0.09073814\n",
      "Iteration 105, loss = 0.08665796\n",
      "Iteration 140, loss = 0.09569919\n",
      "Iteration 106, loss = 0.08440828\n",
      "Iteration 141, loss = 0.09847983\n",
      "Iteration 107, loss = 0.08447927\n",
      "Iteration 142, loss = 0.08929976\n",
      "Iteration 108, loss = 0.08263701\n",
      "Iteration 143, loss = 0.09089777\n",
      "Iteration 109, loss = 0.08142444\n",
      "Iteration 144, loss = 0.08845593\n",
      "Iteration 110, loss = 0.08042096\n",
      "Iteration 111, loss = 0.07940839\n",
      "Iteration 112, loss = 0.07796165\n",
      "Iteration 145, loss = 0.08799273\n",
      "Iteration 146, loss = 0.08666267\n",
      "Iteration 113, loss = 0.07989185\n",
      "Iteration 147, loss = 0.08395536\n",
      "Iteration 114, loss = 0.07643298\n",
      "Iteration 148, loss = 0.08266186\n",
      "Iteration 115, loss = 0.07636275\n",
      "Iteration 149, loss = 0.08291961\n",
      "Iteration 116, loss = 0.07505898\n",
      "Iteration 150, loss = 0.08203083\n",
      "Iteration 117, loss = 0.07570810\n",
      "Iteration 151, loss = 0.08176428\n",
      "Iteration 118, loss = 0.07495558\n",
      "Iteration 152, loss = 0.08511662\n",
      "Iteration 119, loss = 0.07161836\n",
      "Iteration 153, loss = 0.07933111\n",
      "Iteration 120, loss = 0.07206902\n",
      "Iteration 154, loss = 0.08026196\n",
      "Iteration 155, loss = 0.07918230\n",
      "Iteration 121, loss = 0.07222641\n",
      "Iteration 156, loss = 0.07906999\n",
      "Iteration 122, loss = 0.07051993\n",
      "Iteration 123, loss = 0.06851876\n",
      "Iteration 157, loss = 0.07757337\n",
      "Iteration 124, loss = 0.06977289\n",
      "Iteration 158, loss = 0.07726494\n",
      "Iteration 125, loss = 0.06748203\n",
      "Iteration 159, loss = 0.07660416\n",
      "Iteration 126, loss = 0.06841305\n",
      "Iteration 160, loss = 0.07550835\n",
      "Iteration 127, loss = 0.06591634\n",
      "Iteration 161, loss = 0.07618415\n",
      "Iteration 128, loss = 0.06530583\n",
      "Iteration 162, loss = 0.07494784\n",
      "Iteration 129, loss = 0.06539417\n",
      "Iteration 163, loss = 0.07445861\n",
      "Iteration 130, loss = 0.06854872\n",
      "Iteration 164, loss = 0.07453342\n",
      "Iteration 131, loss = 0.06456446\n",
      "Iteration 165, loss = 0.07247292\n",
      "Iteration 132, loss = 0.06618147\n",
      "Iteration 166, loss = 0.07328837\n",
      "Iteration 133, loss = 0.06130736\n",
      "Iteration 167, loss = 0.07247293\n",
      "Iteration 134, loss = 0.06224898\n",
      "Iteration 168, loss = 0.07074345\n",
      "Iteration 135, loss = 0.06216802\n",
      "Iteration 169, loss = 0.07227074\n",
      "Iteration 136, loss = 0.06044813\n",
      "Iteration 170, loss = 0.07251008\n",
      "Iteration 137, loss = 0.05894774\n",
      "Iteration 171, loss = 0.07352916\n",
      "Iteration 138, loss = 0.05819778\n",
      "Iteration 172, loss = 0.06834999\n",
      "Iteration 139, loss = 0.05740296\n",
      "Iteration 173, loss = 0.06846850\n",
      "Iteration 140, loss = 0.05673350\n",
      "Iteration 174, loss = 0.06816112\n",
      "Iteration 141, loss = 0.05643601\n",
      "Iteration 175, loss = 0.06691497\n",
      "Iteration 142, loss = 0.05521369\n",
      "Iteration 176, loss = 0.06605726\n",
      "Iteration 143, loss = 0.05600410\n",
      "Iteration 177, loss = 0.06557009\n",
      "Iteration 144, loss = 0.05407671\n",
      "Iteration 178, loss = 0.06549676\n",
      "Iteration 145, loss = 0.05446690\n",
      "Iteration 179, loss = 0.06534655\n",
      "Iteration 146, loss = 0.05339047\n",
      "Iteration 180, loss = 0.06659069\n",
      "Iteration 147, loss = 0.05338910\n",
      "Iteration 181, loss = 0.06668430\n",
      "Iteration 148, loss = 0.05310544\n",
      "Iteration 182, loss = 0.06622928\n",
      "Iteration 149, loss = 0.05244472\n",
      "Iteration 183, loss = 0.06839832\n",
      "Iteration 150, loss = 0.05144181\n",
      "Iteration 184, loss = 0.06331450\n",
      "Iteration 151, loss = 0.05151623\n",
      "Iteration 185, loss = 0.06253844\n",
      "Iteration 152, loss = 0.05001331\n",
      "Iteration 186, loss = 0.06266891\n",
      "Iteration 153, loss = 0.04993934\n",
      "Iteration 187, loss = 0.06575017\n",
      "Iteration 154, loss = 0.04989390\n",
      "Iteration 188, loss = 0.06429142\n",
      "Iteration 155, loss = 0.04930250\n",
      "Iteration 189, loss = 0.06018794\n",
      "Iteration 156, loss = 0.04744263\n",
      "Iteration 190, loss = 0.06296025\n",
      "Iteration 157, loss = 0.04902873\n",
      "Iteration 191, loss = 0.06075540\n",
      "Iteration 158, loss = 0.04847679\n",
      "Iteration 192, loss = 0.05922303\n",
      "Iteration 159, loss = 0.04941547\n",
      "Iteration 193, loss = 0.06030551\n",
      "Iteration 160, loss = 0.04734602\n",
      "Iteration 194, loss = 0.05657717\n",
      "Iteration 161, loss = 0.04781381\n",
      "Iteration 195, loss = 0.05814609\n",
      "Iteration 162, loss = 0.04828237\n",
      "Iteration 196, loss = 0.06003499\n",
      "Iteration 163, loss = 0.04636895\n",
      "Iteration 197, loss = 0.05988461\n",
      "Iteration 164, loss = 0.04661961\n",
      "Iteration 198, loss = 0.06042166\n",
      "Iteration 165, loss = 0.04400411\n",
      "Iteration 199, loss = 0.05856685\n",
      "Iteration 166, loss = 0.04984792\n",
      "Iteration 200, loss = 0.05775305\n",
      "Iteration 167, loss = 0.05174703\n",
      "Iteration 168, loss = 0.04519958\n",
      "Iteration 169, loss = 0.04659907\n",
      "Iteration 170, loss = 0.04500315\n",
      "Iteration 171, loss = 0.04480434\n",
      "Iteration 172, loss = 0.04230957\n",
      "Iteration 173, loss = 0.04281619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 174, loss = 0.04343505\n",
      "Iteration 175, loss = 0.04154491\n",
      "Iteration 176, loss = 0.04324620\n",
      "Iteration 177, loss = 0.03947442\n",
      "Iteration 178, loss = 0.03713267\n",
      "Iteration 179, loss = 0.03935348\n",
      "Iteration 1, loss = 1.24273541\n",
      "Iteration 180, loss = 0.04101168\n",
      "Iteration 2, loss = 0.88781740\n",
      "Iteration 181, loss = 0.03823028\n",
      "Iteration 3, loss = 0.71064791\n",
      "Iteration 4, loss = 0.60489946\n",
      "Iteration 182, loss = 0.03764623\n",
      "Iteration 5, loss = 0.53862188\n",
      "Iteration 183, loss = 0.03685548\n",
      "Iteration 184, loss = 0.03708340\n",
      "Iteration 6, loss = 0.49186502\n",
      "Iteration 185, loss = 0.03491269\n",
      "Iteration 7, loss = 0.45596678\n",
      "Iteration 8, loss = 0.42811458\n",
      "Iteration 186, loss = 0.03528905\n",
      "Iteration 9, loss = 0.40654803\n",
      "Iteration 187, loss = 0.03418281\n",
      "Iteration 10, loss = 0.38781810\n",
      "Iteration 188, loss = 0.03414099\n",
      "Iteration 11, loss = 0.37234239\n",
      "Iteration 189, loss = 0.03351961\n",
      "Iteration 12, loss = 0.35932572\n",
      "Iteration 190, loss = 0.03405005\n",
      "Iteration 13, loss = 0.34584727\n",
      "Iteration 191, loss = 0.03398444\n",
      "Iteration 14, loss = 0.33514884\n",
      "Iteration 192, loss = 0.03555043\n",
      "Iteration 15, loss = 0.32512035\n",
      "Iteration 193, loss = 0.03714849\n",
      "Iteration 16, loss = 0.31836429\n",
      "Iteration 194, loss = 0.03236076\n",
      "Iteration 17, loss = 0.30869450\n",
      "Iteration 195, loss = 0.03339671\n",
      "Iteration 18, loss = 0.30264580\n",
      "Iteration 196, loss = 0.03301537\n",
      "Iteration 19, loss = 0.29361306\n",
      "Iteration 197, loss = 0.03112138\n",
      "Iteration 20, loss = 0.28794966\n",
      "Iteration 198, loss = 0.03068111\n",
      "Iteration 21, loss = 0.28245088\n",
      "Iteration 199, loss = 0.03063337\n",
      "Iteration 22, loss = 0.27609150\n",
      "Iteration 200, loss = 0.02976828\n",
      "Iteration 23, loss = 0.26916998\n",
      "Iteration 24, loss = 0.26423970\n",
      "Iteration 25, loss = 0.25900648\n",
      "Iteration 26, loss = 0.25408910\n",
      "Iteration 27, loss = 0.24949048\n",
      "Iteration 28, loss = 0.24530652\n",
      "Iteration 29, loss = 0.24106418\n",
      "Iteration 30, loss = 0.23654472\n",
      "Iteration 31, loss = 0.23319000\n",
      "Iteration 32, loss = 0.22954861\n",
      "Iteration 33, loss = 0.22567876\n",
      "Iteration 34, loss = 0.22088660\n",
      "Iteration 35, loss = 0.21788169\n",
      "Iteration 36, loss = 0.21571114\n",
      "Iteration 1, loss = 1.11023557\n",
      "Iteration 37, loss = 0.21081731\n",
      "Iteration 2, loss = 0.84359282\n",
      "Iteration 38, loss = 0.20794344\n",
      "Iteration 3, loss = 0.68602439\n",
      "Iteration 39, loss = 0.20361683\n",
      "Iteration 4, loss = 0.59914716\n",
      "Iteration 40, loss = 0.20106670\n",
      "Iteration 5, loss = 0.54038932\n",
      "Iteration 41, loss = 0.19743764\n",
      "Iteration 6, loss = 0.49774891\n",
      "Iteration 42, loss = 0.19427595\n",
      "Iteration 7, loss = 0.46893399\n",
      "Iteration 43, loss = 0.19191566\n",
      "Iteration 8, loss = 0.44312182\n",
      "Iteration 44, loss = 0.18838683\n",
      "Iteration 9, loss = 0.42356770\n",
      "Iteration 45, loss = 0.18674050\n",
      "Iteration 10, loss = 0.40647452\n",
      "Iteration 46, loss = 0.18471114\n",
      "Iteration 11, loss = 0.39245954\n",
      "Iteration 47, loss = 0.18124731\n",
      "Iteration 12, loss = 0.37893634\n",
      "Iteration 48, loss = 0.17923325\n",
      "Iteration 13, loss = 0.36814165\n",
      "Iteration 49, loss = 0.17556980\n",
      "Iteration 14, loss = 0.35698026\n",
      "Iteration 50, loss = 0.17283140\n",
      "Iteration 15, loss = 0.34873652\n",
      "Iteration 51, loss = 0.17160520\n",
      "Iteration 16, loss = 0.34013111\n",
      "Iteration 52, loss = 0.16918588\n",
      "Iteration 17, loss = 0.33259161\n",
      "Iteration 53, loss = 0.16699836\n",
      "Iteration 18, loss = 0.32459382\n",
      "Iteration 54, loss = 0.16636350\n",
      "Iteration 19, loss = 0.31727091\n",
      "Iteration 55, loss = 0.16358140\n",
      "Iteration 20, loss = 0.31138974\n",
      "Iteration 56, loss = 0.15898459\n",
      "Iteration 21, loss = 0.30594573\n",
      "Iteration 57, loss = 0.15769747\n",
      "Iteration 22, loss = 0.29905119\n",
      "Iteration 58, loss = 0.15559306\n",
      "Iteration 23, loss = 0.29486228\n",
      "Iteration 59, loss = 0.15300994\n",
      "Iteration 24, loss = 0.28872372\n",
      "Iteration 60, loss = 0.15205995\n",
      "Iteration 25, loss = 0.28727527\n",
      "Iteration 61, loss = 0.15686237\n",
      "Iteration 26, loss = 0.27916002\n",
      "Iteration 62, loss = 0.15454162\n",
      "Iteration 27, loss = 0.27452476\n",
      "Iteration 63, loss = 0.14680017\n",
      "Iteration 28, loss = 0.26860965\n",
      "Iteration 64, loss = 0.14686856\n",
      "Iteration 29, loss = 0.26423666\n",
      "Iteration 65, loss = 0.14476588\n",
      "Iteration 30, loss = 0.26098120\n",
      "Iteration 66, loss = 0.14227198\n",
      "Iteration 31, loss = 0.25597752\n",
      "Iteration 67, loss = 0.13927158\n",
      "Iteration 32, loss = 0.25641096\n",
      "Iteration 68, loss = 0.14026364\n",
      "Iteration 33, loss = 0.24807792\n",
      "Iteration 69, loss = 0.13579877\n",
      "Iteration 34, loss = 0.24522420\n",
      "Iteration 70, loss = 0.13711635\n",
      "Iteration 35, loss = 0.24074333\n",
      "Iteration 71, loss = 0.13555760\n",
      "Iteration 36, loss = 0.23724965\n",
      "Iteration 72, loss = 0.13246304\n",
      "Iteration 37, loss = 0.23762743\n",
      "Iteration 73, loss = 0.13268679\n",
      "Iteration 38, loss = 0.23068345\n",
      "Iteration 74, loss = 0.13267568\n",
      "Iteration 39, loss = 0.22881577\n",
      "Iteration 75, loss = 0.13500492\n",
      "Iteration 40, loss = 0.22344603\n",
      "Iteration 76, loss = 0.12948305\n",
      "Iteration 41, loss = 0.22044913\n",
      "Iteration 77, loss = 0.12625115\n",
      "Iteration 42, loss = 0.21739512\n",
      "Iteration 78, loss = 0.12666960\n",
      "Iteration 43, loss = 0.21531975\n",
      "Iteration 44, loss = 0.21101847\n",
      "Iteration 79, loss = 0.12380487\n",
      "Iteration 45, loss = 0.21012232\n",
      "Iteration 80, loss = 0.12127574\n",
      "Iteration 46, loss = 0.21008520\n",
      "Iteration 81, loss = 0.12031712\n",
      "Iteration 47, loss = 0.20129562\n",
      "Iteration 82, loss = 0.11870246\n",
      "Iteration 48, loss = 0.20121548\n",
      "Iteration 83, loss = 0.11985441\n",
      "Iteration 49, loss = 0.19684071\n",
      "Iteration 84, loss = 0.11632621\n",
      "Iteration 50, loss = 0.19386603\n",
      "Iteration 85, loss = 0.11678991\n",
      "Iteration 51, loss = 0.19170051\n",
      "Iteration 86, loss = 0.11462435\n",
      "Iteration 52, loss = 0.18853821\n",
      "Iteration 87, loss = 0.11289030\n",
      "Iteration 53, loss = 0.18588902\n",
      "Iteration 88, loss = 0.11349728\n",
      "Iteration 54, loss = 0.18293408\n",
      "Iteration 89, loss = 0.11850318\n",
      "Iteration 55, loss = 0.18017345\n",
      "Iteration 90, loss = 0.11320816\n",
      "Iteration 56, loss = 0.17793775\n",
      "Iteration 91, loss = 0.11109661\n",
      "Iteration 57, loss = 0.17727951\n",
      "Iteration 92, loss = 0.11101831\n",
      "Iteration 58, loss = 0.17212897\n",
      "Iteration 93, loss = 0.10747833\n",
      "Iteration 59, loss = 0.17096060\n",
      "Iteration 94, loss = 0.10826774\n",
      "Iteration 95, loss = 0.10520042\n",
      "Iteration 60, loss = 0.16857939\n",
      "Iteration 96, loss = 0.10320242\n",
      "Iteration 61, loss = 0.16644237\n",
      "Iteration 97, loss = 0.10429487\n",
      "Iteration 62, loss = 0.16287051\n",
      "Iteration 98, loss = 0.10324266\n",
      "Iteration 63, loss = 0.16477660\n",
      "Iteration 99, loss = 0.10269323\n",
      "Iteration 64, loss = 0.15964122\n",
      "Iteration 100, loss = 0.10015034\n",
      "Iteration 65, loss = 0.15803741\n",
      "Iteration 101, loss = 0.10064582\n",
      "Iteration 66, loss = 0.15710235\n",
      "Iteration 102, loss = 0.10068082\n",
      "Iteration 67, loss = 0.15698715\n",
      "Iteration 68, loss = 0.15550018\n",
      "Iteration 103, loss = 0.09740532\n",
      "Iteration 69, loss = 0.14993345\n",
      "Iteration 104, loss = 0.09791773\n",
      "Iteration 70, loss = 0.14811901\n",
      "Iteration 105, loss = 0.09467734\n",
      "Iteration 106, loss = 0.09511051\n",
      "Iteration 71, loss = 0.14588068\n",
      "Iteration 107, loss = 0.09555943\n",
      "Iteration 72, loss = 0.14411299\n",
      "Iteration 108, loss = 0.09267055\n",
      "Iteration 73, loss = 0.14212565\n",
      "Iteration 109, loss = 0.09167649\n",
      "Iteration 74, loss = 0.14166566\n",
      "Iteration 110, loss = 0.09405187\n",
      "Iteration 75, loss = 0.14088809\n",
      "Iteration 111, loss = 0.09161129\n",
      "Iteration 76, loss = 0.13945824\n",
      "Iteration 112, loss = 0.08885122\n",
      "Iteration 77, loss = 0.13956080\n",
      "Iteration 113, loss = 0.08890609\n",
      "Iteration 78, loss = 0.13289143\n",
      "Iteration 114, loss = 0.08778310\n",
      "Iteration 79, loss = 0.13428430\n",
      "Iteration 115, loss = 0.08741052\n",
      "Iteration 80, loss = 0.13467154\n",
      "Iteration 116, loss = 0.08762932\n",
      "Iteration 81, loss = 0.13010927\n",
      "Iteration 117, loss = 0.08987511\n",
      "Iteration 82, loss = 0.12819692\n",
      "Iteration 118, loss = 0.08809160\n",
      "Iteration 83, loss = 0.12669495\n",
      "Iteration 119, loss = 0.08452830\n",
      "Iteration 84, loss = 0.12516576\n",
      "Iteration 120, loss = 0.08647853\n",
      "Iteration 85, loss = 0.12343752\n",
      "Iteration 121, loss = 0.08448863\n",
      "Iteration 86, loss = 0.12353132\n",
      "Iteration 122, loss = 0.08137989\n",
      "Iteration 87, loss = 0.12143070\n",
      "Iteration 123, loss = 0.08214508\n",
      "Iteration 88, loss = 0.12125709\n",
      "Iteration 124, loss = 0.08242361\n",
      "Iteration 89, loss = 0.11851640\n",
      "Iteration 125, loss = 0.08152331\n",
      "Iteration 90, loss = 0.11754463\n",
      "Iteration 126, loss = 0.07977746\n",
      "Iteration 91, loss = 0.11620250\n",
      "Iteration 127, loss = 0.08126820\n",
      "Iteration 92, loss = 0.11357164\n",
      "Iteration 128, loss = 0.08140254\n",
      "Iteration 93, loss = 0.11440408\n",
      "Iteration 129, loss = 0.07812706\n",
      "Iteration 94, loss = 0.11435800\n",
      "Iteration 130, loss = 0.07670531\n",
      "Iteration 95, loss = 0.11515670\n",
      "Iteration 131, loss = 0.07651755\n",
      "Iteration 96, loss = 0.11621969\n",
      "Iteration 132, loss = 0.07492724\n",
      "Iteration 97, loss = 0.11429731\n",
      "Iteration 133, loss = 0.07589593\n",
      "Iteration 98, loss = 0.10880875\n",
      "Iteration 134, loss = 0.07316344\n",
      "Iteration 99, loss = 0.11112312\n",
      "Iteration 135, loss = 0.07290108\n",
      "Iteration 100, loss = 0.10775731\n",
      "Iteration 136, loss = 0.07469316\n",
      "Iteration 101, loss = 0.10656644\n",
      "Iteration 137, loss = 0.07324540\n",
      "Iteration 102, loss = 0.10629588\n",
      "Iteration 138, loss = 0.07475810\n",
      "Iteration 103, loss = 0.10537433\n",
      "Iteration 139, loss = 0.07197854\n",
      "Iteration 104, loss = 0.10120542\n",
      "Iteration 140, loss = 0.07030722\n",
      "Iteration 105, loss = 0.10188034\n",
      "Iteration 141, loss = 0.06932253\n",
      "Iteration 106, loss = 0.09964381\n",
      "Iteration 142, loss = 0.06861152\n",
      "Iteration 107, loss = 0.09904819\n",
      "Iteration 143, loss = 0.06855189\n",
      "Iteration 108, loss = 0.09793797\n",
      "Iteration 144, loss = 0.06720110\n",
      "Iteration 109, loss = 0.09686599\n",
      "Iteration 145, loss = 0.06856589\n",
      "Iteration 110, loss = 0.09607958\n",
      "Iteration 146, loss = 0.06707453\n",
      "Iteration 147, loss = 0.07000401\n",
      "Iteration 111, loss = 0.09609495\n",
      "Iteration 148, loss = 0.07424068\n",
      "Iteration 112, loss = 0.09509345\n",
      "Iteration 149, loss = 0.06970551\n",
      "Iteration 113, loss = 0.09382002\n",
      "Iteration 150, loss = 0.06589611\n",
      "Iteration 114, loss = 0.09165730\n",
      "Iteration 115, loss = 0.09381715\n",
      "Iteration 151, loss = 0.06651646\n",
      "Iteration 116, loss = 0.09309371\n",
      "Iteration 152, loss = 0.06554030\n",
      "Iteration 117, loss = 0.09345882\n",
      "Iteration 153, loss = 0.07098319\n",
      "Iteration 118, loss = 0.09248924\n",
      "Iteration 154, loss = 0.06672635\n",
      "Iteration 119, loss = 0.08773959\n",
      "Iteration 155, loss = 0.06293598\n",
      "Iteration 120, loss = 0.08881858\n",
      "Iteration 156, loss = 0.06687687\n",
      "Iteration 157, loss = 0.06197166\n",
      "Iteration 121, loss = 0.08921571\n",
      "Iteration 122, loss = 0.08802682\n",
      "Iteration 158, loss = 0.06145957\n",
      "Iteration 123, loss = 0.08651377\n",
      "Iteration 159, loss = 0.05999800\n",
      "Iteration 124, loss = 0.08678534\n",
      "Iteration 160, loss = 0.05959892\n",
      "Iteration 125, loss = 0.08537294\n",
      "Iteration 161, loss = 0.05892399\n",
      "Iteration 126, loss = 0.08371401\n",
      "Iteration 162, loss = 0.05844489\n",
      "Iteration 163, loss = 0.05961135\n",
      "Iteration 127, loss = 0.08497680\n",
      "Iteration 164, loss = 0.05943652\n",
      "Iteration 128, loss = 0.08300443\n",
      "Iteration 165, loss = 0.05837475\n",
      "Iteration 129, loss = 0.08113262\n",
      "Iteration 166, loss = 0.05935466\n",
      "Iteration 130, loss = 0.07937578\n",
      "Iteration 167, loss = 0.05679672\n",
      "Iteration 131, loss = 0.08201470\n",
      "Iteration 168, loss = 0.05613083\n",
      "Iteration 132, loss = 0.07669176\n",
      "Iteration 169, loss = 0.05548102\n",
      "Iteration 133, loss = 0.07708856\n",
      "Iteration 170, loss = 0.05614749\n",
      "Iteration 134, loss = 0.08030182\n",
      "Iteration 171, loss = 0.05545164\n",
      "Iteration 135, loss = 0.08115844\n",
      "Iteration 172, loss = 0.05472571\n",
      "Iteration 136, loss = 0.07574382\n",
      "Iteration 173, loss = 0.05485839\n",
      "Iteration 137, loss = 0.07539877\n",
      "Iteration 174, loss = 0.05486491\n",
      "Iteration 138, loss = 0.07555744\n",
      "Iteration 175, loss = 0.05655654\n",
      "Iteration 139, loss = 0.07481451\n",
      "Iteration 176, loss = 0.05368868\n",
      "Iteration 140, loss = 0.07523202\n",
      "Iteration 177, loss = 0.05411354\n",
      "Iteration 141, loss = 0.07435665\n",
      "Iteration 178, loss = 0.05464342\n",
      "Iteration 142, loss = 0.06862162\n",
      "Iteration 179, loss = 0.05485358\n",
      "Iteration 143, loss = 0.07490957\n",
      "Iteration 180, loss = 0.05415658\n",
      "Iteration 144, loss = 0.07360040\n",
      "Iteration 181, loss = 0.05015510\n",
      "Iteration 145, loss = 0.07135682\n",
      "Iteration 182, loss = 0.04932956\n",
      "Iteration 146, loss = 0.07230749\n",
      "Iteration 183, loss = 0.04986326\n",
      "Iteration 147, loss = 0.07538872\n",
      "Iteration 184, loss = 0.05132432\n",
      "Iteration 148, loss = 0.07148246\n",
      "Iteration 185, loss = 0.05078379\n",
      "Iteration 149, loss = 0.06729628\n",
      "Iteration 186, loss = 0.05139651\n",
      "Iteration 150, loss = 0.06590220\n",
      "Iteration 187, loss = 0.04899958\n",
      "Iteration 151, loss = 0.06714633\n",
      "Iteration 188, loss = 0.04690132\n",
      "Iteration 152, loss = 0.06631460\n",
      "Iteration 189, loss = 0.04684991\n",
      "Iteration 153, loss = 0.06674657\n",
      "Iteration 190, loss = 0.04525883\n",
      "Iteration 154, loss = 0.06624074\n",
      "Iteration 191, loss = 0.04430067\n",
      "Iteration 155, loss = 0.06423345\n",
      "Iteration 192, loss = 0.04439140\n",
      "Iteration 156, loss = 0.06617245\n",
      "Iteration 193, loss = 0.04388634\n",
      "Iteration 194, loss = 0.04394921\n",
      "Iteration 157, loss = 0.06150075\n",
      "Iteration 195, loss = 0.04487949\n",
      "Iteration 158, loss = 0.06006813\n",
      "Iteration 196, loss = 0.04580301\n",
      "Iteration 159, loss = 0.06211373\n",
      "Iteration 197, loss = 0.04853916\n",
      "Iteration 160, loss = 0.06323965\n",
      "Iteration 198, loss = 0.04433640\n",
      "Iteration 161, loss = 0.06215877\n",
      "Iteration 199, loss = 0.04403001\n",
      "Iteration 162, loss = 0.05826594\n",
      "Iteration 200, loss = 0.04530906\n",
      "Iteration 163, loss = 0.06125790\n",
      "Iteration 164, loss = 0.06453315\n",
      "Iteration 165, loss = 0.06061544\n",
      "Iteration 166, loss = 0.05849423\n",
      "Iteration 167, loss = 0.05701255\n",
      "Iteration 168, loss = 0.05628680\n",
      "Iteration 169, loss = 0.05605095\n",
      "Iteration 170, loss = 0.05533608\n",
      "Iteration 171, loss = 0.05332560\n",
      "Iteration 172, loss = 0.05440639\n",
      "Iteration 173, loss = 0.05460119\n",
      "Iteration 174, loss = 0.05432183\n",
      "Iteration 175, loss = 0.05461407\n",
      "Iteration 1, loss = 1.07741614\n",
      "Iteration 176, loss = 0.05383887\n",
      "Iteration 2, loss = 0.80490958\n",
      "Iteration 177, loss = 0.05085542\n",
      "Iteration 3, loss = 0.67289095\n",
      "Iteration 178, loss = 0.05206521\n",
      "Iteration 4, loss = 0.58427701\n",
      "Iteration 179, loss = 0.05321124\n",
      "Iteration 5, loss = 0.53389424\n",
      "Iteration 180, loss = 0.05326807\n",
      "Iteration 6, loss = 0.49288532\n",
      "Iteration 181, loss = 0.05094502\n",
      "Iteration 7, loss = 0.46465264\n",
      "Iteration 182, loss = 0.04876096\n",
      "Iteration 8, loss = 0.44121496\n",
      "Iteration 183, loss = 0.05115729\n",
      "Iteration 9, loss = 0.42093553\n",
      "Iteration 184, loss = 0.04894252\n",
      "Iteration 10, loss = 0.40556594\n",
      "Iteration 185, loss = 0.04779161\n",
      "Iteration 11, loss = 0.39128270\n",
      "Iteration 186, loss = 0.04772314\n",
      "Iteration 12, loss = 0.37906996\n",
      "Iteration 187, loss = 0.04668981\n",
      "Iteration 13, loss = 0.36779054\n",
      "Iteration 188, loss = 0.04738915\n",
      "Iteration 14, loss = 0.35827898\n",
      "Iteration 189, loss = 0.04821243\n",
      "Iteration 15, loss = 0.34828861\n",
      "Iteration 190, loss = 0.04866719\n",
      "Iteration 16, loss = 0.33957443\n",
      "Iteration 191, loss = 0.04709106\n",
      "Iteration 17, loss = 0.33197487\n",
      "Iteration 192, loss = 0.04716475\n",
      "Iteration 18, loss = 0.32435223\n",
      "Iteration 193, loss = 0.04866708\n",
      "Iteration 19, loss = 0.31768149\n",
      "Iteration 194, loss = 0.04832689\n",
      "Iteration 20, loss = 0.31080979\n",
      "Iteration 195, loss = 0.04403699\n",
      "Iteration 21, loss = 0.30477045\n",
      "Iteration 196, loss = 0.04368863\n",
      "Iteration 22, loss = 0.29957103\n",
      "Iteration 197, loss = 0.04173666\n",
      "Iteration 23, loss = 0.29276659\n",
      "Iteration 198, loss = 0.04196290\n",
      "Iteration 24, loss = 0.28748842\n",
      "Iteration 199, loss = 0.04207797\n",
      "Iteration 25, loss = 0.28205756\n",
      "Iteration 200, loss = 0.04430661\n",
      "Iteration 26, loss = 0.27653157\n",
      "Iteration 27, loss = 0.27406522\n",
      "Iteration 28, loss = 0.26746521\n",
      "Iteration 29, loss = 0.26347344\n",
      "Iteration 30, loss = 0.25880629\n",
      "Iteration 31, loss = 0.25555832\n",
      "Iteration 32, loss = 0.25180170\n",
      "Iteration 33, loss = 0.24670811\n",
      "Iteration 34, loss = 0.24380254\n",
      "Iteration 35, loss = 0.24074139\n",
      "Iteration 36, loss = 0.23683747\n",
      "Iteration 37, loss = 0.23313121\n",
      "Iteration 38, loss = 0.22877947\n",
      "Iteration 1, loss = 1.09311871\n",
      "Iteration 39, loss = 0.22631698\n",
      "Iteration 2, loss = 0.80435894\n",
      "Iteration 40, loss = 0.22318539\n",
      "Iteration 3, loss = 0.67025456\n",
      "Iteration 41, loss = 0.21955857\n",
      "Iteration 4, loss = 0.58550885\n",
      "Iteration 42, loss = 0.21656210\n",
      "Iteration 5, loss = 0.53315191\n",
      "Iteration 43, loss = 0.21349055\n",
      "Iteration 6, loss = 0.49258989\n",
      "Iteration 44, loss = 0.21156340\n",
      "Iteration 7, loss = 0.46245276\n",
      "Iteration 45, loss = 0.20764101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.43961131\n",
      "Iteration 46, loss = 0.20573104\n",
      "Iteration 47, loss = 0.20335936\n",
      "Iteration 9, loss = 0.41905080\n",
      "Iteration 10, loss = 0.40291253\n",
      "Iteration 48, loss = 0.20262202\n",
      "Iteration 11, loss = 0.38837916\n",
      "Iteration 49, loss = 0.20215056\n",
      "Iteration 12, loss = 0.37600184\n",
      "Iteration 50, loss = 0.19584207\n",
      "Iteration 13, loss = 0.36416739\n",
      "Iteration 51, loss = 0.19389260\n",
      "Iteration 14, loss = 0.35507170\n",
      "Iteration 52, loss = 0.19044477\n",
      "Iteration 15, loss = 0.34627087\n",
      "Iteration 53, loss = 0.18686207\n",
      "Iteration 16, loss = 0.33762346\n",
      "Iteration 54, loss = 0.18604545\n",
      "Iteration 17, loss = 0.32933983\n",
      "Iteration 55, loss = 0.18519755\n",
      "Iteration 18, loss = 0.32162642\n",
      "Iteration 56, loss = 0.18124277\n",
      "Iteration 19, loss = 0.31354605\n",
      "Iteration 57, loss = 0.18096337\n",
      "Iteration 20, loss = 0.30732795\n",
      "Iteration 58, loss = 0.17668852\n",
      "Iteration 21, loss = 0.30120284\n",
      "Iteration 59, loss = 0.17487215\n",
      "Iteration 22, loss = 0.29504043\n",
      "Iteration 60, loss = 0.17175584\n",
      "Iteration 23, loss = 0.29031508\n",
      "Iteration 61, loss = 0.16984000\n",
      "Iteration 24, loss = 0.28568298\n",
      "Iteration 62, loss = 0.16852880\n",
      "Iteration 25, loss = 0.27848657\n",
      "Iteration 63, loss = 0.16577419\n",
      "Iteration 26, loss = 0.27540989\n",
      "Iteration 64, loss = 0.16444131\n",
      "Iteration 27, loss = 0.27194302\n",
      "Iteration 65, loss = 0.15995316\n",
      "Iteration 28, loss = 0.26526183\n",
      "Iteration 66, loss = 0.16096267\n",
      "Iteration 29, loss = 0.26114867\n",
      "Iteration 67, loss = 0.15843794\n",
      "Iteration 30, loss = 0.25497940\n",
      "Iteration 68, loss = 0.15762733\n",
      "Iteration 31, loss = 0.25371095\n",
      "Iteration 69, loss = 0.15326961\n",
      "Iteration 32, loss = 0.24631602\n",
      "Iteration 70, loss = 0.15298311\n",
      "Iteration 33, loss = 0.24396495\n",
      "Iteration 71, loss = 0.14928586\n",
      "Iteration 34, loss = 0.24022052\n",
      "Iteration 72, loss = 0.14771234\n",
      "Iteration 73, loss = 0.14737363\n",
      "Iteration 35, loss = 0.23576653\n",
      "Iteration 74, loss = 0.14628965\n",
      "Iteration 36, loss = 0.23260216\n",
      "Iteration 75, loss = 0.14135235\n",
      "Iteration 37, loss = 0.22869926\n",
      "Iteration 38, loss = 0.22613797\n",
      "Iteration 76, loss = 0.14317092\n",
      "Iteration 39, loss = 0.22168950\n",
      "Iteration 77, loss = 0.13896751\n",
      "Iteration 40, loss = 0.21883981\n",
      "Iteration 78, loss = 0.13807619\n",
      "Iteration 41, loss = 0.21535852\n",
      "Iteration 79, loss = 0.13572155\n",
      "Iteration 42, loss = 0.21261798\n",
      "Iteration 80, loss = 0.13438086\n",
      "Iteration 43, loss = 0.21042214\n",
      "Iteration 81, loss = 0.13312344\n",
      "Iteration 44, loss = 0.20593584\n",
      "Iteration 82, loss = 0.13429396\n",
      "Iteration 45, loss = 0.20611089\n",
      "Iteration 83, loss = 0.13027286\n",
      "Iteration 46, loss = 0.20001969\n",
      "Iteration 84, loss = 0.13006442\n",
      "Iteration 47, loss = 0.19972446\n",
      "Iteration 85, loss = 0.13008691\n",
      "Iteration 48, loss = 0.19640159\n",
      "Iteration 86, loss = 0.12864140\n",
      "Iteration 49, loss = 0.19194090\n",
      "Iteration 87, loss = 0.12511038\n",
      "Iteration 50, loss = 0.18972698\n",
      "Iteration 88, loss = 0.12468882\n",
      "Iteration 51, loss = 0.19198719Iteration 89, loss = 0.12175182\n",
      "\n",
      "Iteration 90, loss = 0.12070788\n",
      "Iteration 52, loss = 0.18247927\n",
      "Iteration 91, loss = 0.11842032\n",
      "Iteration 53, loss = 0.18257199\n",
      "Iteration 92, loss = 0.11799963\n",
      "Iteration 54, loss = 0.17999474\n",
      "Iteration 93, loss = 0.11704243\n",
      "Iteration 55, loss = 0.17449409\n",
      "Iteration 94, loss = 0.11582099\n",
      "Iteration 56, loss = 0.17249343\n",
      "Iteration 95, loss = 0.11489497\n",
      "Iteration 57, loss = 0.17016679\n",
      "Iteration 96, loss = 0.11255226\n",
      "Iteration 58, loss = 0.16768255\n",
      "Iteration 97, loss = 0.11289314\n",
      "Iteration 59, loss = 0.16607176\n",
      "Iteration 98, loss = 0.11616274\n",
      "Iteration 60, loss = 0.16310972\n",
      "Iteration 99, loss = 0.11078786\n",
      "Iteration 61, loss = 0.15998587\n",
      "Iteration 100, loss = 0.10863660\n",
      "Iteration 62, loss = 0.15804249\n",
      "Iteration 101, loss = 0.10999953\n",
      "Iteration 63, loss = 0.15695423\n",
      "Iteration 102, loss = 0.10626090\n",
      "Iteration 64, loss = 0.15558598\n",
      "Iteration 103, loss = 0.10747872\n",
      "Iteration 65, loss = 0.15101031\n",
      "Iteration 104, loss = 0.10778454\n",
      "Iteration 66, loss = 0.14857431\n",
      "Iteration 105, loss = 0.10995316\n",
      "Iteration 67, loss = 0.14695544\n",
      "Iteration 106, loss = 0.10399120\n",
      "Iteration 68, loss = 0.14662942\n",
      "Iteration 107, loss = 0.10192709\n",
      "Iteration 69, loss = 0.14392193\n",
      "Iteration 108, loss = 0.10252776\n",
      "Iteration 70, loss = 0.14340627\n",
      "Iteration 109, loss = 0.10431053\n",
      "Iteration 71, loss = 0.14088217\n",
      "Iteration 110, loss = 0.10257120\n",
      "Iteration 72, loss = 0.13924567\n",
      "Iteration 111, loss = 0.09729627\n",
      "Iteration 73, loss = 0.13819227\n",
      "Iteration 112, loss = 0.09687118\n",
      "Iteration 74, loss = 0.13554643\n",
      "Iteration 113, loss = 0.09689333\n",
      "Iteration 75, loss = 0.13330113\n",
      "Iteration 114, loss = 0.09361191\n",
      "Iteration 76, loss = 0.13303389\n",
      "Iteration 115, loss = 0.09355192\n",
      "Iteration 77, loss = 0.13689645\n",
      "Iteration 116, loss = 0.09287479\n",
      "Iteration 78, loss = 0.13135383\n",
      "Iteration 117, loss = 0.09190920\n",
      "Iteration 79, loss = 0.13091148\n",
      "Iteration 118, loss = 0.08954682\n",
      "Iteration 80, loss = 0.12671752\n",
      "Iteration 119, loss = 0.09205013\n",
      "Iteration 81, loss = 0.12640243\n",
      "Iteration 120, loss = 0.09287050\n",
      "Iteration 82, loss = 0.12468807\n",
      "Iteration 121, loss = 0.08884391\n",
      "Iteration 83, loss = 0.12091906\n",
      "Iteration 122, loss = 0.08971928\n",
      "Iteration 84, loss = 0.12458259\n",
      "Iteration 123, loss = 0.08491127\n",
      "Iteration 85, loss = 0.12267835\n",
      "Iteration 124, loss = 0.08680492\n",
      "Iteration 86, loss = 0.11890944\n",
      "Iteration 125, loss = 0.08621493\n",
      "Iteration 87, loss = 0.11734648\n",
      "Iteration 126, loss = 0.08246026\n",
      "Iteration 88, loss = 0.11551247\n",
      "Iteration 127, loss = 0.08340879\n",
      "Iteration 89, loss = 0.11420909\n",
      "Iteration 128, loss = 0.08105214\n",
      "Iteration 90, loss = 0.11324315\n",
      "Iteration 129, loss = 0.08074299\n",
      "Iteration 91, loss = 0.11046000\n",
      "Iteration 130, loss = 0.07967309\n",
      "Iteration 92, loss = 0.10942341\n",
      "Iteration 131, loss = 0.07960062\n",
      "Iteration 93, loss = 0.10856464\n",
      "Iteration 132, loss = 0.08043525\n",
      "Iteration 94, loss = 0.10857664\n",
      "Iteration 133, loss = 0.07807300\n",
      "Iteration 95, loss = 0.10633017\n",
      "Iteration 134, loss = 0.08072196\n",
      "Iteration 96, loss = 0.10604894\n",
      "Iteration 135, loss = 0.08225758\n",
      "Iteration 97, loss = 0.10604679\n",
      "Iteration 136, loss = 0.07877310\n",
      "Iteration 98, loss = 0.10259459\n",
      "Iteration 99, loss = 0.10387869\n",
      "Iteration 137, loss = 0.07624172\n",
      "Iteration 100, loss = 0.10431776\n",
      "Iteration 138, loss = 0.07464564\n",
      "Iteration 101, loss = 0.09996579\n",
      "Iteration 139, loss = 0.07680293\n",
      "Iteration 102, loss = 0.09972686\n",
      "Iteration 140, loss = 0.07603178\n",
      "Iteration 103, loss = 0.09961366\n",
      "Iteration 141, loss = 0.07438818\n",
      "Iteration 104, loss = 0.09630910\n",
      "Iteration 142, loss = 0.07596500\n",
      "Iteration 105, loss = 0.09534445\n",
      "Iteration 143, loss = 0.07243876\n",
      "Iteration 144, loss = 0.06871864\n",
      "Iteration 106, loss = 0.09506652\n",
      "Iteration 145, loss = 0.07385949\n",
      "Iteration 107, loss = 0.09396823\n",
      "Iteration 146, loss = 0.07336970\n",
      "Iteration 108, loss = 0.09202491\n",
      "Iteration 147, loss = 0.07280466\n",
      "Iteration 109, loss = 0.09112270\n",
      "Iteration 148, loss = 0.06741479\n",
      "Iteration 110, loss = 0.09109726\n",
      "Iteration 149, loss = 0.07005801\n",
      "Iteration 111, loss = 0.08974582\n",
      "Iteration 150, loss = 0.06938204\n",
      "Iteration 112, loss = 0.08811932\n",
      "Iteration 151, loss = 0.07072841\n",
      "Iteration 113, loss = 0.09064335\n",
      "Iteration 152, loss = 0.06826506\n",
      "Iteration 114, loss = 0.08873390\n",
      "Iteration 153, loss = 0.06554154\n",
      "Iteration 115, loss = 0.08510995\n",
      "Iteration 154, loss = 0.06307595\n",
      "Iteration 116, loss = 0.08672979\n",
      "Iteration 155, loss = 0.06207366\n",
      "Iteration 117, loss = 0.08455904\n",
      "Iteration 156, loss = 0.06394832\n",
      "Iteration 118, loss = 0.08307850\n",
      "Iteration 157, loss = 0.06164556\n",
      "Iteration 119, loss = 0.08362163\n",
      "Iteration 120, loss = 0.08217979\n",
      "Iteration 158, loss = 0.05943488\n",
      "Iteration 121, loss = 0.08139979\n",
      "Iteration 159, loss = 0.05978910\n",
      "Iteration 122, loss = 0.08018434\n",
      "Iteration 160, loss = 0.05993514\n",
      "Iteration 123, loss = 0.07988318\n",
      "Iteration 161, loss = 0.05905591\n",
      "Iteration 124, loss = 0.07967776\n",
      "Iteration 162, loss = 0.05786303\n",
      "Iteration 125, loss = 0.07923360\n",
      "Iteration 163, loss = 0.05722807\n",
      "Iteration 126, loss = 0.07633550\n",
      "Iteration 164, loss = 0.05759765\n",
      "Iteration 127, loss = 0.07738875\n",
      "Iteration 165, loss = 0.05657174\n",
      "Iteration 128, loss = 0.07757250\n",
      "Iteration 166, loss = 0.05508979\n",
      "Iteration 129, loss = 0.07609603\n",
      "Iteration 167, loss = 0.05466042\n",
      "Iteration 130, loss = 0.07386496\n",
      "Iteration 168, loss = 0.05707573\n",
      "Iteration 131, loss = 0.07487430\n",
      "Iteration 169, loss = 0.05818991\n",
      "Iteration 132, loss = 0.07301663\n",
      "Iteration 170, loss = 0.05715564\n",
      "Iteration 133, loss = 0.07149598\n",
      "Iteration 171, loss = 0.05394287\n",
      "Iteration 134, loss = 0.07058312\n",
      "Iteration 172, loss = 0.05233831\n",
      "Iteration 135, loss = 0.06966073\n",
      "Iteration 173, loss = 0.05182448\n",
      "Iteration 136, loss = 0.06944999\n",
      "Iteration 174, loss = 0.05192274\n",
      "Iteration 137, loss = 0.06895891\n",
      "Iteration 175, loss = 0.05053291\n",
      "Iteration 138, loss = 0.06748353\n",
      "Iteration 176, loss = 0.04943884\n",
      "Iteration 139, loss = 0.06860072\n",
      "Iteration 177, loss = 0.04913687\n",
      "Iteration 140, loss = 0.06666085\n",
      "Iteration 178, loss = 0.04889927\n",
      "Iteration 141, loss = 0.06645327\n",
      "Iteration 179, loss = 0.04832992\n",
      "Iteration 142, loss = 0.06551191\n",
      "Iteration 180, loss = 0.05054759\n",
      "Iteration 143, loss = 0.06441929\n",
      "Iteration 181, loss = 0.05150521\n",
      "Iteration 144, loss = 0.06565365\n",
      "Iteration 182, loss = 0.04799783\n",
      "Iteration 145, loss = 0.06264531\n",
      "Iteration 183, loss = 0.04553037\n",
      "Iteration 146, loss = 0.06177632\n",
      "Iteration 184, loss = 0.04667614\n",
      "Iteration 147, loss = 0.06308488\n",
      "Iteration 185, loss = 0.04620108\n",
      "Iteration 148, loss = 0.06264789\n",
      "Iteration 186, loss = 0.04671402\n",
      "Iteration 149, loss = 0.06606603\n",
      "Iteration 187, loss = 0.04408295\n",
      "Iteration 150, loss = 0.06233287\n",
      "Iteration 188, loss = 0.04359383\n",
      "Iteration 151, loss = 0.06164941\n",
      "Iteration 189, loss = 0.04361692\n",
      "Iteration 152, loss = 0.06551659\n",
      "Iteration 190, loss = 0.04413952\n",
      "Iteration 153, loss = 0.07100140\n",
      "Iteration 191, loss = 0.04228631\n",
      "Iteration 154, loss = 0.06403006\n",
      "Iteration 192, loss = 0.04195299\n",
      "Iteration 155, loss = 0.06018123\n",
      "Iteration 193, loss = 0.04305086\n",
      "Iteration 156, loss = 0.05815247\n",
      "Iteration 194, loss = 0.04626725\n",
      "Iteration 157, loss = 0.05965106\n",
      "Iteration 195, loss = 0.04413768\n",
      "Iteration 158, loss = 0.05690318\n",
      "Iteration 196, loss = 0.04667392\n",
      "Iteration 159, loss = 0.05605302\n",
      "Iteration 197, loss = 0.04736748\n",
      "Iteration 160, loss = 0.05478129\n",
      "Iteration 198, loss = 0.04314747\n",
      "Iteration 161, loss = 0.05685319\n",
      "Iteration 199, loss = 0.03982907\n",
      "Iteration 162, loss = 0.05448720\n",
      "Iteration 200, loss = 0.04042475\n",
      "Iteration 163, loss = 0.05462858\n",
      "Iteration 201, loss = 0.03856235\n",
      "Iteration 164, loss = 0.05196786\n",
      "Iteration 202, loss = 0.03867104\n",
      "Iteration 165, loss = 0.05224675\n",
      "Iteration 203, loss = 0.03753533\n",
      "Iteration 166, loss = 0.05217978\n",
      "Iteration 204, loss = 0.03712726\n",
      "Iteration 167, loss = 0.05116128\n",
      "Iteration 205, loss = 0.03625693\n",
      "Iteration 168, loss = 0.05028933\n",
      "Iteration 206, loss = 0.03593678\n",
      "Iteration 169, loss = 0.05016408\n",
      "Iteration 207, loss = 0.03688881\n",
      "Iteration 170, loss = 0.05114495\n",
      "Iteration 208, loss = 0.03517183\n",
      "Iteration 171, loss = 0.05317586\n",
      "Iteration 209, loss = 0.03412567\n",
      "Iteration 172, loss = 0.05249704\n",
      "Iteration 210, loss = 0.03539869\n",
      "Iteration 173, loss = 0.05121246\n",
      "Iteration 211, loss = 0.03331165\n",
      "Iteration 174, loss = 0.04817585\n",
      "Iteration 212, loss = 0.03506915\n",
      "Iteration 175, loss = 0.04837065\n",
      "Iteration 213, loss = 0.03661694\n",
      "Iteration 176, loss = 0.04832438\n",
      "Iteration 214, loss = 0.03779016\n",
      "Iteration 177, loss = 0.04631548\n",
      "Iteration 215, loss = 0.03651940\n",
      "Iteration 178, loss = 0.04628684\n",
      "Iteration 216, loss = 0.03503715\n",
      "Iteration 217, loss = 0.03499044\n",
      "Iteration 179, loss = 0.04622344\n",
      "Iteration 218, loss = 0.03308853\n",
      "Iteration 180, loss = 0.04441706\n",
      "Iteration 181, loss = 0.04499014\n",
      "Iteration 219, loss = 0.03393601\n",
      "Iteration 182, loss = 0.04574618\n",
      "Iteration 220, loss = 0.03328572\n",
      "Iteration 221, loss = 0.03108944\n",
      "Iteration 183, loss = 0.04591727\n",
      "Iteration 222, loss = 0.03081119\n",
      "Iteration 184, loss = 0.04316405\n",
      "Iteration 223, loss = 0.03111174\n",
      "Iteration 185, loss = 0.04328025\n",
      "Iteration 224, loss = 0.03359364\n",
      "Iteration 186, loss = 0.04431188\n",
      "Iteration 225, loss = 0.03444571\n",
      "Iteration 187, loss = 0.04315784\n",
      "Iteration 226, loss = 0.03640304\n",
      "Iteration 188, loss = 0.04312974\n",
      "Iteration 227, loss = 0.03233371\n",
      "Iteration 189, loss = 0.04168071\n",
      "Iteration 190, loss = 0.04104781\n",
      "Iteration 228, loss = 0.03065068\n",
      "Iteration 191, loss = 0.04103198\n",
      "Iteration 229, loss = 0.03355017\n",
      "Iteration 192, loss = 0.04113215\n",
      "Iteration 230, loss = 0.03060382\n",
      "Iteration 193, loss = 0.04034479\n",
      "Iteration 231, loss = 0.03090753\n",
      "Iteration 194, loss = 0.04273037\n",
      "Iteration 232, loss = 0.02944258\n",
      "Iteration 195, loss = 0.04199213\n",
      "Iteration 233, loss = 0.02787568\n",
      "Iteration 196, loss = 0.04022388\n",
      "Iteration 234, loss = 0.02881248\n",
      "Iteration 197, loss = 0.03888791\n",
      "Iteration 235, loss = 0.02642066\n",
      "Iteration 198, loss = 0.03956206\n",
      "Iteration 236, loss = 0.02637438\n",
      "Iteration 199, loss = 0.03830126\n",
      "Iteration 237, loss = 0.02513555\n",
      "Iteration 200, loss = 0.03835955\n",
      "Iteration 238, loss = 0.02433717\n",
      "Iteration 201, loss = 0.03657289\n",
      "Iteration 239, loss = 0.02466417\n",
      "Iteration 202, loss = 0.03873470\n",
      "Iteration 240, loss = 0.02497495\n",
      "Iteration 203, loss = 0.03685367\n",
      "Iteration 241, loss = 0.02524932\n",
      "Iteration 204, loss = 0.03869179\n",
      "Iteration 242, loss = 0.02590767\n",
      "Iteration 205, loss = 0.04143072\n",
      "Iteration 243, loss = 0.02427613\n",
      "Iteration 206, loss = 0.04210369\n",
      "Iteration 244, loss = 0.02474446\n",
      "Iteration 207, loss = 0.04780313\n",
      "Iteration 245, loss = 0.02361622\n",
      "Iteration 208, loss = 0.04195761\n",
      "Iteration 246, loss = 0.02344890\n",
      "Iteration 209, loss = 0.03966923\n",
      "Iteration 247, loss = 0.02287390\n",
      "Iteration 210, loss = 0.03758927\n",
      "Iteration 248, loss = 0.02239594\n",
      "Iteration 211, loss = 0.03597943\n",
      "Iteration 249, loss = 0.02231915\n",
      "Iteration 212, loss = 0.03752058\n",
      "Iteration 250, loss = 0.02241217\n",
      "Iteration 213, loss = 0.03697089\n",
      "Iteration 251, loss = 0.02165891\n",
      "Iteration 214, loss = 0.03372402\n",
      "Iteration 252, loss = 0.02206127\n",
      "Iteration 215, loss = 0.03380234\n",
      "Iteration 253, loss = 0.02174739\n",
      "Iteration 216, loss = 0.03241479\n",
      "Iteration 254, loss = 0.02130152\n",
      "Iteration 217, loss = 0.03287497\n",
      "Iteration 255, loss = 0.02094748\n",
      "Iteration 218, loss = 0.03430109\n",
      "Iteration 256, loss = 0.02019598\n",
      "Iteration 219, loss = 0.03214351\n",
      "Iteration 257, loss = 0.02043290\n",
      "Iteration 220, loss = 0.03204699\n",
      "Iteration 258, loss = 0.01968434\n",
      "Iteration 221, loss = 0.03081290\n",
      "Iteration 259, loss = 0.01975797\n",
      "Iteration 222, loss = 0.03071861\n",
      "Iteration 260, loss = 0.02054330\n",
      "Iteration 223, loss = 0.03524774\n",
      "Iteration 261, loss = 0.01902548\n",
      "Iteration 224, loss = 0.03097389\n",
      "Iteration 262, loss = 0.01953295\n",
      "Iteration 225, loss = 0.03238925\n",
      "Iteration 263, loss = 0.01921514\n",
      "Iteration 226, loss = 0.03059927\n",
      "Iteration 264, loss = 0.01873886\n",
      "Iteration 227, loss = 0.03056240\n",
      "Iteration 265, loss = 0.01880550\n",
      "Iteration 228, loss = 0.03092392\n",
      "Iteration 266, loss = 0.01817931\n",
      "Iteration 229, loss = 0.02949244\n",
      "Iteration 267, loss = 0.01857513\n",
      "Iteration 230, loss = 0.02949850\n",
      "Iteration 268, loss = 0.01789566\n",
      "Iteration 231, loss = 0.03337404\n",
      "Iteration 269, loss = 0.01737177\n",
      "Iteration 232, loss = 0.03290384\n",
      "Iteration 270, loss = 0.01739883\n",
      "Iteration 271, loss = 0.01767942\n",
      "Iteration 233, loss = 0.03009311\n",
      "Iteration 272, loss = 0.01715747\n",
      "Iteration 234, loss = 0.03176653\n",
      "Iteration 273, loss = 0.01694119\n",
      "Iteration 235, loss = 0.02909333\n",
      "Iteration 274, loss = 0.01663525\n",
      "Iteration 236, loss = 0.02701328\n",
      "Iteration 275, loss = 0.01630965\n",
      "Iteration 237, loss = 0.02972050\n",
      "Iteration 276, loss = 0.01611755\n",
      "Iteration 238, loss = 0.03062214\n",
      "Iteration 277, loss = 0.01595768\n",
      "Iteration 239, loss = 0.02894010\n",
      "Iteration 278, loss = 0.01586639\n",
      "Iteration 240, loss = 0.02574980\n",
      "Iteration 279, loss = 0.01578353\n",
      "Iteration 241, loss = 0.02643813\n",
      "Iteration 280, loss = 0.01533400\n",
      "Iteration 242, loss = 0.02956083\n",
      "Iteration 281, loss = 0.01537385\n",
      "Iteration 243, loss = 0.02977537\n",
      "Iteration 282, loss = 0.01514515\n",
      "Iteration 244, loss = 0.02739991\n",
      "Iteration 283, loss = 0.01537116\n",
      "Iteration 245, loss = 0.02520375\n",
      "Iteration 284, loss = 0.01569236\n",
      "Iteration 246, loss = 0.02904694\n",
      "Iteration 285, loss = 0.01482262\n",
      "Iteration 247, loss = 0.02720325\n",
      "Iteration 286, loss = 0.01542981\n",
      "Iteration 248, loss = 0.02571268\n",
      "Iteration 287, loss = 0.01494157\n",
      "Iteration 249, loss = 0.02712074\n",
      "Iteration 288, loss = 0.01405303\n",
      "Iteration 250, loss = 0.02336220\n",
      "Iteration 289, loss = 0.01427418\n",
      "Iteration 251, loss = 0.02511684\n",
      "Iteration 290, loss = 0.01450319\n",
      "Iteration 252, loss = 0.02738425\n",
      "Iteration 291, loss = 0.01436649\n",
      "Iteration 253, loss = 0.02548114\n",
      "Iteration 292, loss = 0.01476888\n",
      "Iteration 254, loss = 0.02484086\n",
      "Iteration 293, loss = 0.01508148\n",
      "Iteration 255, loss = 0.02476304\n",
      "Iteration 294, loss = 0.01505936\n",
      "Iteration 256, loss = 0.02495066\n",
      "Iteration 295, loss = 0.01448932\n",
      "Iteration 257, loss = 0.02187924\n",
      "Iteration 296, loss = 0.01460176\n",
      "Iteration 258, loss = 0.02448640\n",
      "Iteration 297, loss = 0.01283135\n",
      "Iteration 259, loss = 0.02257914\n",
      "Iteration 298, loss = 0.01340379\n",
      "Iteration 260, loss = 0.02516363\n",
      "Iteration 299, loss = 0.01300447\n",
      "Iteration 261, loss = 0.02399068\n",
      "Iteration 300, loss = 0.01285874\n",
      "Iteration 262, loss = 0.02265451\n",
      "Iteration 301, loss = 0.01239091\n",
      "Iteration 263, loss = 0.02195991\n",
      "Iteration 302, loss = 0.01251248\n",
      "Iteration 264, loss = 0.02202267\n",
      "Iteration 303, loss = 0.01210655\n",
      "Iteration 265, loss = 0.02088404\n",
      "Iteration 304, loss = 0.01269324\n",
      "Iteration 266, loss = 0.02174521\n",
      "Iteration 305, loss = 0.01212495\n",
      "Iteration 267, loss = 0.02264619\n",
      "Iteration 306, loss = 0.01203193\n",
      "Iteration 268, loss = 0.02102628\n",
      "Iteration 307, loss = 0.01162024\n",
      "Iteration 269, loss = 0.02117725\n",
      "Iteration 308, loss = 0.01163595\n",
      "Iteration 270, loss = 0.02124821\n",
      "Iteration 309, loss = 0.01163301\n",
      "Iteration 271, loss = 0.02123396\n",
      "Iteration 310, loss = 0.01104694\n",
      "Iteration 272, loss = 0.02290559\n",
      "Iteration 311, loss = 0.01153213\n",
      "Iteration 273, loss = 0.02180166\n",
      "Iteration 312, loss = 0.01119239\n",
      "Iteration 274, loss = 0.02257345\n",
      "Iteration 313, loss = 0.01117175\n",
      "Iteration 275, loss = 0.02736313\n",
      "Iteration 314, loss = 0.01120693\n",
      "Iteration 276, loss = 0.02541081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 315, loss = 0.01060588\n",
      "Iteration 316, loss = 0.01041218\n",
      "Iteration 317, loss = 0.01057262\n",
      "Iteration 318, loss = 0.01073909\n",
      "Iteration 319, loss = 0.01058175\n",
      "Iteration 320, loss = 0.01064045\n",
      "Iteration 321, loss = 0.01024095\n",
      "Iteration 322, loss = 0.01046784\n",
      "Iteration 323, loss = 0.01057759\n",
      "Iteration 324, loss = 0.01053319\n",
      "Iteration 325, loss = 0.00946426\n",
      "Iteration 326, loss = 0.00984900\n",
      "Iteration 327, loss = 0.00941925\n",
      "Iteration 1, loss = 1.24244825\n",
      "Iteration 328, loss = 0.00933024\n",
      "Iteration 2, loss = 0.88015640\n",
      "Iteration 329, loss = 0.00920318\n",
      "Iteration 3, loss = 0.71033093\n",
      "Iteration 330, loss = 0.00928167\n",
      "Iteration 4, loss = 0.60893050\n",
      "Iteration 331, loss = 0.00935105\n",
      "Iteration 5, loss = 0.54212484\n",
      "Iteration 332, loss = 0.00985560\n",
      "Iteration 6, loss = 0.49833783\n",
      "Iteration 333, loss = 0.00924325\n",
      "Iteration 7, loss = 0.46582541\n",
      "Iteration 334, loss = 0.00870849\n",
      "Iteration 8, loss = 0.44075779\n",
      "Iteration 335, loss = 0.00886839\n",
      "Iteration 9, loss = 0.41964223\n",
      "Iteration 336, loss = 0.00862102\n",
      "Iteration 10, loss = 0.40313982\n",
      "Iteration 337, loss = 0.00853254\n",
      "Iteration 11, loss = 0.38896114\n",
      "Iteration 338, loss = 0.00834019\n",
      "Iteration 12, loss = 0.37618494\n",
      "Iteration 339, loss = 0.00827553\n",
      "Iteration 13, loss = 0.36481437\n",
      "Iteration 340, loss = 0.00826133\n",
      "Iteration 14, loss = 0.35549258\n",
      "Iteration 341, loss = 0.00810948\n",
      "Iteration 15, loss = 0.34641622\n",
      "Iteration 342, loss = 0.00798042\n",
      "Iteration 16, loss = 0.33742376\n",
      "Iteration 343, loss = 0.00855978\n",
      "Iteration 17, loss = 0.32980809\n",
      "Iteration 344, loss = 0.00809826\n",
      "Iteration 18, loss = 0.32285663\n",
      "Iteration 345, loss = 0.00800386\n",
      "Iteration 19, loss = 0.31588129\n",
      "Iteration 346, loss = 0.00790884\n",
      "Iteration 20, loss = 0.30992621\n",
      "Iteration 347, loss = 0.00791234\n",
      "Iteration 21, loss = 0.30382853\n",
      "Iteration 348, loss = 0.00765436\n",
      "Iteration 22, loss = 0.29831652\n",
      "Iteration 349, loss = 0.00785873\n",
      "Iteration 23, loss = 0.29277661\n",
      "Iteration 350, loss = 0.00774517\n",
      "Iteration 24, loss = 0.28721614\n",
      "Iteration 351, loss = 0.00760828\n",
      "Iteration 25, loss = 0.28156294\n",
      "Iteration 352, loss = 0.00747476\n",
      "Iteration 26, loss = 0.27720092\n",
      "Iteration 353, loss = 0.00730948\n",
      "Iteration 27, loss = 0.27312983\n",
      "Iteration 354, loss = 0.00717938\n",
      "Iteration 28, loss = 0.26730543\n",
      "Iteration 355, loss = 0.00723731\n",
      "Iteration 29, loss = 0.26356119\n",
      "Iteration 356, loss = 0.00725968\n",
      "Iteration 30, loss = 0.25886814\n",
      "Iteration 357, loss = 0.00736676\n",
      "Iteration 31, loss = 0.25570581\n",
      "Iteration 358, loss = 0.00718299\n",
      "Iteration 32, loss = 0.25148141\n",
      "Iteration 33, loss = 0.24794405\n",
      "Iteration 359, loss = 0.00700340\n",
      "Iteration 34, loss = 0.24329443\n",
      "Iteration 360, loss = 0.00679932\n",
      "Iteration 35, loss = 0.24083695\n",
      "Iteration 361, loss = 0.00690016\n",
      "Iteration 36, loss = 0.23664902\n",
      "Iteration 362, loss = 0.00665088\n",
      "Iteration 37, loss = 0.23604577\n",
      "Iteration 363, loss = 0.00663070\n",
      "Iteration 38, loss = 0.23167073\n",
      "Iteration 364, loss = 0.00666524\n",
      "Iteration 365, loss = 0.00660146\n",
      "Iteration 39, loss = 0.22832748\n",
      "Iteration 40, loss = 0.22359237\n",
      "Iteration 366, loss = 0.00657543\n",
      "Iteration 41, loss = 0.22009542\n",
      "Iteration 367, loss = 0.00664512\n",
      "Iteration 42, loss = 0.21753794\n",
      "Iteration 368, loss = 0.00647742\n",
      "Iteration 43, loss = 0.21297864\n",
      "Iteration 369, loss = 0.00629042\n",
      "Iteration 44, loss = 0.21138404\n",
      "Iteration 370, loss = 0.00625362\n",
      "Iteration 45, loss = 0.20767338\n",
      "Iteration 371, loss = 0.00625839\n",
      "Iteration 46, loss = 0.20441762\n",
      "Iteration 372, loss = 0.00621458\n",
      "Iteration 47, loss = 0.20198281\n",
      "Iteration 373, loss = 0.00640285\n",
      "Iteration 48, loss = 0.19960063\n",
      "Iteration 374, loss = 0.00614795\n",
      "Iteration 49, loss = 0.19777729\n",
      "Iteration 375, loss = 0.00608105\n",
      "Iteration 50, loss = 0.19465498\n",
      "Iteration 376, loss = 0.00614239\n",
      "Iteration 51, loss = 0.19216821\n",
      "Iteration 377, loss = 0.00596072\n",
      "Iteration 52, loss = 0.19031184\n",
      "Iteration 378, loss = 0.00602169\n",
      "Iteration 53, loss = 0.18657641\n",
      "Iteration 379, loss = 0.00593053\n",
      "Iteration 54, loss = 0.18423589\n",
      "Iteration 380, loss = 0.00562928\n",
      "Iteration 55, loss = 0.18224921\n",
      "Iteration 381, loss = 0.00561721\n",
      "Iteration 56, loss = 0.17821807\n",
      "Iteration 382, loss = 0.00598262\n",
      "Iteration 57, loss = 0.17736191\n",
      "Iteration 383, loss = 0.00630421\n",
      "Iteration 58, loss = 0.18113789\n",
      "Iteration 384, loss = 0.00634533\n",
      "Iteration 59, loss = 0.17394222\n",
      "Iteration 385, loss = 0.00616029\n",
      "Iteration 60, loss = 0.16961387\n",
      "Iteration 386, loss = 0.00604718\n",
      "Iteration 61, loss = 0.16797347\n",
      "Iteration 387, loss = 0.00576171\n",
      "Iteration 62, loss = 0.16525818\n",
      "Iteration 388, loss = 0.00623881\n",
      "Iteration 63, loss = 0.16424812\n",
      "Iteration 389, loss = 0.00564383\n",
      "Iteration 64, loss = 0.16211456\n",
      "Iteration 390, loss = 0.00567960\n",
      "Iteration 65, loss = 0.15859372\n",
      "Iteration 391, loss = 0.00547368\n",
      "Iteration 66, loss = 0.15618473\n",
      "Iteration 392, loss = 0.00521972\n",
      "Iteration 67, loss = 0.15607385\n",
      "Iteration 393, loss = 0.00515072\n",
      "Iteration 68, loss = 0.15956986\n",
      "Iteration 394, loss = 0.00509078\n",
      "Iteration 69, loss = 0.15074670\n",
      "Iteration 395, loss = 0.00508305\n",
      "Iteration 70, loss = 0.14927852\n",
      "Iteration 396, loss = 0.00497780\n",
      "Iteration 71, loss = 0.14744314\n",
      "Iteration 397, loss = 0.00501990\n",
      "Iteration 72, loss = 0.14541393\n",
      "Iteration 398, loss = 0.00485804\n",
      "Iteration 73, loss = 0.14287828\n",
      "Iteration 399, loss = 0.00510156\n",
      "Iteration 74, loss = 0.14052383\n",
      "Iteration 400, loss = 0.00503587\n",
      "Iteration 75, loss = 0.13846922\n",
      "Iteration 401, loss = 0.00493145\n",
      "Iteration 76, loss = 0.13713670\n",
      "Iteration 402, loss = 0.00486930\n",
      "Iteration 77, loss = 0.13511079\n",
      "Iteration 403, loss = 0.00479421\n",
      "Iteration 78, loss = 0.13291118\n",
      "Iteration 404, loss = 0.00483142\n",
      "Iteration 79, loss = 0.13192335\n",
      "Iteration 405, loss = 0.00465876\n",
      "Iteration 80, loss = 0.13140095\n",
      "Iteration 406, loss = 0.00467057\n",
      "Iteration 81, loss = 0.12915439\n",
      "Iteration 407, loss = 0.00462116\n",
      "Iteration 82, loss = 0.12931478\n",
      "Iteration 408, loss = 0.00459116\n",
      "Iteration 83, loss = 0.12649521\n",
      "Iteration 409, loss = 0.00441287\n",
      "Iteration 84, loss = 0.12573870\n",
      "Iteration 410, loss = 0.00445586\n",
      "Iteration 85, loss = 0.12298802\n",
      "Iteration 411, loss = 0.00440301\n",
      "Iteration 86, loss = 0.12384899\n",
      "Iteration 412, loss = 0.00447487\n",
      "Iteration 87, loss = 0.12395901\n",
      "Iteration 413, loss = 0.00435322\n",
      "Iteration 88, loss = 0.11885147\n",
      "Iteration 414, loss = 0.00445606\n",
      "Iteration 89, loss = 0.11995274\n",
      "Iteration 415, loss = 0.00436279\n",
      "Iteration 90, loss = 0.11909812\n",
      "Iteration 416, loss = 0.00421596\n",
      "Iteration 91, loss = 0.11636393\n",
      "Iteration 417, loss = 0.00446918\n",
      "Iteration 92, loss = 0.11503210\n",
      "Iteration 418, loss = 0.00447295\n",
      "Iteration 93, loss = 0.11272875\n",
      "Iteration 419, loss = 0.00441987\n",
      "Iteration 94, loss = 0.11271759\n",
      "Iteration 420, loss = 0.00435643\n",
      "Iteration 95, loss = 0.10917954\n",
      "Iteration 421, loss = 0.00407837\n",
      "Iteration 96, loss = 0.11058381\n",
      "Iteration 422, loss = 0.00416828\n",
      "Iteration 97, loss = 0.10737922\n",
      "Iteration 423, loss = 0.00395192\n",
      "Iteration 98, loss = 0.10647195\n",
      "Iteration 424, loss = 0.00401888\n",
      "Iteration 99, loss = 0.10781472\n",
      "Iteration 425, loss = 0.00393980\n",
      "Iteration 100, loss = 0.11150929\n",
      "Iteration 426, loss = 0.00388394\n",
      "Iteration 101, loss = 0.10382528\n",
      "Iteration 427, loss = 0.00396396\n",
      "Iteration 102, loss = 0.10656151\n",
      "Iteration 428, loss = 0.00387622\n",
      "Iteration 103, loss = 0.10806233\n",
      "Iteration 429, loss = 0.00377375\n",
      "Iteration 104, loss = 0.10227135\n",
      "Iteration 430, loss = 0.00377818\n",
      "Iteration 105, loss = 0.10084363\n",
      "Iteration 431, loss = 0.00385657\n",
      "Iteration 106, loss = 0.10038629\n",
      "Iteration 432, loss = 0.00386858\n",
      "Iteration 107, loss = 0.10074967\n",
      "Iteration 433, loss = 0.00365459\n",
      "Iteration 108, loss = 0.09800787\n",
      "Iteration 434, loss = 0.00377493\n",
      "Iteration 109, loss = 0.09725032\n",
      "Iteration 435, loss = 0.00370270\n",
      "Iteration 110, loss = 0.09872183\n",
      "Iteration 436, loss = 0.00359702\n",
      "Iteration 111, loss = 0.09549120\n",
      "Iteration 437, loss = 0.00385014\n",
      "Iteration 112, loss = 0.09301825\n",
      "Iteration 438, loss = 0.00367575\n",
      "Iteration 113, loss = 0.09231017\n",
      "Iteration 439, loss = 0.00370071\n",
      "Iteration 114, loss = 0.09055521\n",
      "Iteration 440, loss = 0.00360226\n",
      "Iteration 115, loss = 0.08973541\n",
      "Iteration 441, loss = 0.00360947\n",
      "Iteration 116, loss = 0.08930165\n",
      "Iteration 442, loss = 0.00336875\n",
      "Iteration 117, loss = 0.08833067\n",
      "Iteration 443, loss = 0.00354611\n",
      "Iteration 118, loss = 0.08821140\n",
      "Iteration 444, loss = 0.00349268\n",
      "Iteration 119, loss = 0.08677748\n",
      "Iteration 445, loss = 0.00336285\n",
      "Iteration 120, loss = 0.08467847\n",
      "Iteration 446, loss = 0.00338528\n",
      "Iteration 121, loss = 0.08585062\n",
      "Iteration 447, loss = 0.00347177\n",
      "Iteration 122, loss = 0.08382978\n",
      "Iteration 448, loss = 0.00337196\n",
      "Iteration 123, loss = 0.08336167\n",
      "Iteration 449, loss = 0.00338109\n",
      "Iteration 124, loss = 0.08408124\n",
      "Iteration 450, loss = 0.00353360\n",
      "Iteration 125, loss = 0.08424420\n",
      "Iteration 451, loss = 0.00340631\n",
      "Iteration 126, loss = 0.08309322\n",
      "Iteration 452, loss = 0.00324911\n",
      "Iteration 127, loss = 0.08094325\n",
      "Iteration 453, loss = 0.00331210\n",
      "Iteration 128, loss = 0.07918374\n",
      "Iteration 454, loss = 0.00327213\n",
      "Iteration 129, loss = 0.07952698\n",
      "Iteration 455, loss = 0.00315666\n",
      "Iteration 130, loss = 0.07805160\n",
      "Iteration 456, loss = 0.00331764\n",
      "Iteration 131, loss = 0.07849121\n",
      "Iteration 457, loss = 0.00320086\n",
      "Iteration 132, loss = 0.07530027\n",
      "Iteration 458, loss = 0.00327652\n",
      "Iteration 133, loss = 0.07579335\n",
      "Iteration 459, loss = 0.00311307\n",
      "Iteration 134, loss = 0.07783368\n",
      "Iteration 460, loss = 0.00309944\n",
      "Iteration 135, loss = 0.07561771\n",
      "Iteration 461, loss = 0.00307100\n",
      "Iteration 136, loss = 0.07807535\n",
      "Iteration 462, loss = 0.00308723\n",
      "Iteration 137, loss = 0.07411695\n",
      "Iteration 463, loss = 0.00310726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 138, loss = 0.07699116\n",
      "Iteration 139, loss = 0.07500728\n",
      "Iteration 140, loss = 0.07334132\n",
      "Iteration 141, loss = 0.07117642\n",
      "Iteration 142, loss = 0.07063839\n",
      "Iteration 143, loss = 0.06902008\n",
      "Iteration 144, loss = 0.06909788\n",
      "Iteration 145, loss = 0.06932550\n",
      "Iteration 146, loss = 0.06692445\n",
      "Iteration 147, loss = 0.06652670\n",
      "Iteration 148, loss = 0.06413114\n",
      "Iteration 149, loss = 0.06518533\n",
      "Iteration 150, loss = 0.06435078\n",
      "Iteration 151, loss = 0.06296900\n",
      "Iteration 1, loss = 1.33361848\n",
      "Iteration 152, loss = 0.06317734\n",
      "Iteration 2, loss = 0.93433415\n",
      "Iteration 153, loss = 0.06225291\n",
      "Iteration 3, loss = 0.78705555\n",
      "Iteration 154, loss = 0.06297362\n",
      "Iteration 4, loss = 0.67390566\n",
      "Iteration 155, loss = 0.06014013\n",
      "Iteration 5, loss = 0.60105067\n",
      "Iteration 156, loss = 0.06148340\n",
      "Iteration 6, loss = 0.55750608\n",
      "Iteration 157, loss = 0.06027682\n",
      "Iteration 7, loss = 0.51897272\n",
      "Iteration 8, loss = 0.48640835\n",
      "Iteration 158, loss = 0.06138993\n",
      "Iteration 9, loss = 0.46321557\n",
      "Iteration 159, loss = 0.05908496\n",
      "Iteration 160, loss = 0.05842884\n",
      "Iteration 10, loss = 0.44337045\n",
      "Iteration 161, loss = 0.05739005\n",
      "Iteration 11, loss = 0.42497158\n",
      "Iteration 162, loss = 0.05672596\n",
      "Iteration 12, loss = 0.41059504\n",
      "Iteration 163, loss = 0.05671778\n",
      "Iteration 13, loss = 0.39733763\n",
      "Iteration 164, loss = 0.05736900\n",
      "Iteration 14, loss = 0.38472465\n",
      "Iteration 15, loss = 0.37503216\n",
      "Iteration 165, loss = 0.05499300\n",
      "Iteration 16, loss = 0.36464226\n",
      "Iteration 166, loss = 0.05400264\n",
      "Iteration 17, loss = 0.35564014\n",
      "Iteration 167, loss = 0.05310628\n",
      "Iteration 18, loss = 0.34716870\n",
      "Iteration 168, loss = 0.05406751\n",
      "Iteration 19, loss = 0.34045305\n",
      "Iteration 169, loss = 0.05470883\n",
      "Iteration 20, loss = 0.33233430\n",
      "Iteration 170, loss = 0.05293860\n",
      "Iteration 21, loss = 0.32696635\n",
      "Iteration 171, loss = 0.05280629\n",
      "Iteration 22, loss = 0.32043143\n",
      "Iteration 172, loss = 0.05533623\n",
      "Iteration 23, loss = 0.31524661\n",
      "Iteration 173, loss = 0.05532514\n",
      "Iteration 24, loss = 0.30876211\n",
      "Iteration 174, loss = 0.05406418\n",
      "Iteration 25, loss = 0.30301254\n",
      "Iteration 175, loss = 0.05313005\n",
      "Iteration 26, loss = 0.29863295\n",
      "Iteration 176, loss = 0.04969315\n",
      "Iteration 27, loss = 0.29399316\n",
      "Iteration 177, loss = 0.05133630\n",
      "Iteration 28, loss = 0.28892238\n",
      "Iteration 178, loss = 0.05240467\n",
      "Iteration 29, loss = 0.28452254\n",
      "Iteration 179, loss = 0.04934492\n",
      "Iteration 30, loss = 0.28059639\n",
      "Iteration 180, loss = 0.04893552\n",
      "Iteration 31, loss = 0.27556939\n",
      "Iteration 181, loss = 0.04814792\n",
      "Iteration 32, loss = 0.27172488\n",
      "Iteration 182, loss = 0.04807006\n",
      "Iteration 33, loss = 0.26835787\n",
      "Iteration 183, loss = 0.04744877\n",
      "Iteration 34, loss = 0.26466827\n",
      "Iteration 184, loss = 0.04728471\n",
      "Iteration 35, loss = 0.26102370\n",
      "Iteration 185, loss = 0.04871052\n",
      "Iteration 36, loss = 0.25875471\n",
      "Iteration 186, loss = 0.04717019\n",
      "Iteration 37, loss = 0.25371004\n",
      "Iteration 187, loss = 0.04592582\n",
      "Iteration 38, loss = 0.25049810\n",
      "Iteration 188, loss = 0.04424018\n",
      "Iteration 39, loss = 0.24684608\n",
      "Iteration 189, loss = 0.04552169\n",
      "Iteration 40, loss = 0.24426317\n",
      "Iteration 190, loss = 0.04542168\n",
      "Iteration 41, loss = 0.24357265\n",
      "Iteration 191, loss = 0.04306983\n",
      "Iteration 42, loss = 0.24139539\n",
      "Iteration 192, loss = 0.04392757\n",
      "Iteration 43, loss = 0.23492263\n",
      "Iteration 193, loss = 0.04313289\n",
      "Iteration 44, loss = 0.23484844\n",
      "Iteration 194, loss = 0.04139851\n",
      "Iteration 45, loss = 0.23096229\n",
      "Iteration 195, loss = 0.04144133\n",
      "Iteration 46, loss = 0.22696860\n",
      "Iteration 196, loss = 0.04021583\n",
      "Iteration 47, loss = 0.22482185\n",
      "Iteration 197, loss = 0.04003948\n",
      "Iteration 48, loss = 0.22170499\n",
      "Iteration 198, loss = 0.04176089\n",
      "Iteration 49, loss = 0.21883871\n",
      "Iteration 199, loss = 0.03941607\n",
      "Iteration 50, loss = 0.21588394\n",
      "Iteration 200, loss = 0.03993162\n",
      "Iteration 51, loss = 0.21388454\n",
      "Iteration 201, loss = 0.04041316\n",
      "Iteration 52, loss = 0.21054773\n",
      "Iteration 202, loss = 0.04060666\n",
      "Iteration 53, loss = 0.20937586\n",
      "Iteration 203, loss = 0.04186800\n",
      "Iteration 54, loss = 0.20928953\n",
      "Iteration 204, loss = 0.04078745\n",
      "Iteration 55, loss = 0.20771626\n",
      "Iteration 205, loss = 0.03878399\n",
      "Iteration 56, loss = 0.20364095\n",
      "Iteration 206, loss = 0.03701809\n",
      "Iteration 57, loss = 0.20051763\n",
      "Iteration 207, loss = 0.03687436\n",
      "Iteration 58, loss = 0.19852087\n",
      "Iteration 208, loss = 0.03735379\n",
      "Iteration 59, loss = 0.19513282\n",
      "Iteration 209, loss = 0.03676971\n",
      "Iteration 60, loss = 0.19263486\n",
      "Iteration 210, loss = 0.03627605\n",
      "Iteration 61, loss = 0.18998963\n",
      "Iteration 211, loss = 0.03816666\n",
      "Iteration 62, loss = 0.18762672\n",
      "Iteration 212, loss = 0.03702140\n",
      "Iteration 63, loss = 0.18625187\n",
      "Iteration 213, loss = 0.03645813\n",
      "Iteration 64, loss = 0.18454370\n",
      "Iteration 214, loss = 0.03560987\n",
      "Iteration 65, loss = 0.18056925\n",
      "Iteration 215, loss = 0.03427050\n",
      "Iteration 66, loss = 0.18002414\n",
      "Iteration 216, loss = 0.03368735\n",
      "Iteration 67, loss = 0.17823411\n",
      "Iteration 217, loss = 0.03456964\n",
      "Iteration 68, loss = 0.17704878\n",
      "Iteration 218, loss = 0.03361949\n",
      "Iteration 69, loss = 0.17443079\n",
      "Iteration 219, loss = 0.03392258\n",
      "Iteration 70, loss = 0.17375677\n",
      "Iteration 220, loss = 0.03345015\n",
      "Iteration 71, loss = 0.17232809\n",
      "Iteration 221, loss = 0.03453954\n",
      "Iteration 72, loss = 0.16937486\n",
      "Iteration 222, loss = 0.03184498\n",
      "Iteration 73, loss = 0.16615964\n",
      "Iteration 223, loss = 0.03147953\n",
      "Iteration 74, loss = 0.16440703\n",
      "Iteration 224, loss = 0.03116902\n",
      "Iteration 75, loss = 0.16190917\n",
      "Iteration 225, loss = 0.03168138\n",
      "Iteration 76, loss = 0.16069106\n",
      "Iteration 226, loss = 0.03054961\n",
      "Iteration 77, loss = 0.15786263\n",
      "Iteration 227, loss = 0.03086865\n",
      "Iteration 78, loss = 0.15610298\n",
      "Iteration 228, loss = 0.03024378\n",
      "Iteration 79, loss = 0.15444207\n",
      "Iteration 229, loss = 0.03037311\n",
      "Iteration 80, loss = 0.15483799\n",
      "Iteration 230, loss = 0.03032340\n",
      "Iteration 81, loss = 0.15241281\n",
      "Iteration 231, loss = 0.03066890\n",
      "Iteration 82, loss = 0.14872551\n",
      "Iteration 232, loss = 0.02913722\n",
      "Iteration 83, loss = 0.14997431\n",
      "Iteration 233, loss = 0.02826111\n",
      "Iteration 84, loss = 0.15012543\n",
      "Iteration 234, loss = 0.02890175\n",
      "Iteration 85, loss = 0.14466980\n",
      "Iteration 235, loss = 0.02780214\n",
      "Iteration 86, loss = 0.14401363\n",
      "Iteration 236, loss = 0.02833529\n",
      "Iteration 87, loss = 0.14268607\n",
      "Iteration 237, loss = 0.02964188\n",
      "Iteration 88, loss = 0.14230055\n",
      "Iteration 238, loss = 0.03274427\n",
      "Iteration 89, loss = 0.13901402\n",
      "Iteration 239, loss = 0.03139098\n",
      "Iteration 90, loss = 0.13764265\n",
      "Iteration 240, loss = 0.02974628\n",
      "Iteration 91, loss = 0.13651374\n",
      "Iteration 241, loss = 0.02925693\n",
      "Iteration 92, loss = 0.13610181\n",
      "Iteration 242, loss = 0.02700798\n",
      "Iteration 93, loss = 0.13405148\n",
      "Iteration 243, loss = 0.02803168\n",
      "Iteration 94, loss = 0.13181210\n",
      "Iteration 244, loss = 0.02623483\n",
      "Iteration 95, loss = 0.13270052\n",
      "Iteration 245, loss = 0.02604340\n",
      "Iteration 96, loss = 0.13061169\n",
      "Iteration 246, loss = 0.02595201\n",
      "Iteration 97, loss = 0.12738522\n",
      "Iteration 247, loss = 0.02504821\n",
      "Iteration 98, loss = 0.12811609\n",
      "Iteration 248, loss = 0.02497333\n",
      "Iteration 99, loss = 0.12810751\n",
      "Iteration 249, loss = 0.02530205\n",
      "Iteration 100, loss = 0.12745403\n",
      "Iteration 250, loss = 0.02662206\n",
      "Iteration 101, loss = 0.12724321\n",
      "Iteration 251, loss = 0.02576039\n",
      "Iteration 102, loss = 0.12538435\n",
      "Iteration 252, loss = 0.02531756\n",
      "Iteration 103, loss = 0.12360597\n",
      "Iteration 253, loss = 0.02523529\n",
      "Iteration 104, loss = 0.11976294\n",
      "Iteration 254, loss = 0.02713513\n",
      "Iteration 105, loss = 0.11966746\n",
      "Iteration 255, loss = 0.02571037\n",
      "Iteration 106, loss = 0.11812402\n",
      "Iteration 256, loss = 0.02415653\n",
      "Iteration 107, loss = 0.12035401\n",
      "Iteration 257, loss = 0.02496499\n",
      "Iteration 108, loss = 0.11737194\n",
      "Iteration 109, loss = 0.12295369\n",
      "Iteration 258, loss = 0.02343126\n",
      "Iteration 110, loss = 0.12285312\n",
      "Iteration 259, loss = 0.02437524\n",
      "Iteration 111, loss = 0.11702111\n",
      "Iteration 260, loss = 0.02388575\n",
      "Iteration 112, loss = 0.11458223\n",
      "Iteration 261, loss = 0.02158699\n",
      "Iteration 113, loss = 0.11366061\n",
      "Iteration 262, loss = 0.02269172\n",
      "Iteration 114, loss = 0.10968814\n",
      "Iteration 263, loss = 0.02218611\n",
      "Iteration 115, loss = 0.10838457\n",
      "Iteration 264, loss = 0.02277313\n",
      "Iteration 116, loss = 0.10776051\n",
      "Iteration 265, loss = 0.02401687\n",
      "Iteration 117, loss = 0.10732983\n",
      "Iteration 266, loss = 0.02297744\n",
      "Iteration 118, loss = 0.10634173\n",
      "Iteration 267, loss = 0.02253480\n",
      "Iteration 119, loss = 0.10518213\n",
      "Iteration 268, loss = 0.02174590\n",
      "Iteration 120, loss = 0.10400457\n",
      "Iteration 269, loss = 0.02177652\n",
      "Iteration 121, loss = 0.10362694\n",
      "Iteration 270, loss = 0.02064226\n",
      "Iteration 122, loss = 0.10157917\n",
      "Iteration 271, loss = 0.02009073\n",
      "Iteration 123, loss = 0.10321169\n",
      "Iteration 272, loss = 0.02086482\n",
      "Iteration 124, loss = 0.10369090\n",
      "Iteration 273, loss = 0.02011776\n",
      "Iteration 125, loss = 0.10551851\n",
      "Iteration 274, loss = 0.02101536\n",
      "Iteration 126, loss = 0.10470294\n",
      "Iteration 275, loss = 0.02124870\n",
      "Iteration 127, loss = 0.10417348\n",
      "Iteration 276, loss = 0.02045832\n",
      "Iteration 128, loss = 0.10134487\n",
      "Iteration 277, loss = 0.02054485\n",
      "Iteration 129, loss = 0.09997397\n",
      "Iteration 278, loss = 0.01967085\n",
      "Iteration 130, loss = 0.09770462\n",
      "Iteration 279, loss = 0.01983982\n",
      "Iteration 131, loss = 0.09707646\n",
      "Iteration 280, loss = 0.01946591\n",
      "Iteration 132, loss = 0.09334832\n",
      "Iteration 281, loss = 0.01849018\n",
      "Iteration 133, loss = 0.09352181\n",
      "Iteration 282, loss = 0.01955590\n",
      "Iteration 134, loss = 0.09221200\n",
      "Iteration 283, loss = 0.01943500\n",
      "Iteration 135, loss = 0.09124946\n",
      "Iteration 284, loss = 0.01872424\n",
      "Iteration 136, loss = 0.09062894\n",
      "Iteration 285, loss = 0.01808848\n",
      "Iteration 137, loss = 0.09261570\n",
      "Iteration 286, loss = 0.01874596\n",
      "Iteration 138, loss = 0.09208512\n",
      "Iteration 287, loss = 0.01771209\n",
      "Iteration 139, loss = 0.08872898\n",
      "Iteration 288, loss = 0.01725651\n",
      "Iteration 140, loss = 0.08942764\n",
      "Iteration 289, loss = 0.01746681\n",
      "Iteration 141, loss = 0.08936164\n",
      "Iteration 290, loss = 0.01728572\n",
      "Iteration 142, loss = 0.08860345\n",
      "Iteration 291, loss = 0.01699726\n",
      "Iteration 143, loss = 0.08604977\n",
      "Iteration 292, loss = 0.01740947\n",
      "Iteration 144, loss = 0.08807201\n",
      "Iteration 293, loss = 0.01731113\n",
      "Iteration 145, loss = 0.08515175\n",
      "Iteration 294, loss = 0.01686799\n",
      "Iteration 146, loss = 0.09004897\n",
      "Iteration 295, loss = 0.01736791\n",
      "Iteration 147, loss = 0.08951690\n",
      "Iteration 296, loss = 0.01692400\n",
      "Iteration 148, loss = 0.08305366\n",
      "Iteration 297, loss = 0.01678998\n",
      "Iteration 149, loss = 0.08793399\n",
      "Iteration 298, loss = 0.01662561\n",
      "Iteration 150, loss = 0.08225930\n",
      "Iteration 299, loss = 0.01584379\n",
      "Iteration 151, loss = 0.08486355\n",
      "Iteration 300, loss = 0.01585373\n",
      "Iteration 152, loss = 0.08281961\n",
      "Iteration 301, loss = 0.01582659\n",
      "Iteration 153, loss = 0.08427716\n",
      "Iteration 302, loss = 0.01610334\n",
      "Iteration 303, loss = 0.01679391\n",
      "Iteration 154, loss = 0.08033872\n",
      "Iteration 304, loss = 0.01681306\n",
      "Iteration 155, loss = 0.07779737\n",
      "Iteration 305, loss = 0.02267981\n",
      "Iteration 156, loss = 0.07744544\n",
      "Iteration 306, loss = 0.02424125\n",
      "Iteration 157, loss = 0.07755634\n",
      "Iteration 307, loss = 0.01810085\n",
      "Iteration 158, loss = 0.07856361\n",
      "Iteration 308, loss = 0.01537600\n",
      "Iteration 159, loss = 0.08051171\n",
      "Iteration 309, loss = 0.01584067\n",
      "Iteration 160, loss = 0.07879576\n",
      "Iteration 310, loss = 0.01460197\n",
      "Iteration 161, loss = 0.07755354\n",
      "Iteration 311, loss = 0.01416350\n",
      "Iteration 162, loss = 0.07545200\n",
      "Iteration 312, loss = 0.01466830\n",
      "Iteration 163, loss = 0.07349886\n",
      "Iteration 313, loss = 0.01488152\n",
      "Iteration 164, loss = 0.07224341\n",
      "Iteration 314, loss = 0.01826523\n",
      "Iteration 165, loss = 0.07306227\n",
      "Iteration 315, loss = 0.01848157\n",
      "Iteration 166, loss = 0.07052594\n",
      "Iteration 316, loss = 0.02335295\n",
      "Iteration 167, loss = 0.07066712\n",
      "Iteration 317, loss = 0.02398466\n",
      "Iteration 168, loss = 0.07129209\n",
      "Iteration 318, loss = 0.02263812\n",
      "Iteration 169, loss = 0.06976914\n",
      "Iteration 319, loss = 0.01870956\n",
      "Iteration 170, loss = 0.06970405\n",
      "Iteration 320, loss = 0.01605132\n",
      "Iteration 171, loss = 0.07073346\n",
      "Iteration 321, loss = 0.01621361\n",
      "Iteration 172, loss = 0.07085961\n",
      "Iteration 322, loss = 0.01401093\n",
      "Iteration 173, loss = 0.06963241\n",
      "Iteration 323, loss = 0.01373883\n",
      "Iteration 174, loss = 0.06785360\n",
      "Iteration 324, loss = 0.01352864\n",
      "Iteration 175, loss = 0.06690239\n",
      "Iteration 325, loss = 0.01336188\n",
      "Iteration 176, loss = 0.06791434\n",
      "Iteration 326, loss = 0.01357243\n",
      "Iteration 177, loss = 0.06680336\n",
      "Iteration 178, loss = 0.06510428\n",
      "Iteration 327, loss = 0.01281836\n",
      "Iteration 328, loss = 0.01341889Iteration 179, loss = 0.06708183\n",
      "\n",
      "Iteration 180, loss = 0.06572301\n",
      "Iteration 329, loss = 0.01245805\n",
      "Iteration 330, loss = 0.01199591\n",
      "Iteration 181, loss = 0.06410332\n",
      "Iteration 331, loss = 0.01250982\n",
      "Iteration 182, loss = 0.06325973\n",
      "Iteration 332, loss = 0.01280075\n",
      "Iteration 183, loss = 0.06548174\n",
      "Iteration 333, loss = 0.01251762\n",
      "Iteration 184, loss = 0.06638474\n",
      "Iteration 334, loss = 0.01230374\n",
      "Iteration 185, loss = 0.06172693\n",
      "Iteration 335, loss = 0.01202377\n",
      "Iteration 186, loss = 0.06151668\n",
      "Iteration 336, loss = 0.01212073\n",
      "Iteration 187, loss = 0.06121437\n",
      "Iteration 337, loss = 0.01172697\n",
      "Iteration 188, loss = 0.06018029\n",
      "Iteration 338, loss = 0.01141672\n",
      "Iteration 189, loss = 0.05956121\n",
      "Iteration 339, loss = 0.01149232\n",
      "Iteration 190, loss = 0.05862770\n",
      "Iteration 340, loss = 0.01121300\n",
      "Iteration 191, loss = 0.05831325\n",
      "Iteration 341, loss = 0.01127675\n",
      "Iteration 192, loss = 0.05908462\n",
      "Iteration 342, loss = 0.01098965\n",
      "Iteration 193, loss = 0.06354646\n",
      "Iteration 343, loss = 0.01122263\n",
      "Iteration 194, loss = 0.05643483\n",
      "Iteration 344, loss = 0.01134916\n",
      "Iteration 195, loss = 0.05652320\n",
      "Iteration 345, loss = 0.01109249\n",
      "Iteration 196, loss = 0.05621130\n",
      "Iteration 346, loss = 0.01168668\n",
      "Iteration 197, loss = 0.05731730\n",
      "Iteration 347, loss = 0.01130234\n",
      "Iteration 198, loss = 0.05707819\n",
      "Iteration 348, loss = 0.01140242\n",
      "Iteration 199, loss = 0.05660000\n",
      "Iteration 349, loss = 0.01287915\n",
      "Iteration 200, loss = 0.05940971\n",
      "Iteration 350, loss = 0.01325656\n",
      "Iteration 201, loss = 0.05417219\n",
      "Iteration 351, loss = 0.01156454\n",
      "Iteration 202, loss = 0.05562699\n",
      "Iteration 352, loss = 0.01121162\n",
      "Iteration 203, loss = 0.05328722\n",
      "Iteration 353, loss = 0.01150389\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 204, loss = 0.05357073\n",
      "Iteration 205, loss = 0.05175806\n",
      "Iteration 206, loss = 0.05424882\n",
      "Iteration 207, loss = 0.05657684\n",
      "Iteration 208, loss = 0.05715933\n",
      "Iteration 209, loss = 0.05272715\n",
      "Iteration 210, loss = 0.05378760\n",
      "Iteration 211, loss = 0.05633358\n",
      "Iteration 212, loss = 0.05333122\n",
      "Iteration 213, loss = 0.04902532\n",
      "Iteration 214, loss = 0.05064950\n",
      "Iteration 215, loss = 0.05062521\n",
      "Iteration 216, loss = 0.05018227\n",
      "Iteration 1, loss = 1.10961587\n",
      "Iteration 217, loss = 0.05053580\n",
      "Iteration 2, loss = 0.89653582\n",
      "Iteration 218, loss = 0.04795348\n",
      "Iteration 219, loss = 0.04892401\n",
      "Iteration 3, loss = 0.73630225\n",
      "Iteration 220, loss = 0.04818715\n",
      "Iteration 4, loss = 0.64029847\n",
      "Iteration 221, loss = 0.04658468\n",
      "Iteration 5, loss = 0.56740189\n",
      "Iteration 222, loss = 0.04560465\n",
      "Iteration 6, loss = 0.51684200\n",
      "Iteration 223, loss = 0.04611037\n",
      "Iteration 7, loss = 0.48177192\n",
      "Iteration 224, loss = 0.04506621\n",
      "Iteration 8, loss = 0.45202877\n",
      "Iteration 225, loss = 0.04383294\n",
      "Iteration 9, loss = 0.43044277\n",
      "Iteration 226, loss = 0.04506438\n",
      "Iteration 10, loss = 0.41013238\n",
      "Iteration 227, loss = 0.04579085\n",
      "Iteration 11, loss = 0.39392647\n",
      "Iteration 228, loss = 0.04496550\n",
      "Iteration 12, loss = 0.38050661\n",
      "Iteration 229, loss = 0.04370198\n",
      "Iteration 13, loss = 0.36812925\n",
      "Iteration 230, loss = 0.04246634\n",
      "Iteration 14, loss = 0.35812565\n",
      "Iteration 231, loss = 0.04474977\n",
      "Iteration 15, loss = 0.34876953\n",
      "Iteration 232, loss = 0.04581638\n",
      "Iteration 16, loss = 0.33983441\n",
      "Iteration 233, loss = 0.04478751\n",
      "Iteration 17, loss = 0.33229656\n",
      "Iteration 234, loss = 0.04135715\n",
      "Iteration 18, loss = 0.32469142\n",
      "Iteration 235, loss = 0.04156665\n",
      "Iteration 19, loss = 0.31823835\n",
      "Iteration 236, loss = 0.04313700\n",
      "Iteration 20, loss = 0.31240305\n",
      "Iteration 237, loss = 0.04290896\n",
      "Iteration 21, loss = 0.30573115\n",
      "Iteration 238, loss = 0.03934667\n",
      "Iteration 22, loss = 0.30147693\n",
      "Iteration 239, loss = 0.03996386\n",
      "Iteration 23, loss = 0.29515587\n",
      "Iteration 240, loss = 0.04004196\n",
      "Iteration 24, loss = 0.29030932\n",
      "Iteration 241, loss = 0.04181441\n",
      "Iteration 25, loss = 0.28516410\n",
      "Iteration 242, loss = 0.04155259\n",
      "Iteration 26, loss = 0.27997686\n",
      "Iteration 243, loss = 0.04612283\n",
      "Iteration 27, loss = 0.27606168\n",
      "Iteration 244, loss = 0.04269831\n",
      "Iteration 28, loss = 0.27092209\n",
      "Iteration 245, loss = 0.04458401\n",
      "Iteration 29, loss = 0.26737664\n",
      "Iteration 246, loss = 0.04558929\n",
      "Iteration 30, loss = 0.26251554\n",
      "Iteration 247, loss = 0.04307078\n",
      "Iteration 31, loss = 0.25916675\n",
      "Iteration 248, loss = 0.04487389\n",
      "Iteration 32, loss = 0.25632342\n",
      "Iteration 249, loss = 0.04566877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.25119102\n",
      "Iteration 34, loss = 0.24740730\n",
      "Iteration 35, loss = 0.24455980\n",
      "Iteration 36, loss = 0.24202701\n",
      "Iteration 37, loss = 0.23781435\n",
      "Iteration 38, loss = 0.23490442\n",
      "Iteration 39, loss = 0.23016822\n",
      "Iteration 40, loss = 0.22814972\n",
      "Iteration 41, loss = 0.22546022\n",
      "Iteration 42, loss = 0.22330590\n",
      "Iteration 43, loss = 0.21726626\n",
      "Iteration 44, loss = 0.21707815\n",
      "Iteration 45, loss = 0.21137819\n",
      "Iteration 1, loss = 1.31875141\n",
      "Iteration 46, loss = 0.20896613\n",
      "Iteration 2, loss = 0.95023132\n",
      "Iteration 47, loss = 0.20563958\n",
      "Iteration 3, loss = 0.75015151\n",
      "Iteration 4, loss = 0.64992874\n",
      "Iteration 48, loss = 0.20348980\n",
      "Iteration 5, loss = 0.57101427\n",
      "Iteration 49, loss = 0.20052271\n",
      "Iteration 6, loss = 0.52561850\n",
      "Iteration 50, loss = 0.19828984\n",
      "Iteration 7, loss = 0.48678575\n",
      "Iteration 51, loss = 0.19560553\n",
      "Iteration 8, loss = 0.45909374\n",
      "Iteration 52, loss = 0.19218223\n",
      "Iteration 9, loss = 0.43607541\n",
      "Iteration 53, loss = 0.19031678\n",
      "Iteration 10, loss = 0.41739070\n",
      "Iteration 54, loss = 0.18792566\n",
      "Iteration 11, loss = 0.40150412\n",
      "Iteration 55, loss = 0.18512361\n",
      "Iteration 12, loss = 0.38913509\n",
      "Iteration 56, loss = 0.18270013\n",
      "Iteration 13, loss = 0.37548410\n",
      "Iteration 57, loss = 0.17995264\n",
      "Iteration 14, loss = 0.36421852\n",
      "Iteration 58, loss = 0.17910335\n",
      "Iteration 15, loss = 0.35501217\n",
      "Iteration 59, loss = 0.17729211\n",
      "Iteration 16, loss = 0.34628325\n",
      "Iteration 60, loss = 0.17409865\n",
      "Iteration 17, loss = 0.33740958\n",
      "Iteration 61, loss = 0.17300545\n",
      "Iteration 18, loss = 0.32974360\n",
      "Iteration 62, loss = 0.16723550\n",
      "Iteration 19, loss = 0.32167429\n",
      "Iteration 63, loss = 0.16599559\n",
      "Iteration 20, loss = 0.31604386\n",
      "Iteration 64, loss = 0.16607477\n",
      "Iteration 21, loss = 0.30855189\n",
      "Iteration 65, loss = 0.16083873\n",
      "Iteration 66, loss = 0.16329534\n",
      "Iteration 22, loss = 0.30238658\n",
      "Iteration 67, loss = 0.15718709\n",
      "Iteration 23, loss = 0.29756968\n",
      "Iteration 24, loss = 0.29025233\n",
      "Iteration 68, loss = 0.15612561\n",
      "Iteration 25, loss = 0.28621978\n",
      "Iteration 69, loss = 0.15524165\n",
      "Iteration 26, loss = 0.28023898\n",
      "Iteration 70, loss = 0.15264397\n",
      "Iteration 27, loss = 0.27655064\n",
      "Iteration 71, loss = 0.15061614\n",
      "Iteration 28, loss = 0.27212908\n",
      "Iteration 72, loss = 0.14881230\n",
      "Iteration 29, loss = 0.26749062\n",
      "Iteration 73, loss = 0.14584960\n",
      "Iteration 30, loss = 0.26457991\n",
      "Iteration 74, loss = 0.14553678\n",
      "Iteration 31, loss = 0.26119478\n",
      "Iteration 75, loss = 0.14354650\n",
      "Iteration 32, loss = 0.25663732\n",
      "Iteration 76, loss = 0.14400991\n",
      "Iteration 33, loss = 0.25181281\n",
      "Iteration 77, loss = 0.14195577\n",
      "Iteration 34, loss = 0.24699177\n",
      "Iteration 78, loss = 0.13780377\n",
      "Iteration 35, loss = 0.24411929\n",
      "Iteration 79, loss = 0.13993211\n",
      "Iteration 36, loss = 0.24059215\n",
      "Iteration 80, loss = 0.13623169\n",
      "Iteration 37, loss = 0.23663154\n",
      "Iteration 81, loss = 0.13331423\n",
      "Iteration 38, loss = 0.23304237\n",
      "Iteration 82, loss = 0.13217762\n",
      "Iteration 39, loss = 0.23085465\n",
      "Iteration 83, loss = 0.13456205\n",
      "Iteration 40, loss = 0.22596041\n",
      "Iteration 84, loss = 0.13184635\n",
      "Iteration 41, loss = 0.22459726\n",
      "Iteration 85, loss = 0.13412237\n",
      "Iteration 42, loss = 0.22028553\n",
      "Iteration 86, loss = 0.12904329\n",
      "Iteration 43, loss = 0.21794148\n",
      "Iteration 87, loss = 0.12597099\n",
      "Iteration 44, loss = 0.21373992\n",
      "Iteration 88, loss = 0.12566475\n",
      "Iteration 45, loss = 0.21055880\n",
      "Iteration 89, loss = 0.12243181\n",
      "Iteration 46, loss = 0.20939405\n",
      "Iteration 90, loss = 0.12332881\n",
      "Iteration 47, loss = 0.20769040\n",
      "Iteration 91, loss = 0.12070958\n",
      "Iteration 48, loss = 0.20367422\n",
      "Iteration 92, loss = 0.11908846\n",
      "Iteration 49, loss = 0.20056774\n",
      "Iteration 93, loss = 0.12376236\n",
      "Iteration 50, loss = 0.19826128\n",
      "Iteration 94, loss = 0.12025398\n",
      "Iteration 51, loss = 0.19527927\n",
      "Iteration 95, loss = 0.11868365\n",
      "Iteration 52, loss = 0.19178731\n",
      "Iteration 96, loss = 0.11655633\n",
      "Iteration 53, loss = 0.19098844\n",
      "Iteration 97, loss = 0.11322665\n",
      "Iteration 54, loss = 0.19039632\n",
      "Iteration 98, loss = 0.11545188\n",
      "Iteration 55, loss = 0.18772574\n",
      "Iteration 99, loss = 0.11510394\n",
      "Iteration 56, loss = 0.18780186\n",
      "Iteration 100, loss = 0.11100354\n",
      "Iteration 57, loss = 0.18073418\n",
      "Iteration 101, loss = 0.10923620\n",
      "Iteration 58, loss = 0.17989074\n",
      "Iteration 102, loss = 0.10838710\n",
      "Iteration 59, loss = 0.18016556\n",
      "Iteration 103, loss = 0.10609054\n",
      "Iteration 60, loss = 0.17336845\n",
      "Iteration 104, loss = 0.10446115\n",
      "Iteration 61, loss = 0.17607368\n",
      "Iteration 105, loss = 0.10810547\n",
      "Iteration 62, loss = 0.16827477\n",
      "Iteration 106, loss = 0.10508609\n",
      "Iteration 63, loss = 0.16574746\n",
      "Iteration 107, loss = 0.10547498\n",
      "Iteration 64, loss = 0.16328543\n",
      "Iteration 108, loss = 0.10367178\n",
      "Iteration 65, loss = 0.16103409\n",
      "Iteration 109, loss = 0.09959183\n",
      "Iteration 66, loss = 0.16091262\n",
      "Iteration 110, loss = 0.09897890\n",
      "Iteration 67, loss = 0.15750459\n",
      "Iteration 111, loss = 0.09760321\n",
      "Iteration 68, loss = 0.15849888\n",
      "Iteration 112, loss = 0.09754854\n",
      "Iteration 69, loss = 0.15606803\n",
      "Iteration 113, loss = 0.09628901\n",
      "Iteration 70, loss = 0.15041616\n",
      "Iteration 114, loss = 0.09753482\n",
      "Iteration 71, loss = 0.15101526\n",
      "Iteration 115, loss = 0.09571381\n",
      "Iteration 72, loss = 0.15219954\n",
      "Iteration 116, loss = 0.09672219\n",
      "Iteration 73, loss = 0.14687048\n",
      "Iteration 117, loss = 0.09293438\n",
      "Iteration 74, loss = 0.14869145\n",
      "Iteration 118, loss = 0.09064906\n",
      "Iteration 75, loss = 0.14979263\n",
      "Iteration 119, loss = 0.09285878\n",
      "Iteration 76, loss = 0.14333662\n",
      "Iteration 120, loss = 0.09747533\n",
      "Iteration 77, loss = 0.14146666\n",
      "Iteration 121, loss = 0.10060490\n",
      "Iteration 78, loss = 0.14148024\n",
      "Iteration 122, loss = 0.09632355\n",
      "Iteration 79, loss = 0.14027971\n",
      "Iteration 123, loss = 0.09010613\n",
      "Iteration 80, loss = 0.13601685\n",
      "Iteration 124, loss = 0.09161882\n",
      "Iteration 81, loss = 0.13252887\n",
      "Iteration 125, loss = 0.08599673\n",
      "Iteration 82, loss = 0.13297825\n",
      "Iteration 126, loss = 0.08629805\n",
      "Iteration 83, loss = 0.13128333\n",
      "Iteration 127, loss = 0.08689405\n",
      "Iteration 84, loss = 0.12933426\n",
      "Iteration 128, loss = 0.08615763\n",
      "Iteration 85, loss = 0.12839451\n",
      "Iteration 129, loss = 0.08427989\n",
      "Iteration 86, loss = 0.12713371\n",
      "Iteration 130, loss = 0.08349592\n",
      "Iteration 87, loss = 0.12381761\n",
      "Iteration 131, loss = 0.08267423\n",
      "Iteration 88, loss = 0.12351111\n",
      "Iteration 132, loss = 0.08023544\n",
      "Iteration 89, loss = 0.12145564\n",
      "Iteration 133, loss = 0.07956863\n",
      "Iteration 90, loss = 0.12097307\n",
      "Iteration 134, loss = 0.08012693\n",
      "Iteration 91, loss = 0.11948178\n",
      "Iteration 135, loss = 0.07790835\n",
      "Iteration 92, loss = 0.12315828\n",
      "Iteration 136, loss = 0.08077106\n",
      "Iteration 93, loss = 0.11640084\n",
      "Iteration 137, loss = 0.08385912\n",
      "Iteration 94, loss = 0.11617076\n",
      "Iteration 138, loss = 0.07940454\n",
      "Iteration 95, loss = 0.11636534\n",
      "Iteration 139, loss = 0.07533637\n",
      "Iteration 96, loss = 0.11476891\n",
      "Iteration 140, loss = 0.07465807\n",
      "Iteration 97, loss = 0.11049976\n",
      "Iteration 141, loss = 0.07648605\n",
      "Iteration 98, loss = 0.11333168\n",
      "Iteration 142, loss = 0.07448369\n",
      "Iteration 99, loss = 0.11066892\n",
      "Iteration 143, loss = 0.07302103\n",
      "Iteration 100, loss = 0.10709782\n",
      "Iteration 144, loss = 0.07076545\n",
      "Iteration 101, loss = 0.11033539\n",
      "Iteration 145, loss = 0.06978664\n",
      "Iteration 102, loss = 0.10693978\n",
      "Iteration 146, loss = 0.07025908\n",
      "Iteration 103, loss = 0.10576665\n",
      "Iteration 147, loss = 0.06967544\n",
      "Iteration 104, loss = 0.10450697\n",
      "Iteration 148, loss = 0.06889485\n",
      "Iteration 105, loss = 0.10401209\n",
      "Iteration 149, loss = 0.06750952\n",
      "Iteration 106, loss = 0.10225739\n",
      "Iteration 150, loss = 0.06823478\n",
      "Iteration 107, loss = 0.10146344\n",
      "Iteration 151, loss = 0.06509732\n",
      "Iteration 108, loss = 0.10297320\n",
      "Iteration 152, loss = 0.06852582\n",
      "Iteration 109, loss = 0.09916395\n",
      "Iteration 153, loss = 0.06826317\n",
      "Iteration 110, loss = 0.09947934\n",
      "Iteration 154, loss = 0.06687505\n",
      "Iteration 111, loss = 0.09650152\n",
      "Iteration 155, loss = 0.06882614\n",
      "Iteration 112, loss = 0.09629067\n",
      "Iteration 156, loss = 0.06733753\n",
      "Iteration 113, loss = 0.09483108\n",
      "Iteration 157, loss = 0.06360695\n",
      "Iteration 114, loss = 0.09424623\n",
      "Iteration 158, loss = 0.06599477\n",
      "Iteration 115, loss = 0.09562476\n",
      "Iteration 159, loss = 0.06257209\n",
      "Iteration 116, loss = 0.09205915\n",
      "Iteration 160, loss = 0.06124728\n",
      "Iteration 117, loss = 0.09166059\n",
      "Iteration 161, loss = 0.05969689\n",
      "Iteration 118, loss = 0.09027882\n",
      "Iteration 162, loss = 0.06017315\n",
      "Iteration 119, loss = 0.08947816\n",
      "Iteration 163, loss = 0.05949069\n",
      "Iteration 120, loss = 0.08922834\n",
      "Iteration 164, loss = 0.06280770\n",
      "Iteration 121, loss = 0.08676988\n",
      "Iteration 165, loss = 0.05960327\n",
      "Iteration 122, loss = 0.08666696\n",
      "Iteration 166, loss = 0.05759047\n",
      "Iteration 123, loss = 0.08487584\n",
      "Iteration 167, loss = 0.05724437\n",
      "Iteration 124, loss = 0.08331926\n",
      "Iteration 168, loss = 0.05760346\n",
      "Iteration 125, loss = 0.08450898\n",
      "Iteration 169, loss = 0.05901546\n",
      "Iteration 126, loss = 0.08316796\n",
      "Iteration 170, loss = 0.05723066\n",
      "Iteration 127, loss = 0.08252129\n",
      "Iteration 171, loss = 0.05882340\n",
      "Iteration 128, loss = 0.08084730\n",
      "Iteration 172, loss = 0.05739470\n",
      "Iteration 129, loss = 0.08238337\n",
      "Iteration 173, loss = 0.05531867\n",
      "Iteration 130, loss = 0.08162423\n",
      "Iteration 174, loss = 0.05483430\n",
      "Iteration 131, loss = 0.08253195\n",
      "Iteration 175, loss = 0.05311024\n",
      "Iteration 132, loss = 0.08006524\n",
      "Iteration 176, loss = 0.05190370\n",
      "Iteration 133, loss = 0.07870832\n",
      "Iteration 177, loss = 0.05309737\n",
      "Iteration 134, loss = 0.07603086\n",
      "Iteration 178, loss = 0.05224157\n",
      "Iteration 135, loss = 0.07597245\n",
      "Iteration 179, loss = 0.05066839\n",
      "Iteration 136, loss = 0.07549451\n",
      "Iteration 180, loss = 0.05124267\n",
      "Iteration 137, loss = 0.07688415\n",
      "Iteration 181, loss = 0.04946813\n",
      "Iteration 138, loss = 0.07434065\n",
      "Iteration 182, loss = 0.04964311\n",
      "Iteration 139, loss = 0.07463470\n",
      "Iteration 183, loss = 0.04841930\n",
      "Iteration 140, loss = 0.07333241\n",
      "Iteration 184, loss = 0.04816978\n",
      "Iteration 141, loss = 0.07367301\n",
      "Iteration 185, loss = 0.04757980\n",
      "Iteration 142, loss = 0.07030579\n",
      "Iteration 186, loss = 0.04770708\n",
      "Iteration 143, loss = 0.07112172\n",
      "Iteration 187, loss = 0.04820797\n",
      "Iteration 144, loss = 0.06919167\n",
      "Iteration 188, loss = 0.05021387\n",
      "Iteration 145, loss = 0.06902722\n",
      "Iteration 189, loss = 0.05050255\n",
      "Iteration 146, loss = 0.07070251\n",
      "Iteration 190, loss = 0.04780535\n",
      "Iteration 147, loss = 0.07047939\n",
      "Iteration 191, loss = 0.04651155\n",
      "Iteration 148, loss = 0.06733332\n",
      "Iteration 192, loss = 0.04536333\n",
      "Iteration 149, loss = 0.06635681\n",
      "Iteration 193, loss = 0.04585404\n",
      "Iteration 150, loss = 0.06456614\n",
      "Iteration 194, loss = 0.04346023\n",
      "Iteration 151, loss = 0.06595320\n",
      "Iteration 195, loss = 0.04449683\n",
      "Iteration 152, loss = 0.06944910\n",
      "Iteration 196, loss = 0.04453741\n",
      "Iteration 153, loss = 0.06640497\n",
      "Iteration 197, loss = 0.04427404\n",
      "Iteration 154, loss = 0.06693851\n",
      "Iteration 198, loss = 0.04268845\n",
      "Iteration 155, loss = 0.06485212\n",
      "Iteration 199, loss = 0.04456622\n",
      "Iteration 156, loss = 0.06563402\n",
      "Iteration 200, loss = 0.04215894\n",
      "Iteration 157, loss = 0.06527306\n",
      "Iteration 201, loss = 0.04154334\n",
      "Iteration 158, loss = 0.06404428\n",
      "Iteration 202, loss = 0.04124173\n",
      "Iteration 159, loss = 0.06106275\n",
      "Iteration 203, loss = 0.04149662\n",
      "Iteration 160, loss = 0.06021274\n",
      "Iteration 204, loss = 0.03958567\n",
      "Iteration 161, loss = 0.06336168\n",
      "Iteration 205, loss = 0.03968022\n",
      "Iteration 162, loss = 0.06052792\n",
      "Iteration 206, loss = 0.03885537\n",
      "Iteration 163, loss = 0.05665253\n",
      "Iteration 207, loss = 0.03780101\n",
      "Iteration 164, loss = 0.05803513\n",
      "Iteration 208, loss = 0.03816638\n",
      "Iteration 165, loss = 0.05829381\n",
      "Iteration 209, loss = 0.03778942\n",
      "Iteration 166, loss = 0.05616348\n",
      "Iteration 210, loss = 0.03682314\n",
      "Iteration 167, loss = 0.05557742\n",
      "Iteration 211, loss = 0.03743322\n",
      "Iteration 168, loss = 0.05570406\n",
      "Iteration 212, loss = 0.03817387\n",
      "Iteration 169, loss = 0.05416517\n",
      "Iteration 213, loss = 0.03714040\n",
      "Iteration 170, loss = 0.05295399\n",
      "Iteration 214, loss = 0.03561609\n",
      "Iteration 171, loss = 0.05316172\n",
      "Iteration 215, loss = 0.03969905\n",
      "Iteration 172, loss = 0.05361730\n",
      "Iteration 216, loss = 0.03694201\n",
      "Iteration 173, loss = 0.05253010\n",
      "Iteration 217, loss = 0.03774067\n",
      "Iteration 174, loss = 0.05138297\n",
      "Iteration 218, loss = 0.03852352\n",
      "Iteration 175, loss = 0.05178883\n",
      "Iteration 219, loss = 0.04031621\n",
      "Iteration 176, loss = 0.05216864\n",
      "Iteration 220, loss = 0.04135712\n",
      "Iteration 177, loss = 0.05153483\n",
      "Iteration 221, loss = 0.03921907\n",
      "Iteration 178, loss = 0.04967099\n",
      "Iteration 222, loss = 0.03811291\n",
      "Iteration 179, loss = 0.05503521\n",
      "Iteration 223, loss = 0.03372444\n",
      "Iteration 180, loss = 0.05028373\n",
      "Iteration 224, loss = 0.03383803\n",
      "Iteration 181, loss = 0.05320843\n",
      "Iteration 225, loss = 0.03607722\n",
      "Iteration 182, loss = 0.05641630\n",
      "Iteration 226, loss = 0.03477056\n",
      "Iteration 183, loss = 0.05642402\n",
      "Iteration 227, loss = 0.03386209\n",
      "Iteration 184, loss = 0.04970354\n",
      "Iteration 228, loss = 0.03752566\n",
      "Iteration 185, loss = 0.04741563\n",
      "Iteration 229, loss = 0.03542672\n",
      "Iteration 186, loss = 0.05386613\n",
      "Iteration 230, loss = 0.03363742\n",
      "Iteration 187, loss = 0.05155405\n",
      "Iteration 231, loss = 0.02990138\n",
      "Iteration 188, loss = 0.04798654\n",
      "Iteration 232, loss = 0.03170676\n",
      "Iteration 189, loss = 0.04672203\n",
      "Iteration 233, loss = 0.03085343\n",
      "Iteration 190, loss = 0.04851803\n",
      "Iteration 234, loss = 0.03257084\n",
      "Iteration 191, loss = 0.04754850\n",
      "Iteration 235, loss = 0.03218661\n",
      "Iteration 192, loss = 0.04449650\n",
      "Iteration 236, loss = 0.03172113\n",
      "Iteration 193, loss = 0.04233822\n",
      "Iteration 237, loss = 0.02926684\n",
      "Iteration 194, loss = 0.04342646\n",
      "Iteration 238, loss = 0.02929278\n",
      "Iteration 195, loss = 0.04261670\n",
      "Iteration 239, loss = 0.02921797\n",
      "Iteration 196, loss = 0.04228163\n",
      "Iteration 240, loss = 0.03026302\n",
      "Iteration 197, loss = 0.04151016\n",
      "Iteration 241, loss = 0.03087627\n",
      "Iteration 242, loss = 0.02996035\n",
      "Iteration 198, loss = 0.04143161\n",
      "Iteration 243, loss = 0.02913180\n",
      "Iteration 199, loss = 0.04095723\n",
      "Iteration 244, loss = 0.02853183\n",
      "Iteration 200, loss = 0.04066868\n",
      "Iteration 245, loss = 0.02737646\n",
      "Iteration 201, loss = 0.04038698\n",
      "Iteration 246, loss = 0.02797878\n",
      "Iteration 202, loss = 0.04023909\n",
      "Iteration 247, loss = 0.02830993\n",
      "Iteration 203, loss = 0.03867139\n",
      "Iteration 248, loss = 0.02899361\n",
      "Iteration 204, loss = 0.03944444\n",
      "Iteration 249, loss = 0.03369074\n",
      "Iteration 205, loss = 0.03912283\n",
      "Iteration 206, loss = 0.04049775\n",
      "Iteration 250, loss = 0.03091697\n",
      "Iteration 207, loss = 0.03852911\n",
      "Iteration 251, loss = 0.02899227\n",
      "Iteration 208, loss = 0.03782432\n",
      "Iteration 252, loss = 0.02784268\n",
      "Iteration 209, loss = 0.03672779\n",
      "Iteration 253, loss = 0.02844270\n",
      "Iteration 210, loss = 0.03794310\n",
      "Iteration 254, loss = 0.02627230\n",
      "Iteration 211, loss = 0.03581605\n",
      "Iteration 255, loss = 0.02700256\n",
      "Iteration 212, loss = 0.03625635\n",
      "Iteration 256, loss = 0.02619713\n",
      "Iteration 213, loss = 0.03578092\n",
      "Iteration 257, loss = 0.02645488\n",
      "Iteration 214, loss = 0.03537930\n",
      "Iteration 258, loss = 0.02504653\n",
      "Iteration 215, loss = 0.03474840\n",
      "Iteration 259, loss = 0.02479506\n",
      "Iteration 260, loss = 0.02408908\n",
      "Iteration 216, loss = 0.03483433\n",
      "Iteration 261, loss = 0.02392642\n",
      "Iteration 217, loss = 0.03421071\n",
      "Iteration 262, loss = 0.02402327\n",
      "Iteration 218, loss = 0.03411960\n",
      "Iteration 219, loss = 0.03576328Iteration 263, loss = 0.02606831\n",
      "\n",
      "Iteration 220, loss = 0.03600801\n",
      "Iteration 264, loss = 0.02535773\n",
      "Iteration 221, loss = 0.03336020\n",
      "Iteration 265, loss = 0.02435935\n",
      "Iteration 222, loss = 0.03424867\n",
      "Iteration 266, loss = 0.02403970\n",
      "Iteration 223, loss = 0.03314713\n",
      "Iteration 267, loss = 0.02563390\n",
      "Iteration 224, loss = 0.03131039\n",
      "Iteration 268, loss = 0.02428536\n",
      "Iteration 225, loss = 0.03244279\n",
      "Iteration 269, loss = 0.02357007\n",
      "Iteration 226, loss = 0.03275595\n",
      "Iteration 270, loss = 0.02362174\n",
      "Iteration 227, loss = 0.03328452\n",
      "Iteration 271, loss = 0.02179078\n",
      "Iteration 228, loss = 0.03084215\n",
      "Iteration 272, loss = 0.02087589\n",
      "Iteration 229, loss = 0.03142330\n",
      "Iteration 273, loss = 0.02147327\n",
      "Iteration 230, loss = 0.03088809\n",
      "Iteration 274, loss = 0.02083343\n",
      "Iteration 231, loss = 0.03494815\n",
      "Iteration 275, loss = 0.02077579\n",
      "Iteration 232, loss = 0.03554931\n",
      "Iteration 276, loss = 0.02102864\n",
      "Iteration 233, loss = 0.04134830\n",
      "Iteration 277, loss = 0.02190686\n",
      "Iteration 234, loss = 0.04296986\n",
      "Iteration 278, loss = 0.02295896\n",
      "Iteration 235, loss = 0.03403846\n",
      "Iteration 279, loss = 0.02052215\n",
      "Iteration 236, loss = 0.02946240\n",
      "Iteration 280, loss = 0.02024651\n",
      "Iteration 237, loss = 0.03156379\n",
      "Iteration 281, loss = 0.02006965\n",
      "Iteration 238, loss = 0.03145158\n",
      "Iteration 282, loss = 0.01921785\n",
      "Iteration 239, loss = 0.02881922\n",
      "Iteration 283, loss = 0.01966833\n",
      "Iteration 240, loss = 0.02837317\n",
      "Iteration 284, loss = 0.01960944\n",
      "Iteration 241, loss = 0.03099043\n",
      "Iteration 285, loss = 0.01953593\n",
      "Iteration 242, loss = 0.03052962\n",
      "Iteration 286, loss = 0.01930509\n",
      "Iteration 243, loss = 0.03083897\n",
      "Iteration 287, loss = 0.02121992\n",
      "Iteration 244, loss = 0.02847604\n",
      "Iteration 288, loss = 0.02022490\n",
      "Iteration 245, loss = 0.02812263\n",
      "Iteration 289, loss = 0.02058838\n",
      "Iteration 246, loss = 0.02858764\n",
      "Iteration 290, loss = 0.02081980\n",
      "Iteration 247, loss = 0.02894315\n",
      "Iteration 291, loss = 0.02127412\n",
      "Iteration 248, loss = 0.02746580\n",
      "Iteration 292, loss = 0.01885695\n",
      "Iteration 249, loss = 0.02602873\n",
      "Iteration 293, loss = 0.01758445\n",
      "Iteration 250, loss = 0.02810528\n",
      "Iteration 294, loss = 0.02065371\n",
      "Iteration 251, loss = 0.02654753\n",
      "Iteration 295, loss = 0.02136644\n",
      "Iteration 252, loss = 0.02653029\n",
      "Iteration 296, loss = 0.01799282\n",
      "Iteration 253, loss = 0.02592900\n",
      "Iteration 297, loss = 0.01757866\n",
      "Iteration 254, loss = 0.02579766\n",
      "Iteration 298, loss = 0.01917577\n",
      "Iteration 255, loss = 0.02466382\n",
      "Iteration 299, loss = 0.01877320\n",
      "Iteration 256, loss = 0.02477877\n",
      "Iteration 300, loss = 0.01743089\n",
      "Iteration 257, loss = 0.02643630\n",
      "Iteration 301, loss = 0.01804825\n",
      "Iteration 258, loss = 0.02394146\n",
      "Iteration 302, loss = 0.01756673\n",
      "Iteration 259, loss = 0.02417540\n",
      "Iteration 303, loss = 0.01983356\n",
      "Iteration 260, loss = 0.02333751\n",
      "Iteration 304, loss = 0.02199423\n",
      "Iteration 261, loss = 0.02375763\n",
      "Iteration 305, loss = 0.02590777\n",
      "Iteration 262, loss = 0.02511068\n",
      "Iteration 306, loss = 0.02540539\n",
      "Iteration 263, loss = 0.02273908\n",
      "Iteration 307, loss = 0.02359378\n",
      "Iteration 264, loss = 0.02313236\n",
      "Iteration 308, loss = 0.01787319\n",
      "Iteration 265, loss = 0.02339306\n",
      "Iteration 309, loss = 0.01768926\n",
      "Iteration 266, loss = 0.02331914\n",
      "Iteration 310, loss = 0.01655819\n",
      "Iteration 267, loss = 0.02244243\n",
      "Iteration 311, loss = 0.01620699\n",
      "Iteration 268, loss = 0.02340291\n",
      "Iteration 312, loss = 0.01729865\n",
      "Iteration 269, loss = 0.02301735\n",
      "Iteration 313, loss = 0.01629039\n",
      "Iteration 270, loss = 0.02176986\n",
      "Iteration 314, loss = 0.01746113\n",
      "Iteration 271, loss = 0.02292687\n",
      "Iteration 315, loss = 0.01621447\n",
      "Iteration 272, loss = 0.02060191\n",
      "Iteration 316, loss = 0.01611773\n",
      "Iteration 273, loss = 0.02117618\n",
      "Iteration 317, loss = 0.01573211\n",
      "Iteration 274, loss = 0.02098074\n",
      "Iteration 318, loss = 0.01608032\n",
      "Iteration 275, loss = 0.02123541\n",
      "Iteration 319, loss = 0.01581651\n",
      "Iteration 276, loss = 0.02118165\n",
      "Iteration 320, loss = 0.01489953\n",
      "Iteration 277, loss = 0.02251505\n",
      "Iteration 321, loss = 0.01464976\n",
      "Iteration 278, loss = 0.02065520\n",
      "Iteration 322, loss = 0.01461462\n",
      "Iteration 279, loss = 0.02098167\n",
      "Iteration 323, loss = 0.01459661\n",
      "Iteration 280, loss = 0.02008657\n",
      "Iteration 324, loss = 0.01491330\n",
      "Iteration 281, loss = 0.01936519\n",
      "Iteration 325, loss = 0.01437085\n",
      "Iteration 282, loss = 0.02006566\n",
      "Iteration 326, loss = 0.01383968\n",
      "Iteration 283, loss = 0.01970473\n",
      "Iteration 327, loss = 0.01545854\n",
      "Iteration 284, loss = 0.02316818\n",
      "Iteration 328, loss = 0.01499723\n",
      "Iteration 285, loss = 0.01890071\n",
      "Iteration 329, loss = 0.01416075\n",
      "Iteration 286, loss = 0.01852786\n",
      "Iteration 330, loss = 0.01398270\n",
      "Iteration 287, loss = 0.01939132\n",
      "Iteration 331, loss = 0.01451130\n",
      "Iteration 288, loss = 0.02112296\n",
      "Iteration 332, loss = 0.01628163\n",
      "Iteration 289, loss = 0.01844559\n",
      "Iteration 333, loss = 0.01607820\n",
      "Iteration 290, loss = 0.01810823\n",
      "Iteration 334, loss = 0.01728039\n",
      "Iteration 291, loss = 0.01779506\n",
      "Iteration 335, loss = 0.02104858\n",
      "Iteration 292, loss = 0.01798638\n",
      "Iteration 336, loss = 0.02132390\n",
      "Iteration 293, loss = 0.01810981\n",
      "Iteration 337, loss = 0.02002562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 294, loss = 0.01772597\n",
      "Iteration 295, loss = 0.01757225\n",
      "Iteration 296, loss = 0.01714244\n",
      "Iteration 297, loss = 0.01799915\n",
      "Iteration 298, loss = 0.01668624\n",
      "Iteration 299, loss = 0.01689302\n",
      "Iteration 300, loss = 0.01672824\n",
      "Iteration 301, loss = 0.01847195\n",
      "Iteration 302, loss = 0.01664283\n",
      "Iteration 303, loss = 0.01623005\n",
      "Iteration 304, loss = 0.01609613\n",
      "Iteration 305, loss = 0.01578571\n",
      "Iteration 306, loss = 0.01564183\n",
      "Iteration 1, loss = 0.94483822\n",
      "Iteration 307, loss = 0.01595887\n",
      "Iteration 2, loss = 0.74046997\n",
      "Iteration 308, loss = 0.01577659\n",
      "Iteration 3, loss = 0.61319075\n",
      "Iteration 309, loss = 0.01581824\n",
      "Iteration 4, loss = 0.54546908\n",
      "Iteration 310, loss = 0.01570451\n",
      "Iteration 5, loss = 0.49536478\n",
      "Iteration 311, loss = 0.01582259\n",
      "Iteration 6, loss = 0.46110556\n",
      "Iteration 312, loss = 0.01582007\n",
      "Iteration 7, loss = 0.43363375\n",
      "Iteration 313, loss = 0.01599139\n",
      "Iteration 8, loss = 0.41292212\n",
      "Iteration 314, loss = 0.01634450\n",
      "Iteration 9, loss = 0.39752009\n",
      "Iteration 315, loss = 0.01613200\n",
      "Iteration 10, loss = 0.38247319\n",
      "Iteration 316, loss = 0.01442594\n",
      "Iteration 11, loss = 0.37003522\n",
      "Iteration 317, loss = 0.01536468\n",
      "Iteration 12, loss = 0.35846106\n",
      "Iteration 318, loss = 0.01478238\n",
      "Iteration 13, loss = 0.34850009\n",
      "Iteration 319, loss = 0.01438732\n",
      "Iteration 14, loss = 0.33951888\n",
      "Iteration 320, loss = 0.01417328\n",
      "Iteration 15, loss = 0.33113298\n",
      "Iteration 321, loss = 0.01375228\n",
      "Iteration 16, loss = 0.32345083\n",
      "Iteration 322, loss = 0.01376204\n",
      "Iteration 17, loss = 0.31523417\n",
      "Iteration 323, loss = 0.01362355\n",
      "Iteration 18, loss = 0.30991674\n",
      "Iteration 324, loss = 0.01340680\n",
      "Iteration 19, loss = 0.30277795\n",
      "Iteration 325, loss = 0.01302903\n",
      "Iteration 20, loss = 0.29768239\n",
      "Iteration 326, loss = 0.01374794\n",
      "Iteration 21, loss = 0.29222910\n",
      "Iteration 327, loss = 0.01358322\n",
      "Iteration 22, loss = 0.28480977\n",
      "Iteration 328, loss = 0.01352340\n",
      "Iteration 23, loss = 0.28307348\n",
      "Iteration 329, loss = 0.01389226\n",
      "Iteration 24, loss = 0.27459494\n",
      "Iteration 330, loss = 0.01279057\n",
      "Iteration 25, loss = 0.27047074\n",
      "Iteration 331, loss = 0.01345282\n",
      "Iteration 26, loss = 0.26533841\n",
      "Iteration 332, loss = 0.01287922\n",
      "Iteration 27, loss = 0.26188900\n",
      "Iteration 333, loss = 0.01265322\n",
      "Iteration 28, loss = 0.25960851\n",
      "Iteration 334, loss = 0.01327636\n",
      "Iteration 29, loss = 0.25355126\n",
      "Iteration 335, loss = 0.01279110\n",
      "Iteration 30, loss = 0.25019275\n",
      "Iteration 336, loss = 0.01216585\n",
      "Iteration 31, loss = 0.24640521\n",
      "Iteration 337, loss = 0.01291200\n",
      "Iteration 32, loss = 0.24256791\n",
      "Iteration 338, loss = 0.01310801\n",
      "Iteration 33, loss = 0.23932638\n",
      "Iteration 339, loss = 0.01300114\n",
      "Iteration 34, loss = 0.23453565\n",
      "Iteration 340, loss = 0.01249521\n",
      "Iteration 35, loss = 0.23158151\n",
      "Iteration 341, loss = 0.01203502\n",
      "Iteration 36, loss = 0.22995543\n",
      "Iteration 342, loss = 0.01234603\n",
      "Iteration 37, loss = 0.22584024\n",
      "Iteration 343, loss = 0.01175066\n",
      "Iteration 38, loss = 0.22278805\n",
      "Iteration 344, loss = 0.01219121\n",
      "Iteration 39, loss = 0.21996719\n",
      "Iteration 345, loss = 0.01154151\n",
      "Iteration 40, loss = 0.21728540\n",
      "Iteration 346, loss = 0.01136409\n",
      "Iteration 41, loss = 0.21955630\n",
      "Iteration 347, loss = 0.01262814\n",
      "Iteration 42, loss = 0.21179643\n",
      "Iteration 348, loss = 0.01396692\n",
      "Iteration 43, loss = 0.20896886\n",
      "Iteration 349, loss = 0.01251615\n",
      "Iteration 44, loss = 0.20603982\n",
      "Iteration 350, loss = 0.01253323\n",
      "Iteration 45, loss = 0.20143549\n",
      "Iteration 351, loss = 0.01236616\n",
      "Iteration 46, loss = 0.19986959\n",
      "Iteration 352, loss = 0.01100365\n",
      "Iteration 47, loss = 0.19834824\n",
      "Iteration 353, loss = 0.01106017\n",
      "Iteration 48, loss = 0.19460894\n",
      "Iteration 354, loss = 0.01070030\n",
      "Iteration 49, loss = 0.19120456\n",
      "Iteration 355, loss = 0.01101682\n",
      "Iteration 50, loss = 0.18955298\n",
      "Iteration 356, loss = 0.01180862\n",
      "Iteration 51, loss = 0.18912678\n",
      "Iteration 357, loss = 0.01057826\n",
      "Iteration 52, loss = 0.18520745\n",
      "Iteration 358, loss = 0.01057233\n",
      "Iteration 53, loss = 0.18065110\n",
      "Iteration 359, loss = 0.01150733\n",
      "Iteration 54, loss = 0.18257265\n",
      "Iteration 55, loss = 0.17758143\n",
      "Iteration 360, loss = 0.01158632\n",
      "Iteration 56, loss = 0.17550257\n",
      "Iteration 361, loss = 0.01183139\n",
      "Iteration 57, loss = 0.17431874\n",
      "Iteration 362, loss = 0.01013742\n",
      "Iteration 58, loss = 0.17079423\n",
      "Iteration 363, loss = 0.00990723\n",
      "Iteration 59, loss = 0.16747370\n",
      "Iteration 364, loss = 0.00988542\n",
      "Iteration 60, loss = 0.16596259\n",
      "Iteration 365, loss = 0.01064656\n",
      "Iteration 61, loss = 0.16541552\n",
      "Iteration 366, loss = 0.01001072\n",
      "Iteration 62, loss = 0.16452938\n",
      "Iteration 367, loss = 0.00970391\n",
      "Iteration 63, loss = 0.16088382\n",
      "Iteration 368, loss = 0.01005798\n",
      "Iteration 64, loss = 0.15967050\n",
      "Iteration 369, loss = 0.00953556\n",
      "Iteration 65, loss = 0.15631258\n",
      "Iteration 370, loss = 0.00938067\n",
      "Iteration 66, loss = 0.15632191\n",
      "Iteration 371, loss = 0.01005294\n",
      "Iteration 67, loss = 0.15384255\n",
      "Iteration 372, loss = 0.01027022\n",
      "Iteration 68, loss = 0.15045923\n",
      "Iteration 373, loss = 0.01035433\n",
      "Iteration 69, loss = 0.15057510\n",
      "Iteration 374, loss = 0.01075007\n",
      "Iteration 70, loss = 0.14825662\n",
      "Iteration 375, loss = 0.00924717\n",
      "Iteration 71, loss = 0.14594926\n",
      "Iteration 376, loss = 0.01003674\n",
      "Iteration 72, loss = 0.14439429\n",
      "Iteration 377, loss = 0.01032913\n",
      "Iteration 73, loss = 0.14337506\n",
      "Iteration 378, loss = 0.01114734\n",
      "Iteration 74, loss = 0.14223582\n",
      "Iteration 379, loss = 0.00986162\n",
      "Iteration 75, loss = 0.13981336\n",
      "Iteration 380, loss = 0.00859146\n",
      "Iteration 76, loss = 0.13886667\n",
      "Iteration 381, loss = 0.00892312\n",
      "Iteration 77, loss = 0.13818405\n",
      "Iteration 382, loss = 0.00904270\n",
      "Iteration 78, loss = 0.13578960\n",
      "Iteration 383, loss = 0.00953858\n",
      "Iteration 79, loss = 0.13619647\n",
      "Iteration 384, loss = 0.00890676\n",
      "Iteration 80, loss = 0.13270344\n",
      "Iteration 385, loss = 0.00869543\n",
      "Iteration 81, loss = 0.13332978\n",
      "Iteration 386, loss = 0.00850915\n",
      "Iteration 82, loss = 0.13329473\n",
      "Iteration 387, loss = 0.00904521\n",
      "Iteration 83, loss = 0.13025222\n",
      "Iteration 388, loss = 0.01058877\n",
      "Iteration 84, loss = 0.12817621\n",
      "Iteration 389, loss = 0.01186294\n",
      "Iteration 85, loss = 0.12915346\n",
      "Iteration 390, loss = 0.01279141\n",
      "Iteration 86, loss = 0.12419103\n",
      "Iteration 391, loss = 0.01589394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 87, loss = 0.12566249\n",
      "Iteration 88, loss = 0.12484663\n",
      "Iteration 89, loss = 0.12214823\n",
      "Iteration 90, loss = 0.12264398\n",
      "Iteration 91, loss = 0.12041344\n",
      "Iteration 92, loss = 0.11893954\n",
      "Iteration 93, loss = 0.11752316\n",
      "Iteration 94, loss = 0.11535817\n",
      "Iteration 95, loss = 0.11246964\n",
      "Iteration 96, loss = 0.11304165\n",
      "Iteration 97, loss = 0.11320250\n",
      "Iteration 98, loss = 0.10968798\n",
      "Iteration 99, loss = 0.11094622\n",
      "Iteration 100, loss = 0.11335449\n",
      "Iteration 1, loss = 1.19150430\n",
      "Iteration 101, loss = 0.10956169\n",
      "Iteration 2, loss = 0.83699589\n",
      "Iteration 102, loss = 0.11568716\n",
      "Iteration 3, loss = 0.66490554\n",
      "Iteration 4, loss = 0.57104910\n",
      "Iteration 103, loss = 0.10877752\n",
      "Iteration 5, loss = 0.51337163\n",
      "Iteration 104, loss = 0.10619894\n",
      "Iteration 6, loss = 0.47198354\n",
      "Iteration 105, loss = 0.10714708\n",
      "Iteration 7, loss = 0.44260868\n",
      "Iteration 106, loss = 0.10655049\n",
      "Iteration 8, loss = 0.41682066\n",
      "Iteration 107, loss = 0.10389173\n",
      "Iteration 9, loss = 0.39794682\n",
      "Iteration 108, loss = 0.10444485\n",
      "Iteration 10, loss = 0.38069683\n",
      "Iteration 109, loss = 0.10368383\n",
      "Iteration 11, loss = 0.36819770\n",
      "Iteration 110, loss = 0.10331520\n",
      "Iteration 12, loss = 0.35479954\n",
      "Iteration 111, loss = 0.10375609\n",
      "Iteration 13, loss = 0.34379581\n",
      "Iteration 112, loss = 0.10041153\n",
      "Iteration 14, loss = 0.33506529\n",
      "Iteration 113, loss = 0.09721158\n",
      "Iteration 15, loss = 0.32698595\n",
      "Iteration 114, loss = 0.09584188\n",
      "Iteration 16, loss = 0.31740718\n",
      "Iteration 115, loss = 0.09639607\n",
      "Iteration 17, loss = 0.31021819\n",
      "Iteration 116, loss = 0.09844854\n",
      "Iteration 18, loss = 0.30213385\n",
      "Iteration 117, loss = 0.09373637\n",
      "Iteration 19, loss = 0.29685994\n",
      "Iteration 118, loss = 0.09191102\n",
      "Iteration 20, loss = 0.28852857\n",
      "Iteration 119, loss = 0.09513160\n",
      "Iteration 21, loss = 0.28115424\n",
      "Iteration 120, loss = 0.09419446\n",
      "Iteration 22, loss = 0.27627564\n",
      "Iteration 121, loss = 0.09269409\n",
      "Iteration 23, loss = 0.27145774\n",
      "Iteration 122, loss = 0.08917812\n",
      "Iteration 24, loss = 0.26728833\n",
      "Iteration 123, loss = 0.09034259\n",
      "Iteration 25, loss = 0.26175340\n",
      "Iteration 124, loss = 0.09644974\n",
      "Iteration 125, loss = 0.09471440\n",
      "Iteration 26, loss = 0.25597947\n",
      "Iteration 126, loss = 0.09439120\n",
      "Iteration 27, loss = 0.25196953\n",
      "Iteration 28, loss = 0.24899602\n",
      "Iteration 127, loss = 0.08978623\n",
      "Iteration 29, loss = 0.24543918\n",
      "Iteration 128, loss = 0.09035827\n",
      "Iteration 30, loss = 0.23850490\n",
      "Iteration 129, loss = 0.09155561\n",
      "Iteration 31, loss = 0.23361844\n",
      "Iteration 130, loss = 0.08503010\n",
      "Iteration 32, loss = 0.23063039\n",
      "Iteration 131, loss = 0.08276275\n",
      "Iteration 33, loss = 0.22562656\n",
      "Iteration 132, loss = 0.08432573\n",
      "Iteration 34, loss = 0.22225691\n",
      "Iteration 133, loss = 0.08105053\n",
      "Iteration 35, loss = 0.21906486\n",
      "Iteration 134, loss = 0.08003330\n",
      "Iteration 36, loss = 0.21520344\n",
      "Iteration 135, loss = 0.08262468\n",
      "Iteration 37, loss = 0.21050103\n",
      "Iteration 136, loss = 0.08117468\n",
      "Iteration 38, loss = 0.20845993\n",
      "Iteration 137, loss = 0.08138972\n",
      "Iteration 39, loss = 0.20829112\n",
      "Iteration 138, loss = 0.08074973\n",
      "Iteration 40, loss = 0.19940878\n",
      "Iteration 139, loss = 0.07887884\n",
      "Iteration 41, loss = 0.19854426\n",
      "Iteration 140, loss = 0.08041499\n",
      "Iteration 42, loss = 0.19787112\n",
      "Iteration 141, loss = 0.07973411\n",
      "Iteration 43, loss = 0.19037088\n",
      "Iteration 142, loss = 0.07770549\n",
      "Iteration 44, loss = 0.18855655\n",
      "Iteration 143, loss = 0.07654754\n",
      "Iteration 45, loss = 0.18611687\n",
      "Iteration 144, loss = 0.07587097\n",
      "Iteration 46, loss = 0.18117848\n",
      "Iteration 145, loss = 0.07541239\n",
      "Iteration 47, loss = 0.17969198\n",
      "Iteration 146, loss = 0.07601058\n",
      "Iteration 48, loss = 0.17706947\n",
      "Iteration 147, loss = 0.07549311\n",
      "Iteration 49, loss = 0.17351784\n",
      "Iteration 148, loss = 0.07633484\n",
      "Iteration 50, loss = 0.17509612\n",
      "Iteration 149, loss = 0.07170414\n",
      "Iteration 51, loss = 0.16494102\n",
      "Iteration 150, loss = 0.07356498\n",
      "Iteration 52, loss = 0.16584212\n",
      "Iteration 151, loss = 0.07327267\n",
      "Iteration 53, loss = 0.16013634\n",
      "Iteration 152, loss = 0.07361260\n",
      "Iteration 54, loss = 0.15788398\n",
      "Iteration 153, loss = 0.07013373\n",
      "Iteration 55, loss = 0.15909041\n",
      "Iteration 154, loss = 0.07154655\n",
      "Iteration 56, loss = 0.15304249\n",
      "Iteration 155, loss = 0.07034241\n",
      "Iteration 57, loss = 0.15418071\n",
      "Iteration 156, loss = 0.06837056\n",
      "Iteration 58, loss = 0.15022401\n",
      "Iteration 157, loss = 0.06846914\n",
      "Iteration 59, loss = 0.14612510\n",
      "Iteration 158, loss = 0.06804216\n",
      "Iteration 60, loss = 0.14744793\n",
      "Iteration 159, loss = 0.06525326\n",
      "Iteration 61, loss = 0.14415079\n",
      "Iteration 160, loss = 0.06654619\n",
      "Iteration 161, loss = 0.06813717\n",
      "Iteration 62, loss = 0.13938718\n",
      "Iteration 162, loss = 0.06330913\n",
      "Iteration 63, loss = 0.13814303\n",
      "Iteration 163, loss = 0.06408229\n",
      "Iteration 64, loss = 0.13465612\n",
      "Iteration 164, loss = 0.06218508\n",
      "Iteration 65, loss = 0.13381389\n",
      "Iteration 165, loss = 0.06407414\n",
      "Iteration 66, loss = 0.13070181\n",
      "Iteration 166, loss = 0.06110024\n",
      "Iteration 67, loss = 0.13308926\n",
      "Iteration 167, loss = 0.06223708\n",
      "Iteration 68, loss = 0.12932843\n",
      "Iteration 168, loss = 0.06735333\n",
      "Iteration 69, loss = 0.12705100\n",
      "Iteration 169, loss = 0.06320858\n",
      "Iteration 70, loss = 0.12846571\n",
      "Iteration 170, loss = 0.06184512\n",
      "Iteration 71, loss = 0.12142479\n",
      "Iteration 72, loss = 0.12103305\n",
      "Iteration 171, loss = 0.05822559\n",
      "Iteration 73, loss = 0.11848609\n",
      "Iteration 172, loss = 0.06079189\n",
      "Iteration 74, loss = 0.11763913\n",
      "Iteration 173, loss = 0.06107230\n",
      "Iteration 75, loss = 0.11691602\n",
      "Iteration 174, loss = 0.05879365\n",
      "Iteration 76, loss = 0.11336597\n",
      "Iteration 175, loss = 0.05904153\n",
      "Iteration 77, loss = 0.11376810\n",
      "Iteration 176, loss = 0.06386183\n",
      "Iteration 78, loss = 0.11317202\n",
      "Iteration 177, loss = 0.06285158\n",
      "Iteration 79, loss = 0.11183274\n",
      "Iteration 178, loss = 0.06269715\n",
      "Iteration 80, loss = 0.11263159\n",
      "Iteration 179, loss = 0.05649762\n",
      "Iteration 81, loss = 0.11008696\n",
      "Iteration 180, loss = 0.05722507\n",
      "Iteration 82, loss = 0.10913658\n",
      "Iteration 181, loss = 0.05764629\n",
      "Iteration 83, loss = 0.10545030\n",
      "Iteration 182, loss = 0.05471738\n",
      "Iteration 183, loss = 0.05384325\n",
      "Iteration 84, loss = 0.10363088\n",
      "Iteration 184, loss = 0.05376933\n",
      "Iteration 85, loss = 0.09996350\n",
      "Iteration 185, loss = 0.05491878\n",
      "Iteration 86, loss = 0.10260091\n",
      "Iteration 87, loss = 0.10152286\n",
      "Iteration 186, loss = 0.05665747\n",
      "Iteration 187, loss = 0.05802468\n",
      "Iteration 88, loss = 0.09757602\n",
      "Iteration 188, loss = 0.05704380\n",
      "Iteration 89, loss = 0.09818690\n",
      "Iteration 189, loss = 0.05625279\n",
      "Iteration 90, loss = 0.09583510\n",
      "Iteration 190, loss = 0.05256718\n",
      "Iteration 91, loss = 0.09216836\n",
      "Iteration 191, loss = 0.05357267\n",
      "Iteration 92, loss = 0.09378953\n",
      "Iteration 192, loss = 0.05126138\n",
      "Iteration 93, loss = 0.09265560\n",
      "Iteration 193, loss = 0.05393375\n",
      "Iteration 94, loss = 0.09258619\n",
      "Iteration 194, loss = 0.05788693\n",
      "Iteration 95, loss = 0.08803252\n",
      "Iteration 195, loss = 0.05406908\n",
      "Iteration 96, loss = 0.08893536\n",
      "Iteration 196, loss = 0.05186838\n",
      "Iteration 97, loss = 0.08721537\n",
      "Iteration 197, loss = 0.05224530\n",
      "Iteration 98, loss = 0.08718721\n",
      "Iteration 198, loss = 0.05074516\n",
      "Iteration 99, loss = 0.08615533\n",
      "Iteration 199, loss = 0.04830932\n",
      "Iteration 100, loss = 0.08180909\n",
      "Iteration 200, loss = 0.05010723\n",
      "Iteration 101, loss = 0.08286450\n",
      "Iteration 201, loss = 0.04901838\n",
      "Iteration 102, loss = 0.08305321\n",
      "Iteration 202, loss = 0.04776301\n",
      "Iteration 103, loss = 0.08113373\n",
      "Iteration 203, loss = 0.04992200\n",
      "Iteration 104, loss = 0.07868210\n",
      "Iteration 204, loss = 0.04784177\n",
      "Iteration 105, loss = 0.07959977\n",
      "Iteration 205, loss = 0.04730888\n",
      "Iteration 106, loss = 0.07810632\n",
      "Iteration 206, loss = 0.04965921\n",
      "Iteration 107, loss = 0.07710355\n",
      "Iteration 207, loss = 0.04769202\n",
      "Iteration 108, loss = 0.07513631\n",
      "Iteration 208, loss = 0.04337453\n",
      "Iteration 109, loss = 0.07458257\n",
      "Iteration 209, loss = 0.04648249\n",
      "Iteration 110, loss = 0.07372420\n",
      "Iteration 210, loss = 0.04691910\n",
      "Iteration 111, loss = 0.07365583\n",
      "Iteration 211, loss = 0.04696961\n",
      "Iteration 112, loss = 0.07252154\n",
      "Iteration 212, loss = 0.04550835\n",
      "Iteration 113, loss = 0.07128269\n",
      "Iteration 213, loss = 0.04661524\n",
      "Iteration 114, loss = 0.07230459\n",
      "Iteration 214, loss = 0.04416440\n",
      "Iteration 115, loss = 0.06979562\n",
      "Iteration 215, loss = 0.04541477\n",
      "Iteration 116, loss = 0.06923881\n",
      "Iteration 216, loss = 0.04203509\n",
      "Iteration 117, loss = 0.06740996\n",
      "Iteration 217, loss = 0.04602931\n",
      "Iteration 118, loss = 0.06811932\n",
      "Iteration 218, loss = 0.04274083\n",
      "Iteration 119, loss = 0.06662449\n",
      "Iteration 219, loss = 0.04285657\n",
      "Iteration 120, loss = 0.06544125\n",
      "Iteration 220, loss = 0.04478734\n",
      "Iteration 121, loss = 0.06478319\n",
      "Iteration 221, loss = 0.04275566\n",
      "Iteration 122, loss = 0.06743726\n",
      "Iteration 222, loss = 0.04042356\n",
      "Iteration 123, loss = 0.06350438\n",
      "Iteration 223, loss = 0.04047345\n",
      "Iteration 124, loss = 0.06363983\n",
      "Iteration 224, loss = 0.04077827\n",
      "Iteration 125, loss = 0.06411370\n",
      "Iteration 225, loss = 0.04087840\n",
      "Iteration 126, loss = 0.06022083\n",
      "Iteration 226, loss = 0.03838335\n",
      "Iteration 127, loss = 0.06220891\n",
      "Iteration 227, loss = 0.04065195\n",
      "Iteration 128, loss = 0.05983349\n",
      "Iteration 228, loss = 0.03955094\n",
      "Iteration 129, loss = 0.06106272\n",
      "Iteration 229, loss = 0.04372091\n",
      "Iteration 130, loss = 0.05769649\n",
      "Iteration 230, loss = 0.04410617\n",
      "Iteration 131, loss = 0.05725895\n",
      "Iteration 231, loss = 0.04068612\n",
      "Iteration 132, loss = 0.05801835\n",
      "Iteration 232, loss = 0.03799798\n",
      "Iteration 133, loss = 0.05643717\n",
      "Iteration 233, loss = 0.03860446\n",
      "Iteration 134, loss = 0.05643817\n",
      "Iteration 234, loss = 0.03836309\n",
      "Iteration 135, loss = 0.05617796\n",
      "Iteration 235, loss = 0.03609724\n",
      "Iteration 136, loss = 0.05528310\n",
      "Iteration 236, loss = 0.03905233\n",
      "Iteration 137, loss = 0.05560784\n",
      "Iteration 237, loss = 0.04986540\n",
      "Iteration 138, loss = 0.05216037\n",
      "Iteration 238, loss = 0.04299740\n",
      "Iteration 139, loss = 0.05320596\n",
      "Iteration 140, loss = 0.05234570\n",
      "Iteration 239, loss = 0.03805883\n",
      "Iteration 141, loss = 0.05350337\n",
      "Iteration 240, loss = 0.03785830\n",
      "Iteration 142, loss = 0.05190034\n",
      "Iteration 241, loss = 0.03440070\n",
      "Iteration 143, loss = 0.05042777\n",
      "Iteration 242, loss = 0.03776274\n",
      "Iteration 144, loss = 0.04999822\n",
      "Iteration 243, loss = 0.03753931\n",
      "Iteration 145, loss = 0.05031234\n",
      "Iteration 244, loss = 0.03454379\n",
      "Iteration 146, loss = 0.04850690\n",
      "Iteration 245, loss = 0.03462090\n",
      "Iteration 246, loss = 0.03510869\n",
      "Iteration 147, loss = 0.04737697\n",
      "Iteration 247, loss = 0.03547808\n",
      "Iteration 148, loss = 0.04874427\n",
      "Iteration 248, loss = 0.03542557\n",
      "Iteration 149, loss = 0.04596415\n",
      "Iteration 249, loss = 0.03474051\n",
      "Iteration 150, loss = 0.04639526\n",
      "Iteration 250, loss = 0.03246083\n",
      "Iteration 151, loss = 0.04579091\n",
      "Iteration 251, loss = 0.03472418\n",
      "Iteration 152, loss = 0.04611484\n",
      "Iteration 252, loss = 0.03294021\n",
      "Iteration 153, loss = 0.04626123\n",
      "Iteration 253, loss = 0.03351362\n",
      "Iteration 154, loss = 0.04443593\n",
      "Iteration 254, loss = 0.03271536\n",
      "Iteration 155, loss = 0.04349296\n",
      "Iteration 255, loss = 0.03648699\n",
      "Iteration 156, loss = 0.04453848\n",
      "Iteration 256, loss = 0.03261606\n",
      "Iteration 157, loss = 0.04342249\n",
      "Iteration 257, loss = 0.03456673\n",
      "Iteration 158, loss = 0.04725845\n",
      "Iteration 258, loss = 0.03624581\n",
      "Iteration 159, loss = 0.04589966\n",
      "Iteration 259, loss = 0.03099543\n",
      "Iteration 160, loss = 0.04283064\n",
      "Iteration 260, loss = 0.03331007\n",
      "Iteration 161, loss = 0.04333384\n",
      "Iteration 261, loss = 0.03185430\n",
      "Iteration 162, loss = 0.04393728\n",
      "Iteration 262, loss = 0.03184042\n",
      "Iteration 163, loss = 0.04449126\n",
      "Iteration 263, loss = 0.02908785\n",
      "Iteration 164, loss = 0.04500154\n",
      "Iteration 264, loss = 0.03054178\n",
      "Iteration 165, loss = 0.04117344\n",
      "Iteration 265, loss = 0.03075594\n",
      "Iteration 166, loss = 0.04134696\n",
      "Iteration 266, loss = 0.03043460\n",
      "Iteration 167, loss = 0.03801489\n",
      "Iteration 267, loss = 0.03228921\n",
      "Iteration 168, loss = 0.04020440\n",
      "Iteration 268, loss = 0.03271873\n",
      "Iteration 169, loss = 0.04236895\n",
      "Iteration 269, loss = 0.03368599\n",
      "Iteration 170, loss = 0.03996620\n",
      "Iteration 270, loss = 0.02928668\n",
      "Iteration 171, loss = 0.03661074\n",
      "Iteration 271, loss = 0.02874274\n",
      "Iteration 172, loss = 0.03753267\n",
      "Iteration 272, loss = 0.02789445\n",
      "Iteration 173, loss = 0.03721274\n",
      "Iteration 273, loss = 0.02839189\n",
      "Iteration 174, loss = 0.03595806\n",
      "Iteration 274, loss = 0.03041697\n",
      "Iteration 175, loss = 0.03486676\n",
      "Iteration 275, loss = 0.02768340\n",
      "Iteration 176, loss = 0.03393584\n",
      "Iteration 276, loss = 0.02800171\n",
      "Iteration 177, loss = 0.03545398\n",
      "Iteration 277, loss = 0.02664355\n",
      "Iteration 178, loss = 0.03670837\n",
      "Iteration 278, loss = 0.02719227\n",
      "Iteration 179, loss = 0.03428924\n",
      "Iteration 279, loss = 0.02639212\n",
      "Iteration 180, loss = 0.03569042\n",
      "Iteration 280, loss = 0.02644522\n",
      "Iteration 181, loss = 0.03241841\n",
      "Iteration 281, loss = 0.02647231\n",
      "Iteration 182, loss = 0.03296828\n",
      "Iteration 282, loss = 0.02679873\n",
      "Iteration 183, loss = 0.03112811\n",
      "Iteration 283, loss = 0.02598947\n",
      "Iteration 184, loss = 0.03225374\n",
      "Iteration 284, loss = 0.02645236\n",
      "Iteration 185, loss = 0.03123955\n",
      "Iteration 285, loss = 0.02659199\n",
      "Iteration 186, loss = 0.03114818\n",
      "Iteration 286, loss = 0.02661361\n",
      "Iteration 187, loss = 0.03100433\n",
      "Iteration 287, loss = 0.02455796\n",
      "Iteration 188, loss = 0.03016934\n",
      "Iteration 288, loss = 0.02733491\n",
      "Iteration 189, loss = 0.03070424\n",
      "Iteration 289, loss = 0.02689341\n",
      "Iteration 190, loss = 0.03053734\n",
      "Iteration 290, loss = 0.02551304\n",
      "Iteration 191, loss = 0.03045815\n",
      "Iteration 291, loss = 0.02482119\n",
      "Iteration 192, loss = 0.02920566\n",
      "Iteration 292, loss = 0.02573129\n",
      "Iteration 193, loss = 0.02964043\n",
      "Iteration 293, loss = 0.02474072\n",
      "Iteration 194, loss = 0.03137329\n",
      "Iteration 294, loss = 0.02546735\n",
      "Iteration 195, loss = 0.02745470\n",
      "Iteration 295, loss = 0.02434118\n",
      "Iteration 196, loss = 0.02823712\n",
      "Iteration 296, loss = 0.02506124\n",
      "Iteration 197, loss = 0.02794351\n",
      "Iteration 297, loss = 0.02332795\n",
      "Iteration 198, loss = 0.02765644\n",
      "Iteration 298, loss = 0.02431578\n",
      "Iteration 199, loss = 0.02793151\n",
      "Iteration 299, loss = 0.02446009\n",
      "Iteration 200, loss = 0.02763697\n",
      "Iteration 300, loss = 0.02438341\n",
      "Iteration 201, loss = 0.02537525\n",
      "Iteration 301, loss = 0.02403641\n",
      "Iteration 202, loss = 0.02496438\n",
      "Iteration 302, loss = 0.02277255\n",
      "Iteration 203, loss = 0.02509741\n",
      "Iteration 303, loss = 0.02383612\n",
      "Iteration 204, loss = 0.02468110\n",
      "Iteration 304, loss = 0.02304010\n",
      "Iteration 205, loss = 0.02419621\n",
      "Iteration 305, loss = 0.02339039\n",
      "Iteration 206, loss = 0.02407881\n",
      "Iteration 306, loss = 0.02242084\n",
      "Iteration 207, loss = 0.02425139\n",
      "Iteration 307, loss = 0.02271915\n",
      "Iteration 208, loss = 0.02379879\n",
      "Iteration 308, loss = 0.02182373\n",
      "Iteration 209, loss = 0.02387528\n",
      "Iteration 309, loss = 0.02127286\n",
      "Iteration 210, loss = 0.02393301\n",
      "Iteration 310, loss = 0.02179039\n",
      "Iteration 211, loss = 0.02316496\n",
      "Iteration 311, loss = 0.02191308\n",
      "Iteration 212, loss = 0.02271485\n",
      "Iteration 312, loss = 0.02127782\n",
      "Iteration 213, loss = 0.02236228\n",
      "Iteration 313, loss = 0.02173852\n",
      "Iteration 214, loss = 0.02230216\n",
      "Iteration 314, loss = 0.02121088\n",
      "Iteration 215, loss = 0.02206437\n",
      "Iteration 315, loss = 0.02074442\n",
      "Iteration 216, loss = 0.02181437\n",
      "Iteration 316, loss = 0.02140561\n",
      "Iteration 217, loss = 0.02131828\n",
      "Iteration 317, loss = 0.02129319\n",
      "Iteration 218, loss = 0.02101203\n",
      "Iteration 318, loss = 0.02276191\n",
      "Iteration 219, loss = 0.02118994\n",
      "Iteration 319, loss = 0.02102149\n",
      "Iteration 220, loss = 0.02087421\n",
      "Iteration 320, loss = 0.02058918\n",
      "Iteration 221, loss = 0.02022042\n",
      "Iteration 321, loss = 0.02190412\n",
      "Iteration 222, loss = 0.02064323\n",
      "Iteration 322, loss = 0.02029669\n",
      "Iteration 223, loss = 0.02065666\n",
      "Iteration 323, loss = 0.01957879\n",
      "Iteration 224, loss = 0.02055207\n",
      "Iteration 324, loss = 0.02115737\n",
      "Iteration 225, loss = 0.02055810\n",
      "Iteration 325, loss = 0.02077060\n",
      "Iteration 226, loss = 0.02033876\n",
      "Iteration 326, loss = 0.02120550\n",
      "Iteration 227, loss = 0.01981302\n",
      "Iteration 327, loss = 0.01992792\n",
      "Iteration 228, loss = 0.01978760\n",
      "Iteration 328, loss = 0.01883443\n",
      "Iteration 229, loss = 0.02090040\n",
      "Iteration 329, loss = 0.01931313\n",
      "Iteration 230, loss = 0.01888598\n",
      "Iteration 330, loss = 0.01998767\n",
      "Iteration 231, loss = 0.01877863\n",
      "Iteration 331, loss = 0.01959972\n",
      "Iteration 232, loss = 0.01857149\n",
      "Iteration 332, loss = 0.01947588\n",
      "Iteration 233, loss = 0.01777698\n",
      "Iteration 333, loss = 0.02012362\n",
      "Iteration 234, loss = 0.01796882\n",
      "Iteration 334, loss = 0.01932551\n",
      "Iteration 235, loss = 0.01759336\n",
      "Iteration 335, loss = 0.01977142\n",
      "Iteration 236, loss = 0.01810269\n",
      "Iteration 336, loss = 0.01865266\n",
      "Iteration 237, loss = 0.01762687\n",
      "Iteration 337, loss = 0.01912673\n",
      "Iteration 238, loss = 0.01690290\n",
      "Iteration 338, loss = 0.01769276\n",
      "Iteration 239, loss = 0.01671380\n",
      "Iteration 339, loss = 0.01897016\n",
      "Iteration 240, loss = 0.01680204\n",
      "Iteration 340, loss = 0.02094756\n",
      "Iteration 241, loss = 0.01668351\n",
      "Iteration 341, loss = 0.02019479\n",
      "Iteration 242, loss = 0.01657099\n",
      "Iteration 342, loss = 0.01870907\n",
      "Iteration 243, loss = 0.01619822\n",
      "Iteration 343, loss = 0.01777808\n",
      "Iteration 244, loss = 0.01691021\n",
      "Iteration 344, loss = 0.01808137\n",
      "Iteration 245, loss = 0.01662480\n",
      "Iteration 345, loss = 0.01785230\n",
      "Iteration 246, loss = 0.01570026\n",
      "Iteration 346, loss = 0.01753219\n",
      "Iteration 247, loss = 0.01585219\n",
      "Iteration 347, loss = 0.01932944\n",
      "Iteration 248, loss = 0.01635174\n",
      "Iteration 348, loss = 0.01879824\n",
      "Iteration 249, loss = 0.01685110\n",
      "Iteration 349, loss = 0.01710748\n",
      "Iteration 250, loss = 0.01533215\n",
      "Iteration 350, loss = 0.01725373\n",
      "Iteration 251, loss = 0.01508532\n",
      "Iteration 351, loss = 0.01674504\n",
      "Iteration 252, loss = 0.01487628\n",
      "Iteration 352, loss = 0.01685292\n",
      "Iteration 253, loss = 0.01530884\n",
      "Iteration 353, loss = 0.01645268\n",
      "Iteration 254, loss = 0.01565272\n",
      "Iteration 354, loss = 0.01650742\n",
      "Iteration 255, loss = 0.01548343\n",
      "Iteration 355, loss = 0.01775846\n",
      "Iteration 256, loss = 0.01617678\n",
      "Iteration 356, loss = 0.01850488\n",
      "Iteration 257, loss = 0.01799172\n",
      "Iteration 357, loss = 0.01658591\n",
      "Iteration 258, loss = 0.01778741\n",
      "Iteration 358, loss = 0.01692508\n",
      "Iteration 259, loss = 0.01699592\n",
      "Iteration 359, loss = 0.01615767\n",
      "Iteration 260, loss = 0.01537666\n",
      "Iteration 360, loss = 0.01591173\n",
      "Iteration 261, loss = 0.01411859\n",
      "Iteration 361, loss = 0.01551458\n",
      "Iteration 262, loss = 0.01384348\n",
      "Iteration 362, loss = 0.01566813\n",
      "Iteration 263, loss = 0.01369291\n",
      "Iteration 363, loss = 0.01633479\n",
      "Iteration 264, loss = 0.01354837\n",
      "Iteration 364, loss = 0.01709738\n",
      "Iteration 265, loss = 0.01296718\n",
      "Iteration 365, loss = 0.01689784\n",
      "Iteration 266, loss = 0.01310208\n",
      "Iteration 366, loss = 0.01633342\n",
      "Iteration 267, loss = 0.01261244\n",
      "Iteration 367, loss = 0.01535360\n",
      "Iteration 268, loss = 0.01331063\n",
      "Iteration 368, loss = 0.01577951\n",
      "Iteration 269, loss = 0.01291608\n",
      "Iteration 369, loss = 0.01844503\n",
      "Iteration 270, loss = 0.01238593\n",
      "Iteration 370, loss = 0.02390153\n",
      "Iteration 271, loss = 0.01203428\n",
      "Iteration 371, loss = 0.02471401\n",
      "Iteration 272, loss = 0.01250833\n",
      "Iteration 372, loss = 0.02064231\n",
      "Iteration 273, loss = 0.01178926\n",
      "Iteration 373, loss = 0.01990814\n",
      "Iteration 274, loss = 0.01189584\n",
      "Iteration 374, loss = 0.01666087\n",
      "Iteration 275, loss = 0.01181918\n",
      "Iteration 375, loss = 0.01527102\n",
      "Iteration 276, loss = 0.01118998\n",
      "Iteration 376, loss = 0.01717085\n",
      "Iteration 277, loss = 0.01174195\n",
      "Iteration 377, loss = 0.01639814\n",
      "Iteration 278, loss = 0.01158204\n",
      "Iteration 378, loss = 0.02022602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 279, loss = 0.01158100\n",
      "Iteration 280, loss = 0.01158755\n",
      "Iteration 281, loss = 0.01120133\n",
      "Iteration 282, loss = 0.01091369\n",
      "Iteration 283, loss = 0.01085636\n",
      "Iteration 284, loss = 0.01082924\n",
      "Iteration 285, loss = 0.01058115\n",
      "Iteration 286, loss = 0.01040905\n",
      "Iteration 287, loss = 0.01032876\n",
      "Iteration 288, loss = 0.01033001\n",
      "Iteration 289, loss = 0.01014095\n",
      "Iteration 290, loss = 0.01051115\n",
      "Iteration 291, loss = 0.01071726\n",
      "Iteration 1, loss = 1.05188433\n",
      "Iteration 292, loss = 0.01000611\n",
      "Iteration 2, loss = 0.77332512\n",
      "Iteration 293, loss = 0.01019433\n",
      "Iteration 3, loss = 0.63568554\n",
      "Iteration 294, loss = 0.01020404\n",
      "Iteration 4, loss = 0.55521722\n",
      "Iteration 295, loss = 0.01080276\n",
      "Iteration 5, loss = 0.50685276\n",
      "Iteration 296, loss = 0.00967468\n",
      "Iteration 6, loss = 0.46615413\n",
      "Iteration 297, loss = 0.00971752\n",
      "Iteration 7, loss = 0.43747921\n",
      "Iteration 298, loss = 0.01107125\n",
      "Iteration 8, loss = 0.41244946\n",
      "Iteration 299, loss = 0.01126278\n",
      "Iteration 9, loss = 0.39334094\n",
      "Iteration 300, loss = 0.00975065\n",
      "Iteration 10, loss = 0.37775956\n",
      "Iteration 301, loss = 0.00969966\n",
      "Iteration 11, loss = 0.36239045\n",
      "Iteration 302, loss = 0.00986047\n",
      "Iteration 12, loss = 0.34992453\n",
      "Iteration 303, loss = 0.00978111\n",
      "Iteration 13, loss = 0.33743072\n",
      "Iteration 304, loss = 0.00949348\n",
      "Iteration 14, loss = 0.32772869\n",
      "Iteration 305, loss = 0.00885079\n",
      "Iteration 15, loss = 0.31845034\n",
      "Iteration 306, loss = 0.00883430\n",
      "Iteration 16, loss = 0.30932415\n",
      "Iteration 307, loss = 0.00978973\n",
      "Iteration 17, loss = 0.30227795\n",
      "Iteration 308, loss = 0.00866551\n",
      "Iteration 18, loss = 0.29433451\n",
      "Iteration 309, loss = 0.00837024\n",
      "Iteration 19, loss = 0.28723649\n",
      "Iteration 310, loss = 0.00861466\n",
      "Iteration 20, loss = 0.27998399\n",
      "Iteration 311, loss = 0.00877983\n",
      "Iteration 21, loss = 0.27492567\n",
      "Iteration 312, loss = 0.00840851\n",
      "Iteration 22, loss = 0.27010317\n",
      "Iteration 313, loss = 0.00839643\n",
      "Iteration 23, loss = 0.26356770\n",
      "Iteration 314, loss = 0.00811439\n",
      "Iteration 24, loss = 0.25784899\n",
      "Iteration 315, loss = 0.00808142\n",
      "Iteration 25, loss = 0.25322994\n",
      "Iteration 316, loss = 0.00831695\n",
      "Iteration 26, loss = 0.24792790\n",
      "Iteration 317, loss = 0.00823892\n",
      "Iteration 27, loss = 0.24336178\n",
      "Iteration 318, loss = 0.00852201\n",
      "Iteration 28, loss = 0.23844291\n",
      "Iteration 319, loss = 0.00807159\n",
      "Iteration 29, loss = 0.23408915\n",
      "Iteration 320, loss = 0.00750605\n",
      "Iteration 30, loss = 0.23019107\n",
      "Iteration 321, loss = 0.00789267\n",
      "Iteration 31, loss = 0.22537383\n",
      "Iteration 322, loss = 0.00845733\n",
      "Iteration 32, loss = 0.22208776\n",
      "Iteration 323, loss = 0.00747928\n",
      "Iteration 33, loss = 0.21794168\n",
      "Iteration 324, loss = 0.00758957\n",
      "Iteration 34, loss = 0.21449065\n",
      "Iteration 325, loss = 0.00724915\n",
      "Iteration 35, loss = 0.21015432\n",
      "Iteration 326, loss = 0.00737815\n",
      "Iteration 36, loss = 0.20578474\n",
      "Iteration 327, loss = 0.00725372\n",
      "Iteration 37, loss = 0.20406358\n",
      "Iteration 328, loss = 0.00762347\n",
      "Iteration 38, loss = 0.20012975\n",
      "Iteration 329, loss = 0.00734759\n",
      "Iteration 39, loss = 0.19788499\n",
      "Iteration 330, loss = 0.00757205\n",
      "Iteration 40, loss = 0.19271274\n",
      "Iteration 331, loss = 0.00699355\n",
      "Iteration 41, loss = 0.19067771\n",
      "Iteration 332, loss = 0.00730783\n",
      "Iteration 42, loss = 0.18649957\n",
      "Iteration 333, loss = 0.00759059\n",
      "Iteration 43, loss = 0.18434042\n",
      "Iteration 334, loss = 0.00717301\n",
      "Iteration 44, loss = 0.18226214\n",
      "Iteration 335, loss = 0.00752307\n",
      "Iteration 45, loss = 0.17810903\n",
      "Iteration 336, loss = 0.00653139\n",
      "Iteration 46, loss = 0.17668273\n",
      "Iteration 337, loss = 0.00667315\n",
      "Iteration 47, loss = 0.17321429\n",
      "Iteration 338, loss = 0.00696919\n",
      "Iteration 48, loss = 0.17057560\n",
      "Iteration 339, loss = 0.00656091\n",
      "Iteration 49, loss = 0.16862284\n",
      "Iteration 340, loss = 0.00639154\n",
      "Iteration 50, loss = 0.16965921\n",
      "Iteration 341, loss = 0.00649830\n",
      "Iteration 51, loss = 0.16312867\n",
      "Iteration 342, loss = 0.00645813\n",
      "Iteration 52, loss = 0.16384101\n",
      "Iteration 343, loss = 0.00640050\n",
      "Iteration 53, loss = 0.15929654\n",
      "Iteration 344, loss = 0.00629271\n",
      "Iteration 54, loss = 0.15672911\n",
      "Iteration 345, loss = 0.00607843\n",
      "Iteration 55, loss = 0.15516018\n",
      "Iteration 346, loss = 0.00610824\n",
      "Iteration 56, loss = 0.15438047\n",
      "Iteration 347, loss = 0.00603729\n",
      "Iteration 57, loss = 0.15146099\n",
      "Iteration 348, loss = 0.00617886\n",
      "Iteration 58, loss = 0.14828110\n",
      "Iteration 349, loss = 0.00628108\n",
      "Iteration 59, loss = 0.14666747\n",
      "Iteration 350, loss = 0.00589366\n",
      "Iteration 60, loss = 0.14616838\n",
      "Iteration 351, loss = 0.00602647\n",
      "Iteration 61, loss = 0.14207798\n",
      "Iteration 352, loss = 0.00586270\n",
      "Iteration 62, loss = 0.14342843\n",
      "Iteration 353, loss = 0.00605016\n",
      "Iteration 63, loss = 0.13939455\n",
      "Iteration 354, loss = 0.00623283\n",
      "Iteration 64, loss = 0.13810385\n",
      "Iteration 355, loss = 0.00605039\n",
      "Iteration 65, loss = 0.13705879\n",
      "Iteration 356, loss = 0.00584698\n",
      "Iteration 66, loss = 0.13596844\n",
      "Iteration 357, loss = 0.00596905\n",
      "Iteration 67, loss = 0.13305982\n",
      "Iteration 358, loss = 0.00573469\n",
      "Iteration 68, loss = 0.13265910\n",
      "Iteration 359, loss = 0.00608342\n",
      "Iteration 69, loss = 0.13109692\n",
      "Iteration 360, loss = 0.00552188\n",
      "Iteration 70, loss = 0.12898715\n",
      "Iteration 361, loss = 0.00563957\n",
      "Iteration 71, loss = 0.13066345Iteration 362, loss = 0.00540467\n",
      "\n",
      "Iteration 363, loss = 0.00534004\n",
      "Iteration 72, loss = 0.12616744\n",
      "Iteration 364, loss = 0.00535013\n",
      "Iteration 73, loss = 0.12817191\n",
      "Iteration 365, loss = 0.00517585\n",
      "Iteration 74, loss = 0.12679557\n",
      "Iteration 366, loss = 0.00511371\n",
      "Iteration 75, loss = 0.12569490\n",
      "Iteration 367, loss = 0.00522501\n",
      "Iteration 76, loss = 0.12148735\n",
      "Iteration 77, loss = 0.12138718\n",
      "Iteration 368, loss = 0.00517305\n",
      "Iteration 78, loss = 0.11871283\n",
      "Iteration 369, loss = 0.00503185\n",
      "Iteration 79, loss = 0.11579161\n",
      "Iteration 370, loss = 0.00512638\n",
      "Iteration 80, loss = 0.11504957\n",
      "Iteration 371, loss = 0.00514061\n",
      "Iteration 81, loss = 0.11447626\n",
      "Iteration 372, loss = 0.00521685\n",
      "Iteration 82, loss = 0.11236170\n",
      "Iteration 373, loss = 0.00498027\n",
      "Iteration 83, loss = 0.11171945\n",
      "Iteration 374, loss = 0.00485516\n",
      "Iteration 84, loss = 0.11165993\n",
      "Iteration 375, loss = 0.00486089\n",
      "Iteration 85, loss = 0.10914544\n",
      "Iteration 376, loss = 0.00494461\n",
      "Iteration 377, loss = 0.00513014\n",
      "Iteration 86, loss = 0.10805326\n",
      "Iteration 378, loss = 0.00531883\n",
      "Iteration 87, loss = 0.10868303\n",
      "Iteration 379, loss = 0.00502482\n",
      "Iteration 88, loss = 0.10533477\n",
      "Iteration 380, loss = 0.00475707\n",
      "Iteration 89, loss = 0.10441148\n",
      "Iteration 381, loss = 0.00520984\n",
      "Iteration 90, loss = 0.10582673\n",
      "Iteration 382, loss = 0.00544033\n",
      "Iteration 91, loss = 0.10302828\n",
      "Iteration 383, loss = 0.00456172\n",
      "Iteration 92, loss = 0.10179880\n",
      "Iteration 384, loss = 0.00445953\n",
      "Iteration 93, loss = 0.09996058\n",
      "Iteration 385, loss = 0.00449300\n",
      "Iteration 94, loss = 0.10091769\n",
      "Iteration 386, loss = 0.00450181\n",
      "Iteration 95, loss = 0.09870077\n",
      "Iteration 387, loss = 0.00432921Iteration 96, loss = 0.09709739\n",
      "\n",
      "Iteration 388, loss = 0.00439041\n",
      "Iteration 97, loss = 0.09798162\n",
      "Iteration 389, loss = 0.00460140\n",
      "Iteration 98, loss = 0.09656553\n",
      "Iteration 390, loss = 0.00446293\n",
      "Iteration 99, loss = 0.09384657\n",
      "Iteration 391, loss = 0.00440162\n",
      "Iteration 100, loss = 0.09558092\n",
      "Iteration 392, loss = 0.00430275\n",
      "Iteration 101, loss = 0.09732215\n",
      "Iteration 393, loss = 0.00414253\n",
      "Iteration 102, loss = 0.09250016\n",
      "Iteration 394, loss = 0.00428268\n",
      "Iteration 103, loss = 0.09396763\n",
      "Iteration 395, loss = 0.00426200\n",
      "Iteration 104, loss = 0.09084500\n",
      "Iteration 396, loss = 0.00408990\n",
      "Iteration 105, loss = 0.09197550\n",
      "Iteration 397, loss = 0.00404621\n",
      "Iteration 106, loss = 0.08909174\n",
      "Iteration 398, loss = 0.00398846\n",
      "Iteration 107, loss = 0.08906618\n",
      "Iteration 399, loss = 0.00407663\n",
      "Iteration 108, loss = 0.08740482\n",
      "Iteration 400, loss = 0.00403596\n",
      "Iteration 109, loss = 0.08898504\n",
      "Iteration 401, loss = 0.00400211\n",
      "Iteration 110, loss = 0.08667056\n",
      "Iteration 402, loss = 0.00385695\n",
      "Iteration 111, loss = 0.08867014\n",
      "Iteration 403, loss = 0.00413737\n",
      "Iteration 112, loss = 0.09052757\n",
      "Iteration 404, loss = 0.00385022\n",
      "Iteration 113, loss = 0.08841874\n",
      "Iteration 405, loss = 0.00394703\n",
      "Iteration 114, loss = 0.08724841\n",
      "Iteration 406, loss = 0.00391543\n",
      "Iteration 115, loss = 0.08420377\n",
      "Iteration 407, loss = 0.00448206\n",
      "Iteration 116, loss = 0.08520253\n",
      "Iteration 408, loss = 0.00388293\n",
      "Iteration 117, loss = 0.08344296\n",
      "Iteration 409, loss = 0.00407944\n",
      "Iteration 118, loss = 0.08835507\n",
      "Iteration 410, loss = 0.00373325\n",
      "Iteration 119, loss = 0.08117958\n",
      "Iteration 411, loss = 0.00385743\n",
      "Iteration 120, loss = 0.08145396\n",
      "Iteration 412, loss = 0.00376231\n",
      "Iteration 121, loss = 0.08552947\n",
      "Iteration 413, loss = 0.00363854\n",
      "Iteration 122, loss = 0.07722370\n",
      "Iteration 414, loss = 0.00360119\n",
      "Iteration 123, loss = 0.07889156\n",
      "Iteration 415, loss = 0.00358309\n",
      "Iteration 124, loss = 0.08253890\n",
      "Iteration 416, loss = 0.00353763\n",
      "Iteration 125, loss = 0.08436075\n",
      "Iteration 417, loss = 0.00366319\n",
      "Iteration 126, loss = 0.08049514\n",
      "Iteration 418, loss = 0.00372967\n",
      "Iteration 127, loss = 0.08146484\n",
      "Iteration 419, loss = 0.00374730\n",
      "Iteration 128, loss = 0.08532246\n",
      "Iteration 420, loss = 0.00387019\n",
      "Iteration 129, loss = 0.08065533\n",
      "Iteration 421, loss = 0.00379166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 130, loss = 0.07179826\n",
      "Iteration 131, loss = 0.07662318\n",
      "Iteration 132, loss = 0.07597142\n",
      "Iteration 133, loss = 0.07574338\n",
      "Iteration 134, loss = 0.06971525\n",
      "Iteration 135, loss = 0.07133808\n",
      "Iteration 136, loss = 0.06763780\n",
      "Iteration 137, loss = 0.06943039\n",
      "Iteration 138, loss = 0.06670927\n",
      "Iteration 139, loss = 0.06698652\n",
      "Iteration 140, loss = 0.06535339\n",
      "Iteration 141, loss = 0.06447786\n",
      "Iteration 142, loss = 0.06447272\n",
      "Iteration 1, loss = 1.06619103\n",
      "Iteration 143, loss = 0.06356512\n",
      "Iteration 2, loss = 0.82058001\n",
      "Iteration 144, loss = 0.06350341\n",
      "Iteration 3, loss = 0.68829534\n",
      "Iteration 145, loss = 0.06267869\n",
      "Iteration 4, loss = 0.59909741\n",
      "Iteration 146, loss = 0.06437583\n",
      "Iteration 5, loss = 0.54504579\n",
      "Iteration 147, loss = 0.06111619\n",
      "Iteration 6, loss = 0.50227985\n",
      "Iteration 148, loss = 0.06643296\n",
      "Iteration 7, loss = 0.46770292\n",
      "Iteration 149, loss = 0.06159870\n",
      "Iteration 8, loss = 0.44292344\n",
      "Iteration 150, loss = 0.06483830\n",
      "Iteration 9, loss = 0.41990091\n",
      "Iteration 151, loss = 0.05895809\n",
      "Iteration 10, loss = 0.40259553\n",
      "Iteration 152, loss = 0.05958319\n",
      "Iteration 11, loss = 0.38716535\n",
      "Iteration 153, loss = 0.06024540\n",
      "Iteration 12, loss = 0.37361302\n",
      "Iteration 154, loss = 0.05783305\n",
      "Iteration 13, loss = 0.36085649\n",
      "Iteration 155, loss = 0.06013747\n",
      "Iteration 14, loss = 0.35067088\n",
      "Iteration 156, loss = 0.05987555\n",
      "Iteration 15, loss = 0.34037613\n",
      "Iteration 157, loss = 0.05675437\n",
      "Iteration 16, loss = 0.33288089\n",
      "Iteration 158, loss = 0.05771119\n",
      "Iteration 17, loss = 0.32318431\n",
      "Iteration 159, loss = 0.05527051\n",
      "Iteration 18, loss = 0.31611096\n",
      "Iteration 160, loss = 0.05581987\n",
      "Iteration 19, loss = 0.30895852\n",
      "Iteration 161, loss = 0.05550102\n",
      "Iteration 20, loss = 0.30244768\n",
      "Iteration 162, loss = 0.05320494\n",
      "Iteration 21, loss = 0.29737734\n",
      "Iteration 163, loss = 0.05470958\n",
      "Iteration 22, loss = 0.29070331\n",
      "Iteration 164, loss = 0.05339606\n",
      "Iteration 23, loss = 0.28439299\n",
      "Iteration 165, loss = 0.05391084\n",
      "Iteration 24, loss = 0.28066732\n",
      "Iteration 166, loss = 0.05510500\n",
      "Iteration 25, loss = 0.27531515\n",
      "Iteration 167, loss = 0.05109605\n",
      "Iteration 26, loss = 0.27042321\n",
      "Iteration 168, loss = 0.05237576\n",
      "Iteration 27, loss = 0.26634030\n",
      "Iteration 169, loss = 0.05281155\n",
      "Iteration 28, loss = 0.26126152\n",
      "Iteration 170, loss = 0.05015228\n",
      "Iteration 29, loss = 0.25802906\n",
      "Iteration 171, loss = 0.04946877\n",
      "Iteration 30, loss = 0.25634980\n",
      "Iteration 172, loss = 0.04997172\n",
      "Iteration 31, loss = 0.25173511\n",
      "Iteration 173, loss = 0.05669866\n",
      "Iteration 32, loss = 0.24692302\n",
      "Iteration 174, loss = 0.05586287\n",
      "Iteration 33, loss = 0.24231271\n",
      "Iteration 175, loss = 0.05293716\n",
      "Iteration 34, loss = 0.23891218\n",
      "Iteration 176, loss = 0.05337095\n",
      "Iteration 35, loss = 0.23534329\n",
      "Iteration 177, loss = 0.05077858\n",
      "Iteration 36, loss = 0.23395370\n",
      "Iteration 178, loss = 0.04997455\n",
      "Iteration 37, loss = 0.22754050\n",
      "Iteration 179, loss = 0.05114949\n",
      "Iteration 38, loss = 0.22678583\n",
      "Iteration 180, loss = 0.04922258\n",
      "Iteration 39, loss = 0.22199599\n",
      "Iteration 181, loss = 0.04648621\n",
      "Iteration 40, loss = 0.21897952\n",
      "Iteration 182, loss = 0.04783595\n",
      "Iteration 41, loss = 0.21627287\n",
      "Iteration 183, loss = 0.04530288\n",
      "Iteration 42, loss = 0.21448322\n",
      "Iteration 184, loss = 0.04456046\n",
      "Iteration 43, loss = 0.21140783\n",
      "Iteration 185, loss = 0.04398201\n",
      "Iteration 44, loss = 0.20877897\n",
      "Iteration 186, loss = 0.04402477\n",
      "Iteration 45, loss = 0.20496597\n",
      "Iteration 187, loss = 0.04279806\n",
      "Iteration 46, loss = 0.20143122\n",
      "Iteration 188, loss = 0.04190609\n",
      "Iteration 47, loss = 0.20045700\n",
      "Iteration 189, loss = 0.04176571\n",
      "Iteration 48, loss = 0.19717603\n",
      "Iteration 190, loss = 0.04124340\n",
      "Iteration 49, loss = 0.19523265\n",
      "Iteration 191, loss = 0.04185314\n",
      "Iteration 50, loss = 0.19140580\n",
      "Iteration 192, loss = 0.04052266\n",
      "Iteration 51, loss = 0.19144904\n",
      "Iteration 193, loss = 0.04063472\n",
      "Iteration 52, loss = 0.18745981\n",
      "Iteration 194, loss = 0.04012587\n",
      "Iteration 53, loss = 0.18669104\n",
      "Iteration 195, loss = 0.03981690\n",
      "Iteration 54, loss = 0.18304945\n",
      "Iteration 196, loss = 0.03964999\n",
      "Iteration 55, loss = 0.17984911\n",
      "Iteration 197, loss = 0.03944445\n",
      "Iteration 56, loss = 0.17896842\n",
      "Iteration 198, loss = 0.04007260\n",
      "Iteration 57, loss = 0.17478652\n",
      "Iteration 199, loss = 0.03808610\n",
      "Iteration 58, loss = 0.17287765\n",
      "Iteration 200, loss = 0.03721834\n",
      "Iteration 59, loss = 0.17062225\n",
      "Iteration 201, loss = 0.03893696\n",
      "Iteration 60, loss = 0.16840497\n",
      "Iteration 202, loss = 0.03771022\n",
      "Iteration 61, loss = 0.16616838\n",
      "Iteration 203, loss = 0.03907886\n",
      "Iteration 62, loss = 0.16413106\n",
      "Iteration 204, loss = 0.03822165\n",
      "Iteration 63, loss = 0.16260177\n",
      "Iteration 205, loss = 0.03661845\n",
      "Iteration 64, loss = 0.16246404\n",
      "Iteration 206, loss = 0.03700708\n",
      "Iteration 65, loss = 0.15595232\n",
      "Iteration 207, loss = 0.03763422\n",
      "Iteration 66, loss = 0.15668073\n",
      "Iteration 208, loss = 0.03919828\n",
      "Iteration 67, loss = 0.15430268\n",
      "Iteration 68, loss = 0.15081069\n",
      "Iteration 209, loss = 0.04481627\n",
      "Iteration 69, loss = 0.14995176\n",
      "Iteration 210, loss = 0.04938416\n",
      "Iteration 70, loss = 0.14740941\n",
      "Iteration 211, loss = 0.04784383\n",
      "Iteration 71, loss = 0.14510650\n",
      "Iteration 212, loss = 0.04500275\n",
      "Iteration 72, loss = 0.14478750\n",
      "Iteration 213, loss = 0.03580794\n",
      "Iteration 73, loss = 0.14250632\n",
      "Iteration 214, loss = 0.03485727\n",
      "Iteration 74, loss = 0.14034688\n",
      "Iteration 215, loss = 0.03360823\n",
      "Iteration 216, loss = 0.03523472\n",
      "Iteration 75, loss = 0.14036852\n",
      "Iteration 76, loss = 0.13535453\n",
      "Iteration 217, loss = 0.03867425\n",
      "Iteration 77, loss = 0.13460269\n",
      "Iteration 218, loss = 0.03477317\n",
      "Iteration 78, loss = 0.13255507\n",
      "Iteration 219, loss = 0.03777012\n",
      "Iteration 79, loss = 0.13346542\n",
      "Iteration 220, loss = 0.03421280\n",
      "Iteration 80, loss = 0.12886075\n",
      "Iteration 221, loss = 0.03454736\n",
      "Iteration 81, loss = 0.12972320\n",
      "Iteration 222, loss = 0.03629009\n",
      "Iteration 82, loss = 0.12835987\n",
      "Iteration 223, loss = 0.03270023\n",
      "Iteration 83, loss = 0.12571194\n",
      "Iteration 224, loss = 0.03186255\n",
      "Iteration 84, loss = 0.12361187\n",
      "Iteration 225, loss = 0.03306551\n",
      "Iteration 85, loss = 0.12258012\n",
      "Iteration 226, loss = 0.02991991\n",
      "Iteration 86, loss = 0.12518196\n",
      "Iteration 227, loss = 0.03142111\n",
      "Iteration 87, loss = 0.12686577\n",
      "Iteration 228, loss = 0.02954364\n",
      "Iteration 88, loss = 0.12707694\n",
      "Iteration 229, loss = 0.02983258\n",
      "Iteration 89, loss = 0.11903687\n",
      "Iteration 230, loss = 0.03037513\n",
      "Iteration 90, loss = 0.11888947\n",
      "Iteration 231, loss = 0.02813720\n",
      "Iteration 91, loss = 0.11589165\n",
      "Iteration 232, loss = 0.03018703\n",
      "Iteration 92, loss = 0.11587713\n",
      "Iteration 233, loss = 0.03179538\n",
      "Iteration 93, loss = 0.11429797\n",
      "Iteration 234, loss = 0.03035479\n",
      "Iteration 94, loss = 0.11205226\n",
      "Iteration 235, loss = 0.02993221\n",
      "Iteration 95, loss = 0.11138150\n",
      "Iteration 236, loss = 0.02929200\n",
      "Iteration 96, loss = 0.11185674\n",
      "Iteration 237, loss = 0.03348475\n",
      "Iteration 97, loss = 0.10865710\n",
      "Iteration 238, loss = 0.03197706\n",
      "Iteration 98, loss = 0.10779678\n",
      "Iteration 239, loss = 0.03185964\n",
      "Iteration 99, loss = 0.10533956\n",
      "Iteration 240, loss = 0.03301977\n",
      "Iteration 100, loss = 0.10892826\n",
      "Iteration 241, loss = 0.03266638\n",
      "Iteration 101, loss = 0.10397076\n",
      "Iteration 242, loss = 0.02965152\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 102, loss = 0.10456674\n",
      "Iteration 103, loss = 0.10455040\n",
      "Iteration 104, loss = 0.10175493\n",
      "Iteration 105, loss = 0.10213610\n",
      "Iteration 106, loss = 0.09840264\n",
      "Iteration 107, loss = 0.09751915\n",
      "Iteration 108, loss = 0.09632946\n",
      "Iteration 109, loss = 0.09673368\n",
      "Iteration 110, loss = 0.09398698\n",
      "Iteration 111, loss = 0.09615213\n",
      "Iteration 112, loss = 0.09571395\n",
      "Iteration 113, loss = 0.09454586\n",
      "Iteration 114, loss = 0.09097711\n",
      "Iteration 1, loss = 1.28824585\n",
      "Iteration 115, loss = 0.08954752\n",
      "Iteration 2, loss = 0.87995395\n",
      "Iteration 116, loss = 0.09009162\n",
      "Iteration 3, loss = 0.69874687\n",
      "Iteration 117, loss = 0.08982847\n",
      "Iteration 4, loss = 0.60261356\n",
      "Iteration 118, loss = 0.08930943\n",
      "Iteration 5, loss = 0.54095028\n",
      "Iteration 119, loss = 0.09060630\n",
      "Iteration 6, loss = 0.50072594\n",
      "Iteration 120, loss = 0.08692648\n",
      "Iteration 7, loss = 0.47108615\n",
      "Iteration 121, loss = 0.08921102\n",
      "Iteration 8, loss = 0.44701171\n",
      "Iteration 122, loss = 0.08773449\n",
      "Iteration 9, loss = 0.42658907\n",
      "Iteration 123, loss = 0.08351304\n",
      "Iteration 10, loss = 0.40856303\n",
      "Iteration 124, loss = 0.08555705\n",
      "Iteration 11, loss = 0.39440599\n",
      "Iteration 125, loss = 0.08489324\n",
      "Iteration 12, loss = 0.38023155\n",
      "Iteration 126, loss = 0.08654013\n",
      "Iteration 13, loss = 0.36883753\n",
      "Iteration 14, loss = 0.35815245\n",
      "Iteration 127, loss = 0.08590246\n",
      "Iteration 15, loss = 0.34769661\n",
      "Iteration 128, loss = 0.08211267\n",
      "Iteration 16, loss = 0.33972149\n",
      "Iteration 129, loss = 0.08166537\n",
      "Iteration 17, loss = 0.33130572\n",
      "Iteration 130, loss = 0.08077705\n",
      "Iteration 18, loss = 0.32355192\n",
      "Iteration 131, loss = 0.07893429\n",
      "Iteration 19, loss = 0.31667827\n",
      "Iteration 132, loss = 0.07880809\n",
      "Iteration 20, loss = 0.30887686\n",
      "Iteration 133, loss = 0.07859554\n",
      "Iteration 21, loss = 0.30539141\n",
      "Iteration 134, loss = 0.07495785\n",
      "Iteration 22, loss = 0.29825260\n",
      "Iteration 135, loss = 0.07469804\n",
      "Iteration 23, loss = 0.29235329\n",
      "Iteration 136, loss = 0.07426986\n",
      "Iteration 24, loss = 0.29025672\n",
      "Iteration 137, loss = 0.07295255\n",
      "Iteration 25, loss = 0.28028548\n",
      "Iteration 138, loss = 0.07500547\n",
      "Iteration 26, loss = 0.28128697\n",
      "Iteration 139, loss = 0.07414345\n",
      "Iteration 27, loss = 0.27313999\n",
      "Iteration 140, loss = 0.07284396\n",
      "Iteration 28, loss = 0.26942679\n",
      "Iteration 141, loss = 0.06948632\n",
      "Iteration 29, loss = 0.26518322\n",
      "Iteration 142, loss = 0.07139891\n",
      "Iteration 30, loss = 0.25994274\n",
      "Iteration 143, loss = 0.07066661\n",
      "Iteration 31, loss = 0.25492874\n",
      "Iteration 144, loss = 0.07107381\n",
      "Iteration 32, loss = 0.25298070\n",
      "Iteration 145, loss = 0.06856202\n",
      "Iteration 33, loss = 0.24827533\n",
      "Iteration 146, loss = 0.06584855\n",
      "Iteration 34, loss = 0.24417835\n",
      "Iteration 147, loss = 0.06620009\n",
      "Iteration 35, loss = 0.24136241\n",
      "Iteration 148, loss = 0.06606117\n",
      "Iteration 36, loss = 0.23880025\n",
      "Iteration 149, loss = 0.06706934\n",
      "Iteration 37, loss = 0.23340953\n",
      "Iteration 150, loss = 0.06592923\n",
      "Iteration 38, loss = 0.23156836\n",
      "Iteration 151, loss = 0.06463479\n",
      "Iteration 39, loss = 0.22712859\n",
      "Iteration 152, loss = 0.06350955\n",
      "Iteration 40, loss = 0.22535624\n",
      "Iteration 153, loss = 0.06424520\n",
      "Iteration 41, loss = 0.22183358\n",
      "Iteration 154, loss = 0.06182212\n",
      "Iteration 42, loss = 0.21731055\n",
      "Iteration 155, loss = 0.06108321\n",
      "Iteration 43, loss = 0.21473801\n",
      "Iteration 156, loss = 0.06222983\n",
      "Iteration 44, loss = 0.21124965\n",
      "Iteration 157, loss = 0.06022522\n",
      "Iteration 45, loss = 0.20993325\n",
      "Iteration 158, loss = 0.06168965\n",
      "Iteration 46, loss = 0.20660169\n",
      "Iteration 159, loss = 0.05915284\n",
      "Iteration 47, loss = 0.20384443\n",
      "Iteration 160, loss = 0.05948698\n",
      "Iteration 48, loss = 0.20412727\n",
      "Iteration 161, loss = 0.05818716\n",
      "Iteration 49, loss = 0.19989390\n",
      "Iteration 162, loss = 0.05911455\n",
      "Iteration 50, loss = 0.19815353\n",
      "Iteration 163, loss = 0.05983789\n",
      "Iteration 51, loss = 0.19454427\n",
      "Iteration 164, loss = 0.05817654\n",
      "Iteration 52, loss = 0.19492569\n",
      "Iteration 165, loss = 0.05611436\n",
      "Iteration 53, loss = 0.18983235\n",
      "Iteration 166, loss = 0.05636078\n",
      "Iteration 54, loss = 0.18693368\n",
      "Iteration 167, loss = 0.05514824\n",
      "Iteration 55, loss = 0.18592558\n",
      "Iteration 168, loss = 0.05742202\n",
      "Iteration 56, loss = 0.18380369\n",
      "Iteration 169, loss = 0.05859242\n",
      "Iteration 57, loss = 0.18185641\n",
      "Iteration 170, loss = 0.05457251\n",
      "Iteration 58, loss = 0.17984636\n",
      "Iteration 171, loss = 0.05815608\n",
      "Iteration 59, loss = 0.17761420\n",
      "Iteration 172, loss = 0.05432081\n",
      "Iteration 60, loss = 0.17726402\n",
      "Iteration 173, loss = 0.05224043\n",
      "Iteration 61, loss = 0.17135084\n",
      "Iteration 174, loss = 0.05302547\n",
      "Iteration 62, loss = 0.17210467\n",
      "Iteration 175, loss = 0.05157671\n",
      "Iteration 63, loss = 0.16744132\n",
      "Iteration 176, loss = 0.05181627\n",
      "Iteration 64, loss = 0.16445050\n",
      "Iteration 177, loss = 0.05166056\n",
      "Iteration 65, loss = 0.16279873\n",
      "Iteration 178, loss = 0.05334549\n",
      "Iteration 66, loss = 0.16089414\n",
      "Iteration 179, loss = 0.04951127\n",
      "Iteration 67, loss = 0.15942461\n",
      "Iteration 180, loss = 0.04857137\n",
      "Iteration 68, loss = 0.15820340\n",
      "Iteration 181, loss = 0.04854095\n",
      "Iteration 69, loss = 0.15827783\n",
      "Iteration 182, loss = 0.04683103\n",
      "Iteration 70, loss = 0.15433123\n",
      "Iteration 183, loss = 0.04808780\n",
      "Iteration 71, loss = 0.14983851\n",
      "Iteration 184, loss = 0.04776531\n",
      "Iteration 72, loss = 0.15261158\n",
      "Iteration 185, loss = 0.04587228\n",
      "Iteration 73, loss = 0.14817166\n",
      "Iteration 186, loss = 0.04573029\n",
      "Iteration 74, loss = 0.14577506\n",
      "Iteration 187, loss = 0.04548009\n",
      "Iteration 75, loss = 0.14513508\n",
      "Iteration 188, loss = 0.04609307\n",
      "Iteration 76, loss = 0.14339644\n",
      "Iteration 189, loss = 0.04458059\n",
      "Iteration 77, loss = 0.14342257\n",
      "Iteration 190, loss = 0.04301079\n",
      "Iteration 78, loss = 0.13836757\n",
      "Iteration 191, loss = 0.04297895\n",
      "Iteration 79, loss = 0.13988294\n",
      "Iteration 192, loss = 0.04263787\n",
      "Iteration 80, loss = 0.13584548\n",
      "Iteration 193, loss = 0.04277689\n",
      "Iteration 81, loss = 0.13591449\n",
      "Iteration 194, loss = 0.04224342\n",
      "Iteration 82, loss = 0.13225289\n",
      "Iteration 195, loss = 0.04055333\n",
      "Iteration 83, loss = 0.13253327\n",
      "Iteration 196, loss = 0.04235967\n",
      "Iteration 84, loss = 0.13176716\n",
      "Iteration 197, loss = 0.04401591\n",
      "Iteration 85, loss = 0.12794392\n",
      "Iteration 198, loss = 0.04156433\n",
      "Iteration 86, loss = 0.12930566\n",
      "Iteration 199, loss = 0.03992357\n",
      "Iteration 87, loss = 0.12869717\n",
      "Iteration 200, loss = 0.04079251\n",
      "Iteration 88, loss = 0.12775166\n",
      "Iteration 201, loss = 0.03911908\n",
      "Iteration 89, loss = 0.12732111\n",
      "Iteration 202, loss = 0.03877769\n",
      "Iteration 90, loss = 0.12357696\n",
      "Iteration 203, loss = 0.03816414\n",
      "Iteration 91, loss = 0.12227171\n",
      "Iteration 204, loss = 0.03773894\n",
      "Iteration 92, loss = 0.12405668\n",
      "Iteration 205, loss = 0.03722942\n",
      "Iteration 93, loss = 0.12741199\n",
      "Iteration 206, loss = 0.03839863\n",
      "Iteration 94, loss = 0.12367849\n",
      "Iteration 207, loss = 0.03799398\n",
      "Iteration 95, loss = 0.11920337\n",
      "Iteration 208, loss = 0.04122384\n",
      "Iteration 96, loss = 0.11969479\n",
      "Iteration 209, loss = 0.04013059\n",
      "Iteration 97, loss = 0.11778688\n",
      "Iteration 210, loss = 0.03920476\n",
      "Iteration 98, loss = 0.11367582\n",
      "Iteration 211, loss = 0.04035132\n",
      "Iteration 99, loss = 0.11275089\n",
      "Iteration 212, loss = 0.03762919\n",
      "Iteration 100, loss = 0.11069065\n",
      "Iteration 213, loss = 0.03450849\n",
      "Iteration 214, loss = 0.03830533\n",
      "Iteration 101, loss = 0.10932411\n",
      "Iteration 215, loss = 0.03740686\n",
      "Iteration 102, loss = 0.10942177\n",
      "Iteration 216, loss = 0.03321303\n",
      "Iteration 103, loss = 0.10813261\n",
      "Iteration 217, loss = 0.03621661\n",
      "Iteration 104, loss = 0.10733924\n",
      "Iteration 105, loss = 0.10662067\n",
      "Iteration 218, loss = 0.03512802\n",
      "Iteration 106, loss = 0.10413919\n",
      "Iteration 219, loss = 0.03393566\n",
      "Iteration 107, loss = 0.10442616\n",
      "Iteration 220, loss = 0.03241329\n",
      "Iteration 108, loss = 0.10167845\n",
      "Iteration 221, loss = 0.03307219\n",
      "Iteration 109, loss = 0.10113096\n",
      "Iteration 222, loss = 0.03186558\n",
      "Iteration 110, loss = 0.09984042\n",
      "Iteration 223, loss = 0.03235051\n",
      "Iteration 111, loss = 0.09961875\n",
      "Iteration 224, loss = 0.03328522\n",
      "Iteration 112, loss = 0.09903156\n",
      "Iteration 225, loss = 0.03255748\n",
      "Iteration 113, loss = 0.09703273\n",
      "Iteration 226, loss = 0.03257535\n",
      "Iteration 114, loss = 0.09750701\n",
      "Iteration 227, loss = 0.03121630\n",
      "Iteration 228, loss = 0.03250031Iteration 115, loss = 0.09579112\n",
      "\n",
      "Iteration 229, loss = 0.03122293Iteration 116, loss = 0.09916558\n",
      "\n",
      "Iteration 230, loss = 0.03166516\n",
      "Iteration 117, loss = 0.09998715\n",
      "Iteration 231, loss = 0.03036642\n",
      "Iteration 118, loss = 0.09293408\n",
      "Iteration 232, loss = 0.03380485\n",
      "Iteration 119, loss = 0.09207097\n",
      "Iteration 233, loss = 0.03253531\n",
      "Iteration 120, loss = 0.09312169\n",
      "Iteration 234, loss = 0.02890879\n",
      "Iteration 121, loss = 0.08998419\n",
      "Iteration 235, loss = 0.03005475\n",
      "Iteration 122, loss = 0.08983981\n",
      "Iteration 236, loss = 0.02712703\n",
      "Iteration 123, loss = 0.09074724\n",
      "Iteration 237, loss = 0.02857540\n",
      "Iteration 124, loss = 0.08775214\n",
      "Iteration 238, loss = 0.02813569\n",
      "Iteration 125, loss = 0.08674548\n",
      "Iteration 239, loss = 0.02696812\n",
      "Iteration 126, loss = 0.08793396\n",
      "Iteration 240, loss = 0.02852692\n",
      "Iteration 127, loss = 0.08590393\n",
      "Iteration 241, loss = 0.02746865\n",
      "Iteration 128, loss = 0.08777584\n",
      "Iteration 242, loss = 0.02672480\n",
      "Iteration 129, loss = 0.08474473\n",
      "Iteration 243, loss = 0.02713822\n",
      "Iteration 130, loss = 0.08493272\n",
      "Iteration 244, loss = 0.02614731\n",
      "Iteration 131, loss = 0.08066580\n",
      "Iteration 245, loss = 0.02667477\n",
      "Iteration 132, loss = 0.08464077\n",
      "Iteration 133, loss = 0.08357625\n",
      "Iteration 246, loss = 0.02847414\n",
      "Iteration 134, loss = 0.08318366\n",
      "Iteration 247, loss = 0.02692313\n",
      "Iteration 135, loss = 0.08199668\n",
      "Iteration 248, loss = 0.02939881\n",
      "Iteration 136, loss = 0.08111464\n",
      "Iteration 249, loss = 0.02611066\n",
      "Iteration 137, loss = 0.08137532\n",
      "Iteration 250, loss = 0.02575648\n",
      "Iteration 138, loss = 0.07943349\n",
      "Iteration 251, loss = 0.02653999\n",
      "Iteration 139, loss = 0.07842210\n",
      "Iteration 252, loss = 0.02734905\n",
      "Iteration 140, loss = 0.07707432\n",
      "Iteration 253, loss = 0.02777742\n",
      "Iteration 141, loss = 0.07658618\n",
      "Iteration 254, loss = 0.02568006\n",
      "Iteration 142, loss = 0.07330727\n",
      "Iteration 255, loss = 0.02358541\n",
      "Iteration 143, loss = 0.07315038\n",
      "Iteration 256, loss = 0.02373807\n",
      "Iteration 257, loss = 0.02335660\n",
      "Iteration 144, loss = 0.07248222\n",
      "Iteration 258, loss = 0.02488098\n",
      "Iteration 145, loss = 0.07279114\n",
      "Iteration 259, loss = 0.02358071\n",
      "Iteration 146, loss = 0.07012590\n",
      "Iteration 260, loss = 0.02543394\n",
      "Iteration 147, loss = 0.07038948\n",
      "Iteration 148, loss = 0.06954489\n",
      "Iteration 261, loss = 0.02492623\n",
      "Iteration 149, loss = 0.07012390\n",
      "Iteration 262, loss = 0.02263071\n",
      "Iteration 150, loss = 0.06802013\n",
      "Iteration 263, loss = 0.02497330\n",
      "Iteration 151, loss = 0.06707466\n",
      "Iteration 264, loss = 0.02445654\n",
      "Iteration 152, loss = 0.06825665\n",
      "Iteration 265, loss = 0.02352800\n",
      "Iteration 153, loss = 0.06705950\n",
      "Iteration 266, loss = 0.02308160\n",
      "Iteration 154, loss = 0.06481896\n",
      "Iteration 267, loss = 0.02309617\n",
      "Iteration 155, loss = 0.06453930\n",
      "Iteration 268, loss = 0.02609699\n",
      "Iteration 156, loss = 0.06343810\n",
      "Iteration 269, loss = 0.02530492\n",
      "Iteration 157, loss = 0.06541260\n",
      "Iteration 270, loss = 0.02747367\n",
      "Iteration 158, loss = 0.06461005\n",
      "Iteration 271, loss = 0.01984278\n",
      "Iteration 159, loss = 0.06350391\n",
      "Iteration 272, loss = 0.02360395\n",
      "Iteration 160, loss = 0.06614057\n",
      "Iteration 273, loss = 0.02093288\n",
      "Iteration 161, loss = 0.06278792\n",
      "Iteration 274, loss = 0.02187995\n",
      "Iteration 162, loss = 0.06285884\n",
      "Iteration 275, loss = 0.01957810\n",
      "Iteration 163, loss = 0.06006161\n",
      "Iteration 276, loss = 0.01955117\n",
      "Iteration 164, loss = 0.05982095\n",
      "Iteration 277, loss = 0.01991589\n",
      "Iteration 165, loss = 0.06011478\n",
      "Iteration 278, loss = 0.01882626\n",
      "Iteration 166, loss = 0.05772016\n",
      "Iteration 279, loss = 0.01924397\n",
      "Iteration 167, loss = 0.05868659\n",
      "Iteration 280, loss = 0.01967054\n",
      "Iteration 168, loss = 0.05937718\n",
      "Iteration 281, loss = 0.01927140\n",
      "Iteration 169, loss = 0.05934917\n",
      "Iteration 282, loss = 0.01875583\n",
      "Iteration 170, loss = 0.05691329\n",
      "Iteration 283, loss = 0.01880469\n",
      "Iteration 171, loss = 0.05658332\n",
      "Iteration 284, loss = 0.01797598\n",
      "Iteration 172, loss = 0.05675406\n",
      "Iteration 285, loss = 0.01840148\n",
      "Iteration 173, loss = 0.05540759\n",
      "Iteration 286, loss = 0.01791047\n",
      "Iteration 174, loss = 0.05588177\n",
      "Iteration 287, loss = 0.01745762\n",
      "Iteration 175, loss = 0.05770049\n",
      "Iteration 288, loss = 0.01759134\n",
      "Iteration 176, loss = 0.05359570\n",
      "Iteration 289, loss = 0.01741886\n",
      "Iteration 177, loss = 0.05437440\n",
      "Iteration 290, loss = 0.01696523\n",
      "Iteration 178, loss = 0.05172385\n",
      "Iteration 291, loss = 0.01726632\n",
      "Iteration 292, loss = 0.01756664\n",
      "Iteration 179, loss = 0.05398363\n",
      "Iteration 293, loss = 0.01735087\n",
      "Iteration 180, loss = 0.05229382\n",
      "Iteration 294, loss = 0.01704404\n",
      "Iteration 181, loss = 0.05128092\n",
      "Iteration 295, loss = 0.01632983\n",
      "Iteration 182, loss = 0.05139217\n",
      "Iteration 296, loss = 0.01690016\n",
      "Iteration 183, loss = 0.04964241\n",
      "Iteration 297, loss = 0.01727369\n",
      "Iteration 184, loss = 0.04949334\n",
      "Iteration 298, loss = 0.01799251\n",
      "Iteration 185, loss = 0.05111699\n",
      "Iteration 299, loss = 0.01750054\n",
      "Iteration 186, loss = 0.04889804\n",
      "Iteration 300, loss = 0.01762183\n",
      "Iteration 187, loss = 0.04759116\n",
      "Iteration 301, loss = 0.01626398\n",
      "Iteration 188, loss = 0.05134250\n",
      "Iteration 302, loss = 0.01566142\n",
      "Iteration 189, loss = 0.04878847\n",
      "Iteration 303, loss = 0.01641609\n",
      "Iteration 190, loss = 0.04786744\n",
      "Iteration 304, loss = 0.01640906\n",
      "Iteration 191, loss = 0.04718681\n",
      "Iteration 305, loss = 0.01524165\n",
      "Iteration 192, loss = 0.04748618\n",
      "Iteration 306, loss = 0.01472988\n",
      "Iteration 193, loss = 0.04871610\n",
      "Iteration 307, loss = 0.01470005\n",
      "Iteration 194, loss = 0.04530671\n",
      "Iteration 308, loss = 0.01443845\n",
      "Iteration 195, loss = 0.04620555\n",
      "Iteration 309, loss = 0.01475092\n",
      "Iteration 196, loss = 0.04961126\n",
      "Iteration 310, loss = 0.01640773\n",
      "Iteration 197, loss = 0.04724469\n",
      "Iteration 311, loss = 0.01480866\n",
      "Iteration 198, loss = 0.04900486\n",
      "Iteration 312, loss = 0.01398923\n",
      "Iteration 199, loss = 0.05234615\n",
      "Iteration 313, loss = 0.01451365\n",
      "Iteration 200, loss = 0.04483250\n",
      "Iteration 314, loss = 0.01491068\n",
      "Iteration 201, loss = 0.04188628\n",
      "Iteration 315, loss = 0.01452591\n",
      "Iteration 202, loss = 0.04151298\n",
      "Iteration 316, loss = 0.01407492\n",
      "Iteration 203, loss = 0.04136128\n",
      "Iteration 317, loss = 0.01332789\n",
      "Iteration 204, loss = 0.04029974\n",
      "Iteration 318, loss = 0.01338250\n",
      "Iteration 205, loss = 0.04267691\n",
      "Iteration 319, loss = 0.01362813\n",
      "Iteration 206, loss = 0.04496040\n",
      "Iteration 207, loss = 0.04292545\n",
      "Iteration 320, loss = 0.01341726\n",
      "Iteration 208, loss = 0.04049635\n",
      "Iteration 321, loss = 0.01395387\n",
      "Iteration 209, loss = 0.03862981\n",
      "Iteration 322, loss = 0.01361871\n",
      "Iteration 210, loss = 0.04179502\n",
      "Iteration 323, loss = 0.01268224\n",
      "Iteration 211, loss = 0.04274503\n",
      "Iteration 324, loss = 0.01534674\n",
      "Iteration 212, loss = 0.03904879\n",
      "Iteration 325, loss = 0.01363523\n",
      "Iteration 326, loss = 0.01377274\n",
      "Iteration 213, loss = 0.03797575\n",
      "Iteration 327, loss = 0.01303061\n",
      "Iteration 214, loss = 0.03662476\n",
      "Iteration 328, loss = 0.01340871\n",
      "Iteration 215, loss = 0.03710764\n",
      "Iteration 329, loss = 0.01223617\n",
      "Iteration 216, loss = 0.03629006\n",
      "Iteration 330, loss = 0.01454941\n",
      "Iteration 217, loss = 0.03541533\n",
      "Iteration 331, loss = 0.01785404\n",
      "Iteration 218, loss = 0.03480582\n",
      "Iteration 332, loss = 0.01783608\n",
      "Iteration 219, loss = 0.03446705\n",
      "Iteration 333, loss = 0.01549963\n",
      "Iteration 220, loss = 0.03471521\n",
      "Iteration 334, loss = 0.01358732\n",
      "Iteration 221, loss = 0.03338051\n",
      "Iteration 335, loss = 0.01437194\n",
      "Iteration 222, loss = 0.03244574\n",
      "Iteration 336, loss = 0.01254262\n",
      "Iteration 223, loss = 0.03209044\n",
      "Iteration 337, loss = 0.01225086\n",
      "Iteration 224, loss = 0.03318026\n",
      "Iteration 338, loss = 0.01147974\n",
      "Iteration 225, loss = 0.03356427\n",
      "Iteration 339, loss = 0.01160438\n",
      "Iteration 226, loss = 0.03265234\n",
      "Iteration 340, loss = 0.01241069\n",
      "Iteration 227, loss = 0.03256944\n",
      "Iteration 341, loss = 0.01184974\n",
      "Iteration 228, loss = 0.03077402\n",
      "Iteration 342, loss = 0.01131252\n",
      "Iteration 229, loss = 0.03048319\n",
      "Iteration 343, loss = 0.01069999\n",
      "Iteration 230, loss = 0.03029168\n",
      "Iteration 231, loss = 0.03038588Iteration 344, loss = 0.01080971\n",
      "\n",
      "Iteration 232, loss = 0.02981672\n",
      "Iteration 345, loss = 0.01071129\n",
      "Iteration 346, loss = 0.01062629\n",
      "Iteration 233, loss = 0.02945398\n",
      "Iteration 234, loss = 0.02911889\n",
      "Iteration 347, loss = 0.01038094\n",
      "Iteration 348, loss = 0.01099337\n",
      "Iteration 235, loss = 0.03067176\n",
      "Iteration 236, loss = 0.03018193\n",
      "Iteration 349, loss = 0.01040402\n",
      "Iteration 350, loss = 0.01045861\n",
      "Iteration 237, loss = 0.03132034\n",
      "Iteration 351, loss = 0.01022375\n",
      "Iteration 238, loss = 0.02989509\n",
      "Iteration 352, loss = 0.01000947\n",
      "Iteration 239, loss = 0.02941458\n",
      "Iteration 353, loss = 0.01016195\n",
      "Iteration 240, loss = 0.02813401\n",
      "Iteration 354, loss = 0.01009745\n",
      "Iteration 241, loss = 0.02841507\n",
      "Iteration 355, loss = 0.01021595\n",
      "Iteration 242, loss = 0.02786151\n",
      "Iteration 356, loss = 0.01091053\n",
      "Iteration 243, loss = 0.02979414\n",
      "Iteration 357, loss = 0.01042588\n",
      "Iteration 244, loss = 0.02847161\n",
      "Iteration 358, loss = 0.01006582\n",
      "Iteration 245, loss = 0.02580376\n",
      "Iteration 359, loss = 0.00951184\n",
      "Iteration 246, loss = 0.02683152\n",
      "Iteration 360, loss = 0.00982128\n",
      "Iteration 247, loss = 0.02562602\n",
      "Iteration 361, loss = 0.00957459\n",
      "Iteration 248, loss = 0.02619078\n",
      "Iteration 362, loss = 0.01025410\n",
      "Iteration 249, loss = 0.02595976\n",
      "Iteration 363, loss = 0.00991340\n",
      "Iteration 250, loss = 0.02673573\n",
      "Iteration 364, loss = 0.01034368\n",
      "Iteration 251, loss = 0.02563474\n",
      "Iteration 365, loss = 0.01193128\n",
      "Iteration 252, loss = 0.02506054\n",
      "Iteration 366, loss = 0.01008096\n",
      "Iteration 253, loss = 0.02456800\n",
      "Iteration 367, loss = 0.01088359\n",
      "Iteration 254, loss = 0.02606676\n",
      "Iteration 368, loss = 0.00919896\n",
      "Iteration 255, loss = 0.02640730\n",
      "Iteration 369, loss = 0.00874384\n",
      "Iteration 256, loss = 0.02648067\n",
      "Iteration 370, loss = 0.00916230\n",
      "Iteration 257, loss = 0.02623771\n",
      "Iteration 371, loss = 0.00931533\n",
      "Iteration 258, loss = 0.02406324\n",
      "Iteration 372, loss = 0.00894047\n",
      "Iteration 259, loss = 0.02307841\n",
      "Iteration 373, loss = 0.00910232\n",
      "Iteration 260, loss = 0.02288064\n",
      "Iteration 374, loss = 0.00879636\n",
      "Iteration 261, loss = 0.02373060\n",
      "Iteration 375, loss = 0.00920846\n",
      "Iteration 262, loss = 0.02221256\n",
      "Iteration 376, loss = 0.00878991\n",
      "Iteration 263, loss = 0.02403746\n",
      "Iteration 377, loss = 0.00872509\n",
      "Iteration 264, loss = 0.02219731\n",
      "Iteration 378, loss = 0.00845210\n",
      "Iteration 265, loss = 0.02245226\n",
      "Iteration 379, loss = 0.00828201\n",
      "Iteration 266, loss = 0.02205415\n",
      "Iteration 380, loss = 0.00887888\n",
      "Iteration 267, loss = 0.02093945\n",
      "Iteration 381, loss = 0.00834255\n",
      "Iteration 268, loss = 0.02025745\n",
      "Iteration 382, loss = 0.00852216\n",
      "Iteration 269, loss = 0.02054317\n",
      "Iteration 383, loss = 0.00796217\n",
      "Iteration 270, loss = 0.02049417\n",
      "Iteration 384, loss = 0.00807575\n",
      "Iteration 271, loss = 0.02137032\n",
      "Iteration 385, loss = 0.00784550\n",
      "Iteration 272, loss = 0.02089379\n",
      "Iteration 386, loss = 0.00784288\n",
      "Iteration 273, loss = 0.02037152\n",
      "Iteration 387, loss = 0.00786512\n",
      "Iteration 274, loss = 0.01994414\n",
      "Iteration 388, loss = 0.00762220\n",
      "Iteration 275, loss = 0.02070666\n",
      "Iteration 389, loss = 0.00743929\n",
      "Iteration 276, loss = 0.02033054\n",
      "Iteration 390, loss = 0.00789895\n",
      "Iteration 277, loss = 0.02134987\n",
      "Iteration 391, loss = 0.00819392\n",
      "Iteration 278, loss = 0.02263550\n",
      "Iteration 392, loss = 0.00779614\n",
      "Iteration 279, loss = 0.02465455\n",
      "Iteration 393, loss = 0.00832370\n",
      "Iteration 280, loss = 0.02601624\n",
      "Iteration 394, loss = 0.00774340\n",
      "Iteration 281, loss = 0.02790016\n",
      "Iteration 395, loss = 0.00756125\n",
      "Iteration 282, loss = 0.02510282\n",
      "Iteration 396, loss = 0.00747233\n",
      "Iteration 283, loss = 0.02404667\n",
      "Iteration 397, loss = 0.00750176\n",
      "Iteration 284, loss = 0.02357623\n",
      "Iteration 398, loss = 0.00732134\n",
      "Iteration 285, loss = 0.02121448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 399, loss = 0.00719673\n",
      "Iteration 400, loss = 0.00698374\n",
      "Iteration 401, loss = 0.00728963\n",
      "Iteration 402, loss = 0.00792799\n",
      "Iteration 403, loss = 0.00757261\n",
      "Iteration 404, loss = 0.00875436\n",
      "Iteration 405, loss = 0.00730697\n",
      "Iteration 406, loss = 0.00715131\n",
      "Iteration 407, loss = 0.00682225\n",
      "Iteration 408, loss = 0.00705597\n",
      "Iteration 409, loss = 0.00656101\n",
      "Iteration 410, loss = 0.00687308\n",
      "Iteration 411, loss = 0.00655848\n",
      "Iteration 1, loss = 1.42576738\n",
      "Iteration 412, loss = 0.00634469\n",
      "Iteration 2, loss = 0.94869787\n",
      "Iteration 413, loss = 0.00625830\n",
      "Iteration 3, loss = 0.79519601\n",
      "Iteration 414, loss = 0.00630281\n",
      "Iteration 4, loss = 0.65892458\n",
      "Iteration 415, loss = 0.00622709\n",
      "Iteration 5, loss = 0.58289461\n",
      "Iteration 416, loss = 0.00608623\n",
      "Iteration 6, loss = 0.52933550\n",
      "Iteration 417, loss = 0.00609534\n",
      "Iteration 7, loss = 0.48867647\n",
      "Iteration 418, loss = 0.00625914\n",
      "Iteration 8, loss = 0.46108164\n",
      "Iteration 419, loss = 0.00676085\n",
      "Iteration 9, loss = 0.43947606\n",
      "Iteration 420, loss = 0.00659403\n",
      "Iteration 10, loss = 0.41842951\n",
      "Iteration 421, loss = 0.00592689\n",
      "Iteration 11, loss = 0.40318556\n",
      "Iteration 422, loss = 0.00619748\n",
      "Iteration 12, loss = 0.38913640\n",
      "Iteration 423, loss = 0.00620709\n",
      "Iteration 13, loss = 0.37708641\n",
      "Iteration 424, loss = 0.00597308\n",
      "Iteration 14, loss = 0.36696055\n",
      "Iteration 425, loss = 0.00578935\n",
      "Iteration 15, loss = 0.35747561\n",
      "Iteration 426, loss = 0.00676011\n",
      "Iteration 16, loss = 0.34786754\n",
      "Iteration 427, loss = 0.00678422\n",
      "Iteration 17, loss = 0.33998199\n",
      "Iteration 428, loss = 0.00820860\n",
      "Iteration 18, loss = 0.33093993\n",
      "Iteration 429, loss = 0.00744050\n",
      "Iteration 19, loss = 0.32385504\n",
      "Iteration 430, loss = 0.00712570\n",
      "Iteration 20, loss = 0.31779938\n",
      "Iteration 431, loss = 0.00607750\n",
      "Iteration 21, loss = 0.31105805\n",
      "Iteration 432, loss = 0.00636292\n",
      "Iteration 22, loss = 0.30471249\n",
      "Iteration 433, loss = 0.00580347\n",
      "Iteration 23, loss = 0.29890404\n",
      "Iteration 434, loss = 0.00576513\n",
      "Iteration 24, loss = 0.29319980\n",
      "Iteration 435, loss = 0.00546249\n",
      "Iteration 25, loss = 0.28847874\n",
      "Iteration 436, loss = 0.00552084\n",
      "Iteration 26, loss = 0.28400707\n",
      "Iteration 437, loss = 0.00566649\n",
      "Iteration 27, loss = 0.27757759\n",
      "Iteration 438, loss = 0.00578161\n",
      "Iteration 28, loss = 0.27296093\n",
      "Iteration 439, loss = 0.00540692\n",
      "Iteration 29, loss = 0.27005326\n",
      "Iteration 440, loss = 0.00564656\n",
      "Iteration 30, loss = 0.26645663\n",
      "Iteration 441, loss = 0.00587528\n",
      "Iteration 31, loss = 0.26088754\n",
      "Iteration 442, loss = 0.00530739\n",
      "Iteration 32, loss = 0.25612036\n",
      "Iteration 443, loss = 0.00529024\n",
      "Iteration 33, loss = 0.25277271\n",
      "Iteration 444, loss = 0.00506031\n",
      "Iteration 34, loss = 0.24733778\n",
      "Iteration 35, loss = 0.24858478\n",
      "Iteration 445, loss = 0.00518164\n",
      "Iteration 36, loss = 0.24253119\n",
      "Iteration 446, loss = 0.00548502\n",
      "Iteration 37, loss = 0.23821050\n",
      "Iteration 447, loss = 0.00500617\n",
      "Iteration 38, loss = 0.23441845\n",
      "Iteration 448, loss = 0.00522678\n",
      "Iteration 39, loss = 0.23052342\n",
      "Iteration 449, loss = 0.00486860\n",
      "Iteration 40, loss = 0.22783603\n",
      "Iteration 450, loss = 0.00511134\n",
      "Iteration 41, loss = 0.22322881\n",
      "Iteration 451, loss = 0.00505446\n",
      "Iteration 42, loss = 0.22177993\n",
      "Iteration 452, loss = 0.00524451\n",
      "Iteration 43, loss = 0.21747084\n",
      "Iteration 453, loss = 0.00506102\n",
      "Iteration 44, loss = 0.21771666\n",
      "Iteration 454, loss = 0.00462983\n",
      "Iteration 45, loss = 0.21194048\n",
      "Iteration 455, loss = 0.00513978\n",
      "Iteration 46, loss = 0.20888074\n",
      "Iteration 456, loss = 0.00484559\n",
      "Iteration 47, loss = 0.20721348\n",
      "Iteration 457, loss = 0.00458523\n",
      "Iteration 48, loss = 0.20470846\n",
      "Iteration 458, loss = 0.00457975\n",
      "Iteration 49, loss = 0.19977585\n",
      "Iteration 459, loss = 0.00452675\n",
      "Iteration 50, loss = 0.19724530\n",
      "Iteration 460, loss = 0.00455521\n",
      "Iteration 51, loss = 0.19530585\n",
      "Iteration 461, loss = 0.00466209\n",
      "Iteration 52, loss = 0.19331238\n",
      "Iteration 462, loss = 0.00454775\n",
      "Iteration 53, loss = 0.19256234\n",
      "Iteration 463, loss = 0.00442323\n",
      "Iteration 54, loss = 0.18814587\n",
      "Iteration 464, loss = 0.00452205\n",
      "Iteration 55, loss = 0.18465576\n",
      "Iteration 465, loss = 0.00492523\n",
      "Iteration 56, loss = 0.18341573\n",
      "Iteration 466, loss = 0.00457748\n",
      "Iteration 57, loss = 0.18017066\n",
      "Iteration 467, loss = 0.00467975\n",
      "Iteration 58, loss = 0.17833504\n",
      "Iteration 468, loss = 0.00442422\n",
      "Iteration 59, loss = 0.17558917\n",
      "Iteration 469, loss = 0.00452739\n",
      "Iteration 60, loss = 0.17298826\n",
      "Iteration 470, loss = 0.00451673\n",
      "Iteration 61, loss = 0.17302623\n",
      "Iteration 471, loss = 0.00503419\n",
      "Iteration 62, loss = 0.16814782\n",
      "Iteration 472, loss = 0.00440716\n",
      "Iteration 63, loss = 0.16716109\n",
      "Iteration 473, loss = 0.00418832\n",
      "Iteration 64, loss = 0.16339203\n",
      "Iteration 474, loss = 0.00414562\n",
      "Iteration 65, loss = 0.16115274\n",
      "Iteration 475, loss = 0.00423928\n",
      "Iteration 66, loss = 0.15958371\n",
      "Iteration 476, loss = 0.00427623\n",
      "Iteration 67, loss = 0.15801398\n",
      "Iteration 477, loss = 0.00413381\n",
      "Iteration 68, loss = 0.15586861\n",
      "Iteration 478, loss = 0.00406903\n",
      "Iteration 69, loss = 0.15448472\n",
      "Iteration 479, loss = 0.00393536\n",
      "Iteration 70, loss = 0.15146133\n",
      "Iteration 480, loss = 0.00395526\n",
      "Iteration 71, loss = 0.15056309\n",
      "Iteration 481, loss = 0.00390784\n",
      "Iteration 72, loss = 0.15197826\n",
      "Iteration 482, loss = 0.00384828\n",
      "Iteration 73, loss = 0.14777824\n",
      "Iteration 483, loss = 0.00393103\n",
      "Iteration 74, loss = 0.14681470\n",
      "Iteration 484, loss = 0.00386721\n",
      "Iteration 75, loss = 0.14464920\n",
      "Iteration 485, loss = 0.00378063\n",
      "Iteration 76, loss = 0.14154732\n",
      "Iteration 486, loss = 0.00377570\n",
      "Iteration 77, loss = 0.14004095\n",
      "Iteration 487, loss = 0.00389277\n",
      "Iteration 78, loss = 0.13854142\n",
      "Iteration 488, loss = 0.00389467\n",
      "Iteration 79, loss = 0.13773874\n",
      "Iteration 489, loss = 0.00369771\n",
      "Iteration 80, loss = 0.13527863\n",
      "Iteration 490, loss = 0.00377273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.13618458\n",
      "Iteration 82, loss = 0.13901588\n",
      "Iteration 83, loss = 0.13302640\n",
      "Iteration 84, loss = 0.12975468\n",
      "Iteration 85, loss = 0.13047542\n",
      "Iteration 86, loss = 0.12616731\n",
      "Iteration 87, loss = 0.12657343\n",
      "Iteration 88, loss = 0.12321302\n",
      "Iteration 89, loss = 0.12210020\n",
      "Iteration 90, loss = 0.12073865\n",
      "Iteration 91, loss = 0.12214155\n",
      "Iteration 92, loss = 0.12037787\n",
      "Iteration 93, loss = 0.11817932\n",
      "Iteration 94, loss = 0.11557389\n",
      "Iteration 1, loss = 1.10978645\n",
      "Iteration 95, loss = 0.11500924\n",
      "Iteration 2, loss = 0.87077401\n",
      "Iteration 96, loss = 0.11306301\n",
      "Iteration 3, loss = 0.71321207\n",
      "Iteration 97, loss = 0.11315665\n",
      "Iteration 4, loss = 0.61722657\n",
      "Iteration 98, loss = 0.11247678\n",
      "Iteration 5, loss = 0.55475440\n",
      "Iteration 99, loss = 0.11045512\n",
      "Iteration 6, loss = 0.50678445\n",
      "Iteration 100, loss = 0.10853309\n",
      "Iteration 7, loss = 0.47525242\n",
      "Iteration 101, loss = 0.10836917\n",
      "Iteration 8, loss = 0.45082038\n",
      "Iteration 102, loss = 0.10675683\n",
      "Iteration 9, loss = 0.43006701\n",
      "Iteration 103, loss = 0.10700148\n",
      "Iteration 10, loss = 0.41232684\n",
      "Iteration 104, loss = 0.10511062\n",
      "Iteration 11, loss = 0.39784875\n",
      "Iteration 105, loss = 0.10290176\n",
      "Iteration 12, loss = 0.38584461\n",
      "Iteration 106, loss = 0.10293692\n",
      "Iteration 13, loss = 0.37343502\n",
      "Iteration 107, loss = 0.10350769\n",
      "Iteration 14, loss = 0.36350284\n",
      "Iteration 108, loss = 0.10354843\n",
      "Iteration 15, loss = 0.35468196\n",
      "Iteration 109, loss = 0.10479931\n",
      "Iteration 16, loss = 0.34423372\n",
      "Iteration 110, loss = 0.10520374\n",
      "Iteration 17, loss = 0.33748205\n",
      "Iteration 111, loss = 0.10233332\n",
      "Iteration 18, loss = 0.32920230\n",
      "Iteration 19, loss = 0.32253800\n",
      "Iteration 112, loss = 0.09670149\n",
      "Iteration 20, loss = 0.31420907\n",
      "Iteration 113, loss = 0.09896173\n",
      "Iteration 21, loss = 0.30761896\n",
      "Iteration 114, loss = 0.09774503\n",
      "Iteration 22, loss = 0.30234486\n",
      "Iteration 115, loss = 0.09395683\n",
      "Iteration 23, loss = 0.29724303\n",
      "Iteration 116, loss = 0.09336855\n",
      "Iteration 24, loss = 0.28954418\n",
      "Iteration 117, loss = 0.09360791\n",
      "Iteration 25, loss = 0.28464484\n",
      "Iteration 118, loss = 0.09080588\n",
      "Iteration 26, loss = 0.27833709\n",
      "Iteration 119, loss = 0.09291826\n",
      "Iteration 27, loss = 0.27349139\n",
      "Iteration 120, loss = 0.09003114\n",
      "Iteration 28, loss = 0.26890845\n",
      "Iteration 121, loss = 0.09069437\n",
      "Iteration 122, loss = 0.08741911\n",
      "Iteration 29, loss = 0.26468821\n",
      "Iteration 123, loss = 0.08700344\n",
      "Iteration 30, loss = 0.26072707\n",
      "Iteration 124, loss = 0.08580648\n",
      "Iteration 31, loss = 0.25575415\n",
      "Iteration 125, loss = 0.08621131\n",
      "Iteration 32, loss = 0.25197120\n",
      "Iteration 33, loss = 0.24756052\n",
      "Iteration 126, loss = 0.08522455\n",
      "Iteration 34, loss = 0.24317251\n",
      "Iteration 127, loss = 0.08336197\n",
      "Iteration 128, loss = 0.08317188\n",
      "Iteration 35, loss = 0.23946629\n",
      "Iteration 36, loss = 0.23602806\n",
      "Iteration 129, loss = 0.08231117\n",
      "Iteration 37, loss = 0.23193570\n",
      "Iteration 130, loss = 0.08137862\n",
      "Iteration 38, loss = 0.22836129\n",
      "Iteration 131, loss = 0.08027434\n",
      "Iteration 39, loss = 0.22671741\n",
      "Iteration 132, loss = 0.07991723\n",
      "Iteration 40, loss = 0.22152600\n",
      "Iteration 133, loss = 0.07924020\n",
      "Iteration 41, loss = 0.21869099\n",
      "Iteration 134, loss = 0.08237524\n",
      "Iteration 42, loss = 0.21610306\n",
      "Iteration 135, loss = 0.08163254\n",
      "Iteration 43, loss = 0.21129210\n",
      "Iteration 136, loss = 0.07754336\n",
      "Iteration 44, loss = 0.21142969\n",
      "Iteration 137, loss = 0.07846907\n",
      "Iteration 45, loss = 0.20618532\n",
      "Iteration 138, loss = 0.07834803\n",
      "Iteration 46, loss = 0.20363569\n",
      "Iteration 139, loss = 0.07559879\n",
      "Iteration 47, loss = 0.20094473\n",
      "Iteration 140, loss = 0.07593196\n",
      "Iteration 48, loss = 0.20004963\n",
      "Iteration 141, loss = 0.07307249\n",
      "Iteration 49, loss = 0.19339588\n",
      "Iteration 142, loss = 0.07440141\n",
      "Iteration 50, loss = 0.19379374\n",
      "Iteration 143, loss = 0.07646828\n",
      "Iteration 51, loss = 0.18836583\n",
      "Iteration 144, loss = 0.07316659\n",
      "Iteration 52, loss = 0.18516210\n",
      "Iteration 145, loss = 0.07300409\n",
      "Iteration 53, loss = 0.18299396\n",
      "Iteration 146, loss = 0.07045300\n",
      "Iteration 54, loss = 0.18108188\n",
      "Iteration 147, loss = 0.06983431\n",
      "Iteration 55, loss = 0.17987349\n",
      "Iteration 148, loss = 0.07074439\n",
      "Iteration 56, loss = 0.17585131\n",
      "Iteration 149, loss = 0.07205703\n",
      "Iteration 57, loss = 0.17468581\n",
      "Iteration 150, loss = 0.06937852\n",
      "Iteration 58, loss = 0.17141301\n",
      "Iteration 151, loss = 0.06980608\n",
      "Iteration 59, loss = 0.16939836\n",
      "Iteration 152, loss = 0.06742889\n",
      "Iteration 60, loss = 0.16644444\n",
      "Iteration 153, loss = 0.06619920\n",
      "Iteration 61, loss = 0.16285512\n",
      "Iteration 154, loss = 0.06556381\n",
      "Iteration 62, loss = 0.16041489\n",
      "Iteration 155, loss = 0.06455848\n",
      "Iteration 63, loss = 0.15892202\n",
      "Iteration 156, loss = 0.06520820\n",
      "Iteration 64, loss = 0.15555810\n",
      "Iteration 157, loss = 0.06578071\n",
      "Iteration 65, loss = 0.15957165\n",
      "Iteration 158, loss = 0.06203280\n",
      "Iteration 66, loss = 0.15440372\n",
      "Iteration 159, loss = 0.06218158\n",
      "Iteration 67, loss = 0.15609970\n",
      "Iteration 160, loss = 0.06183603\n",
      "Iteration 68, loss = 0.15626291\n",
      "Iteration 161, loss = 0.06120387\n",
      "Iteration 69, loss = 0.14507197\n",
      "Iteration 162, loss = 0.06086629\n",
      "Iteration 70, loss = 0.14931941\n",
      "Iteration 163, loss = 0.05944510\n",
      "Iteration 71, loss = 0.14388847\n",
      "Iteration 164, loss = 0.06018082\n",
      "Iteration 72, loss = 0.14123904\n",
      "Iteration 165, loss = 0.06004971\n",
      "Iteration 73, loss = 0.14036580\n",
      "Iteration 166, loss = 0.05966473\n",
      "Iteration 74, loss = 0.13768957\n",
      "Iteration 167, loss = 0.05758923\n",
      "Iteration 75, loss = 0.13535644\n",
      "Iteration 168, loss = 0.05710050\n",
      "Iteration 76, loss = 0.13417165\n",
      "Iteration 169, loss = 0.05891578\n",
      "Iteration 77, loss = 0.13163324\n",
      "Iteration 170, loss = 0.06117470\n",
      "Iteration 78, loss = 0.13064593\n",
      "Iteration 171, loss = 0.05880508\n",
      "Iteration 79, loss = 0.12803492\n",
      "Iteration 172, loss = 0.05497278\n",
      "Iteration 80, loss = 0.12667101\n",
      "Iteration 173, loss = 0.05955096\n",
      "Iteration 81, loss = 0.12846881\n",
      "Iteration 174, loss = 0.05795444\n",
      "Iteration 82, loss = 0.12656142\n",
      "Iteration 175, loss = 0.05463189\n",
      "Iteration 83, loss = 0.12554135\n",
      "Iteration 176, loss = 0.05328038\n",
      "Iteration 84, loss = 0.12404432\n",
      "Iteration 177, loss = 0.05302270\n",
      "Iteration 85, loss = 0.12704312\n",
      "Iteration 178, loss = 0.05271161\n",
      "Iteration 86, loss = 0.12664554\n",
      "Iteration 179, loss = 0.05098223\n",
      "Iteration 87, loss = 0.12288137\n",
      "Iteration 180, loss = 0.05276200\n",
      "Iteration 88, loss = 0.11647041\n",
      "Iteration 181, loss = 0.05628692\n",
      "Iteration 89, loss = 0.12370327\n",
      "Iteration 182, loss = 0.05641466\n",
      "Iteration 90, loss = 0.12109714\n",
      "Iteration 183, loss = 0.05365128\n",
      "Iteration 91, loss = 0.11272038\n",
      "Iteration 184, loss = 0.05263533\n",
      "Iteration 92, loss = 0.11070815\n",
      "Iteration 185, loss = 0.05093952\n",
      "Iteration 93, loss = 0.11303405\n",
      "Iteration 186, loss = 0.05000288\n",
      "Iteration 94, loss = 0.11356518\n",
      "Iteration 187, loss = 0.04943920\n",
      "Iteration 95, loss = 0.10950056\n",
      "Iteration 188, loss = 0.05033385\n",
      "Iteration 96, loss = 0.10788148\n",
      "Iteration 189, loss = 0.04791736\n",
      "Iteration 97, loss = 0.10782704\n",
      "Iteration 190, loss = 0.04804904\n",
      "Iteration 98, loss = 0.10428437\n",
      "Iteration 191, loss = 0.04669518\n",
      "Iteration 99, loss = 0.10321989\n",
      "Iteration 192, loss = 0.04579795\n",
      "Iteration 100, loss = 0.10343528\n",
      "Iteration 193, loss = 0.04648324\n",
      "Iteration 101, loss = 0.10102004\n",
      "Iteration 194, loss = 0.04787356\n",
      "Iteration 102, loss = 0.10214843\n",
      "Iteration 195, loss = 0.04591114\n",
      "Iteration 103, loss = 0.09756749\n",
      "Iteration 196, loss = 0.04627892\n",
      "Iteration 104, loss = 0.09855201\n",
      "Iteration 197, loss = 0.04490167\n",
      "Iteration 105, loss = 0.09689598\n",
      "Iteration 198, loss = 0.04423722\n",
      "Iteration 106, loss = 0.09650113\n",
      "Iteration 199, loss = 0.04448983\n",
      "Iteration 107, loss = 0.09447755\n",
      "Iteration 200, loss = 0.04541890\n",
      "Iteration 108, loss = 0.09471431\n",
      "Iteration 201, loss = 0.04475711\n",
      "Iteration 109, loss = 0.09199361\n",
      "Iteration 202, loss = 0.04334407\n",
      "Iteration 110, loss = 0.09379089\n",
      "Iteration 203, loss = 0.04370851\n",
      "Iteration 111, loss = 0.09393550\n",
      "Iteration 204, loss = 0.04383348\n",
      "Iteration 112, loss = 0.09297686\n",
      "Iteration 205, loss = 0.04178420\n",
      "Iteration 113, loss = 0.08792655\n",
      "Iteration 206, loss = 0.04083439\n",
      "Iteration 114, loss = 0.09006953\n",
      "Iteration 207, loss = 0.04166949\n",
      "Iteration 115, loss = 0.09047618\n",
      "Iteration 208, loss = 0.04074140\n",
      "Iteration 116, loss = 0.08770135\n",
      "Iteration 209, loss = 0.04178305\n",
      "Iteration 117, loss = 0.08910377\n",
      "Iteration 210, loss = 0.04281648\n",
      "Iteration 118, loss = 0.08552924\n",
      "Iteration 211, loss = 0.04099376\n",
      "Iteration 119, loss = 0.08352794\n",
      "Iteration 212, loss = 0.03820400\n",
      "Iteration 120, loss = 0.08747777\n",
      "Iteration 213, loss = 0.04048713\n",
      "Iteration 121, loss = 0.08269853\n",
      "Iteration 214, loss = 0.03821686\n",
      "Iteration 122, loss = 0.08234390\n",
      "Iteration 215, loss = 0.04082529\n",
      "Iteration 123, loss = 0.08210428\n",
      "Iteration 216, loss = 0.03937017\n",
      "Iteration 124, loss = 0.08157389\n",
      "Iteration 217, loss = 0.03965398\n",
      "Iteration 125, loss = 0.07780982\n",
      "Iteration 218, loss = 0.04143071\n",
      "Iteration 126, loss = 0.07804559\n",
      "Iteration 219, loss = 0.03867009\n",
      "Iteration 127, loss = 0.07720193\n",
      "Iteration 220, loss = 0.03972280\n",
      "Iteration 128, loss = 0.07673192\n",
      "Iteration 221, loss = 0.03654221\n",
      "Iteration 129, loss = 0.07625326\n",
      "Iteration 222, loss = 0.03691356\n",
      "Iteration 130, loss = 0.07447382\n",
      "Iteration 223, loss = 0.03746413\n",
      "Iteration 131, loss = 0.07544121\n",
      "Iteration 224, loss = 0.03599184\n",
      "Iteration 132, loss = 0.07516406\n",
      "Iteration 225, loss = 0.03748478\n",
      "Iteration 133, loss = 0.07533431\n",
      "Iteration 226, loss = 0.03497654\n",
      "Iteration 134, loss = 0.07124680\n",
      "Iteration 227, loss = 0.03488536\n",
      "Iteration 135, loss = 0.07026411\n",
      "Iteration 228, loss = 0.03614457\n",
      "Iteration 136, loss = 0.07108051\n",
      "Iteration 229, loss = 0.03482069\n",
      "Iteration 137, loss = 0.06941360\n",
      "Iteration 230, loss = 0.03352568\n",
      "Iteration 138, loss = 0.07095142\n",
      "Iteration 231, loss = 0.03262785\n",
      "Iteration 139, loss = 0.06898205\n",
      "Iteration 232, loss = 0.03287254\n",
      "Iteration 140, loss = 0.06762044\n",
      "Iteration 233, loss = 0.03277249\n",
      "Iteration 141, loss = 0.06842444\n",
      "Iteration 234, loss = 0.03196425\n",
      "Iteration 142, loss = 0.06622305\n",
      "Iteration 235, loss = 0.03194585\n",
      "Iteration 143, loss = 0.06723441\n",
      "Iteration 236, loss = 0.03186271\n",
      "Iteration 144, loss = 0.06534929\n",
      "Iteration 237, loss = 0.03160868\n",
      "Iteration 145, loss = 0.06558436\n",
      "Iteration 238, loss = 0.03143127\n",
      "Iteration 146, loss = 0.06761629\n",
      "Iteration 239, loss = 0.03285191\n",
      "Iteration 147, loss = 0.06352399\n",
      "Iteration 240, loss = 0.03145426\n",
      "Iteration 148, loss = 0.06536226\n",
      "Iteration 241, loss = 0.03111207\n",
      "Iteration 149, loss = 0.06391887\n",
      "Iteration 242, loss = 0.03092833\n",
      "Iteration 150, loss = 0.06625889\n",
      "Iteration 243, loss = 0.03084045\n",
      "Iteration 151, loss = 0.06302468\n",
      "Iteration 244, loss = 0.02994205\n",
      "Iteration 152, loss = 0.06014405\n",
      "Iteration 245, loss = 0.02983285\n",
      "Iteration 153, loss = 0.05924161\n",
      "Iteration 246, loss = 0.02963037\n",
      "Iteration 154, loss = 0.05972173\n",
      "Iteration 247, loss = 0.03033050\n",
      "Iteration 155, loss = 0.06235350\n",
      "Iteration 248, loss = 0.02934878\n",
      "Iteration 156, loss = 0.06065644\n",
      "Iteration 249, loss = 0.02856450\n",
      "Iteration 157, loss = 0.05942163\n",
      "Iteration 250, loss = 0.02935104\n",
      "Iteration 158, loss = 0.05809782\n",
      "Iteration 251, loss = 0.02809274\n",
      "Iteration 159, loss = 0.05580579\n",
      "Iteration 252, loss = 0.02761197\n",
      "Iteration 160, loss = 0.05722612\n",
      "Iteration 253, loss = 0.02884910\n",
      "Iteration 161, loss = 0.05462005\n",
      "Iteration 254, loss = 0.02737870\n",
      "Iteration 162, loss = 0.05643237\n",
      "Iteration 255, loss = 0.02722560\n",
      "Iteration 163, loss = 0.05470330\n",
      "Iteration 256, loss = 0.02735240\n",
      "Iteration 164, loss = 0.05303531\n",
      "Iteration 257, loss = 0.02736028\n",
      "Iteration 165, loss = 0.05245682\n",
      "Iteration 258, loss = 0.02706856\n",
      "Iteration 166, loss = 0.05306073\n",
      "Iteration 259, loss = 0.02773908\n",
      "Iteration 167, loss = 0.05322766\n",
      "Iteration 260, loss = 0.02846014\n",
      "Iteration 168, loss = 0.05100538\n",
      "Iteration 261, loss = 0.02936137\n",
      "Iteration 169, loss = 0.05100763\n",
      "Iteration 262, loss = 0.02714494\n",
      "Iteration 170, loss = 0.04985651\n",
      "Iteration 263, loss = 0.02621459\n",
      "Iteration 171, loss = 0.05076429\n",
      "Iteration 264, loss = 0.02582093\n",
      "Iteration 172, loss = 0.05178080\n",
      "Iteration 265, loss = 0.02476641\n",
      "Iteration 173, loss = 0.05597980\n",
      "Iteration 266, loss = 0.02513570\n",
      "Iteration 174, loss = 0.05391823\n",
      "Iteration 267, loss = 0.02650917\n",
      "Iteration 175, loss = 0.05163360\n",
      "Iteration 268, loss = 0.02635873\n",
      "Iteration 176, loss = 0.04669580\n",
      "Iteration 269, loss = 0.02419306\n",
      "Iteration 177, loss = 0.04679821\n",
      "Iteration 270, loss = 0.02602915\n",
      "Iteration 178, loss = 0.04668557\n",
      "Iteration 271, loss = 0.02444440\n",
      "Iteration 179, loss = 0.04545493\n",
      "Iteration 272, loss = 0.02329766\n",
      "Iteration 180, loss = 0.04546687\n",
      "Iteration 273, loss = 0.02446986\n",
      "Iteration 181, loss = 0.04621691\n",
      "Iteration 274, loss = 0.02345567\n",
      "Iteration 182, loss = 0.04487124\n",
      "Iteration 275, loss = 0.02420874\n",
      "Iteration 183, loss = 0.04719456\n",
      "Iteration 276, loss = 0.02343004\n",
      "Iteration 184, loss = 0.04310831\n",
      "Iteration 277, loss = 0.02302089\n",
      "Iteration 185, loss = 0.04356763\n",
      "Iteration 278, loss = 0.02402316\n",
      "Iteration 186, loss = 0.04302180\n",
      "Iteration 279, loss = 0.02386779\n",
      "Iteration 187, loss = 0.04365242\n",
      "Iteration 280, loss = 0.02322057\n",
      "Iteration 188, loss = 0.04737102\n",
      "Iteration 281, loss = 0.02223560\n",
      "Iteration 189, loss = 0.05081898\n",
      "Iteration 282, loss = 0.02384318\n",
      "Iteration 190, loss = 0.04516993\n",
      "Iteration 283, loss = 0.02387024\n",
      "Iteration 191, loss = 0.04051175\n",
      "Iteration 284, loss = 0.02178928\n",
      "Iteration 192, loss = 0.04060496\n",
      "Iteration 285, loss = 0.02185285\n",
      "Iteration 193, loss = 0.04022255\n",
      "Iteration 286, loss = 0.02246267\n",
      "Iteration 194, loss = 0.04087024\n",
      "Iteration 287, loss = 0.02122790\n",
      "Iteration 195, loss = 0.04031469\n",
      "Iteration 288, loss = 0.02058509\n",
      "Iteration 196, loss = 0.04435307\n",
      "Iteration 289, loss = 0.02099215\n",
      "Iteration 197, loss = 0.04546599\n",
      "Iteration 290, loss = 0.02112469\n",
      "Iteration 198, loss = 0.03989864\n",
      "Iteration 291, loss = 0.02377325\n",
      "Iteration 199, loss = 0.04081593\n",
      "Iteration 292, loss = 0.02023972\n",
      "Iteration 200, loss = 0.04164341\n",
      "Iteration 293, loss = 0.02042878\n",
      "Iteration 294, loss = 0.02053439\n",
      "Iteration 201, loss = 0.03728947\n",
      "Iteration 295, loss = 0.01989499\n",
      "Iteration 202, loss = 0.03932639\n",
      "Iteration 296, loss = 0.02090528\n",
      "Iteration 203, loss = 0.03919188\n",
      "Iteration 297, loss = 0.01992100\n",
      "Iteration 204, loss = 0.03538374\n",
      "Iteration 298, loss = 0.01943653\n",
      "Iteration 205, loss = 0.03665483\n",
      "Iteration 299, loss = 0.01889305\n",
      "Iteration 206, loss = 0.03641718\n",
      "Iteration 300, loss = 0.01917486\n",
      "Iteration 207, loss = 0.03669764\n",
      "Iteration 301, loss = 0.01886631\n",
      "Iteration 208, loss = 0.03655453\n",
      "Iteration 302, loss = 0.01967771\n",
      "Iteration 209, loss = 0.03624404\n",
      "Iteration 303, loss = 0.02049560\n",
      "Iteration 210, loss = 0.03466605\n",
      "Iteration 304, loss = 0.02243196\n",
      "Iteration 211, loss = 0.03468112\n",
      "Iteration 305, loss = 0.02432368\n",
      "Iteration 212, loss = 0.03422350\n",
      "Iteration 306, loss = 0.02015716\n",
      "Iteration 213, loss = 0.03226017\n",
      "Iteration 307, loss = 0.02249298\n",
      "Iteration 214, loss = 0.03257431\n",
      "Iteration 308, loss = 0.02198387\n",
      "Iteration 215, loss = 0.03379848\n",
      "Iteration 309, loss = 0.02187505\n",
      "Iteration 216, loss = 0.03217940\n",
      "Iteration 310, loss = 0.02576610\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 217, loss = 0.03178374\n",
      "Iteration 218, loss = 0.03175584\n",
      "Iteration 219, loss = 0.03112587\n",
      "Iteration 220, loss = 0.03099400\n",
      "Iteration 221, loss = 0.03037604\n",
      "Iteration 222, loss = 0.03041296\n",
      "Iteration 223, loss = 0.02913830\n",
      "Iteration 224, loss = 0.02953016\n",
      "Iteration 225, loss = 0.03131332\n",
      "Iteration 226, loss = 0.02896266\n",
      "Iteration 227, loss = 0.02886796\n",
      "Iteration 228, loss = 0.02946205\n",
      "Iteration 229, loss = 0.02819922\n",
      "Iteration 230, loss = 0.02818564\n",
      "Iteration 1, loss = 1.24086878\n",
      "Iteration 231, loss = 0.02773160\n",
      "Iteration 2, loss = 0.95032426\n",
      "Iteration 232, loss = 0.02836669\n",
      "Iteration 3, loss = 0.76171334\n",
      "Iteration 233, loss = 0.02864268\n",
      "Iteration 4, loss = 0.64825612\n",
      "Iteration 234, loss = 0.02733533\n",
      "Iteration 5, loss = 0.57125487\n",
      "Iteration 6, loss = 0.51614280\n",
      "Iteration 235, loss = 0.02769021\n",
      "Iteration 236, loss = 0.02538771\n",
      "Iteration 7, loss = 0.48300161\n",
      "Iteration 237, loss = 0.02807120\n",
      "Iteration 8, loss = 0.45201936\n",
      "Iteration 238, loss = 0.02834614\n",
      "Iteration 9, loss = 0.42808477\n",
      "Iteration 239, loss = 0.02872360\n",
      "Iteration 10, loss = 0.40917714\n",
      "Iteration 240, loss = 0.02672528\n",
      "Iteration 11, loss = 0.39273598\n",
      "Iteration 241, loss = 0.02560085\n",
      "Iteration 12, loss = 0.37889949\n",
      "Iteration 242, loss = 0.02517656\n",
      "Iteration 13, loss = 0.36820360\n",
      "Iteration 243, loss = 0.02707708\n",
      "Iteration 14, loss = 0.35726507\n",
      "Iteration 244, loss = 0.02663577\n",
      "Iteration 15, loss = 0.34802773\n",
      "Iteration 245, loss = 0.02599074\n",
      "Iteration 16, loss = 0.33870395\n",
      "Iteration 246, loss = 0.02579341\n",
      "Iteration 17, loss = 0.33041456\n",
      "Iteration 247, loss = 0.02448679\n",
      "Iteration 18, loss = 0.32493651\n",
      "Iteration 248, loss = 0.02463718\n",
      "Iteration 19, loss = 0.31589921\n",
      "Iteration 249, loss = 0.02314324\n",
      "Iteration 20, loss = 0.31002258\n",
      "Iteration 250, loss = 0.02822702\n",
      "Iteration 21, loss = 0.30281289\n",
      "Iteration 251, loss = 0.02689698\n",
      "Iteration 22, loss = 0.29737127\n",
      "Iteration 252, loss = 0.03065090\n",
      "Iteration 23, loss = 0.29439239\n",
      "Iteration 253, loss = 0.02570683\n",
      "Iteration 24, loss = 0.28945844\n",
      "Iteration 254, loss = 0.02284716\n",
      "Iteration 25, loss = 0.28302799\n",
      "Iteration 255, loss = 0.02155447\n",
      "Iteration 26, loss = 0.27842304\n",
      "Iteration 256, loss = 0.02191104\n",
      "Iteration 27, loss = 0.27341993\n",
      "Iteration 257, loss = 0.02146104\n",
      "Iteration 28, loss = 0.26882737\n",
      "Iteration 258, loss = 0.02139252\n",
      "Iteration 29, loss = 0.26544281\n",
      "Iteration 259, loss = 0.02175286\n",
      "Iteration 30, loss = 0.25929111\n",
      "Iteration 260, loss = 0.02113433\n",
      "Iteration 31, loss = 0.25764869\n",
      "Iteration 261, loss = 0.02190093\n",
      "Iteration 32, loss = 0.25393932\n",
      "Iteration 262, loss = 0.02233918\n",
      "Iteration 33, loss = 0.25050596\n",
      "Iteration 263, loss = 0.01975547\n",
      "Iteration 34, loss = 0.24649676\n",
      "Iteration 264, loss = 0.02032469\n",
      "Iteration 35, loss = 0.24398237\n",
      "Iteration 265, loss = 0.01965070\n",
      "Iteration 36, loss = 0.24030728\n",
      "Iteration 266, loss = 0.02002911\n",
      "Iteration 37, loss = 0.23665708\n",
      "Iteration 267, loss = 0.02057936\n",
      "Iteration 38, loss = 0.23336479\n",
      "Iteration 268, loss = 0.01964729\n",
      "Iteration 39, loss = 0.23101894\n",
      "Iteration 269, loss = 0.01872757\n",
      "Iteration 40, loss = 0.22737461\n",
      "Iteration 270, loss = 0.01962185\n",
      "Iteration 41, loss = 0.22443901\n",
      "Iteration 271, loss = 0.02085156\n",
      "Iteration 42, loss = 0.22258979\n",
      "Iteration 272, loss = 0.01948887\n",
      "Iteration 43, loss = 0.22011351\n",
      "Iteration 273, loss = 0.01839958\n",
      "Iteration 44, loss = 0.21623845\n",
      "Iteration 274, loss = 0.01822846\n",
      "Iteration 45, loss = 0.21597255\n",
      "Iteration 275, loss = 0.01788459\n",
      "Iteration 46, loss = 0.21141093\n",
      "Iteration 276, loss = 0.01785701\n",
      "Iteration 47, loss = 0.21010991\n",
      "Iteration 277, loss = 0.01798801\n",
      "Iteration 48, loss = 0.20884221\n",
      "Iteration 278, loss = 0.01731288\n",
      "Iteration 49, loss = 0.20620716\n",
      "Iteration 279, loss = 0.01736603\n",
      "Iteration 50, loss = 0.20398379\n",
      "Iteration 280, loss = 0.01864095\n",
      "Iteration 51, loss = 0.19815179\n",
      "Iteration 281, loss = 0.01820086\n",
      "Iteration 52, loss = 0.19814679\n",
      "Iteration 282, loss = 0.01787168\n",
      "Iteration 53, loss = 0.19359131\n",
      "Iteration 283, loss = 0.01709053\n",
      "Iteration 54, loss = 0.19355296\n",
      "Iteration 284, loss = 0.01723231\n",
      "Iteration 55, loss = 0.19161961\n",
      "Iteration 285, loss = 0.01729862\n",
      "Iteration 56, loss = 0.18858125\n",
      "Iteration 286, loss = 0.01731356\n",
      "Iteration 57, loss = 0.18622742\n",
      "Iteration 287, loss = 0.01627967\n",
      "Iteration 58, loss = 0.18474926\n",
      "Iteration 288, loss = 0.01643698\n",
      "Iteration 59, loss = 0.18231007\n",
      "Iteration 289, loss = 0.01609018\n",
      "Iteration 290, loss = 0.01535407\n",
      "Iteration 60, loss = 0.17964737\n",
      "Iteration 291, loss = 0.01560044\n",
      "Iteration 61, loss = 0.17762206\n",
      "Iteration 292, loss = 0.01575613\n",
      "Iteration 62, loss = 0.17517342\n",
      "Iteration 293, loss = 0.01532086\n",
      "Iteration 63, loss = 0.17230855\n",
      "Iteration 294, loss = 0.01539459\n",
      "Iteration 64, loss = 0.17217808\n",
      "Iteration 295, loss = 0.01521810\n",
      "Iteration 65, loss = 0.16848130\n",
      "Iteration 296, loss = 0.01554303\n",
      "Iteration 66, loss = 0.16880559\n",
      "Iteration 297, loss = 0.01516321\n",
      "Iteration 67, loss = 0.16898162\n",
      "Iteration 298, loss = 0.01507786\n",
      "Iteration 68, loss = 0.16793377\n",
      "Iteration 299, loss = 0.01430584\n",
      "Iteration 69, loss = 0.16390016\n",
      "Iteration 300, loss = 0.01454844\n",
      "Iteration 70, loss = 0.15917539\n",
      "Iteration 301, loss = 0.01459832\n",
      "Iteration 71, loss = 0.15889501\n",
      "Iteration 302, loss = 0.01393625\n",
      "Iteration 72, loss = 0.15649202\n",
      "Iteration 303, loss = 0.01363324\n",
      "Iteration 73, loss = 0.15543898\n",
      "Iteration 304, loss = 0.01382843\n",
      "Iteration 74, loss = 0.15324299\n",
      "Iteration 305, loss = 0.01373592\n",
      "Iteration 75, loss = 0.15183758\n",
      "Iteration 306, loss = 0.01358430\n",
      "Iteration 76, loss = 0.14989252\n",
      "Iteration 307, loss = 0.01378482\n",
      "Iteration 77, loss = 0.14664048\n",
      "Iteration 308, loss = 0.01469253\n",
      "Iteration 78, loss = 0.14549560\n",
      "Iteration 309, loss = 0.01405705\n",
      "Iteration 79, loss = 0.14353588\n",
      "Iteration 310, loss = 0.01314307\n",
      "Iteration 80, loss = 0.14323857\n",
      "Iteration 311, loss = 0.01330967\n",
      "Iteration 81, loss = 0.14526179\n",
      "Iteration 312, loss = 0.01324570\n",
      "Iteration 82, loss = 0.15195014\n",
      "Iteration 313, loss = 0.01344101\n",
      "Iteration 83, loss = 0.13947335\n",
      "Iteration 314, loss = 0.01312202\n",
      "Iteration 84, loss = 0.14060821\n",
      "Iteration 315, loss = 0.01298107\n",
      "Iteration 85, loss = 0.13812725\n",
      "Iteration 316, loss = 0.01326616\n",
      "Iteration 86, loss = 0.13740764\n",
      "Iteration 317, loss = 0.01286061\n",
      "Iteration 87, loss = 0.13423229\n",
      "Iteration 318, loss = 0.01268370\n",
      "Iteration 88, loss = 0.13110329\n",
      "Iteration 319, loss = 0.01250288\n",
      "Iteration 89, loss = 0.13433803\n",
      "Iteration 320, loss = 0.01264249\n",
      "Iteration 90, loss = 0.13142863\n",
      "Iteration 321, loss = 0.01175613\n",
      "Iteration 91, loss = 0.13026286\n",
      "Iteration 322, loss = 0.01270868\n",
      "Iteration 92, loss = 0.12800435\n",
      "Iteration 323, loss = 0.01190086\n",
      "Iteration 93, loss = 0.12568185\n",
      "Iteration 324, loss = 0.01191473\n",
      "Iteration 94, loss = 0.12473877\n",
      "Iteration 325, loss = 0.01225029\n",
      "Iteration 95, loss = 0.12420622\n",
      "Iteration 326, loss = 0.01147601\n",
      "Iteration 96, loss = 0.12387231\n",
      "Iteration 327, loss = 0.01148582\n",
      "Iteration 97, loss = 0.12052997\n",
      "Iteration 328, loss = 0.01150448\n",
      "Iteration 98, loss = 0.12021319\n",
      "Iteration 329, loss = 0.01078098\n",
      "Iteration 99, loss = 0.12028588\n",
      "Iteration 330, loss = 0.01121979\n",
      "Iteration 100, loss = 0.11746268\n",
      "Iteration 331, loss = 0.01113188\n",
      "Iteration 101, loss = 0.11823076\n",
      "Iteration 332, loss = 0.01189099\n",
      "Iteration 102, loss = 0.11555218\n",
      "Iteration 333, loss = 0.01232639\n",
      "Iteration 103, loss = 0.11267808\n",
      "Iteration 334, loss = 0.01166799\n",
      "Iteration 104, loss = 0.11751681\n",
      "Iteration 335, loss = 0.01156349\n",
      "Iteration 105, loss = 0.11295015\n",
      "Iteration 336, loss = 0.01055476\n",
      "Iteration 106, loss = 0.11258594\n",
      "Iteration 337, loss = 0.01019632\n",
      "Iteration 107, loss = 0.11011206\n",
      "Iteration 338, loss = 0.01075290\n",
      "Iteration 108, loss = 0.10982400\n",
      "Iteration 339, loss = 0.01121383\n",
      "Iteration 109, loss = 0.10792947\n",
      "Iteration 340, loss = 0.01083495\n",
      "Iteration 110, loss = 0.10669443\n",
      "Iteration 341, loss = 0.01040452\n",
      "Iteration 111, loss = 0.10699990\n",
      "Iteration 342, loss = 0.01000527\n",
      "Iteration 112, loss = 0.10582431\n",
      "Iteration 343, loss = 0.01176006\n",
      "Iteration 113, loss = 0.10566235\n",
      "Iteration 344, loss = 0.01401526\n",
      "Iteration 114, loss = 0.10516594\n",
      "Iteration 345, loss = 0.01158594\n",
      "Iteration 115, loss = 0.10587577\n",
      "Iteration 346, loss = 0.00976294\n",
      "Iteration 116, loss = 0.10164394\n",
      "Iteration 347, loss = 0.00995154\n",
      "Iteration 117, loss = 0.10104125\n",
      "Iteration 348, loss = 0.01018588\n",
      "Iteration 118, loss = 0.10286041\n",
      "Iteration 349, loss = 0.01020133\n",
      "Iteration 119, loss = 0.09939482\n",
      "Iteration 350, loss = 0.01009102\n",
      "Iteration 351, loss = 0.01007939\n",
      "Iteration 120, loss = 0.09995501\n",
      "Iteration 352, loss = 0.01012275\n",
      "Iteration 121, loss = 0.09798102\n",
      "Iteration 353, loss = 0.01052041\n",
      "Iteration 122, loss = 0.09566952\n",
      "Iteration 354, loss = 0.01068703\n",
      "Iteration 123, loss = 0.09490115\n",
      "Iteration 355, loss = 0.01166797\n",
      "Iteration 124, loss = 0.09415728\n",
      "Iteration 356, loss = 0.01172742\n",
      "Iteration 125, loss = 0.09296270\n",
      "Iteration 357, loss = 0.01231521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 126, loss = 0.09217529\n",
      "Iteration 127, loss = 0.09132341\n",
      "Iteration 128, loss = 0.09108436\n",
      "Iteration 129, loss = 0.09001875\n",
      "Iteration 130, loss = 0.09141415\n",
      "Iteration 131, loss = 0.09492958\n",
      "Iteration 132, loss = 0.09307143\n",
      "Iteration 133, loss = 0.08799265\n",
      "Iteration 134, loss = 0.08962126\n",
      "Iteration 135, loss = 0.09089001\n",
      "Iteration 136, loss = 0.08800298\n",
      "Iteration 137, loss = 0.08670181\n",
      "Iteration 138, loss = 0.08513105\n",
      "Iteration 1, loss = 1.06212231\n",
      "Iteration 139, loss = 0.08522449\n",
      "Iteration 2, loss = 0.79120739\n",
      "Iteration 140, loss = 0.08644353\n",
      "Iteration 3, loss = 0.64716550\n",
      "Iteration 141, loss = 0.08410251\n",
      "Iteration 4, loss = 0.56782736\n",
      "Iteration 142, loss = 0.08177016\n",
      "Iteration 5, loss = 0.51716476\n",
      "Iteration 143, loss = 0.07981600\n",
      "Iteration 6, loss = 0.48344266\n",
      "Iteration 144, loss = 0.08064415\n",
      "Iteration 7, loss = 0.45450528\n",
      "Iteration 145, loss = 0.08120519\n",
      "Iteration 8, loss = 0.43322710\n",
      "Iteration 146, loss = 0.08203328\n",
      "Iteration 9, loss = 0.41657272\n",
      "Iteration 147, loss = 0.07843330\n",
      "Iteration 10, loss = 0.39988181\n",
      "Iteration 148, loss = 0.07814237\n",
      "Iteration 11, loss = 0.38647428\n",
      "Iteration 149, loss = 0.07986869\n",
      "Iteration 12, loss = 0.37475252\n",
      "Iteration 150, loss = 0.07822952\n",
      "Iteration 13, loss = 0.36485688\n",
      "Iteration 151, loss = 0.07770244\n",
      "Iteration 14, loss = 0.35497048\n",
      "Iteration 152, loss = 0.07431059\n",
      "Iteration 15, loss = 0.34664307\n",
      "Iteration 153, loss = 0.07445679\n",
      "Iteration 16, loss = 0.33900477\n",
      "Iteration 154, loss = 0.07359368\n",
      "Iteration 17, loss = 0.33114908\n",
      "Iteration 155, loss = 0.07386646\n",
      "Iteration 18, loss = 0.32442068\n",
      "Iteration 156, loss = 0.07346446\n",
      "Iteration 19, loss = 0.31709474\n",
      "Iteration 157, loss = 0.07336547\n",
      "Iteration 20, loss = 0.31175234\n",
      "Iteration 158, loss = 0.06976371\n",
      "Iteration 21, loss = 0.30552942\n",
      "Iteration 159, loss = 0.07195709\n",
      "Iteration 22, loss = 0.30041533\n",
      "Iteration 160, loss = 0.07355370\n",
      "Iteration 23, loss = 0.29644551\n",
      "Iteration 161, loss = 0.06966859\n",
      "Iteration 24, loss = 0.28910634\n",
      "Iteration 162, loss = 0.07106969\n",
      "Iteration 25, loss = 0.28672796\n",
      "Iteration 163, loss = 0.06933153\n",
      "Iteration 26, loss = 0.28167889\n",
      "Iteration 164, loss = 0.06861478\n",
      "Iteration 27, loss = 0.27745483\n",
      "Iteration 165, loss = 0.06763924\n",
      "Iteration 28, loss = 0.27232117\n",
      "Iteration 166, loss = 0.07085089\n",
      "Iteration 29, loss = 0.26949926\n",
      "Iteration 167, loss = 0.06783404\n",
      "Iteration 30, loss = 0.26407223\n",
      "Iteration 31, loss = 0.26053877\n",
      "Iteration 168, loss = 0.06811023\n",
      "Iteration 169, loss = 0.06876130\n",
      "Iteration 32, loss = 0.25732518\n",
      "Iteration 170, loss = 0.06610606\n",
      "Iteration 33, loss = 0.25310205\n",
      "Iteration 34, loss = 0.24999948\n",
      "Iteration 171, loss = 0.06418588\n",
      "Iteration 35, loss = 0.24599051\n",
      "Iteration 172, loss = 0.06440277\n",
      "Iteration 36, loss = 0.24259890\n",
      "Iteration 173, loss = 0.06269559\n",
      "Iteration 37, loss = 0.23938922\n",
      "Iteration 174, loss = 0.06295180\n",
      "Iteration 38, loss = 0.23613254\n",
      "Iteration 175, loss = 0.06216672\n",
      "Iteration 39, loss = 0.23239400\n",
      "Iteration 176, loss = 0.06087932\n",
      "Iteration 40, loss = 0.23033105\n",
      "Iteration 177, loss = 0.05953138\n",
      "Iteration 41, loss = 0.22526317\n",
      "Iteration 178, loss = 0.05956974\n",
      "Iteration 42, loss = 0.22409678\n",
      "Iteration 179, loss = 0.06046855\n",
      "Iteration 43, loss = 0.22012375\n",
      "Iteration 180, loss = 0.05928885\n",
      "Iteration 44, loss = 0.21878583\n",
      "Iteration 181, loss = 0.06124591\n",
      "Iteration 45, loss = 0.21409453\n",
      "Iteration 182, loss = 0.05917828\n",
      "Iteration 46, loss = 0.21153871\n",
      "Iteration 183, loss = 0.05959444\n",
      "Iteration 47, loss = 0.21019758\n",
      "Iteration 184, loss = 0.05733183\n",
      "Iteration 185, loss = 0.05799453\n",
      "Iteration 48, loss = 0.20679486\n",
      "Iteration 186, loss = 0.06450072\n",
      "Iteration 49, loss = 0.20350953\n",
      "Iteration 50, loss = 0.20230150\n",
      "Iteration 187, loss = 0.05884907\n",
      "Iteration 51, loss = 0.19810817\n",
      "Iteration 188, loss = 0.06222468\n",
      "Iteration 52, loss = 0.19683144\n",
      "Iteration 189, loss = 0.06375949\n",
      "Iteration 53, loss = 0.19589442\n",
      "Iteration 190, loss = 0.06413768\n",
      "Iteration 54, loss = 0.19649127\n",
      "Iteration 191, loss = 0.05674855\n",
      "Iteration 55, loss = 0.19550416\n",
      "Iteration 192, loss = 0.06282662\n",
      "Iteration 56, loss = 0.18875279\n",
      "Iteration 193, loss = 0.06289429\n",
      "Iteration 57, loss = 0.18454818\n",
      "Iteration 194, loss = 0.06006607\n",
      "Iteration 58, loss = 0.18026855\n",
      "Iteration 195, loss = 0.05416124\n",
      "Iteration 59, loss = 0.18030404\n",
      "Iteration 196, loss = 0.05945596\n",
      "Iteration 60, loss = 0.17787587\n",
      "Iteration 197, loss = 0.05718014\n",
      "Iteration 61, loss = 0.17597168\n",
      "Iteration 198, loss = 0.05700718\n",
      "Iteration 62, loss = 0.17128965\n",
      "Iteration 199, loss = 0.05698296\n",
      "Iteration 63, loss = 0.17007129\n",
      "Iteration 200, loss = 0.05529040\n",
      "Iteration 64, loss = 0.16624727\n",
      "Iteration 201, loss = 0.05556699\n",
      "Iteration 65, loss = 0.16381084\n",
      "Iteration 202, loss = 0.05957358\n",
      "Iteration 66, loss = 0.16401259\n",
      "Iteration 203, loss = 0.05876285\n",
      "Iteration 67, loss = 0.16007259\n",
      "Iteration 204, loss = 0.05571760\n",
      "Iteration 68, loss = 0.15819016\n",
      "Iteration 205, loss = 0.05091753\n",
      "Iteration 69, loss = 0.15767973\n",
      "Iteration 206, loss = 0.04969655\n",
      "Iteration 70, loss = 0.15366591\n",
      "Iteration 207, loss = 0.05101414\n",
      "Iteration 71, loss = 0.15429935\n",
      "Iteration 208, loss = 0.04654070\n",
      "Iteration 72, loss = 0.15424986\n",
      "Iteration 209, loss = 0.04781854\n",
      "Iteration 73, loss = 0.15170591\n",
      "Iteration 210, loss = 0.04699438\n",
      "Iteration 74, loss = 0.14770392\n",
      "Iteration 211, loss = 0.04691735\n",
      "Iteration 75, loss = 0.14828816\n",
      "Iteration 212, loss = 0.04599387\n",
      "Iteration 76, loss = 0.15212238\n",
      "Iteration 213, loss = 0.04597601\n",
      "Iteration 77, loss = 0.14539042\n",
      "Iteration 214, loss = 0.04495575\n",
      "Iteration 78, loss = 0.14186038\n",
      "Iteration 215, loss = 0.04492440\n",
      "Iteration 79, loss = 0.14298972\n",
      "Iteration 216, loss = 0.04553429\n",
      "Iteration 80, loss = 0.13805137\n",
      "Iteration 217, loss = 0.04438214\n",
      "Iteration 81, loss = 0.13686568\n",
      "Iteration 218, loss = 0.04322781\n",
      "Iteration 82, loss = 0.13505572\n",
      "Iteration 219, loss = 0.04341307\n",
      "Iteration 83, loss = 0.13307648\n",
      "Iteration 220, loss = 0.04296866\n",
      "Iteration 84, loss = 0.13349073\n",
      "Iteration 221, loss = 0.04305015\n",
      "Iteration 85, loss = 0.13250059\n",
      "Iteration 222, loss = 0.04231079\n",
      "Iteration 86, loss = 0.12811694\n",
      "Iteration 223, loss = 0.04189897\n",
      "Iteration 87, loss = 0.13027330\n",
      "Iteration 224, loss = 0.04170833\n",
      "Iteration 88, loss = 0.13101755\n",
      "Iteration 225, loss = 0.04219713\n",
      "Iteration 89, loss = 0.12552702\n",
      "Iteration 226, loss = 0.04119043\n",
      "Iteration 90, loss = 0.12533878\n",
      "Iteration 227, loss = 0.04262101\n",
      "Iteration 91, loss = 0.12122511\n",
      "Iteration 228, loss = 0.04958632\n",
      "Iteration 92, loss = 0.12381709\n",
      "Iteration 229, loss = 0.05449669\n",
      "Iteration 93, loss = 0.12069069\n",
      "Iteration 230, loss = 0.05211174\n",
      "Iteration 94, loss = 0.12051905\n",
      "Iteration 231, loss = 0.04374712\n",
      "Iteration 95, loss = 0.11760523\n",
      "Iteration 232, loss = 0.04687726\n",
      "Iteration 96, loss = 0.11618536\n",
      "Iteration 233, loss = 0.04349812\n",
      "Iteration 97, loss = 0.11624515\n",
      "Iteration 234, loss = 0.04088869\n",
      "Iteration 98, loss = 0.11395587\n",
      "Iteration 235, loss = 0.03982523\n",
      "Iteration 99, loss = 0.11473266\n",
      "Iteration 236, loss = 0.03975561\n",
      "Iteration 100, loss = 0.11467224\n",
      "Iteration 237, loss = 0.04218196\n",
      "Iteration 101, loss = 0.11305724\n",
      "Iteration 238, loss = 0.03649020\n",
      "Iteration 102, loss = 0.11220833\n",
      "Iteration 239, loss = 0.03828380\n",
      "Iteration 103, loss = 0.11069602\n",
      "Iteration 240, loss = 0.03808426\n",
      "Iteration 104, loss = 0.11096962\n",
      "Iteration 241, loss = 0.03592826\n",
      "Iteration 105, loss = 0.10656318\n",
      "Iteration 242, loss = 0.03563177\n",
      "Iteration 106, loss = 0.10743458\n",
      "Iteration 243, loss = 0.03562612\n",
      "Iteration 107, loss = 0.10476908\n",
      "Iteration 244, loss = 0.03533103\n",
      "Iteration 108, loss = 0.10210543\n",
      "Iteration 245, loss = 0.03592008\n",
      "Iteration 109, loss = 0.10224694\n",
      "Iteration 246, loss = 0.03649712\n",
      "Iteration 110, loss = 0.10709487\n",
      "Iteration 247, loss = 0.03517350\n",
      "Iteration 111, loss = 0.10717393\n",
      "Iteration 248, loss = 0.03388354\n",
      "Iteration 112, loss = 0.09984294\n",
      "Iteration 249, loss = 0.03399249\n",
      "Iteration 113, loss = 0.10129667\n",
      "Iteration 250, loss = 0.03374890\n",
      "Iteration 114, loss = 0.09876633\n",
      "Iteration 251, loss = 0.03527982\n",
      "Iteration 115, loss = 0.10008971\n",
      "Iteration 252, loss = 0.03329795\n",
      "Iteration 116, loss = 0.09524227\n",
      "Iteration 253, loss = 0.03369723\n",
      "Iteration 117, loss = 0.09635716\n",
      "Iteration 254, loss = 0.03197844\n",
      "Iteration 118, loss = 0.09464854\n",
      "Iteration 255, loss = 0.03246043\n",
      "Iteration 119, loss = 0.09431069\n",
      "Iteration 256, loss = 0.03305830\n",
      "Iteration 120, loss = 0.09167609\n",
      "Iteration 257, loss = 0.03481125\n",
      "Iteration 121, loss = 0.09220761\n",
      "Iteration 258, loss = 0.03310775\n",
      "Iteration 122, loss = 0.09242479\n",
      "Iteration 259, loss = 0.03219720\n",
      "Iteration 123, loss = 0.09005826\n",
      "Iteration 260, loss = 0.03191908\n",
      "Iteration 124, loss = 0.09019354\n",
      "Iteration 261, loss = 0.03356796\n",
      "Iteration 125, loss = 0.08736213\n",
      "Iteration 262, loss = 0.02930183\n",
      "Iteration 126, loss = 0.08840358\n",
      "Iteration 263, loss = 0.03083902\n",
      "Iteration 127, loss = 0.08724159\n",
      "Iteration 264, loss = 0.02944137\n",
      "Iteration 128, loss = 0.08877403\n",
      "Iteration 265, loss = 0.02980398\n",
      "Iteration 129, loss = 0.08532809\n",
      "Iteration 266, loss = 0.02930912\n",
      "Iteration 130, loss = 0.08906608\n",
      "Iteration 267, loss = 0.02899460\n",
      "Iteration 131, loss = 0.08544003\n",
      "Iteration 268, loss = 0.02918930\n",
      "Iteration 132, loss = 0.08494859\n",
      "Iteration 269, loss = 0.02834213\n",
      "Iteration 133, loss = 0.08221832\n",
      "Iteration 270, loss = 0.02826135\n",
      "Iteration 134, loss = 0.08163592\n",
      "Iteration 271, loss = 0.03037763\n",
      "Iteration 135, loss = 0.08451319\n",
      "Iteration 272, loss = 0.03195381\n",
      "Iteration 136, loss = 0.08000782\n",
      "Iteration 273, loss = 0.03342855\n",
      "Iteration 137, loss = 0.07728064\n",
      "Iteration 274, loss = 0.02782962\n",
      "Iteration 138, loss = 0.07848452\n",
      "Iteration 275, loss = 0.02974984\n",
      "Iteration 139, loss = 0.08077509\n",
      "Iteration 276, loss = 0.02728328\n",
      "Iteration 140, loss = 0.07997582\n",
      "Iteration 277, loss = 0.02994255\n",
      "Iteration 141, loss = 0.07535249\n",
      "Iteration 278, loss = 0.02765316\n",
      "Iteration 142, loss = 0.07658054\n",
      "Iteration 143, loss = 0.07746083\n",
      "Iteration 279, loss = 0.02712076\n",
      "Iteration 144, loss = 0.07872177\n",
      "Iteration 280, loss = 0.02688855\n",
      "Iteration 145, loss = 0.07945193\n",
      "Iteration 281, loss = 0.02660924\n",
      "Iteration 146, loss = 0.08095416\n",
      "Iteration 282, loss = 0.02688185\n",
      "Iteration 147, loss = 0.07400503\n",
      "Iteration 283, loss = 0.02603765\n",
      "Iteration 148, loss = 0.07709803\n",
      "Iteration 284, loss = 0.02587855\n",
      "Iteration 149, loss = 0.07273130\n",
      "Iteration 285, loss = 0.02558660\n",
      "Iteration 150, loss = 0.07256235\n",
      "Iteration 286, loss = 0.02512307\n",
      "Iteration 151, loss = 0.07004263\n",
      "Iteration 287, loss = 0.02538937\n",
      "Iteration 152, loss = 0.06827153\n",
      "Iteration 288, loss = 0.02585044\n",
      "Iteration 153, loss = 0.07038812\n",
      "Iteration 289, loss = 0.02476133\n",
      "Iteration 154, loss = 0.06707646\n",
      "Iteration 290, loss = 0.02423700\n",
      "Iteration 155, loss = 0.07059597\n",
      "Iteration 291, loss = 0.02439406\n",
      "Iteration 156, loss = 0.06795320\n",
      "Iteration 292, loss = 0.02508999\n",
      "Iteration 157, loss = 0.06807624\n",
      "Iteration 293, loss = 0.02683869\n",
      "Iteration 158, loss = 0.06514302\n",
      "Iteration 294, loss = 0.02731115\n",
      "Iteration 159, loss = 0.06484796\n",
      "Iteration 295, loss = 0.02990199\n",
      "Iteration 160, loss = 0.06633375\n",
      "Iteration 296, loss = 0.02633703\n",
      "Iteration 161, loss = 0.06135953\n",
      "Iteration 297, loss = 0.02542517\n",
      "Iteration 298, loss = 0.02814212\n",
      "Iteration 162, loss = 0.06334449\n",
      "Iteration 299, loss = 0.02746823\n",
      "Iteration 163, loss = 0.06582423\n",
      "Iteration 300, loss = 0.02416773\n",
      "Iteration 164, loss = 0.06532160\n",
      "Iteration 301, loss = 0.02300771\n",
      "Iteration 165, loss = 0.06329085\n",
      "Iteration 302, loss = 0.02266388\n",
      "Iteration 166, loss = 0.05878670\n",
      "Iteration 303, loss = 0.02269294\n",
      "Iteration 167, loss = 0.05855114\n",
      "Iteration 304, loss = 0.02316149\n",
      "Iteration 168, loss = 0.05999477\n",
      "Iteration 305, loss = 0.02357829\n",
      "Iteration 169, loss = 0.05893169\n",
      "Iteration 170, loss = 0.05653661\n",
      "Iteration 306, loss = 0.02297967\n",
      "Iteration 171, loss = 0.05595816\n",
      "Iteration 307, loss = 0.02129039\n",
      "Iteration 172, loss = 0.05529917\n",
      "Iteration 308, loss = 0.02208376\n",
      "Iteration 173, loss = 0.05520221\n",
      "Iteration 309, loss = 0.02132700\n",
      "Iteration 174, loss = 0.05460749\n",
      "Iteration 310, loss = 0.02094932\n",
      "Iteration 175, loss = 0.05414995\n",
      "Iteration 311, loss = 0.02125382\n",
      "Iteration 176, loss = 0.05477389\n",
      "Iteration 312, loss = 0.02124392\n",
      "Iteration 177, loss = 0.05275684\n",
      "Iteration 313, loss = 0.02108443\n",
      "Iteration 178, loss = 0.05647116\n",
      "Iteration 314, loss = 0.02010102\n",
      "Iteration 179, loss = 0.05391562\n",
      "Iteration 315, loss = 0.02037333\n",
      "Iteration 316, loss = 0.01964603\n",
      "Iteration 180, loss = 0.05553592\n",
      "Iteration 317, loss = 0.02007506\n",
      "Iteration 181, loss = 0.05193040\n",
      "Iteration 318, loss = 0.02041978\n",
      "Iteration 182, loss = 0.05366736\n",
      "Iteration 183, loss = 0.05052575\n",
      "Iteration 319, loss = 0.02042012\n",
      "Iteration 184, loss = 0.04993963Iteration 320, loss = 0.02006547\n",
      "\n",
      "Iteration 321, loss = 0.01957443\n",
      "Iteration 185, loss = 0.05168499\n",
      "Iteration 186, loss = 0.04798420\n",
      "Iteration 322, loss = 0.01934840\n",
      "Iteration 187, loss = 0.04911201\n",
      "Iteration 323, loss = 0.01937404\n",
      "Iteration 188, loss = 0.04858432\n",
      "Iteration 324, loss = 0.01892157\n",
      "Iteration 189, loss = 0.04698402\n",
      "Iteration 325, loss = 0.01883838\n",
      "Iteration 190, loss = 0.04676516\n",
      "Iteration 326, loss = 0.01849280\n",
      "Iteration 191, loss = 0.04846271\n",
      "Iteration 327, loss = 0.01879217\n",
      "Iteration 192, loss = 0.04778269\n",
      "Iteration 328, loss = 0.01926956\n",
      "Iteration 329, loss = 0.01922827\n",
      "Iteration 193, loss = 0.04389554\n",
      "Iteration 330, loss = 0.01873544\n",
      "Iteration 194, loss = 0.04590669\n",
      "Iteration 331, loss = 0.02359805\n",
      "Iteration 195, loss = 0.04794394\n",
      "Iteration 332, loss = 0.02490643\n",
      "Iteration 196, loss = 0.04643255\n",
      "Iteration 333, loss = 0.02689697\n",
      "Iteration 197, loss = 0.04573423\n",
      "Iteration 334, loss = 0.02411293\n",
      "Iteration 198, loss = 0.04862609\n",
      "Iteration 335, loss = 0.03047676\n",
      "Iteration 199, loss = 0.04505051\n",
      "Iteration 336, loss = 0.02153548\n",
      "Iteration 200, loss = 0.04304489\n",
      "Iteration 337, loss = 0.01968033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 201, loss = 0.04344428\n",
      "Iteration 202, loss = 0.04194141\n",
      "Iteration 203, loss = 0.04368142\n",
      "Iteration 204, loss = 0.04256228\n",
      "Iteration 205, loss = 0.04304397\n",
      "Iteration 206, loss = 0.04130413\n",
      "Iteration 207, loss = 0.04039472\n",
      "Iteration 208, loss = 0.03942795\n",
      "Iteration 209, loss = 0.03968876\n",
      "Iteration 210, loss = 0.03995776\n",
      "Iteration 211, loss = 0.04132508\n",
      "Iteration 212, loss = 0.03843642\n",
      "Iteration 213, loss = 0.03796705\n",
      "Iteration 214, loss = 0.04042249\n",
      "Iteration 1, loss = 1.24137180\n",
      "Iteration 215, loss = 0.03900336\n",
      "Iteration 2, loss = 0.83286757\n",
      "Iteration 216, loss = 0.03651644\n",
      "Iteration 3, loss = 0.68506434\n",
      "Iteration 217, loss = 0.03713334\n",
      "Iteration 4, loss = 0.58992620\n",
      "Iteration 218, loss = 0.03642532\n",
      "Iteration 5, loss = 0.52362730\n",
      "Iteration 219, loss = 0.03645509\n",
      "Iteration 6, loss = 0.48417079\n",
      "Iteration 220, loss = 0.03566228\n",
      "Iteration 7, loss = 0.45367871\n",
      "Iteration 221, loss = 0.03602428\n",
      "Iteration 8, loss = 0.42804041\n",
      "Iteration 222, loss = 0.03413882\n",
      "Iteration 9, loss = 0.41139121\n",
      "Iteration 223, loss = 0.03548506\n",
      "Iteration 10, loss = 0.39578235\n",
      "Iteration 224, loss = 0.03327272\n",
      "Iteration 11, loss = 0.38050501\n",
      "Iteration 225, loss = 0.03324527\n",
      "Iteration 12, loss = 0.36896900\n",
      "Iteration 226, loss = 0.03498509\n",
      "Iteration 13, loss = 0.35835678\n",
      "Iteration 14, loss = 0.34793491\n",
      "Iteration 227, loss = 0.03559789\n",
      "Iteration 228, loss = 0.03314569\n",
      "Iteration 15, loss = 0.33990126\n",
      "Iteration 229, loss = 0.03741533\n",
      "Iteration 16, loss = 0.33188989\n",
      "Iteration 230, loss = 0.03568861\n",
      "Iteration 17, loss = 0.32410924\n",
      "Iteration 231, loss = 0.03112104\n",
      "Iteration 18, loss = 0.31747464\n",
      "Iteration 232, loss = 0.03186081\n",
      "Iteration 19, loss = 0.31104917\n",
      "Iteration 233, loss = 0.03246329\n",
      "Iteration 20, loss = 0.30512928\n",
      "Iteration 21, loss = 0.29899345Iteration 234, loss = 0.03862746\n",
      "\n",
      "Iteration 235, loss = 0.03523285\n",
      "Iteration 22, loss = 0.29386731\n",
      "Iteration 236, loss = 0.03131860\n",
      "Iteration 23, loss = 0.28906306\n",
      "Iteration 237, loss = 0.02978866Iteration 24, loss = 0.28393731\n",
      "\n",
      "Iteration 238, loss = 0.02996575\n",
      "Iteration 25, loss = 0.27888922\n",
      "Iteration 26, loss = 0.27408032\n",
      "Iteration 239, loss = 0.03328263\n",
      "Iteration 27, loss = 0.27072877\n",
      "Iteration 240, loss = 0.03383392\n",
      "Iteration 28, loss = 0.26746951\n",
      "Iteration 241, loss = 0.03024649\n",
      "Iteration 29, loss = 0.26258523\n",
      "Iteration 242, loss = 0.02879943\n",
      "Iteration 243, loss = 0.02902053\n",
      "Iteration 30, loss = 0.25914118\n",
      "Iteration 244, loss = 0.02751523\n",
      "Iteration 31, loss = 0.25513139\n",
      "Iteration 245, loss = 0.02801941\n",
      "Iteration 32, loss = 0.25084703\n",
      "Iteration 33, loss = 0.24776184\n",
      "Iteration 246, loss = 0.02724628\n",
      "Iteration 34, loss = 0.24392482\n",
      "Iteration 247, loss = 0.02792301\n",
      "Iteration 35, loss = 0.24160701\n",
      "Iteration 248, loss = 0.02908086\n",
      "Iteration 36, loss = 0.24011990\n",
      "Iteration 249, loss = 0.03010417\n",
      "Iteration 37, loss = 0.23546780\n",
      "Iteration 250, loss = 0.03029783\n",
      "Iteration 38, loss = 0.23076569\n",
      "Iteration 251, loss = 0.03118941\n",
      "Iteration 39, loss = 0.22708158\n",
      "Iteration 252, loss = 0.02999830\n",
      "Iteration 40, loss = 0.22538233\n",
      "Iteration 253, loss = 0.02583431\n",
      "Iteration 41, loss = 0.22202257\n",
      "Iteration 254, loss = 0.02697829\n",
      "Iteration 42, loss = 0.21932716\n",
      "Iteration 255, loss = 0.02868698\n",
      "Iteration 43, loss = 0.21567848\n",
      "Iteration 256, loss = 0.02758411\n",
      "Iteration 44, loss = 0.21289737\n",
      "Iteration 257, loss = 0.02603664\n",
      "Iteration 45, loss = 0.20993068\n",
      "Iteration 258, loss = 0.02514953\n",
      "Iteration 46, loss = 0.20763741\n",
      "Iteration 259, loss = 0.02410682\n",
      "Iteration 47, loss = 0.20442270\n",
      "Iteration 260, loss = 0.02492802\n",
      "Iteration 48, loss = 0.20271651\n",
      "Iteration 261, loss = 0.02483734\n",
      "Iteration 49, loss = 0.19976187\n",
      "Iteration 262, loss = 0.02474844\n",
      "Iteration 50, loss = 0.19731663\n",
      "Iteration 263, loss = 0.02528606\n",
      "Iteration 51, loss = 0.19672943\n",
      "Iteration 264, loss = 0.02532587\n",
      "Iteration 52, loss = 0.19316679\n",
      "Iteration 265, loss = 0.02560326\n",
      "Iteration 53, loss = 0.18804773\n",
      "Iteration 266, loss = 0.02599690\n",
      "Iteration 54, loss = 0.19001570\n",
      "Iteration 267, loss = 0.02627277\n",
      "Iteration 55, loss = 0.18771040\n",
      "Iteration 268, loss = 0.02757958\n",
      "Iteration 56, loss = 0.18256179\n",
      "Iteration 269, loss = 0.02579631\n",
      "Iteration 57, loss = 0.18157180\n",
      "Iteration 270, loss = 0.02398559\n",
      "Iteration 58, loss = 0.17943844\n",
      "Iteration 271, loss = 0.02235380\n",
      "Iteration 59, loss = 0.17676045\n",
      "Iteration 272, loss = 0.02342368\n",
      "Iteration 60, loss = 0.17454746\n",
      "Iteration 273, loss = 0.02328580\n",
      "Iteration 61, loss = 0.17059829\n",
      "Iteration 274, loss = 0.02261590\n",
      "Iteration 62, loss = 0.17097421\n",
      "Iteration 275, loss = 0.02378834\n",
      "Iteration 63, loss = 0.16924951\n",
      "Iteration 276, loss = 0.02088528\n",
      "Iteration 64, loss = 0.16495446\n",
      "Iteration 277, loss = 0.02119636\n",
      "Iteration 65, loss = 0.16406905\n",
      "Iteration 278, loss = 0.02084883\n",
      "Iteration 66, loss = 0.16124366\n",
      "Iteration 279, loss = 0.02126368\n",
      "Iteration 67, loss = 0.15908728\n",
      "Iteration 280, loss = 0.02334213\n",
      "Iteration 68, loss = 0.15725752\n",
      "Iteration 281, loss = 0.02186609\n",
      "Iteration 69, loss = 0.15466630\n",
      "Iteration 282, loss = 0.02175460\n",
      "Iteration 70, loss = 0.15491893\n",
      "Iteration 283, loss = 0.02039427\n",
      "Iteration 71, loss = 0.15173759\n",
      "Iteration 284, loss = 0.02000538\n",
      "Iteration 72, loss = 0.14950395\n",
      "Iteration 285, loss = 0.01908992\n",
      "Iteration 73, loss = 0.14959673\n",
      "Iteration 286, loss = 0.01923376\n",
      "Iteration 74, loss = 0.14870412\n",
      "Iteration 287, loss = 0.01895071\n",
      "Iteration 75, loss = 0.14446213\n",
      "Iteration 288, loss = 0.01887564\n",
      "Iteration 76, loss = 0.14253938\n",
      "Iteration 289, loss = 0.02139231\n",
      "Iteration 77, loss = 0.14148415\n",
      "Iteration 290, loss = 0.01956713\n",
      "Iteration 78, loss = 0.13941503\n",
      "Iteration 291, loss = 0.02003408\n",
      "Iteration 79, loss = 0.13813220\n",
      "Iteration 292, loss = 0.01862848\n",
      "Iteration 80, loss = 0.13704983\n",
      "Iteration 293, loss = 0.01884679\n",
      "Iteration 81, loss = 0.13653681\n",
      "Iteration 294, loss = 0.01826758\n",
      "Iteration 82, loss = 0.13641598\n",
      "Iteration 295, loss = 0.01828187\n",
      "Iteration 83, loss = 0.13463382\n",
      "Iteration 296, loss = 0.01811908\n",
      "Iteration 84, loss = 0.13432973\n",
      "Iteration 297, loss = 0.01810762\n",
      "Iteration 85, loss = 0.13151113\n",
      "Iteration 298, loss = 0.01768368\n",
      "Iteration 86, loss = 0.12708417\n",
      "Iteration 299, loss = 0.01768415\n",
      "Iteration 87, loss = 0.12713847\n",
      "Iteration 300, loss = 0.01830808\n",
      "Iteration 88, loss = 0.12840540\n",
      "Iteration 301, loss = 0.01759299\n",
      "Iteration 89, loss = 0.12569806\n",
      "Iteration 302, loss = 0.01872986\n",
      "Iteration 90, loss = 0.12589811\n",
      "Iteration 303, loss = 0.01800761\n",
      "Iteration 91, loss = 0.12131346\n",
      "Iteration 304, loss = 0.01694623\n",
      "Iteration 92, loss = 0.12252440\n",
      "Iteration 305, loss = 0.01677998\n",
      "Iteration 93, loss = 0.11914625\n",
      "Iteration 306, loss = 0.01653948\n",
      "Iteration 94, loss = 0.12132206\n",
      "Iteration 307, loss = 0.01635498\n",
      "Iteration 95, loss = 0.11899286\n",
      "Iteration 308, loss = 0.01773670\n",
      "Iteration 96, loss = 0.11829441\n",
      "Iteration 309, loss = 0.01713412\n",
      "Iteration 97, loss = 0.11560570\n",
      "Iteration 310, loss = 0.01714654\n",
      "Iteration 98, loss = 0.11280536\n",
      "Iteration 311, loss = 0.01621065\n",
      "Iteration 99, loss = 0.11259960\n",
      "Iteration 312, loss = 0.01625579\n",
      "Iteration 100, loss = 0.10979119\n",
      "Iteration 313, loss = 0.01580248\n",
      "Iteration 101, loss = 0.11028212\n",
      "Iteration 314, loss = 0.01568004\n",
      "Iteration 102, loss = 0.10869090\n",
      "Iteration 315, loss = 0.01551477\n",
      "Iteration 316, loss = 0.01669549\n",
      "Iteration 103, loss = 0.10656702\n",
      "Iteration 104, loss = 0.10655087\n",
      "Iteration 317, loss = 0.01590610\n",
      "Iteration 105, loss = 0.10476001\n",
      "Iteration 318, loss = 0.01656313\n",
      "Iteration 106, loss = 0.10484816\n",
      "Iteration 319, loss = 0.01554008\n",
      "Iteration 107, loss = 0.10392577\n",
      "Iteration 320, loss = 0.01654257\n",
      "Iteration 108, loss = 0.10385250\n",
      "Iteration 321, loss = 0.01697297\n",
      "Iteration 109, loss = 0.10225262\n",
      "Iteration 322, loss = 0.01895435\n",
      "Iteration 110, loss = 0.09996799\n",
      "Iteration 323, loss = 0.01973214\n",
      "Iteration 111, loss = 0.10009691\n",
      "Iteration 324, loss = 0.01856599\n",
      "Iteration 112, loss = 0.09973065\n",
      "Iteration 325, loss = 0.01964585\n",
      "Iteration 113, loss = 0.09680673\n",
      "Iteration 326, loss = 0.02251524\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 114, loss = 0.09861841\n",
      "Iteration 115, loss = 0.09526763\n",
      "Iteration 116, loss = 0.09616314\n",
      "Iteration 117, loss = 0.09350439\n",
      "Iteration 118, loss = 0.09529440\n",
      "Iteration 119, loss = 0.09322890\n",
      "Iteration 120, loss = 0.09372638\n",
      "Iteration 121, loss = 0.09043729\n",
      "Iteration 122, loss = 0.09142000\n",
      "Iteration 123, loss = 0.08829098\n",
      "Iteration 124, loss = 0.08802971\n",
      "Iteration 125, loss = 0.08638374\n",
      "Iteration 126, loss = 0.08686150\n",
      "Iteration 1, loss = 1.40902569\n",
      "Iteration 127, loss = 0.08621165\n",
      "Iteration 2, loss = 0.93381475\n",
      "Iteration 128, loss = 0.08691421\n",
      "Iteration 3, loss = 0.71437754\n",
      "Iteration 129, loss = 0.08704962\n",
      "Iteration 4, loss = 0.59731932\n",
      "Iteration 130, loss = 0.08955328\n",
      "Iteration 5, loss = 0.53694565\n",
      "Iteration 131, loss = 0.08784535\n",
      "Iteration 6, loss = 0.49608350\n",
      "Iteration 132, loss = 0.08470699\n",
      "Iteration 7, loss = 0.46490983\n",
      "Iteration 133, loss = 0.08047927\n",
      "Iteration 8, loss = 0.44422482\n",
      "Iteration 134, loss = 0.08267644\n",
      "Iteration 9, loss = 0.42535813\n",
      "Iteration 135, loss = 0.08268618\n",
      "Iteration 10, loss = 0.40877859\n",
      "Iteration 136, loss = 0.07951834\n",
      "Iteration 11, loss = 0.39519107\n",
      "Iteration 137, loss = 0.08377272\n",
      "Iteration 12, loss = 0.38408258\n",
      "Iteration 138, loss = 0.08141741\n",
      "Iteration 13, loss = 0.37340280\n",
      "Iteration 139, loss = 0.08006445\n",
      "Iteration 14, loss = 0.36337932\n",
      "Iteration 140, loss = 0.07864445\n",
      "Iteration 15, loss = 0.35482706\n",
      "Iteration 141, loss = 0.07939905\n",
      "Iteration 16, loss = 0.34652525\n",
      "Iteration 142, loss = 0.08025754\n",
      "Iteration 17, loss = 0.33976735\n",
      "Iteration 143, loss = 0.07965230\n",
      "Iteration 18, loss = 0.33075330\n",
      "Iteration 19, loss = 0.32612350\n",
      "Iteration 144, loss = 0.07361825\n",
      "Iteration 145, loss = 0.07321966\n",
      "Iteration 20, loss = 0.31861440\n",
      "Iteration 146, loss = 0.07403325\n",
      "Iteration 21, loss = 0.31293283\n",
      "Iteration 147, loss = 0.07121197\n",
      "Iteration 22, loss = 0.30692626\n",
      "Iteration 148, loss = 0.06983919\n",
      "Iteration 23, loss = 0.30144711\n",
      "Iteration 149, loss = 0.06919570\n",
      "Iteration 24, loss = 0.29626535\n",
      "Iteration 150, loss = 0.07018233\n",
      "Iteration 25, loss = 0.29135654\n",
      "Iteration 151, loss = 0.06804229\n",
      "Iteration 26, loss = 0.28644820\n",
      "Iteration 152, loss = 0.06712188\n",
      "Iteration 27, loss = 0.28306496\n",
      "Iteration 153, loss = 0.06600803\n",
      "Iteration 28, loss = 0.27866560\n",
      "Iteration 154, loss = 0.06807318\n",
      "Iteration 29, loss = 0.27410231\n",
      "Iteration 155, loss = 0.06893219\n",
      "Iteration 30, loss = 0.27019336\n",
      "Iteration 156, loss = 0.06662343\n",
      "Iteration 31, loss = 0.26665818\n",
      "Iteration 157, loss = 0.06430364\n",
      "Iteration 32, loss = 0.26167252\n",
      "Iteration 158, loss = 0.06619917\n",
      "Iteration 33, loss = 0.25960109\n",
      "Iteration 159, loss = 0.06323344\n",
      "Iteration 34, loss = 0.25396245\n",
      "Iteration 160, loss = 0.06339679\n",
      "Iteration 35, loss = 0.25178998\n",
      "Iteration 161, loss = 0.06165078\n",
      "Iteration 36, loss = 0.24832920\n",
      "Iteration 162, loss = 0.06311932\n",
      "Iteration 37, loss = 0.24458595\n",
      "Iteration 163, loss = 0.06766530\n",
      "Iteration 38, loss = 0.24363297\n",
      "Iteration 164, loss = 0.06355547\n",
      "Iteration 39, loss = 0.23862693\n",
      "Iteration 165, loss = 0.06036526\n",
      "Iteration 40, loss = 0.23675254\n",
      "Iteration 166, loss = 0.06248627\n",
      "Iteration 41, loss = 0.23187273\n",
      "Iteration 167, loss = 0.05981899\n",
      "Iteration 42, loss = 0.22940649\n",
      "Iteration 168, loss = 0.05734707\n",
      "Iteration 43, loss = 0.22682524\n",
      "Iteration 169, loss = 0.05843623\n",
      "Iteration 44, loss = 0.22461240\n",
      "Iteration 170, loss = 0.05623214\n",
      "Iteration 45, loss = 0.22196771\n",
      "Iteration 171, loss = 0.05756190\n",
      "Iteration 46, loss = 0.21966032\n",
      "Iteration 172, loss = 0.05643383\n",
      "Iteration 47, loss = 0.21590066\n",
      "Iteration 173, loss = 0.05471407\n",
      "Iteration 48, loss = 0.21401511\n",
      "Iteration 174, loss = 0.05525098\n",
      "Iteration 49, loss = 0.21166560\n",
      "Iteration 175, loss = 0.05359608\n",
      "Iteration 50, loss = 0.20800078\n",
      "Iteration 176, loss = 0.05440796\n",
      "Iteration 51, loss = 0.20588258\n",
      "Iteration 177, loss = 0.05230899\n",
      "Iteration 52, loss = 0.20256592\n",
      "Iteration 178, loss = 0.05340272\n",
      "Iteration 53, loss = 0.20134924\n",
      "Iteration 179, loss = 0.05426670\n",
      "Iteration 54, loss = 0.19960985\n",
      "Iteration 180, loss = 0.06203178\n",
      "Iteration 55, loss = 0.19552736\n",
      "Iteration 181, loss = 0.06387841\n",
      "Iteration 56, loss = 0.19474832\n",
      "Iteration 182, loss = 0.05327429\n",
      "Iteration 57, loss = 0.19146664\n",
      "Iteration 183, loss = 0.05393261\n",
      "Iteration 58, loss = 0.19278733\n",
      "Iteration 184, loss = 0.05256051\n",
      "Iteration 59, loss = 0.18533803\n",
      "Iteration 185, loss = 0.05201988\n",
      "Iteration 60, loss = 0.18934945\n",
      "Iteration 186, loss = 0.05223447\n",
      "Iteration 61, loss = 0.18244770\n",
      "Iteration 187, loss = 0.05531235\n",
      "Iteration 62, loss = 0.18295509\n",
      "Iteration 188, loss = 0.05168560\n",
      "Iteration 63, loss = 0.17911472\n",
      "Iteration 189, loss = 0.04925657\n",
      "Iteration 64, loss = 0.17697420\n",
      "Iteration 190, loss = 0.05490830\n",
      "Iteration 65, loss = 0.17377275\n",
      "Iteration 191, loss = 0.05131968\n",
      "Iteration 66, loss = 0.17215084\n",
      "Iteration 192, loss = 0.04700669\n",
      "Iteration 67, loss = 0.17149846\n",
      "Iteration 193, loss = 0.04746264\n",
      "Iteration 68, loss = 0.16976713\n",
      "Iteration 194, loss = 0.04942995\n",
      "Iteration 69, loss = 0.16773302\n",
      "Iteration 195, loss = 0.04570693\n",
      "Iteration 70, loss = 0.16687277\n",
      "Iteration 196, loss = 0.04483190\n",
      "Iteration 71, loss = 0.16281522\n",
      "Iteration 197, loss = 0.04760325\n",
      "Iteration 72, loss = 0.16042286\n",
      "Iteration 198, loss = 0.04470143\n",
      "Iteration 73, loss = 0.16015040\n",
      "Iteration 199, loss = 0.04403874\n",
      "Iteration 74, loss = 0.16158548\n",
      "Iteration 200, loss = 0.04535590\n",
      "Iteration 75, loss = 0.15845261\n",
      "Iteration 201, loss = 0.04541918\n",
      "Iteration 76, loss = 0.15682894\n",
      "Iteration 202, loss = 0.04457277\n",
      "Iteration 77, loss = 0.15224220\n",
      "Iteration 203, loss = 0.04531006\n",
      "Iteration 78, loss = 0.15327518\n",
      "Iteration 204, loss = 0.04140028\n",
      "Iteration 79, loss = 0.15362386\n",
      "Iteration 205, loss = 0.04162631\n",
      "Iteration 80, loss = 0.14874883\n",
      "Iteration 206, loss = 0.04149034\n",
      "Iteration 81, loss = 0.14697548\n",
      "Iteration 207, loss = 0.04435535\n",
      "Iteration 82, loss = 0.14868576\n",
      "Iteration 208, loss = 0.04070921\n",
      "Iteration 83, loss = 0.14459728\n",
      "Iteration 209, loss = 0.04023876\n",
      "Iteration 84, loss = 0.14656245\n",
      "Iteration 210, loss = 0.04052613\n",
      "Iteration 85, loss = 0.14008480\n",
      "Iteration 211, loss = 0.03935265\n",
      "Iteration 86, loss = 0.13920801\n",
      "Iteration 212, loss = 0.03926359\n",
      "Iteration 87, loss = 0.13904583\n",
      "Iteration 213, loss = 0.03920542\n",
      "Iteration 88, loss = 0.13630240\n",
      "Iteration 214, loss = 0.03929023\n",
      "Iteration 89, loss = 0.13596217\n",
      "Iteration 215, loss = 0.03959879\n",
      "Iteration 90, loss = 0.13399939\n",
      "Iteration 216, loss = 0.03857248\n",
      "Iteration 91, loss = 0.13037470\n",
      "Iteration 217, loss = 0.03811333\n",
      "Iteration 92, loss = 0.13062650\n",
      "Iteration 218, loss = 0.03815055\n",
      "Iteration 93, loss = 0.12947868\n",
      "Iteration 219, loss = 0.03734905\n",
      "Iteration 94, loss = 0.12789451\n",
      "Iteration 220, loss = 0.03709876\n",
      "Iteration 95, loss = 0.12634727\n",
      "Iteration 221, loss = 0.03660955\n",
      "Iteration 96, loss = 0.12550484\n",
      "Iteration 222, loss = 0.03724137\n",
      "Iteration 97, loss = 0.12425819\n",
      "Iteration 223, loss = 0.03560265\n",
      "Iteration 98, loss = 0.12156570\n",
      "Iteration 224, loss = 0.03639691\n",
      "Iteration 99, loss = 0.12273566\n",
      "Iteration 225, loss = 0.03583819\n",
      "Iteration 100, loss = 0.11949890\n",
      "Iteration 226, loss = 0.03545288\n",
      "Iteration 101, loss = 0.11975509\n",
      "Iteration 227, loss = 0.03757805\n",
      "Iteration 102, loss = 0.11838883\n",
      "Iteration 228, loss = 0.03526226\n",
      "Iteration 103, loss = 0.11656850\n",
      "Iteration 229, loss = 0.03503803\n",
      "Iteration 104, loss = 0.11648467\n",
      "Iteration 230, loss = 0.03308922\n",
      "Iteration 105, loss = 0.11694299\n",
      "Iteration 231, loss = 0.03419895\n",
      "Iteration 106, loss = 0.11604102\n",
      "Iteration 232, loss = 0.03549807\n",
      "Iteration 107, loss = 0.11214467\n",
      "Iteration 233, loss = 0.03436934\n",
      "Iteration 108, loss = 0.11209319\n",
      "Iteration 234, loss = 0.03158804\n",
      "Iteration 109, loss = 0.11190569\n",
      "Iteration 235, loss = 0.03137822\n",
      "Iteration 110, loss = 0.10886789\n",
      "Iteration 236, loss = 0.03249157\n",
      "Iteration 111, loss = 0.10838633\n",
      "Iteration 237, loss = 0.03154442\n",
      "Iteration 112, loss = 0.10980946\n",
      "Iteration 238, loss = 0.03027890\n",
      "Iteration 113, loss = 0.10622601\n",
      "Iteration 239, loss = 0.03101551\n",
      "Iteration 114, loss = 0.10487007\n",
      "Iteration 240, loss = 0.03051035\n",
      "Iteration 115, loss = 0.10806730\n",
      "Iteration 241, loss = 0.02998662\n",
      "Iteration 116, loss = 0.10274602\n",
      "Iteration 242, loss = 0.03141427\n",
      "Iteration 117, loss = 0.10346578\n",
      "Iteration 243, loss = 0.03383088\n",
      "Iteration 118, loss = 0.10325979\n",
      "Iteration 244, loss = 0.03290523\n",
      "Iteration 119, loss = 0.10223293\n",
      "Iteration 245, loss = 0.02842624\n",
      "Iteration 120, loss = 0.10162042\n",
      "Iteration 246, loss = 0.03030464\n",
      "Iteration 121, loss = 0.10207707\n",
      "Iteration 247, loss = 0.03186774\n",
      "Iteration 122, loss = 0.10213729\n",
      "Iteration 248, loss = 0.03033760\n",
      "Iteration 123, loss = 0.10344019\n",
      "Iteration 249, loss = 0.02902748\n",
      "Iteration 124, loss = 0.09502350\n",
      "Iteration 250, loss = 0.03116046\n",
      "Iteration 125, loss = 0.10061204\n",
      "Iteration 251, loss = 0.03058467\n",
      "Iteration 126, loss = 0.10672844\n",
      "Iteration 252, loss = 0.03013492\n",
      "Iteration 127, loss = 0.10036143\n",
      "Iteration 253, loss = 0.02963666\n",
      "Iteration 128, loss = 0.09486542\n",
      "Iteration 254, loss = 0.03179320\n",
      "Iteration 129, loss = 0.09673733\n",
      "Iteration 255, loss = 0.03103994\n",
      "Iteration 130, loss = 0.09278089\n",
      "Iteration 256, loss = 0.03054574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 131, loss = 0.09391919\n",
      "Iteration 132, loss = 0.09016002\n",
      "Iteration 133, loss = 0.08993088\n",
      "Iteration 134, loss = 0.08931704\n",
      "Iteration 135, loss = 0.09020986\n",
      "Iteration 136, loss = 0.09049711\n",
      "Iteration 137, loss = 0.09874455\n",
      "Iteration 138, loss = 0.09261528\n",
      "Iteration 139, loss = 0.09022564\n",
      "Iteration 140, loss = 0.08514877\n",
      "Iteration 141, loss = 0.08736506\n",
      "Iteration 142, loss = 0.09066420\n",
      "Iteration 143, loss = 0.08506866\n",
      "Iteration 1, loss = 1.31289503\n",
      "Iteration 144, loss = 0.08730184\n",
      "Iteration 2, loss = 0.90442131\n",
      "Iteration 145, loss = 0.08429237\n",
      "Iteration 3, loss = 0.71796211\n",
      "Iteration 4, loss = 0.61685227\n",
      "Iteration 146, loss = 0.08170474\n",
      "Iteration 5, loss = 0.54796954\n",
      "Iteration 147, loss = 0.08044701\n",
      "Iteration 6, loss = 0.50130903\n",
      "Iteration 148, loss = 0.08011823\n",
      "Iteration 7, loss = 0.46723783\n",
      "Iteration 149, loss = 0.07908432\n",
      "Iteration 8, loss = 0.44076923\n",
      "Iteration 150, loss = 0.07797557\n",
      "Iteration 9, loss = 0.42031083\n",
      "Iteration 151, loss = 0.07797630\n",
      "Iteration 10, loss = 0.40170192\n",
      "Iteration 152, loss = 0.07755910\n",
      "Iteration 11, loss = 0.38542487\n",
      "Iteration 153, loss = 0.07839826\n",
      "Iteration 12, loss = 0.37182661\n",
      "Iteration 154, loss = 0.07547990\n",
      "Iteration 13, loss = 0.35975291\n",
      "Iteration 155, loss = 0.07608312\n",
      "Iteration 14, loss = 0.34812455\n",
      "Iteration 156, loss = 0.07861302\n",
      "Iteration 15, loss = 0.33801924\n",
      "Iteration 157, loss = 0.07838455\n",
      "Iteration 16, loss = 0.32861337\n",
      "Iteration 158, loss = 0.07342006\n",
      "Iteration 17, loss = 0.32037155\n",
      "Iteration 159, loss = 0.07393050\n",
      "Iteration 18, loss = 0.31111157\n",
      "Iteration 160, loss = 0.07226521\n",
      "Iteration 19, loss = 0.30748176\n",
      "Iteration 161, loss = 0.07237070\n",
      "Iteration 20, loss = 0.29711383\n",
      "Iteration 162, loss = 0.07227845\n",
      "Iteration 21, loss = 0.29065657\n",
      "Iteration 163, loss = 0.07043513\n",
      "Iteration 22, loss = 0.28612115\n",
      "Iteration 164, loss = 0.06944053\n",
      "Iteration 23, loss = 0.28330509\n",
      "Iteration 165, loss = 0.06906860\n",
      "Iteration 24, loss = 0.27246761\n",
      "Iteration 166, loss = 0.06950189\n",
      "Iteration 25, loss = 0.26920222\n",
      "Iteration 167, loss = 0.07273030\n",
      "Iteration 26, loss = 0.26180454\n",
      "Iteration 168, loss = 0.07096022\n",
      "Iteration 27, loss = 0.25826928\n",
      "Iteration 169, loss = 0.06993343\n",
      "Iteration 28, loss = 0.25230194\n",
      "Iteration 170, loss = 0.06931041\n",
      "Iteration 29, loss = 0.24783721\n",
      "Iteration 171, loss = 0.06915075\n",
      "Iteration 30, loss = 0.24383799\n",
      "Iteration 172, loss = 0.07165498\n",
      "Iteration 31, loss = 0.23773711\n",
      "Iteration 173, loss = 0.06348781\n",
      "Iteration 32, loss = 0.23645301\n",
      "Iteration 174, loss = 0.06507218\n",
      "Iteration 33, loss = 0.22978658\n",
      "Iteration 175, loss = 0.06604494\n",
      "Iteration 34, loss = 0.22781887\n",
      "Iteration 176, loss = 0.06793424\n",
      "Iteration 35, loss = 0.22298987\n",
      "Iteration 177, loss = 0.06221308\n",
      "Iteration 36, loss = 0.21924054\n",
      "Iteration 178, loss = 0.06502568\n",
      "Iteration 37, loss = 0.21448989\n",
      "Iteration 179, loss = 0.06479935\n",
      "Iteration 38, loss = 0.21003580\n",
      "Iteration 180, loss = 0.06765527\n",
      "Iteration 39, loss = 0.20614762\n",
      "Iteration 181, loss = 0.06503083\n",
      "Iteration 40, loss = 0.20436090\n",
      "Iteration 182, loss = 0.06346069\n",
      "Iteration 41, loss = 0.19969025\n",
      "Iteration 183, loss = 0.06266720\n",
      "Iteration 42, loss = 0.19685500\n",
      "Iteration 184, loss = 0.05987250\n",
      "Iteration 43, loss = 0.19348866\n",
      "Iteration 185, loss = 0.06108132\n",
      "Iteration 44, loss = 0.19074252\n",
      "Iteration 186, loss = 0.05995699\n",
      "Iteration 45, loss = 0.18607317\n",
      "Iteration 187, loss = 0.05762605\n",
      "Iteration 46, loss = 0.18292217\n",
      "Iteration 188, loss = 0.05769845\n",
      "Iteration 47, loss = 0.18288475\n",
      "Iteration 189, loss = 0.05838804\n",
      "Iteration 48, loss = 0.17929808\n",
      "Iteration 190, loss = 0.05815828\n",
      "Iteration 49, loss = 0.17545117\n",
      "Iteration 191, loss = 0.05787766\n",
      "Iteration 50, loss = 0.17375214\n",
      "Iteration 192, loss = 0.05495365\n",
      "Iteration 51, loss = 0.16972485\n",
      "Iteration 193, loss = 0.05564177\n",
      "Iteration 52, loss = 0.16611868\n",
      "Iteration 194, loss = 0.05734345\n",
      "Iteration 53, loss = 0.16247190\n",
      "Iteration 195, loss = 0.05613494\n",
      "Iteration 54, loss = 0.16098031\n",
      "Iteration 196, loss = 0.05374127\n",
      "Iteration 55, loss = 0.15665338\n",
      "Iteration 197, loss = 0.05297809\n",
      "Iteration 56, loss = 0.15625764\n",
      "Iteration 198, loss = 0.05329829\n",
      "Iteration 57, loss = 0.15280165\n",
      "Iteration 199, loss = 0.05672558\n",
      "Iteration 58, loss = 0.14835052\n",
      "Iteration 200, loss = 0.05686924\n",
      "Iteration 59, loss = 0.14862958\n",
      "Iteration 201, loss = 0.05397268\n",
      "Iteration 60, loss = 0.14743124\n",
      "Iteration 202, loss = 0.05324250\n",
      "Iteration 61, loss = 0.14416490\n",
      "Iteration 203, loss = 0.05260632\n",
      "Iteration 62, loss = 0.13932850\n",
      "Iteration 204, loss = 0.05150371\n",
      "Iteration 63, loss = 0.13835622\n",
      "Iteration 205, loss = 0.05200400\n",
      "Iteration 64, loss = 0.13619871\n",
      "Iteration 206, loss = 0.05386036\n",
      "Iteration 65, loss = 0.13402018\n",
      "Iteration 207, loss = 0.04943191\n",
      "Iteration 66, loss = 0.13378740\n",
      "Iteration 208, loss = 0.04850617\n",
      "Iteration 67, loss = 0.13083198\n",
      "Iteration 209, loss = 0.04793735\n",
      "Iteration 68, loss = 0.12912000\n",
      "Iteration 210, loss = 0.05013596\n",
      "Iteration 69, loss = 0.12752891\n",
      "Iteration 211, loss = 0.04952600\n",
      "Iteration 70, loss = 0.12496234\n",
      "Iteration 212, loss = 0.05183054\n",
      "Iteration 71, loss = 0.12638794\n",
      "Iteration 213, loss = 0.04835792\n",
      "Iteration 72, loss = 0.12388890\n",
      "Iteration 214, loss = 0.05137654\n",
      "Iteration 73, loss = 0.11844486\n",
      "Iteration 215, loss = 0.04834475\n",
      "Iteration 74, loss = 0.11792544\n",
      "Iteration 216, loss = 0.04962803\n",
      "Iteration 75, loss = 0.11902171\n",
      "Iteration 217, loss = 0.04682459\n",
      "Iteration 76, loss = 0.11750781\n",
      "Iteration 218, loss = 0.04946416\n",
      "Iteration 77, loss = 0.11206841\n",
      "Iteration 219, loss = 0.04740705\n",
      "Iteration 78, loss = 0.11005128\n",
      "Iteration 220, loss = 0.04395357\n",
      "Iteration 79, loss = 0.11008640\n",
      "Iteration 221, loss = 0.04578573\n",
      "Iteration 80, loss = 0.10718414\n",
      "Iteration 222, loss = 0.04488887\n",
      "Iteration 81, loss = 0.10709358\n",
      "Iteration 223, loss = 0.04486251\n",
      "Iteration 82, loss = 0.10425423\n",
      "Iteration 224, loss = 0.04342091\n",
      "Iteration 83, loss = 0.10210543\n",
      "Iteration 225, loss = 0.04419710\n",
      "Iteration 84, loss = 0.10243394\n",
      "Iteration 226, loss = 0.04448216\n",
      "Iteration 85, loss = 0.10209646\n",
      "Iteration 227, loss = 0.04288020\n",
      "Iteration 86, loss = 0.09953548\n",
      "Iteration 228, loss = 0.04275964\n",
      "Iteration 87, loss = 0.09690037\n",
      "Iteration 229, loss = 0.04445720\n",
      "Iteration 88, loss = 0.09497297\n",
      "Iteration 230, loss = 0.04509776\n",
      "Iteration 89, loss = 0.09710661\n",
      "Iteration 231, loss = 0.04369203\n",
      "Iteration 90, loss = 0.09534077\n",
      "Iteration 232, loss = 0.04171163\n",
      "Iteration 91, loss = 0.09351780\n",
      "Iteration 233, loss = 0.03983755\n",
      "Iteration 92, loss = 0.08880503\n",
      "Iteration 234, loss = 0.04036696\n",
      "Iteration 93, loss = 0.09146158\n",
      "Iteration 235, loss = 0.04009383\n",
      "Iteration 94, loss = 0.09021453\n",
      "Iteration 236, loss = 0.03973125\n",
      "Iteration 95, loss = 0.08829553\n",
      "Iteration 237, loss = 0.04296560\n",
      "Iteration 96, loss = 0.08622771\n",
      "Iteration 238, loss = 0.04184034\n",
      "Iteration 97, loss = 0.08703245\n",
      "Iteration 239, loss = 0.03940323\n",
      "Iteration 98, loss = 0.08760458\n",
      "Iteration 240, loss = 0.03843130\n",
      "Iteration 99, loss = 0.08589922\n",
      "Iteration 241, loss = 0.03910563\n",
      "Iteration 100, loss = 0.08567006\n",
      "Iteration 242, loss = 0.03805474\n",
      "Iteration 101, loss = 0.08316613\n",
      "Iteration 243, loss = 0.03996511\n",
      "Iteration 102, loss = 0.08087597\n",
      "Iteration 244, loss = 0.03886247\n",
      "Iteration 103, loss = 0.08052207\n",
      "Iteration 245, loss = 0.04086836\n",
      "Iteration 104, loss = 0.07768824\n",
      "Iteration 246, loss = 0.03755067\n",
      "Iteration 105, loss = 0.07889530\n",
      "Iteration 247, loss = 0.03735829\n",
      "Iteration 106, loss = 0.07637064\n",
      "Iteration 248, loss = 0.03846004\n",
      "Iteration 107, loss = 0.07565267\n",
      "Iteration 249, loss = 0.03919749\n",
      "Iteration 108, loss = 0.07522230\n",
      "Iteration 250, loss = 0.03719878\n",
      "Iteration 109, loss = 0.07323882\n",
      "Iteration 251, loss = 0.03645796\n",
      "Iteration 110, loss = 0.07198234\n",
      "Iteration 252, loss = 0.03466175\n",
      "Iteration 111, loss = 0.07453415\n",
      "Iteration 253, loss = 0.03872016\n",
      "Iteration 112, loss = 0.07191045\n",
      "Iteration 254, loss = 0.03981356\n",
      "Iteration 113, loss = 0.07094065\n",
      "Iteration 255, loss = 0.03819315\n",
      "Iteration 114, loss = 0.06933553\n",
      "Iteration 256, loss = 0.03793084\n",
      "Iteration 115, loss = 0.06801543\n",
      "Iteration 116, loss = 0.06730328\n",
      "Iteration 257, loss = 0.03490951\n",
      "Iteration 117, loss = 0.06930349\n",
      "Iteration 258, loss = 0.03393663\n",
      "Iteration 118, loss = 0.06729357\n",
      "Iteration 259, loss = 0.03310769\n",
      "Iteration 119, loss = 0.06714345\n",
      "Iteration 260, loss = 0.03289256\n",
      "Iteration 120, loss = 0.06512694\n",
      "Iteration 261, loss = 0.03310273\n",
      "Iteration 121, loss = 0.06379262\n",
      "Iteration 262, loss = 0.03261800\n",
      "Iteration 122, loss = 0.06464191\n",
      "Iteration 263, loss = 0.03428121\n",
      "Iteration 123, loss = 0.06285661\n",
      "Iteration 264, loss = 0.03276563\n",
      "Iteration 124, loss = 0.06247193\n",
      "Iteration 265, loss = 0.03286471\n",
      "Iteration 125, loss = 0.06110561\n",
      "Iteration 266, loss = 0.03619046\n",
      "Iteration 126, loss = 0.05930247\n",
      "Iteration 267, loss = 0.03677738\n",
      "Iteration 127, loss = 0.06042532\n",
      "Iteration 268, loss = 0.03626032\n",
      "Iteration 128, loss = 0.05850060\n",
      "Iteration 269, loss = 0.03371607\n",
      "Iteration 129, loss = 0.05708014\n",
      "Iteration 270, loss = 0.03337825\n",
      "Iteration 130, loss = 0.05775609\n",
      "Iteration 271, loss = 0.03373337\n",
      "Iteration 131, loss = 0.05585578\n",
      "Iteration 272, loss = 0.03233778\n",
      "Iteration 132, loss = 0.05673602\n",
      "Iteration 273, loss = 0.03002747\n",
      "Iteration 133, loss = 0.05492498\n",
      "Iteration 274, loss = 0.03151340\n",
      "Iteration 134, loss = 0.05443163\n",
      "Iteration 275, loss = 0.02996605\n",
      "Iteration 135, loss = 0.05327927\n",
      "Iteration 276, loss = 0.02978695\n",
      "Iteration 136, loss = 0.05370373\n",
      "Iteration 277, loss = 0.03040839\n",
      "Iteration 137, loss = 0.05348459\n",
      "Iteration 278, loss = 0.02977499\n",
      "Iteration 138, loss = 0.05219684\n",
      "Iteration 279, loss = 0.03044398\n",
      "Iteration 139, loss = 0.05106234\n",
      "Iteration 280, loss = 0.03065592\n",
      "Iteration 140, loss = 0.05141216\n",
      "Iteration 281, loss = 0.02929317\n",
      "Iteration 141, loss = 0.05244807\n",
      "Iteration 282, loss = 0.02892771\n",
      "Iteration 142, loss = 0.05200267\n",
      "Iteration 283, loss = 0.03112384\n",
      "Iteration 143, loss = 0.05014744\n",
      "Iteration 284, loss = 0.03049579\n",
      "Iteration 144, loss = 0.04974597\n",
      "Iteration 285, loss = 0.02934223\n",
      "Iteration 145, loss = 0.04768514\n",
      "Iteration 146, loss = 0.04861636\n",
      "Iteration 286, loss = 0.03080594\n",
      "Iteration 147, loss = 0.04624356\n",
      "Iteration 287, loss = 0.03024467\n",
      "Iteration 148, loss = 0.04734503\n",
      "Iteration 288, loss = 0.02817092\n",
      "Iteration 149, loss = 0.04565230\n",
      "Iteration 289, loss = 0.02867209\n",
      "Iteration 150, loss = 0.04525098\n",
      "Iteration 290, loss = 0.02952580\n",
      "Iteration 291, loss = 0.03333513\n",
      "Iteration 151, loss = 0.04392322\n",
      "Iteration 292, loss = 0.03238597\n",
      "Iteration 152, loss = 0.04377314\n",
      "Iteration 153, loss = 0.04448416\n",
      "Iteration 293, loss = 0.03982913\n",
      "Iteration 154, loss = 0.04328775\n",
      "Iteration 294, loss = 0.04121158\n",
      "Iteration 155, loss = 0.04280845\n",
      "Iteration 295, loss = 0.04310605\n",
      "Iteration 156, loss = 0.04254378\n",
      "Iteration 296, loss = 0.03190453\n",
      "Iteration 297, loss = 0.02657279\n",
      "Iteration 157, loss = 0.04248047\n",
      "Iteration 158, loss = 0.04075106\n",
      "Iteration 298, loss = 0.02585973\n",
      "Iteration 159, loss = 0.04042857\n",
      "Iteration 299, loss = 0.02606334\n",
      "Iteration 160, loss = 0.03987377\n",
      "Iteration 300, loss = 0.02778269\n",
      "Iteration 161, loss = 0.04022450\n",
      "Iteration 301, loss = 0.02654632\n",
      "Iteration 162, loss = 0.04047778\n",
      "Iteration 302, loss = 0.02997003\n",
      "Iteration 163, loss = 0.04144239\n",
      "Iteration 303, loss = 0.02593375\n",
      "Iteration 304, loss = 0.02722278\n",
      "Iteration 164, loss = 0.04333819\n",
      "Iteration 165, loss = 0.04228533\n",
      "Iteration 305, loss = 0.02662329\n",
      "Iteration 166, loss = 0.03765700\n",
      "Iteration 306, loss = 0.02808939\n",
      "Iteration 167, loss = 0.03990213\n",
      "Iteration 307, loss = 0.02690690\n",
      "Iteration 168, loss = 0.03605953\n",
      "Iteration 308, loss = 0.02568184\n",
      "Iteration 169, loss = 0.03723477\n",
      "Iteration 309, loss = 0.02445122\n",
      "Iteration 170, loss = 0.03725872\n",
      "Iteration 310, loss = 0.02304813\n",
      "Iteration 171, loss = 0.03604300\n",
      "Iteration 311, loss = 0.02421048\n",
      "Iteration 172, loss = 0.03585594\n",
      "Iteration 312, loss = 0.02486722\n",
      "Iteration 173, loss = 0.03403279\n",
      "Iteration 313, loss = 0.02433309\n",
      "Iteration 174, loss = 0.03456919\n",
      "Iteration 314, loss = 0.02282032\n",
      "Iteration 175, loss = 0.03426725\n",
      "Iteration 315, loss = 0.02344437\n",
      "Iteration 176, loss = 0.03389451\n",
      "Iteration 316, loss = 0.02511455\n",
      "Iteration 177, loss = 0.03316909\n",
      "Iteration 317, loss = 0.02546476\n",
      "Iteration 178, loss = 0.03248574\n",
      "Iteration 318, loss = 0.02311481\n",
      "Iteration 179, loss = 0.03217371\n",
      "Iteration 319, loss = 0.02379050\n",
      "Iteration 180, loss = 0.03170109\n",
      "Iteration 320, loss = 0.02279797\n",
      "Iteration 181, loss = 0.03221199\n",
      "Iteration 321, loss = 0.02531808\n",
      "Iteration 182, loss = 0.03086140\n",
      "Iteration 322, loss = 0.02515437\n",
      "Iteration 183, loss = 0.03005695\n",
      "Iteration 323, loss = 0.02148564\n",
      "Iteration 184, loss = 0.03265263Iteration 324, loss = 0.02147057\n",
      "\n",
      "Iteration 185, loss = 0.02943551\n",
      "Iteration 325, loss = 0.02105127\n",
      "Iteration 186, loss = 0.03002487\n",
      "Iteration 326, loss = 0.02255034\n",
      "Iteration 187, loss = 0.02971363\n",
      "Iteration 327, loss = 0.02339090\n",
      "Iteration 188, loss = 0.02924609\n",
      "Iteration 328, loss = 0.02064142\n",
      "Iteration 189, loss = 0.03079412\n",
      "Iteration 329, loss = 0.02241429\n",
      "Iteration 190, loss = 0.02791992\n",
      "Iteration 330, loss = 0.02159813\n",
      "Iteration 191, loss = 0.02912564\n",
      "Iteration 331, loss = 0.02080006\n",
      "Iteration 192, loss = 0.02793246\n",
      "Iteration 332, loss = 0.02075877\n",
      "Iteration 193, loss = 0.02774623\n",
      "Iteration 333, loss = 0.02123549\n",
      "Iteration 334, loss = 0.02050897\n",
      "Iteration 194, loss = 0.02683963\n",
      "Iteration 335, loss = 0.01967060\n",
      "Iteration 195, loss = 0.02957411\n",
      "Iteration 196, loss = 0.03201157\n",
      "Iteration 336, loss = 0.02013415\n",
      "Iteration 197, loss = 0.03549708\n",
      "Iteration 337, loss = 0.01928234\n",
      "Iteration 198, loss = 0.03076387\n",
      "Iteration 338, loss = 0.01948708\n",
      "Iteration 199, loss = 0.03165828\n",
      "Iteration 339, loss = 0.01948454\n",
      "Iteration 200, loss = 0.02817749\n",
      "Iteration 340, loss = 0.01928939\n",
      "Iteration 201, loss = 0.02749942\n",
      "Iteration 341, loss = 0.01972107\n",
      "Iteration 202, loss = 0.02597255\n",
      "Iteration 342, loss = 0.01950532\n",
      "Iteration 203, loss = 0.02481053\n",
      "Iteration 343, loss = 0.01988310\n",
      "Iteration 204, loss = 0.02412926\n",
      "Iteration 344, loss = 0.02197648\n",
      "Iteration 205, loss = 0.02335707\n",
      "Iteration 345, loss = 0.02164013\n",
      "Iteration 206, loss = 0.02393126\n",
      "Iteration 346, loss = 0.01984234\n",
      "Iteration 207, loss = 0.02420020\n",
      "Iteration 347, loss = 0.02019420\n",
      "Iteration 208, loss = 0.02328049\n",
      "Iteration 348, loss = 0.01852818\n",
      "Iteration 349, loss = 0.01813093\n",
      "Iteration 209, loss = 0.02257071\n",
      "Iteration 350, loss = 0.02014200\n",
      "Iteration 210, loss = 0.02440293\n",
      "Iteration 351, loss = 0.01902317\n",
      "Iteration 211, loss = 0.02383528\n",
      "Iteration 352, loss = 0.02108686\n",
      "Iteration 212, loss = 0.02170169\n",
      "Iteration 213, loss = 0.02223633\n",
      "Iteration 353, loss = 0.02086128\n",
      "Iteration 214, loss = 0.02136647\n",
      "Iteration 354, loss = 0.01970460\n",
      "Iteration 215, loss = 0.02095674\n",
      "Iteration 355, loss = 0.01792556\n",
      "Iteration 216, loss = 0.02092829\n",
      "Iteration 356, loss = 0.02012146\n",
      "Iteration 357, loss = 0.02097239\n",
      "Iteration 217, loss = 0.02275887\n",
      "Iteration 358, loss = 0.02073255\n",
      "Iteration 218, loss = 0.02133591\n",
      "Iteration 359, loss = 0.02021262\n",
      "Iteration 219, loss = 0.02027841\n",
      "Iteration 360, loss = 0.02534902\n",
      "Iteration 220, loss = 0.02027794\n",
      "Iteration 361, loss = 0.01831191\n",
      "Iteration 221, loss = 0.01946361\n",
      "Iteration 362, loss = 0.01722512\n",
      "Iteration 222, loss = 0.01982329\n",
      "Iteration 363, loss = 0.01727218\n",
      "Iteration 223, loss = 0.01969452\n",
      "Iteration 364, loss = 0.01733393\n",
      "Iteration 224, loss = 0.01976996\n",
      "Iteration 365, loss = 0.01748362\n",
      "Iteration 225, loss = 0.01896103\n",
      "Iteration 366, loss = 0.02067035\n",
      "Iteration 226, loss = 0.01884853\n",
      "Iteration 367, loss = 0.02263093\n",
      "Iteration 227, loss = 0.01858523\n",
      "Iteration 368, loss = 0.02744280\n",
      "Iteration 228, loss = 0.01844671\n",
      "Iteration 369, loss = 0.02665498\n",
      "Iteration 229, loss = 0.01784160\n",
      "Iteration 370, loss = 0.02100126\n",
      "Iteration 230, loss = 0.01806176\n",
      "Iteration 371, loss = 0.01732286\n",
      "Iteration 231, loss = 0.01821813\n",
      "Iteration 372, loss = 0.01675667\n",
      "Iteration 232, loss = 0.01808882\n",
      "Iteration 373, loss = 0.01826058\n",
      "Iteration 233, loss = 0.01758236\n",
      "Iteration 374, loss = 0.02389480\n",
      "Iteration 234, loss = 0.01734690\n",
      "Iteration 375, loss = 0.01695000\n",
      "Iteration 235, loss = 0.01714289\n",
      "Iteration 376, loss = 0.01636243\n",
      "Iteration 236, loss = 0.01689267\n",
      "Iteration 377, loss = 0.01642302\n",
      "Iteration 237, loss = 0.01695546\n",
      "Iteration 378, loss = 0.01591615\n",
      "Iteration 238, loss = 0.01632101\n",
      "Iteration 379, loss = 0.01638341\n",
      "Iteration 239, loss = 0.01620823\n",
      "Iteration 380, loss = 0.01586987\n",
      "Iteration 240, loss = 0.01620554\n",
      "Iteration 241, loss = 0.01567787\n",
      "Iteration 381, loss = 0.01513677\n",
      "Iteration 242, loss = 0.01714237\n",
      "Iteration 382, loss = 0.01458628\n",
      "Iteration 243, loss = 0.01813732\n",
      "Iteration 383, loss = 0.01571204\n",
      "Iteration 384, loss = 0.01569427\n",
      "Iteration 244, loss = 0.01735565\n",
      "Iteration 385, loss = 0.01459061\n",
      "Iteration 245, loss = 0.01658652\n",
      "Iteration 246, loss = 0.01583085\n",
      "Iteration 386, loss = 0.01483845\n",
      "Iteration 247, loss = 0.01523507\n",
      "Iteration 387, loss = 0.01571974\n",
      "Iteration 248, loss = 0.01521899\n",
      "Iteration 388, loss = 0.01501268\n",
      "Iteration 389, loss = 0.01411993\n",
      "Iteration 249, loss = 0.01461621\n",
      "Iteration 390, loss = 0.01411350\n",
      "Iteration 250, loss = 0.01431367\n",
      "Iteration 391, loss = 0.01396431\n",
      "Iteration 251, loss = 0.01485994\n",
      "Iteration 392, loss = 0.01486628\n",
      "Iteration 252, loss = 0.01404285\n",
      "Iteration 393, loss = 0.01393627\n",
      "Iteration 253, loss = 0.01397793\n",
      "Iteration 254, loss = 0.01414530\n",
      "Iteration 394, loss = 0.01382455\n",
      "Iteration 255, loss = 0.01417976\n",
      "Iteration 395, loss = 0.01350902\n",
      "Iteration 256, loss = 0.01388749\n",
      "Iteration 396, loss = 0.01364112\n",
      "Iteration 257, loss = 0.01351536\n",
      "Iteration 397, loss = 0.01362365\n",
      "Iteration 258, loss = 0.01448968\n",
      "Iteration 398, loss = 0.01337891\n",
      "Iteration 259, loss = 0.01357922\n",
      "Iteration 399, loss = 0.01442862\n",
      "Iteration 260, loss = 0.01288172\n",
      "Iteration 400, loss = 0.01426024\n",
      "Iteration 261, loss = 0.01304638\n",
      "Iteration 401, loss = 0.01313049\n",
      "Iteration 262, loss = 0.01324722\n",
      "Iteration 402, loss = 0.01351481\n",
      "Iteration 263, loss = 0.01303847\n",
      "Iteration 403, loss = 0.01370475\n",
      "Iteration 264, loss = 0.01267954\n",
      "Iteration 404, loss = 0.01354611\n",
      "Iteration 265, loss = 0.01272597\n",
      "Iteration 405, loss = 0.01374025\n",
      "Iteration 406, loss = 0.01310390\n",
      "Iteration 266, loss = 0.01209637\n",
      "Iteration 407, loss = 0.01250245\n",
      "Iteration 267, loss = 0.01217849\n",
      "Iteration 408, loss = 0.01368015\n",
      "Iteration 268, loss = 0.01194254\n",
      "Iteration 409, loss = 0.01289316\n",
      "Iteration 269, loss = 0.01269236\n",
      "Iteration 410, loss = 0.01255913\n",
      "Iteration 270, loss = 0.01376156\n",
      "Iteration 411, loss = 0.01229369\n",
      "Iteration 271, loss = 0.01241354\n",
      "Iteration 412, loss = 0.01246641\n",
      "Iteration 272, loss = 0.01119596\n",
      "Iteration 413, loss = 0.01203704\n",
      "Iteration 273, loss = 0.01170924\n",
      "Iteration 414, loss = 0.01233943\n",
      "Iteration 274, loss = 0.01133849\n",
      "Iteration 415, loss = 0.01244841\n",
      "Iteration 275, loss = 0.01091468\n",
      "Iteration 416, loss = 0.01195244\n",
      "Iteration 276, loss = 0.01123798\n",
      "Iteration 417, loss = 0.01229698\n",
      "Iteration 277, loss = 0.01107543\n",
      "Iteration 418, loss = 0.01418796\n",
      "Iteration 278, loss = 0.01079795\n",
      "Iteration 419, loss = 0.01592290\n",
      "Iteration 279, loss = 0.01081757\n",
      "Iteration 420, loss = 0.01280046\n",
      "Iteration 280, loss = 0.01094308\n",
      "Iteration 421, loss = 0.01213759\n",
      "Iteration 281, loss = 0.01063103\n",
      "Iteration 422, loss = 0.01263146\n",
      "Iteration 282, loss = 0.01033761\n",
      "Iteration 423, loss = 0.01165535\n",
      "Iteration 283, loss = 0.01068394\n",
      "Iteration 424, loss = 0.01147138\n",
      "Iteration 284, loss = 0.01050319\n",
      "Iteration 425, loss = 0.01183097\n",
      "Iteration 285, loss = 0.01105833\n",
      "Iteration 286, loss = 0.01026980\n",
      "Iteration 426, loss = 0.01312457\n",
      "Iteration 427, loss = 0.01212414\n",
      "Iteration 287, loss = 0.01065247\n",
      "Iteration 428, loss = 0.01132817\n",
      "Iteration 288, loss = 0.00991921\n",
      "Iteration 429, loss = 0.01320556\n",
      "Iteration 289, loss = 0.01002280\n",
      "Iteration 430, loss = 0.01466658\n",
      "Iteration 290, loss = 0.01012089\n",
      "Iteration 431, loss = 0.01222418\n",
      "Iteration 291, loss = 0.01128533\n",
      "Iteration 432, loss = 0.01238126\n",
      "Iteration 292, loss = 0.00939910\n",
      "Iteration 433, loss = 0.01322236\n",
      "Iteration 293, loss = 0.00979595\n",
      "Iteration 434, loss = 0.01109792\n",
      "Iteration 294, loss = 0.00957094\n",
      "Iteration 295, loss = 0.00955711\n",
      "Iteration 435, loss = 0.01337751\n",
      "Iteration 296, loss = 0.00937823\n",
      "Iteration 436, loss = 0.01407557\n",
      "Iteration 297, loss = 0.00944460\n",
      "Iteration 437, loss = 0.01316154\n",
      "Iteration 298, loss = 0.00893007\n",
      "Iteration 438, loss = 0.01212158\n",
      "Iteration 299, loss = 0.00934425\n",
      "Iteration 439, loss = 0.01070397\n",
      "Iteration 300, loss = 0.00969850\n",
      "Iteration 440, loss = 0.01081430\n",
      "Iteration 301, loss = 0.00969846\n",
      "Iteration 441, loss = 0.01024821\n",
      "Iteration 302, loss = 0.00909976\n",
      "Iteration 442, loss = 0.01162042\n",
      "Iteration 303, loss = 0.01012669\n",
      "Iteration 443, loss = 0.01074093\n",
      "Iteration 304, loss = 0.01068883\n",
      "Iteration 444, loss = 0.00996950\n",
      "Iteration 305, loss = 0.01242825\n",
      "Iteration 445, loss = 0.01074528\n",
      "Iteration 306, loss = 0.01042279\n",
      "Iteration 446, loss = 0.01090303\n",
      "Iteration 307, loss = 0.00822649\n",
      "Iteration 447, loss = 0.01142777\n",
      "Iteration 308, loss = 0.00902769\n",
      "Iteration 448, loss = 0.01015190\n",
      "Iteration 309, loss = 0.00875008\n",
      "Iteration 449, loss = 0.00999028\n",
      "Iteration 310, loss = 0.00842817\n",
      "Iteration 450, loss = 0.01013665\n",
      "Iteration 311, loss = 0.00794842\n",
      "Iteration 451, loss = 0.01044488\n",
      "Iteration 312, loss = 0.00787515\n",
      "Iteration 452, loss = 0.01074118\n",
      "Iteration 313, loss = 0.00771262\n",
      "Iteration 453, loss = 0.00952381\n",
      "Iteration 314, loss = 0.00783107\n",
      "Iteration 454, loss = 0.00928881\n",
      "Iteration 315, loss = 0.00777354\n",
      "Iteration 455, loss = 0.01008972\n",
      "Iteration 316, loss = 0.00813450\n",
      "Iteration 456, loss = 0.00946663\n",
      "Iteration 317, loss = 0.00724746\n",
      "Iteration 457, loss = 0.00988176\n",
      "Iteration 318, loss = 0.00791476\n",
      "Iteration 458, loss = 0.00998013\n",
      "Iteration 319, loss = 0.00748074\n",
      "Iteration 459, loss = 0.01047415\n",
      "Iteration 320, loss = 0.00758067\n",
      "Iteration 460, loss = 0.01045313\n",
      "Iteration 321, loss = 0.00729936\n",
      "Iteration 461, loss = 0.01085293\n",
      "Iteration 322, loss = 0.00748208\n",
      "Iteration 462, loss = 0.01049941\n",
      "Iteration 323, loss = 0.00703414\n",
      "Iteration 463, loss = 0.00906999\n",
      "Iteration 324, loss = 0.00724682\n",
      "Iteration 464, loss = 0.00907238\n",
      "Iteration 325, loss = 0.00698738\n",
      "Iteration 465, loss = 0.00915262\n",
      "Iteration 326, loss = 0.00748101\n",
      "Iteration 466, loss = 0.00986438\n",
      "Iteration 327, loss = 0.00692616\n",
      "Iteration 467, loss = 0.00882410\n",
      "Iteration 328, loss = 0.00663664\n",
      "Iteration 468, loss = 0.00920452\n",
      "Iteration 329, loss = 0.00679638\n",
      "Iteration 469, loss = 0.00884322\n",
      "Iteration 330, loss = 0.00706606\n",
      "Iteration 470, loss = 0.00918947\n",
      "Iteration 331, loss = 0.00694235\n",
      "Iteration 471, loss = 0.00864140\n",
      "Iteration 332, loss = 0.00677830\n",
      "Iteration 472, loss = 0.00864228\n",
      "Iteration 333, loss = 0.00637556\n",
      "Iteration 473, loss = 0.00899278\n",
      "Iteration 334, loss = 0.00671012\n",
      "Iteration 474, loss = 0.00918616\n",
      "Iteration 335, loss = 0.00653032\n",
      "Iteration 475, loss = 0.00835371\n",
      "Iteration 336, loss = 0.00677695\n",
      "Iteration 476, loss = 0.00829498\n",
      "Iteration 337, loss = 0.00667912\n",
      "Iteration 477, loss = 0.00823559\n",
      "Iteration 338, loss = 0.00643146\n",
      "Iteration 478, loss = 0.00913593\n",
      "Iteration 339, loss = 0.00607389\n",
      "Iteration 479, loss = 0.00921728\n",
      "Iteration 340, loss = 0.00610305\n",
      "Iteration 480, loss = 0.00934152\n",
      "Iteration 341, loss = 0.00617005\n",
      "Iteration 481, loss = 0.00905151\n",
      "Iteration 342, loss = 0.00616383\n",
      "Iteration 482, loss = 0.00811914\n",
      "Iteration 343, loss = 0.00609406\n",
      "Iteration 483, loss = 0.00893120\n",
      "Iteration 344, loss = 0.00586008\n",
      "Iteration 484, loss = 0.00870587\n",
      "Iteration 345, loss = 0.00593512\n",
      "Iteration 485, loss = 0.00866793\n",
      "Iteration 346, loss = 0.00587818\n",
      "Iteration 486, loss = 0.00836188\n",
      "Iteration 347, loss = 0.00573719\n",
      "Iteration 487, loss = 0.00793874\n",
      "Iteration 348, loss = 0.00566248\n",
      "Iteration 488, loss = 0.00918582\n",
      "Iteration 349, loss = 0.00592826\n",
      "Iteration 489, loss = 0.00845953\n",
      "Iteration 350, loss = 0.00563541\n",
      "Iteration 490, loss = 0.00905120\n",
      "Iteration 351, loss = 0.00588135\n",
      "Iteration 491, loss = 0.00899612\n",
      "Iteration 352, loss = 0.00605548\n",
      "Iteration 492, loss = 0.00862986\n",
      "Iteration 353, loss = 0.00564915\n",
      "Iteration 493, loss = 0.00836966\n",
      "Iteration 354, loss = 0.00539004\n",
      "Iteration 494, loss = 0.00883939\n",
      "Iteration 355, loss = 0.00553273\n",
      "Iteration 495, loss = 0.00875787\n",
      "Iteration 356, loss = 0.00536010\n",
      "Iteration 496, loss = 0.00891840\n",
      "Iteration 357, loss = 0.00532545\n",
      "Iteration 497, loss = 0.01080005\n",
      "Iteration 358, loss = 0.00528553\n",
      "Iteration 498, loss = 0.00973358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 359, loss = 0.00529343\n",
      "Iteration 360, loss = 0.00533417\n",
      "Iteration 361, loss = 0.00502240\n",
      "Iteration 362, loss = 0.00502572\n",
      "Iteration 363, loss = 0.00510551\n",
      "Iteration 364, loss = 0.00487479\n",
      "Iteration 365, loss = 0.00483364\n",
      "Iteration 366, loss = 0.00487923\n",
      "Iteration 367, loss = 0.00481560\n",
      "Iteration 368, loss = 0.00473195\n",
      "Iteration 369, loss = 0.00478935\n",
      "Iteration 370, loss = 0.00465990\n",
      "Iteration 371, loss = 0.00461187\n",
      "Iteration 1, loss = 1.17377055\n",
      "Iteration 372, loss = 0.00456654\n",
      "Iteration 2, loss = 0.92476133\n",
      "Iteration 373, loss = 0.00468282\n",
      "Iteration 3, loss = 0.77481563\n",
      "Iteration 374, loss = 0.00456519\n",
      "Iteration 4, loss = 0.67214027\n",
      "Iteration 375, loss = 0.00449624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.60421994\n",
      "Iteration 6, loss = 0.54970175\n",
      "Iteration 7, loss = 0.50881952\n",
      "Iteration 8, loss = 0.47586763\n",
      "Iteration 9, loss = 0.44835671\n",
      "Iteration 10, loss = 0.42592982\n",
      "Iteration 11, loss = 0.40532640\n",
      "Iteration 12, loss = 0.39069603\n",
      "Iteration 13, loss = 0.37497529\n",
      "Iteration 14, loss = 0.36223874\n",
      "Iteration 15, loss = 0.35020621\n",
      "Iteration 16, loss = 0.34036400\n",
      "Iteration 17, loss = 0.32914051\n",
      "Iteration 1, loss = 1.44469205\n",
      "Iteration 18, loss = 0.32222377\n",
      "Iteration 2, loss = 0.96085288\n",
      "Iteration 19, loss = 0.31185580\n",
      "Iteration 3, loss = 0.78884302\n",
      "Iteration 20, loss = 0.30567596\n",
      "Iteration 4, loss = 0.66896549\n",
      "Iteration 21, loss = 0.29765519\n",
      "Iteration 5, loss = 0.59093380\n",
      "Iteration 22, loss = 0.29079813\n",
      "Iteration 6, loss = 0.54944415\n",
      "Iteration 23, loss = 0.28367553\n",
      "Iteration 7, loss = 0.51313657\n",
      "Iteration 24, loss = 0.27669068\n",
      "Iteration 8, loss = 0.48339555\n",
      "Iteration 25, loss = 0.27141239\n",
      "Iteration 9, loss = 0.45994090\n",
      "Iteration 26, loss = 0.26538252\n",
      "Iteration 10, loss = 0.43922438\n",
      "Iteration 27, loss = 0.26031266\n",
      "Iteration 11, loss = 0.42106089\n",
      "Iteration 28, loss = 0.25464437\n",
      "Iteration 12, loss = 0.40526226\n",
      "Iteration 29, loss = 0.25067538\n",
      "Iteration 13, loss = 0.39226338\n",
      "Iteration 30, loss = 0.24640668\n",
      "Iteration 14, loss = 0.37926598\n",
      "Iteration 31, loss = 0.24052563\n",
      "Iteration 15, loss = 0.36863960\n",
      "Iteration 32, loss = 0.23894327\n",
      "Iteration 16, loss = 0.35839991\n",
      "Iteration 33, loss = 0.23337774\n",
      "Iteration 17, loss = 0.35024224\n",
      "Iteration 34, loss = 0.22980194\n",
      "Iteration 18, loss = 0.34109961\n",
      "Iteration 35, loss = 0.22446462\n",
      "Iteration 19, loss = 0.33279476\n",
      "Iteration 36, loss = 0.22314470\n",
      "Iteration 20, loss = 0.32714734\n",
      "Iteration 21, loss = 0.32032768\n",
      "Iteration 37, loss = 0.21704873\n",
      "Iteration 22, loss = 0.31243282\n",
      "Iteration 38, loss = 0.21505789\n",
      "Iteration 23, loss = 0.30680200\n",
      "Iteration 39, loss = 0.21244636\n",
      "Iteration 24, loss = 0.29999912\n",
      "Iteration 40, loss = 0.20611161\n",
      "Iteration 25, loss = 0.29479890\n",
      "Iteration 41, loss = 0.20544034\n",
      "Iteration 26, loss = 0.28989424\n",
      "Iteration 42, loss = 0.19973110\n",
      "Iteration 27, loss = 0.28381922\n",
      "Iteration 43, loss = 0.19775245\n",
      "Iteration 28, loss = 0.28027876\n",
      "Iteration 44, loss = 0.19263896\n",
      "Iteration 29, loss = 0.27494134\n",
      "Iteration 45, loss = 0.19274490\n",
      "Iteration 30, loss = 0.27071627\n",
      "Iteration 46, loss = 0.18986060\n",
      "Iteration 31, loss = 0.26665528\n",
      "Iteration 47, loss = 0.18635181\n",
      "Iteration 32, loss = 0.26282693\n",
      "Iteration 48, loss = 0.18343355\n",
      "Iteration 33, loss = 0.25912720\n",
      "Iteration 49, loss = 0.18026987\n",
      "Iteration 34, loss = 0.25341986\n",
      "Iteration 50, loss = 0.17690701\n",
      "Iteration 35, loss = 0.25201150\n",
      "Iteration 51, loss = 0.17523045\n",
      "Iteration 52, loss = 0.17532442\n",
      "Iteration 36, loss = 0.24957440\n",
      "Iteration 53, loss = 0.17095385\n",
      "Iteration 37, loss = 0.24519929\n",
      "Iteration 54, loss = 0.16842396\n",
      "Iteration 38, loss = 0.24046983\n",
      "Iteration 39, loss = 0.23646485\n",
      "Iteration 55, loss = 0.16961897\n",
      "Iteration 40, loss = 0.23377956\n",
      "Iteration 56, loss = 0.16141029\n",
      "Iteration 41, loss = 0.23051423\n",
      "Iteration 57, loss = 0.16398397\n",
      "Iteration 58, loss = 0.15901270\n",
      "Iteration 42, loss = 0.22765107\n",
      "Iteration 59, loss = 0.15753462\n",
      "Iteration 43, loss = 0.22567217\n",
      "Iteration 60, loss = 0.15673414\n",
      "Iteration 44, loss = 0.22364798\n",
      "Iteration 61, loss = 0.15190834\n",
      "Iteration 45, loss = 0.21922075\n",
      "Iteration 62, loss = 0.15305027\n",
      "Iteration 46, loss = 0.21532671\n",
      "Iteration 63, loss = 0.14966727\n",
      "Iteration 47, loss = 0.21323895\n",
      "Iteration 64, loss = 0.14830250\n",
      "Iteration 48, loss = 0.20997317\n",
      "Iteration 65, loss = 0.14853330\n",
      "Iteration 49, loss = 0.20664482\n",
      "Iteration 66, loss = 0.14373872\n",
      "Iteration 50, loss = 0.20463219\n",
      "Iteration 67, loss = 0.14387547\n",
      "Iteration 51, loss = 0.20224902\n",
      "Iteration 68, loss = 0.13960348\n",
      "Iteration 52, loss = 0.19898500\n",
      "Iteration 53, loss = 0.19752498\n",
      "Iteration 69, loss = 0.13932053\n",
      "Iteration 54, loss = 0.19346778\n",
      "Iteration 70, loss = 0.13694524\n",
      "Iteration 55, loss = 0.19128327\n",
      "Iteration 71, loss = 0.13671790\n",
      "Iteration 56, loss = 0.18891639\n",
      "Iteration 72, loss = 0.13512266\n",
      "Iteration 73, loss = 0.13291622\n",
      "Iteration 57, loss = 0.18633896\n",
      "Iteration 58, loss = 0.18379074\n",
      "Iteration 74, loss = 0.13222334\n",
      "Iteration 59, loss = 0.18124800\n",
      "Iteration 75, loss = 0.13120867\n",
      "Iteration 76, loss = 0.13009796\n",
      "Iteration 60, loss = 0.17938933\n",
      "Iteration 77, loss = 0.12683669\n",
      "Iteration 61, loss = 0.17718866\n",
      "Iteration 78, loss = 0.12635867\n",
      "Iteration 62, loss = 0.17541081\n",
      "Iteration 79, loss = 0.12506400\n",
      "Iteration 63, loss = 0.17387240\n",
      "Iteration 80, loss = 0.12409967\n",
      "Iteration 64, loss = 0.17124610\n",
      "Iteration 81, loss = 0.12153276\n",
      "Iteration 65, loss = 0.16845506\n",
      "Iteration 66, loss = 0.16654261\n",
      "Iteration 82, loss = 0.12148037\n",
      "Iteration 67, loss = 0.16465327\n",
      "Iteration 83, loss = 0.11984856\n",
      "Iteration 68, loss = 0.16620264\n",
      "Iteration 84, loss = 0.11832592\n",
      "Iteration 69, loss = 0.16334994\n",
      "Iteration 85, loss = 0.11744084\n",
      "Iteration 70, loss = 0.16133550\n",
      "Iteration 86, loss = 0.11675368\n",
      "Iteration 71, loss = 0.16042746\n",
      "Iteration 87, loss = 0.11366445\n",
      "Iteration 72, loss = 0.15735797\n",
      "Iteration 88, loss = 0.11324705\n",
      "Iteration 73, loss = 0.15489435\n",
      "Iteration 89, loss = 0.11314381\n",
      "Iteration 74, loss = 0.15075976\n",
      "Iteration 90, loss = 0.11224436\n",
      "Iteration 75, loss = 0.14879276\n",
      "Iteration 91, loss = 0.11212980\n",
      "Iteration 76, loss = 0.14759098\n",
      "Iteration 92, loss = 0.11026041\n",
      "Iteration 77, loss = 0.14588513\n",
      "Iteration 93, loss = 0.11020825\n",
      "Iteration 78, loss = 0.14352432\n",
      "Iteration 94, loss = 0.10622392\n",
      "Iteration 79, loss = 0.14402173\n",
      "Iteration 95, loss = 0.10732350\n",
      "Iteration 80, loss = 0.14047773\n",
      "Iteration 96, loss = 0.10600400\n",
      "Iteration 81, loss = 0.13941527\n",
      "Iteration 97, loss = 0.10351872\n",
      "Iteration 82, loss = 0.13727796\n",
      "Iteration 98, loss = 0.10364542\n",
      "Iteration 83, loss = 0.13513077\n",
      "Iteration 99, loss = 0.10122045\n",
      "Iteration 84, loss = 0.13382772\n",
      "Iteration 100, loss = 0.10143757\n",
      "Iteration 85, loss = 0.13330733\n",
      "Iteration 101, loss = 0.09997205\n",
      "Iteration 86, loss = 0.13401097\n",
      "Iteration 102, loss = 0.09960485\n",
      "Iteration 87, loss = 0.12971888\n",
      "Iteration 103, loss = 0.09788722\n",
      "Iteration 88, loss = 0.13189781\n",
      "Iteration 104, loss = 0.09837571\n",
      "Iteration 89, loss = 0.12780097\n",
      "Iteration 105, loss = 0.09865328\n",
      "Iteration 90, loss = 0.12631346\n",
      "Iteration 106, loss = 0.09679879\n",
      "Iteration 91, loss = 0.12535114\n",
      "Iteration 107, loss = 0.09482645\n",
      "Iteration 92, loss = 0.12461951\n",
      "Iteration 108, loss = 0.09424082\n",
      "Iteration 93, loss = 0.12249886\n",
      "Iteration 109, loss = 0.09386239\n",
      "Iteration 94, loss = 0.12141848\n",
      "Iteration 110, loss = 0.09279089\n",
      "Iteration 95, loss = 0.11923132\n",
      "Iteration 111, loss = 0.09337936\n",
      "Iteration 96, loss = 0.11877233\n",
      "Iteration 112, loss = 0.09161934\n",
      "Iteration 97, loss = 0.11825636\n",
      "Iteration 113, loss = 0.08996859\n",
      "Iteration 98, loss = 0.11667810\n",
      "Iteration 114, loss = 0.09191688\n",
      "Iteration 99, loss = 0.11569547\n",
      "Iteration 115, loss = 0.09384285\n",
      "Iteration 100, loss = 0.11502763\n",
      "Iteration 116, loss = 0.09091724\n",
      "Iteration 101, loss = 0.11648540\n",
      "Iteration 117, loss = 0.08811691\n",
      "Iteration 102, loss = 0.11238232\n",
      "Iteration 118, loss = 0.08784599\n",
      "Iteration 103, loss = 0.11077717\n",
      "Iteration 119, loss = 0.08623678\n",
      "Iteration 104, loss = 0.11008817\n",
      "Iteration 120, loss = 0.08478158\n",
      "Iteration 105, loss = 0.11089349\n",
      "Iteration 121, loss = 0.08376805\n",
      "Iteration 106, loss = 0.10723057\n",
      "Iteration 122, loss = 0.08329420\n",
      "Iteration 107, loss = 0.10872139\n",
      "Iteration 123, loss = 0.08752194\n",
      "Iteration 108, loss = 0.10786076\n",
      "Iteration 124, loss = 0.08310941\n",
      "Iteration 109, loss = 0.10380350\n",
      "Iteration 125, loss = 0.08401769\n",
      "Iteration 110, loss = 0.10678063\n",
      "Iteration 126, loss = 0.08167041\n",
      "Iteration 111, loss = 0.10692697\n",
      "Iteration 127, loss = 0.08337906\n",
      "Iteration 112, loss = 0.10412903\n",
      "Iteration 128, loss = 0.08037294\n",
      "Iteration 113, loss = 0.10902421\n",
      "Iteration 129, loss = 0.08340895\n",
      "Iteration 114, loss = 0.10466087\n",
      "Iteration 130, loss = 0.08555539\n",
      "Iteration 115, loss = 0.10486818\n",
      "Iteration 131, loss = 0.07920348\n",
      "Iteration 116, loss = 0.10037482\n",
      "Iteration 132, loss = 0.07883506\n",
      "Iteration 117, loss = 0.09898650\n",
      "Iteration 133, loss = 0.08014379\n",
      "Iteration 118, loss = 0.09952716\n",
      "Iteration 134, loss = 0.07519897\n",
      "Iteration 119, loss = 0.09603014\n",
      "Iteration 135, loss = 0.07588823\n",
      "Iteration 120, loss = 0.09412618\n",
      "Iteration 136, loss = 0.07505077\n",
      "Iteration 121, loss = 0.09542583\n",
      "Iteration 137, loss = 0.07302951\n",
      "Iteration 122, loss = 0.09367929\n",
      "Iteration 138, loss = 0.07317693\n",
      "Iteration 123, loss = 0.09273732\n",
      "Iteration 139, loss = 0.07344824\n",
      "Iteration 124, loss = 0.09252062\n",
      "Iteration 140, loss = 0.07115495\n",
      "Iteration 125, loss = 0.09079645\n",
      "Iteration 141, loss = 0.07333007\n",
      "Iteration 126, loss = 0.08983343\n",
      "Iteration 142, loss = 0.07236555\n",
      "Iteration 127, loss = 0.08985947\n",
      "Iteration 143, loss = 0.07350332\n",
      "Iteration 128, loss = 0.08897596\n",
      "Iteration 144, loss = 0.07066008\n",
      "Iteration 129, loss = 0.08969723\n",
      "Iteration 145, loss = 0.07023364\n",
      "Iteration 130, loss = 0.08874998\n",
      "Iteration 146, loss = 0.06773707\n",
      "Iteration 131, loss = 0.08884075\n",
      "Iteration 147, loss = 0.07320832\n",
      "Iteration 132, loss = 0.08645227\n",
      "Iteration 148, loss = 0.06711599\n",
      "Iteration 133, loss = 0.08541144\n",
      "Iteration 149, loss = 0.07000013\n",
      "Iteration 134, loss = 0.08576674\n",
      "Iteration 150, loss = 0.06565623\n",
      "Iteration 135, loss = 0.08379561\n",
      "Iteration 151, loss = 0.06649614\n",
      "Iteration 136, loss = 0.08414968\n",
      "Iteration 152, loss = 0.06411224\n",
      "Iteration 137, loss = 0.08110637\n",
      "Iteration 153, loss = 0.06683151\n",
      "Iteration 138, loss = 0.08097182\n",
      "Iteration 154, loss = 0.06376385\n",
      "Iteration 139, loss = 0.08210475\n",
      "Iteration 155, loss = 0.06360220\n",
      "Iteration 140, loss = 0.08305967\n",
      "Iteration 156, loss = 0.06319564\n",
      "Iteration 141, loss = 0.08239119\n",
      "Iteration 157, loss = 0.06291140\n",
      "Iteration 142, loss = 0.08266718\n",
      "Iteration 158, loss = 0.06275970\n",
      "Iteration 143, loss = 0.07977197\n",
      "Iteration 159, loss = 0.06139145\n",
      "Iteration 144, loss = 0.07909484\n",
      "Iteration 160, loss = 0.06091833\n",
      "Iteration 145, loss = 0.07680893\n",
      "Iteration 161, loss = 0.05962091\n",
      "Iteration 146, loss = 0.07814521\n",
      "Iteration 162, loss = 0.06054797\n",
      "Iteration 147, loss = 0.07526303\n",
      "Iteration 148, loss = 0.07459485\n",
      "Iteration 163, loss = 0.06024243\n",
      "Iteration 149, loss = 0.07441670\n",
      "Iteration 164, loss = 0.05777487\n",
      "Iteration 150, loss = 0.07286524\n",
      "Iteration 165, loss = 0.05888228\n",
      "Iteration 151, loss = 0.07350895\n",
      "Iteration 166, loss = 0.06251887\n",
      "Iteration 152, loss = 0.07184791\n",
      "Iteration 167, loss = 0.06047866\n",
      "Iteration 153, loss = 0.07089288\n",
      "Iteration 168, loss = 0.05824428\n",
      "Iteration 169, loss = 0.06229310\n",
      "Iteration 154, loss = 0.06950480\n",
      "Iteration 170, loss = 0.05897774\n",
      "Iteration 155, loss = 0.06939656\n",
      "Iteration 171, loss = 0.05516360\n",
      "Iteration 156, loss = 0.06960586\n",
      "Iteration 172, loss = 0.05523849\n",
      "Iteration 157, loss = 0.07098069\n",
      "Iteration 173, loss = 0.05493603\n",
      "Iteration 158, loss = 0.07165505\n",
      "Iteration 174, loss = 0.05571122\n",
      "Iteration 159, loss = 0.06579634\n",
      "Iteration 175, loss = 0.05351488\n",
      "Iteration 160, loss = 0.06792725\n",
      "Iteration 176, loss = 0.05482488\n",
      "Iteration 161, loss = 0.06759574\n",
      "Iteration 177, loss = 0.05375190Iteration 162, loss = 0.06413708\n",
      "\n",
      "Iteration 163, loss = 0.06479672Iteration 178, loss = 0.05520847\n",
      "\n",
      "Iteration 164, loss = 0.06370947\n",
      "Iteration 179, loss = 0.05308276\n",
      "Iteration 180, loss = 0.05307174\n",
      "Iteration 165, loss = 0.06366077\n",
      "Iteration 181, loss = 0.05472196\n",
      "Iteration 166, loss = 0.06231012\n",
      "Iteration 182, loss = 0.04993690\n",
      "Iteration 167, loss = 0.06265938\n",
      "Iteration 183, loss = 0.05014541\n",
      "Iteration 168, loss = 0.06214014\n",
      "Iteration 184, loss = 0.05054857\n",
      "Iteration 169, loss = 0.06095277\n",
      "Iteration 185, loss = 0.04861999\n",
      "Iteration 170, loss = 0.06423298\n",
      "Iteration 186, loss = 0.04782306\n",
      "Iteration 171, loss = 0.06120029\n",
      "Iteration 172, loss = 0.05879071\n",
      "Iteration 187, loss = 0.04732188\n",
      "Iteration 173, loss = 0.05821877\n",
      "Iteration 188, loss = 0.04633448\n",
      "Iteration 174, loss = 0.05903643\n",
      "Iteration 189, loss = 0.04818859\n",
      "Iteration 175, loss = 0.05987763\n",
      "Iteration 190, loss = 0.04721987\n",
      "Iteration 176, loss = 0.05686977\n",
      "Iteration 191, loss = 0.04836225\n",
      "Iteration 177, loss = 0.05572615\n",
      "Iteration 192, loss = 0.04739017\n",
      "Iteration 178, loss = 0.05889148\n",
      "Iteration 193, loss = 0.04777269\n",
      "Iteration 179, loss = 0.05641958\n",
      "Iteration 194, loss = 0.04643031\n",
      "Iteration 180, loss = 0.05414937\n",
      "Iteration 195, loss = 0.04554100\n",
      "Iteration 181, loss = 0.05569674\n",
      "Iteration 196, loss = 0.04402677\n",
      "Iteration 182, loss = 0.05431488\n",
      "Iteration 197, loss = 0.04382189\n",
      "Iteration 183, loss = 0.05413907\n",
      "Iteration 198, loss = 0.04469811\n",
      "Iteration 184, loss = 0.05228773\n",
      "Iteration 199, loss = 0.04355329\n",
      "Iteration 185, loss = 0.05384887\n",
      "Iteration 200, loss = 0.04316647\n",
      "Iteration 186, loss = 0.05778860\n",
      "Iteration 201, loss = 0.04510284\n",
      "Iteration 187, loss = 0.05354691\n",
      "Iteration 202, loss = 0.04407739\n",
      "Iteration 188, loss = 0.05046271\n",
      "Iteration 203, loss = 0.04357446\n",
      "Iteration 189, loss = 0.05251429\n",
      "Iteration 204, loss = 0.04165318\n",
      "Iteration 190, loss = 0.04975328\n",
      "Iteration 205, loss = 0.04140720\n",
      "Iteration 191, loss = 0.04980179\n",
      "Iteration 206, loss = 0.04036526\n",
      "Iteration 192, loss = 0.05053214\n",
      "Iteration 207, loss = 0.04077620\n",
      "Iteration 193, loss = 0.04759198\n",
      "Iteration 208, loss = 0.04197441\n",
      "Iteration 194, loss = 0.04781015\n",
      "Iteration 209, loss = 0.03866262\n",
      "Iteration 195, loss = 0.04778353\n",
      "Iteration 210, loss = 0.03945287\n",
      "Iteration 196, loss = 0.04812253\n",
      "Iteration 211, loss = 0.04123901\n",
      "Iteration 197, loss = 0.04716933\n",
      "Iteration 212, loss = 0.04351487\n",
      "Iteration 198, loss = 0.04708422\n",
      "Iteration 213, loss = 0.04019902\n",
      "Iteration 199, loss = 0.04636365\n",
      "Iteration 214, loss = 0.03802021\n",
      "Iteration 200, loss = 0.05070353\n",
      "Iteration 215, loss = 0.04183434\n",
      "Iteration 201, loss = 0.04837578\n",
      "Iteration 216, loss = 0.04540906\n",
      "Iteration 202, loss = 0.04853411\n",
      "Iteration 217, loss = 0.03951687\n",
      "Iteration 203, loss = 0.04924246\n",
      "Iteration 218, loss = 0.03908695\n",
      "Iteration 204, loss = 0.04819239\n",
      "Iteration 219, loss = 0.03931099\n",
      "Iteration 205, loss = 0.04347466\n",
      "Iteration 220, loss = 0.03616990\n",
      "Iteration 206, loss = 0.04303306\n",
      "Iteration 221, loss = 0.03688579\n",
      "Iteration 207, loss = 0.04362417\n",
      "Iteration 222, loss = 0.03572566\n",
      "Iteration 208, loss = 0.04377792\n",
      "Iteration 223, loss = 0.03431339\n",
      "Iteration 209, loss = 0.04630971\n",
      "Iteration 224, loss = 0.03660599\n",
      "Iteration 210, loss = 0.04666861\n",
      "Iteration 225, loss = 0.03512823\n",
      "Iteration 211, loss = 0.04312657\n",
      "Iteration 226, loss = 0.03581981\n",
      "Iteration 212, loss = 0.04337040\n",
      "Iteration 227, loss = 0.03488412\n",
      "Iteration 213, loss = 0.04277397\n",
      "Iteration 228, loss = 0.03313487\n",
      "Iteration 214, loss = 0.03934895\n",
      "Iteration 229, loss = 0.03440244\n",
      "Iteration 215, loss = 0.04194495\n",
      "Iteration 230, loss = 0.03310124\n",
      "Iteration 216, loss = 0.04406520\n",
      "Iteration 231, loss = 0.03341964\n",
      "Iteration 217, loss = 0.04254221\n",
      "Iteration 232, loss = 0.03485402\n",
      "Iteration 218, loss = 0.04213520\n",
      "Iteration 233, loss = 0.03619816\n",
      "Iteration 219, loss = 0.03819338\n",
      "Iteration 234, loss = 0.03494838\n",
      "Iteration 220, loss = 0.04018795\n",
      "Iteration 235, loss = 0.03183206\n",
      "Iteration 221, loss = 0.03721281\n",
      "Iteration 236, loss = 0.03087436\n",
      "Iteration 222, loss = 0.03821986\n",
      "Iteration 237, loss = 0.03147617\n",
      "Iteration 223, loss = 0.03717572\n",
      "Iteration 238, loss = 0.03125247\n",
      "Iteration 224, loss = 0.03746720\n",
      "Iteration 239, loss = 0.03261456\n",
      "Iteration 225, loss = 0.03661681\n",
      "Iteration 240, loss = 0.03045243\n",
      "Iteration 226, loss = 0.03553857\n",
      "Iteration 241, loss = 0.03232326\n",
      "Iteration 227, loss = 0.03607548\n",
      "Iteration 242, loss = 0.03502894\n",
      "Iteration 228, loss = 0.03411391\n",
      "Iteration 243, loss = 0.03345418\n",
      "Iteration 229, loss = 0.03506668\n",
      "Iteration 244, loss = 0.03161754\n",
      "Iteration 230, loss = 0.03659837\n",
      "Iteration 245, loss = 0.03547379\n",
      "Iteration 231, loss = 0.03581678\n",
      "Iteration 246, loss = 0.03855939\n",
      "Iteration 232, loss = 0.03365938\n",
      "Iteration 247, loss = 0.04401581\n",
      "Iteration 233, loss = 0.03488358\n",
      "Iteration 248, loss = 0.03963469\n",
      "Iteration 234, loss = 0.03340960\n",
      "Iteration 249, loss = 0.02909688\n",
      "Iteration 235, loss = 0.03316801\n",
      "Iteration 250, loss = 0.03010341\n",
      "Iteration 236, loss = 0.03196230\n",
      "Iteration 251, loss = 0.02982074\n",
      "Iteration 237, loss = 0.03260274\n",
      "Iteration 252, loss = 0.02885012\n",
      "Iteration 238, loss = 0.03290846\n",
      "Iteration 253, loss = 0.02825457\n",
      "Iteration 239, loss = 0.03268056\n",
      "Iteration 254, loss = 0.03072252\n",
      "Iteration 240, loss = 0.03282206\n",
      "Iteration 255, loss = 0.02938125\n",
      "Iteration 241, loss = 0.03513136\n",
      "Iteration 256, loss = 0.02787344\n",
      "Iteration 242, loss = 0.03558402\n",
      "Iteration 257, loss = 0.02778524\n",
      "Iteration 243, loss = 0.03387879\n",
      "Iteration 258, loss = 0.02762349\n",
      "Iteration 244, loss = 0.03111788\n",
      "Iteration 259, loss = 0.02725782\n",
      "Iteration 245, loss = 0.03159516\n",
      "Iteration 260, loss = 0.02722212\n",
      "Iteration 246, loss = 0.03206712\n",
      "Iteration 261, loss = 0.02775314\n",
      "Iteration 247, loss = 0.03667751\n",
      "Iteration 262, loss = 0.02629727\n",
      "Iteration 248, loss = 0.03257693\n",
      "Iteration 263, loss = 0.02620756\n",
      "Iteration 249, loss = 0.03099623\n",
      "Iteration 264, loss = 0.02411448\n",
      "Iteration 250, loss = 0.02842338\n",
      "Iteration 265, loss = 0.02403146\n",
      "Iteration 251, loss = 0.02844379\n",
      "Iteration 266, loss = 0.02482821\n",
      "Iteration 252, loss = 0.02862555\n",
      "Iteration 267, loss = 0.02539012\n",
      "Iteration 253, loss = 0.02928847\n",
      "Iteration 268, loss = 0.02683031\n",
      "Iteration 254, loss = 0.02782295\n",
      "Iteration 269, loss = 0.02675922\n",
      "Iteration 255, loss = 0.02843234\n",
      "Iteration 270, loss = 0.02516344\n",
      "Iteration 256, loss = 0.02740211\n",
      "Iteration 271, loss = 0.02691982\n",
      "Iteration 257, loss = 0.02574621\n",
      "Iteration 272, loss = 0.02441250\n",
      "Iteration 258, loss = 0.02576133\n",
      "Iteration 273, loss = 0.02370963\n",
      "Iteration 259, loss = 0.02563914\n",
      "Iteration 274, loss = 0.02308279\n",
      "Iteration 260, loss = 0.02515984\n",
      "Iteration 275, loss = 0.02275447\n",
      "Iteration 261, loss = 0.02523039\n",
      "Iteration 276, loss = 0.02246759\n",
      "Iteration 262, loss = 0.02569038\n",
      "Iteration 277, loss = 0.02211029\n",
      "Iteration 263, loss = 0.02490123\n",
      "Iteration 278, loss = 0.02193568\n",
      "Iteration 264, loss = 0.02450066\n",
      "Iteration 279, loss = 0.02203358\n",
      "Iteration 265, loss = 0.02425608\n",
      "Iteration 280, loss = 0.02317211\n",
      "Iteration 266, loss = 0.02442608\n",
      "Iteration 281, loss = 0.02186145\n",
      "Iteration 267, loss = 0.02425358\n",
      "Iteration 282, loss = 0.02053653\n",
      "Iteration 268, loss = 0.02534315\n",
      "Iteration 283, loss = 0.02244173\n",
      "Iteration 269, loss = 0.02597612\n",
      "Iteration 284, loss = 0.02186899\n",
      "Iteration 270, loss = 0.02453589\n",
      "Iteration 285, loss = 0.02239093\n",
      "Iteration 271, loss = 0.02331530\n",
      "Iteration 286, loss = 0.02202671\n",
      "Iteration 272, loss = 0.02407235\n",
      "Iteration 287, loss = 0.02148962\n",
      "Iteration 273, loss = 0.02402324\n",
      "Iteration 288, loss = 0.02117817\n",
      "Iteration 274, loss = 0.02386289\n",
      "Iteration 289, loss = 0.02251022\n",
      "Iteration 275, loss = 0.02242013\n",
      "Iteration 290, loss = 0.02161020\n",
      "Iteration 276, loss = 0.02301953\n",
      "Iteration 291, loss = 0.01897275\n",
      "Iteration 277, loss = 0.02246127\n",
      "Iteration 292, loss = 0.02192880\n",
      "Iteration 278, loss = 0.02217709\n",
      "Iteration 293, loss = 0.01960345\n",
      "Iteration 279, loss = 0.02158413\n",
      "Iteration 294, loss = 0.01908790\n",
      "Iteration 280, loss = 0.02093519\n",
      "Iteration 295, loss = 0.01906742\n",
      "Iteration 281, loss = 0.02116119\n",
      "Iteration 296, loss = 0.01846164\n",
      "Iteration 282, loss = 0.02107163\n",
      "Iteration 297, loss = 0.01890050\n",
      "Iteration 283, loss = 0.02051052\n",
      "Iteration 298, loss = 0.01913045\n",
      "Iteration 284, loss = 0.02022490\n",
      "Iteration 299, loss = 0.01995908\n",
      "Iteration 285, loss = 0.02012647\n",
      "Iteration 300, loss = 0.02129716\n",
      "Iteration 286, loss = 0.02065183\n",
      "Iteration 301, loss = 0.02035648\n",
      "Iteration 287, loss = 0.02056461\n",
      "Iteration 302, loss = 0.02126346\n",
      "Iteration 288, loss = 0.02070923\n",
      "Iteration 303, loss = 0.01914206\n",
      "Iteration 289, loss = 0.02050460\n",
      "Iteration 304, loss = 0.01718584\n",
      "Iteration 290, loss = 0.01895023\n",
      "Iteration 305, loss = 0.01868484\n",
      "Iteration 291, loss = 0.01950268\n",
      "Iteration 306, loss = 0.01856579\n",
      "Iteration 292, loss = 0.01929290\n",
      "Iteration 307, loss = 0.01826336\n",
      "Iteration 293, loss = 0.01860887\n",
      "Iteration 308, loss = 0.01886048\n",
      "Iteration 294, loss = 0.01895016\n",
      "Iteration 309, loss = 0.01815796\n",
      "Iteration 295, loss = 0.01831467\n",
      "Iteration 310, loss = 0.01665084\n",
      "Iteration 296, loss = 0.01862487\n",
      "Iteration 311, loss = 0.01731943\n",
      "Iteration 297, loss = 0.01808559\n",
      "Iteration 312, loss = 0.01839392\n",
      "Iteration 298, loss = 0.01804876\n",
      "Iteration 313, loss = 0.01711101\n",
      "Iteration 299, loss = 0.01865486\n",
      "Iteration 314, loss = 0.01882426\n",
      "Iteration 300, loss = 0.01822891\n",
      "Iteration 315, loss = 0.01683027\n",
      "Iteration 301, loss = 0.01811787\n",
      "Iteration 316, loss = 0.01697170\n",
      "Iteration 302, loss = 0.01751623\n",
      "Iteration 317, loss = 0.01812754\n",
      "Iteration 303, loss = 0.01774970\n",
      "Iteration 318, loss = 0.01521625\n",
      "Iteration 304, loss = 0.01886291\n",
      "Iteration 319, loss = 0.01687448\n",
      "Iteration 305, loss = 0.01770288\n",
      "Iteration 320, loss = 0.01778507\n",
      "Iteration 306, loss = 0.01666972\n",
      "Iteration 321, loss = 0.01570442\n",
      "Iteration 307, loss = 0.01656593\n",
      "Iteration 322, loss = 0.01466521\n",
      "Iteration 308, loss = 0.01644530\n",
      "Iteration 323, loss = 0.01526453\n",
      "Iteration 309, loss = 0.01652806\n",
      "Iteration 324, loss = 0.01488097\n",
      "Iteration 310, loss = 0.01715157\n",
      "Iteration 311, loss = 0.01620804\n",
      "Iteration 325, loss = 0.01531031\n",
      "Iteration 312, loss = 0.01722381\n",
      "Iteration 326, loss = 0.01576871\n",
      "Iteration 327, loss = 0.01644381\n",
      "Iteration 313, loss = 0.01829505\n",
      "Iteration 314, loss = 0.01793951\n",
      "Iteration 328, loss = 0.01373790\n",
      "Iteration 315, loss = 0.01855898\n",
      "Iteration 329, loss = 0.01519630\n",
      "Iteration 316, loss = 0.01712757\n",
      "Iteration 330, loss = 0.01563827\n",
      "Iteration 317, loss = 0.01607548\n",
      "Iteration 331, loss = 0.01612840\n",
      "Iteration 332, loss = 0.01478844\n",
      "Iteration 318, loss = 0.01492624\n",
      "Iteration 333, loss = 0.01405067\n",
      "Iteration 319, loss = 0.01499504\n",
      "Iteration 334, loss = 0.01483897\n",
      "Iteration 320, loss = 0.01771255\n",
      "Iteration 321, loss = 0.01672485\n",
      "Iteration 335, loss = 0.01574112\n",
      "Iteration 322, loss = 0.01654275\n",
      "Iteration 336, loss = 0.01375254\n",
      "Iteration 323, loss = 0.01519510\n",
      "Iteration 337, loss = 0.01339099\n",
      "Iteration 324, loss = 0.01483742\n",
      "Iteration 338, loss = 0.01386270\n",
      "Iteration 325, loss = 0.01464514\n",
      "Iteration 339, loss = 0.01363366\n",
      "Iteration 326, loss = 0.01437829\n",
      "Iteration 340, loss = 0.01301678\n",
      "Iteration 327, loss = 0.01374760\n",
      "Iteration 341, loss = 0.01342864\n",
      "Iteration 328, loss = 0.01459456\n",
      "Iteration 342, loss = 0.01372096\n",
      "Iteration 329, loss = 0.01463473\n",
      "Iteration 343, loss = 0.01266852\n",
      "Iteration 330, loss = 0.01492704\n",
      "Iteration 344, loss = 0.01388072\n",
      "Iteration 331, loss = 0.01415185\n",
      "Iteration 345, loss = 0.01338064\n",
      "Iteration 332, loss = 0.01462007\n",
      "Iteration 346, loss = 0.01265706\n",
      "Iteration 333, loss = 0.01360426\n",
      "Iteration 347, loss = 0.01275345\n",
      "Iteration 334, loss = 0.01304431\n",
      "Iteration 348, loss = 0.01280222\n",
      "Iteration 335, loss = 0.01331442\n",
      "Iteration 349, loss = 0.01243724\n",
      "Iteration 336, loss = 0.01286602\n",
      "Iteration 350, loss = 0.01242251\n",
      "Iteration 337, loss = 0.01271605\n",
      "Iteration 351, loss = 0.01272841\n",
      "Iteration 338, loss = 0.01282740\n",
      "Iteration 352, loss = 0.01196155\n",
      "Iteration 339, loss = 0.01468215\n",
      "Iteration 353, loss = 0.01241660\n",
      "Iteration 340, loss = 0.01844680\n",
      "Iteration 354, loss = 0.01296480\n",
      "Iteration 341, loss = 0.01962909\n",
      "Iteration 355, loss = 0.01240126\n",
      "Iteration 342, loss = 0.01837002\n",
      "Iteration 356, loss = 0.01210652\n",
      "Iteration 343, loss = 0.01669042\n",
      "Iteration 357, loss = 0.01133689\n",
      "Iteration 344, loss = 0.01731066\n",
      "Iteration 358, loss = 0.01127763\n",
      "Iteration 345, loss = 0.01374440\n",
      "Iteration 359, loss = 0.01174115\n",
      "Iteration 346, loss = 0.01393247\n",
      "Iteration 360, loss = 0.01173683\n",
      "Iteration 347, loss = 0.01244696\n",
      "Iteration 361, loss = 0.01138123\n",
      "Iteration 348, loss = 0.01279700\n",
      "Iteration 362, loss = 0.01201307\n",
      "Iteration 349, loss = 0.01228515\n",
      "Iteration 363, loss = 0.01183180\n",
      "Iteration 350, loss = 0.01196088\n",
      "Iteration 364, loss = 0.01239784\n",
      "Iteration 351, loss = 0.01160295\n",
      "Iteration 365, loss = 0.01161441\n",
      "Iteration 352, loss = 0.01170085\n",
      "Iteration 366, loss = 0.01124981\n",
      "Iteration 353, loss = 0.01171347\n",
      "Iteration 367, loss = 0.01119724\n",
      "Iteration 354, loss = 0.01152810\n",
      "Iteration 368, loss = 0.01250802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 355, loss = 0.01107244\n",
      "Iteration 356, loss = 0.01117270\n",
      "Iteration 357, loss = 0.01237917\n",
      "Iteration 358, loss = 0.01152536\n",
      "Iteration 359, loss = 0.01097880\n",
      "Iteration 360, loss = 0.01066501\n",
      "Iteration 361, loss = 0.01079448\n",
      "Iteration 362, loss = 0.01075195\n",
      "Iteration 363, loss = 0.01051606\n",
      "Iteration 364, loss = 0.01030215\n",
      "Iteration 365, loss = 0.01013039\n",
      "Iteration 366, loss = 0.00999947\n",
      "Iteration 367, loss = 0.00995971\n",
      "Iteration 1, loss = 17.89462468\n",
      "Iteration 368, loss = 0.00979254\n",
      "Iteration 2, loss = 15.59682572\n",
      "Iteration 3, loss = 9.28843559\n",
      "Iteration 369, loss = 0.01048537\n",
      "Iteration 370, loss = 0.01168947\n",
      "Iteration 4, loss = 8.38705184\n",
      "Iteration 371, loss = 0.01019011\n",
      "Iteration 5, loss = 7.17732267\n",
      "Iteration 372, loss = 0.00969138\n",
      "Iteration 6, loss = 8.44473823\n",
      "Iteration 7, loss = 5.99256633\n",
      "Iteration 373, loss = 0.00949753\n",
      "Iteration 374, loss = 0.00954595\n",
      "Iteration 8, loss = 5.98090814\n",
      "Iteration 375, loss = 0.00965535\n",
      "Iteration 9, loss = 6.72801946\n",
      "Iteration 10, loss = 7.37884650\n",
      "Iteration 376, loss = 0.00928749\n",
      "Iteration 11, loss = 4.61905840\n",
      "Iteration 377, loss = 0.00949706\n",
      "Iteration 12, loss = 5.40086195\n",
      "Iteration 378, loss = 0.00948791\n",
      "Iteration 13, loss = 4.08545432\n",
      "Iteration 379, loss = 0.00905928\n",
      "Iteration 14, loss = 5.16740088\n",
      "Iteration 380, loss = 0.00915005\n",
      "Iteration 15, loss = 4.46283304\n",
      "Iteration 381, loss = 0.00908180\n",
      "Iteration 16, loss = 3.72463622\n",
      "Iteration 382, loss = 0.00906063\n",
      "Iteration 17, loss = 3.78271294\n",
      "Iteration 383, loss = 0.00949469\n",
      "Iteration 18, loss = 4.35789198\n",
      "Iteration 384, loss = 0.00892267\n",
      "Iteration 19, loss = 3.29494549\n",
      "Iteration 385, loss = 0.00861098\n",
      "Iteration 20, loss = 3.33086883\n",
      "Iteration 386, loss = 0.00861950\n",
      "Iteration 21, loss = 3.80473665\n",
      "Iteration 387, loss = 0.00857201\n",
      "Iteration 22, loss = 3.68626004\n",
      "Iteration 388, loss = 0.00882233\n",
      "Iteration 23, loss = 3.72712512\n",
      "Iteration 389, loss = 0.00852092\n",
      "Iteration 24, loss = 3.00375699\n",
      "Iteration 390, loss = 0.00851175\n",
      "Iteration 25, loss = 2.67773023\n",
      "Iteration 391, loss = 0.00888822\n",
      "Iteration 26, loss = 2.53786694\n",
      "Iteration 392, loss = 0.00857841\n",
      "Iteration 27, loss = 2.43477002\n",
      "Iteration 393, loss = 0.00875167\n",
      "Iteration 28, loss = 3.55616433\n",
      "Iteration 394, loss = 0.00828543\n",
      "Iteration 29, loss = 3.32738366\n",
      "Iteration 395, loss = 0.00824311\n",
      "Iteration 30, loss = 2.76000507\n",
      "Iteration 396, loss = 0.00807920\n",
      "Iteration 31, loss = 3.07621228\n",
      "Iteration 397, loss = 0.00797857\n",
      "Iteration 32, loss = 2.55571929\n",
      "Iteration 398, loss = 0.00897440\n",
      "Iteration 33, loss = 2.56778351\n",
      "Iteration 399, loss = 0.01000752\n",
      "Iteration 34, loss = 2.16189206\n",
      "Iteration 400, loss = 0.00991561\n",
      "Iteration 35, loss = 2.89969790\n",
      "Iteration 401, loss = 0.00970631\n",
      "Iteration 36, loss = 3.89070771\n",
      "Iteration 402, loss = 0.01106649\n",
      "Iteration 37, loss = 3.17968796\n",
      "Iteration 403, loss = 0.01034879\n",
      "Iteration 38, loss = 3.75055267\n",
      "Iteration 404, loss = 0.00773315\n",
      "Iteration 39, loss = 2.59404152\n",
      "Iteration 405, loss = 0.00850474\n",
      "Iteration 40, loss = 2.57735506\n",
      "Iteration 406, loss = 0.00872227\n",
      "Iteration 41, loss = 2.83867729\n",
      "Iteration 407, loss = 0.00969493\n",
      "Iteration 42, loss = 2.56160103\n",
      "Iteration 408, loss = 0.01017523\n",
      "Iteration 43, loss = 2.34190744\n",
      "Iteration 409, loss = 0.01178801\n",
      "Iteration 44, loss = 1.69851802\n",
      "Iteration 410, loss = 0.01044473\n",
      "Iteration 411, loss = 0.00988636\n",
      "Iteration 45, loss = 1.61433246\n",
      "Iteration 46, loss = 2.31853784\n",
      "Iteration 412, loss = 0.01323059\n",
      "Iteration 47, loss = 1.91258508\n",
      "Iteration 413, loss = 0.01465527\n",
      "Iteration 48, loss = 1.39682680\n",
      "Iteration 414, loss = 0.01627825\n",
      "Iteration 49, loss = 1.75384274\n",
      "Iteration 415, loss = 0.01508474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 1.79447749\n",
      "Iteration 51, loss = 1.80369949\n",
      "Iteration 52, loss = 1.78703452\n",
      "Iteration 53, loss = 1.81980270\n",
      "Iteration 54, loss = 1.70929108\n",
      "Iteration 55, loss = 1.80663697\n",
      "Iteration 56, loss = 2.21880829\n",
      "Iteration 57, loss = 2.76057968\n",
      "Iteration 58, loss = 2.19611051\n",
      "Iteration 59, loss = 2.10211292\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.77400529\n",
      "Iteration 2, loss = 10.83912297\n",
      "Iteration 3, loss = 11.35416819\n",
      "Iteration 4, loss = 10.20720617\n",
      "Iteration 5, loss = 7.78921254\n",
      "Iteration 6, loss = 7.42131397\n",
      "Iteration 7, loss = 6.90977973\n",
      "Iteration 8, loss = 7.07469118\n",
      "Iteration 9, loss = 5.33873929\n",
      "Iteration 10, loss = 5.28748168\n",
      "Iteration 11, loss = 6.38002960\n",
      "Iteration 1, loss = 17.37366468\n",
      "Iteration 12, loss = 4.73672995\n",
      "Iteration 2, loss = 14.16294693\n",
      "Iteration 13, loss = 5.65563432\n",
      "Iteration 3, loss = 11.47197914\n",
      "Iteration 14, loss = 4.98037536\n",
      "Iteration 4, loss = 9.16653343\n",
      "Iteration 15, loss = 4.71435085\n",
      "Iteration 5, loss = 6.81245259\n",
      "Iteration 16, loss = 3.73008197\n",
      "Iteration 6, loss = 7.90501911\n",
      "Iteration 17, loss = 3.72843162\n",
      "Iteration 7, loss = 9.59427535\n",
      "Iteration 8, loss = 9.50186954\n",
      "Iteration 18, loss = 3.86136856\n",
      "Iteration 9, loss = 8.31081501\n",
      "Iteration 19, loss = 3.57626177\n",
      "Iteration 10, loss = 5.19619004\n",
      "Iteration 20, loss = 3.13319809\n",
      "Iteration 11, loss = 5.91636014\n",
      "Iteration 21, loss = 3.17491436\n",
      "Iteration 12, loss = 4.76155097\n",
      "Iteration 22, loss = 2.82953209\n",
      "Iteration 13, loss = 5.10406940\n",
      "Iteration 23, loss = 3.35807261\n",
      "Iteration 14, loss = 5.51071070\n",
      "Iteration 24, loss = 3.95158965\n",
      "Iteration 15, loss = 5.17839811\n",
      "Iteration 25, loss = 4.04432797\n",
      "Iteration 16, loss = 5.04684074\n",
      "Iteration 26, loss = 3.73925080\n",
      "Iteration 17, loss = 4.70723262\n",
      "Iteration 27, loss = 3.23666541\n",
      "Iteration 18, loss = 3.71154412\n",
      "Iteration 28, loss = 3.01290529\n",
      "Iteration 19, loss = 4.40411004\n",
      "Iteration 29, loss = 3.53935101\n",
      "Iteration 20, loss = 3.12662143\n",
      "Iteration 30, loss = 2.76537664\n",
      "Iteration 21, loss = 3.52156338\n",
      "Iteration 31, loss = 2.60997505\n",
      "Iteration 22, loss = 3.06084696\n",
      "Iteration 32, loss = 2.53700083\n",
      "Iteration 23, loss = 3.62820118\n",
      "Iteration 33, loss = 2.10629014\n",
      "Iteration 24, loss = 3.20691183\n",
      "Iteration 34, loss = 2.99464634\n",
      "Iteration 25, loss = 3.13085506\n",
      "Iteration 35, loss = 2.36385684\n",
      "Iteration 26, loss = 3.55923733\n",
      "Iteration 36, loss = 2.03571485\n",
      "Iteration 27, loss = 2.65920057\n",
      "Iteration 37, loss = 1.78471357\n",
      "Iteration 28, loss = 2.49835171\n",
      "Iteration 38, loss = 2.26170871\n",
      "Iteration 29, loss = 2.22984087\n",
      "Iteration 39, loss = 1.57112416\n",
      "Iteration 30, loss = 3.17909963\n",
      "Iteration 40, loss = 1.89611536\n",
      "Iteration 31, loss = 3.12046313\n",
      "Iteration 41, loss = 1.27125761\n",
      "Iteration 32, loss = 1.90199356\n",
      "Iteration 42, loss = 1.53947200\n",
      "Iteration 33, loss = 2.19686338\n",
      "Iteration 43, loss = 1.54524829\n",
      "Iteration 34, loss = 2.25036829\n",
      "Iteration 44, loss = 1.49871622\n",
      "Iteration 35, loss = 1.76222899\n",
      "Iteration 45, loss = 1.89922413\n",
      "Iteration 36, loss = 2.10847736\n",
      "Iteration 46, loss = 2.01455202\n",
      "Iteration 37, loss = 1.86247284\n",
      "Iteration 47, loss = 1.24761057\n",
      "Iteration 38, loss = 2.56210026\n",
      "Iteration 48, loss = 1.76054515\n",
      "Iteration 39, loss = 2.12358114\n",
      "Iteration 49, loss = 1.62618540\n",
      "Iteration 40, loss = 2.66043266\n",
      "Iteration 50, loss = 1.87377984\n",
      "Iteration 41, loss = 1.96036143\n",
      "Iteration 51, loss = 1.35605529\n",
      "Iteration 42, loss = 2.53445036\n",
      "Iteration 52, loss = 1.55479274\n",
      "Iteration 43, loss = 2.29228348\n",
      "Iteration 53, loss = 1.38855287\n",
      "Iteration 44, loss = 2.48798959\n",
      "Iteration 54, loss = 1.30721102\n",
      "Iteration 45, loss = 1.73486356\n",
      "Iteration 55, loss = 1.56003874\n",
      "Iteration 46, loss = 1.72194608\n",
      "Iteration 56, loss = 1.35867042\n",
      "Iteration 47, loss = 2.21182588\n",
      "Iteration 57, loss = 1.17529631\n",
      "Iteration 48, loss = 1.95862303\n",
      "Iteration 58, loss = 0.93608970\n",
      "Iteration 49, loss = 1.68281063\n",
      "Iteration 59, loss = 1.15305314\n",
      "Iteration 50, loss = 1.10757021\n",
      "Iteration 60, loss = 1.55441343\n",
      "Iteration 51, loss = 1.31042066\n",
      "Iteration 61, loss = 1.37264989\n",
      "Iteration 52, loss = 1.29190286\n",
      "Iteration 62, loss = 0.87917757\n",
      "Iteration 53, loss = 1.41637296\n",
      "Iteration 63, loss = 1.45284149\n",
      "Iteration 54, loss = 1.57653603\n",
      "Iteration 64, loss = 2.41301616\n",
      "Iteration 55, loss = 1.18488542\n",
      "Iteration 65, loss = 3.09317801\n",
      "Iteration 56, loss = 2.13789781\n",
      "Iteration 66, loss = 2.65788618\n",
      "Iteration 57, loss = 1.73015228\n",
      "Iteration 67, loss = 2.26654453\n",
      "Iteration 58, loss = 1.91830335\n",
      "Iteration 68, loss = 2.32334005\n",
      "Iteration 59, loss = 2.30904867\n",
      "Iteration 69, loss = 2.43051267\n",
      "Iteration 60, loss = 2.58725302\n",
      "Iteration 70, loss = 1.60754148\n",
      "Iteration 61, loss = 3.61378487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 71, loss = 2.42560184\n",
      "Iteration 72, loss = 2.45614944\n",
      "Iteration 73, loss = 1.98435053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.42362859\n",
      "Iteration 2, loss = 15.85435857\n",
      "Iteration 3, loss = 11.54534370\n",
      "Iteration 4, loss = 7.29338029\n",
      "Iteration 1, loss = 19.11001798\n",
      "Iteration 5, loss = 8.17375515\n",
      "Iteration 2, loss = 17.25547147\n",
      "Iteration 6, loss = 8.04626670\n",
      "Iteration 3, loss = 12.76331879\n",
      "Iteration 7, loss = 7.72861594\n",
      "Iteration 4, loss = 11.65147626\n",
      "Iteration 8, loss = 6.52489863\n",
      "Iteration 5, loss = 8.97822903\n",
      "Iteration 9, loss = 7.77162281\n",
      "Iteration 6, loss = 7.09847771\n",
      "Iteration 10, loss = 7.49924744\n",
      "Iteration 7, loss = 6.98604185\n",
      "Iteration 11, loss = 8.40729479\n",
      "Iteration 8, loss = 6.28544603\n",
      "Iteration 12, loss = 8.43200784\n",
      "Iteration 9, loss = 7.00695146\n",
      "Iteration 13, loss = 4.92486562\n",
      "Iteration 10, loss = 9.39143910\n",
      "Iteration 14, loss = 4.32751624\n",
      "Iteration 11, loss = 7.87109670\n",
      "Iteration 15, loss = 4.01620280\n",
      "Iteration 12, loss = 6.22893321\n",
      "Iteration 16, loss = 3.70359843\n",
      "Iteration 13, loss = 5.56413515\n",
      "Iteration 17, loss = 3.85112433\n",
      "Iteration 14, loss = 6.08590875\n",
      "Iteration 18, loss = 4.65987909\n",
      "Iteration 15, loss = 10.32336939\n",
      "Iteration 19, loss = 3.88073477\n",
      "Iteration 16, loss = 7.69812260\n",
      "Iteration 20, loss = 3.48061287\n",
      "Iteration 17, loss = 5.60715323\n",
      "Iteration 21, loss = 2.76469050\n",
      "Iteration 18, loss = 5.07813539\n",
      "Iteration 22, loss = 3.30375730\n",
      "Iteration 19, loss = 5.09662848\n",
      "Iteration 23, loss = 3.39939033\n",
      "Iteration 20, loss = 3.83969360\n",
      "Iteration 24, loss = 2.41619928\n",
      "Iteration 21, loss = 4.41588209\n",
      "Iteration 25, loss = 2.93020651\n",
      "Iteration 22, loss = 3.94289073\n",
      "Iteration 26, loss = 2.39121626\n",
      "Iteration 23, loss = 3.18509547\n",
      "Iteration 27, loss = 2.44684747\n",
      "Iteration 24, loss = 3.15648160\n",
      "Iteration 28, loss = 2.44696561\n",
      "Iteration 25, loss = 2.83163605\n",
      "Iteration 29, loss = 2.55749544\n",
      "Iteration 26, loss = 3.31507723\n",
      "Iteration 30, loss = 2.91171688\n",
      "Iteration 27, loss = 2.99757903\n",
      "Iteration 31, loss = 2.51598448\n",
      "Iteration 28, loss = 3.15615163\n",
      "Iteration 32, loss = 2.07852943\n",
      "Iteration 29, loss = 3.16293376\n",
      "Iteration 33, loss = 2.07366836\n",
      "Iteration 30, loss = 2.90068184\n",
      "Iteration 34, loss = 2.45010060\n",
      "Iteration 31, loss = 2.81670635\n",
      "Iteration 35, loss = 2.05050575\n",
      "Iteration 32, loss = 2.21010513\n",
      "Iteration 36, loss = 2.27520946\n",
      "Iteration 33, loss = 1.91167888\n",
      "Iteration 37, loss = 2.78241310\n",
      "Iteration 34, loss = 2.03194890\n",
      "Iteration 38, loss = 2.56500347\n",
      "Iteration 35, loss = 1.70427241\n",
      "Iteration 39, loss = 2.53589104\n",
      "Iteration 36, loss = 1.92392114\n",
      "Iteration 40, loss = 2.69761656\n",
      "Iteration 37, loss = 1.97138678\n",
      "Iteration 41, loss = 2.69126161\n",
      "Iteration 38, loss = 2.09683075\n",
      "Iteration 42, loss = 2.19244403\n",
      "Iteration 39, loss = 1.80364119\n",
      "Iteration 43, loss = 2.61828171\n",
      "Iteration 40, loss = 1.93734250\n",
      "Iteration 44, loss = 3.01915544\n",
      "Iteration 41, loss = 1.76553976\n",
      "Iteration 45, loss = 2.18242484\n",
      "Iteration 42, loss = 1.76514614\n",
      "Iteration 46, loss = 2.44941951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 2.19555781\n",
      "Iteration 44, loss = 1.52472832\n",
      "Iteration 45, loss = 2.05555746\n",
      "Iteration 46, loss = 1.78910252\n",
      "Iteration 47, loss = 1.85474286\n",
      "Iteration 48, loss = 1.60903001\n",
      "Iteration 49, loss = 1.74251918\n",
      "Iteration 50, loss = 1.77609133\n",
      "Iteration 51, loss = 1.31461952\n",
      "Iteration 52, loss = 1.68135491\n",
      "Iteration 53, loss = 1.98872603\n",
      "Iteration 54, loss = 1.12580689\n",
      "Iteration 1, loss = 17.73475577\n",
      "Iteration 55, loss = 1.41619187\n",
      "Iteration 2, loss = 16.08643065\n",
      "Iteration 56, loss = 1.27099799\n",
      "Iteration 3, loss = 9.93502559\n",
      "Iteration 57, loss = 1.65164574\n",
      "Iteration 4, loss = 8.45909979\n",
      "Iteration 58, loss = 1.44800531\n",
      "Iteration 5, loss = 7.44958095\n",
      "Iteration 59, loss = 1.95876902\n",
      "Iteration 6, loss = 6.48523638\n",
      "Iteration 60, loss = 2.24929935\n",
      "Iteration 7, loss = 7.24101617\n",
      "Iteration 61, loss = 2.47158937\n",
      "Iteration 8, loss = 8.81544262\n",
      "Iteration 62, loss = 2.20643870\n",
      "Iteration 9, loss = 7.67033727\n",
      "Iteration 63, loss = 1.91114876\n",
      "Iteration 10, loss = 6.52886804\n",
      "Iteration 64, loss = 1.92487960\n",
      "Iteration 11, loss = 6.38678636\n",
      "Iteration 65, loss = 2.00071066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 4.24770106\n",
      "Iteration 13, loss = 4.77616442\n",
      "Iteration 14, loss = 3.84491655\n",
      "Iteration 15, loss = 4.29700136\n",
      "Iteration 16, loss = 3.71094880\n",
      "Iteration 17, loss = 4.43700239\n",
      "Iteration 18, loss = 5.20436580\n",
      "Iteration 19, loss = 3.46271618\n",
      "Iteration 20, loss = 3.60730583\n",
      "Iteration 21, loss = 3.60901664\n",
      "Iteration 22, loss = 3.33635988\n",
      "Iteration 23, loss = 4.72631196\n",
      "Iteration 1, loss = 18.01935484\n",
      "Iteration 24, loss = 4.65958794\n",
      "Iteration 2, loss = 12.44230079\n",
      "Iteration 25, loss = 5.79477014\n",
      "Iteration 3, loss = 11.23471186\n",
      "Iteration 26, loss = 6.85146593\n",
      "Iteration 4, loss = 10.70612491\n",
      "Iteration 27, loss = 5.28780973\n",
      "Iteration 5, loss = 8.19836032\n",
      "Iteration 28, loss = 3.03455868\n",
      "Iteration 6, loss = 5.37245915\n",
      "Iteration 29, loss = 3.39297224\n",
      "Iteration 7, loss = 6.00720427\n",
      "Iteration 30, loss = 2.88316348\n",
      "Iteration 8, loss = 5.32375492\n",
      "Iteration 31, loss = 2.31012259\n",
      "Iteration 9, loss = 6.17657861\n",
      "Iteration 32, loss = 2.63739223\n",
      "Iteration 10, loss = 6.98102902\n",
      "Iteration 33, loss = 3.54605223\n",
      "Iteration 11, loss = 6.52540245\n",
      "Iteration 34, loss = 3.16951813\n",
      "Iteration 12, loss = 4.85597547\n",
      "Iteration 35, loss = 2.05146608\n",
      "Iteration 13, loss = 4.48879794\n",
      "Iteration 36, loss = 2.28722409\n",
      "Iteration 14, loss = 4.19390585\n",
      "Iteration 37, loss = 2.89974734\n",
      "Iteration 15, loss = 3.52405096\n",
      "Iteration 38, loss = 2.12025153\n",
      "Iteration 16, loss = 5.22024714\n",
      "Iteration 39, loss = 2.10187297\n",
      "Iteration 17, loss = 6.66769577\n",
      "Iteration 40, loss = 3.21152474\n",
      "Iteration 18, loss = 6.09219938\n",
      "Iteration 41, loss = 3.33093046\n",
      "Iteration 19, loss = 3.95293855\n",
      "Iteration 42, loss = 3.25438983\n",
      "Iteration 20, loss = 4.24957130\n",
      "Iteration 43, loss = 2.87839400\n",
      "Iteration 21, loss = 4.09667779\n",
      "Iteration 44, loss = 1.95207246\n",
      "Iteration 22, loss = 3.44630231\n",
      "Iteration 45, loss = 1.72758059\n",
      "Iteration 23, loss = 3.01703754\n",
      "Iteration 46, loss = 1.68708415\n",
      "Iteration 24, loss = 3.06225375\n",
      "Iteration 47, loss = 1.52461843\n",
      "Iteration 25, loss = 2.95026786\n",
      "Iteration 48, loss = 1.69824420\n",
      "Iteration 26, loss = 2.49391750\n",
      "Iteration 49, loss = 1.99873183\n",
      "Iteration 27, loss = 2.84389683\n",
      "Iteration 50, loss = 1.38116021\n",
      "Iteration 28, loss = 4.04703898\n",
      "Iteration 51, loss = 2.38790510\n",
      "Iteration 29, loss = 2.87241556\n",
      "Iteration 52, loss = 2.10398486\n",
      "Iteration 30, loss = 3.37986236\n",
      "Iteration 53, loss = 1.71741489\n",
      "Iteration 31, loss = 3.13735590\n",
      "Iteration 54, loss = 2.51316061\n",
      "Iteration 32, loss = 3.23215459\n",
      "Iteration 55, loss = 1.98971733\n",
      "Iteration 33, loss = 3.08893234\n",
      "Iteration 56, loss = 1.84884908\n",
      "Iteration 34, loss = 2.23783242\n",
      "Iteration 57, loss = 1.71442667\n",
      "Iteration 35, loss = 2.32879039\n",
      "Iteration 58, loss = 1.27581511\n",
      "Iteration 36, loss = 2.31387445\n",
      "Iteration 59, loss = 1.94586483\n",
      "Iteration 37, loss = 2.57295409\n",
      "Iteration 60, loss = 2.06148131\n",
      "Iteration 38, loss = 2.82127227\n",
      "Iteration 61, loss = 1.42094300\n",
      "Iteration 39, loss = 3.64027240\n",
      "Iteration 62, loss = 1.85148231\n",
      "Iteration 40, loss = 3.06418549\n",
      "Iteration 63, loss = 1.84077736\n",
      "Iteration 41, loss = 2.44721709\n",
      "Iteration 64, loss = 2.08686262\n",
      "Iteration 42, loss = 4.26110477\n",
      "Iteration 65, loss = 1.71362829\n",
      "Iteration 43, loss = 3.52926471\n",
      "Iteration 66, loss = 1.80007755\n",
      "Iteration 44, loss = 2.47816265\n",
      "Iteration 67, loss = 2.28720049\n",
      "Iteration 45, loss = 2.87319912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 68, loss = 2.21895791\n",
      "Iteration 69, loss = 2.34015036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.78196294\n",
      "Iteration 2, loss = 17.30691528\n",
      "Iteration 1, loss = 19.89971769\n",
      "Iteration 3, loss = 10.08566094\n",
      "Iteration 2, loss = 14.75746162\n",
      "Iteration 4, loss = 12.50231813\n",
      "Iteration 3, loss = 12.02459269\n",
      "Iteration 5, loss = 10.18901905\n",
      "Iteration 4, loss = 6.68139943\n",
      "Iteration 6, loss = 9.74760610\n",
      "Iteration 5, loss = 6.11142339\n",
      "Iteration 7, loss = 8.55424250\n",
      "Iteration 6, loss = 6.78909637\n",
      "Iteration 8, loss = 7.12544016\n",
      "Iteration 7, loss = 7.25048929\n",
      "Iteration 9, loss = 5.69894163\n",
      "Iteration 8, loss = 6.99137966\n",
      "Iteration 10, loss = 5.29686274\n",
      "Iteration 9, loss = 7.00056068\n",
      "Iteration 11, loss = 4.57844996\n",
      "Iteration 10, loss = 5.49873799\n",
      "Iteration 12, loss = 3.69764853\n",
      "Iteration 11, loss = 4.70267356\n",
      "Iteration 13, loss = 4.70352023\n",
      "Iteration 12, loss = 4.73342194\n",
      "Iteration 14, loss = 3.94238728\n",
      "Iteration 13, loss = 4.42327121\n",
      "Iteration 15, loss = 3.34460540\n",
      "Iteration 14, loss = 4.36885173\n",
      "Iteration 16, loss = 3.59551221\n",
      "Iteration 15, loss = 4.20547541\n",
      "Iteration 17, loss = 4.71000143\n",
      "Iteration 16, loss = 3.32635839\n",
      "Iteration 18, loss = 4.72911914\n",
      "Iteration 17, loss = 3.34101993\n",
      "Iteration 19, loss = 5.07749321\n",
      "Iteration 18, loss = 2.95287366\n",
      "Iteration 20, loss = 4.06807892\n",
      "Iteration 19, loss = 3.54998586\n",
      "Iteration 21, loss = 2.95124758\n",
      "Iteration 20, loss = 2.93756988\n",
      "Iteration 22, loss = 3.60748142\n",
      "Iteration 21, loss = 2.70848636\n",
      "Iteration 23, loss = 2.84033351\n",
      "Iteration 22, loss = 4.19630341\n",
      "Iteration 24, loss = 2.94437845\n",
      "Iteration 23, loss = 3.07521539\n",
      "Iteration 25, loss = 3.04673057\n",
      "Iteration 24, loss = 2.70608684\n",
      "Iteration 26, loss = 2.32668435\n",
      "Iteration 25, loss = 3.21644428\n",
      "Iteration 27, loss = 2.12806689\n",
      "Iteration 26, loss = 2.64552437\n",
      "Iteration 28, loss = 2.23764013\n",
      "Iteration 27, loss = 3.40758423\n",
      "Iteration 29, loss = 1.67113706\n",
      "Iteration 28, loss = 3.03755636\n",
      "Iteration 30, loss = 1.46462764\n",
      "Iteration 29, loss = 3.32986146\n",
      "Iteration 31, loss = 1.98623596\n",
      "Iteration 30, loss = 2.48217903\n",
      "Iteration 32, loss = 1.86791263\n",
      "Iteration 31, loss = 2.63364065\n",
      "Iteration 33, loss = 1.45097442\n",
      "Iteration 32, loss = 2.58452222\n",
      "Iteration 34, loss = 1.72183808\n",
      "Iteration 33, loss = 2.52997056\n",
      "Iteration 35, loss = 2.27866564\n",
      "Iteration 34, loss = 1.65489993\n",
      "Iteration 36, loss = 1.89771190\n",
      "Iteration 35, loss = 1.95904276\n",
      "Iteration 37, loss = 1.75579367\n",
      "Iteration 36, loss = 1.52232200\n",
      "Iteration 38, loss = 2.63515246\n",
      "Iteration 37, loss = 1.21603520\n",
      "Iteration 39, loss = 2.12196431\n",
      "Iteration 38, loss = 2.24057858\n",
      "Iteration 40, loss = 1.71129855\n",
      "Iteration 39, loss = 2.16919781\n",
      "Iteration 41, loss = 1.15631673\n",
      "Iteration 40, loss = 2.18425731\n",
      "Iteration 42, loss = 1.20204022\n",
      "Iteration 41, loss = 1.90696869\n",
      "Iteration 43, loss = 1.64149923\n",
      "Iteration 42, loss = 1.74381984\n",
      "Iteration 44, loss = 1.30646656\n",
      "Iteration 43, loss = 1.57260666\n",
      "Iteration 45, loss = 1.58936975\n",
      "Iteration 44, loss = 1.36159976\n",
      "Iteration 46, loss = 0.96089500\n",
      "Iteration 45, loss = 1.16889501\n",
      "Iteration 47, loss = 0.96516255\n",
      "Iteration 46, loss = 1.41174131\n",
      "Iteration 48, loss = 0.64904305\n",
      "Iteration 47, loss = 1.66538239\n",
      "Iteration 49, loss = 0.99556948\n",
      "Iteration 48, loss = 2.02448515\n",
      "Iteration 50, loss = 1.03651730\n",
      "Iteration 49, loss = 1.38621408\n",
      "Iteration 51, loss = 0.91889485\n",
      "Iteration 50, loss = 1.18387034\n",
      "Iteration 52, loss = 0.54416121\n",
      "Iteration 51, loss = 1.22725807\n",
      "Iteration 53, loss = 0.92393631\n",
      "Iteration 52, loss = 1.27056896\n",
      "Iteration 54, loss = 1.05552074\n",
      "Iteration 53, loss = 1.31916837\n",
      "Iteration 55, loss = 0.88310985\n",
      "Iteration 54, loss = 1.48015068\n",
      "Iteration 56, loss = 0.92348843\n",
      "Iteration 55, loss = 2.10633728\n",
      "Iteration 57, loss = 0.76966309\n",
      "Iteration 56, loss = 1.82819421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.44637537\n",
      "Iteration 59, loss = 1.39704838\n",
      "Iteration 60, loss = 1.28162355\n",
      "Iteration 61, loss = 1.47275187\n",
      "Iteration 62, loss = 2.18077708\n",
      "Iteration 63, loss = 1.49024836\n",
      "Iteration 64, loss = 1.31859207\n",
      "Iteration 65, loss = 0.87877571\n",
      "Iteration 66, loss = 0.80272108\n",
      "Iteration 67, loss = 0.73921822\n",
      "Iteration 68, loss = 0.71372471\n",
      "Iteration 69, loss = 0.42491883\n",
      "Iteration 1, loss = 16.39728563\n",
      "Iteration 70, loss = 0.48188087\n",
      "Iteration 2, loss = 19.58210969\n",
      "Iteration 71, loss = 0.59763276\n",
      "Iteration 3, loss = 10.45035510\n",
      "Iteration 72, loss = 0.52088357\n",
      "Iteration 4, loss = 11.00673096\n",
      "Iteration 73, loss = 0.23070007\n",
      "Iteration 5, loss = 8.26887771\n",
      "Iteration 74, loss = 0.44943309\n",
      "Iteration 6, loss = 8.47484726\n",
      "Iteration 75, loss = 0.86180163\n",
      "Iteration 7, loss = 11.41505688\n",
      "Iteration 76, loss = 1.20402230\n",
      "Iteration 8, loss = 6.84056715\n",
      "Iteration 77, loss = 0.68381545\n",
      "Iteration 9, loss = 6.15245064\n",
      "Iteration 78, loss = 0.66577044\n",
      "Iteration 10, loss = 9.55514378\n",
      "Iteration 79, loss = 0.61243087\n",
      "Iteration 11, loss = 8.49517879\n",
      "Iteration 80, loss = 0.68094377\n",
      "Iteration 12, loss = 7.17044871\n",
      "Iteration 81, loss = 1.05694600\n",
      "Iteration 13, loss = 5.44790838\n",
      "Iteration 82, loss = 0.61008325\n",
      "Iteration 14, loss = 4.72146757\n",
      "Iteration 83, loss = 0.74328174\n",
      "Iteration 15, loss = 5.61000941\n",
      "Iteration 84, loss = 0.57889150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 5.15652755\n",
      "Iteration 17, loss = 4.69087640\n",
      "Iteration 18, loss = 4.67748709\n",
      "Iteration 19, loss = 4.03100450\n",
      "Iteration 20, loss = 3.67295619\n",
      "Iteration 21, loss = 4.09835128\n",
      "Iteration 22, loss = 3.39637593\n",
      "Iteration 23, loss = 3.85060875\n",
      "Iteration 24, loss = 3.70332812\n",
      "Iteration 25, loss = 3.73725900\n",
      "Iteration 26, loss = 3.13633387\n",
      "Iteration 27, loss = 4.28493240\n",
      "Iteration 28, loss = 4.28698445\n",
      "Iteration 1, loss = 15.93165211\n",
      "Iteration 29, loss = 3.42644901\n",
      "Iteration 2, loss = 14.73574089\n",
      "Iteration 3, loss = 10.93440873\n",
      "Iteration 30, loss = 3.24628622\n",
      "Iteration 4, loss = 8.35036151\n",
      "Iteration 31, loss = 2.70362119\n",
      "Iteration 5, loss = 6.47295072\n",
      "Iteration 32, loss = 2.97106724\n",
      "Iteration 6, loss = 6.46226459\n",
      "Iteration 33, loss = 3.53726750\n",
      "Iteration 7, loss = 9.77180741\n",
      "Iteration 34, loss = 3.23827881\n",
      "Iteration 8, loss = 6.06054545\n",
      "Iteration 35, loss = 3.44044021\n",
      "Iteration 9, loss = 5.86663206\n",
      "Iteration 36, loss = 2.87822207\n",
      "Iteration 10, loss = 4.60615452\n",
      "Iteration 37, loss = 3.64143823\n",
      "Iteration 11, loss = 5.41923580\n",
      "Iteration 38, loss = 3.01138859\n",
      "Iteration 12, loss = 8.15069994\n",
      "Iteration 39, loss = 2.92863945\n",
      "Iteration 40, loss = 2.88022259\n",
      "Iteration 13, loss = 5.96460319\n",
      "Iteration 14, loss = 5.32433744\n",
      "Iteration 41, loss = 3.01396783\n",
      "Iteration 15, loss = 4.88275164\n",
      "Iteration 42, loss = 2.52617612\n",
      "Iteration 16, loss = 3.92022420\n",
      "Iteration 43, loss = 2.69779288\n",
      "Iteration 17, loss = 4.05015385\n",
      "Iteration 44, loss = 2.59572096\n",
      "Iteration 18, loss = 3.40108628\n",
      "Iteration 45, loss = 2.97703349\n",
      "Iteration 19, loss = 2.91171364\n",
      "Iteration 46, loss = 2.24370017\n",
      "Iteration 20, loss = 3.05363662\n",
      "Iteration 47, loss = 2.91148927\n",
      "Iteration 21, loss = 3.42179388\n",
      "Iteration 48, loss = 3.34534696\n",
      "Iteration 22, loss = 3.27887445\n",
      "Iteration 49, loss = 2.38811275\n",
      "Iteration 23, loss = 3.06266175\n",
      "Iteration 50, loss = 2.31919421\n",
      "Iteration 24, loss = 3.80823874\n",
      "Iteration 51, loss = 2.27687097\n",
      "Iteration 25, loss = 4.06456266\n",
      "Iteration 52, loss = 2.06649784\n",
      "Iteration 26, loss = 3.36857023\n",
      "Iteration 53, loss = 2.01984736\n",
      "Iteration 27, loss = 2.91665267\n",
      "Iteration 54, loss = 1.64388223\n",
      "Iteration 28, loss = 3.02414502\n",
      "Iteration 55, loss = 1.66759791\n",
      "Iteration 29, loss = 2.44298882\n",
      "Iteration 56, loss = 1.45294504\n",
      "Iteration 30, loss = 2.40058318\n",
      "Iteration 57, loss = 1.66961701\n",
      "Iteration 31, loss = 2.37221122\n",
      "Iteration 58, loss = 1.98255663\n",
      "Iteration 32, loss = 2.03687889\n",
      "Iteration 59, loss = 1.72171216\n",
      "Iteration 33, loss = 2.28790746\n",
      "Iteration 60, loss = 1.88967615\n",
      "Iteration 34, loss = 2.42022489\n",
      "Iteration 61, loss = 1.73617436\n",
      "Iteration 35, loss = 2.97109213\n",
      "Iteration 62, loss = 2.45403139\n",
      "Iteration 36, loss = 2.64990846\n",
      "Iteration 63, loss = 2.32629737\n",
      "Iteration 37, loss = 2.23727416\n",
      "Iteration 64, loss = 2.23248597\n",
      "Iteration 38, loss = 2.16192799\n",
      "Iteration 65, loss = 2.35784301\n",
      "Iteration 39, loss = 1.72721124\n",
      "Iteration 66, loss = 3.01102587\n",
      "Iteration 40, loss = 1.68015342\n",
      "Iteration 67, loss = 2.29369040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 1.90543145\n",
      "Iteration 42, loss = 1.80640935\n",
      "Iteration 43, loss = 1.91501418\n",
      "Iteration 44, loss = 1.96335574\n",
      "Iteration 45, loss = 3.46372933\n",
      "Iteration 46, loss = 4.15152497\n",
      "Iteration 47, loss = 2.58383813\n",
      "Iteration 48, loss = 1.92366487\n",
      "Iteration 49, loss = 3.04222163\n",
      "Iteration 50, loss = 2.10601870\n",
      "Iteration 51, loss = 2.03672805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.16316228\n",
      "Iteration 2, loss = 13.89283940\n",
      "Iteration 3, loss = 9.76058343\n",
      "Iteration 4, loss = 8.97218261\n",
      "Iteration 5, loss = 7.65483324\n",
      "Iteration 6, loss = 7.13924548\n",
      "Iteration 7, loss = 7.12420662\n",
      "Iteration 8, loss = 6.58316650\n",
      "Iteration 9, loss = 6.03161448\n",
      "Iteration 10, loss = 5.06674315\n",
      "Iteration 11, loss = 6.38048489\n",
      "Iteration 1, loss = 18.23787310\n",
      "Iteration 12, loss = 5.83875437\n",
      "Iteration 2, loss = 16.44757531\n",
      "Iteration 13, loss = 9.05499565\n",
      "Iteration 3, loss = 12.32653066\n",
      "Iteration 14, loss = 9.52176048\n",
      "Iteration 4, loss = 11.18887310\n",
      "Iteration 15, loss = 5.19676098\n",
      "Iteration 5, loss = 10.82370628\n",
      "Iteration 16, loss = 4.81571062\n",
      "Iteration 6, loss = 7.40836229\n",
      "Iteration 17, loss = 4.04512355\n",
      "Iteration 7, loss = 8.68115153\n",
      "Iteration 18, loss = 4.17163059\n",
      "Iteration 8, loss = 9.20796648\n",
      "Iteration 19, loss = 4.47333497\n",
      "Iteration 9, loss = 7.59765891\n",
      "Iteration 20, loss = 5.83984653\n",
      "Iteration 10, loss = 6.83100681\n",
      "Iteration 21, loss = 4.23763686\n",
      "Iteration 11, loss = 6.97968066\n",
      "Iteration 22, loss = 3.42621994\n",
      "Iteration 12, loss = 5.82769680\n",
      "Iteration 23, loss = 3.88718406\n",
      "Iteration 13, loss = 4.36318050\n",
      "Iteration 24, loss = 3.54369900\n",
      "Iteration 14, loss = 7.10565052\n",
      "Iteration 25, loss = 2.46982542\n",
      "Iteration 15, loss = 5.46393239\n",
      "Iteration 26, loss = 4.11894209\n",
      "Iteration 16, loss = 4.42069527\n",
      "Iteration 27, loss = 2.96963860\n",
      "Iteration 17, loss = 4.68055167\n",
      "Iteration 28, loss = 2.11288980\n",
      "Iteration 18, loss = 5.09488057\n",
      "Iteration 29, loss = 2.51715668\n",
      "Iteration 19, loss = 4.89284298\n",
      "Iteration 30, loss = 2.18610368\n",
      "Iteration 20, loss = 4.26486052\n",
      "Iteration 31, loss = 2.01494010\n",
      "Iteration 21, loss = 3.28781833\n",
      "Iteration 32, loss = 2.52593006\n",
      "Iteration 22, loss = 5.47609752\n",
      "Iteration 33, loss = 1.99958304\n",
      "Iteration 23, loss = 4.26601578\n",
      "Iteration 34, loss = 2.62092612\n",
      "Iteration 24, loss = 9.35508647\n",
      "Iteration 35, loss = 3.02921810\n",
      "Iteration 36, loss = 2.43180428\n",
      "Iteration 25, loss = 8.52738638\n",
      "Iteration 26, loss = 8.07457965\n",
      "Iteration 37, loss = 3.73025321\n",
      "Iteration 27, loss = 4.61627921Iteration 38, loss = 3.80136707\n",
      "\n",
      "Iteration 28, loss = 3.97783904\n",
      "Iteration 39, loss = 4.03210593\n",
      "Iteration 29, loss = 4.99172796\n",
      "Iteration 40, loss = 3.21964228\n",
      "Iteration 30, loss = 4.93540419\n",
      "Iteration 41, loss = 2.75896968\n",
      "Iteration 31, loss = 4.87259621\n",
      "Iteration 42, loss = 2.72889511\n",
      "Iteration 32, loss = 5.09616250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 2.32978706\n",
      "Iteration 44, loss = 1.87826005\n",
      "Iteration 45, loss = 1.67574812\n",
      "Iteration 46, loss = 1.48074284\n",
      "Iteration 47, loss = 1.04504171\n",
      "Iteration 48, loss = 1.03462461\n",
      "Iteration 49, loss = 1.03794875\n",
      "Iteration 50, loss = 0.79301444\n",
      "Iteration 51, loss = 0.92118842\n",
      "Iteration 52, loss = 0.95259471\n",
      "Iteration 53, loss = 0.63990193\n",
      "Iteration 54, loss = 0.70803044\n",
      "Iteration 55, loss = 1.16584413\n",
      "Iteration 1, loss = 18.51994688\n",
      "Iteration 56, loss = 1.02455388\n",
      "Iteration 2, loss = 14.56588971\n",
      "Iteration 57, loss = 0.79253318\n",
      "Iteration 3, loss = 16.52705931\n",
      "Iteration 58, loss = 0.86096334\n",
      "Iteration 4, loss = 10.68890814\n",
      "Iteration 59, loss = 1.16689737\n",
      "Iteration 5, loss = 7.84719062\n",
      "Iteration 60, loss = 1.00515947\n",
      "Iteration 6, loss = 7.82928807\n",
      "Iteration 61, loss = 1.00484776\n",
      "Iteration 7, loss = 5.72556472\n",
      "Iteration 62, loss = 0.89951053\n",
      "Iteration 8, loss = 7.17766900\n",
      "Iteration 63, loss = 0.70343428\n",
      "Iteration 9, loss = 8.39072551\n",
      "Iteration 64, loss = 0.44042883\n",
      "Iteration 10, loss = 8.08404907\n",
      "Iteration 65, loss = 0.62278459\n",
      "Iteration 11, loss = 6.43518330\n",
      "Iteration 66, loss = 0.62365476\n",
      "Iteration 12, loss = 5.11577570\n",
      "Iteration 67, loss = 0.76382470\n",
      "Iteration 13, loss = 4.68522981\n",
      "Iteration 68, loss = 0.59979828\n",
      "Iteration 14, loss = 4.65489539\n",
      "Iteration 69, loss = 0.74233906\n",
      "Iteration 15, loss = 4.98284039\n",
      "Iteration 70, loss = 0.73268701\n",
      "Iteration 16, loss = 4.79136862\n",
      "Iteration 71, loss = 0.59969556\n",
      "Iteration 17, loss = 3.71093648\n",
      "Iteration 72, loss = 0.81076821\n",
      "Iteration 18, loss = 4.49995127\n",
      "Iteration 73, loss = 0.83073997\n",
      "Iteration 19, loss = 3.68470330\n",
      "Iteration 74, loss = 1.20590861\n",
      "Iteration 20, loss = 3.98348650\n",
      "Iteration 75, loss = 1.02101862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 3.26622502\n",
      "Iteration 22, loss = 3.86021072\n",
      "Iteration 23, loss = 2.75234366\n",
      "Iteration 24, loss = 2.96674617\n",
      "Iteration 25, loss = 3.48107613\n",
      "Iteration 26, loss = 3.62111547\n",
      "Iteration 27, loss = 3.61567039\n",
      "Iteration 28, loss = 2.98956299\n",
      "Iteration 29, loss = 3.04892303\n",
      "Iteration 30, loss = 2.96493406\n",
      "Iteration 31, loss = 2.39497423\n",
      "Iteration 32, loss = 2.40649058\n",
      "Iteration 33, loss = 2.64237832\n",
      "Iteration 1, loss = 17.94731094\n",
      "Iteration 34, loss = 2.74819230\n",
      "Iteration 2, loss = 20.08976824\n",
      "Iteration 35, loss = 3.00396021\n",
      "Iteration 3, loss = 13.24962309\n",
      "Iteration 36, loss = 2.85596662\n",
      "Iteration 4, loss = 7.55136919\n",
      "Iteration 37, loss = 2.32953736\n",
      "Iteration 5, loss = 8.16913048\n",
      "Iteration 38, loss = 2.26330382\n",
      "Iteration 6, loss = 10.55841783\n",
      "Iteration 39, loss = 2.13922794\n",
      "Iteration 7, loss = 8.14300689\n",
      "Iteration 40, loss = 2.62263148\n",
      "Iteration 8, loss = 7.19413400\n",
      "Iteration 9, loss = 6.49682906\n",
      "Iteration 41, loss = 2.13426766\n",
      "Iteration 10, loss = 5.37182153\n",
      "Iteration 42, loss = 2.23387105\n",
      "Iteration 11, loss = 5.33730399\n",
      "Iteration 43, loss = 2.10406181\n",
      "Iteration 12, loss = 4.93625378\n",
      "Iteration 44, loss = 1.81065030\n",
      "Iteration 13, loss = 5.24967089\n",
      "Iteration 45, loss = 1.56302609\n",
      "Iteration 14, loss = 4.84566456\n",
      "Iteration 46, loss = 1.54888463\n",
      "Iteration 15, loss = 5.22144044\n",
      "Iteration 47, loss = 2.14891143\n",
      "Iteration 16, loss = 4.63532110\n",
      "Iteration 48, loss = 1.73630443\n",
      "Iteration 17, loss = 3.37945541\n",
      "Iteration 49, loss = 1.98856499\n",
      "Iteration 18, loss = 4.48315032\n",
      "Iteration 50, loss = 1.91512410\n",
      "Iteration 19, loss = 4.14703492\n",
      "Iteration 51, loss = 1.49208183\n",
      "Iteration 20, loss = 3.38967453\n",
      "Iteration 52, loss = 1.94670125\n",
      "Iteration 21, loss = 3.80075996\n",
      "Iteration 53, loss = 1.84848483\n",
      "Iteration 22, loss = 3.89708137\n",
      "Iteration 54, loss = 1.87389899\n",
      "Iteration 23, loss = 4.47457210\n",
      "Iteration 55, loss = 1.45000186\n",
      "Iteration 24, loss = 3.93038152\n",
      "Iteration 56, loss = 1.65424782\n",
      "Iteration 25, loss = 4.08525057\n",
      "Iteration 57, loss = 1.90967509\n",
      "Iteration 26, loss = 3.61817708\n",
      "Iteration 58, loss = 1.83629939\n",
      "Iteration 27, loss = 3.99189188\n",
      "Iteration 59, loss = 3.23901114\n",
      "Iteration 28, loss = 3.54491472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 3.36780790\n",
      "Iteration 61, loss = 2.99563027\n",
      "Iteration 62, loss = 2.55480955\n",
      "Iteration 63, loss = 2.03201225\n",
      "Iteration 64, loss = 2.14867287\n",
      "Iteration 65, loss = 1.76758254\n",
      "Iteration 66, loss = 2.12241362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.16043814\n",
      "Iteration 2, loss = 13.33487103\n",
      "Iteration 3, loss = 13.19653114\n",
      "Iteration 4, loss = 11.29810570\n",
      "Iteration 5, loss = 10.22299759\n",
      "Iteration 6, loss = 8.12719401\n",
      "Iteration 7, loss = 7.64205981\n",
      "Iteration 1, loss = 15.49651569\n",
      "Iteration 8, loss = 10.37695794\n",
      "Iteration 2, loss = 15.41804787\n",
      "Iteration 9, loss = 8.49606564\n",
      "Iteration 3, loss = 11.39231941\n",
      "Iteration 10, loss = 7.10331414\n",
      "Iteration 4, loss = 9.44279247\n",
      "Iteration 11, loss = 6.63264148\n",
      "Iteration 5, loss = 6.97525082\n",
      "Iteration 12, loss = 5.89656351\n",
      "Iteration 6, loss = 6.99272169\n",
      "Iteration 13, loss = 5.09005075\n",
      "Iteration 7, loss = 7.62355471\n",
      "Iteration 14, loss = 5.29601399\n",
      "Iteration 8, loss = 5.58162443\n",
      "Iteration 15, loss = 5.60078605\n",
      "Iteration 9, loss = 6.04211670\n",
      "Iteration 16, loss = 3.89339366\n",
      "Iteration 10, loss = 7.73891984\n",
      "Iteration 17, loss = 3.55382385\n",
      "Iteration 11, loss = 6.65389044\n",
      "Iteration 18, loss = 3.25963543\n",
      "Iteration 12, loss = 4.82913310\n",
      "Iteration 19, loss = 3.26028519\n",
      "Iteration 13, loss = 4.70381220\n",
      "Iteration 20, loss = 3.13251572\n",
      "Iteration 14, loss = 4.29389447\n",
      "Iteration 21, loss = 3.30521202\n",
      "Iteration 15, loss = 4.02373452\n",
      "Iteration 22, loss = 3.66244041\n",
      "Iteration 16, loss = 4.56890423\n",
      "Iteration 23, loss = 3.77985622\n",
      "Iteration 17, loss = 3.93191986\n",
      "Iteration 24, loss = 3.26262758\n",
      "Iteration 18, loss = 4.68684037\n",
      "Iteration 25, loss = 2.89674731\n",
      "Iteration 19, loss = 4.36613253\n",
      "Iteration 26, loss = 2.23318628\n",
      "Iteration 20, loss = 3.69072383\n",
      "Iteration 27, loss = 2.44063705\n",
      "Iteration 21, loss = 4.00894122\n",
      "Iteration 28, loss = 2.20679991\n",
      "Iteration 22, loss = 4.07358843\n",
      "Iteration 29, loss = 3.02598374\n",
      "Iteration 23, loss = 3.35020389\n",
      "Iteration 30, loss = 2.43167302\n",
      "Iteration 24, loss = 2.45981727\n",
      "Iteration 31, loss = 1.63638303\n",
      "Iteration 25, loss = 3.56244600\n",
      "Iteration 32, loss = 1.88406404\n",
      "Iteration 26, loss = 3.00699188\n",
      "Iteration 33, loss = 1.98827705\n",
      "Iteration 27, loss = 2.86806530\n",
      "Iteration 34, loss = 2.15526549\n",
      "Iteration 28, loss = 3.20564810\n",
      "Iteration 35, loss = 1.65895377\n",
      "Iteration 29, loss = 2.66156402\n",
      "Iteration 36, loss = 1.39929752\n",
      "Iteration 30, loss = 3.64960103\n",
      "Iteration 37, loss = 1.52208405\n",
      "Iteration 31, loss = 2.65225057\n",
      "Iteration 38, loss = 1.52569443\n",
      "Iteration 32, loss = 2.35511211\n",
      "Iteration 39, loss = 0.93992127\n",
      "Iteration 33, loss = 2.24574296\n",
      "Iteration 40, loss = 1.10430214\n",
      "Iteration 34, loss = 2.37134301\n",
      "Iteration 41, loss = 1.27883304\n",
      "Iteration 35, loss = 2.55546647\n",
      "Iteration 42, loss = 1.30789634\n",
      "Iteration 36, loss = 2.00040472\n",
      "Iteration 43, loss = 0.98520571\n",
      "Iteration 37, loss = 2.45030205\n",
      "Iteration 44, loss = 0.86479016\n",
      "Iteration 38, loss = 2.34049465\n",
      "Iteration 45, loss = 1.00237320\n",
      "Iteration 39, loss = 2.60241571\n",
      "Iteration 46, loss = 1.16874898\n",
      "Iteration 40, loss = 2.71062499\n",
      "Iteration 47, loss = 0.91052258\n",
      "Iteration 41, loss = 2.17003644\n",
      "Iteration 48, loss = 0.93439378\n",
      "Iteration 42, loss = 2.28681019\n",
      "Iteration 49, loss = 0.91340401\n",
      "Iteration 43, loss = 2.15436178\n",
      "Iteration 50, loss = 0.96825736\n",
      "Iteration 44, loss = 2.27858205\n",
      "Iteration 51, loss = 0.90030899\n",
      "Iteration 45, loss = 1.60642796\n",
      "Iteration 52, loss = 0.80482297\n",
      "Iteration 46, loss = 1.94154238\n",
      "Iteration 53, loss = 0.98115186\n",
      "Iteration 47, loss = 2.01498757\n",
      "Iteration 54, loss = 1.22439001\n",
      "Iteration 48, loss = 2.06989547\n",
      "Iteration 55, loss = 1.19012812\n",
      "Iteration 49, loss = 2.30128487\n",
      "Iteration 56, loss = 1.00914462\n",
      "Iteration 50, loss = 2.09780364\n",
      "Iteration 57, loss = 0.91439352\n",
      "Iteration 51, loss = 3.39114227\n",
      "Iteration 58, loss = 0.93318289\n",
      "Iteration 52, loss = 2.43372693\n",
      "Iteration 59, loss = 1.18785018\n",
      "Iteration 53, loss = 2.13671711\n",
      "Iteration 60, loss = 1.03053743\n",
      "Iteration 54, loss = 1.91859107\n",
      "Iteration 61, loss = 1.12549432\n",
      "Iteration 55, loss = 2.35659092\n",
      "Iteration 62, loss = 0.80785599\n",
      "Iteration 56, loss = 2.10876036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 63, loss = 0.66275526\n",
      "Iteration 64, loss = 1.01546605\n",
      "Iteration 65, loss = 1.17025295\n",
      "Iteration 66, loss = 1.07678336\n",
      "Iteration 67, loss = 1.23170586\n",
      "Iteration 68, loss = 0.99413073\n",
      "Iteration 69, loss = 1.02272595\n",
      "Iteration 70, loss = 0.94301608\n",
      "Iteration 71, loss = 1.06673557\n",
      "Iteration 72, loss = 1.23090265\n",
      "Iteration 73, loss = 1.21095365\n",
      "Iteration 74, loss = 1.58379011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.36199087\n",
      "Iteration 2, loss = 16.84923416\n",
      "Iteration 3, loss = 13.66487919\n",
      "Iteration 4, loss = 7.90435281\n",
      "Iteration 5, loss = 6.82457645\n",
      "Iteration 6, loss = 7.30268622\n",
      "Iteration 7, loss = 7.05359337\n",
      "Iteration 8, loss = 6.00030504\n",
      "Iteration 9, loss = 5.75566600\n",
      "Iteration 10, loss = 5.28022236\n",
      "Iteration 11, loss = 5.89626336\n",
      "Iteration 12, loss = 4.96908742\n",
      "Iteration 1, loss = 15.80447033\n",
      "Iteration 13, loss = 4.30750813\n",
      "Iteration 2, loss = 9.22260936\n",
      "Iteration 14, loss = 6.79421644\n",
      "Iteration 3, loss = 7.46058600\n",
      "Iteration 15, loss = 5.07263280\n",
      "Iteration 4, loss = 5.60711692\n",
      "Iteration 16, loss = 5.23700548\n",
      "Iteration 5, loss = 6.08645616\n",
      "Iteration 17, loss = 3.81212180\n",
      "Iteration 6, loss = 6.77932524\n",
      "Iteration 18, loss = 3.19131428\n",
      "Iteration 7, loss = 6.26819924\n",
      "Iteration 19, loss = 3.43897297\n",
      "Iteration 8, loss = 4.59131634\n",
      "Iteration 20, loss = 4.76884680\n",
      "Iteration 9, loss = 5.45225856\n",
      "Iteration 21, loss = 3.48632360\n",
      "Iteration 10, loss = 4.78962375\n",
      "Iteration 22, loss = 3.35016303\n",
      "Iteration 11, loss = 5.17730691\n",
      "Iteration 23, loss = 3.39834876\n",
      "Iteration 12, loss = 5.94665282\n",
      "Iteration 24, loss = 4.09480550\n",
      "Iteration 13, loss = 6.29510062\n",
      "Iteration 25, loss = 3.14778200\n",
      "Iteration 14, loss = 3.95961263\n",
      "Iteration 26, loss = 2.48634392\n",
      "Iteration 15, loss = 3.00493962\n",
      "Iteration 27, loss = 2.30480821\n",
      "Iteration 16, loss = 3.41205090\n",
      "Iteration 28, loss = 1.87306483\n",
      "Iteration 17, loss = 3.08198748\n",
      "Iteration 29, loss = 2.89445912\n",
      "Iteration 18, loss = 2.65458467\n",
      "Iteration 30, loss = 2.75677652\n",
      "Iteration 19, loss = 2.82094854\n",
      "Iteration 31, loss = 2.72139720\n",
      "Iteration 20, loss = 4.25282987\n",
      "Iteration 32, loss = 2.87490222\n",
      "Iteration 21, loss = 5.08975178\n",
      "Iteration 33, loss = 2.01797657\n",
      "Iteration 22, loss = 3.92605386\n",
      "Iteration 34, loss = 1.77627080\n",
      "Iteration 23, loss = 3.04222441\n",
      "Iteration 35, loss = 2.24121544\n",
      "Iteration 24, loss = 3.19468035\n",
      "Iteration 36, loss = 2.09192375\n",
      "Iteration 25, loss = 5.00866981\n",
      "Iteration 37, loss = 2.05375065\n",
      "Iteration 26, loss = 3.47770031\n",
      "Iteration 38, loss = 2.23409646\n",
      "Iteration 27, loss = 2.82943858\n",
      "Iteration 39, loss = 2.28570607\n",
      "Iteration 28, loss = 2.70760986\n",
      "Iteration 40, loss = 2.11370171\n",
      "Iteration 29, loss = 2.60523930\n",
      "Iteration 41, loss = 2.02750989\n",
      "Iteration 30, loss = 2.46902325\n",
      "Iteration 42, loss = 2.23896687\n",
      "Iteration 31, loss = 2.37475634\n",
      "Iteration 43, loss = 1.69638654\n",
      "Iteration 32, loss = 2.16539958\n",
      "Iteration 44, loss = 1.80625769\n",
      "Iteration 33, loss = 2.12676589\n",
      "Iteration 45, loss = 1.08229705\n",
      "Iteration 34, loss = 1.80862962\n",
      "Iteration 46, loss = 1.14817769\n",
      "Iteration 35, loss = 1.99463715\n",
      "Iteration 47, loss = 1.61325184\n",
      "Iteration 36, loss = 2.13398848\n",
      "Iteration 48, loss = 1.15603032\n",
      "Iteration 37, loss = 1.99775457\n",
      "Iteration 49, loss = 1.21348326\n",
      "Iteration 38, loss = 2.68008530\n",
      "Iteration 50, loss = 1.10241467\n",
      "Iteration 39, loss = 2.21058826\n",
      "Iteration 51, loss = 1.87855689\n",
      "Iteration 40, loss = 2.24323628\n",
      "Iteration 52, loss = 1.86321614\n",
      "Iteration 41, loss = 2.54588554\n",
      "Iteration 53, loss = 1.49367555\n",
      "Iteration 42, loss = 2.07474264\n",
      "Iteration 54, loss = 1.46884471\n",
      "Iteration 43, loss = 2.10522243\n",
      "Iteration 55, loss = 1.35517500\n",
      "Iteration 44, loss = 1.67786528\n",
      "Iteration 56, loss = 1.50020950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 1.84279584\n",
      "Iteration 46, loss = 1.57730754\n",
      "Iteration 47, loss = 1.63256195\n",
      "Iteration 48, loss = 1.85557705\n",
      "Iteration 49, loss = 1.49714716\n",
      "Iteration 50, loss = 1.19707345\n",
      "Iteration 51, loss = 1.24949186\n",
      "Iteration 52, loss = 1.07180433\n",
      "Iteration 53, loss = 1.09642181\n",
      "Iteration 54, loss = 0.71394274\n",
      "Iteration 55, loss = 1.02717558\n",
      "Iteration 56, loss = 1.00913904\n",
      "Iteration 57, loss = 1.34789310\n",
      "Iteration 1, loss = 18.44195408\n",
      "Iteration 58, loss = 1.12374136\n",
      "Iteration 2, loss = 17.93507722\n",
      "Iteration 59, loss = 1.12151698\n",
      "Iteration 3, loss = 11.81187665\n",
      "Iteration 60, loss = 2.24936552\n",
      "Iteration 4, loss = 8.76417952\n",
      "Iteration 61, loss = 1.66224178\n",
      "Iteration 5, loss = 10.39058195\n",
      "Iteration 62, loss = 1.96734534\n",
      "Iteration 6, loss = 7.99726001\n",
      "Iteration 63, loss = 2.08553288\n",
      "Iteration 7, loss = 8.43075607\n",
      "Iteration 64, loss = 1.23194658\n",
      "Iteration 8, loss = 7.59397620\n",
      "Iteration 65, loss = 1.26353356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 6.20259531\n",
      "Iteration 10, loss = 6.76366643\n",
      "Iteration 11, loss = 7.86273606\n",
      "Iteration 12, loss = 5.97384035\n",
      "Iteration 13, loss = 5.73237251\n",
      "Iteration 14, loss = 4.39608001\n",
      "Iteration 15, loss = 4.24208732\n",
      "Iteration 16, loss = 5.00831332\n",
      "Iteration 17, loss = 4.96140886\n",
      "Iteration 18, loss = 5.72575840\n",
      "Iteration 19, loss = 4.60086099\n",
      "Iteration 20, loss = 3.85591801\n",
      "Iteration 21, loss = 3.46643642\n",
      "Iteration 1, loss = 16.91601847\n",
      "Iteration 22, loss = 5.19779340\n",
      "Iteration 2, loss = 16.17146484\n",
      "Iteration 23, loss = 4.99647711\n",
      "Iteration 3, loss = 11.44448264\n",
      "Iteration 24, loss = 4.02007208\n",
      "Iteration 4, loss = 11.54446044\n",
      "Iteration 25, loss = 3.29787294\n",
      "Iteration 5, loss = 7.65725589\n",
      "Iteration 26, loss = 3.19896618\n",
      "Iteration 6, loss = 7.76486617\n",
      "Iteration 27, loss = 3.32852369\n",
      "Iteration 7, loss = 7.28898693\n",
      "Iteration 28, loss = 2.95046665\n",
      "Iteration 8, loss = 7.34912406\n",
      "Iteration 29, loss = 3.36630503\n",
      "Iteration 9, loss = 8.72735263\n",
      "Iteration 30, loss = 2.80351744\n",
      "Iteration 10, loss = 6.64397418\n",
      "Iteration 31, loss = 2.09068662\n",
      "Iteration 11, loss = 5.01827759\n",
      "Iteration 32, loss = 2.26333524\n",
      "Iteration 12, loss = 4.64235391\n",
      "Iteration 33, loss = 2.23420838\n",
      "Iteration 13, loss = 4.57594712\n",
      "Iteration 34, loss = 1.97780197\n",
      "Iteration 14, loss = 4.54885089\n",
      "Iteration 35, loss = 1.73501466\n",
      "Iteration 15, loss = 4.52212995\n",
      "Iteration 36, loss = 1.34529906\n",
      "Iteration 16, loss = 3.75836279\n",
      "Iteration 37, loss = 1.62268478\n",
      "Iteration 17, loss = 3.06001355\n",
      "Iteration 38, loss = 1.31049198\n",
      "Iteration 18, loss = 3.13022322\n",
      "Iteration 39, loss = 1.54007352\n",
      "Iteration 19, loss = 4.74458359\n",
      "Iteration 40, loss = 1.06534130\n",
      "Iteration 20, loss = 3.50343455\n",
      "Iteration 41, loss = 0.97849193\n",
      "Iteration 21, loss = 3.09789777\n",
      "Iteration 42, loss = 1.06641239\n",
      "Iteration 22, loss = 2.80666771\n",
      "Iteration 43, loss = 0.78505082\n",
      "Iteration 23, loss = 2.95673623\n",
      "Iteration 44, loss = 0.90671479\n",
      "Iteration 24, loss = 2.96077038\n",
      "Iteration 45, loss = 0.84714235\n",
      "Iteration 25, loss = 3.66340585\n",
      "Iteration 46, loss = 0.82351446\n",
      "Iteration 26, loss = 3.10558876\n",
      "Iteration 47, loss = 0.67464410\n",
      "Iteration 27, loss = 2.31989771\n",
      "Iteration 48, loss = 0.54361682\n",
      "Iteration 28, loss = 2.49497472\n",
      "Iteration 49, loss = 0.66588512\n",
      "Iteration 29, loss = 2.35699996\n",
      "Iteration 50, loss = 0.88354198\n",
      "Iteration 30, loss = 2.16474497\n",
      "Iteration 51, loss = 0.97757887\n",
      "Iteration 31, loss = 2.30296713\n",
      "Iteration 52, loss = 0.80701597\n",
      "Iteration 32, loss = 1.80572639\n",
      "Iteration 53, loss = 1.11021447\n",
      "Iteration 33, loss = 1.77624976\n",
      "Iteration 54, loss = 1.34266844\n",
      "Iteration 34, loss = 1.96066935\n",
      "Iteration 55, loss = 1.14514961\n",
      "Iteration 56, loss = 0.83495425\n",
      "Iteration 35, loss = 1.93793801\n",
      "Iteration 57, loss = 0.78407108\n",
      "Iteration 36, loss = 1.85023175\n",
      "Iteration 58, loss = 0.86781490\n",
      "Iteration 37, loss = 1.72262777\n",
      "Iteration 38, loss = 1.75165666\n",
      "Iteration 59, loss = 0.87452857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 1.85493511\n",
      "Iteration 40, loss = 1.46292243\n",
      "Iteration 41, loss = 1.70195696\n",
      "Iteration 42, loss = 1.52532147\n",
      "Iteration 43, loss = 1.40274076\n",
      "Iteration 44, loss = 1.50603968\n",
      "Iteration 45, loss = 1.29497514\n",
      "Iteration 46, loss = 2.75709459\n",
      "Iteration 47, loss = 2.78446448\n",
      "Iteration 48, loss = 2.51873985\n",
      "Iteration 49, loss = 2.47393245\n",
      "Iteration 50, loss = 2.15862862\n",
      "Iteration 51, loss = 3.04906638\n",
      "Iteration 1, loss = 17.31150691\n",
      "Iteration 52, loss = 2.00229205\n",
      "Iteration 2, loss = 21.89503623\n",
      "Iteration 53, loss = 2.39472978\n",
      "Iteration 3, loss = 11.72920405\n",
      "Iteration 54, loss = 2.15119057\n",
      "Iteration 4, loss = 13.16483287\n",
      "Iteration 55, loss = 2.45272038\n",
      "Iteration 5, loss = 8.82505041\n",
      "Iteration 56, loss = 2.38893121\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 7.48522647\n",
      "Iteration 7, loss = 7.01208161\n",
      "Iteration 8, loss = 11.16916738\n",
      "Iteration 9, loss = 6.19054122\n",
      "Iteration 10, loss = 5.81914992\n",
      "Iteration 11, loss = 4.49890925\n",
      "Iteration 12, loss = 4.37618218\n",
      "Iteration 13, loss = 6.05762017\n",
      "Iteration 14, loss = 5.69376169\n",
      "Iteration 15, loss = 4.73224975\n",
      "Iteration 16, loss = 4.92965564\n",
      "Iteration 17, loss = 5.41659141\n",
      "Iteration 18, loss = 3.91249803\n",
      "Iteration 19, loss = 4.50534981\n",
      "Iteration 1, loss = 16.00867826\n",
      "Iteration 20, loss = 4.48819495\n",
      "Iteration 2, loss = 15.89251713\n",
      "Iteration 21, loss = 3.83949092\n",
      "Iteration 3, loss = 11.08600897\n",
      "Iteration 22, loss = 4.32100581\n",
      "Iteration 4, loss = 7.15973480\n",
      "Iteration 23, loss = 4.55955209\n",
      "Iteration 5, loss = 9.14363502\n",
      "Iteration 24, loss = 4.40872222\n",
      "Iteration 6, loss = 7.80556378\n",
      "Iteration 25, loss = 4.27394788\n",
      "Iteration 7, loss = 7.57822059\n",
      "Iteration 26, loss = 3.40341311\n",
      "Iteration 8, loss = 8.09539160\n",
      "Iteration 27, loss = 4.00324112\n",
      "Iteration 9, loss = 7.18889174\n",
      "Iteration 10, loss = 13.31351197\n",
      "Iteration 28, loss = 3.86164724\n",
      "Iteration 11, loss = 6.24230467\n",
      "Iteration 29, loss = 3.28859576\n",
      "Iteration 12, loss = 5.70748628\n",
      "Iteration 30, loss = 3.31580558\n",
      "Iteration 13, loss = 4.99787706\n",
      "Iteration 31, loss = 3.54546175\n",
      "Iteration 14, loss = 4.44524788\n",
      "Iteration 32, loss = 2.73915600\n",
      "Iteration 15, loss = 3.99140544\n",
      "Iteration 33, loss = 1.99309650\n",
      "Iteration 16, loss = 4.48422166\n",
      "Iteration 34, loss = 1.93044737\n",
      "Iteration 17, loss = 4.29278433\n",
      "Iteration 35, loss = 2.50061039\n",
      "Iteration 18, loss = 4.57759448\n",
      "Iteration 36, loss = 2.20372728\n",
      "Iteration 19, loss = 5.02360712\n",
      "Iteration 37, loss = 2.22394134\n",
      "Iteration 20, loss = 4.44591705\n",
      "Iteration 38, loss = 2.67083744\n",
      "Iteration 21, loss = 4.22525134\n",
      "Iteration 39, loss = 2.62263227\n",
      "Iteration 22, loss = 3.72360566\n",
      "Iteration 40, loss = 2.15109264\n",
      "Iteration 23, loss = 3.25381903\n",
      "Iteration 41, loss = 2.00428646\n",
      "Iteration 24, loss = 2.62134835\n",
      "Iteration 42, loss = 2.82446174\n",
      "Iteration 25, loss = 2.42875865\n",
      "Iteration 43, loss = 3.46276462\n",
      "Iteration 26, loss = 2.48106294\n",
      "Iteration 44, loss = 2.96838764\n",
      "Iteration 27, loss = 3.09567497\n",
      "Iteration 45, loss = 2.40939004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 3.31304539\n",
      "Iteration 29, loss = 2.94594675\n",
      "Iteration 30, loss = 2.47872736\n",
      "Iteration 31, loss = 2.11687720\n",
      "Iteration 32, loss = 1.59060847\n",
      "Iteration 33, loss = 1.84320788\n",
      "Iteration 34, loss = 2.07771033\n",
      "Iteration 35, loss = 1.77678672\n",
      "Iteration 36, loss = 1.38161140\n",
      "Iteration 37, loss = 1.71642691\n",
      "Iteration 38, loss = 1.72882325\n",
      "Iteration 39, loss = 2.00246088\n",
      "Iteration 40, loss = 2.80025749\n",
      "Iteration 1, loss = 20.76865247\n",
      "Iteration 41, loss = 2.43995229\n",
      "Iteration 2, loss = 17.65692816\n",
      "Iteration 42, loss = 2.16066434\n",
      "Iteration 3, loss = 9.23229491\n",
      "Iteration 43, loss = 2.14140588\n",
      "Iteration 4, loss = 10.02380253\n",
      "Iteration 44, loss = 1.83788502\n",
      "Iteration 5, loss = 9.67801197\n",
      "Iteration 45, loss = 2.21826792\n",
      "Iteration 6, loss = 7.72086926\n",
      "Iteration 46, loss = 2.38991243\n",
      "Iteration 7, loss = 6.24099880\n",
      "Iteration 47, loss = 2.03428458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 7.16852496\n",
      "Iteration 9, loss = 7.28305071\n",
      "Iteration 10, loss = 6.23012898\n",
      "Iteration 11, loss = 4.77106480\n",
      "Iteration 12, loss = 5.80955520\n",
      "Iteration 13, loss = 4.79659243\n",
      "Iteration 14, loss = 4.98192427\n",
      "Iteration 15, loss = 4.68311775\n",
      "Iteration 16, loss = 3.62146112\n",
      "Iteration 17, loss = 3.87544723\n",
      "Iteration 18, loss = 3.74811111\n",
      "Iteration 19, loss = 4.70911913\n",
      "Iteration 20, loss = 4.25200068\n",
      "Iteration 1, loss = 14.74993487\n",
      "Iteration 21, loss = 2.90074322\n",
      "Iteration 2, loss = 13.33664126\n",
      "Iteration 22, loss = 4.04128916\n",
      "Iteration 3, loss = 13.07885433\n",
      "Iteration 23, loss = 3.32706688\n",
      "Iteration 4, loss = 7.33882792\n",
      "Iteration 24, loss = 3.39030435\n",
      "Iteration 5, loss = 8.26821781\n",
      "Iteration 25, loss = 2.65943913\n",
      "Iteration 6, loss = 7.29458934\n",
      "Iteration 26, loss = 3.01762155\n",
      "Iteration 7, loss = 7.29696017\n",
      "Iteration 27, loss = 4.46639831\n",
      "Iteration 8, loss = 6.16596501\n",
      "Iteration 28, loss = 4.57923150\n",
      "Iteration 9, loss = 9.99907521\n",
      "Iteration 29, loss = 3.96329122\n",
      "Iteration 10, loss = 9.19747645\n",
      "Iteration 30, loss = 3.46022362\n",
      "Iteration 11, loss = 7.82723537\n",
      "Iteration 31, loss = 2.83450291\n",
      "Iteration 12, loss = 6.30725330\n",
      "Iteration 32, loss = 2.41511382\n",
      "Iteration 13, loss = 6.10587840\n",
      "Iteration 33, loss = 2.59947483\n",
      "Iteration 14, loss = 5.53595398\n",
      "Iteration 34, loss = 2.62823730\n",
      "Iteration 15, loss = 6.16313587\n",
      "Iteration 35, loss = 2.35330723\n",
      "Iteration 16, loss = 5.57636626\n",
      "Iteration 36, loss = 2.56703769\n",
      "Iteration 17, loss = 4.55024146\n",
      "Iteration 37, loss = 2.15657116\n",
      "Iteration 18, loss = 4.54254550\n",
      "Iteration 38, loss = 2.61379608\n",
      "Iteration 19, loss = 4.91610994\n",
      "Iteration 39, loss = 2.78046785\n",
      "Iteration 20, loss = 4.37604595\n",
      "Iteration 40, loss = 2.26085669\n",
      "Iteration 21, loss = 3.96647153\n",
      "Iteration 41, loss = 2.09470038\n",
      "Iteration 22, loss = 3.60208945\n",
      "Iteration 42, loss = 2.36789272\n",
      "Iteration 23, loss = 3.83295507\n",
      "Iteration 43, loss = 2.43299880\n",
      "Iteration 24, loss = 3.84689995\n",
      "Iteration 44, loss = 1.88425285\n",
      "Iteration 25, loss = 3.23098740\n",
      "Iteration 45, loss = 2.41336711\n",
      "Iteration 26, loss = 3.27926021\n",
      "Iteration 46, loss = 2.81170589\n",
      "Iteration 27, loss = 3.79243823\n",
      "Iteration 47, loss = 1.87271481\n",
      "Iteration 28, loss = 3.08739418\n",
      "Iteration 48, loss = 1.24761999\n",
      "Iteration 29, loss = 2.83835862\n",
      "Iteration 49, loss = 2.16512053\n",
      "Iteration 30, loss = 2.69734285\n",
      "Iteration 50, loss = 1.52861189\n",
      "Iteration 31, loss = 2.96238436\n",
      "Iteration 51, loss = 2.04896565\n",
      "Iteration 32, loss = 2.56850053\n",
      "Iteration 52, loss = 1.95582503\n",
      "Iteration 33, loss = 2.54868215\n",
      "Iteration 53, loss = 1.92418531\n",
      "Iteration 34, loss = 2.25360515\n",
      "Iteration 54, loss = 1.89834969\n",
      "Iteration 35, loss = 1.82594280\n",
      "Iteration 55, loss = 1.64766006\n",
      "Iteration 36, loss = 1.39246550\n",
      "Iteration 56, loss = 1.61462700\n",
      "Iteration 37, loss = 2.25717475\n",
      "Iteration 57, loss = 1.37788908\n",
      "Iteration 38, loss = 2.08150226\n",
      "Iteration 58, loss = 1.37825519\n",
      "Iteration 39, loss = 1.57892574\n",
      "Iteration 59, loss = 1.14745565\n",
      "Iteration 40, loss = 1.33305609\n",
      "Iteration 60, loss = 1.05323529\n",
      "Iteration 41, loss = 1.35418921\n",
      "Iteration 61, loss = 1.19384144\n",
      "Iteration 42, loss = 1.53894028\n",
      "Iteration 62, loss = 0.87021827\n",
      "Iteration 43, loss = 2.27941084\n",
      "Iteration 63, loss = 1.86256096\n",
      "Iteration 44, loss = 1.79609009\n",
      "Iteration 64, loss = 1.99367819\n",
      "Iteration 45, loss = 1.32498823\n",
      "Iteration 65, loss = 1.26639423\n",
      "Iteration 46, loss = 0.89673634\n",
      "Iteration 66, loss = 1.44016254\n",
      "Iteration 47, loss = 1.17102365\n",
      "Iteration 67, loss = 1.98006533\n",
      "Iteration 48, loss = 1.20961276\n",
      "Iteration 68, loss = 2.29625431\n",
      "Iteration 49, loss = 1.20545627\n",
      "Iteration 69, loss = 2.12989206\n",
      "Iteration 50, loss = 1.23422201\n",
      "Iteration 70, loss = 1.75812775\n",
      "Iteration 51, loss = 1.16546872\n",
      "Iteration 71, loss = 2.13037647\n",
      "Iteration 52, loss = 1.23103976\n",
      "Iteration 72, loss = 2.27958161\n",
      "Iteration 53, loss = 0.98141857\n",
      "Iteration 73, loss = 2.01803675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 1.19755214\n",
      "Iteration 55, loss = 1.03710363\n",
      "Iteration 56, loss = 1.28308142\n",
      "Iteration 57, loss = 1.34905942\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.17886554\n",
      "Iteration 2, loss = 16.40371665\n",
      "Iteration 3, loss = 10.79268678\n",
      "Iteration 4, loss = 8.79887116\n",
      "Iteration 5, loss = 10.22208211\n",
      "Iteration 1, loss = 17.22639911\n",
      "Iteration 6, loss = 9.21213984\n",
      "Iteration 2, loss = 17.22686809\n",
      "Iteration 3, loss = 11.94093503\n",
      "Iteration 7, loss = 9.49627577\n",
      "Iteration 4, loss = 10.94577671\n",
      "Iteration 8, loss = 13.13395834\n",
      "Iteration 5, loss = 10.50199662\n",
      "Iteration 9, loss = 9.54368639\n",
      "Iteration 6, loss = 7.07558541\n",
      "Iteration 10, loss = 10.46742819\n",
      "Iteration 7, loss = 8.44280717\n",
      "Iteration 11, loss = 6.12181144\n",
      "Iteration 8, loss = 7.21036561\n",
      "Iteration 12, loss = 6.11651280\n",
      "Iteration 9, loss = 6.88982633\n",
      "Iteration 13, loss = 5.32933691\n",
      "Iteration 10, loss = 5.85114750\n",
      "Iteration 14, loss = 5.20640516\n",
      "Iteration 11, loss = 7.52633031\n",
      "Iteration 15, loss = 9.76082621\n",
      "Iteration 12, loss = 8.35825991\n",
      "Iteration 16, loss = 7.43506572\n",
      "Iteration 13, loss = 8.96758462\n",
      "Iteration 17, loss = 5.05481567\n",
      "Iteration 14, loss = 7.41150564\n",
      "Iteration 18, loss = 5.08861265\n",
      "Iteration 15, loss = 5.45482828\n",
      "Iteration 19, loss = 4.52243424\n",
      "Iteration 16, loss = 4.74559420\n",
      "Iteration 20, loss = 3.98565745\n",
      "Iteration 17, loss = 6.85804669\n",
      "Iteration 21, loss = 3.63391357\n",
      "Iteration 18, loss = 6.24400505\n",
      "Iteration 22, loss = 3.67164602\n",
      "Iteration 19, loss = 4.95868725\n",
      "Iteration 23, loss = 3.18178787\n",
      "Iteration 20, loss = 4.56758786\n",
      "Iteration 24, loss = 3.05346362\n",
      "Iteration 21, loss = 4.15668648\n",
      "Iteration 25, loss = 3.03993635\n",
      "Iteration 22, loss = 4.42076900\n",
      "Iteration 26, loss = 2.83896584\n",
      "Iteration 23, loss = 4.14029151\n",
      "Iteration 27, loss = 2.44811823\n",
      "Iteration 24, loss = 4.23984789\n",
      "Iteration 28, loss = 2.68216215\n",
      "Iteration 25, loss = 4.61471669\n",
      "Iteration 29, loss = 2.17745310\n",
      "Iteration 26, loss = 4.02665238\n",
      "Iteration 30, loss = 2.63722431\n",
      "Iteration 27, loss = 3.15427212\n",
      "Iteration 31, loss = 2.99327539\n",
      "Iteration 28, loss = 3.30531199\n",
      "Iteration 32, loss = 2.68212931\n",
      "Iteration 29, loss = 3.22299480\n",
      "Iteration 33, loss = 2.48282613\n",
      "Iteration 30, loss = 3.10237885\n",
      "Iteration 34, loss = 1.99122390\n",
      "Iteration 31, loss = 2.47698591\n",
      "Iteration 35, loss = 2.03325105\n",
      "Iteration 32, loss = 2.55533129\n",
      "Iteration 36, loss = 1.68269937\n",
      "Iteration 33, loss = 2.58313432\n",
      "Iteration 37, loss = 1.49637035\n",
      "Iteration 34, loss = 3.06627456\n",
      "Iteration 38, loss = 1.63350487\n",
      "Iteration 35, loss = 2.81348625\n",
      "Iteration 39, loss = 1.27297044\n",
      "Iteration 36, loss = 1.90241535\n",
      "Iteration 40, loss = 1.15776836\n",
      "Iteration 37, loss = 1.85768120\n",
      "Iteration 41, loss = 1.45777855\n",
      "Iteration 38, loss = 1.57454711\n",
      "Iteration 42, loss = 1.60549087\n",
      "Iteration 39, loss = 1.67008246\n",
      "Iteration 43, loss = 1.38840939\n",
      "Iteration 40, loss = 1.31351943\n",
      "Iteration 44, loss = 1.06276658\n",
      "Iteration 41, loss = 1.25774216\n",
      "Iteration 45, loss = 0.96251809\n",
      "Iteration 42, loss = 1.17544765\n",
      "Iteration 46, loss = 1.00829136\n",
      "Iteration 43, loss = 1.28015246\n",
      "Iteration 47, loss = 0.97276267\n",
      "Iteration 44, loss = 1.58405306\n",
      "Iteration 48, loss = 1.41885632\n",
      "Iteration 45, loss = 2.11613895\n",
      "Iteration 49, loss = 0.89251963\n",
      "Iteration 46, loss = 1.66291635\n",
      "Iteration 50, loss = 0.70691359\n",
      "Iteration 47, loss = 1.78416606\n",
      "Iteration 51, loss = 0.63921161\n",
      "Iteration 48, loss = 2.18704179\n",
      "Iteration 52, loss = 0.65792217\n",
      "Iteration 49, loss = 2.20607620\n",
      "Iteration 53, loss = 0.74927711\n",
      "Iteration 50, loss = 2.38479026\n",
      "Iteration 54, loss = 0.63080848\n",
      "Iteration 51, loss = 1.75046261\n",
      "Iteration 55, loss = 0.64573392\n",
      "Iteration 52, loss = 1.27107061\n",
      "Iteration 56, loss = 0.69747320\n",
      "Iteration 53, loss = 1.04179913\n",
      "Iteration 54, loss = 1.30939976\n",
      "Iteration 57, loss = 0.58024493\n",
      "Iteration 55, loss = 1.35856029\n",
      "Iteration 58, loss = 0.62098825\n",
      "Iteration 56, loss = 0.94828510\n",
      "Iteration 59, loss = 0.52010433\n",
      "Iteration 57, loss = 1.39411556\n",
      "Iteration 60, loss = 0.70748828\n",
      "Iteration 58, loss = 1.12575139\n",
      "Iteration 61, loss = 0.75525949\n",
      "Iteration 59, loss = 1.73375210\n",
      "Iteration 62, loss = 0.63733962\n",
      "Iteration 60, loss = 1.26049173\n",
      "Iteration 63, loss = 0.56601260\n",
      "Iteration 61, loss = 1.20060269\n",
      "Iteration 64, loss = 0.68296322\n",
      "Iteration 62, loss = 0.98013267\n",
      "Iteration 65, loss = 0.87129328\n",
      "Iteration 63, loss = 0.94634316\n",
      "Iteration 66, loss = 0.45372682\n",
      "Iteration 64, loss = 1.15588465\n",
      "Iteration 67, loss = 0.53113310\n",
      "Iteration 65, loss = 1.07251102\n",
      "Iteration 68, loss = 0.60912954\n",
      "Iteration 66, loss = 1.19805419\n",
      "Iteration 69, loss = 0.52389956\n",
      "Iteration 67, loss = 0.86305791\n",
      "Iteration 70, loss = 0.80457573\n",
      "Iteration 68, loss = 1.14241425\n",
      "Iteration 71, loss = 0.73365447\n",
      "Iteration 69, loss = 1.19173899\n",
      "Iteration 72, loss = 0.69363594\n",
      "Iteration 70, loss = 0.87802498\n",
      "Iteration 73, loss = 0.75539274\n",
      "Iteration 71, loss = 1.20637695\n",
      "Iteration 74, loss = 0.74446683\n",
      "Iteration 72, loss = 0.91153371\n",
      "Iteration 75, loss = 0.68156528\n",
      "Iteration 73, loss = 0.86899619\n",
      "Iteration 76, loss = 0.57144732\n",
      "Iteration 74, loss = 0.92751053\n",
      "Iteration 77, loss = 0.45623834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 75, loss = 1.26392645\n",
      "Iteration 76, loss = 1.17949173\n",
      "Iteration 77, loss = 0.95518532\n",
      "Iteration 78, loss = 0.86351626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.16335468\n",
      "Iteration 2, loss = 11.18753936\n",
      "Iteration 3, loss = 11.82322512\n",
      "Iteration 4, loss = 10.53139499\n",
      "Iteration 1, loss = 14.81484635\n",
      "Iteration 5, loss = 5.64047890\n",
      "Iteration 2, loss = 15.46114227\n",
      "Iteration 6, loss = 5.68617990\n",
      "Iteration 3, loss = 13.27000129\n",
      "Iteration 7, loss = 5.84931919\n",
      "Iteration 4, loss = 10.03150298\n",
      "Iteration 8, loss = 6.08831787\n",
      "Iteration 5, loss = 8.85754687\n",
      "Iteration 9, loss = 8.16895753\n",
      "Iteration 6, loss = 5.56215226\n",
      "Iteration 10, loss = 7.00768775\n",
      "Iteration 7, loss = 6.65830999\n",
      "Iteration 11, loss = 4.63882986\n",
      "Iteration 8, loss = 5.46462179\n",
      "Iteration 12, loss = 5.50963813\n",
      "Iteration 9, loss = 5.95092075\n",
      "Iteration 13, loss = 4.00722434\n",
      "Iteration 10, loss = 6.01896297\n",
      "Iteration 14, loss = 4.10622167\n",
      "Iteration 11, loss = 4.75860592\n",
      "Iteration 15, loss = 5.35562968\n",
      "Iteration 12, loss = 4.69688705\n",
      "Iteration 16, loss = 5.02417700\n",
      "Iteration 13, loss = 3.69200607\n",
      "Iteration 17, loss = 3.93862129\n",
      "Iteration 14, loss = 3.82286027\n",
      "Iteration 18, loss = 3.35337258\n",
      "Iteration 15, loss = 3.12160779\n",
      "Iteration 19, loss = 3.25929313\n",
      "Iteration 16, loss = 3.53913466\n",
      "Iteration 20, loss = 3.22383646\n",
      "Iteration 17, loss = 3.52925980\n",
      "Iteration 21, loss = 2.83213338\n",
      "Iteration 18, loss = 2.85272431\n",
      "Iteration 22, loss = 2.74002826\n",
      "Iteration 19, loss = 2.97190693\n",
      "Iteration 23, loss = 2.64499229\n",
      "Iteration 20, loss = 2.86079005\n",
      "Iteration 24, loss = 2.23637162\n",
      "Iteration 21, loss = 2.45373017\n",
      "Iteration 25, loss = 2.18762807\n",
      "Iteration 22, loss = 3.00132669\n",
      "Iteration 26, loss = 2.43538099\n",
      "Iteration 23, loss = 2.52344012\n",
      "Iteration 27, loss = 1.98707940\n",
      "Iteration 24, loss = 2.17503411\n",
      "Iteration 28, loss = 1.84361843\n",
      "Iteration 25, loss = 1.90788851\n",
      "Iteration 29, loss = 1.78925945\n",
      "Iteration 26, loss = 2.30964509\n",
      "Iteration 30, loss = 1.77944836\n",
      "Iteration 27, loss = 3.12204811\n",
      "Iteration 31, loss = 1.29573347\n",
      "Iteration 28, loss = 2.24048981\n",
      "Iteration 32, loss = 1.54301994\n",
      "Iteration 29, loss = 2.37876352\n",
      "Iteration 33, loss = 1.61961194\n",
      "Iteration 30, loss = 2.20113263\n",
      "Iteration 34, loss = 1.68341164\n",
      "Iteration 31, loss = 1.97867570\n",
      "Iteration 32, loss = 2.30110021\n",
      "Iteration 35, loss = 1.99128686\n",
      "Iteration 36, loss = 2.41819985\n",
      "Iteration 33, loss = 2.58993962\n",
      "Iteration 34, loss = 2.52478173\n",
      "Iteration 37, loss = 2.30427266\n",
      "Iteration 35, loss = 2.57316081\n",
      "Iteration 38, loss = 2.31199838\n",
      "Iteration 36, loss = 2.67749302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 2.15542157\n",
      "Iteration 40, loss = 2.38319914\n",
      "Iteration 41, loss = 1.86041910\n",
      "Iteration 42, loss = 1.87612927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.98370982\n",
      "Iteration 2, loss = 13.25847390\n",
      "Iteration 3, loss = 9.83383497\n",
      "Iteration 4, loss = 11.36049264\n",
      "Iteration 1, loss = 16.92005616\n",
      "Iteration 5, loss = 11.82022879\n",
      "Iteration 2, loss = 13.20539799\n",
      "Iteration 6, loss = 7.56934250\n",
      "Iteration 3, loss = 11.80861655\n",
      "Iteration 7, loss = 8.75315739\n",
      "Iteration 4, loss = 8.74017228\n",
      "Iteration 8, loss = 6.95889335\n",
      "Iteration 5, loss = 7.39470858\n",
      "Iteration 9, loss = 7.43418210\n",
      "Iteration 6, loss = 6.18497116\n",
      "Iteration 10, loss = 10.70500004\n",
      "Iteration 7, loss = 8.08396493\n",
      "Iteration 11, loss = 8.94090869\n",
      "Iteration 8, loss = 7.18958984\n",
      "Iteration 12, loss = 7.68409492\n",
      "Iteration 9, loss = 6.30999100\n",
      "Iteration 13, loss = 5.88230558\n",
      "Iteration 10, loss = 5.31592948\n",
      "Iteration 14, loss = 5.71846838\n",
      "Iteration 11, loss = 9.47314914\n",
      "Iteration 15, loss = 4.66083589\n",
      "Iteration 12, loss = 6.89658402\n",
      "Iteration 16, loss = 4.06030442\n",
      "Iteration 13, loss = 7.37375057\n",
      "Iteration 17, loss = 4.02551747\n",
      "Iteration 14, loss = 6.59804946\n",
      "Iteration 18, loss = 3.54001543\n",
      "Iteration 15, loss = 5.41335767\n",
      "Iteration 19, loss = 3.25360240\n",
      "Iteration 16, loss = 4.68244003\n",
      "Iteration 17, loss = 3.82474288\n",
      "Iteration 20, loss = 2.87716410\n",
      "Iteration 21, loss = 2.86419769\n",
      "Iteration 18, loss = 4.18279484\n",
      "Iteration 19, loss = 3.94104125\n",
      "Iteration 22, loss = 3.29294500\n",
      "Iteration 20, loss = 4.42471298\n",
      "Iteration 23, loss = 3.34555660\n",
      "Iteration 21, loss = 3.52894389\n",
      "Iteration 24, loss = 3.34367630\n",
      "Iteration 22, loss = 3.17284215\n",
      "Iteration 25, loss = 4.11523770\n",
      "Iteration 23, loss = 3.12393511\n",
      "Iteration 26, loss = 2.88403348\n",
      "Iteration 24, loss = 3.27630383\n",
      "Iteration 27, loss = 2.65460173\n",
      "Iteration 25, loss = 2.92139096\n",
      "Iteration 28, loss = 2.10644302\n",
      "Iteration 26, loss = 3.37673805\n",
      "Iteration 29, loss = 2.36309380\n",
      "Iteration 27, loss = 3.49863805\n",
      "Iteration 30, loss = 2.15358373\n",
      "Iteration 28, loss = 3.06109375\n",
      "Iteration 31, loss = 3.74033014\n",
      "Iteration 29, loss = 2.32694050\n",
      "Iteration 32, loss = 2.34623182\n",
      "Iteration 30, loss = 2.27800023\n",
      "Iteration 33, loss = 2.65313284\n",
      "Iteration 31, loss = 1.96460505\n",
      "Iteration 34, loss = 3.03080774\n",
      "Iteration 32, loss = 1.78303531\n",
      "Iteration 35, loss = 3.03736709\n",
      "Iteration 33, loss = 1.75854277\n",
      "Iteration 36, loss = 3.69015815\n",
      "Iteration 34, loss = 1.93513470\n",
      "Iteration 37, loss = 3.31922836\n",
      "Iteration 35, loss = 1.77082499\n",
      "Iteration 38, loss = 2.92344571\n",
      "Iteration 36, loss = 1.68972807\n",
      "Iteration 39, loss = 3.62078159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 2.10251471\n",
      "Iteration 38, loss = 1.76425336\n",
      "Iteration 39, loss = 1.93509920\n",
      "Iteration 40, loss = 1.77081308\n",
      "Iteration 41, loss = 1.25707516\n",
      "Iteration 42, loss = 1.37800961\n",
      "Iteration 43, loss = 1.63224342\n",
      "Iteration 44, loss = 1.50035944\n",
      "Iteration 45, loss = 1.26864120\n",
      "Iteration 46, loss = 1.64924556\n",
      "Iteration 47, loss = 1.36683165\n",
      "Iteration 48, loss = 1.70887446\n",
      "Iteration 49, loss = 2.01729668\n",
      "Iteration 1, loss = 13.77050159\n",
      "Iteration 50, loss = 2.37780742\n",
      "Iteration 2, loss = 9.82170439\n",
      "Iteration 51, loss = 1.78691208\n",
      "Iteration 3, loss = 8.84434439\n",
      "Iteration 52, loss = 2.14471588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 10.37535844\n",
      "Iteration 5, loss = 8.01019398\n",
      "Iteration 6, loss = 8.07693187\n",
      "Iteration 7, loss = 8.25525044\n",
      "Iteration 8, loss = 6.66985093\n",
      "Iteration 9, loss = 5.85518659\n",
      "Iteration 10, loss = 4.89796993\n",
      "Iteration 11, loss = 5.16522758\n",
      "Iteration 12, loss = 7.54602955\n",
      "Iteration 13, loss = 7.44823197\n",
      "Iteration 14, loss = 6.87340141\n",
      "Iteration 15, loss = 5.36122186\n",
      "Iteration 16, loss = 4.79085676\n",
      "Iteration 17, loss = 3.84419728\n",
      "Iteration 1, loss = 18.81356425\n",
      "Iteration 18, loss = 3.91478752\n",
      "Iteration 2, loss = 14.20796605\n",
      "Iteration 19, loss = 3.14256721\n",
      "Iteration 3, loss = 11.82312322\n",
      "Iteration 20, loss = 3.25013321\n",
      "Iteration 4, loss = 9.36337025\n",
      "Iteration 21, loss = 3.10695360\n",
      "Iteration 5, loss = 7.12248273\n",
      "Iteration 22, loss = 3.27729415\n",
      "Iteration 6, loss = 6.86437226\n",
      "Iteration 23, loss = 3.59971858\n",
      "Iteration 7, loss = 6.29782766\n",
      "Iteration 24, loss = 2.25213368\n",
      "Iteration 8, loss = 6.75452936\n",
      "Iteration 25, loss = 3.01832975\n",
      "Iteration 9, loss = 4.68284796\n",
      "Iteration 26, loss = 2.53287158\n",
      "Iteration 10, loss = 5.95644645\n",
      "Iteration 27, loss = 2.06148261\n",
      "Iteration 11, loss = 6.57426228\n",
      "Iteration 28, loss = 2.28451258\n",
      "Iteration 12, loss = 6.59682534\n",
      "Iteration 29, loss = 1.69308068\n",
      "Iteration 13, loss = 5.50926094\n",
      "Iteration 30, loss = 1.81876399\n",
      "Iteration 14, loss = 4.86977429\n",
      "Iteration 31, loss = 1.53477147\n",
      "Iteration 15, loss = 4.60272711\n",
      "Iteration 32, loss = 1.12884377\n",
      "Iteration 16, loss = 3.81125777\n",
      "Iteration 33, loss = 1.80266477\n",
      "Iteration 17, loss = 3.38226673\n",
      "Iteration 34, loss = 2.65906159\n",
      "Iteration 18, loss = 4.41486475\n",
      "Iteration 35, loss = 2.22942296\n",
      "Iteration 19, loss = 5.50549471\n",
      "Iteration 36, loss = 1.95567355\n",
      "Iteration 20, loss = 3.91523363\n",
      "Iteration 37, loss = 2.13349123\n",
      "Iteration 21, loss = 3.65201485\n",
      "Iteration 38, loss = 1.74610546\n",
      "Iteration 22, loss = 3.25536084\n",
      "Iteration 39, loss = 1.32196585\n",
      "Iteration 23, loss = 3.31462873\n",
      "Iteration 40, loss = 1.41279911\n",
      "Iteration 24, loss = 3.47926094\n",
      "Iteration 41, loss = 1.53739467\n",
      "Iteration 25, loss = 2.80083336\n",
      "Iteration 42, loss = 1.98197158\n",
      "Iteration 26, loss = 2.76534342\n",
      "Iteration 43, loss = 0.93053578\n",
      "Iteration 27, loss = 2.43565797\n",
      "Iteration 44, loss = 1.09665841\n",
      "Iteration 28, loss = 2.59382468\n",
      "Iteration 45, loss = 1.39657286\n",
      "Iteration 29, loss = 2.44152120\n",
      "Iteration 46, loss = 0.91348840\n",
      "Iteration 30, loss = 2.77428861\n",
      "Iteration 47, loss = 1.01438433\n",
      "Iteration 31, loss = 2.50577824\n",
      "Iteration 48, loss = 1.09004449\n",
      "Iteration 32, loss = 2.63548709\n",
      "Iteration 49, loss = 1.01837331\n",
      "Iteration 33, loss = 2.61556491\n",
      "Iteration 50, loss = 1.12395124\n",
      "Iteration 51, loss = 0.84224933\n",
      "Iteration 34, loss = 3.02600416\n",
      "Iteration 52, loss = 1.07912783\n",
      "Iteration 35, loss = 2.57512280\n",
      "Iteration 53, loss = 0.85976739\n",
      "Iteration 36, loss = 3.05519424\n",
      "Iteration 54, loss = 0.82951729\n",
      "Iteration 37, loss = 2.52161652\n",
      "Iteration 55, loss = 0.49490666\n",
      "Iteration 38, loss = 2.42232160\n",
      "Iteration 56, loss = 0.67029334\n",
      "Iteration 39, loss = 2.46743434\n",
      "Iteration 57, loss = 0.58958494\n",
      "Iteration 40, loss = 2.04770617\n",
      "Iteration 58, loss = 0.70226324\n",
      "Iteration 41, loss = 2.46238550\n",
      "Iteration 59, loss = 0.56890819\n",
      "Iteration 42, loss = 2.76799814\n",
      "Iteration 60, loss = 0.68160162\n",
      "Iteration 43, loss = 2.99252326\n",
      "Iteration 61, loss = 0.90184593\n",
      "Iteration 44, loss = 2.49830135\n",
      "Iteration 62, loss = 0.70950174\n",
      "Iteration 45, loss = 2.63444934\n",
      "Iteration 63, loss = 0.51838348\n",
      "Iteration 46, loss = 2.82789766\n",
      "Iteration 64, loss = 0.49574054\n",
      "Iteration 47, loss = 2.41232049\n",
      "Iteration 65, loss = 0.75223383\n",
      "Iteration 48, loss = 2.18196463\n",
      "Iteration 66, loss = 0.65044002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 1.80156972\n",
      "Iteration 50, loss = 1.93555092\n",
      "Iteration 51, loss = 1.85298975\n",
      "Iteration 52, loss = 2.02622103\n",
      "Iteration 53, loss = 2.28588412\n",
      "Iteration 54, loss = 2.34449361\n",
      "Iteration 55, loss = 1.76233304\n",
      "Iteration 56, loss = 1.66711208\n",
      "Iteration 57, loss = 1.47991195\n",
      "Iteration 58, loss = 1.91483661\n",
      "Iteration 59, loss = 1.99957133\n",
      "Iteration 60, loss = 1.92164761\n",
      "Iteration 61, loss = 1.62661255\n",
      "Iteration 1, loss = 19.65518323\n",
      "Iteration 62, loss = 1.24273278\n",
      "Iteration 2, loss = 20.05297651\n",
      "Iteration 63, loss = 1.98159625\n",
      "Iteration 3, loss = 12.87353963\n",
      "Iteration 64, loss = 1.39744810\n",
      "Iteration 4, loss = 9.83674418\n",
      "Iteration 65, loss = 1.29257868\n",
      "Iteration 5, loss = 7.90418611\n",
      "Iteration 66, loss = 2.20735581\n",
      "Iteration 6, loss = 6.89440079\n",
      "Iteration 67, loss = 1.98883924\n",
      "Iteration 7, loss = 6.82057874\n",
      "Iteration 68, loss = 1.73388890\n",
      "Iteration 8, loss = 7.86219062\n",
      "Iteration 69, loss = 1.79021812\n",
      "Iteration 9, loss = 5.95936421\n",
      "Iteration 70, loss = 1.75254970\n",
      "Iteration 10, loss = 5.24184427\n",
      "Iteration 71, loss = 1.97434653\n",
      "Iteration 11, loss = 5.84126172\n",
      "Iteration 72, loss = 1.93332765\n",
      "Iteration 12, loss = 4.90634766\n",
      "Iteration 73, loss = 2.05339940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 4.65287958\n",
      "Iteration 14, loss = 6.76888209\n",
      "Iteration 15, loss = 4.77306419\n",
      "Iteration 16, loss = 3.83110637\n",
      "Iteration 17, loss = 5.08671219\n",
      "Iteration 18, loss = 4.45992428\n",
      "Iteration 19, loss = 3.08542888\n",
      "Iteration 20, loss = 3.70725180\n",
      "Iteration 21, loss = 4.00799576\n",
      "Iteration 22, loss = 3.92548516\n",
      "Iteration 23, loss = 6.17671875\n",
      "Iteration 24, loss = 4.84350336\n",
      "Iteration 25, loss = 3.88274059\n",
      "Iteration 1, loss = 21.04068256\n",
      "Iteration 26, loss = 4.56108625\n",
      "Iteration 2, loss = 16.42354646\n",
      "Iteration 27, loss = 3.85894049\n",
      "Iteration 3, loss = 10.92188796\n",
      "Iteration 28, loss = 3.37176798\n",
      "Iteration 4, loss = 10.64542653\n",
      "Iteration 29, loss = 4.14494195\n",
      "Iteration 5, loss = 10.49820925\n",
      "Iteration 30, loss = 3.75377010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 7.25090576\n",
      "Iteration 7, loss = 6.78953578\n",
      "Iteration 8, loss = 8.77811027\n",
      "Iteration 9, loss = 6.90948830\n",
      "Iteration 10, loss = 11.12897512\n",
      "Iteration 11, loss = 9.84030785\n",
      "Iteration 12, loss = 7.74085287\n",
      "Iteration 13, loss = 8.62366649\n",
      "Iteration 14, loss = 7.06460294\n",
      "Iteration 15, loss = 5.96753899\n",
      "Iteration 16, loss = 5.52052489\n",
      "Iteration 17, loss = 5.24644383\n",
      "Iteration 18, loss = 5.15356678\n",
      "Iteration 1, loss = 17.88877337\n",
      "Iteration 19, loss = 6.83606276\n",
      "Iteration 2, loss = 14.52286791\n",
      "Iteration 20, loss = 5.87560702\n",
      "Iteration 21, loss = 4.27488693\n",
      "Iteration 3, loss = 8.22618083\n",
      "Iteration 4, loss = 10.30168792\n",
      "Iteration 22, loss = 3.89833687\n",
      "Iteration 5, loss = 9.20948356\n",
      "Iteration 23, loss = 3.60031515\n",
      "Iteration 6, loss = 10.09699544\n",
      "Iteration 24, loss = 4.66773015\n",
      "Iteration 7, loss = 7.14611437\n",
      "Iteration 25, loss = 3.49929860\n",
      "Iteration 8, loss = 6.32538558\n",
      "Iteration 26, loss = 3.58867539\n",
      "Iteration 9, loss = 4.75989051\n",
      "Iteration 27, loss = 3.16363113\n",
      "Iteration 10, loss = 5.09881273\n",
      "Iteration 28, loss = 3.77136725\n",
      "Iteration 11, loss = 5.14123176\n",
      "Iteration 29, loss = 3.37357985\n",
      "Iteration 12, loss = 5.51060950\n",
      "Iteration 30, loss = 3.11428839\n",
      "Iteration 13, loss = 5.60920101\n",
      "Iteration 31, loss = 2.99866471\n",
      "Iteration 14, loss = 4.30164872\n",
      "Iteration 32, loss = 2.57064016\n",
      "Iteration 15, loss = 5.33179016\n",
      "Iteration 33, loss = 2.81433810\n",
      "Iteration 16, loss = 4.74642736\n",
      "Iteration 34, loss = 2.92542994\n",
      "Iteration 17, loss = 4.80013377\n",
      "Iteration 35, loss = 2.22137585\n",
      "Iteration 18, loss = 3.71314928\n",
      "Iteration 36, loss = 2.62573486\n",
      "Iteration 19, loss = 3.89667697\n",
      "Iteration 37, loss = 2.53914809\n",
      "Iteration 20, loss = 3.42571263\n",
      "Iteration 38, loss = 2.61103689\n",
      "Iteration 21, loss = 3.47694602\n",
      "Iteration 39, loss = 2.24901323\n",
      "Iteration 22, loss = 3.43547811\n",
      "Iteration 40, loss = 1.98418729\n",
      "Iteration 41, loss = 1.83392214\n",
      "Iteration 23, loss = 3.33877931\n",
      "Iteration 42, loss = 1.72799055\n",
      "Iteration 24, loss = 3.13933598\n",
      "Iteration 43, loss = 1.86952169\n",
      "Iteration 25, loss = 2.24306797\n",
      "Iteration 44, loss = 2.12498488\n",
      "Iteration 26, loss = 2.26425270\n",
      "Iteration 45, loss = 1.71057969\n",
      "Iteration 27, loss = 2.68701340\n",
      "Iteration 46, loss = 1.54268108\n",
      "Iteration 28, loss = 2.68190853\n",
      "Iteration 47, loss = 1.52636224\n",
      "Iteration 29, loss = 2.93072140\n",
      "Iteration 48, loss = 1.89181817\n",
      "Iteration 30, loss = 3.22236042\n",
      "Iteration 49, loss = 1.89091759\n",
      "Iteration 31, loss = 3.11136440\n",
      "Iteration 50, loss = 1.56729691\n",
      "Iteration 32, loss = 2.69073714\n",
      "Iteration 51, loss = 1.49897365\n",
      "Iteration 33, loss = 2.74071429\n",
      "Iteration 52, loss = 1.78230487\n",
      "Iteration 34, loss = 3.01161066\n",
      "Iteration 53, loss = 1.24580000\n",
      "Iteration 35, loss = 2.52912370\n",
      "Iteration 54, loss = 1.80980524\n",
      "Iteration 36, loss = 2.70656362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 1.48319303\n",
      "Iteration 56, loss = 1.27668637\n",
      "Iteration 57, loss = 1.92917314\n",
      "Iteration 58, loss = 1.74144509\n",
      "Iteration 59, loss = 2.59925106\n",
      "Iteration 60, loss = 1.64247651\n",
      "Iteration 61, loss = 1.45435731\n",
      "Iteration 62, loss = 1.23233589\n",
      "Iteration 63, loss = 1.36540149\n",
      "Iteration 64, loss = 1.48100808\n",
      "Iteration 65, loss = 1.30548066\n",
      "Iteration 66, loss = 1.38074825\n",
      "Iteration 1, loss = 13.71946739\n",
      "Iteration 67, loss = 1.26620633\n",
      "Iteration 2, loss = 16.01170031\n",
      "Iteration 68, loss = 1.23050492\n",
      "Iteration 3, loss = 13.51750094\n",
      "Iteration 69, loss = 1.08679662\n",
      "Iteration 4, loss = 13.86680221\n",
      "Iteration 70, loss = 0.80075972\n",
      "Iteration 5, loss = 11.67562441\n",
      "Iteration 71, loss = 0.95004946\n",
      "Iteration 6, loss = 9.09209094\n",
      "Iteration 72, loss = 0.96767502\n",
      "Iteration 7, loss = 8.27151442\n",
      "Iteration 73, loss = 1.03990827\n",
      "Iteration 8, loss = 6.90572194\n",
      "Iteration 74, loss = 0.86894920\n",
      "Iteration 9, loss = 6.37474670\n",
      "Iteration 75, loss = 0.73320721\n",
      "Iteration 10, loss = 7.65997595\n",
      "Iteration 76, loss = 0.61937592\n",
      "Iteration 11, loss = 9.00250387\n",
      "Iteration 77, loss = 0.83850290\n",
      "Iteration 12, loss = 9.98122874\n",
      "Iteration 78, loss = 0.62675356\n",
      "Iteration 13, loss = 8.95923602\n",
      "Iteration 79, loss = 0.81641824\n",
      "Iteration 14, loss = 5.79142979\n",
      "Iteration 80, loss = 1.28558787\n",
      "Iteration 15, loss = 5.38050752\n",
      "Iteration 81, loss = 1.30020592\n",
      "Iteration 16, loss = 5.30494423\n",
      "Iteration 82, loss = 0.74641805\n",
      "Iteration 17, loss = 4.96008991\n",
      "Iteration 83, loss = 1.06822622\n",
      "Iteration 18, loss = 6.08696142\n",
      "Iteration 84, loss = 1.13511317\n",
      "Iteration 19, loss = 5.08148949\n",
      "Iteration 85, loss = 0.85445338\n",
      "Iteration 20, loss = 3.86605667\n",
      "Iteration 86, loss = 1.10768524\n",
      "Iteration 21, loss = 3.26541246\n",
      "Iteration 87, loss = 0.99858101\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 3.20990169\n",
      "Iteration 23, loss = 2.85657640\n",
      "Iteration 24, loss = 3.11475098\n",
      "Iteration 25, loss = 2.94128564\n",
      "Iteration 26, loss = 2.95332607\n",
      "Iteration 27, loss = 3.06318487\n",
      "Iteration 28, loss = 3.27713689\n",
      "Iteration 29, loss = 2.40325395\n",
      "Iteration 30, loss = 2.06761280\n",
      "Iteration 31, loss = 1.94370925\n",
      "Iteration 32, loss = 2.01415142\n",
      "Iteration 33, loss = 1.79733591\n",
      "Iteration 34, loss = 1.91262556\n",
      "Iteration 1, loss = 15.91658305\n",
      "Iteration 2, loss = 13.30712636\n",
      "Iteration 35, loss = 1.77830257\n",
      "Iteration 3, loss = 12.53441412\n",
      "Iteration 36, loss = 1.58684695\n",
      "Iteration 4, loss = 9.60074270\n",
      "Iteration 37, loss = 2.46831353\n",
      "Iteration 5, loss = 8.14070777\n",
      "Iteration 38, loss = 1.98210879\n",
      "Iteration 39, loss = 1.79219236\n",
      "Iteration 6, loss = 7.67158273\n",
      "Iteration 40, loss = 2.13778707\n",
      "Iteration 7, loss = 6.11890938\n",
      "Iteration 41, loss = 2.26241954\n",
      "Iteration 8, loss = 5.68796324\n",
      "Iteration 42, loss = 3.28018396\n",
      "Iteration 9, loss = 6.02935369\n",
      "Iteration 43, loss = 3.31744566\n",
      "Iteration 10, loss = 6.46446403\n",
      "Iteration 44, loss = 2.71322407\n",
      "Iteration 11, loss = 5.83212688\n",
      "Iteration 45, loss = 2.70240050\n",
      "Iteration 12, loss = 4.05459044\n",
      "Iteration 46, loss = 2.17462884\n",
      "Iteration 13, loss = 3.88738514\n",
      "Iteration 47, loss = 2.57428514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 3.54120899\n",
      "Iteration 15, loss = 2.95742917\n",
      "Iteration 16, loss = 3.00057018\n",
      "Iteration 17, loss = 2.94609582\n",
      "Iteration 18, loss = 3.51344708\n",
      "Iteration 19, loss = 3.34083709\n",
      "Iteration 20, loss = 3.25547106\n",
      "Iteration 21, loss = 2.78357898\n",
      "Iteration 22, loss = 2.58556211\n",
      "Iteration 23, loss = 3.03913760\n",
      "Iteration 24, loss = 2.22205694\n",
      "Iteration 25, loss = 2.80762781\n",
      "Iteration 26, loss = 2.33563785\n",
      "Iteration 1, loss = 17.59933287\n",
      "Iteration 27, loss = 2.13160739\n",
      "Iteration 2, loss = 12.24718725\n",
      "Iteration 28, loss = 2.27769446\n",
      "Iteration 3, loss = 11.86068174\n",
      "Iteration 29, loss = 2.75992178\n",
      "Iteration 4, loss = 10.97081793\n",
      "Iteration 30, loss = 2.98583505\n",
      "Iteration 5, loss = 6.54136566\n",
      "Iteration 31, loss = 3.82086599\n",
      "Iteration 6, loss = 6.42940561\n",
      "Iteration 32, loss = 4.14533031\n",
      "Iteration 7, loss = 12.52516176\n",
      "Iteration 33, loss = 2.93096363\n",
      "Iteration 8, loss = 6.99953603\n",
      "Iteration 34, loss = 3.28469033\n",
      "Iteration 9, loss = 6.78889365\n",
      "Iteration 35, loss = 3.50385880\n",
      "Iteration 10, loss = 6.50327471\n",
      "Iteration 36, loss = 2.48220386\n",
      "Iteration 11, loss = 6.86310974\n",
      "Iteration 37, loss = 2.28010461\n",
      "Iteration 12, loss = 7.62933346\n",
      "Iteration 38, loss = 2.83493531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 5.07198678\n",
      "Iteration 14, loss = 4.60396624\n",
      "Iteration 15, loss = 4.53472188\n",
      "Iteration 16, loss = 4.71890880\n",
      "Iteration 17, loss = 4.09913864\n",
      "Iteration 18, loss = 4.01896702\n",
      "Iteration 19, loss = 3.04328819\n",
      "Iteration 20, loss = 3.25602105\n",
      "Iteration 21, loss = 3.51399698\n",
      "Iteration 22, loss = 2.96527988\n",
      "Iteration 23, loss = 3.26483887\n",
      "Iteration 24, loss = 2.92069948\n",
      "Iteration 25, loss = 2.25568603\n",
      "Iteration 1, loss = 17.76068830\n",
      "Iteration 26, loss = 2.39527617\n",
      "Iteration 2, loss = 12.53522298\n",
      "Iteration 27, loss = 2.10705087\n",
      "Iteration 3, loss = 8.80258335\n",
      "Iteration 28, loss = 2.10115833\n",
      "Iteration 4, loss = 7.68086812\n",
      "Iteration 29, loss = 2.02567629\n",
      "Iteration 5, loss = 7.75946243\n",
      "Iteration 30, loss = 1.99093559\n",
      "Iteration 6, loss = 11.56523335\n",
      "Iteration 31, loss = 2.00003121\n",
      "Iteration 7, loss = 6.61581619\n",
      "Iteration 32, loss = 1.71666978\n",
      "Iteration 8, loss = 5.39524794\n",
      "Iteration 33, loss = 1.95173726\n",
      "Iteration 9, loss = 4.83868579\n",
      "Iteration 34, loss = 1.99210912\n",
      "Iteration 10, loss = 6.63658782\n",
      "Iteration 35, loss = 1.54707108\n",
      "Iteration 11, loss = 8.37845656\n",
      "Iteration 36, loss = 1.16054870\n",
      "Iteration 12, loss = 9.04174823\n",
      "Iteration 37, loss = 1.53277154\n",
      "Iteration 13, loss = 6.15589615\n",
      "Iteration 38, loss = 1.99385587\n",
      "Iteration 14, loss = 5.32064509\n",
      "Iteration 39, loss = 1.66785493\n",
      "Iteration 15, loss = 7.72574653\n",
      "Iteration 40, loss = 1.78784455\n",
      "Iteration 16, loss = 4.13849156\n",
      "Iteration 41, loss = 1.84387770\n",
      "Iteration 17, loss = 5.51177611\n",
      "Iteration 42, loss = 1.23080693\n",
      "Iteration 18, loss = 6.93580429\n",
      "Iteration 43, loss = 1.16918637\n",
      "Iteration 19, loss = 4.71079810\n",
      "Iteration 44, loss = 1.68827441\n",
      "Iteration 20, loss = 4.66257161\n",
      "Iteration 45, loss = 1.81712432\n",
      "Iteration 21, loss = 5.07270874\n",
      "Iteration 46, loss = 1.41962144\n",
      "Iteration 22, loss = 9.19655161\n",
      "Iteration 47, loss = 1.10569918\n",
      "Iteration 23, loss = 6.90308106\n",
      "Iteration 48, loss = 1.22227113\n",
      "Iteration 24, loss = 6.36933534\n",
      "Iteration 49, loss = 1.19458699\n",
      "Iteration 25, loss = 4.11860222\n",
      "Iteration 50, loss = 1.41553475\n",
      "Iteration 26, loss = 4.36026848\n",
      "Iteration 51, loss = 1.71017097\n",
      "Iteration 27, loss = 4.16120510\n",
      "Iteration 52, loss = 1.48137291\n",
      "Iteration 28, loss = 4.25614095\n",
      "Iteration 53, loss = 1.54105048\n",
      "Iteration 29, loss = 3.23189048\n",
      "Iteration 54, loss = 1.26431186\n",
      "Iteration 30, loss = 3.58808081\n",
      "Iteration 55, loss = 1.29071254\n",
      "Iteration 31, loss = 3.24131060\n",
      "Iteration 56, loss = 1.14402071\n",
      "Iteration 32, loss = 3.26480547\n",
      "Iteration 57, loss = 1.19274092\n",
      "Iteration 33, loss = 3.50016240\n",
      "Iteration 58, loss = 0.75097826\n",
      "Iteration 34, loss = 2.81603714\n",
      "Iteration 59, loss = 0.97124078\n",
      "Iteration 35, loss = 3.16457163\n",
      "Iteration 60, loss = 0.93163691\n",
      "Iteration 36, loss = 2.77081562\n",
      "Iteration 61, loss = 1.15800078\n",
      "Iteration 37, loss = 2.48995409\n",
      "Iteration 62, loss = 1.26450198\n",
      "Iteration 38, loss = 3.64281654\n",
      "Iteration 63, loss = 1.03305159\n",
      "Iteration 39, loss = 2.77312562\n",
      "Iteration 64, loss = 0.96212129\n",
      "Iteration 40, loss = 2.52563543\n",
      "Iteration 65, loss = 0.94363600\n",
      "Iteration 41, loss = 2.35115314\n",
      "Iteration 66, loss = 0.65573317\n",
      "Iteration 42, loss = 2.60544315\n",
      "Iteration 67, loss = 1.03405175\n",
      "Iteration 43, loss = 2.12710883\n",
      "Iteration 68, loss = 1.21110969\n",
      "Iteration 44, loss = 2.14358191\n",
      "Iteration 69, loss = 0.93034630\n",
      "Iteration 45, loss = 1.93721791\n",
      "Iteration 70, loss = 0.84207827\n",
      "Iteration 46, loss = 1.79100332\n",
      "Iteration 71, loss = 1.27331224\n",
      "Iteration 47, loss = 1.73818526\n",
      "Iteration 72, loss = 1.02836691\n",
      "Iteration 48, loss = 1.44950574\n",
      "Iteration 73, loss = 0.93129545\n",
      "Iteration 49, loss = 1.34415840\n",
      "Iteration 74, loss = 0.89588716\n",
      "Iteration 50, loss = 0.98068620\n",
      "Iteration 75, loss = 1.15605727\n",
      "Iteration 51, loss = 1.20301666\n",
      "Iteration 76, loss = 1.50988042\n",
      "Iteration 52, loss = 0.96673736\n",
      "Iteration 77, loss = 1.12556115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.95756582\n",
      "Iteration 54, loss = 1.16775182\n",
      "Iteration 55, loss = 0.66959597\n",
      "Iteration 56, loss = 0.70283045\n",
      "Iteration 57, loss = 0.68324330\n",
      "Iteration 58, loss = 1.24122783\n",
      "Iteration 59, loss = 1.81099762\n",
      "Iteration 60, loss = 1.44642155\n",
      "Iteration 61, loss = 1.06931186\n",
      "Iteration 62, loss = 0.85344551\n",
      "Iteration 63, loss = 0.85255749\n",
      "Iteration 64, loss = 1.00969639\n",
      "Iteration 65, loss = 0.75665473\n",
      "Iteration 1, loss = 18.24312539\n",
      "Iteration 66, loss = 0.87400907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 23.24132318\n",
      "Iteration 3, loss = 14.12370968\n",
      "Iteration 4, loss = 13.65312102\n",
      "Iteration 5, loss = 11.70270512\n",
      "Iteration 6, loss = 10.10075881\n",
      "Iteration 7, loss = 9.59594996\n",
      "Iteration 8, loss = 9.55049101\n",
      "Iteration 9, loss = 9.37911805\n",
      "Iteration 10, loss = 9.51990975\n",
      "Iteration 11, loss = 9.54849709\n",
      "Iteration 12, loss = 11.32430266\n",
      "Iteration 13, loss = 10.26228957\n",
      "Iteration 14, loss = 9.07801839\n",
      "Iteration 1, loss = 17.68515690\n",
      "Iteration 15, loss = 10.09145372\n",
      "Iteration 2, loss = 17.68454455\n",
      "Iteration 16, loss = 12.52728751\n",
      "Iteration 3, loss = 13.16871281\n",
      "Iteration 17, loss = 10.79273117\n",
      "Iteration 4, loss = 11.08972129\n",
      "Iteration 18, loss = 9.14999348\n",
      "Iteration 5, loss = 10.27716404\n",
      "Iteration 19, loss = 10.23263523\n",
      "Iteration 6, loss = 9.76164250\n",
      "Iteration 20, loss = 10.21585734\n",
      "Iteration 7, loss = 9.48190366\n",
      "Iteration 21, loss = 9.43102298\n",
      "Iteration 8, loss = 11.33041405\n",
      "Iteration 22, loss = 10.25716008\n",
      "Iteration 9, loss = 10.76917842\n",
      "Iteration 23, loss = 11.91772644\n",
      "Iteration 24, loss = 10.38583789\n",
      "Iteration 10, loss = 12.42076652\n",
      "Iteration 11, loss = 11.20258956\n",
      "Iteration 25, loss = 10.16955617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 10.09975407\n",
      "Iteration 13, loss = 9.59777657\n",
      "Iteration 14, loss = 10.04727732\n",
      "Iteration 15, loss = 10.52361139\n",
      "Iteration 16, loss = 10.87649289\n",
      "Iteration 17, loss = 10.67983270\n",
      "Iteration 18, loss = 10.33298074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.89487756\n",
      "Iteration 2, loss = 18.87153729\n",
      "Iteration 3, loss = 13.26368112\n",
      "Iteration 4, loss = 11.15544233\n",
      "Iteration 5, loss = 11.97952769\n",
      "Iteration 6, loss = 12.40607138\n",
      "Iteration 7, loss = 12.31545539\n",
      "Iteration 8, loss = 13.03770307\n",
      "Iteration 1, loss = 21.33926069\n",
      "Iteration 9, loss = 13.19811571\n",
      "Iteration 2, loss = 17.16787253\n",
      "Iteration 10, loss = 11.97686113\n",
      "Iteration 3, loss = 15.75977400\n",
      "Iteration 11, loss = 10.88286686\n",
      "Iteration 4, loss = 13.31682494\n",
      "Iteration 12, loss = 11.73748853\n",
      "Iteration 5, loss = 12.26780509\n",
      "Iteration 13, loss = 12.17461379\n",
      "Iteration 6, loss = 10.55902157\n",
      "Iteration 14, loss = 11.96875417\n",
      "Iteration 7, loss = 10.26784692\n",
      "Iteration 15, loss = 12.58219453\n",
      "Iteration 8, loss = 10.07812671\n",
      "Iteration 16, loss = 11.67112731\n",
      "Iteration 9, loss = 10.14222520\n",
      "Iteration 17, loss = 10.97547975\n",
      "Iteration 10, loss = 11.96819653\n",
      "Iteration 18, loss = 11.16748531\n",
      "Iteration 11, loss = 11.71393751\n",
      "Iteration 19, loss = 10.55831757\n",
      "Iteration 12, loss = 10.33789379\n",
      "Iteration 20, loss = 10.24828016\n",
      "Iteration 13, loss = 11.41361284\n",
      "Iteration 21, loss = 10.12362071\n",
      "Iteration 14, loss = 10.16050893\n",
      "Iteration 22, loss = 10.30139932\n",
      "Iteration 15, loss = 10.37474374\n",
      "Iteration 23, loss = 11.89055303\n",
      "Iteration 16, loss = 9.67325383\n",
      "Iteration 24, loss = 11.20562874\n",
      "Iteration 17, loss = 10.01885524\n",
      "Iteration 25, loss = 10.75185080\n",
      "Iteration 18, loss = 10.62684131\n",
      "Iteration 26, loss = 10.85694869\n",
      "Iteration 19, loss = 11.14617422\n",
      "Iteration 27, loss = 10.69705547\n",
      "Iteration 20, loss = 9.93538744\n",
      "Iteration 28, loss = 10.88892726\n",
      "Iteration 21, loss = 9.70145159\n",
      "Iteration 29, loss = 10.71880109\n",
      "Iteration 22, loss = 9.66284927\n",
      "Iteration 23, loss = 9.11169691\n",
      "Iteration 30, loss = 10.73531214\n",
      "Iteration 24, loss = 10.32569858\n",
      "Iteration 31, loss = 10.75178007\n",
      "Iteration 25, loss = 9.13064032\n",
      "Iteration 32, loss = 10.60950173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 10.12697312\n",
      "Iteration 27, loss = 11.06788943\n",
      "Iteration 28, loss = 11.64988959\n",
      "Iteration 29, loss = 11.24463819\n",
      "Iteration 30, loss = 10.17779212\n",
      "Iteration 31, loss = 10.79682916\n",
      "Iteration 32, loss = 9.76485684\n",
      "Iteration 33, loss = 9.01346435\n",
      "Iteration 34, loss = 8.89326354\n",
      "Iteration 35, loss = 9.16795457\n",
      "Iteration 36, loss = 8.61889775\n",
      "Iteration 37, loss = 8.77622091\n",
      "Iteration 38, loss = 8.66152818\n",
      "Iteration 1, loss = 19.91261706\n",
      "Iteration 39, loss = 9.36867248\n",
      "Iteration 2, loss = 20.81734544\n",
      "Iteration 40, loss = 9.20894520\n",
      "Iteration 3, loss = 13.74871816\n",
      "Iteration 41, loss = 8.57978509\n",
      "Iteration 4, loss = 17.32184363\n",
      "Iteration 42, loss = 8.85331343\n",
      "Iteration 5, loss = 11.52936421\n",
      "Iteration 43, loss = 8.77378687\n",
      "Iteration 6, loss = 12.91249586\n",
      "Iteration 44, loss = 8.81145734\n",
      "Iteration 7, loss = 11.13466045\n",
      "Iteration 45, loss = 9.55434467\n",
      "Iteration 8, loss = 11.02556776\n",
      "Iteration 46, loss = 8.61069121\n",
      "Iteration 9, loss = 11.01795080\n",
      "Iteration 10, loss = 13.15068149\n",
      "Iteration 47, loss = 10.01878364\n",
      "Iteration 48, loss = 10.80292198\n",
      "Iteration 11, loss = 14.83949491\n",
      "Iteration 49, loss = 9.97969474\n",
      "Iteration 12, loss = 13.29171227\n",
      "Iteration 50, loss = 8.99799632\n",
      "Iteration 13, loss = 12.79130949\n",
      "Iteration 51, loss = 8.84115387\n",
      "Iteration 14, loss = 10.22778696\n",
      "Iteration 52, loss = 8.60771331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 9.87200070\n",
      "Iteration 16, loss = 10.40500749\n",
      "Iteration 17, loss = 10.38897876\n",
      "Iteration 18, loss = 11.60445686\n",
      "Iteration 19, loss = 9.81378042\n",
      "Iteration 20, loss = 10.33009153\n",
      "Iteration 21, loss = 11.23295355\n",
      "Iteration 22, loss = 10.59595301\n",
      "Iteration 23, loss = 10.50300006\n",
      "Iteration 24, loss = 9.58909952\n",
      "Iteration 25, loss = 10.47830744\n",
      "Iteration 26, loss = 10.92990591\n",
      "Iteration 27, loss = 10.94404194\n",
      "Iteration 1, loss = 16.67770252\n",
      "Iteration 28, loss = 9.89276197\n",
      "Iteration 2, loss = 22.76630962\n",
      "Iteration 29, loss = 9.82081392\n",
      "Iteration 3, loss = 15.05931146\n",
      "Iteration 30, loss = 9.98253666\n",
      "Iteration 4, loss = 16.22469867\n",
      "Iteration 31, loss = 9.55603313\n",
      "Iteration 5, loss = 14.28349940\n",
      "Iteration 32, loss = 9.05530367\n",
      "Iteration 6, loss = 13.82063478\n",
      "Iteration 33, loss = 9.10137058\n",
      "Iteration 7, loss = 11.37415351\n",
      "Iteration 34, loss = 8.98860942\n",
      "Iteration 8, loss = 12.78243198\n",
      "Iteration 35, loss = 8.55916968\n",
      "Iteration 9, loss = 11.35492436\n",
      "Iteration 36, loss = 8.63767555\n",
      "Iteration 10, loss = 12.72455607\n",
      "Iteration 37, loss = 8.51989536\n",
      "Iteration 38, loss = 9.38306244\n",
      "Iteration 11, loss = 12.30341523\n",
      "Iteration 39, loss = 9.14749106\n",
      "Iteration 12, loss = 10.35394183\n",
      "Iteration 40, loss = 9.02899302\n",
      "Iteration 13, loss = 10.59126760\n",
      "Iteration 14, loss = 9.95891672\n",
      "Iteration 41, loss = 8.67447892\n",
      "Iteration 15, loss = 10.82457348\n",
      "Iteration 42, loss = 8.35925296\n",
      "Iteration 16, loss = 10.71861803\n",
      "Iteration 43, loss = 8.94764919\n",
      "Iteration 17, loss = 10.39010896\n",
      "Iteration 44, loss = 8.71127346\n",
      "Iteration 18, loss = 10.51690797\n",
      "Iteration 45, loss = 9.65062202\n",
      "Iteration 19, loss = 11.02404905\n",
      "Iteration 46, loss = 10.00170029\n",
      "Iteration 20, loss = 10.93059927\n",
      "Iteration 47, loss = 9.17719046\n",
      "Iteration 21, loss = 10.63264020\n",
      "Iteration 48, loss = 8.86483271\n",
      "Iteration 22, loss = 9.85492753\n",
      "Iteration 49, loss = 8.98324760\n",
      "Iteration 23, loss = 9.43187063\n",
      "Iteration 50, loss = 8.70736672\n",
      "Iteration 51, loss = 8.41207998\n",
      "Iteration 24, loss = 8.73249445\n",
      "Iteration 52, loss = 8.62571152\n",
      "Iteration 25, loss = 9.01334465\n",
      "Iteration 53, loss = 8.50713921\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 9.52663971\n",
      "Iteration 27, loss = 9.21281104\n",
      "Iteration 28, loss = 9.40313516\n",
      "Iteration 29, loss = 9.59443042\n",
      "Iteration 30, loss = 8.81004894\n",
      "Iteration 31, loss = 8.61633071\n",
      "Iteration 32, loss = 9.64116218\n",
      "Iteration 33, loss = 8.97555443\n",
      "Iteration 34, loss = 8.81840063\n",
      "Iteration 35, loss = 8.34548010\n",
      "Iteration 36, loss = 7.83618374\n",
      "Iteration 37, loss = 9.17408106\n",
      "Iteration 38, loss = 8.51176485\n",
      "Iteration 1, loss = 18.42492172\n",
      "Iteration 39, loss = 8.43529860\n",
      "Iteration 2, loss = 17.55804841\n",
      "Iteration 40, loss = 8.79048022\n",
      "Iteration 3, loss = 14.23596481\n",
      "Iteration 41, loss = 8.24447146\n",
      "Iteration 4, loss = 12.24143927\n",
      "Iteration 42, loss = 9.05064761\n",
      "Iteration 5, loss = 13.42884500\n",
      "Iteration 43, loss = 9.38849354\n",
      "Iteration 6, loss = 13.35331659\n",
      "Iteration 44, loss = 8.91816030\n",
      "Iteration 7, loss = 11.61155931\n",
      "Iteration 45, loss = 9.11311039\n",
      "Iteration 8, loss = 11.81853210\n",
      "Iteration 46, loss = 8.87955031\n",
      "Iteration 9, loss = 12.59322956\n",
      "Iteration 47, loss = 8.65255599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 14.45255584\n",
      "Iteration 11, loss = 13.72535604\n",
      "Iteration 12, loss = 13.97680902\n",
      "Iteration 13, loss = 12.39406615\n",
      "Iteration 14, loss = 11.70138296\n",
      "Iteration 15, loss = 11.69881022\n",
      "Iteration 16, loss = 11.87279371\n",
      "Iteration 17, loss = 11.62809572\n",
      "Iteration 18, loss = 10.99269440\n",
      "Iteration 19, loss = 11.19471291\n",
      "Iteration 20, loss = 12.41715269\n",
      "Iteration 21, loss = 10.45658414\n",
      "Iteration 22, loss = 11.15245285\n",
      "Iteration 1, loss = 19.33847388\n",
      "Iteration 23, loss = 10.75316104\n",
      "Iteration 2, loss = 18.04940735\n",
      "Iteration 24, loss = 13.10872352\n",
      "Iteration 3, loss = 17.59540020\n",
      "Iteration 25, loss = 11.53924316\n",
      "Iteration 4, loss = 11.42415119\n",
      "Iteration 26, loss = 11.11085095\n",
      "Iteration 5, loss = 12.26519321\n",
      "Iteration 27, loss = 10.80824787\n",
      "Iteration 6, loss = 10.08267889\n",
      "Iteration 28, loss = 10.98030296\n",
      "Iteration 7, loss = 10.23596902\n",
      "Iteration 29, loss = 10.20720787\n",
      "Iteration 8, loss = 10.23890640\n",
      "Iteration 30, loss = 11.08143650\n",
      "Iteration 9, loss = 11.76938777\n",
      "Iteration 31, loss = 11.21461296\n",
      "Iteration 10, loss = 10.67225869\n",
      "Iteration 32, loss = 11.38434317\n",
      "Iteration 11, loss = 10.50672522\n",
      "Iteration 33, loss = 10.64568787\n",
      "Iteration 12, loss = 11.71748779\n",
      "Iteration 34, loss = 10.32797704\n",
      "Iteration 13, loss = 12.36312882\n",
      "Iteration 35, loss = 10.82968109\n",
      "Iteration 14, loss = 11.44611946\n",
      "Iteration 36, loss = 10.39563782\n",
      "Iteration 15, loss = 10.99882988\n",
      "Iteration 37, loss = 11.79909580\n",
      "Iteration 16, loss = 9.94859881\n",
      "Iteration 38, loss = 11.43162855\n",
      "Iteration 17, loss = 10.90125007\n",
      "Iteration 39, loss = 12.00309075\n",
      "Iteration 18, loss = 10.12868184\n",
      "Iteration 40, loss = 10.95427745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 9.74099191\n",
      "Iteration 20, loss = 10.25771469\n",
      "Iteration 21, loss = 9.80658286\n",
      "Iteration 22, loss = 9.55770159\n",
      "Iteration 23, loss = 9.84447772\n",
      "Iteration 24, loss = 8.70673738\n",
      "Iteration 25, loss = 9.71041506\n",
      "Iteration 26, loss = 10.16071777\n",
      "Iteration 27, loss = 9.98746124\n",
      "Iteration 28, loss = 8.79565997\n",
      "Iteration 29, loss = 9.63597663\n",
      "Iteration 30, loss = 9.33477811\n",
      "Iteration 31, loss = 8.68270973\n",
      "Iteration 1, loss = 16.85411091\n",
      "Iteration 32, loss = 8.54697701\n",
      "Iteration 2, loss = 14.69537931\n",
      "Iteration 33, loss = 9.98785425\n",
      "Iteration 3, loss = 14.80587673\n",
      "Iteration 34, loss = 9.39371944\n",
      "Iteration 4, loss = 10.79017091\n",
      "Iteration 35, loss = 8.88276204\n",
      "Iteration 5, loss = 10.62106427\n",
      "Iteration 6, loss = 9.06136888\n",
      "Iteration 36, loss = 8.56082753\n",
      "Iteration 7, loss = 12.94973839\n",
      "Iteration 37, loss = 9.22354750\n",
      "Iteration 8, loss = 16.43074785\n",
      "Iteration 38, loss = 8.90059933\n",
      "Iteration 9, loss = 14.80572071\n",
      "Iteration 39, loss = 8.76732057\n",
      "Iteration 10, loss = 11.74483109\n",
      "Iteration 40, loss = 9.14221063\n",
      "Iteration 11, loss = 11.22192393\n",
      "Iteration 41, loss = 9.01300623\n",
      "Iteration 12, loss = 11.41681121\n",
      "Iteration 42, loss = 8.80356510\n",
      "Iteration 13, loss = 11.14280271\n",
      "Iteration 43, loss = 8.50653566\n",
      "Iteration 14, loss = 11.64351234\n",
      "Iteration 44, loss = 9.41834149\n",
      "Iteration 45, loss = 8.28670936\n",
      "Iteration 15, loss = 11.68515903\n",
      "Iteration 46, loss = 8.21147896\n",
      "Iteration 16, loss = 10.93784092\n",
      "Iteration 17, loss = 10.91341437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 8.21677904\n",
      "Iteration 48, loss = 8.53605124\n",
      "Iteration 49, loss = 8.26336988\n",
      "Iteration 50, loss = 8.22408431\n",
      "Iteration 51, loss = 8.62022729\n",
      "Iteration 52, loss = 8.70481957\n",
      "Iteration 53, loss = 8.66860309\n",
      "Iteration 54, loss = 9.41764288\n",
      "Iteration 55, loss = 8.95563169\n",
      "Iteration 56, loss = 8.93504143\n",
      "Iteration 57, loss = 9.35086353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.35880888\n",
      "Iteration 2, loss = 14.69058124\n",
      "Iteration 3, loss = 15.60306999\n",
      "Iteration 4, loss = 11.57078745\n",
      "Iteration 5, loss = 10.57782450\n",
      "Iteration 6, loss = 9.64589407\n",
      "Iteration 7, loss = 9.97403946\n",
      "Iteration 8, loss = 12.06247426\n",
      "Iteration 9, loss = 11.04784579\n",
      "Iteration 10, loss = 11.29330507\n",
      "Iteration 11, loss = 11.19777618\n",
      "Iteration 1, loss = 20.82685823\n",
      "Iteration 12, loss = 11.34258532\n",
      "Iteration 2, loss = 20.16063022\n",
      "Iteration 13, loss = 11.58695820\n",
      "Iteration 3, loss = 14.63196822\n",
      "Iteration 14, loss = 10.57901640\n",
      "Iteration 4, loss = 11.46665059\n",
      "Iteration 15, loss = 10.08850068\n",
      "Iteration 5, loss = 12.45324906\n",
      "Iteration 16, loss = 10.36539676\n",
      "Iteration 6, loss = 12.72122783\n",
      "Iteration 17, loss = 10.83042964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 12.04834918\n",
      "Iteration 8, loss = 14.33111849\n",
      "Iteration 9, loss = 11.69663570\n",
      "Iteration 10, loss = 11.47614352\n",
      "Iteration 11, loss = 11.37066616\n",
      "Iteration 12, loss = 11.46393506\n",
      "Iteration 13, loss = 11.06763078\n",
      "Iteration 14, loss = 11.23322560\n",
      "Iteration 15, loss = 13.31749574\n",
      "Iteration 16, loss = 11.59844321\n",
      "Iteration 17, loss = 12.21153322\n",
      "Iteration 18, loss = 11.68172575\n",
      "Iteration 19, loss = 11.02817332\n",
      "Iteration 20, loss = 11.07559575\n",
      "Iteration 1, loss = 19.27218270\n",
      "Iteration 21, loss = 11.62160053\n",
      "Iteration 2, loss = 16.51864084\n",
      "Iteration 22, loss = 11.50075474\n",
      "Iteration 3, loss = 12.98942565\n",
      "Iteration 23, loss = 11.06120702\n",
      "Iteration 4, loss = 10.52511451\n",
      "Iteration 24, loss = 10.73094677\n",
      "Iteration 5, loss = 10.76877305\n",
      "Iteration 25, loss = 11.14146467\n",
      "Iteration 6, loss = 12.49664625\n",
      "Iteration 26, loss = 11.39064642\n",
      "Iteration 7, loss = 14.05385475\n",
      "Iteration 27, loss = 10.89560496\n",
      "Iteration 8, loss = 11.95347255\n",
      "Iteration 28, loss = 10.08412231\n",
      "Iteration 9, loss = 10.89456678\n",
      "Iteration 29, loss = 10.52448576\n",
      "Iteration 10, loss = 11.54416541\n",
      "Iteration 30, loss = 10.84765129\n",
      "Iteration 11, loss = 11.74776048\n",
      "Iteration 31, loss = 11.36680948\n",
      "Iteration 12, loss = 12.13929758\n",
      "Iteration 32, loss = 10.70211912\n",
      "Iteration 13, loss = 11.77618695\n",
      "Iteration 33, loss = 10.74138806\n",
      "Iteration 14, loss = 11.40255128\n",
      "Iteration 34, loss = 10.78097872\n",
      "Iteration 15, loss = 12.64990960\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 10.07708018\n",
      "Iteration 36, loss = 10.31560609\n",
      "Iteration 37, loss = 9.96470853\n",
      "Iteration 38, loss = 9.89101195\n",
      "Iteration 39, loss = 10.08963977\n",
      "Iteration 40, loss = 10.01137534\n",
      "Iteration 41, loss = 10.95323251\n",
      "Iteration 42, loss = 11.54005518\n",
      "Iteration 43, loss = 11.18018143\n",
      "Iteration 44, loss = 9.99530398\n",
      "Iteration 45, loss = 10.26881311\n",
      "Iteration 46, loss = 10.54535358\n",
      "Iteration 47, loss = 9.76344589\n",
      "Iteration 1, loss = 20.75030465\n",
      "Iteration 48, loss = 9.45148692\n",
      "Iteration 2, loss = 24.83820534\n",
      "Iteration 49, loss = 9.61171394\n",
      "Iteration 3, loss = 16.20881734\n",
      "Iteration 50, loss = 9.53874543\n",
      "Iteration 4, loss = 14.15320262\n",
      "Iteration 51, loss = 9.38549651\n",
      "Iteration 5, loss = 10.48337852\n",
      "Iteration 52, loss = 9.45205959\n",
      "Iteration 6, loss = 10.38640090\n",
      "Iteration 53, loss = 9.89870727\n",
      "Iteration 7, loss = 12.36440747\n",
      "Iteration 54, loss = 9.85989147\n",
      "Iteration 8, loss = 13.39496992\n",
      "Iteration 55, loss = 9.34941776\n",
      "Iteration 9, loss = 10.33574083\n",
      "Iteration 56, loss = 9.38844899\n",
      "Iteration 10, loss = 10.44701975\n",
      "Iteration 57, loss = 9.74175181\n",
      "Iteration 11, loss = 12.59657369\n",
      "Iteration 58, loss = 10.09884146\n",
      "Iteration 12, loss = 12.17947809\n",
      "Iteration 59, loss = 10.18149597\n",
      "Iteration 13, loss = 11.01913784\n",
      "Iteration 60, loss = 9.90915012\n",
      "Iteration 14, loss = 11.90237453\n",
      "Iteration 61, loss = 10.11161410\n",
      "Iteration 15, loss = 11.37424658\n",
      "Iteration 62, loss = 10.66693302\n",
      "Iteration 16, loss = 13.05976629\n",
      "Iteration 63, loss = 10.71732907\n",
      "Iteration 17, loss = 12.31915927\n",
      "Iteration 64, loss = 10.45157378\n",
      "Iteration 18, loss = 13.01653000\n",
      "Iteration 65, loss = 10.77629498\n",
      "Iteration 19, loss = 13.65422880\n",
      "Iteration 66, loss = 9.87810675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 12.99846109\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.80667221\n",
      "Iteration 2, loss = 19.31901783\n",
      "Iteration 1, loss = 16.62934017\n",
      "Iteration 2, loss = 15.45714350\n",
      "Iteration 3, loss = 10.29575967\n",
      "Iteration 3, loss = 10.70259000\n",
      "Iteration 4, loss = 12.28954414\n",
      "Iteration 4, loss = 12.49265212\n",
      "Iteration 5, loss = 13.81267019\n",
      "Iteration 5, loss = 13.49594652\n",
      "Iteration 6, loss = 11.23521187\n",
      "Iteration 6, loss = 9.94097063\n",
      "Iteration 7, loss = 11.03539308\n",
      "Iteration 7, loss = 9.64501582\n",
      "Iteration 8, loss = 12.88044849\n",
      "Iteration 8, loss = 11.68060187\n",
      "Iteration 9, loss = 12.78745199\n",
      "Iteration 10, loss = 10.48397378\n",
      "Iteration 9, loss = 15.71067503\n",
      "Iteration 11, loss = 10.38447377\n",
      "Iteration 10, loss = 16.28650618\n",
      "Iteration 12, loss = 10.99218149\n",
      "Iteration 11, loss = 12.97865134\n",
      "Iteration 12, loss = 14.97733728\n",
      "Iteration 13, loss = 10.39244522\n",
      "Iteration 13, loss = 13.88830448\n",
      "Iteration 14, loss = 10.15351240\n",
      "Iteration 14, loss = 12.88704733\n",
      "Iteration 15, loss = 10.77581791\n",
      "Iteration 15, loss = 12.23321897\n",
      "Iteration 16, loss = 12.10701426\n",
      "Iteration 16, loss = 12.33524554\n",
      "Iteration 17, loss = 10.74447058\n",
      "Iteration 17, loss = 12.16655281\n",
      "Iteration 18, loss = 11.73711214\n",
      "Iteration 18, loss = 11.66245574\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 11.62811887\n",
      "Iteration 20, loss = 10.88166651\n",
      "Iteration 21, loss = 11.14480929\n",
      "Iteration 22, loss = 9.83567298\n",
      "Iteration 23, loss = 9.66214969\n",
      "Iteration 24, loss = 10.02995790\n",
      "Iteration 25, loss = 9.48917486\n",
      "Iteration 26, loss = 9.57464223\n",
      "Iteration 27, loss = 9.96984832\n",
      "Iteration 28, loss = 10.67736248\n",
      "Iteration 29, loss = 10.75742017\n",
      "Iteration 30, loss = 9.62233503\n",
      "Iteration 31, loss = 9.46733565\n",
      "Iteration 1, loss = 18.72685343\n",
      "Iteration 32, loss = 9.43155685\n",
      "Iteration 2, loss = 13.48125938\n",
      "Iteration 33, loss = 9.51090049\n",
      "Iteration 3, loss = 13.69327805\n",
      "Iteration 34, loss = 10.68868799\n",
      "Iteration 4, loss = 10.84550485\n",
      "Iteration 35, loss = 10.17882204\n",
      "Iteration 5, loss = 12.48721609\n",
      "Iteration 36, loss = 9.39601128\n",
      "Iteration 6, loss = 13.60703669\n",
      "Iteration 37, loss = 8.96941992\n",
      "Iteration 7, loss = 13.36194324\n",
      "Iteration 38, loss = 9.28778421\n",
      "Iteration 8, loss = 11.65180518\n",
      "Iteration 39, loss = 9.21055738\n",
      "Iteration 9, loss = 12.61112333\n",
      "Iteration 40, loss = 9.20939902\n",
      "Iteration 10, loss = 11.60955576\n",
      "Iteration 41, loss = 9.12978481\n",
      "Iteration 11, loss = 11.49584665Iteration 42, loss = 8.69760081\n",
      "\n",
      "Iteration 12, loss = 11.03751913\n",
      "Iteration 43, loss = 9.24361076\n",
      "Iteration 13, loss = 11.51334791\n",
      "Iteration 44, loss = 8.92930158\n",
      "Iteration 14, loss = 10.33980588\n",
      "Iteration 45, loss = 9.04738889\n",
      "Iteration 46, loss = 9.47695869\n",
      "Iteration 15, loss = 10.27202927\n",
      "Iteration 47, loss = 9.70941656\n",
      "Iteration 16, loss = 10.50084131\n",
      "Iteration 48, loss = 9.39460868\n",
      "Iteration 17, loss = 11.07149828\n",
      "Iteration 49, loss = 9.39735604\n",
      "Iteration 18, loss = 9.95273421\n",
      "Iteration 50, loss = 9.69995966\n",
      "Iteration 19, loss = 10.49528848\n",
      "Iteration 51, loss = 9.08501142\n",
      "Iteration 20, loss = 10.84033900\n",
      "Iteration 52, loss = 9.32308143\n",
      "Iteration 21, loss = 10.29686212\n",
      "Iteration 53, loss = 9.48555129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 9.86897279\n",
      "Iteration 23, loss = 10.26436311\n",
      "Iteration 24, loss = 10.15012133\n",
      "Iteration 25, loss = 9.87661318\n",
      "Iteration 26, loss = 9.72073890\n",
      "Iteration 27, loss = 10.31151651\n",
      "Iteration 28, loss = 11.05895453\n",
      "Iteration 29, loss = 10.15808306\n",
      "Iteration 30, loss = 9.92241474\n",
      "Iteration 31, loss = 8.86532627\n",
      "Iteration 32, loss = 9.61389334\n",
      "Iteration 33, loss = 9.65603810\n",
      "Iteration 34, loss = 9.73742106\n",
      "Iteration 1, loss = 18.40934891\n",
      "Iteration 35, loss = 9.22967448\n",
      "Iteration 2, loss = 15.98927399\n",
      "Iteration 36, loss = 9.15250571\n",
      "Iteration 3, loss = 11.03522229\n",
      "Iteration 37, loss = 9.07564074\n",
      "Iteration 4, loss = 9.29048191\n",
      "Iteration 38, loss = 9.43160130\n",
      "Iteration 5, loss = 11.13977485\n",
      "Iteration 39, loss = 9.86457198\n",
      "Iteration 6, loss = 15.43740696\n",
      "Iteration 40, loss = 9.27628136\n",
      "Iteration 7, loss = 12.55899116\n",
      "Iteration 41, loss = 10.02205666\n",
      "Iteration 8, loss = 10.57223986\n",
      "Iteration 42, loss = 9.00170300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 11.49531851\n",
      "Iteration 10, loss = 11.30594985\n",
      "Iteration 11, loss = 10.87087184\n",
      "Iteration 12, loss = 12.21089221\n",
      "Iteration 13, loss = 11.92834407\n",
      "Iteration 14, loss = 13.60153244\n",
      "Iteration 15, loss = 12.20786037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.28282734\n",
      "Iteration 2, loss = 15.84151587\n",
      "Iteration 3, loss = 11.86422012\n",
      "Iteration 4, loss = 11.17695511\n",
      "Iteration 5, loss = 12.23308256\n",
      "Iteration 6, loss = 16.35646168\n",
      "Iteration 7, loss = 14.52329161\n",
      "Iteration 8, loss = 16.11460780\n",
      "Iteration 1, loss = 15.78420728\n",
      "Iteration 9, loss = 12.33232407\n",
      "Iteration 2, loss = 16.51866818\n",
      "Iteration 10, loss = 11.41651804\n",
      "Iteration 3, loss = 14.67178640\n",
      "Iteration 11, loss = 10.38539781\n",
      "Iteration 4, loss = 14.50498722\n",
      "Iteration 12, loss = 14.25849531\n",
      "Iteration 5, loss = 11.24808541\n",
      "Iteration 13, loss = 11.40799406\n",
      "Iteration 6, loss = 12.29342576\n",
      "Iteration 14, loss = 11.08047845\n",
      "Iteration 7, loss = 11.69801909\n",
      "Iteration 15, loss = 11.33409973\n",
      "Iteration 8, loss = 9.87625411\n",
      "Iteration 16, loss = 11.02817688\n",
      "Iteration 9, loss = 10.88928201\n",
      "Iteration 17, loss = 10.98938242\n",
      "Iteration 10, loss = 11.98591643\n",
      "Iteration 18, loss = 11.57505379\n",
      "Iteration 11, loss = 11.15019505\n",
      "Iteration 19, loss = 11.63535138\n",
      "Iteration 12, loss = 10.63437015\n",
      "Iteration 20, loss = 10.81749312\n",
      "Iteration 13, loss = 9.58152648\n",
      "Iteration 21, loss = 11.08158262\n",
      "Iteration 14, loss = 9.86330032\n",
      "Iteration 22, loss = 11.21455544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 10.46292442\n",
      "Iteration 16, loss = 9.94557566\n",
      "Iteration 17, loss = 10.30692061\n",
      "Iteration 18, loss = 9.86776872\n",
      "Iteration 19, loss = 10.51962059\n",
      "Iteration 20, loss = 10.65373448\n",
      "Iteration 21, loss = 9.22614652\n",
      "Iteration 22, loss = 9.32297553\n",
      "Iteration 23, loss = 10.04307317\n",
      "Iteration 24, loss = 9.50252171\n",
      "Iteration 25, loss = 10.09602146\n",
      "Iteration 26, loss = 9.19625236\n",
      "Iteration 27, loss = 9.04022310\n",
      "Iteration 1, loss = 17.84335746\n",
      "Iteration 28, loss = 8.84309675\n",
      "Iteration 2, loss = 17.77091247\n",
      "Iteration 29, loss = 8.69677387\n",
      "Iteration 3, loss = 18.59204706\n",
      "Iteration 30, loss = 8.97856833\n",
      "Iteration 4, loss = 10.68277854\n",
      "Iteration 31, loss = 9.33429276\n",
      "Iteration 5, loss = 9.26794742\n",
      "Iteration 32, loss = 8.86481260\n",
      "Iteration 6, loss = 10.14393119\n",
      "Iteration 33, loss = 8.84882590\n",
      "Iteration 7, loss = 9.55528831\n",
      "Iteration 34, loss = 8.58797327\n",
      "Iteration 8, loss = 10.29432264\n",
      "Iteration 35, loss = 8.78200126\n",
      "Iteration 9, loss = 14.33962964\n",
      "Iteration 36, loss = 9.17089082\n",
      "Iteration 10, loss = 12.90397237\n",
      "Iteration 37, loss = 9.83389898\n",
      "Iteration 11, loss = 12.71912152\n",
      "Iteration 38, loss = 9.63740777\n",
      "Iteration 12, loss = 11.75636203\n",
      "Iteration 39, loss = 8.67990780\n",
      "Iteration 13, loss = 9.96593251\n",
      "Iteration 40, loss = 9.34156221\n",
      "Iteration 14, loss = 9.85399603\n",
      "Iteration 41, loss = 9.56359445\n",
      "Iteration 15, loss = 9.91343716\n",
      "Iteration 42, loss = 9.30843518\n",
      "Iteration 16, loss = 12.97245444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 8.81201665\n",
      "Iteration 44, loss = 8.46427418\n",
      "Iteration 45, loss = 8.85762700\n",
      "Iteration 46, loss = 8.31634788\n",
      "Iteration 47, loss = 8.41744471\n",
      "Iteration 48, loss = 8.55814298\n",
      "Iteration 49, loss = 8.72904385\n",
      "Iteration 50, loss = 8.93312487\n",
      "Iteration 51, loss = 8.50488577\n",
      "Iteration 52, loss = 8.78114958\n",
      "Iteration 53, loss = 8.82119830\n",
      "Iteration 54, loss = 8.66561223\n",
      "Iteration 55, loss = 8.66521626\n",
      "Iteration 1, loss = 20.32989751\n",
      "Iteration 56, loss = 9.40826698\n",
      "Iteration 2, loss = 25.04434076\n",
      "Iteration 57, loss = 9.01401049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 16.24166203\n",
      "Iteration 4, loss = 12.79488158\n",
      "Iteration 5, loss = 12.28255685\n",
      "Iteration 6, loss = 10.03094162\n",
      "Iteration 7, loss = 10.86789477\n",
      "Iteration 8, loss = 10.28094288\n",
      "Iteration 9, loss = 10.66295754\n",
      "Iteration 10, loss = 13.43886137\n",
      "Iteration 11, loss = 13.12255572\n",
      "Iteration 12, loss = 12.61334237\n",
      "Iteration 13, loss = 11.93051302\n",
      "Iteration 14, loss = 11.12543327\n",
      "Iteration 15, loss = 11.81446905\n",
      "Iteration 16, loss = 9.96935266\n",
      "Iteration 1, loss = 16.70739198\n",
      "Iteration 17, loss = 10.75432806\n",
      "Iteration 2, loss = 18.02830915\n",
      "Iteration 18, loss = 10.75847441\n",
      "Iteration 3, loss = 11.33582046\n",
      "Iteration 19, loss = 11.91669647\n",
      "Iteration 4, loss = 11.46023491\n",
      "Iteration 20, loss = 11.29331213\n",
      "Iteration 5, loss = 12.14769069\n",
      "Iteration 21, loss = 10.96680847\n",
      "Iteration 6, loss = 12.58621125\n",
      "Iteration 22, loss = 10.51525750\n",
      "Iteration 7, loss = 11.29250148\n",
      "Iteration 23, loss = 10.80333082\n",
      "Iteration 8, loss = 11.02553329\n",
      "Iteration 24, loss = 10.57747690\n",
      "Iteration 9, loss = 10.13633252\n",
      "Iteration 25, loss = 10.31097002\n",
      "Iteration 10, loss = 10.83874799\n",
      "Iteration 26, loss = 9.87696088\n",
      "Iteration 11, loss = 11.03506391\n",
      "Iteration 27, loss = 9.79818335\n",
      "Iteration 12, loss = 12.36056049\n",
      "Iteration 28, loss = 10.58893329\n",
      "Iteration 13, loss = 11.16047665\n",
      "Iteration 29, loss = 10.79155885\n",
      "Iteration 14, loss = 10.86345717\n",
      "Iteration 30, loss = 10.20681421\n",
      "Iteration 15, loss = 11.41354091\n",
      "Iteration 31, loss = 10.01493391\n",
      "Iteration 16, loss = 10.23194506\n",
      "Iteration 32, loss = 9.78403980\n",
      "Iteration 17, loss = 9.85377548\n",
      "Iteration 33, loss = 9.94646758\n",
      "Iteration 18, loss = 10.19267738\n",
      "Iteration 34, loss = 9.00962243\n",
      "Iteration 19, loss = 9.36178764\n",
      "Iteration 35, loss = 9.40440620\n",
      "Iteration 20, loss = 9.57442577\n",
      "Iteration 36, loss = 9.87732807\n",
      "Iteration 21, loss = 10.50258716\n",
      "Iteration 37, loss = 9.80295872\n",
      "Iteration 22, loss = 10.55784539\n",
      "Iteration 38, loss = 9.41532121\n",
      "Iteration 23, loss = 9.85247533\n",
      "Iteration 39, loss = 8.78799925\n",
      "Iteration 24, loss = 9.05544344\n",
      "Iteration 40, loss = 8.94407811\n",
      "Iteration 25, loss = 9.58358052\n",
      "Iteration 41, loss = 10.90985059\n",
      "Iteration 26, loss = 9.51534425\n",
      "Iteration 42, loss = 11.89558242\n",
      "Iteration 27, loss = 10.02908488\n",
      "Iteration 43, loss = 10.82747876\n",
      "Iteration 28, loss = 9.76003394\n",
      "Iteration 44, loss = 10.19309071\n",
      "Iteration 29, loss = 10.07937418\n",
      "Iteration 45, loss = 9.60534831\n",
      "Iteration 30, loss = 9.57319593\n",
      "Iteration 46, loss = 10.23421451\n",
      "Iteration 31, loss = 9.50170316\n",
      "Iteration 47, loss = 10.51065182\n",
      "Iteration 32, loss = 9.31396512\n",
      "Iteration 48, loss = 9.61177146\n",
      "Iteration 33, loss = 8.88998809\n",
      "Iteration 49, loss = 9.73579488\n",
      "Iteration 34, loss = 8.97428066\n",
      "Iteration 50, loss = 10.81205938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 8.89643935\n",
      "Iteration 36, loss = 8.47018453\n",
      "Iteration 37, loss = 9.99657694\n",
      "Iteration 38, loss = 8.78636748\n",
      "Iteration 39, loss = 8.59354775\n",
      "Iteration 40, loss = 8.91024119\n",
      "Iteration 41, loss = 9.42409845\n",
      "Iteration 42, loss = 8.99388292\n",
      "Iteration 43, loss = 8.48299634\n",
      "Iteration 44, loss = 9.03119933\n",
      "Iteration 45, loss = 11.07756434\n",
      "Iteration 46, loss = 10.53656522\n",
      "Iteration 47, loss = 10.46948641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.73212719\n",
      "Iteration 2, loss = 13.84587394\n",
      "Iteration 3, loss = 13.30393874\n",
      "Iteration 4, loss = 11.84574636\n",
      "Iteration 5, loss = 12.71218425\n",
      "Iteration 6, loss = 13.11401464\n",
      "Iteration 7, loss = 10.90845263\n",
      "Iteration 8, loss = 10.99889798\n",
      "Iteration 9, loss = 14.28493135\n",
      "Iteration 10, loss = 16.72694442\n",
      "Iteration 11, loss = 12.42778252\n",
      "Iteration 12, loss = 12.97958691\n",
      "Iteration 13, loss = 11.45866806\n",
      "Iteration 1, loss = 17.81441894\n",
      "Iteration 14, loss = 11.39061849\n",
      "Iteration 2, loss = 14.34349613\n",
      "Iteration 15, loss = 11.06136276\n",
      "Iteration 3, loss = 12.57532496\n",
      "Iteration 16, loss = 11.36362964\n",
      "Iteration 4, loss = 12.19276648\n",
      "Iteration 17, loss = 12.19318589\n",
      "Iteration 5, loss = 9.25556907\n",
      "Iteration 18, loss = 11.77921048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 10.20299380\n",
      "Iteration 7, loss = 10.45889997\n",
      "Iteration 8, loss = 10.69446495\n",
      "Iteration 9, loss = 11.45675187\n",
      "Iteration 10, loss = 10.05222220\n",
      "Iteration 11, loss = 9.11037001\n",
      "Iteration 12, loss = 8.62801183\n",
      "Iteration 13, loss = 10.45891720\n",
      "Iteration 14, loss = 9.58961463\n",
      "Iteration 15, loss = 9.56202946\n",
      "Iteration 16, loss = 8.84900359\n",
      "Iteration 17, loss = 10.24081235\n",
      "Iteration 18, loss = 9.82347189\n",
      "Iteration 1, loss = 17.92350893\n",
      "Iteration 19, loss = 9.67825294\n",
      "Iteration 2, loss = 13.35008934\n",
      "Iteration 20, loss = 9.99759258\n",
      "Iteration 3, loss = 11.96689918\n",
      "Iteration 21, loss = 9.72833929\n",
      "Iteration 4, loss = 14.96995660\n",
      "Iteration 22, loss = 9.10680379\n",
      "Iteration 5, loss = 14.62572192\n",
      "Iteration 23, loss = 8.33079492\n",
      "Iteration 6, loss = 10.90504408\n",
      "Iteration 7, loss = 14.68893414\n",
      "Iteration 24, loss = 9.28090188\n",
      "Iteration 8, loss = 12.68536835\n",
      "Iteration 25, loss = 9.13247107\n",
      "Iteration 9, loss = 11.42356141\n",
      "Iteration 26, loss = 9.41508854\n",
      "Iteration 27, loss = 9.81064905\n",
      "Iteration 10, loss = 10.25270709\n",
      "Iteration 28, loss = 9.81668345\n",
      "Iteration 11, loss = 9.55303920\n",
      "Iteration 29, loss = 8.60447605\n",
      "Iteration 12, loss = 10.28636819\n",
      "Iteration 30, loss = 7.94479536\n",
      "Iteration 13, loss = 11.35237334\n",
      "Iteration 31, loss = 8.42440342\n",
      "Iteration 14, loss = 11.09604599\n",
      "Iteration 32, loss = 8.70270611\n",
      "Iteration 15, loss = 10.66895324\n",
      "Iteration 16, loss = 9.63892014\n",
      "Iteration 33, loss = 8.27744065\n",
      "Iteration 34, loss = 9.10720762\n",
      "Iteration 17, loss = 11.65778389\n",
      "Iteration 35, loss = 9.18929048\n",
      "Iteration 18, loss = 11.82506048\n",
      "Iteration 36, loss = 8.79893935\n",
      "Iteration 19, loss = 10.88030918\n",
      "Iteration 37, loss = 10.92062000\n",
      "Iteration 20, loss = 9.79066676\n",
      "Iteration 38, loss = 10.30031870\n",
      "Iteration 21, loss = 9.40976796\n",
      "Iteration 39, loss = 9.01784743\n",
      "Iteration 22, loss = 10.16781556\n",
      "Iteration 40, loss = 8.29123610\n",
      "Iteration 23, loss = 9.86621387\n",
      "Iteration 41, loss = 8.38713241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 8.66423007\n",
      "Iteration 25, loss = 9.49903062\n",
      "Iteration 26, loss = 10.25211267\n",
      "Iteration 27, loss = 9.90418400\n",
      "Iteration 28, loss = 9.67180415\n",
      "Iteration 29, loss = 9.83026959\n",
      "Iteration 30, loss = 9.40775657\n",
      "Iteration 31, loss = 9.93462147\n",
      "Iteration 32, loss = 9.00927119\n",
      "Iteration 33, loss = 8.62737927\n",
      "Iteration 34, loss = 8.21159370\n",
      "Iteration 35, loss = 8.42780925\n",
      "Iteration 36, loss = 7.90422116\n",
      "Iteration 1, loss = 18.74676522\n",
      "Iteration 37, loss = 7.72853128\n",
      "Iteration 2, loss = 15.08637829\n",
      "Iteration 3, loss = 12.71033247\n",
      "Iteration 38, loss = 7.78411607\n",
      "Iteration 4, loss = 13.86848997\n",
      "Iteration 39, loss = 7.87678376\n",
      "Iteration 5, loss = 10.90516934\n",
      "Iteration 40, loss = 8.12306854\n",
      "Iteration 6, loss = 9.93716443\n",
      "Iteration 41, loss = 7.97501887\n",
      "Iteration 7, loss = 10.66694115\n",
      "Iteration 42, loss = 7.11698119\n",
      "Iteration 8, loss = 10.15947883\n",
      "Iteration 43, loss = 8.38129235\n",
      "Iteration 9, loss = 9.77140032\n",
      "Iteration 44, loss = 7.95579846\n",
      "Iteration 10, loss = 10.43865132\n",
      "Iteration 45, loss = 8.27206953\n",
      "Iteration 11, loss = 10.88738904\n",
      "Iteration 46, loss = 8.54808650\n",
      "Iteration 12, loss = 10.35837863\n",
      "Iteration 47, loss = 9.13995175\n",
      "Iteration 13, loss = 11.61828607\n",
      "Iteration 48, loss = 8.04902765\n",
      "Iteration 14, loss = 11.89716027\n",
      "Iteration 49, loss = 8.62962701\n",
      "Iteration 15, loss = 13.20561573\n",
      "Iteration 50, loss = 8.27473887\n",
      "Iteration 16, loss = 11.16305334\n",
      "Iteration 51, loss = 8.46907334\n",
      "Iteration 17, loss = 10.43647803\n",
      "Iteration 52, loss = 8.59480661\n",
      "Iteration 18, loss = 9.49349393\n",
      "Iteration 53, loss = 8.28473540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 9.29588498\n",
      "Iteration 20, loss = 9.33073630\n",
      "Iteration 21, loss = 8.81589397\n",
      "Iteration 22, loss = 8.30821420\n",
      "Iteration 23, loss = 9.20882943\n",
      "Iteration 24, loss = 8.02843549\n",
      "Iteration 25, loss = 8.81247060\n",
      "Iteration 26, loss = 8.30828884\n",
      "Iteration 27, loss = 8.59020805\n",
      "Iteration 28, loss = 8.67007415\n",
      "Iteration 29, loss = 8.04468856\n",
      "Iteration 30, loss = 9.02826520\n",
      "Iteration 31, loss = 8.52146283\n",
      "Iteration 1, loss = 15.44134609\n",
      "Iteration 32, loss = 8.60175188\n",
      "Iteration 33, loss = 8.75881495\n",
      "Iteration 2, loss = 18.03023138\n",
      "Iteration 34, loss = 9.34926480\n",
      "Iteration 3, loss = 14.79739098\n",
      "Iteration 35, loss = 8.92049876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 13.38014189\n",
      "Iteration 5, loss = 13.21041707\n",
      "Iteration 6, loss = 11.43638805\n",
      "Iteration 7, loss = 12.84552787\n",
      "Iteration 8, loss = 13.20846816\n",
      "Iteration 9, loss = 12.90510257\n",
      "Iteration 10, loss = 14.91903079\n",
      "Iteration 11, loss = 12.51499438\n",
      "Iteration 12, loss = 13.80483600\n",
      "Iteration 13, loss = 13.38505847\n",
      "Iteration 14, loss = 12.68340226\n",
      "Iteration 15, loss = 11.41276697\n",
      "Iteration 16, loss = 11.26115009\n",
      "Iteration 1, loss = 17.53007810\n",
      "Iteration 17, loss = 12.94479919\n",
      "Iteration 18, loss = 14.53687730\n",
      "Iteration 2, loss = 21.46931054\n",
      "Iteration 19, loss = 13.41753484\n",
      "Iteration 3, loss = 15.11870984\n",
      "Iteration 4, loss = 13.90739464\n",
      "Iteration 20, loss = 13.01358168\n",
      "Iteration 5, loss = 14.62191807\n",
      "Iteration 21, loss = 13.10956391\n",
      "Iteration 6, loss = 10.45472447\n",
      "Iteration 22, loss = 11.09694273\n",
      "Iteration 7, loss = 10.05573439\n",
      "Iteration 23, loss = 10.76331376\n",
      "Iteration 8, loss = 11.08857671\n",
      "Iteration 24, loss = 10.95370796\n",
      "Iteration 9, loss = 13.27993121\n",
      "Iteration 25, loss = 17.28638077\n",
      "Iteration 26, loss = 15.87543278\n",
      "Iteration 10, loss = 11.57419122\n",
      "Iteration 27, loss = 13.24338986\n",
      "Iteration 11, loss = 11.78896820\n",
      "Iteration 28, loss = 12.65972560\n",
      "Iteration 12, loss = 11.54708319\n",
      "Iteration 29, loss = 11.96552106\n",
      "Iteration 13, loss = 12.28835515\n",
      "Iteration 30, loss = 12.12509300\n",
      "Iteration 14, loss = 10.41130646\n",
      "Iteration 31, loss = 13.03167070\n",
      "Iteration 15, loss = 10.07869979\n",
      "Iteration 32, loss = 12.00489770\n",
      "Iteration 16, loss = 10.70775060\n",
      "Iteration 33, loss = 11.87450223\n",
      "Iteration 17, loss = 12.05040322\n",
      "Iteration 34, loss = 12.34466993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 11.89791772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.83056842\n",
      "Iteration 1, loss = 16.98854764\n",
      "Iteration 2, loss = 15.04463353\n",
      "Iteration 2, loss = 15.48313346\n",
      "Iteration 3, loss = 11.83382955\n",
      "Iteration 3, loss = 15.67654824\n",
      "Iteration 4, loss = 11.35892766\n",
      "Iteration 4, loss = 13.93660885\n",
      "Iteration 5, loss = 9.14163199\n",
      "Iteration 5, loss = 16.23050191\n",
      "Iteration 6, loss = 9.46644135\n",
      "Iteration 6, loss = 13.29198685\n",
      "Iteration 7, loss = 9.66734910\n",
      "Iteration 7, loss = 16.77805744\n",
      "Iteration 8, loss = 12.69385316\n",
      "Iteration 8, loss = 15.12958627\n",
      "Iteration 9, loss = 14.17024080\n",
      "Iteration 9, loss = 15.07287253\n",
      "Iteration 10, loss = 14.35120103\n",
      "Iteration 10, loss = 11.10165520\n",
      "Iteration 11, loss = 13.96350420\n",
      "Iteration 11, loss = 10.69463228\n",
      "Iteration 12, loss = 12.96200165\n",
      "Iteration 12, loss = 13.34514007\n",
      "Iteration 13, loss = 12.08011817\n",
      "Iteration 13, loss = 11.14799657\n",
      "Iteration 14, loss = 12.82005876\n",
      "Iteration 14, loss = 11.37975428\n",
      "Iteration 15, loss = 13.01574256\n",
      "Iteration 15, loss = 10.36480606\n",
      "Iteration 16, loss = 14.77120006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 11.87198361\n",
      "Iteration 17, loss = 16.22080801\n",
      "Iteration 18, loss = 12.51175219\n",
      "Iteration 19, loss = 12.02886340\n",
      "Iteration 20, loss = 13.13574656\n",
      "Iteration 21, loss = 12.16178104\n",
      "Iteration 22, loss = 12.06619719\n",
      "Iteration 23, loss = 12.95916780\n",
      "Iteration 24, loss = 11.04109961\n",
      "Iteration 25, loss = 10.75587342\n",
      "Iteration 26, loss = 12.64475805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.25670553\n",
      "Iteration 2, loss = 18.49233383\n",
      "Iteration 3, loss = 10.14695838\n",
      "Iteration 4, loss = 10.01123897\n",
      "Iteration 5, loss = 12.26213272\n",
      "Iteration 6, loss = 10.05377701\n",
      "Iteration 7, loss = 11.07942864\n",
      "Iteration 8, loss = 10.58050045\n",
      "Iteration 9, loss = 11.70070660\n",
      "Iteration 10, loss = 13.16658444\n",
      "Iteration 11, loss = 11.36381736\n",
      "Iteration 1, loss = 15.11065560\n",
      "Iteration 12, loss = 12.37311525\n",
      "Iteration 2, loss = 15.73273453\n",
      "Iteration 13, loss = 12.19966390\n",
      "Iteration 3, loss = 11.85767311\n",
      "Iteration 14, loss = 12.03388649\n",
      "Iteration 4, loss = 10.88223768\n",
      "Iteration 15, loss = 12.44731902\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 9.36596474\n",
      "Iteration 6, loss = 13.78399495\n",
      "Iteration 7, loss = 13.11788709\n",
      "Iteration 8, loss = 11.82046332\n",
      "Iteration 9, loss = 10.60839648\n",
      "Iteration 10, loss = 13.41935051\n",
      "Iteration 11, loss = 13.78602669\n",
      "Iteration 12, loss = 12.42139379\n",
      "Iteration 13, loss = 12.05169613\n",
      "Iteration 14, loss = 11.99677881\n",
      "Iteration 15, loss = 11.93204982\n",
      "Iteration 16, loss = 12.99090370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.62591548\n",
      "Iteration 2, loss = 16.77546976\n",
      "Iteration 3, loss = 16.61092827\n",
      "Iteration 4, loss = 13.69377792\n",
      "Iteration 5, loss = 14.20074651\n",
      "Iteration 6, loss = 14.33581471\n",
      "Iteration 7, loss = 15.79326072\n",
      "Iteration 8, loss = 12.20200834\n",
      "Iteration 9, loss = 10.50147113\n",
      "Iteration 10, loss = 10.59979626\n",
      "Iteration 11, loss = 11.32563455\n",
      "Iteration 12, loss = 11.58842539\n",
      "Iteration 13, loss = 12.85875919\n",
      "Iteration 1, loss = 20.84540496\n",
      "Iteration 14, loss = 12.00637640\n",
      "Iteration 2, loss = 18.92580673\n",
      "Iteration 15, loss = 11.14600828\n",
      "Iteration 3, loss = 15.03166271\n",
      "Iteration 16, loss = 10.31958710\n",
      "Iteration 4, loss = 12.84017127\n",
      "Iteration 17, loss = 11.38625449\n",
      "Iteration 5, loss = 12.87191816\n",
      "Iteration 18, loss = 11.30132428\n",
      "Iteration 6, loss = 11.17948967\n",
      "Iteration 19, loss = 10.89471342\n",
      "Iteration 7, loss = 12.12765010\n",
      "Iteration 20, loss = 10.80682679\n",
      "Iteration 8, loss = 10.50360210\n",
      "Iteration 21, loss = 10.93059176\n",
      "Iteration 9, loss = 13.18717433\n",
      "Iteration 22, loss = 10.74815025\n",
      "Iteration 10, loss = 10.72462061\n",
      "Iteration 23, loss = 10.68177007\n",
      "Iteration 11, loss = 11.45952799\n",
      "Iteration 24, loss = 11.35823083\n",
      "Iteration 12, loss = 11.67292227\n",
      "Iteration 25, loss = 10.22774249\n",
      "Iteration 13, loss = 11.69041113\n",
      "Iteration 26, loss = 9.76442526\n",
      "Iteration 14, loss = 13.46853884\n",
      "Iteration 27, loss = 10.20226943\n",
      "Iteration 15, loss = 10.90671512\n",
      "Iteration 28, loss = 9.65946264\n",
      "Iteration 16, loss = 11.23921800\n",
      "Iteration 29, loss = 10.09647120\n",
      "Iteration 17, loss = 11.88222792\n",
      "Iteration 30, loss = 9.78763807\n",
      "Iteration 18, loss = 11.02566756\n",
      "Iteration 19, loss = 11.29410463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 9.75455224\n",
      "Iteration 32, loss = 10.46599366\n",
      "Iteration 33, loss = 10.50796629\n",
      "Iteration 34, loss = 11.09938520\n",
      "Iteration 35, loss = 9.69342727\n",
      "Iteration 36, loss = 9.93287331\n",
      "Iteration 37, loss = 9.73960771\n",
      "Iteration 38, loss = 9.35106465\n",
      "Iteration 39, loss = 9.90166294\n",
      "Iteration 40, loss = 9.90466007\n",
      "Iteration 41, loss = 9.12698200\n",
      "Iteration 42, loss = 9.56359581\n",
      "Iteration 43, loss = 9.64433141\n",
      "Iteration 44, loss = 10.19358198\n",
      "Iteration 1, loss = 17.07042769\n",
      "Iteration 45, loss = 10.03896725\n",
      "Iteration 2, loss = 20.25222994\n",
      "Iteration 46, loss = 10.23810334\n",
      "Iteration 3, loss = 18.69925130\n",
      "Iteration 47, loss = 9.69247243\n",
      "Iteration 4, loss = 10.69181166\n",
      "Iteration 48, loss = 9.30405149\n",
      "Iteration 5, loss = 12.92434783\n",
      "Iteration 49, loss = 9.50285210\n",
      "Iteration 6, loss = 11.10895888\n",
      "Iteration 50, loss = 9.14888826\n",
      "Iteration 7, loss = 14.82789205\n",
      "Iteration 51, loss = 9.30586616\n",
      "Iteration 8, loss = 12.87755769\n",
      "Iteration 52, loss = 9.81327712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 16.26887442\n",
      "Iteration 10, loss = 11.83404012\n",
      "Iteration 11, loss = 11.32288696\n",
      "Iteration 12, loss = 12.51818635\n",
      "Iteration 13, loss = 10.57560050\n",
      "Iteration 14, loss = 12.08007057\n",
      "Iteration 15, loss = 12.16546770\n",
      "Iteration 16, loss = 12.26662063\n",
      "Iteration 17, loss = 11.13395240\n",
      "Iteration 18, loss = 10.34622377\n",
      "Iteration 19, loss = 11.55096956\n",
      "Iteration 20, loss = 11.96589057\n",
      "Iteration 21, loss = 10.72904602\n",
      "Iteration 1, loss = 21.21647499\n",
      "Iteration 22, loss = 11.01227174\n",
      "Iteration 2, loss = 22.35291396\n",
      "Iteration 23, loss = 12.62384779\n",
      "Iteration 3, loss = 14.33866033\n",
      "Iteration 24, loss = 10.51315737\n",
      "Iteration 4, loss = 10.94577200\n",
      "Iteration 25, loss = 10.76533497\n",
      "Iteration 5, loss = 12.86132663\n",
      "Iteration 26, loss = 11.05684198\n",
      "Iteration 6, loss = 13.24017055\n",
      "Iteration 27, loss = 10.87988328\n",
      "Iteration 7, loss = 14.41115087\n",
      "Iteration 28, loss = 10.98015919\n",
      "Iteration 8, loss = 11.15431339\n",
      "Iteration 29, loss = 11.34602245\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 11.88114350\n",
      "Iteration 10, loss = 13.64267554\n",
      "Iteration 11, loss = 14.23990707\n",
      "Iteration 12, loss = 13.35523556\n",
      "Iteration 13, loss = 12.74457545\n",
      "Iteration 14, loss = 13.76521583\n",
      "Iteration 15, loss = 11.85786443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.74610104\n",
      "Iteration 2, loss = 12.65945567\n",
      "Iteration 3, loss = 11.86749690\n",
      "Iteration 4, loss = 11.16376201\n",
      "Iteration 5, loss = 10.10211387\n",
      "Iteration 6, loss = 13.83077100\n",
      "Iteration 7, loss = 13.95021908\n",
      "Iteration 1, loss = 18.00678659\n",
      "Iteration 8, loss = 16.24697354\n",
      "Iteration 2, loss = 14.82873840\n",
      "Iteration 9, loss = 13.92868721\n",
      "Iteration 10, loss = 13.13918147\n",
      "Iteration 3, loss = 13.12805050\n",
      "Iteration 11, loss = 12.53789967\n",
      "Iteration 4, loss = 14.03140126\n",
      "Iteration 12, loss = 10.43800385\n",
      "Iteration 5, loss = 11.66909833\n",
      "Iteration 13, loss = 10.44902244\n",
      "Iteration 6, loss = 10.44692910\n",
      "Iteration 14, loss = 10.80421654\n",
      "Iteration 7, loss = 11.51607394\n",
      "Iteration 15, loss = 11.35793643\n",
      "Iteration 8, loss = 11.48517222\n",
      "Iteration 16, loss = 11.29704683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 11.73316456\n",
      "Iteration 10, loss = 12.94076340\n",
      "Iteration 11, loss = 11.96355056\n",
      "Iteration 12, loss = 11.68454964\n",
      "Iteration 13, loss = 11.67564811\n",
      "Iteration 14, loss = 11.83206549\n",
      "Iteration 15, loss = 12.39645745\n",
      "Iteration 16, loss = 11.52248649\n",
      "Iteration 17, loss = 11.77147524\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.41785985\n",
      "Iteration 2, loss = 22.13180897\n",
      "Iteration 3, loss = 18.89358417\n",
      "Iteration 4, loss = 15.12506538\n",
      "Iteration 5, loss = 13.46458635\n",
      "Iteration 6, loss = 11.50576959\n",
      "Iteration 7, loss = 10.90240881\n",
      "Iteration 1, loss = 1.21445698\n",
      "Iteration 8, loss = 10.93104918\n",
      "Iteration 2, loss = 0.47900298\n",
      "Iteration 9, loss = 10.74849738\n",
      "Iteration 10, loss = 9.16151174\n",
      "Iteration 11, loss = 9.68228272\n",
      "Iteration 12, loss = 10.61450008\n",
      "Iteration 13, loss = 11.46311746\n",
      "Iteration 14, loss = 9.35937576\n",
      "Iteration 15, loss = 8.76631714\n",
      "Iteration 16, loss = 9.59668753\n",
      "Iteration 17, loss = 8.53511099\n",
      "Iteration 18, loss = 8.16774112\n",
      "Iteration 19, loss = 8.57057056\n",
      "Iteration 20, loss = 8.46127065\n",
      "Iteration 21, loss = 9.29596549\n",
      "Iteration 3, loss = 0.41935384\n",
      "Iteration 22, loss = 8.63880123\n",
      "Iteration 23, loss = 7.73991947\n",
      "Iteration 4, loss = 0.36359511\n",
      "Iteration 24, loss = 8.64004545\n",
      "Iteration 5, loss = 0.33474317\n",
      "Iteration 25, loss = 7.61451662\n",
      "Iteration 6, loss = 0.29790037\n",
      "Iteration 26, loss = 7.96439205\n",
      "Iteration 27, loss = 7.80657955\n",
      "Iteration 7, loss = 0.26613003\n",
      "Iteration 28, loss = 8.16203808\n",
      "Iteration 8, loss = 0.26003183\n",
      "Iteration 29, loss = 8.20460500\n",
      "Iteration 30, loss = 8.17067472\n",
      "Iteration 31, loss = 7.11743915\n",
      "Iteration 32, loss = 7.43329237\n",
      "Iteration 33, loss = 8.25864797\n",
      "Iteration 34, loss = 8.13745474\n",
      "Iteration 9, loss = 0.25056456\n",
      "Iteration 35, loss = 7.97982751\n",
      "Iteration 10, loss = 0.22365831\n",
      "Iteration 36, loss = 8.79185674\n",
      "Iteration 37, loss = 7.48891182\n",
      "Iteration 11, loss = 0.21530215\n",
      "Iteration 38, loss = 8.54004876\n",
      "Iteration 12, loss = 0.19114339\n",
      "Iteration 39, loss = 9.16442083\n",
      "Iteration 13, loss = 0.18565314\n",
      "Iteration 14, loss = 0.18567746\n",
      "Iteration 40, loss = 8.47929280\n",
      "Iteration 15, loss = 0.17130032\n",
      "Iteration 16, loss = 0.16953612\n",
      "Iteration 41, loss = 9.17522127\n",
      "Iteration 17, loss = 0.18077867\n",
      "Iteration 42, loss = 8.31136514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.18189712\n",
      "Iteration 19, loss = 0.16280419\n",
      "Iteration 20, loss = 0.16270801\n",
      "Iteration 21, loss = 0.18056238\n",
      "Iteration 22, loss = 0.17035315\n",
      "Iteration 23, loss = 0.15538093\n",
      "Iteration 24, loss = 0.16647092\n",
      "Iteration 25, loss = 0.13932232\n",
      "Iteration 26, loss = 0.15635656\n",
      "Iteration 27, loss = 0.12556895\n",
      "Iteration 28, loss = 0.11531128\n",
      "Iteration 1, loss = 18.06291572\n",
      "Iteration 2, loss = 16.47280398\n",
      "Iteration 29, loss = 0.11208334\n",
      "Iteration 3, loss = 12.10291716\n",
      "Iteration 4, loss = 11.26291597\n",
      "Iteration 5, loss = 9.49435657\n",
      "Iteration 6, loss = 11.19128107\n",
      "Iteration 30, loss = 0.11059111\n",
      "Iteration 7, loss = 9.90074444\n",
      "Iteration 31, loss = 0.09942234\n",
      "Iteration 8, loss = 9.62052650\n",
      "Iteration 9, loss = 14.00630128\n",
      "Iteration 32, loss = 0.10772051\n",
      "Iteration 10, loss = 11.36413544\n",
      "Iteration 33, loss = 0.12018978\n",
      "Iteration 11, loss = 14.77680554\n",
      "Iteration 34, loss = 0.11169315\n",
      "Iteration 12, loss = 10.72484814\n",
      "Iteration 35, loss = 0.09762655\n",
      "Iteration 13, loss = 10.92781904\n",
      "Iteration 14, loss = 10.06060303\n",
      "Iteration 36, loss = 0.10600515\n",
      "Iteration 15, loss = 10.23393553\n",
      "Iteration 37, loss = 0.10860151\n",
      "Iteration 16, loss = 10.36768199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.09623412\n",
      "Iteration 39, loss = 0.11286119\n",
      "Iteration 40, loss = 0.14106234\n",
      "Iteration 41, loss = 0.13165697\n",
      "Iteration 42, loss = 0.12225715\n",
      "Iteration 43, loss = 0.15343799\n",
      "Iteration 44, loss = 0.12667860\n",
      "Iteration 45, loss = 0.11125630\n",
      "Iteration 46, loss = 0.10605839\n",
      "Iteration 47, loss = 0.11238915\n",
      "Iteration 48, loss = 0.10055683\n",
      "Iteration 1, loss = 0.84016996\n",
      "Iteration 49, loss = 0.13207923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.45955436\n",
      "Iteration 3, loss = 0.42352992\n",
      "Iteration 4, loss = 0.37454732\n",
      "Iteration 5, loss = 0.31633614\n",
      "Iteration 6, loss = 0.29020600\n",
      "Iteration 7, loss = 0.27345123\n",
      "Iteration 8, loss = 0.23800334\n",
      "Iteration 9, loss = 0.22994876\n",
      "Iteration 10, loss = 0.21869688\n",
      "Iteration 11, loss = 0.22683796\n",
      "Iteration 12, loss = 0.21380964\n",
      "Iteration 13, loss = 0.21639706\n",
      "Iteration 14, loss = 0.20812529\n",
      "Iteration 15, loss = 0.17722655\n",
      "Iteration 16, loss = 0.16006565\n",
      "Iteration 17, loss = 0.14621223\n",
      "Iteration 1, loss = 1.08740781\n",
      "Iteration 18, loss = 0.17339742\n",
      "Iteration 19, loss = 0.13692383\n",
      "Iteration 20, loss = 0.13557180\n",
      "Iteration 2, loss = 0.47507609\n",
      "Iteration 21, loss = 0.12718261\n",
      "Iteration 3, loss = 0.41078870\n",
      "Iteration 22, loss = 0.13694586\n",
      "Iteration 4, loss = 0.36882997\n",
      "Iteration 23, loss = 0.13288211\n",
      "Iteration 24, loss = 0.12877253\n",
      "Iteration 5, loss = 0.32871114\n",
      "Iteration 25, loss = 0.10926832\n",
      "Iteration 26, loss = 0.11250755\n",
      "Iteration 6, loss = 0.28501352\n",
      "Iteration 27, loss = 0.11245378\n",
      "Iteration 28, loss = 0.12950716\n",
      "Iteration 29, loss = 0.13941506\n",
      "Iteration 30, loss = 0.18043499\n",
      "Iteration 7, loss = 0.28042657\n",
      "Iteration 31, loss = 0.14249975\n",
      "Iteration 8, loss = 0.24912393\n",
      "Iteration 32, loss = 0.14853791\n",
      "Iteration 9, loss = 0.22484988\n",
      "Iteration 33, loss = 0.11859251\n",
      "Iteration 10, loss = 0.21022423\n",
      "Iteration 34, loss = 0.13264516\n",
      "Iteration 35, loss = 0.11021186\n",
      "Iteration 11, loss = 0.19762275\n",
      "Iteration 36, loss = 0.09747505\n",
      "Iteration 12, loss = 0.18333395\n",
      "Iteration 37, loss = 0.10893873\n",
      "Iteration 13, loss = 0.17751505\n",
      "Iteration 14, loss = 0.16980019\n",
      "Iteration 38, loss = 0.17051881\n",
      "Iteration 15, loss = 0.15497471\n",
      "Iteration 39, loss = 0.15182060\n",
      "Iteration 16, loss = 0.15505260\n",
      "Iteration 40, loss = 0.10991005\n",
      "Iteration 17, loss = 0.14444373\n",
      "Iteration 41, loss = 0.12387634\n",
      "Iteration 18, loss = 0.17645538\n",
      "Iteration 42, loss = 0.09145659\n",
      "Iteration 19, loss = 0.18287265\n",
      "Iteration 20, loss = 0.20495335\n",
      "Iteration 43, loss = 0.08423587\n",
      "Iteration 21, loss = 0.19433480\n",
      "Iteration 44, loss = 0.07033482\n",
      "Iteration 22, loss = 0.20535375\n",
      "Iteration 45, loss = 0.06880429\n",
      "Iteration 23, loss = 0.16452242\n",
      "Iteration 46, loss = 0.06815864\n",
      "Iteration 24, loss = 0.18521883\n",
      "Iteration 47, loss = 0.06335388\n",
      "Iteration 25, loss = 0.14243485\n",
      "Iteration 48, loss = 0.05882912\n",
      "Iteration 49, loss = 0.06570964\n",
      "Iteration 26, loss = 0.12971325\n",
      "Iteration 50, loss = 0.07792224\n",
      "Iteration 51, loss = 0.07375610\n",
      "Iteration 52, loss = 0.06886597\n",
      "Iteration 27, loss = 0.10939848\n",
      "Iteration 28, loss = 0.11003175\n",
      "Iteration 29, loss = 0.10361291\n",
      "Iteration 30, loss = 0.10287289\n",
      "Iteration 53, loss = 0.05496183\n",
      "Iteration 54, loss = 0.08255791\n",
      "Iteration 31, loss = 0.09768908\n",
      "Iteration 55, loss = 0.05870211\n",
      "Iteration 56, loss = 0.06199964\n",
      "Iteration 32, loss = 0.10652747\n",
      "Iteration 57, loss = 0.06289517\n",
      "Iteration 33, loss = 0.09072917\n",
      "Iteration 34, loss = 0.07889047\n",
      "Iteration 58, loss = 0.05530447\n",
      "Iteration 35, loss = 0.07870560\n",
      "Iteration 59, loss = 0.04436842\n",
      "Iteration 60, loss = 0.05190840\n",
      "Iteration 36, loss = 0.09167903\n",
      "Iteration 61, loss = 0.06078781\n",
      "Iteration 37, loss = 0.09328321\n",
      "Iteration 38, loss = 0.09650632\n",
      "Iteration 39, loss = 0.10786830\n",
      "Iteration 62, loss = 0.06359612\n",
      "Iteration 63, loss = 0.07210424\n",
      "Iteration 40, loss = 0.11706050\n",
      "Iteration 64, loss = 0.07136144\n",
      "Iteration 65, loss = 0.10639393\n",
      "Iteration 41, loss = 0.10892637\n",
      "Iteration 42, loss = 0.14043195\n",
      "Iteration 66, loss = 0.11213890\n",
      "Iteration 43, loss = 0.14884428\n",
      "Iteration 44, loss = 0.15150654\n",
      "Iteration 67, loss = 0.10079423\n",
      "Iteration 68, loss = 0.13144344\n",
      "Iteration 69, loss = 0.23170761\n",
      "Iteration 45, loss = 0.13621508\n",
      "Iteration 70, loss = 0.18885697\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.09330493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97919862\n",
      "Iteration 1, loss = 1.08505867\n",
      "Iteration 2, loss = 0.51652627\n",
      "Iteration 2, loss = 0.51707071\n",
      "Iteration 3, loss = 0.44361540\n",
      "Iteration 4, loss = 0.36676148\n",
      "Iteration 3, loss = 0.41370916\n",
      "Iteration 5, loss = 0.32901343\n",
      "Iteration 4, loss = 0.36485443\n",
      "Iteration 6, loss = 0.30467011\n",
      "Iteration 5, loss = 0.32885395\n",
      "Iteration 7, loss = 0.27995885\n",
      "Iteration 8, loss = 0.26436490\n",
      "Iteration 9, loss = 0.25432869\n",
      "Iteration 6, loss = 0.31577878\n",
      "Iteration 10, loss = 0.23697199\n",
      "Iteration 7, loss = 0.28828546\n",
      "Iteration 8, loss = 0.25720876\n",
      "Iteration 9, loss = 0.27522275\n",
      "Iteration 11, loss = 0.22051043\n",
      "Iteration 10, loss = 0.24272069\n",
      "Iteration 11, loss = 0.23719996\n",
      "Iteration 12, loss = 0.21619964\n",
      "Iteration 12, loss = 0.21473212\n",
      "Iteration 13, loss = 0.20619095\n",
      "Iteration 13, loss = 0.20996352\n",
      "Iteration 14, loss = 0.19009158\n",
      "Iteration 15, loss = 0.18375036\n",
      "Iteration 14, loss = 0.18923128\n",
      "Iteration 15, loss = 0.18171399\n",
      "Iteration 16, loss = 0.17864629\n",
      "Iteration 16, loss = 0.17067133\n",
      "Iteration 17, loss = 0.16367749\n",
      "Iteration 18, loss = 0.16230698\n",
      "Iteration 17, loss = 0.17015325\n",
      "Iteration 19, loss = 0.14325181\n",
      "Iteration 18, loss = 0.17108588\n",
      "Iteration 20, loss = 0.14179759\n",
      "Iteration 19, loss = 0.19755076\n",
      "Iteration 21, loss = 0.14555870\n",
      "Iteration 20, loss = 0.19526687\n",
      "Iteration 22, loss = 0.13821654\n",
      "Iteration 21, loss = 0.17992352\n",
      "Iteration 23, loss = 0.15824911\n",
      "Iteration 22, loss = 0.16161340\n",
      "Iteration 23, loss = 0.16210582\n",
      "Iteration 24, loss = 0.12726984\n",
      "Iteration 24, loss = 0.15996537\n",
      "Iteration 25, loss = 0.18261618\n",
      "Iteration 25, loss = 0.12914616\n",
      "Iteration 26, loss = 0.15830326\n",
      "Iteration 26, loss = 0.14772299\n",
      "Iteration 27, loss = 0.13795985\n",
      "Iteration 28, loss = 0.13270878\n",
      "Iteration 27, loss = 0.15542766\n",
      "Iteration 28, loss = 0.17150576\n",
      "Iteration 29, loss = 0.22731260\n",
      "Iteration 29, loss = 0.13837766\n",
      "Iteration 30, loss = 0.16494702\n",
      "Iteration 30, loss = 0.15677952\n",
      "Iteration 31, loss = 0.15770948\n",
      "Iteration 31, loss = 0.15319717\n",
      "Iteration 32, loss = 0.15056573\n",
      "Iteration 32, loss = 0.13206731\n",
      "Iteration 33, loss = 0.13870940\n",
      "Iteration 33, loss = 0.12442193\n",
      "Iteration 34, loss = 0.11326886\n",
      "Iteration 34, loss = 0.13708781\n",
      "Iteration 35, loss = 0.11464304\n",
      "Iteration 35, loss = 0.14551301\n",
      "Iteration 36, loss = 0.10408090\n",
      "Iteration 36, loss = 0.12359705\n",
      "Iteration 37, loss = 0.14889150\n",
      "Iteration 37, loss = 0.09177295\n",
      "Iteration 38, loss = 0.12193439Iteration 38, loss = 0.09452739\n",
      "\n",
      "Iteration 39, loss = 0.09753453\n",
      "Iteration 39, loss = 0.13704275\n",
      "Iteration 40, loss = 0.11013110\n",
      "Iteration 40, loss = 0.11117592\n",
      "Iteration 41, loss = 0.10734469\n",
      "Iteration 41, loss = 0.11140418\n",
      "Iteration 42, loss = 0.11654436\n",
      "Iteration 42, loss = 0.10499817\n",
      "Iteration 43, loss = 0.10328116\n",
      "Iteration 44, loss = 0.10825913\n",
      "Iteration 45, loss = 0.12498213\n",
      "Iteration 43, loss = 0.12124191\n",
      "Iteration 46, loss = 0.11098040\n",
      "Iteration 44, loss = 0.09043700\n",
      "Iteration 45, loss = 0.11357622\n",
      "Iteration 46, loss = 0.08884327\n",
      "Iteration 47, loss = 0.10206071\n",
      "Iteration 48, loss = 0.09450278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.10254003\n",
      "Iteration 48, loss = 0.08145696\n",
      "Iteration 49, loss = 0.07581375\n",
      "Iteration 1, loss = 1.19179383\n",
      "Iteration 50, loss = 0.09285781\n",
      "Iteration 2, loss = 0.53674013\n",
      "Iteration 51, loss = 0.10862268\n",
      "Iteration 52, loss = 0.14587201\n",
      "Iteration 3, loss = 0.43338235\n",
      "Iteration 53, loss = 0.10365295\n",
      "Iteration 54, loss = 0.08976570\n",
      "Iteration 4, loss = 0.37386788\n",
      "Iteration 55, loss = 0.07031404\n",
      "Iteration 56, loss = 0.06720358\n",
      "Iteration 57, loss = 0.08841812\n",
      "Iteration 5, loss = 0.33205554\n",
      "Iteration 58, loss = 0.06918409\n",
      "Iteration 59, loss = 0.06102914\n",
      "Iteration 60, loss = 0.06405087\n",
      "Iteration 6, loss = 0.29013922\n",
      "Iteration 61, loss = 0.05826132\n",
      "Iteration 7, loss = 0.27391305\n",
      "Iteration 8, loss = 0.25587659\n",
      "Iteration 62, loss = 0.06131726\n",
      "Iteration 9, loss = 0.23516099\n",
      "Iteration 63, loss = 0.06637029\n",
      "Iteration 64, loss = 0.06256955\n",
      "Iteration 65, loss = 0.06530723\n",
      "Iteration 66, loss = 0.04557458\n",
      "Iteration 10, loss = 0.22477880\n",
      "Iteration 67, loss = 0.05457261\n",
      "Iteration 68, loss = 0.04455234\n",
      "Iteration 69, loss = 0.04973861\n",
      "Iteration 70, loss = 0.05128141\n",
      "Iteration 71, loss = 0.06156350\n",
      "Iteration 72, loss = 0.08319425\n",
      "Iteration 73, loss = 0.06562078\n",
      "Iteration 11, loss = 0.21606935\n",
      "Iteration 74, loss = 0.07228340\n",
      "Iteration 75, loss = 0.05908307\n",
      "Iteration 12, loss = 0.20637431\n",
      "Iteration 76, loss = 0.05797511\n",
      "Iteration 77, loss = 0.06952779\n",
      "Iteration 78, loss = 0.07610747\n",
      "Iteration 13, loss = 0.19205558\n",
      "Iteration 79, loss = 0.10927946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.19048082\n",
      "Iteration 15, loss = 0.21276810\n",
      "Iteration 16, loss = 0.21590783\n",
      "Iteration 1, loss = 0.90777647\n",
      "Iteration 2, loss = 0.50396141\n",
      "Iteration 3, loss = 0.39061938\n",
      "Iteration 4, loss = 0.33484646\n",
      "Iteration 5, loss = 0.29923188\n",
      "Iteration 6, loss = 0.27104349\n",
      "Iteration 17, loss = 0.21384111\n",
      "Iteration 7, loss = 0.26339262\n",
      "Iteration 18, loss = 0.22409205\n",
      "Iteration 19, loss = 0.19898607\n",
      "Iteration 20, loss = 0.17919120\n",
      "Iteration 8, loss = 0.24936923\n",
      "Iteration 9, loss = 0.21808209\n",
      "Iteration 10, loss = 0.20081386\n",
      "Iteration 11, loss = 0.19685900\n",
      "Iteration 21, loss = 0.16457282\n",
      "Iteration 22, loss = 0.16994568\n",
      "Iteration 12, loss = 0.17979719\n",
      "Iteration 23, loss = 0.13261836\n",
      "Iteration 13, loss = 0.17659648\n",
      "Iteration 14, loss = 0.16648010\n",
      "Iteration 24, loss = 0.14215354\n",
      "Iteration 15, loss = 0.19222790\n",
      "Iteration 25, loss = 0.13323243\n",
      "Iteration 26, loss = 0.19464479\n",
      "Iteration 16, loss = 0.13976842\n",
      "Iteration 27, loss = 0.14924081\n",
      "Iteration 17, loss = 0.17735152\n",
      "Iteration 28, loss = 0.13976897\n",
      "Iteration 18, loss = 0.15696743\n",
      "Iteration 29, loss = 0.17284661\n",
      "Iteration 19, loss = 0.18871576\n",
      "Iteration 20, loss = 0.19517422\n",
      "Iteration 30, loss = 0.16626293\n",
      "Iteration 31, loss = 0.13857780\n",
      "Iteration 21, loss = 0.20381387\n",
      "Iteration 32, loss = 0.14000690\n",
      "Iteration 22, loss = 0.19333725\n",
      "Iteration 23, loss = 0.17318587\n",
      "Iteration 33, loss = 0.12423881\n",
      "Iteration 24, loss = 0.19738547\n",
      "Iteration 34, loss = 0.14225864\n",
      "Iteration 35, loss = 0.14508849\n",
      "Iteration 36, loss = 0.14441804\n",
      "Iteration 25, loss = 0.16914281\n",
      "Iteration 26, loss = 0.17774458\n",
      "Iteration 37, loss = 0.13939864\n",
      "Iteration 27, loss = 0.14085328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.11737239\n",
      "Iteration 39, loss = 0.11410294\n",
      "Iteration 40, loss = 0.11611531\n",
      "Iteration 41, loss = 0.09870773\n",
      "Iteration 42, loss = 0.08779527\n",
      "Iteration 43, loss = 0.09179604\n",
      "Iteration 44, loss = 0.10338390\n",
      "Iteration 45, loss = 0.10400778\n",
      "Iteration 46, loss = 0.11388018\n",
      "Iteration 47, loss = 0.09530060\n",
      "Iteration 48, loss = 0.11014857\n",
      "Iteration 1, loss = 1.12157399\n",
      "Iteration 2, loss = 0.49502660\n",
      "Iteration 3, loss = 0.40649991\n",
      "Iteration 4, loss = 0.34310250\n",
      "Iteration 49, loss = 0.11431420\n",
      "Iteration 5, loss = 0.29618117\n",
      "Iteration 6, loss = 0.26486726\n",
      "Iteration 50, loss = 0.15428457\n",
      "Iteration 7, loss = 0.26613028\n",
      "Iteration 51, loss = 0.11954384\n",
      "Iteration 8, loss = 0.22545652\n",
      "Iteration 9, loss = 0.21721434\n",
      "Iteration 10, loss = 0.20402529\n",
      "Iteration 11, loss = 0.19214924\n",
      "Iteration 52, loss = 0.12018186\n",
      "Iteration 12, loss = 0.20301926\n",
      "Iteration 53, loss = 0.10747127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.21339904\n",
      "Iteration 14, loss = 0.18480665\n",
      "Iteration 15, loss = 0.15727447\n",
      "Iteration 16, loss = 0.15129588\n",
      "Iteration 17, loss = 0.14049156\n",
      "Iteration 1, loss = 0.93229623\n",
      "Iteration 18, loss = 0.15593775\n",
      "Iteration 19, loss = 0.14971397\n",
      "Iteration 20, loss = 0.12980334\n",
      "Iteration 2, loss = 0.50417949\n",
      "Iteration 3, loss = 0.39807308\n",
      "Iteration 21, loss = 0.12971284\n",
      "Iteration 22, loss = 0.12588675\n",
      "Iteration 23, loss = 0.13978399\n",
      "Iteration 24, loss = 0.13122741\n",
      "Iteration 25, loss = 0.16097138\n",
      "Iteration 4, loss = 0.34543992\n",
      "Iteration 5, loss = 0.31694413\n",
      "Iteration 6, loss = 0.26917847\n",
      "Iteration 7, loss = 0.23885159\n",
      "Iteration 26, loss = 0.16189134\n",
      "Iteration 27, loss = 0.11842404\n",
      "Iteration 8, loss = 0.22844040\n",
      "Iteration 28, loss = 0.11288753\n",
      "Iteration 29, loss = 0.11038426\n",
      "Iteration 30, loss = 0.11606615\n",
      "Iteration 9, loss = 0.21151863\n",
      "Iteration 10, loss = 0.20670876\n",
      "Iteration 11, loss = 0.18634635\n",
      "Iteration 12, loss = 0.17950747\n",
      "Iteration 13, loss = 0.18763743\n",
      "Iteration 31, loss = 0.10103255\n",
      "Iteration 32, loss = 0.11530526\n",
      "Iteration 14, loss = 0.17019188\n",
      "Iteration 33, loss = 0.14147526\n",
      "Iteration 15, loss = 0.15623276\n",
      "Iteration 16, loss = 0.15005983\n",
      "Iteration 17, loss = 0.13185383\n",
      "Iteration 34, loss = 0.11804677\n",
      "Iteration 18, loss = 0.14745113\n",
      "Iteration 35, loss = 0.11806097\n",
      "Iteration 19, loss = 0.12096155\n",
      "Iteration 36, loss = 0.12348933\n",
      "Iteration 20, loss = 0.10820565\n",
      "Iteration 21, loss = 0.09752437\n",
      "Iteration 37, loss = 0.13572488\n",
      "Iteration 38, loss = 0.11838499\n",
      "Iteration 22, loss = 0.09110515\n",
      "Iteration 39, loss = 0.12826700\n",
      "Iteration 23, loss = 0.10074625\n",
      "Iteration 40, loss = 0.09740273\n",
      "Iteration 24, loss = 0.08664336\n",
      "Iteration 41, loss = 0.08532427\n",
      "Iteration 25, loss = 0.07649010\n",
      "Iteration 42, loss = 0.11558885\n",
      "Iteration 26, loss = 0.07538224\n",
      "Iteration 43, loss = 0.09437279\n",
      "Iteration 27, loss = 0.08306279\n",
      "Iteration 44, loss = 0.08907669\n",
      "Iteration 28, loss = 0.06291029\n",
      "Iteration 45, loss = 0.07803219\n",
      "Iteration 46, loss = 0.11484910\n",
      "Iteration 29, loss = 0.06309640\n",
      "Iteration 30, loss = 0.08976092\n",
      "Iteration 31, loss = 0.09105168\n",
      "Iteration 47, loss = 0.14451940\n",
      "Iteration 32, loss = 0.12917034\n",
      "Iteration 48, loss = 0.09414701\n",
      "Iteration 33, loss = 0.15934397\n",
      "Iteration 49, loss = 0.08228496\n",
      "Iteration 50, loss = 0.07340451\n",
      "Iteration 34, loss = 0.16192053\n",
      "Iteration 51, loss = 0.07353246\n",
      "Iteration 35, loss = 0.13514617\n",
      "Iteration 36, loss = 0.15591185\n",
      "Iteration 37, loss = 0.13449400\n",
      "Iteration 38, loss = 0.11404176\n",
      "Iteration 52, loss = 0.07513089\n",
      "Iteration 39, loss = 0.10522161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.06091344\n",
      "Iteration 54, loss = 0.06261429\n",
      "Iteration 55, loss = 0.06268114\n",
      "Iteration 56, loss = 0.07077001\n",
      "Iteration 57, loss = 0.06043068\n",
      "Iteration 58, loss = 0.05937254\n",
      "Iteration 59, loss = 0.06024466\n",
      "Iteration 60, loss = 0.05937965\n",
      "Iteration 1, loss = 1.15019905\n",
      "Iteration 61, loss = 0.06990816\n",
      "Iteration 2, loss = 0.55624198\n",
      "Iteration 62, loss = 0.08178983\n",
      "Iteration 3, loss = 0.42593954\n",
      "Iteration 63, loss = 0.08664282\n",
      "Iteration 4, loss = 0.37056037\n",
      "Iteration 64, loss = 0.09947607\n",
      "Iteration 5, loss = 0.33590439\n",
      "Iteration 65, loss = 0.11301793\n",
      "Iteration 66, loss = 0.10083534\n",
      "Iteration 6, loss = 0.30888390\n",
      "Iteration 7, loss = 0.28138613\n",
      "Iteration 67, loss = 0.09550137\n",
      "Iteration 8, loss = 0.25554036\n",
      "Iteration 9, loss = 0.24353134\n",
      "Iteration 68, loss = 0.13240625\n",
      "Iteration 10, loss = 0.23398013\n",
      "Iteration 11, loss = 0.22056884\n",
      "Iteration 12, loss = 0.20974779\n",
      "Iteration 69, loss = 0.12021898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.21849127\n",
      "Iteration 14, loss = 0.19934235\n",
      "Iteration 15, loss = 0.18667448\n",
      "Iteration 16, loss = 0.16299916\n",
      "Iteration 17, loss = 0.15448774\n",
      "Iteration 18, loss = 0.15080052\n",
      "Iteration 19, loss = 0.14726978\n",
      "Iteration 20, loss = 0.17106310\n",
      "Iteration 1, loss = 1.08516421\n",
      "Iteration 21, loss = 0.16711850\n",
      "Iteration 2, loss = 0.50245636\n",
      "Iteration 22, loss = 0.19491815\n",
      "Iteration 3, loss = 0.40504662\n",
      "Iteration 4, loss = 0.36304900\n",
      "Iteration 23, loss = 0.16532693\n",
      "Iteration 5, loss = 0.33269280\n",
      "Iteration 24, loss = 0.15899039\n",
      "Iteration 25, loss = 0.17355112\n",
      "Iteration 6, loss = 0.28072213\n",
      "Iteration 26, loss = 0.14280343\n",
      "Iteration 7, loss = 0.27173278\n",
      "Iteration 27, loss = 0.13827776\n",
      "Iteration 28, loss = 0.12104922\n",
      "Iteration 8, loss = 0.24158547\n",
      "Iteration 29, loss = 0.11561575\n",
      "Iteration 30, loss = 0.11262472\n",
      "Iteration 9, loss = 0.23453299\n",
      "Iteration 31, loss = 0.10564377\n",
      "Iteration 10, loss = 0.22181276\n",
      "Iteration 11, loss = 0.22820548\n",
      "Iteration 12, loss = 0.21240334\n",
      "Iteration 32, loss = 0.09809061\n",
      "Iteration 13, loss = 0.19468282\n",
      "Iteration 14, loss = 0.18133417\n",
      "Iteration 15, loss = 0.17839411\n",
      "Iteration 33, loss = 0.09616228\n",
      "Iteration 34, loss = 0.08767867\n",
      "Iteration 16, loss = 0.16071412\n",
      "Iteration 35, loss = 0.10201900\n",
      "Iteration 36, loss = 0.11759637\n",
      "Iteration 17, loss = 0.17095676\n",
      "Iteration 18, loss = 0.15359713\n",
      "Iteration 19, loss = 0.15469723\n",
      "Iteration 37, loss = 0.10605536\n",
      "Iteration 20, loss = 0.15264697\n",
      "Iteration 38, loss = 0.11372867\n",
      "Iteration 39, loss = 0.13174353\n",
      "Iteration 21, loss = 0.16812556\n",
      "Iteration 40, loss = 0.12885096\n",
      "Iteration 22, loss = 0.17087792\n",
      "Iteration 23, loss = 0.17299325\n",
      "Iteration 41, loss = 0.12360049\n",
      "Iteration 24, loss = 0.13112739\n",
      "Iteration 42, loss = 0.10839800\n",
      "Iteration 25, loss = 0.12933778\n",
      "Iteration 43, loss = 0.13065020\n",
      "Iteration 26, loss = 0.12995367\n",
      "Iteration 44, loss = 0.09642210\n",
      "Iteration 45, loss = 0.09725043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.13469794\n",
      "Iteration 28, loss = 0.13935310\n",
      "Iteration 29, loss = 0.12463680\n",
      "Iteration 30, loss = 0.12710560\n",
      "Iteration 31, loss = 0.11328073\n",
      "Iteration 32, loss = 0.12131041\n",
      "Iteration 33, loss = 0.11608553\n",
      "Iteration 34, loss = 0.12084334\n",
      "Iteration 1, loss = 1.00945935\n",
      "Iteration 35, loss = 0.10304068\n",
      "Iteration 2, loss = 0.48023536\n",
      "Iteration 36, loss = 0.11982150\n",
      "Iteration 3, loss = 0.40911099\n",
      "Iteration 37, loss = 0.10005563\n",
      "Iteration 4, loss = 0.35854462\n",
      "Iteration 38, loss = 0.13925135\n",
      "Iteration 5, loss = 0.30113536\n",
      "Iteration 39, loss = 0.12422752\n",
      "Iteration 6, loss = 0.28003983\n",
      "Iteration 7, loss = 0.24918654\n",
      "Iteration 40, loss = 0.10975779\n",
      "Iteration 8, loss = 0.22991347\n",
      "Iteration 41, loss = 0.11597112\n",
      "Iteration 9, loss = 0.22022085\n",
      "Iteration 42, loss = 0.11132794\n",
      "Iteration 10, loss = 0.20363575\n",
      "Iteration 43, loss = 0.11341972\n",
      "Iteration 44, loss = 0.09288445\n",
      "Iteration 11, loss = 0.18984079\n",
      "Iteration 45, loss = 0.08444413\n",
      "Iteration 12, loss = 0.18326198\n",
      "Iteration 46, loss = 0.08561034\n",
      "Iteration 13, loss = 0.17705420\n",
      "Iteration 47, loss = 0.08901133\n",
      "Iteration 48, loss = 0.08998847\n",
      "Iteration 14, loss = 0.16632365\n",
      "Iteration 49, loss = 0.07044975\n",
      "Iteration 15, loss = 0.16986121\n",
      "Iteration 50, loss = 0.06661285\n",
      "Iteration 16, loss = 0.17208179\n",
      "Iteration 51, loss = 0.06594995\n",
      "Iteration 52, loss = 0.06102311\n",
      "Iteration 17, loss = 0.20052906\n",
      "Iteration 53, loss = 0.06942825\n",
      "Iteration 18, loss = 0.17531909\n",
      "Iteration 54, loss = 0.08178584\n",
      "Iteration 55, loss = 0.10506476\n",
      "Iteration 56, loss = 0.09336215\n",
      "Iteration 19, loss = 0.14887508\n",
      "Iteration 57, loss = 0.09982968\n",
      "Iteration 20, loss = 0.16050349\n",
      "Iteration 58, loss = 0.06760945\n",
      "Iteration 21, loss = 0.18722748\n",
      "Iteration 59, loss = 0.06936837\n",
      "Iteration 22, loss = 0.15284654\n",
      "Iteration 60, loss = 0.06958819\n",
      "Iteration 23, loss = 0.15545377\n",
      "Iteration 61, loss = 0.08159422\n",
      "Iteration 24, loss = 0.16805688\n",
      "Iteration 25, loss = 0.15404521\n",
      "Iteration 62, loss = 0.09610569\n",
      "Iteration 26, loss = 0.13343569\n",
      "Iteration 63, loss = 0.05956232\n",
      "Iteration 64, loss = 0.05243039\n",
      "Iteration 65, loss = 0.06024236\n",
      "Iteration 27, loss = 0.12583811\n",
      "Iteration 66, loss = 0.05953058\n",
      "Iteration 28, loss = 0.12963457\n",
      "Iteration 29, loss = 0.13178764\n",
      "Iteration 67, loss = 0.06377725\n",
      "Iteration 30, loss = 0.14686655\n",
      "Iteration 68, loss = 0.07304276\n",
      "Iteration 69, loss = 0.07468897\n",
      "Iteration 31, loss = 0.14059792\n",
      "Iteration 32, loss = 0.11995427\n",
      "Iteration 70, loss = 0.09371023\n",
      "Iteration 33, loss = 0.12319351\n",
      "Iteration 71, loss = 0.08894321\n",
      "Iteration 34, loss = 0.11391827\n",
      "Iteration 72, loss = 0.08500991\n",
      "Iteration 35, loss = 0.13701381\n",
      "Iteration 73, loss = 0.09222770\n",
      "Iteration 36, loss = 0.14475374\n",
      "Iteration 74, loss = 0.09773027\n",
      "Iteration 37, loss = 0.11681794\n",
      "Iteration 38, loss = 0.11454075\n",
      "Iteration 39, loss = 0.11498606\n",
      "Iteration 40, loss = 0.10239405\n",
      "Iteration 75, loss = 0.13770799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.10095880\n",
      "Iteration 42, loss = 0.10638495\n",
      "Iteration 43, loss = 0.10909992\n",
      "Iteration 44, loss = 0.08445089\n",
      "Iteration 45, loss = 0.07566488\n",
      "Iteration 46, loss = 0.06701526\n",
      "Iteration 47, loss = 0.08204637\n",
      "Iteration 48, loss = 0.06565429\n",
      "Iteration 49, loss = 0.05827730\n",
      "Iteration 1, loss = 0.94979719\n",
      "Iteration 50, loss = 0.06321231\n",
      "Iteration 2, loss = 0.50489993\n",
      "Iteration 51, loss = 0.06466482\n",
      "Iteration 52, loss = 0.07558287\n",
      "Iteration 3, loss = 0.39601876\n",
      "Iteration 53, loss = 0.08648449\n",
      "Iteration 54, loss = 0.06102812\n",
      "Iteration 55, loss = 0.07839780\n",
      "Iteration 56, loss = 0.07022829\n",
      "Iteration 4, loss = 0.36796348\n",
      "Iteration 57, loss = 0.05758178\n",
      "Iteration 58, loss = 0.05514392\n",
      "Iteration 5, loss = 0.32602431\n",
      "Iteration 59, loss = 0.07143779\n",
      "Iteration 60, loss = 0.06540620\n",
      "Iteration 61, loss = 0.06913494\n",
      "Iteration 62, loss = 0.10649889\n",
      "Iteration 6, loss = 0.32418210\n",
      "Iteration 63, loss = 0.13500616\n",
      "Iteration 64, loss = 0.16854283\n",
      "Iteration 65, loss = 0.15925192\n",
      "Iteration 7, loss = 0.29042865\n",
      "Iteration 66, loss = 0.16705400\n",
      "Iteration 67, loss = 0.13513273\n",
      "Iteration 68, loss = 0.09433220\n",
      "Iteration 69, loss = 0.05874481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.28203285\n",
      "Iteration 9, loss = 0.25404131\n",
      "Iteration 10, loss = 0.23513233\n",
      "Iteration 11, loss = 0.22658967\n",
      "Iteration 1, loss = 1.18645532\n",
      "Iteration 12, loss = 0.21039218\n",
      "Iteration 2, loss = 0.55433747\n",
      "Iteration 13, loss = 0.19990869\n",
      "Iteration 3, loss = 0.43126236\n",
      "Iteration 14, loss = 0.19329062\n",
      "Iteration 4, loss = 0.36071295\n",
      "Iteration 5, loss = 0.32059097\n",
      "Iteration 15, loss = 0.18796377\n",
      "Iteration 6, loss = 0.31715634\n",
      "Iteration 16, loss = 0.17426576\n",
      "Iteration 17, loss = 0.17277201\n",
      "Iteration 7, loss = 0.29249319\n",
      "Iteration 18, loss = 0.17021329\n",
      "Iteration 8, loss = 0.26302061\n",
      "Iteration 9, loss = 0.24910699\n",
      "Iteration 19, loss = 0.16963769\n",
      "Iteration 10, loss = 0.23857815\n",
      "Iteration 20, loss = 0.15760269\n",
      "Iteration 11, loss = 0.22475117\n",
      "Iteration 21, loss = 0.15499617\n",
      "Iteration 12, loss = 0.23126359\n",
      "Iteration 22, loss = 0.16579645\n",
      "Iteration 13, loss = 0.21313397\n",
      "Iteration 23, loss = 0.13446822\n",
      "Iteration 24, loss = 0.12830997\n",
      "Iteration 25, loss = 0.12943143\n",
      "Iteration 26, loss = 0.12805042\n",
      "Iteration 14, loss = 0.19643902\n",
      "Iteration 27, loss = 0.11887380\n",
      "Iteration 15, loss = 0.18562285\n",
      "Iteration 28, loss = 0.11227617\n",
      "Iteration 29, loss = 0.11279382\n",
      "Iteration 16, loss = 0.16853914\n",
      "Iteration 30, loss = 0.13818197\n",
      "Iteration 17, loss = 0.15913720\n",
      "Iteration 18, loss = 0.16277235\n",
      "Iteration 31, loss = 0.12783847\n",
      "Iteration 19, loss = 0.17110753\n",
      "Iteration 32, loss = 0.12325318\n",
      "Iteration 33, loss = 0.11616549\n",
      "Iteration 20, loss = 0.20901230\n",
      "Iteration 34, loss = 0.14461445\n",
      "Iteration 21, loss = 0.17189853\n",
      "Iteration 35, loss = 0.12541520\n",
      "Iteration 22, loss = 0.17021113\n",
      "Iteration 23, loss = 0.15282116Iteration 36, loss = 0.15011289\n",
      "\n",
      "Iteration 24, loss = 0.14768578\n",
      "Iteration 37, loss = 0.11543917\n",
      "Iteration 25, loss = 0.13052353\n",
      "Iteration 26, loss = 0.13912986\n",
      "Iteration 27, loss = 0.12501860\n",
      "Iteration 28, loss = 0.14536744\n",
      "Iteration 38, loss = 0.10890546\n",
      "Iteration 29, loss = 0.16519860\n",
      "Iteration 39, loss = 0.09002400\n",
      "Iteration 40, loss = 0.09431005\n",
      "Iteration 41, loss = 0.10723233\n",
      "Iteration 30, loss = 0.16771343\n",
      "Iteration 42, loss = 0.09438147\n",
      "Iteration 31, loss = 0.18915142\n",
      "Iteration 32, loss = 0.17800925\n",
      "Iteration 33, loss = 0.13661229\n",
      "Iteration 43, loss = 0.07496398\n",
      "Iteration 34, loss = 0.12556508\n",
      "Iteration 35, loss = 0.15527904\n",
      "Iteration 36, loss = 0.16182868\n",
      "Iteration 37, loss = 0.11445975\n",
      "Iteration 38, loss = 0.12619565\n",
      "Iteration 44, loss = 0.07799919\n",
      "Iteration 39, loss = 0.11516073\n",
      "Iteration 40, loss = 0.12772927\n",
      "Iteration 45, loss = 0.06024084\n",
      "Iteration 46, loss = 0.07567894\n",
      "Iteration 47, loss = 0.06618961\n",
      "Iteration 41, loss = 0.10399154\n",
      "Iteration 42, loss = 0.09334428\n",
      "Iteration 43, loss = 0.08955041\n",
      "Iteration 44, loss = 0.08831686\n",
      "Iteration 48, loss = 0.07322288\n",
      "Iteration 49, loss = 0.09638571\n",
      "Iteration 50, loss = 0.12914801\n",
      "Iteration 51, loss = 0.10270320\n",
      "Iteration 45, loss = 0.10998652\n",
      "Iteration 46, loss = 0.13053775\n",
      "Iteration 52, loss = 0.10885805\n",
      "Iteration 47, loss = 0.12947316\n",
      "Iteration 53, loss = 0.09583909\n",
      "Iteration 48, loss = 0.11408432\n",
      "Iteration 49, loss = 0.09591295\n",
      "Iteration 54, loss = 0.08616481\n",
      "Iteration 55, loss = 0.06927447\n",
      "Iteration 50, loss = 0.10048402\n",
      "Iteration 56, loss = 0.07736568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.09235508\n",
      "Iteration 52, loss = 0.08994215\n",
      "Iteration 53, loss = 0.10871738\n",
      "Iteration 54, loss = 0.08807391\n",
      "Iteration 55, loss = 0.11166958\n",
      "Iteration 56, loss = 0.11024828\n",
      "Iteration 1, loss = 1.12751162\n",
      "Iteration 2, loss = 0.48389167\n",
      "Iteration 57, loss = 0.09904071\n",
      "Iteration 3, loss = 0.41136140\n",
      "Iteration 4, loss = 0.36100825\n",
      "Iteration 5, loss = 0.33620281\n",
      "Iteration 58, loss = 0.07389563\n",
      "Iteration 6, loss = 0.28701962\n",
      "Iteration 7, loss = 0.27672490\n",
      "Iteration 8, loss = 0.24765797\n",
      "Iteration 9, loss = 0.23437527\n",
      "Iteration 59, loss = 0.08852278\n",
      "Iteration 10, loss = 0.22391053\n",
      "Iteration 11, loss = 0.21247148\n",
      "Iteration 12, loss = 0.20210389\n",
      "Iteration 60, loss = 0.07204909\n",
      "Iteration 61, loss = 0.05682477\n",
      "Iteration 13, loss = 0.19449090\n",
      "Iteration 62, loss = 0.07231721\n",
      "Iteration 14, loss = 0.19205497\n",
      "Iteration 15, loss = 0.18174481\n",
      "Iteration 63, loss = 0.05803399\n",
      "Iteration 64, loss = 0.06229551\n",
      "Iteration 16, loss = 0.17240388\n",
      "Iteration 65, loss = 0.06435545\n",
      "Iteration 17, loss = 0.16850998\n",
      "Iteration 66, loss = 0.13469253\n",
      "Iteration 18, loss = 0.15698973\n",
      "Iteration 19, loss = 0.14790324\n",
      "Iteration 67, loss = 0.10541863\n",
      "Iteration 68, loss = 0.10277939\n",
      "Iteration 20, loss = 0.17009930\n",
      "Iteration 69, loss = 0.09181176\n",
      "Iteration 70, loss = 0.07625595\n",
      "Iteration 21, loss = 0.15388294\n",
      "Iteration 71, loss = 0.06018215\n",
      "Iteration 72, loss = 0.06444212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.14906789\n",
      "Iteration 23, loss = 0.15544681\n",
      "Iteration 24, loss = 0.20050514\n",
      "Iteration 25, loss = 0.20837250\n",
      "Iteration 26, loss = 0.20550997\n",
      "Iteration 27, loss = 0.17436004\n",
      "Iteration 28, loss = 0.14841255\n",
      "Iteration 29, loss = 0.13564388\n",
      "Iteration 30, loss = 0.13108332\n",
      "Iteration 1, loss = 0.97408176\n",
      "Iteration 2, loss = 0.47711387\n",
      "Iteration 31, loss = 0.12592433\n",
      "Iteration 3, loss = 0.39642795\n",
      "Iteration 32, loss = 0.12497607\n",
      "Iteration 33, loss = 0.13275575\n",
      "Iteration 34, loss = 0.11048122\n",
      "Iteration 4, loss = 0.34636366\n",
      "Iteration 5, loss = 0.30544130\n",
      "Iteration 35, loss = 0.12023158\n",
      "Iteration 36, loss = 0.12087294\n",
      "Iteration 6, loss = 0.28100669\n",
      "Iteration 37, loss = 0.12601932\n",
      "Iteration 38, loss = 0.12493839\n",
      "Iteration 7, loss = 0.26041119\n",
      "Iteration 8, loss = 0.24882282\n",
      "Iteration 39, loss = 0.10580323\n",
      "Iteration 9, loss = 0.24024605\n",
      "Iteration 40, loss = 0.11999495\n",
      "Iteration 10, loss = 0.22785902\n",
      "Iteration 41, loss = 0.15210953\n",
      "Iteration 11, loss = 0.21702355\n",
      "Iteration 42, loss = 0.13218787\n",
      "Iteration 12, loss = 0.22626858\n",
      "Iteration 13, loss = 0.20459346\n",
      "Iteration 43, loss = 0.12289092\n",
      "Iteration 14, loss = 0.20763648\n",
      "Iteration 15, loss = 0.18778382\n",
      "Iteration 44, loss = 0.11274806\n",
      "Iteration 45, loss = 0.10632342\n",
      "Iteration 16, loss = 0.18032704\n",
      "Iteration 46, loss = 0.09049457\n",
      "Iteration 17, loss = 0.16236737\n",
      "Iteration 47, loss = 0.08554908\n",
      "Iteration 18, loss = 0.17141648\n",
      "Iteration 19, loss = 0.17252175\n",
      "Iteration 48, loss = 0.09326228\n",
      "Iteration 20, loss = 0.18605920\n",
      "Iteration 21, loss = 0.21832636\n",
      "Iteration 49, loss = 0.09027065\n",
      "Iteration 22, loss = 0.19350743\n",
      "Iteration 23, loss = 0.19316041\n",
      "Iteration 24, loss = 0.20171541\n",
      "Iteration 50, loss = 0.08993476\n",
      "Iteration 25, loss = 0.20804604\n",
      "Iteration 51, loss = 0.09433320\n",
      "Iteration 26, loss = 0.17244844\n",
      "Iteration 52, loss = 0.08119158\n",
      "Iteration 53, loss = 0.09123887\n",
      "Iteration 27, loss = 0.14740935\n",
      "Iteration 28, loss = 0.13839653\n",
      "Iteration 29, loss = 0.13346061\n",
      "Iteration 54, loss = 0.09782411\n",
      "Iteration 30, loss = 0.13805292\n",
      "Iteration 55, loss = 0.08110817\n",
      "Iteration 31, loss = 0.13212388\n",
      "Iteration 56, loss = 0.07496473\n",
      "Iteration 32, loss = 0.11368525\n",
      "Iteration 57, loss = 0.06817023\n",
      "Iteration 33, loss = 0.11133418\n",
      "Iteration 58, loss = 0.10277153\n",
      "Iteration 34, loss = 0.10493439\n",
      "Iteration 35, loss = 0.10480991\n",
      "Iteration 36, loss = 0.12408026\n",
      "Iteration 59, loss = 0.13003161\n",
      "Iteration 37, loss = 0.12859404\n",
      "Iteration 38, loss = 0.13271651\n",
      "Iteration 60, loss = 0.24791709\n",
      "Iteration 39, loss = 0.20884778\n",
      "Iteration 40, loss = 0.22128461\n",
      "Iteration 41, loss = 0.26963226\n",
      "Iteration 42, loss = 0.17856197\n",
      "Iteration 43, loss = 0.19379420\n",
      "Iteration 44, loss = 0.19129441\n",
      "Iteration 61, loss = 0.20266616\n",
      "Iteration 62, loss = 0.18363118\n",
      "Iteration 45, loss = 0.16277434\n",
      "Iteration 63, loss = 0.17205872\n",
      "Iteration 46, loss = 0.16821521\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.16834324\n",
      "Iteration 65, loss = 0.11908802\n",
      "Iteration 66, loss = 0.14227909\n",
      "Iteration 67, loss = 0.08609141\n",
      "Iteration 68, loss = 0.09073118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93990037\n",
      "Iteration 2, loss = 0.48508933\n",
      "Iteration 3, loss = 0.41118661\n",
      "Iteration 4, loss = 0.33759231\n",
      "Iteration 5, loss = 0.29986186\n",
      "Iteration 6, loss = 0.27881227\n",
      "Iteration 7, loss = 0.25228797\n",
      "Iteration 8, loss = 0.24574262\n",
      "Iteration 9, loss = 0.23335749\n",
      "Iteration 1, loss = 0.86332113\n",
      "Iteration 10, loss = 0.22462454\n",
      "Iteration 2, loss = 0.43614077\n",
      "Iteration 11, loss = 0.21535244\n",
      "Iteration 12, loss = 0.20568835\n",
      "Iteration 3, loss = 0.36828692\n",
      "Iteration 13, loss = 0.21394961\n",
      "Iteration 4, loss = 0.32105513\n",
      "Iteration 14, loss = 0.21261507\n",
      "Iteration 5, loss = 0.27959332\n",
      "Iteration 15, loss = 0.23541869\n",
      "Iteration 6, loss = 0.25827327\n",
      "Iteration 16, loss = 0.19010671\n",
      "Iteration 17, loss = 0.19647081\n",
      "Iteration 7, loss = 0.23547591\n",
      "Iteration 18, loss = 0.17287902\n",
      "Iteration 19, loss = 0.16510446\n",
      "Iteration 20, loss = 0.16461797\n",
      "Iteration 21, loss = 0.15615848\n",
      "Iteration 8, loss = 0.21482473\n",
      "Iteration 22, loss = 0.14874478\n",
      "Iteration 23, loss = 0.14513917\n",
      "Iteration 9, loss = 0.20000735\n",
      "Iteration 24, loss = 0.12747835\n",
      "Iteration 25, loss = 0.13381039\n",
      "Iteration 26, loss = 0.13377739\n",
      "Iteration 10, loss = 0.18846285\n",
      "Iteration 27, loss = 0.11749751\n",
      "Iteration 11, loss = 0.17945271\n",
      "Iteration 28, loss = 0.11646750\n",
      "Iteration 29, loss = 0.11989869\n",
      "Iteration 30, loss = 0.11368615\n",
      "Iteration 31, loss = 0.12305129\n",
      "Iteration 12, loss = 0.16394915\n",
      "Iteration 32, loss = 0.10927852\n",
      "Iteration 13, loss = 0.17576220\n",
      "Iteration 14, loss = 0.15025184\n",
      "Iteration 33, loss = 0.11611791\n",
      "Iteration 15, loss = 0.13903857\n",
      "Iteration 16, loss = 0.16087676\n",
      "Iteration 34, loss = 0.13381939\n",
      "Iteration 17, loss = 0.16369549\n",
      "Iteration 18, loss = 0.18552675\n",
      "Iteration 35, loss = 0.11374214\n",
      "Iteration 19, loss = 0.16559596\n",
      "Iteration 36, loss = 0.20073727\n",
      "Iteration 37, loss = 0.13737097\n",
      "Iteration 20, loss = 0.19222741\n",
      "Iteration 38, loss = 0.13093825\n",
      "Iteration 39, loss = 0.12488996\n",
      "Iteration 21, loss = 0.16984295\n",
      "Iteration 40, loss = 0.14634019\n",
      "Iteration 22, loss = 0.15508314\n",
      "Iteration 23, loss = 0.16091055\n",
      "Iteration 24, loss = 0.14521441\n",
      "Iteration 41, loss = 0.11781014\n",
      "Iteration 25, loss = 0.14168813\n",
      "Iteration 42, loss = 0.11036294\n",
      "Iteration 43, loss = 0.10316816\n",
      "Iteration 44, loss = 0.10145866\n",
      "Iteration 26, loss = 0.15833486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.08934797\n",
      "Iteration 46, loss = 0.09164025\n",
      "Iteration 47, loss = 0.09323323\n",
      "Iteration 48, loss = 0.09923098\n",
      "Iteration 49, loss = 0.09994657\n",
      "Iteration 50, loss = 0.09976493\n",
      "Iteration 51, loss = 0.11234858\n",
      "Iteration 52, loss = 0.18345504\n",
      "Iteration 53, loss = 0.16477469\n",
      "Iteration 54, loss = 0.17307925\n",
      "Iteration 55, loss = 0.10748908\n",
      "Iteration 1, loss = 1.10746213\n",
      "Iteration 56, loss = 0.09381646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.50776413\n",
      "Iteration 3, loss = 0.47767824\n",
      "Iteration 4, loss = 0.38709789\n",
      "Iteration 5, loss = 0.33224915\n",
      "Iteration 6, loss = 0.28791509\n",
      "Iteration 7, loss = 0.26978377\n",
      "Iteration 8, loss = 0.27279609\n",
      "Iteration 9, loss = 0.24606652\n",
      "Iteration 10, loss = 0.23483669\n",
      "Iteration 1, loss = 1.04999030\n",
      "Iteration 11, loss = 0.22865246\n",
      "Iteration 2, loss = 0.51745049\n",
      "Iteration 3, loss = 0.41857529\n",
      "Iteration 12, loss = 0.22152209\n",
      "Iteration 4, loss = 0.33355171\n",
      "Iteration 13, loss = 0.22228809\n",
      "Iteration 5, loss = 0.30483868\n",
      "Iteration 14, loss = 0.21134293\n",
      "Iteration 6, loss = 0.26272641\n",
      "Iteration 7, loss = 0.23652866\n",
      "Iteration 15, loss = 0.18990673\n",
      "Iteration 8, loss = 0.23043341\n",
      "Iteration 9, loss = 0.20614064\n",
      "Iteration 16, loss = 0.17422105\n",
      "Iteration 17, loss = 0.16630485\n",
      "Iteration 18, loss = 0.15897666\n",
      "Iteration 10, loss = 0.20074675\n",
      "Iteration 11, loss = 0.18450254\n",
      "Iteration 12, loss = 0.16401808\n",
      "Iteration 19, loss = 0.14527629\n",
      "Iteration 13, loss = 0.15887774\n",
      "Iteration 20, loss = 0.14607713\n",
      "Iteration 14, loss = 0.14446565\n",
      "Iteration 15, loss = 0.13124788\n",
      "Iteration 21, loss = 0.15644758\n",
      "Iteration 22, loss = 0.14879608\n",
      "Iteration 23, loss = 0.12962906\n",
      "Iteration 24, loss = 0.15562546\n",
      "Iteration 16, loss = 0.14449507\n",
      "Iteration 25, loss = 0.13364305\n",
      "Iteration 17, loss = 0.12458355\n",
      "Iteration 26, loss = 0.11777436\n",
      "Iteration 18, loss = 0.11529060\n",
      "Iteration 27, loss = 0.14001233\n",
      "Iteration 19, loss = 0.09848717\n",
      "Iteration 20, loss = 0.09614083\n",
      "Iteration 28, loss = 0.16326554\n",
      "Iteration 29, loss = 0.14052753\n",
      "Iteration 21, loss = 0.11118280\n",
      "Iteration 30, loss = 0.13747625\n",
      "Iteration 22, loss = 0.09760761\n",
      "Iteration 31, loss = 0.16222149\n",
      "Iteration 23, loss = 0.09257047\n",
      "Iteration 32, loss = 0.20547266\n",
      "Iteration 24, loss = 0.07400688\n",
      "Iteration 33, loss = 0.18968571\n",
      "Iteration 34, loss = 0.18598168\n",
      "Iteration 35, loss = 0.14669908\n",
      "Iteration 25, loss = 0.07389708\n",
      "Iteration 36, loss = 0.11247623\n",
      "Iteration 26, loss = 0.07298724\n",
      "Iteration 37, loss = 0.11090494\n",
      "Iteration 27, loss = 0.08587204\n",
      "Iteration 38, loss = 0.10031796\n",
      "Iteration 39, loss = 0.09409030\n",
      "Iteration 28, loss = 0.06975364\n",
      "Iteration 40, loss = 0.09526628\n",
      "Iteration 29, loss = 0.09329377\n",
      "Iteration 41, loss = 0.11296907\n",
      "Iteration 30, loss = 0.14794932\n",
      "Iteration 42, loss = 0.10800945\n",
      "Iteration 43, loss = 0.08872212\n",
      "Iteration 31, loss = 0.11187410\n",
      "Iteration 32, loss = 0.11518772\n",
      "Iteration 44, loss = 0.08380815\n",
      "Iteration 45, loss = 0.08203775\n",
      "Iteration 33, loss = 0.08788208\n",
      "Iteration 46, loss = 0.08182886\n",
      "Iteration 34, loss = 0.08490141\n",
      "Iteration 35, loss = 0.07156896\n",
      "Iteration 47, loss = 0.08338355\n",
      "Iteration 36, loss = 0.08926042\n",
      "Iteration 37, loss = 0.10840998\n",
      "Iteration 48, loss = 0.07536316\n",
      "Iteration 49, loss = 0.08604515\n",
      "Iteration 50, loss = 0.10970861\n",
      "Iteration 51, loss = 0.10362188\n",
      "Iteration 38, loss = 0.08451365\n",
      "Iteration 52, loss = 0.09788257\n",
      "Iteration 53, loss = 0.10257832\n",
      "Iteration 39, loss = 0.07252343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.08693423\n",
      "Iteration 55, loss = 0.07749236\n",
      "Iteration 56, loss = 0.07186896\n",
      "Iteration 57, loss = 0.06404931\n",
      "Iteration 58, loss = 0.05753047\n",
      "Iteration 59, loss = 0.05625579\n",
      "Iteration 60, loss = 0.05460246\n",
      "Iteration 61, loss = 0.04587524\n",
      "Iteration 62, loss = 0.07081206\n",
      "Iteration 1, loss = 0.94944864\n",
      "Iteration 63, loss = 0.06485304\n",
      "Iteration 2, loss = 0.47125388\n",
      "Iteration 64, loss = 0.05856864\n",
      "Iteration 3, loss = 0.48267918\n",
      "Iteration 65, loss = 0.08792988\n",
      "Iteration 66, loss = 0.10118863\n",
      "Iteration 4, loss = 0.37864819\n",
      "Iteration 67, loss = 0.10691772\n",
      "Iteration 5, loss = 0.31477294\n",
      "Iteration 68, loss = 0.10294360\n",
      "Iteration 6, loss = 0.27542448\n",
      "Iteration 69, loss = 0.07874544\n",
      "Iteration 7, loss = 0.27541785\n",
      "Iteration 70, loss = 0.08019027\n",
      "Iteration 8, loss = 0.27398116\n",
      "Iteration 71, loss = 0.07337523\n",
      "Iteration 9, loss = 0.25493198\n",
      "Iteration 10, loss = 0.23658697\n",
      "Iteration 11, loss = 0.22355465\n",
      "Iteration 12, loss = 0.23230557\n",
      "Iteration 72, loss = 0.05779603\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.22059460\n",
      "Iteration 14, loss = 0.21807428\n",
      "Iteration 15, loss = 0.18019537\n",
      "Iteration 16, loss = 0.18676004\n",
      "Iteration 17, loss = 0.16623977\n",
      "Iteration 18, loss = 0.16574426\n",
      "Iteration 19, loss = 0.16144823\n",
      "Iteration 20, loss = 0.16020680\n",
      "Iteration 21, loss = 0.19472546\n",
      "Iteration 22, loss = 0.15780094\n",
      "Iteration 23, loss = 0.19238804\n",
      "Iteration 24, loss = 0.23254481\n",
      "Iteration 1, loss = 1.07796304\n",
      "Iteration 25, loss = 0.21705417\n",
      "Iteration 2, loss = 0.49055631\n",
      "Iteration 3, loss = 0.40997787\n",
      "Iteration 26, loss = 0.23114647\n",
      "Iteration 4, loss = 0.38319007\n",
      "Iteration 5, loss = 0.33399484\n",
      "Iteration 27, loss = 0.19058463\n",
      "Iteration 6, loss = 0.28989121\n",
      "Iteration 28, loss = 0.15622740\n",
      "Iteration 7, loss = 0.27296742\n",
      "Iteration 8, loss = 0.25653663\n",
      "Iteration 29, loss = 0.17346185\n",
      "Iteration 30, loss = 0.14150583\n",
      "Iteration 31, loss = 0.17911179\n",
      "Iteration 32, loss = 0.13736778\n",
      "Iteration 9, loss = 0.23496446\n",
      "Iteration 33, loss = 0.12652887\n",
      "Iteration 34, loss = 0.11122234\n",
      "Iteration 35, loss = 0.13095899\n",
      "Iteration 10, loss = 0.23617687\n",
      "Iteration 36, loss = 0.10489141\n",
      "Iteration 11, loss = 0.21734260\n",
      "Iteration 37, loss = 0.10017130\n",
      "Iteration 12, loss = 0.24071476\n",
      "Iteration 13, loss = 0.22046980\n",
      "Iteration 38, loss = 0.09043986\n",
      "Iteration 39, loss = 0.09142707\n",
      "Iteration 40, loss = 0.09071912\n",
      "Iteration 41, loss = 0.09168088\n",
      "Iteration 14, loss = 0.19756964\n",
      "Iteration 15, loss = 0.16806207\n",
      "Iteration 16, loss = 0.16242253\n",
      "Iteration 42, loss = 0.08535085\n",
      "Iteration 17, loss = 0.15540490\n",
      "Iteration 18, loss = 0.15251296\n",
      "Iteration 43, loss = 0.08074270\n",
      "Iteration 19, loss = 0.14370202\n",
      "Iteration 44, loss = 0.08879740\n",
      "Iteration 20, loss = 0.13112671\n",
      "Iteration 45, loss = 0.10936849\n",
      "Iteration 21, loss = 0.15278090\n",
      "Iteration 22, loss = 0.13317596\n",
      "Iteration 23, loss = 0.13328951\n",
      "Iteration 46, loss = 0.12730291\n",
      "Iteration 24, loss = 0.12583758\n",
      "Iteration 47, loss = 0.11258764\n",
      "Iteration 25, loss = 0.16312812\n",
      "Iteration 48, loss = 0.13736833\n",
      "Iteration 26, loss = 0.10742036\n",
      "Iteration 49, loss = 0.10524144\n",
      "Iteration 50, loss = 0.10947296\n",
      "Iteration 27, loss = 0.10032683\n",
      "Iteration 28, loss = 0.10875527\n",
      "Iteration 51, loss = 0.11104267\n",
      "Iteration 29, loss = 0.09300449\n",
      "Iteration 52, loss = 0.11044896\n",
      "Iteration 30, loss = 0.13437009\n",
      "Iteration 53, loss = 0.10902311\n",
      "Iteration 54, loss = 0.13154711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.14252473\n",
      "Iteration 32, loss = 0.14211906\n",
      "Iteration 33, loss = 0.14489424\n",
      "Iteration 34, loss = 0.14456392\n",
      "Iteration 35, loss = 0.17704236\n",
      "Iteration 36, loss = 0.13922782\n",
      "Iteration 37, loss = 0.14031253\n",
      "Iteration 38, loss = 0.10439269\n",
      "Iteration 1, loss = 1.23072055\n",
      "Iteration 39, loss = 0.11664654\n",
      "Iteration 2, loss = 0.54604972\n",
      "Iteration 3, loss = 0.41908978\n",
      "Iteration 4, loss = 0.37359865\n",
      "Iteration 40, loss = 0.11963580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.32191575\n",
      "Iteration 6, loss = 0.28583987\n",
      "Iteration 7, loss = 0.26747014\n",
      "Iteration 8, loss = 0.26306850\n",
      "Iteration 9, loss = 0.24143887\n",
      "Iteration 10, loss = 0.22609365\n",
      "Iteration 11, loss = 0.19892690\n",
      "Iteration 1, loss = 1.30555792\n",
      "Iteration 12, loss = 0.19107236\n",
      "Iteration 13, loss = 0.17365851\n",
      "Iteration 14, loss = 0.16353414\n",
      "Iteration 2, loss = 0.50538558\n",
      "Iteration 15, loss = 0.15738184\n",
      "Iteration 16, loss = 0.15216772\n",
      "Iteration 17, loss = 0.14757803\n",
      "Iteration 3, loss = 0.43961937\n",
      "Iteration 4, loss = 0.36226972\n",
      "Iteration 5, loss = 0.33849743\n",
      "Iteration 6, loss = 0.29860611\n",
      "Iteration 7, loss = 0.27711577\n",
      "Iteration 18, loss = 0.15789687\n",
      "Iteration 8, loss = 0.26204004\n",
      "Iteration 9, loss = 0.24054163\n",
      "Iteration 10, loss = 0.22897048\n",
      "Iteration 11, loss = 0.21906342\n",
      "Iteration 19, loss = 0.17479441\n",
      "Iteration 12, loss = 0.23517989\n",
      "Iteration 13, loss = 0.23610657\n",
      "Iteration 20, loss = 0.15925802\n",
      "Iteration 21, loss = 0.15231184\n",
      "Iteration 14, loss = 0.22556748\n",
      "Iteration 22, loss = 0.13648282\n",
      "Iteration 15, loss = 0.19392762\n",
      "Iteration 16, loss = 0.18285954\n",
      "Iteration 23, loss = 0.12861127\n",
      "Iteration 24, loss = 0.11715129\n",
      "Iteration 25, loss = 0.12536931\n",
      "Iteration 26, loss = 0.13313200\n",
      "Iteration 17, loss = 0.16944381\n",
      "Iteration 27, loss = 0.11528405\n",
      "Iteration 18, loss = 0.18088606\n",
      "Iteration 28, loss = 0.11241446\n",
      "Iteration 19, loss = 0.17382856\n",
      "Iteration 29, loss = 0.11452646\n",
      "Iteration 20, loss = 0.16347739\n",
      "Iteration 30, loss = 0.13361018\n",
      "Iteration 31, loss = 0.12071489\n",
      "Iteration 21, loss = 0.14572521\n",
      "Iteration 22, loss = 0.16684988\n",
      "Iteration 32, loss = 0.14401132\n",
      "Iteration 23, loss = 0.16291872\n",
      "Iteration 33, loss = 0.15377820\n",
      "Iteration 24, loss = 0.17678704\n",
      "Iteration 34, loss = 0.15278473\n",
      "Iteration 25, loss = 0.13282171\n",
      "Iteration 35, loss = 0.12094998\n",
      "Iteration 26, loss = 0.14054511\n",
      "Iteration 27, loss = 0.14259800\n",
      "Iteration 28, loss = 0.14598302\n",
      "Iteration 36, loss = 0.10920206\n",
      "Iteration 29, loss = 0.13486239\n",
      "Iteration 37, loss = 0.10477877\n",
      "Iteration 30, loss = 0.12469140\n",
      "Iteration 38, loss = 0.07791442\n",
      "Iteration 39, loss = 0.07949374\n",
      "Iteration 31, loss = 0.11110527\n",
      "Iteration 32, loss = 0.11379370\n",
      "Iteration 33, loss = 0.11056459\n",
      "Iteration 40, loss = 0.08796949\n",
      "Iteration 41, loss = 0.08163252\n",
      "Iteration 34, loss = 0.13484077\n",
      "Iteration 35, loss = 0.11882085\n",
      "Iteration 36, loss = 0.10101975\n",
      "Iteration 42, loss = 0.08952435\n",
      "Iteration 37, loss = 0.12692919\n",
      "Iteration 38, loss = 0.12196445\n",
      "Iteration 43, loss = 0.09119417\n",
      "Iteration 44, loss = 0.07563110\n",
      "Iteration 45, loss = 0.06508420\n",
      "Iteration 39, loss = 0.12623573\n",
      "Iteration 46, loss = 0.08971722\n",
      "Iteration 40, loss = 0.14686821\n",
      "Iteration 41, loss = 0.12519837\n",
      "Iteration 47, loss = 0.09562885\n",
      "Iteration 42, loss = 0.13266328\n",
      "Iteration 48, loss = 0.11619718\n",
      "Iteration 49, loss = 0.14147946\n",
      "Iteration 43, loss = 0.10992915\n",
      "Iteration 50, loss = 0.15643469\n",
      "Iteration 44, loss = 0.11364419\n",
      "Iteration 45, loss = 0.11765832\n",
      "Iteration 51, loss = 0.14225029\n",
      "Iteration 46, loss = 0.08459889\n",
      "Iteration 52, loss = 0.18218206\n",
      "Iteration 53, loss = 0.10159773\n",
      "Iteration 47, loss = 0.08648010\n",
      "Iteration 54, loss = 0.08796493\n",
      "Iteration 48, loss = 0.09149794\n",
      "Iteration 55, loss = 0.09707537\n",
      "Iteration 49, loss = 0.08492072\n",
      "Iteration 50, loss = 0.11347121\n",
      "Iteration 56, loss = 0.08855559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.11140790\n",
      "Iteration 52, loss = 0.10880101\n",
      "Iteration 53, loss = 0.09547690\n",
      "Iteration 54, loss = 0.09904643\n",
      "Iteration 55, loss = 0.11128108\n",
      "Iteration 56, loss = 0.08058917\n",
      "Iteration 57, loss = 0.08145123\n",
      "Iteration 58, loss = 0.07715462\n",
      "Iteration 59, loss = 0.08493395\n",
      "Iteration 60, loss = 0.10831196\n",
      "Iteration 61, loss = 0.08751179\n",
      "Iteration 62, loss = 0.07343471\n",
      "Iteration 1, loss = 0.81506563\n",
      "Iteration 2, loss = 0.50272350\n",
      "Iteration 63, loss = 0.09647526\n",
      "Iteration 3, loss = 0.42856677\n",
      "Iteration 4, loss = 0.34468975\n",
      "Iteration 64, loss = 0.07501885\n",
      "Iteration 65, loss = 0.06521255\n",
      "Iteration 66, loss = 0.05959490\n",
      "Iteration 5, loss = 0.32695653\n",
      "Iteration 67, loss = 0.05246987\n",
      "Iteration 6, loss = 0.29812802\n",
      "Iteration 7, loss = 0.28422833\n",
      "Iteration 8, loss = 0.26238505\n",
      "Iteration 68, loss = 0.07126275\n",
      "Iteration 69, loss = 0.07376273\n",
      "Iteration 9, loss = 0.23886642\n",
      "Iteration 70, loss = 0.11970865\n",
      "Iteration 10, loss = 0.24144247\n",
      "Iteration 11, loss = 0.22834278\n",
      "Iteration 71, loss = 0.12213436\n",
      "Iteration 12, loss = 0.22245620\n",
      "Iteration 72, loss = 0.10296448\n",
      "Iteration 13, loss = 0.23450800\n",
      "Iteration 73, loss = 0.08689013\n",
      "Iteration 14, loss = 0.21057966\n",
      "Iteration 74, loss = 0.10565595\n",
      "Iteration 15, loss = 0.21646001\n",
      "Iteration 16, loss = 0.18827355\n",
      "Iteration 75, loss = 0.16129602\n",
      "Iteration 76, loss = 0.11124529\n",
      "Iteration 17, loss = 0.22207527\n",
      "Iteration 77, loss = 0.13091982\n",
      "Iteration 18, loss = 0.20557924\n",
      "Iteration 19, loss = 0.18651534\n",
      "Iteration 78, loss = 0.13368386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.20062208\n",
      "Iteration 21, loss = 0.17653177\n",
      "Iteration 22, loss = 0.14645870\n",
      "Iteration 23, loss = 0.12756716\n",
      "Iteration 24, loss = 0.13138184\n",
      "Iteration 25, loss = 0.12080194\n",
      "Iteration 26, loss = 0.12079297\n",
      "Iteration 27, loss = 0.13099619\n",
      "Iteration 1, loss = 1.01673371\n",
      "Iteration 2, loss = 0.57595911\n",
      "Iteration 28, loss = 0.12652461\n",
      "Iteration 3, loss = 0.44721315\n",
      "Iteration 4, loss = 0.38422307\n",
      "Iteration 5, loss = 0.33952797\n",
      "Iteration 29, loss = 0.14665436\n",
      "Iteration 6, loss = 0.29647355\n",
      "Iteration 30, loss = 0.14442231\n",
      "Iteration 31, loss = 0.19266215\n",
      "Iteration 7, loss = 0.27017337\n",
      "Iteration 8, loss = 0.27934021\n",
      "Iteration 32, loss = 0.17260926\n",
      "Iteration 9, loss = 0.24544594\n",
      "Iteration 33, loss = 0.17830365\n",
      "Iteration 10, loss = 0.24299368\n",
      "Iteration 11, loss = 0.22555049\n",
      "Iteration 12, loss = 0.21189690\n",
      "Iteration 34, loss = 0.16112679\n",
      "Iteration 13, loss = 0.19278128\n",
      "Iteration 35, loss = 0.16236559\n",
      "Iteration 14, loss = 0.18938954\n",
      "Iteration 15, loss = 0.17883802\n",
      "Iteration 36, loss = 0.17346735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.16879438\n",
      "Iteration 17, loss = 0.15844704\n",
      "Iteration 18, loss = 0.15542444\n",
      "Iteration 19, loss = 0.15729206\n",
      "Iteration 20, loss = 0.18069357\n",
      "Iteration 21, loss = 0.17420661\n",
      "Iteration 1, loss = 0.82924204\n",
      "Iteration 22, loss = 0.16034966\n",
      "Iteration 23, loss = 0.14143271\n",
      "Iteration 2, loss = 0.44063372\n",
      "Iteration 24, loss = 0.17043939\n",
      "Iteration 3, loss = 0.36737331\n",
      "Iteration 25, loss = 0.18422528\n",
      "Iteration 4, loss = 0.31765177\n",
      "Iteration 5, loss = 0.28477143\n",
      "Iteration 26, loss = 0.16808900\n",
      "Iteration 27, loss = 0.16169567\n",
      "Iteration 6, loss = 0.27368625\n",
      "Iteration 28, loss = 0.16497299\n",
      "Iteration 29, loss = 0.15924921\n",
      "Iteration 7, loss = 0.26543460\n",
      "Iteration 30, loss = 0.12316458\n",
      "Iteration 8, loss = 0.24313544\n",
      "Iteration 31, loss = 0.11976767\n",
      "Iteration 9, loss = 0.22820650\n",
      "Iteration 32, loss = 0.11164635\n",
      "Iteration 10, loss = 0.21227410\n",
      "Iteration 11, loss = 0.19145657\n",
      "Iteration 12, loss = 0.18068889\n",
      "Iteration 33, loss = 0.10760226\n",
      "Iteration 13, loss = 0.18235286\n",
      "Iteration 34, loss = 0.09645679\n",
      "Iteration 14, loss = 0.17933434\n",
      "Iteration 35, loss = 0.09988201\n",
      "Iteration 15, loss = 0.17912523\n",
      "Iteration 36, loss = 0.10771852\n",
      "Iteration 16, loss = 0.19239836\n",
      "Iteration 37, loss = 0.13854481\n",
      "Iteration 17, loss = 0.17160162\n",
      "Iteration 38, loss = 0.13198167\n",
      "Iteration 18, loss = 0.16595053\n",
      "Iteration 19, loss = 0.15750459\n",
      "Iteration 20, loss = 0.13937085\n",
      "Iteration 21, loss = 0.15941999\n",
      "Iteration 39, loss = 0.16896799\n",
      "Iteration 40, loss = 0.18818439\n",
      "Iteration 22, loss = 0.15969432\n",
      "Iteration 23, loss = 0.13743105\n",
      "Iteration 24, loss = 0.14449657\n",
      "Iteration 25, loss = 0.12042102\n",
      "Iteration 41, loss = 0.27282397\n",
      "Iteration 42, loss = 0.23788784\n",
      "Iteration 43, loss = 0.15651535\n",
      "Iteration 26, loss = 0.11106712\n",
      "Iteration 44, loss = 0.13285191\n",
      "Iteration 27, loss = 0.09518459\n",
      "Iteration 28, loss = 0.10222847\n",
      "Iteration 45, loss = 0.14004003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.12464144\n",
      "Iteration 30, loss = 0.11494597\n",
      "Iteration 31, loss = 0.11801250\n",
      "Iteration 32, loss = 0.12290719\n",
      "Iteration 33, loss = 0.13403439\n",
      "Iteration 1, loss = 0.98812989\n",
      "Iteration 34, loss = 0.15371896\n",
      "Iteration 2, loss = 0.43904897\n",
      "Iteration 35, loss = 0.15164013\n",
      "Iteration 3, loss = 0.38769087\n",
      "Iteration 4, loss = 0.32434987\n",
      "Iteration 36, loss = 0.16275794\n",
      "Iteration 5, loss = 0.29414900\n",
      "Iteration 6, loss = 0.25171241\n",
      "Iteration 37, loss = 0.11359075\n",
      "Iteration 7, loss = 0.24509810\n",
      "Iteration 38, loss = 0.14216952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.21600883\n",
      "Iteration 9, loss = 0.19795952\n",
      "Iteration 10, loss = 0.18514989\n",
      "Iteration 11, loss = 0.17438358\n",
      "Iteration 12, loss = 0.16926861\n",
      "Iteration 1, loss = 0.94337402\n",
      "Iteration 2, loss = 0.44927595\n",
      "Iteration 13, loss = 0.17254977\n",
      "Iteration 3, loss = 0.37136614\n",
      "Iteration 14, loss = 0.16292287\n",
      "Iteration 15, loss = 0.14873781\n",
      "Iteration 4, loss = 0.30201141\n",
      "Iteration 5, loss = 0.28435816\n",
      "Iteration 16, loss = 0.14349501\n",
      "Iteration 6, loss = 0.24523534\n",
      "Iteration 17, loss = 0.13154898\n",
      "Iteration 7, loss = 0.22957053\n",
      "Iteration 18, loss = 0.11105006\n",
      "Iteration 19, loss = 0.11084549\n",
      "Iteration 8, loss = 0.21563921\n",
      "Iteration 20, loss = 0.11834480\n",
      "Iteration 21, loss = 0.13666434\n",
      "Iteration 22, loss = 0.11465055\n",
      "Iteration 9, loss = 0.19292707\n",
      "Iteration 23, loss = 0.15074934\n",
      "Iteration 24, loss = 0.18054180\n",
      "Iteration 25, loss = 0.14488500\n",
      "Iteration 10, loss = 0.18218924\n",
      "Iteration 11, loss = 0.18320408\n",
      "Iteration 26, loss = 0.14073562\n",
      "Iteration 12, loss = 0.16354440\n",
      "Iteration 13, loss = 0.15399259\n",
      "Iteration 14, loss = 0.15469412\n",
      "Iteration 27, loss = 0.12191855\n",
      "Iteration 15, loss = 0.14708810\n",
      "Iteration 28, loss = 0.11776418\n",
      "Iteration 29, loss = 0.08266176\n",
      "Iteration 16, loss = 0.13100750\n",
      "Iteration 30, loss = 0.08448458\n",
      "Iteration 17, loss = 0.13110091\n",
      "Iteration 18, loss = 0.13524411\n",
      "Iteration 31, loss = 0.08834898\n",
      "Iteration 19, loss = 0.14637245\n",
      "Iteration 32, loss = 0.07654111\n",
      "Iteration 33, loss = 0.09190197\n",
      "Iteration 20, loss = 0.15404481\n",
      "Iteration 34, loss = 0.09056189\n",
      "Iteration 21, loss = 0.17998933\n",
      "Iteration 35, loss = 0.09254928\n",
      "Iteration 22, loss = 0.18668914\n",
      "Iteration 23, loss = 0.16460569\n",
      "Iteration 36, loss = 0.10385052\n",
      "Iteration 37, loss = 0.08704611\n",
      "Iteration 24, loss = 0.15710216\n",
      "Iteration 38, loss = 0.09224567\n",
      "Iteration 39, loss = 0.08714896\n",
      "Iteration 25, loss = 0.12078601\n",
      "Iteration 40, loss = 0.06235598\n",
      "Iteration 26, loss = 0.12879662\n",
      "Iteration 41, loss = 0.06025124\n",
      "Iteration 42, loss = 0.06238166\n",
      "Iteration 27, loss = 0.11494085\n",
      "Iteration 43, loss = 0.06877032\n",
      "Iteration 44, loss = 0.05243766\n",
      "Iteration 28, loss = 0.09791559\n",
      "Iteration 45, loss = 0.05738760\n",
      "Iteration 29, loss = 0.09751947\n",
      "Iteration 46, loss = 0.09521211\n",
      "Iteration 30, loss = 0.08858581\n",
      "Iteration 31, loss = 0.08897154\n",
      "Iteration 47, loss = 0.07024178\n",
      "Iteration 48, loss = 0.07928554\n",
      "Iteration 32, loss = 0.10574132\n",
      "Iteration 49, loss = 0.09285878\n",
      "Iteration 33, loss = 0.09108235\n",
      "Iteration 50, loss = 0.09199488\n",
      "Iteration 51, loss = 0.13930665\n",
      "Iteration 52, loss = 0.11142134\n",
      "Iteration 34, loss = 0.09049204\n",
      "Iteration 53, loss = 0.08189732\n",
      "Iteration 54, loss = 0.06485568\n",
      "Iteration 35, loss = 0.11393242\n",
      "Iteration 36, loss = 0.15276294\n",
      "Iteration 37, loss = 0.12677220\n",
      "Iteration 38, loss = 0.10838252\n",
      "Iteration 55, loss = 0.05637642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.12619236\n",
      "Iteration 40, loss = 0.09418800\n",
      "Iteration 41, loss = 0.07706308\n",
      "Iteration 42, loss = 0.08553440\n",
      "Iteration 43, loss = 0.06771220\n",
      "Iteration 1, loss = 1.07351449\n",
      "Iteration 2, loss = 0.55531847\n",
      "Iteration 44, loss = 0.07951626\n",
      "Iteration 3, loss = 0.44041600\n",
      "Iteration 4, loss = 0.38996746\n",
      "Iteration 45, loss = 0.07942412\n",
      "Iteration 46, loss = 0.06989348\n",
      "Iteration 5, loss = 0.38913359\n",
      "Iteration 47, loss = 0.07429399\n",
      "Iteration 48, loss = 0.06026003\n",
      "Iteration 6, loss = 0.33963734\n",
      "Iteration 7, loss = 0.30917913\n",
      "Iteration 49, loss = 0.05994602\n",
      "Iteration 8, loss = 0.26848076\n",
      "Iteration 9, loss = 0.25236006\n",
      "Iteration 50, loss = 0.06609095\n",
      "Iteration 10, loss = 0.23299017\n",
      "Iteration 11, loss = 0.22358378\n",
      "Iteration 51, loss = 0.06755121\n",
      "Iteration 12, loss = 0.20745586\n",
      "Iteration 52, loss = 0.05859872\n",
      "Iteration 13, loss = 0.19975637\n",
      "Iteration 53, loss = 0.05416275\n",
      "Iteration 14, loss = 0.19395537\n",
      "Iteration 54, loss = 0.04961192\n",
      "Iteration 15, loss = 0.18453606\n",
      "Iteration 55, loss = 0.06682693\n",
      "Iteration 56, loss = 0.15097252\n",
      "Iteration 57, loss = 0.16711387\n",
      "Iteration 16, loss = 0.17947648\n",
      "Iteration 58, loss = 0.14052602\n",
      "Iteration 59, loss = 0.10387138\n",
      "Iteration 17, loss = 0.17106804\n",
      "Iteration 60, loss = 0.09540238\n",
      "Iteration 18, loss = 0.19054170\n",
      "Iteration 61, loss = 0.10074429\n",
      "Iteration 19, loss = 0.19797946\n",
      "Iteration 62, loss = 0.16404029\n",
      "Iteration 20, loss = 0.21211018\n",
      "Iteration 63, loss = 0.13389612\n",
      "Iteration 21, loss = 0.22608354\n",
      "Iteration 64, loss = 0.13371551\n",
      "Iteration 65, loss = 0.07911189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.18865728\n",
      "Iteration 23, loss = 0.19559253\n",
      "Iteration 24, loss = 0.18530146\n",
      "Iteration 25, loss = 0.18272686\n",
      "Iteration 26, loss = 0.16930283\n",
      "Iteration 27, loss = 0.15028843\n",
      "Iteration 28, loss = 0.13989774\n",
      "Iteration 29, loss = 0.13442434\n",
      "Iteration 30, loss = 0.12095506\n",
      "Iteration 31, loss = 0.11631202\n",
      "Iteration 32, loss = 0.11482173\n",
      "Iteration 33, loss = 0.11999297\n",
      "Iteration 34, loss = 0.11659001\n",
      "Iteration 35, loss = 0.10694480\n",
      "Iteration 1, loss = 1.08836366\n",
      "Iteration 36, loss = 0.11143238\n",
      "Iteration 2, loss = 0.60843725\n",
      "Iteration 3, loss = 0.45736766\n",
      "Iteration 4, loss = 0.40051537\n",
      "Iteration 5, loss = 0.36280973\n",
      "Iteration 37, loss = 0.10465664\n",
      "Iteration 6, loss = 0.30735271\n",
      "Iteration 7, loss = 0.30673601\n",
      "Iteration 38, loss = 0.10030100\n",
      "Iteration 8, loss = 0.29421001\n",
      "Iteration 9, loss = 0.29609469\n",
      "Iteration 39, loss = 0.09574104\n",
      "Iteration 10, loss = 0.27463774\n",
      "Iteration 11, loss = 0.23913400\n",
      "Iteration 12, loss = 0.22392262\n",
      "Iteration 40, loss = 0.09798326\n",
      "Iteration 41, loss = 0.08854968\n",
      "Iteration 13, loss = 0.23463213\n",
      "Iteration 42, loss = 0.09427436\n",
      "Iteration 43, loss = 0.10186331\n",
      "Iteration 44, loss = 0.11013747\n",
      "Iteration 14, loss = 0.19844548\n",
      "Iteration 15, loss = 0.17818279\n",
      "Iteration 45, loss = 0.10892183\n",
      "Iteration 16, loss = 0.18664540\n",
      "Iteration 17, loss = 0.17188437\n",
      "Iteration 46, loss = 0.13986703\n",
      "Iteration 47, loss = 0.12721939\n",
      "Iteration 18, loss = 0.17123282\n",
      "Iteration 48, loss = 0.11441160\n",
      "Iteration 49, loss = 0.11303501\n",
      "Iteration 19, loss = 0.15526315\n",
      "Iteration 50, loss = 0.12493024\n",
      "Iteration 20, loss = 0.14499375\n",
      "Iteration 51, loss = 0.14568848\n",
      "Iteration 21, loss = 0.14684771\n",
      "Iteration 52, loss = 0.10530086\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.13496035\n",
      "Iteration 23, loss = 0.12968435\n",
      "Iteration 24, loss = 0.15523590\n",
      "Iteration 25, loss = 0.16577150\n",
      "Iteration 26, loss = 0.21547689\n",
      "Iteration 27, loss = 0.18625089\n",
      "Iteration 28, loss = 0.19630515\n",
      "Iteration 1, loss = 0.94708197\n",
      "Iteration 29, loss = 0.20867318\n",
      "Iteration 2, loss = 0.53029034\n",
      "Iteration 30, loss = 0.21035233\n",
      "Iteration 3, loss = 0.41423061\n",
      "Iteration 31, loss = 0.17454503\n",
      "Iteration 4, loss = 0.34384838\n",
      "Iteration 32, loss = 0.16172612\n",
      "Iteration 33, loss = 0.14677590\n",
      "Iteration 5, loss = 0.32179434\n",
      "Iteration 34, loss = 0.10767945\n",
      "Iteration 6, loss = 0.28008124\n",
      "Iteration 7, loss = 0.26306880\n",
      "Iteration 35, loss = 0.10891701\n",
      "Iteration 8, loss = 0.25066552\n",
      "Iteration 9, loss = 0.25006942\n",
      "Iteration 36, loss = 0.10048662\n",
      "Iteration 37, loss = 0.11000686\n",
      "Iteration 38, loss = 0.08938408\n",
      "Iteration 39, loss = 0.08807907\n",
      "Iteration 10, loss = 0.25105705\n",
      "Iteration 40, loss = 0.08480184\n",
      "Iteration 11, loss = 0.21590583\n",
      "Iteration 12, loss = 0.20626268\n",
      "Iteration 13, loss = 0.18951772\n",
      "Iteration 41, loss = 0.10053831\n",
      "Iteration 42, loss = 0.08608016\n",
      "Iteration 43, loss = 0.08215409\n",
      "Iteration 14, loss = 0.17921115\n",
      "Iteration 44, loss = 0.08801258\n",
      "Iteration 15, loss = 0.18623115\n",
      "Iteration 16, loss = 0.17298025\n",
      "Iteration 45, loss = 0.08686518\n",
      "Iteration 17, loss = 0.15094032\n",
      "Iteration 18, loss = 0.14551066\n",
      "Iteration 19, loss = 0.14207551\n",
      "Iteration 46, loss = 0.10203666Iteration 20, loss = 0.13987223\n",
      "\n",
      "Iteration 21, loss = 0.13637183\n",
      "Iteration 22, loss = 0.15971203\n",
      "Iteration 47, loss = 0.11083481\n",
      "Iteration 23, loss = 0.13394154\n",
      "Iteration 24, loss = 0.12273166\n",
      "Iteration 48, loss = 0.12054487\n",
      "Iteration 49, loss = 0.10000967\n",
      "Iteration 50, loss = 0.12715877\n",
      "Iteration 25, loss = 0.12475969\n",
      "Iteration 51, loss = 0.12505667\n",
      "Iteration 52, loss = 0.15668079\n",
      "Iteration 53, loss = 0.14238399\n",
      "Iteration 26, loss = 0.11243944\n",
      "Iteration 27, loss = 0.10466592\n",
      "Iteration 54, loss = 0.09825229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.10986500\n",
      "Iteration 29, loss = 0.11139965\n",
      "Iteration 30, loss = 0.09836419\n",
      "Iteration 31, loss = 0.09905199\n",
      "Iteration 32, loss = 0.10361049\n",
      "Iteration 33, loss = 0.09869329\n",
      "Iteration 34, loss = 0.10203427\n",
      "Iteration 1, loss = 1.25104959\n",
      "Iteration 35, loss = 0.12588061\n",
      "Iteration 2, loss = 0.57500067\n",
      "Iteration 36, loss = 0.15697045\n",
      "Iteration 3, loss = 0.45977217\n",
      "Iteration 37, loss = 0.20297515\n",
      "Iteration 38, loss = 0.17655038\n",
      "Iteration 39, loss = 0.12449662\n",
      "Iteration 4, loss = 0.36033100\n",
      "Iteration 5, loss = 0.32948294\n",
      "Iteration 40, loss = 0.14703703\n",
      "Iteration 6, loss = 0.28796112\n",
      "Iteration 41, loss = 0.08825683\n",
      "Iteration 7, loss = 0.26480555\n",
      "Iteration 42, loss = 0.08647602\n",
      "Iteration 43, loss = 0.09128222\n",
      "Iteration 8, loss = 0.25622560\n",
      "Iteration 44, loss = 0.07749363\n",
      "Iteration 9, loss = 0.26636032\n",
      "Iteration 45, loss = 0.08517799\n",
      "Iteration 10, loss = 0.23702491\n",
      "Iteration 46, loss = 0.09242858\n",
      "Iteration 11, loss = 0.22483055\n",
      "Iteration 47, loss = 0.06627951\n",
      "Iteration 12, loss = 0.20936410\n",
      "Iteration 48, loss = 0.07214851\n",
      "Iteration 13, loss = 0.19593629\n",
      "Iteration 49, loss = 0.06406792\n",
      "Iteration 50, loss = 0.10744145\n",
      "Iteration 14, loss = 0.18966339\n",
      "Iteration 51, loss = 0.11521432\n",
      "Iteration 52, loss = 0.10637085\n",
      "Iteration 15, loss = 0.18455029\n",
      "Iteration 53, loss = 0.09179737\n",
      "Iteration 16, loss = 0.18313521\n",
      "Iteration 54, loss = 0.11214511\n",
      "Iteration 17, loss = 0.18576793\n",
      "Iteration 55, loss = 0.12089911\n",
      "Iteration 18, loss = 0.15460385\n",
      "Iteration 56, loss = 0.09488426\n",
      "Iteration 19, loss = 0.14503961\n",
      "Iteration 20, loss = 0.15337927\n",
      "Iteration 57, loss = 0.12712177\n",
      "Iteration 58, loss = 0.08973814\n",
      "Iteration 21, loss = 0.13094602\n",
      "Iteration 59, loss = 0.12912948\n",
      "Iteration 60, loss = 0.14227979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.15894745\n",
      "Iteration 23, loss = 0.15681165\n",
      "Iteration 24, loss = 0.13607381\n",
      "Iteration 25, loss = 0.15900103\n",
      "Iteration 26, loss = 0.11242556\n",
      "Iteration 27, loss = 0.11045444\n",
      "Iteration 28, loss = 0.10544735\n",
      "Iteration 29, loss = 0.09163083\n",
      "Iteration 30, loss = 0.09890934\n",
      "Iteration 31, loss = 0.12177041\n",
      "Iteration 1, loss = 1.21292453\n",
      "Iteration 32, loss = 0.10303057\n",
      "Iteration 2, loss = 0.59378706\n",
      "Iteration 33, loss = 0.09685664\n",
      "Iteration 34, loss = 0.10620492\n",
      "Iteration 3, loss = 0.44506217\n",
      "Iteration 4, loss = 0.39546610\n",
      "Iteration 35, loss = 0.09365294\n",
      "Iteration 5, loss = 0.33555584\n",
      "Iteration 36, loss = 0.11419371\n",
      "Iteration 6, loss = 0.30765712\n",
      "Iteration 37, loss = 0.08371549\n",
      "Iteration 38, loss = 0.08099617\n",
      "Iteration 7, loss = 0.28649557\n",
      "Iteration 39, loss = 0.11140478\n",
      "Iteration 8, loss = 0.26496422\n",
      "Iteration 40, loss = 0.10489230\n",
      "Iteration 9, loss = 0.25093377\n",
      "Iteration 10, loss = 0.24147327\n",
      "Iteration 11, loss = 0.24485737\n",
      "Iteration 41, loss = 0.11009200\n",
      "Iteration 12, loss = 0.22759923\n",
      "Iteration 42, loss = 0.12286450\n",
      "Iteration 43, loss = 0.13554486\n",
      "Iteration 44, loss = 0.14336011\n",
      "Iteration 13, loss = 0.21555579\n",
      "Iteration 14, loss = 0.19629908\n",
      "Iteration 45, loss = 0.17218313\n",
      "Iteration 15, loss = 0.19043739\n",
      "Iteration 46, loss = 0.17208489\n",
      "Iteration 16, loss = 0.18586334\n",
      "Iteration 47, loss = 0.16020462\n",
      "Iteration 48, loss = 0.10594032\n",
      "Iteration 17, loss = 0.18622805\n",
      "Iteration 49, loss = 0.09238557\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.17940849\n",
      "Iteration 19, loss = 0.18113958\n",
      "Iteration 20, loss = 0.18650397\n",
      "Iteration 21, loss = 0.15949992\n",
      "Iteration 22, loss = 0.15723529\n",
      "Iteration 23, loss = 0.15149839\n",
      "Iteration 24, loss = 0.16361113\n",
      "Iteration 25, loss = 0.15673086\n",
      "Iteration 26, loss = 0.15188878\n",
      "Iteration 1, loss = 0.88268592\n",
      "Iteration 27, loss = 0.13602881\n",
      "Iteration 2, loss = 0.45456723\n",
      "Iteration 28, loss = 0.14926909\n",
      "Iteration 3, loss = 0.41101095\n",
      "Iteration 29, loss = 0.15510383\n",
      "Iteration 4, loss = 0.33405393\n",
      "Iteration 5, loss = 0.32165143\n",
      "Iteration 30, loss = 0.15362713\n",
      "Iteration 31, loss = 0.17459396\n",
      "Iteration 32, loss = 0.18193585\n",
      "Iteration 33, loss = 0.18529488\n",
      "Iteration 6, loss = 0.29898893\n",
      "Iteration 34, loss = 0.15875282\n",
      "Iteration 7, loss = 0.26329897\n",
      "Iteration 8, loss = 0.24897903\n",
      "Iteration 35, loss = 0.12091626\n",
      "Iteration 9, loss = 0.23141095\n",
      "Iteration 36, loss = 0.10414548\n",
      "Iteration 37, loss = 0.09942716\n",
      "Iteration 10, loss = 0.21713096\n",
      "Iteration 38, loss = 0.08916362\n",
      "Iteration 11, loss = 0.21452510\n",
      "Iteration 12, loss = 0.21036752\n",
      "Iteration 39, loss = 0.09170827\n",
      "Iteration 13, loss = 0.19038808\n",
      "Iteration 40, loss = 0.10213124\n",
      "Iteration 41, loss = 0.10250264\n",
      "Iteration 14, loss = 0.18774146\n",
      "Iteration 42, loss = 0.08472946\n",
      "Iteration 15, loss = 0.17101995\n",
      "Iteration 43, loss = 0.08032917\n",
      "Iteration 16, loss = 0.16043992\n",
      "Iteration 44, loss = 0.07287028\n",
      "Iteration 17, loss = 0.17860551\n",
      "Iteration 18, loss = 0.16776212\n",
      "Iteration 45, loss = 0.07268199\n",
      "Iteration 19, loss = 0.17895826\n",
      "Iteration 46, loss = 0.07986780\n",
      "Iteration 20, loss = 0.16409850\n",
      "Iteration 47, loss = 0.13271218\n",
      "Iteration 21, loss = 0.15499789\n",
      "Iteration 48, loss = 0.12832424\n",
      "Iteration 49, loss = 0.11050382\n",
      "Iteration 22, loss = 0.13798273\n",
      "Iteration 50, loss = 0.08397396\n",
      "Iteration 51, loss = 0.10682693\n",
      "Iteration 23, loss = 0.13579093\n",
      "Iteration 52, loss = 0.13653736\n",
      "Iteration 24, loss = 0.13864773\n",
      "Iteration 25, loss = 0.19167483\n",
      "Iteration 53, loss = 0.13674252\n",
      "Iteration 54, loss = 0.17065183\n",
      "Iteration 55, loss = 0.15194751\n",
      "Iteration 56, loss = 0.16650812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.20790998\n",
      "Iteration 27, loss = 0.20852305\n",
      "Iteration 28, loss = 0.17374531\n",
      "Iteration 29, loss = 0.18466874\n",
      "Iteration 30, loss = 0.16617440\n",
      "Iteration 31, loss = 0.12563024\n",
      "Iteration 32, loss = 0.12960394\n",
      "Iteration 33, loss = 0.11350162\n",
      "Iteration 34, loss = 0.10704778\n",
      "Iteration 1, loss = 0.90948469\n",
      "Iteration 35, loss = 0.11174427\n",
      "Iteration 2, loss = 0.50898935\n",
      "Iteration 36, loss = 0.13886489\n",
      "Iteration 37, loss = 0.12987339\n",
      "Iteration 3, loss = 0.39987397\n",
      "Iteration 38, loss = 0.11453732\n",
      "Iteration 4, loss = 0.36337905\n",
      "Iteration 39, loss = 0.10279700\n",
      "Iteration 5, loss = 0.31691642\n",
      "Iteration 6, loss = 0.28834114\n",
      "Iteration 40, loss = 0.09787689\n",
      "Iteration 41, loss = 0.08313507\n",
      "Iteration 7, loss = 0.27756387\n",
      "Iteration 42, loss = 0.08493161\n",
      "Iteration 8, loss = 0.25624137\n",
      "Iteration 43, loss = 0.07945057\n",
      "Iteration 9, loss = 0.25161776\n",
      "Iteration 10, loss = 0.23779842\n",
      "Iteration 44, loss = 0.09397651\n",
      "Iteration 11, loss = 0.21896290\n",
      "Iteration 45, loss = 0.10350854\n",
      "Iteration 12, loss = 0.19172766\n",
      "Iteration 46, loss = 0.08721849\n",
      "Iteration 13, loss = 0.18360405\n",
      "Iteration 47, loss = 0.08819158\n",
      "Iteration 14, loss = 0.19064459\n",
      "Iteration 15, loss = 0.17218343\n",
      "Iteration 48, loss = 0.10274182\n",
      "Iteration 49, loss = 0.11347156\n",
      "Iteration 16, loss = 0.17025966\n",
      "Iteration 50, loss = 0.07736211\n",
      "Iteration 17, loss = 0.14889086\n",
      "Iteration 51, loss = 0.08407046\n",
      "Iteration 18, loss = 0.14929232\n",
      "Iteration 52, loss = 0.08229992\n",
      "Iteration 19, loss = 0.13625803\n",
      "Iteration 53, loss = 0.08705807\n",
      "Iteration 20, loss = 0.14346372\n",
      "Iteration 54, loss = 0.08427949\n",
      "Iteration 21, loss = 0.14012160\n",
      "Iteration 55, loss = 0.12581524\n",
      "Iteration 22, loss = 0.15438838\n",
      "Iteration 56, loss = 0.16681251\n",
      "Iteration 23, loss = 0.14057550\n",
      "Iteration 57, loss = 0.25965695\n",
      "Iteration 58, loss = 0.25064130\n",
      "Iteration 59, loss = 0.26942705\n",
      "Iteration 24, loss = 0.16845502\n",
      "Iteration 60, loss = 0.21989241\n",
      "Iteration 61, loss = 0.17622322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.12723223\n",
      "Iteration 26, loss = 0.11824199\n",
      "Iteration 27, loss = 0.13231741\n",
      "Iteration 28, loss = 0.10214587\n",
      "Iteration 29, loss = 0.10290065\n",
      "Iteration 30, loss = 0.09838098\n",
      "Iteration 31, loss = 0.10273158\n",
      "Iteration 32, loss = 0.11762302\n",
      "Iteration 33, loss = 0.12945328\n",
      "Iteration 1, loss = 0.94789941\n",
      "Iteration 2, loss = 0.46321521\n",
      "Iteration 34, loss = 0.10307658\n",
      "Iteration 3, loss = 0.40523170\n",
      "Iteration 4, loss = 0.34313465\n",
      "Iteration 35, loss = 0.13309552\n",
      "Iteration 5, loss = 0.31746097\n",
      "Iteration 6, loss = 0.29645601\n",
      "Iteration 7, loss = 0.28118331\n",
      "Iteration 36, loss = 0.14035711\n",
      "Iteration 8, loss = 0.27738432\n",
      "Iteration 9, loss = 0.26544393\n",
      "Iteration 10, loss = 0.25346672\n",
      "Iteration 37, loss = 0.11770065\n",
      "Iteration 11, loss = 0.23917938\n",
      "Iteration 38, loss = 0.11532141\n",
      "Iteration 39, loss = 0.13059177\n",
      "Iteration 40, loss = 0.10057366\n",
      "Iteration 41, loss = 0.08849940\n",
      "Iteration 12, loss = 0.21837696\n",
      "Iteration 42, loss = 0.09216088\n",
      "Iteration 13, loss = 0.20444794\n",
      "Iteration 14, loss = 0.21899211\n",
      "Iteration 15, loss = 0.20833792\n",
      "Iteration 43, loss = 0.11285352\n",
      "Iteration 44, loss = 0.10105128\n",
      "Iteration 16, loss = 0.20786136\n",
      "Iteration 45, loss = 0.11883560\n",
      "Iteration 46, loss = 0.16448808\n",
      "Iteration 17, loss = 0.19330282\n",
      "Iteration 18, loss = 0.21414494\n",
      "Iteration 47, loss = 0.13388084\n",
      "Iteration 19, loss = 0.20597043\n",
      "Iteration 20, loss = 0.20489261\n",
      "Iteration 48, loss = 0.12866145\n",
      "Iteration 49, loss = 0.09981780\n",
      "Iteration 50, loss = 0.11273971\n",
      "Iteration 21, loss = 0.17825768\n",
      "Iteration 51, loss = 0.07048107\n",
      "Iteration 22, loss = 0.17602309\n",
      "Iteration 23, loss = 0.17351737\n",
      "Iteration 24, loss = 0.13418869\n",
      "Iteration 52, loss = 0.08069271\n",
      "Iteration 53, loss = 0.08603652\n",
      "Iteration 54, loss = 0.07072334\n",
      "Iteration 55, loss = 0.08781142\n",
      "Iteration 25, loss = 0.14513758\n",
      "Iteration 26, loss = 0.12988196\n",
      "Iteration 27, loss = 0.12645587\n",
      "Iteration 28, loss = 0.11760321\n",
      "Iteration 56, loss = 0.09662816\n",
      "Iteration 57, loss = 0.09873749\n",
      "Iteration 58, loss = 0.11730639\n",
      "Iteration 59, loss = 0.10032268\n",
      "Iteration 29, loss = 0.11318775\n",
      "Iteration 30, loss = 0.12230951\n",
      "Iteration 31, loss = 0.12354619\n",
      "Iteration 32, loss = 0.10737497\n",
      "Iteration 60, loss = 0.08348482\n",
      "Iteration 33, loss = 0.09823361\n",
      "Iteration 61, loss = 0.07795705\n",
      "Iteration 62, loss = 0.06798665\n",
      "Iteration 63, loss = 0.06782118\n",
      "Iteration 64, loss = 0.06120494\n",
      "Iteration 34, loss = 0.09215244\n",
      "Iteration 65, loss = 0.05208101\n",
      "Iteration 66, loss = 0.05724397\n",
      "Iteration 35, loss = 0.11933729\n",
      "Iteration 67, loss = 0.04974642\n",
      "Iteration 68, loss = 0.05214241\n",
      "Iteration 69, loss = 0.05728299\n",
      "Iteration 70, loss = 0.05879032\n",
      "Iteration 36, loss = 0.11514669\n",
      "Iteration 71, loss = 0.10306561\n",
      "Iteration 72, loss = 0.11661774\n",
      "Iteration 73, loss = 0.11981264\n",
      "Iteration 74, loss = 0.20345192\n",
      "Iteration 37, loss = 0.10619509\n",
      "Iteration 38, loss = 0.10358905\n",
      "Iteration 75, loss = 0.13696321\n",
      "Iteration 39, loss = 0.10158506\n",
      "Iteration 76, loss = 0.13536298\n",
      "Iteration 40, loss = 0.10130521\n",
      "Iteration 77, loss = 0.08727834\n",
      "Iteration 78, loss = 0.09736583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.09708185\n",
      "Iteration 42, loss = 0.11980236\n",
      "Iteration 43, loss = 0.12798918\n",
      "Iteration 44, loss = 0.13671226\n",
      "Iteration 45, loss = 0.15707571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01521365\n",
      "Iteration 2, loss = 0.43609306\n",
      "Iteration 3, loss = 0.36800646\n",
      "Iteration 4, loss = 0.30692804\n",
      "Iteration 5, loss = 0.30085361\n",
      "Iteration 6, loss = 0.27792710\n",
      "Iteration 7, loss = 0.26604096\n",
      "Iteration 1, loss = 0.84568519\n",
      "Iteration 8, loss = 0.24434151\n",
      "Iteration 2, loss = 0.46184683\n",
      "Iteration 9, loss = 0.25204778\n",
      "Iteration 10, loss = 0.24805968\n",
      "Iteration 11, loss = 0.21659650\n",
      "Iteration 3, loss = 0.39274629\n",
      "Iteration 12, loss = 0.20285095\n",
      "Iteration 4, loss = 0.33573520\n",
      "Iteration 5, loss = 0.29113569\n",
      "Iteration 13, loss = 0.18098040\n",
      "Iteration 14, loss = 0.16110225\n",
      "Iteration 6, loss = 0.27333553\n",
      "Iteration 15, loss = 0.14846523\n",
      "Iteration 16, loss = 0.15708603\n",
      "Iteration 7, loss = 0.23199152\n",
      "Iteration 8, loss = 0.21802567\n",
      "Iteration 9, loss = 0.21332204\n",
      "Iteration 17, loss = 0.16162307\n",
      "Iteration 18, loss = 0.16318381\n",
      "Iteration 10, loss = 0.21378591\n",
      "Iteration 11, loss = 0.18976690\n",
      "Iteration 12, loss = 0.17866939\n",
      "Iteration 19, loss = 0.14112966\n",
      "Iteration 13, loss = 0.16402847\n",
      "Iteration 20, loss = 0.15569396\n",
      "Iteration 14, loss = 0.15395160\n",
      "Iteration 21, loss = 0.22550569\n",
      "Iteration 22, loss = 0.15080130\n",
      "Iteration 15, loss = 0.14443921\n",
      "Iteration 23, loss = 0.14680518\n",
      "Iteration 16, loss = 0.13585692\n",
      "Iteration 24, loss = 0.13177502\n",
      "Iteration 17, loss = 0.12613531\n",
      "Iteration 25, loss = 0.12485264\n",
      "Iteration 26, loss = 0.13080559\n",
      "Iteration 27, loss = 0.12613567\n",
      "Iteration 18, loss = 0.13749240\n",
      "Iteration 19, loss = 0.12221829\n",
      "Iteration 20, loss = 0.11306724\n",
      "Iteration 21, loss = 0.12170049\n",
      "Iteration 28, loss = 0.10537620\n",
      "Iteration 29, loss = 0.10866099\n",
      "Iteration 30, loss = 0.15017668\n",
      "Iteration 22, loss = 0.11385944\n",
      "Iteration 31, loss = 0.12405424\n",
      "Iteration 32, loss = 0.10375803\n",
      "Iteration 23, loss = 0.11328726\n",
      "Iteration 24, loss = 0.09450301\n",
      "Iteration 25, loss = 0.07942493\n",
      "Iteration 33, loss = 0.10324122\n",
      "Iteration 26, loss = 0.08599406\n",
      "Iteration 34, loss = 0.10624115\n",
      "Iteration 35, loss = 0.09553757\n",
      "Iteration 27, loss = 0.08067691\n",
      "Iteration 28, loss = 0.07877047\n",
      "Iteration 36, loss = 0.10303991\n",
      "Iteration 29, loss = 0.07294861\n",
      "Iteration 30, loss = 0.06397847\n",
      "Iteration 37, loss = 0.10283082\n",
      "Iteration 38, loss = 0.10966954\n",
      "Iteration 39, loss = 0.11332974\n",
      "Iteration 31, loss = 0.12228584\n",
      "Iteration 40, loss = 0.11160847\n",
      "Iteration 32, loss = 0.09752584\n",
      "Iteration 33, loss = 0.09038848\n",
      "Iteration 34, loss = 0.07899193\n",
      "Iteration 35, loss = 0.06303779\n",
      "Iteration 41, loss = 0.11215882\n",
      "Iteration 36, loss = 0.06792177\n",
      "Iteration 42, loss = 0.12933544\n",
      "Iteration 37, loss = 0.05968938\n",
      "Iteration 38, loss = 0.06981595\n",
      "Iteration 39, loss = 0.09173681\n",
      "Iteration 40, loss = 0.06558741\n",
      "Iteration 43, loss = 0.09828639\n",
      "Iteration 41, loss = 0.08138913\n",
      "Iteration 42, loss = 0.07985046\n",
      "Iteration 44, loss = 0.12601159\n",
      "Iteration 45, loss = 0.12345783\n",
      "Iteration 43, loss = 0.06454081\n",
      "Iteration 46, loss = 0.12029763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.07933829\n",
      "Iteration 45, loss = 0.10407864\n",
      "Iteration 46, loss = 0.07792051\n",
      "Iteration 47, loss = 0.09162861\n",
      "Iteration 48, loss = 0.07993008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.19267177\n",
      "Iteration 2, loss = 0.43570416\n",
      "Iteration 3, loss = 0.39809481\n",
      "Iteration 4, loss = 0.34401763\n",
      "Iteration 5, loss = 0.30317177\n",
      "Iteration 1, loss = 1.14311898\n",
      "Iteration 2, loss = 0.77054826\n",
      "Iteration 3, loss = 0.60480063\n",
      "Iteration 6, loss = 0.28992368\n",
      "Iteration 4, loss = 0.52205462\n",
      "Iteration 7, loss = 0.25914346\n",
      "Iteration 5, loss = 0.46620391\n",
      "Iteration 6, loss = 0.42642334\n",
      "Iteration 8, loss = 0.27006927\n",
      "Iteration 7, loss = 0.40032585\n",
      "Iteration 9, loss = 0.25327871\n",
      "Iteration 10, loss = 0.25340639\n",
      "Iteration 11, loss = 0.23223246\n",
      "Iteration 8, loss = 0.37836240\n",
      "Iteration 12, loss = 0.22719275\n",
      "Iteration 13, loss = 0.20896884\n",
      "Iteration 9, loss = 0.36259980\n",
      "Iteration 14, loss = 0.20334080\n",
      "Iteration 10, loss = 0.34819521\n",
      "Iteration 15, loss = 0.18256724\n",
      "Iteration 11, loss = 0.33528668\n",
      "Iteration 12, loss = 0.32408788\n",
      "Iteration 16, loss = 0.18871028\n",
      "Iteration 17, loss = 0.15683518\n",
      "Iteration 13, loss = 0.31695813\n",
      "Iteration 18, loss = 0.14221397\n",
      "Iteration 14, loss = 0.30951682\n",
      "Iteration 19, loss = 0.13023447\n",
      "Iteration 15, loss = 0.29774676\n",
      "Iteration 16, loss = 0.29103194\n",
      "Iteration 17, loss = 0.28426658\n",
      "Iteration 20, loss = 0.12920405\n",
      "Iteration 18, loss = 0.27838163\n",
      "Iteration 21, loss = 0.11624087\n",
      "Iteration 19, loss = 0.27191506\n",
      "Iteration 22, loss = 0.12185325\n",
      "Iteration 23, loss = 0.12410653\n",
      "Iteration 20, loss = 0.26391290\n",
      "Iteration 21, loss = 0.25955156\n",
      "Iteration 22, loss = 0.25379780\n",
      "Iteration 24, loss = 0.14845152\n",
      "Iteration 23, loss = 0.24987251\n",
      "Iteration 25, loss = 0.12718448\n",
      "Iteration 26, loss = 0.14454211\n",
      "Iteration 24, loss = 0.24407476\n",
      "Iteration 27, loss = 0.12107422\n",
      "Iteration 25, loss = 0.23908974\n",
      "Iteration 28, loss = 0.11316768\n",
      "Iteration 26, loss = 0.23599141\n",
      "Iteration 29, loss = 0.12144479\n",
      "Iteration 27, loss = 0.23061234\n",
      "Iteration 28, loss = 0.22567747\n",
      "Iteration 29, loss = 0.22493092\n",
      "Iteration 30, loss = 0.11658245\n",
      "Iteration 30, loss = 0.21964530\n",
      "Iteration 31, loss = 0.14706489\n",
      "Iteration 31, loss = 0.21423782Iteration 32, loss = 0.13541848\n",
      "\n",
      "Iteration 33, loss = 0.10636666\n",
      "Iteration 34, loss = 0.10318656\n",
      "Iteration 32, loss = 0.21081405\n",
      "Iteration 35, loss = 0.10174301\n",
      "Iteration 33, loss = 0.21248102\n",
      "Iteration 34, loss = 0.20896225\n",
      "Iteration 36, loss = 0.09822277\n",
      "Iteration 35, loss = 0.20017076\n",
      "Iteration 37, loss = 0.10549017\n",
      "Iteration 36, loss = 0.19824667\n",
      "Iteration 38, loss = 0.10967779\n",
      "Iteration 37, loss = 0.19794786\n",
      "Iteration 39, loss = 0.16435387\n",
      "Iteration 38, loss = 0.19078170\n",
      "Iteration 40, loss = 0.11950951\n",
      "Iteration 39, loss = 0.19186871\n",
      "Iteration 41, loss = 0.10804828\n",
      "Iteration 40, loss = 0.18619270\n",
      "Iteration 42, loss = 0.12947564\n",
      "Iteration 43, loss = 0.11844982\n",
      "Iteration 41, loss = 0.18364502\n",
      "Iteration 44, loss = 0.10013027\n",
      "Iteration 45, loss = 0.08354211\n",
      "Iteration 42, loss = 0.18069233\n",
      "Iteration 46, loss = 0.08013882\n",
      "Iteration 43, loss = 0.17853821\n",
      "Iteration 47, loss = 0.07658441\n",
      "Iteration 48, loss = 0.06812556\n",
      "Iteration 44, loss = 0.17486112\n",
      "Iteration 49, loss = 0.07116966\n",
      "Iteration 45, loss = 0.17233754\n",
      "Iteration 50, loss = 0.08849776\n",
      "Iteration 51, loss = 0.06262455\n",
      "Iteration 46, loss = 0.16939471\n",
      "Iteration 47, loss = 0.16822883\n",
      "Iteration 48, loss = 0.16723714\n",
      "Iteration 52, loss = 0.06537249\n",
      "Iteration 49, loss = 0.16168713\n",
      "Iteration 53, loss = 0.05883650\n",
      "Iteration 54, loss = 0.06353998\n",
      "Iteration 50, loss = 0.16189155\n",
      "Iteration 55, loss = 0.05165575\n",
      "Iteration 51, loss = 0.15860506\n",
      "Iteration 56, loss = 0.08307305\n",
      "Iteration 57, loss = 0.07711138\n",
      "Iteration 58, loss = 0.13087285\n",
      "Iteration 52, loss = 0.15615108\n",
      "Iteration 53, loss = 0.15210901\n",
      "Iteration 59, loss = 0.10913159\n",
      "Iteration 60, loss = 0.09990636\n",
      "Iteration 61, loss = 0.12752335\n",
      "Iteration 62, loss = 0.15541182\n",
      "Iteration 63, loss = 0.12391369\n",
      "Iteration 64, loss = 0.15061131\n",
      "Iteration 54, loss = 0.15108581\n",
      "Iteration 65, loss = 0.18976991\n",
      "Iteration 66, loss = 0.15561291\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 0.14837074\n",
      "Iteration 56, loss = 0.14539708\n",
      "Iteration 57, loss = 0.14292609\n",
      "Iteration 58, loss = 0.14215750\n",
      "Iteration 59, loss = 0.14269654\n",
      "Iteration 60, loss = 0.13942637\n",
      "Iteration 61, loss = 0.13931058\n",
      "Iteration 1, loss = 1.18955882Iteration 62, loss = 0.13480761\n",
      "\n",
      "Iteration 63, loss = 0.13192243\n",
      "Iteration 2, loss = 0.83230253\n",
      "Iteration 64, loss = 0.13124946\n",
      "Iteration 65, loss = 0.13234411\n",
      "Iteration 66, loss = 0.12817680\n",
      "Iteration 3, loss = 0.65640737\n",
      "Iteration 67, loss = 0.12691258\n",
      "Iteration 4, loss = 0.56998447\n",
      "Iteration 68, loss = 0.12404833\n",
      "Iteration 69, loss = 0.12397466\n",
      "Iteration 70, loss = 0.12251537\n",
      "Iteration 5, loss = 0.50806739\n",
      "Iteration 71, loss = 0.12184971\n",
      "Iteration 6, loss = 0.47149635\n",
      "Iteration 72, loss = 0.12071747\n",
      "Iteration 73, loss = 0.11943910\n",
      "Iteration 74, loss = 0.11576533\n",
      "Iteration 7, loss = 0.43811242\n",
      "Iteration 8, loss = 0.41300239\n",
      "Iteration 75, loss = 0.11420266\n",
      "Iteration 9, loss = 0.39498786\n",
      "Iteration 10, loss = 0.37625727\n",
      "Iteration 76, loss = 0.11196728\n",
      "Iteration 11, loss = 0.36349144\n",
      "Iteration 77, loss = 0.11478538\n",
      "Iteration 78, loss = 0.11439044\n",
      "Iteration 12, loss = 0.35034723\n",
      "Iteration 79, loss = 0.10933242\n",
      "Iteration 13, loss = 0.34090961\n",
      "Iteration 80, loss = 0.10720656\n",
      "Iteration 14, loss = 0.33021107\n",
      "Iteration 81, loss = 0.10700546\n",
      "Iteration 82, loss = 0.10597253\n",
      "Iteration 83, loss = 0.10582242\n",
      "Iteration 15, loss = 0.32067755\n",
      "Iteration 16, loss = 0.31343949\n",
      "Iteration 17, loss = 0.30522916\n",
      "Iteration 84, loss = 0.10232048\n",
      "Iteration 85, loss = 0.10105838\n",
      "Iteration 86, loss = 0.10188296\n",
      "Iteration 87, loss = 0.10233036\n",
      "Iteration 88, loss = 0.10280045\n",
      "Iteration 89, loss = 0.11013072\n",
      "Iteration 90, loss = 0.10918122\n",
      "Iteration 91, loss = 0.10141183\n",
      "Iteration 18, loss = 0.29873435\n",
      "Iteration 92, loss = 0.09566789\n",
      "Iteration 19, loss = 0.29059187\n",
      "Iteration 20, loss = 0.28458465\n",
      "Iteration 21, loss = 0.27893072\n",
      "Iteration 93, loss = 0.09177594\n",
      "Iteration 22, loss = 0.27304013\n",
      "Iteration 23, loss = 0.26883060\n",
      "Iteration 24, loss = 0.26441506\n",
      "Iteration 94, loss = 0.09227890\n",
      "Iteration 25, loss = 0.25830455\n",
      "Iteration 26, loss = 0.25410748\n",
      "Iteration 27, loss = 0.24816369\n",
      "Iteration 28, loss = 0.24360651\n",
      "Iteration 95, loss = 0.09193593\n",
      "Iteration 29, loss = 0.23883700\n",
      "Iteration 30, loss = 0.23601367\n",
      "Iteration 96, loss = 0.09290140\n",
      "Iteration 31, loss = 0.23053622\n",
      "Iteration 97, loss = 0.09389627\n",
      "Iteration 32, loss = 0.22619296\n",
      "Iteration 33, loss = 0.22246578\n",
      "Iteration 34, loss = 0.21765022\n",
      "Iteration 98, loss = 0.09442417\n",
      "Iteration 35, loss = 0.21685717\n",
      "Iteration 36, loss = 0.21086714\n",
      "Iteration 99, loss = 0.09111800\n",
      "Iteration 37, loss = 0.20728073\n",
      "Iteration 100, loss = 0.08874583\n",
      "Iteration 38, loss = 0.20811759\n",
      "Iteration 39, loss = 0.20388345\n",
      "Iteration 40, loss = 0.19847696\n",
      "Iteration 41, loss = 0.19346303\n",
      "Iteration 42, loss = 0.19531291\n",
      "Iteration 43, loss = 0.19015298\n",
      "Iteration 44, loss = 0.18631301\n",
      "Iteration 45, loss = 0.18138329\n",
      "Iteration 1, loss = 1.29116956\n",
      "Iteration 46, loss = 0.17849157\n",
      "Iteration 2, loss = 0.84458784\n",
      "Iteration 47, loss = 0.17441181\n",
      "Iteration 3, loss = 0.64328048\n",
      "Iteration 48, loss = 0.17506503\n",
      "Iteration 4, loss = 0.55664392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.49240295\n",
      "Iteration 49, loss = 0.17138382\n",
      "Iteration 50, loss = 0.16747995\n",
      "Iteration 51, loss = 0.16472460\n",
      "Iteration 52, loss = 0.16118386\n",
      "Iteration 6, loss = 0.45092644\n",
      "Iteration 7, loss = 0.42178818\n",
      "Iteration 8, loss = 0.39756968\n",
      "Iteration 53, loss = 0.16030926\n",
      "Iteration 9, loss = 0.38105472\n",
      "Iteration 54, loss = 0.15935977\n",
      "Iteration 55, loss = 0.16019405\n",
      "Iteration 10, loss = 0.36663298\n",
      "Iteration 56, loss = 0.15111568\n",
      "Iteration 57, loss = 0.15112650\n",
      "Iteration 11, loss = 0.35327678\n",
      "Iteration 58, loss = 0.14851185\n",
      "Iteration 12, loss = 0.34202856\n",
      "Iteration 13, loss = 0.33165976\n",
      "Iteration 14, loss = 0.32211656\n",
      "Iteration 59, loss = 0.14877478\n",
      "Iteration 60, loss = 0.14224031\n",
      "Iteration 15, loss = 0.31385797\n",
      "Iteration 61, loss = 0.14170277\n",
      "Iteration 62, loss = 0.14040101\n",
      "Iteration 16, loss = 0.30596392\n",
      "Iteration 17, loss = 0.29812943\n",
      "Iteration 63, loss = 0.13766903\n",
      "Iteration 18, loss = 0.29132461\n",
      "Iteration 19, loss = 0.28443186\n",
      "Iteration 64, loss = 0.13560062\n",
      "Iteration 65, loss = 0.13347622\n",
      "Iteration 20, loss = 0.27922336\n",
      "Iteration 21, loss = 0.27268578\n",
      "Iteration 22, loss = 0.26852623\n",
      "Iteration 66, loss = 0.13286228\n",
      "Iteration 23, loss = 0.26250454\n",
      "Iteration 67, loss = 0.13037243\n",
      "Iteration 68, loss = 0.12900213\n",
      "Iteration 24, loss = 0.26044023\n",
      "Iteration 69, loss = 0.12852107\n",
      "Iteration 25, loss = 0.25106264\n",
      "Iteration 26, loss = 0.25086258\n",
      "Iteration 27, loss = 0.24612485\n",
      "Iteration 70, loss = 0.13102858\n",
      "Iteration 28, loss = 0.24069054\n",
      "Iteration 71, loss = 0.13117495\n",
      "Iteration 72, loss = 0.12917021\n",
      "Iteration 73, loss = 0.12345048\n",
      "Iteration 29, loss = 0.23682054\n",
      "Iteration 30, loss = 0.23279322\n",
      "Iteration 31, loss = 0.22931778\n",
      "Iteration 74, loss = 0.12016008\n",
      "Iteration 75, loss = 0.11893691\n",
      "Iteration 32, loss = 0.22255070\n",
      "Iteration 76, loss = 0.11695224\n",
      "Iteration 33, loss = 0.22314331\n",
      "Iteration 34, loss = 0.21615906\n",
      "Iteration 77, loss = 0.12239263\n",
      "Iteration 35, loss = 0.21517848\n",
      "Iteration 78, loss = 0.11881205\n",
      "Iteration 79, loss = 0.11890383\n",
      "Iteration 36, loss = 0.20829573\n",
      "Iteration 80, loss = 0.12256359\n",
      "Iteration 37, loss = 0.20864883\n",
      "Iteration 38, loss = 0.20343390\n",
      "Iteration 81, loss = 0.11082032\n",
      "Iteration 82, loss = 0.10772938\n",
      "Iteration 39, loss = 0.20139880Iteration 83, loss = 0.10870380\n",
      "\n",
      "Iteration 40, loss = 0.19619011\n",
      "Iteration 84, loss = 0.10939274\n",
      "Iteration 41, loss = 0.19267786\n",
      "Iteration 85, loss = 0.10538109\n",
      "Iteration 42, loss = 0.18985926\n",
      "Iteration 86, loss = 0.10711184\n",
      "Iteration 43, loss = 0.18704858\n",
      "Iteration 87, loss = 0.10566800\n",
      "Iteration 44, loss = 0.18419536\n",
      "Iteration 45, loss = 0.18211464\n",
      "Iteration 88, loss = 0.10860820\n",
      "Iteration 46, loss = 0.17888430\n",
      "Iteration 89, loss = 0.10737000\n",
      "Iteration 90, loss = 0.10377837\n",
      "Iteration 47, loss = 0.17554790\n",
      "Iteration 91, loss = 0.09802146\n",
      "Iteration 92, loss = 0.09715657\n",
      "Iteration 48, loss = 0.17439548\n",
      "Iteration 93, loss = 0.09557456\n",
      "Iteration 94, loss = 0.09728224\n",
      "Iteration 49, loss = 0.17070437\n",
      "Iteration 50, loss = 0.16876499\n",
      "Iteration 95, loss = 0.09472971\n",
      "Iteration 51, loss = 0.16561103\n",
      "Iteration 52, loss = 0.16147615\n",
      "Iteration 96, loss = 0.09169473\n",
      "Iteration 53, loss = 0.15954385\n",
      "Iteration 54, loss = 0.15819130\n",
      "Iteration 97, loss = 0.09091785\n",
      "Iteration 98, loss = 0.09213621\n",
      "Iteration 99, loss = 0.09289477\n",
      "Iteration 100, loss = 0.09001809\n",
      "Iteration 55, loss = 0.15362308\n",
      "Iteration 56, loss = 0.15375867\n",
      "Iteration 57, loss = 0.14966368\n",
      "Iteration 58, loss = 0.14831802\n",
      "Iteration 59, loss = 0.14692830\n",
      "Iteration 60, loss = 0.14349358\n",
      "Iteration 61, loss = 0.14437318\n",
      "Iteration 62, loss = 0.13987447\n",
      "Iteration 1, loss = 1.17351887\n",
      "Iteration 63, loss = 0.13942837\n",
      "Iteration 2, loss = 0.79946558\n",
      "Iteration 64, loss = 0.13738138\n",
      "Iteration 3, loss = 0.62203704\n",
      "Iteration 4, loss = 0.53818233\n",
      "Iteration 65, loss = 0.13377151\n",
      "Iteration 66, loss = 0.13266853\n",
      "Iteration 67, loss = 0.12989654\n",
      "Iteration 5, loss = 0.48059253\n",
      "Iteration 68, loss = 0.13039747\n",
      "Iteration 6, loss = 0.44551049\n",
      "Iteration 7, loss = 0.41892604\n",
      "Iteration 8, loss = 0.39520095\n",
      "Iteration 69, loss = 0.12919997\n",
      "Iteration 9, loss = 0.38000234\n",
      "Iteration 70, loss = 0.12663657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 0.12409209\n",
      "Iteration 72, loss = 0.12402479\n",
      "Iteration 10, loss = 0.36551337\n",
      "Iteration 11, loss = 0.35504160\n",
      "Iteration 73, loss = 0.12085037\n",
      "Iteration 12, loss = 0.34465539\n",
      "Iteration 13, loss = 0.33503829\n",
      "Iteration 14, loss = 0.32547442\n",
      "Iteration 74, loss = 0.12152874\n",
      "Iteration 75, loss = 0.12051545\n",
      "Iteration 76, loss = 0.12125498\n",
      "Iteration 77, loss = 0.11460693\n",
      "Iteration 15, loss = 0.31664697\n",
      "Iteration 78, loss = 0.11756887\n",
      "Iteration 16, loss = 0.31096041\n",
      "Iteration 79, loss = 0.11378265\n",
      "Iteration 80, loss = 0.11312791\n",
      "Iteration 17, loss = 0.30438334\n",
      "Iteration 81, loss = 0.11224480\n",
      "Iteration 82, loss = 0.10634118\n",
      "Iteration 18, loss = 0.29914098\n",
      "Iteration 83, loss = 0.10696495\n",
      "Iteration 19, loss = 0.28927553\n",
      "Iteration 20, loss = 0.28725898\n",
      "Iteration 84, loss = 0.10496533\n",
      "Iteration 21, loss = 0.28033742\n",
      "Iteration 85, loss = 0.10896217\n",
      "Iteration 86, loss = 0.10595022\n",
      "Iteration 22, loss = 0.27332548\n",
      "Iteration 87, loss = 0.10505728\n",
      "Iteration 88, loss = 0.10396099\n",
      "Iteration 23, loss = 0.27093635\n",
      "Iteration 24, loss = 0.26347555\n",
      "Iteration 89, loss = 0.09790469\n",
      "Iteration 25, loss = 0.26075362\n",
      "Iteration 90, loss = 0.09849765\n",
      "Iteration 91, loss = 0.09717143\n",
      "Iteration 26, loss = 0.25756213\n",
      "Iteration 92, loss = 0.09491013\n",
      "Iteration 27, loss = 0.25124661\n",
      "Iteration 93, loss = 0.09244178\n",
      "Iteration 94, loss = 0.09293133\n",
      "Iteration 95, loss = 0.09409730\n",
      "Iteration 28, loss = 0.24741001\n",
      "Iteration 29, loss = 0.24357751\n",
      "Iteration 96, loss = 0.09034789\n",
      "Iteration 30, loss = 0.23900766\n",
      "Iteration 97, loss = 0.08762303\n",
      "Iteration 98, loss = 0.08842546\n",
      "Iteration 31, loss = 0.23421060\n",
      "Iteration 99, loss = 0.08703590\n",
      "Iteration 32, loss = 0.23220448\n",
      "Iteration 100, loss = 0.08852688\n",
      "Iteration 33, loss = 0.22789437\n",
      "Iteration 34, loss = 0.22394505\n",
      "Iteration 35, loss = 0.22144020\n",
      "Iteration 36, loss = 0.21842278\n",
      "Iteration 37, loss = 0.21626902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.21836864\n",
      "Iteration 39, loss = 0.20876012\n",
      "Iteration 40, loss = 0.20683140\n",
      "Iteration 1, loss = 1.43763994\n",
      "Iteration 2, loss = 0.93156246\n",
      "Iteration 3, loss = 0.70869357\n",
      "Iteration 41, loss = 0.20408203\n",
      "Iteration 42, loss = 0.19895161\n",
      "Iteration 43, loss = 0.19773330\n",
      "Iteration 4, loss = 0.60308253\n",
      "Iteration 44, loss = 0.19661869\n",
      "Iteration 5, loss = 0.53811276\n",
      "Iteration 45, loss = 0.19025376\n",
      "Iteration 6, loss = 0.49359727\n",
      "Iteration 7, loss = 0.46167552\n",
      "Iteration 46, loss = 0.19082180\n",
      "Iteration 8, loss = 0.43562895\n",
      "Iteration 47, loss = 0.19257401\n",
      "Iteration 9, loss = 0.41356821\n",
      "Iteration 48, loss = 0.18659085\n",
      "Iteration 10, loss = 0.39570542\n",
      "Iteration 49, loss = 0.17961022\n",
      "Iteration 11, loss = 0.38043885\n",
      "Iteration 50, loss = 0.18613374\n",
      "Iteration 12, loss = 0.36732881\n",
      "Iteration 51, loss = 0.17882832\n",
      "Iteration 52, loss = 0.17490165\n",
      "Iteration 13, loss = 0.35560748\n",
      "Iteration 53, loss = 0.17111118\n",
      "Iteration 14, loss = 0.34421150\n",
      "Iteration 54, loss = 0.16829106\n",
      "Iteration 15, loss = 0.33450675\n",
      "Iteration 16, loss = 0.32548091\n",
      "Iteration 55, loss = 0.16740782\n",
      "Iteration 56, loss = 0.16286043\n",
      "Iteration 57, loss = 0.16084872\n",
      "Iteration 17, loss = 0.31730387\n",
      "Iteration 58, loss = 0.15878982\n",
      "Iteration 18, loss = 0.30928483\n",
      "Iteration 19, loss = 0.30273889\n",
      "Iteration 20, loss = 0.29656978\n",
      "Iteration 59, loss = 0.15801019\n",
      "Iteration 60, loss = 0.15448406\n",
      "Iteration 61, loss = 0.15432914\n",
      "Iteration 21, loss = 0.29047683\n",
      "Iteration 62, loss = 0.15081751\n",
      "Iteration 22, loss = 0.28568449\n",
      "Iteration 63, loss = 0.14970606\n",
      "Iteration 23, loss = 0.27892852\n",
      "Iteration 24, loss = 0.27419622\n",
      "Iteration 64, loss = 0.15004926\n",
      "Iteration 25, loss = 0.26854758\n",
      "Iteration 65, loss = 0.14493050\n",
      "Iteration 26, loss = 0.26498972\n",
      "Iteration 66, loss = 0.14467919\n",
      "Iteration 27, loss = 0.25949134\n",
      "Iteration 67, loss = 0.14417909\n",
      "Iteration 68, loss = 0.14160092\n",
      "Iteration 69, loss = 0.14018172\n",
      "Iteration 28, loss = 0.25516737\n",
      "Iteration 29, loss = 0.25120501\n",
      "Iteration 30, loss = 0.24788955\n",
      "Iteration 70, loss = 0.13688496\n",
      "Iteration 31, loss = 0.24264529\n",
      "Iteration 71, loss = 0.13841423\n",
      "Iteration 72, loss = 0.13960696\n",
      "Iteration 32, loss = 0.24219658\n",
      "Iteration 73, loss = 0.13582519\n",
      "Iteration 33, loss = 0.23624077\n",
      "Iteration 34, loss = 0.23376994\n",
      "Iteration 35, loss = 0.22824689\n",
      "Iteration 74, loss = 0.13633476\n",
      "Iteration 36, loss = 0.22422940\n",
      "Iteration 37, loss = 0.22189217\n",
      "Iteration 75, loss = 0.13023053\n",
      "Iteration 38, loss = 0.21893162\n",
      "Iteration 76, loss = 0.12810834\n",
      "Iteration 39, loss = 0.21740137\n",
      "Iteration 77, loss = 0.12732682\n",
      "Iteration 40, loss = 0.21022539\n",
      "Iteration 78, loss = 0.12638221\n",
      "Iteration 41, loss = 0.20837795\n",
      "Iteration 42, loss = 0.20324860\n",
      "Iteration 79, loss = 0.12506994\n",
      "Iteration 80, loss = 0.12382048\n",
      "Iteration 43, loss = 0.20248136\n",
      "Iteration 81, loss = 0.11815553\n",
      "Iteration 82, loss = 0.11952433\n",
      "Iteration 83, loss = 0.11818602\n",
      "Iteration 44, loss = 0.20028008\n",
      "Iteration 84, loss = 0.11852990\n",
      "Iteration 85, loss = 0.11483029\n",
      "Iteration 86, loss = 0.11390094\n",
      "Iteration 87, loss = 0.11241428\n",
      "Iteration 45, loss = 0.19473564\n",
      "Iteration 88, loss = 0.11297384\n",
      "Iteration 46, loss = 0.19426680\n",
      "Iteration 89, loss = 0.11024509\n",
      "Iteration 47, loss = 0.19370360\n",
      "Iteration 90, loss = 0.10746418\n",
      "Iteration 48, loss = 0.18829593\n",
      "Iteration 91, loss = 0.10767416\n",
      "Iteration 49, loss = 0.18831463\n",
      "Iteration 92, loss = 0.10802583\n",
      "Iteration 93, loss = 0.10741964\n",
      "Iteration 94, loss = 0.10997752\n",
      "Iteration 50, loss = 0.18199032\n",
      "Iteration 95, loss = 0.10620698\n",
      "Iteration 96, loss = 0.10304794\n",
      "Iteration 51, loss = 0.18015981\n",
      "Iteration 97, loss = 0.10299435\n",
      "Iteration 52, loss = 0.18094083\n",
      "Iteration 98, loss = 0.10306574\n",
      "Iteration 99, loss = 0.10078982\n",
      "Iteration 53, loss = 0.17458702\n",
      "Iteration 100, loss = 0.09973639\n",
      "Iteration 54, loss = 0.17360761\n",
      "Iteration 55, loss = 0.16757204\n",
      "Iteration 56, loss = 0.16899142\n",
      "Iteration 57, loss = 0.16229716\n",
      "Iteration 58, loss = 0.16245791\n",
      "Iteration 59, loss = 0.16093135\n",
      "Iteration 60, loss = 0.15723344\n",
      "Iteration 61, loss = 0.15748252\n",
      "Iteration 1, loss = 0.94923276\n",
      "Iteration 2, loss = 0.68069714\n",
      "Iteration 62, loss = 0.15374339\n",
      "Iteration 3, loss = 0.55477210\n",
      "Iteration 4, loss = 0.48914248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.44663496\n",
      "Iteration 63, loss = 0.14860585\n",
      "Iteration 6, loss = 0.41611691\n",
      "Iteration 64, loss = 0.15163091\n",
      "Iteration 7, loss = 0.39528686\n",
      "Iteration 65, loss = 0.14693676\n",
      "Iteration 8, loss = 0.37837928\n",
      "Iteration 9, loss = 0.36305653\n",
      "Iteration 10, loss = 0.34781669\n",
      "Iteration 11, loss = 0.33712468\n",
      "Iteration 66, loss = 0.14429619\n",
      "Iteration 12, loss = 0.32631746\n",
      "Iteration 67, loss = 0.14392433\n",
      "Iteration 13, loss = 0.31702052\n",
      "Iteration 68, loss = 0.14196441\n",
      "Iteration 14, loss = 0.31062981\n",
      "Iteration 15, loss = 0.30031294\n",
      "Iteration 16, loss = 0.29255494\n",
      "Iteration 69, loss = 0.13881165\n",
      "Iteration 17, loss = 0.28441372\n",
      "Iteration 70, loss = 0.13981459\n",
      "Iteration 18, loss = 0.27763706\n",
      "Iteration 19, loss = 0.27198787\n",
      "Iteration 20, loss = 0.26645518\n",
      "Iteration 21, loss = 0.26010157\n",
      "Iteration 71, loss = 0.14291707\n",
      "Iteration 72, loss = 0.14240195\n",
      "Iteration 22, loss = 0.25484774\n",
      "Iteration 73, loss = 0.13937545\n",
      "Iteration 74, loss = 0.13399051\n",
      "Iteration 23, loss = 0.25018994\n",
      "Iteration 24, loss = 0.25502018\n",
      "Iteration 25, loss = 0.24513000\n",
      "Iteration 75, loss = 0.13181276\n",
      "Iteration 26, loss = 0.23715379\n",
      "Iteration 76, loss = 0.12895825\n",
      "Iteration 77, loss = 0.12976303\n",
      "Iteration 78, loss = 0.12411957\n",
      "Iteration 27, loss = 0.23381295\n",
      "Iteration 28, loss = 0.22987080\n",
      "Iteration 79, loss = 0.12445413\n",
      "Iteration 29, loss = 0.22486299\n",
      "Iteration 30, loss = 0.21840159\n",
      "Iteration 80, loss = 0.12775534\n",
      "Iteration 31, loss = 0.21664106\n",
      "Iteration 81, loss = 0.13224130\n",
      "Iteration 32, loss = 0.20999112\n",
      "Iteration 82, loss = 0.12835647\n",
      "Iteration 33, loss = 0.20661310\n",
      "Iteration 34, loss = 0.20157244\n",
      "Iteration 83, loss = 0.12070087\n",
      "Iteration 35, loss = 0.19886284\n",
      "Iteration 36, loss = 0.19695638\n",
      "Iteration 37, loss = 0.19564959\n",
      "Iteration 38, loss = 0.18986639\n",
      "Iteration 39, loss = 0.18671908\n",
      "Iteration 84, loss = 0.11846771\n",
      "Iteration 40, loss = 0.18359791\n",
      "Iteration 41, loss = 0.18053263\n",
      "Iteration 85, loss = 0.12581669\n",
      "Iteration 86, loss = 0.12420959\n",
      "Iteration 42, loss = 0.17711465\n",
      "Iteration 43, loss = 0.17417264\n",
      "Iteration 44, loss = 0.17323306\n",
      "Iteration 45, loss = 0.16783653\n",
      "Iteration 46, loss = 0.16684734\n",
      "Iteration 47, loss = 0.16341552\n",
      "Iteration 48, loss = 0.16094216\n",
      "Iteration 49, loss = 0.15907761\n",
      "Iteration 87, loss = 0.11790866\n",
      "Iteration 88, loss = 0.11636268\n",
      "Iteration 50, loss = 0.15835240\n",
      "Iteration 89, loss = 0.11497087\n",
      "Iteration 51, loss = 0.15875059\n",
      "Iteration 52, loss = 0.15145534\n",
      "Iteration 53, loss = 0.14970876Iteration 90, loss = 0.11504102\n",
      "\n",
      "Iteration 54, loss = 0.15104791\n",
      "Iteration 91, loss = 0.10901067\n",
      "Iteration 55, loss = 0.14808609\n",
      "Iteration 56, loss = 0.14741460\n",
      "Iteration 57, loss = 0.14734466\n",
      "Iteration 58, loss = 0.14491401\n",
      "Iteration 59, loss = 0.14363531\n",
      "Iteration 60, loss = 0.13822483\n",
      "Iteration 61, loss = 0.13789546\n",
      "Iteration 92, loss = 0.10573469\n",
      "Iteration 62, loss = 0.14308840\n",
      "Iteration 63, loss = 0.14785125\n",
      "Iteration 93, loss = 0.10591834\n",
      "Iteration 94, loss = 0.10557527\n",
      "Iteration 64, loss = 0.13847165\n",
      "Iteration 95, loss = 0.10411336\n",
      "Iteration 65, loss = 0.13381183\n",
      "Iteration 66, loss = 0.13661463\n",
      "Iteration 67, loss = 0.13339313\n",
      "Iteration 96, loss = 0.10333547\n",
      "Iteration 68, loss = 0.13454745\n",
      "Iteration 69, loss = 0.12304959\n",
      "Iteration 97, loss = 0.10130423\n",
      "Iteration 70, loss = 0.12723659\n",
      "Iteration 71, loss = 0.12552203\n",
      "Iteration 98, loss = 0.10241301\n",
      "Iteration 99, loss = 0.09689539\n",
      "Iteration 72, loss = 0.12565324\n",
      "Iteration 100, loss = 0.09704750\n",
      "Iteration 73, loss = 0.11689375\n",
      "Iteration 74, loss = 0.11685677\n",
      "Iteration 75, loss = 0.11993970\n",
      "Iteration 76, loss = 0.11380790\n",
      "Iteration 77, loss = 0.11406086\n",
      "Iteration 1, loss = 1.11839459\n",
      "Iteration 78, loss = 0.11136362\n",
      "Iteration 79, loss = 0.11050227\n",
      "Iteration 80, loss = 0.10838807\n",
      "Iteration 81, loss = 0.10837874\n",
      "Iteration 2, loss = 0.78645744\n",
      "Iteration 82, loss = 0.10590355\n",
      "Iteration 3, loss = 0.62367343\n",
      "Iteration 4, loss = 0.53852406\n",
      "Iteration 5, loss = 0.48369131\n",
      "Iteration 83, loss = 0.10634092\n",
      "Iteration 84, loss = 0.10175469\n",
      "Iteration 6, loss = 0.44701297\n",
      "Iteration 85, loss = 0.10353160\n",
      "Iteration 7, loss = 0.41786670\n",
      "Iteration 86, loss = 0.10062769\n",
      "Iteration 8, loss = 0.39779085\n",
      "Iteration 9, loss = 0.38008771\n",
      "Iteration 10, loss = 0.36046621\n",
      "Iteration 87, loss = 0.09949788\n",
      "Iteration 88, loss = 0.09706367\n",
      "Iteration 89, loss = 0.09810039\n",
      "Iteration 11, loss = 0.35065147\n",
      "Iteration 12, loss = 0.33819622\n",
      "Iteration 90, loss = 0.09830343\n",
      "Iteration 13, loss = 0.32701501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.31871199\n",
      "Iteration 15, loss = 0.31256040\n",
      "Iteration 91, loss = 0.09607440\n",
      "Iteration 16, loss = 0.30481644\n",
      "Iteration 92, loss = 0.09431464\n",
      "Iteration 93, loss = 0.09334593\n",
      "Iteration 17, loss = 0.29596628\n",
      "Iteration 94, loss = 0.09133597\n",
      "Iteration 18, loss = 0.29209226\n",
      "Iteration 95, loss = 0.09322508\n",
      "Iteration 19, loss = 0.28159731\n",
      "Iteration 96, loss = 0.09187753\n",
      "Iteration 97, loss = 0.08977601\n",
      "Iteration 20, loss = 0.27891298\n",
      "Iteration 98, loss = 0.08928228\n",
      "Iteration 21, loss = 0.27220180\n",
      "Iteration 22, loss = 0.26777817\n",
      "Iteration 99, loss = 0.09333031\n",
      "Iteration 23, loss = 0.26035102\n",
      "Iteration 100, loss = 0.09329670\n",
      "Iteration 24, loss = 0.25588213\n",
      "Iteration 25, loss = 0.25119012\n",
      "Iteration 26, loss = 0.24843947\n",
      "Iteration 27, loss = 0.24149600\n",
      "Iteration 28, loss = 0.23874784\n",
      "Iteration 29, loss = 0.23497642\n",
      "Iteration 30, loss = 0.23227354\n",
      "Iteration 31, loss = 0.22578899\n",
      "Iteration 32, loss = 0.22777791\n",
      "Iteration 33, loss = 0.22547729\n",
      "Iteration 34, loss = 0.21806418\n",
      "Iteration 35, loss = 0.21491917\n",
      "Iteration 36, loss = 0.21123252\n",
      "Iteration 1, loss = 1.19953646\n",
      "Iteration 37, loss = 0.20509463\n",
      "Iteration 2, loss = 0.85754419\n",
      "Iteration 38, loss = 0.20484445\n",
      "Iteration 39, loss = 0.19854464\n",
      "Iteration 40, loss = 0.19434232\n",
      "Iteration 3, loss = 0.66289063\n",
      "Iteration 41, loss = 0.19324837\n",
      "Iteration 42, loss = 0.18854394\n",
      "Iteration 4, loss = 0.55522131\n",
      "Iteration 5, loss = 0.49209612\n",
      "Iteration 6, loss = 0.44660658\n",
      "Iteration 7, loss = 0.42216630\n",
      "Iteration 43, loss = 0.18840787\n",
      "Iteration 8, loss = 0.39505096\n",
      "Iteration 44, loss = 0.18500581\n",
      "Iteration 9, loss = 0.37520535\n",
      "Iteration 45, loss = 0.17878564\n",
      "Iteration 46, loss = 0.17775087\n",
      "Iteration 10, loss = 0.36022935\n",
      "Iteration 47, loss = 0.17403680\n",
      "Iteration 48, loss = 0.17260204\n",
      "Iteration 11, loss = 0.34492704\n",
      "Iteration 49, loss = 0.16954922\n",
      "Iteration 12, loss = 0.33305412\n",
      "Iteration 13, loss = 0.32211381\n",
      "Iteration 50, loss = 0.16606137\n",
      "Iteration 51, loss = 0.16596563\n",
      "Iteration 14, loss = 0.31144837\n",
      "Iteration 52, loss = 0.16008019\n",
      "Iteration 15, loss = 0.30255597\n",
      "Iteration 53, loss = 0.15999218\n",
      "Iteration 16, loss = 0.29448297\n",
      "Iteration 54, loss = 0.15834975\n",
      "Iteration 17, loss = 0.28600485\n",
      "Iteration 18, loss = 0.27933657\n",
      "Iteration 19, loss = 0.27131770\n",
      "Iteration 20, loss = 0.26466657\n",
      "Iteration 55, loss = 0.15436479\n",
      "Iteration 21, loss = 0.25873294\n",
      "Iteration 22, loss = 0.25271361\n",
      "Iteration 56, loss = 0.15376168\n",
      "Iteration 23, loss = 0.24563975\n",
      "Iteration 24, loss = 0.24136848\n",
      "Iteration 25, loss = 0.23703308\n",
      "Iteration 57, loss = 0.14984824\n",
      "Iteration 26, loss = 0.22940400\n",
      "Iteration 27, loss = 0.22914596\n",
      "Iteration 28, loss = 0.22307486\n",
      "Iteration 58, loss = 0.14793844\n",
      "Iteration 59, loss = 0.14477858\n",
      "Iteration 60, loss = 0.14358448\n",
      "Iteration 29, loss = 0.21954213\n",
      "Iteration 61, loss = 0.14248746\n",
      "Iteration 30, loss = 0.21331403\n",
      "Iteration 31, loss = 0.21028123\n",
      "Iteration 62, loss = 0.13986437\n",
      "Iteration 63, loss = 0.13658114\n",
      "Iteration 32, loss = 0.20557166\n",
      "Iteration 64, loss = 0.13727435\n",
      "Iteration 65, loss = 0.13679038\n",
      "Iteration 33, loss = 0.20006520\n",
      "Iteration 66, loss = 0.13053364\n",
      "Iteration 34, loss = 0.19883892\n",
      "Iteration 67, loss = 0.13118044\n",
      "Iteration 35, loss = 0.19631479\n",
      "Iteration 68, loss = 0.13746456\n",
      "Iteration 36, loss = 0.18877383\n",
      "Iteration 69, loss = 0.13302653\n",
      "Iteration 37, loss = 0.18566720\n",
      "Iteration 38, loss = 0.17954025\n",
      "Iteration 70, loss = 0.12964946\n",
      "Iteration 39, loss = 0.17684579\n",
      "Iteration 71, loss = 0.12393329\n",
      "Iteration 72, loss = 0.12510624\n",
      "Iteration 40, loss = 0.17197381\n",
      "Iteration 41, loss = 0.16993143\n",
      "Iteration 73, loss = 0.12648770\n",
      "Iteration 42, loss = 0.16816494\n",
      "Iteration 74, loss = 0.12264826\n",
      "Iteration 43, loss = 0.16377257\n",
      "Iteration 75, loss = 0.11712056\n",
      "Iteration 44, loss = 0.16190562\n",
      "Iteration 76, loss = 0.11887848\n",
      "Iteration 45, loss = 0.15878287\n",
      "Iteration 46, loss = 0.15419634\n",
      "Iteration 77, loss = 0.11696801\n",
      "Iteration 78, loss = 0.11456320\n",
      "Iteration 79, loss = 0.11089934\n",
      "Iteration 47, loss = 0.14933578\n",
      "Iteration 80, loss = 0.10938048\n",
      "Iteration 48, loss = 0.14838390\n",
      "Iteration 49, loss = 0.14448696\n",
      "Iteration 81, loss = 0.10966402\n",
      "Iteration 82, loss = 0.10729452\n",
      "Iteration 50, loss = 0.14196896\n",
      "Iteration 83, loss = 0.10622303\n",
      "Iteration 84, loss = 0.10722617\n",
      "Iteration 85, loss = 0.10566302\n",
      "Iteration 51, loss = 0.14109835\n",
      "Iteration 86, loss = 0.10063130\n",
      "Iteration 87, loss = 0.10423233\n",
      "Iteration 88, loss = 0.10049787\n",
      "Iteration 52, loss = 0.13753182\n",
      "Iteration 89, loss = 0.10459226\n",
      "Iteration 53, loss = 0.13431906\n",
      "Iteration 90, loss = 0.09887408\n",
      "Iteration 54, loss = 0.13284813\n",
      "Iteration 55, loss = 0.12940964\n",
      "Iteration 56, loss = 0.12807016\n",
      "Iteration 91, loss = 0.09647128\n",
      "Iteration 57, loss = 0.12804447\n",
      "Iteration 58, loss = 0.12350112\n",
      "Iteration 59, loss = 0.12340859\n",
      "Iteration 60, loss = 0.11783235\n",
      "Iteration 92, loss = 0.09630304\n",
      "Iteration 61, loss = 0.11727517\n",
      "Iteration 93, loss = 0.09483584\n",
      "Iteration 62, loss = 0.11644788\n",
      "Iteration 94, loss = 0.09234926\n",
      "Iteration 63, loss = 0.11309210\n",
      "Iteration 95, loss = 0.09270371\n",
      "Iteration 64, loss = 0.11078557\n",
      "Iteration 65, loss = 0.10942895\n",
      "Iteration 96, loss = 0.09372617\n",
      "Iteration 66, loss = 0.10905377\n",
      "Iteration 97, loss = 0.09122389\n",
      "Iteration 67, loss = 0.10726949\n",
      "Iteration 98, loss = 0.09263115\n",
      "Iteration 68, loss = 0.10268415\n",
      "Iteration 99, loss = 0.09144035\n",
      "Iteration 69, loss = 0.10892648\n",
      "Iteration 100, loss = 0.08903956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.10383285\n",
      "Iteration 71, loss = 0.10270920\n",
      "Iteration 72, loss = 0.10362475\n",
      "Iteration 73, loss = 0.09748743\n",
      "Iteration 74, loss = 0.09947509\n",
      "Iteration 75, loss = 0.09767228\n",
      "Iteration 76, loss = 0.09142262\n",
      "Iteration 77, loss = 0.08986019\n",
      "Iteration 78, loss = 0.08978981\n",
      "Iteration 79, loss = 0.08775041\n",
      "Iteration 1, loss = 1.13198765Iteration 80, loss = 0.08665380\n",
      "\n",
      "Iteration 81, loss = 0.08253959\n",
      "Iteration 2, loss = 0.73649795\n",
      "Iteration 3, loss = 0.58071777\n",
      "Iteration 82, loss = 0.08314211\n",
      "Iteration 4, loss = 0.50207287\n",
      "Iteration 5, loss = 0.45182288\n",
      "Iteration 6, loss = 0.41725194\n",
      "Iteration 7, loss = 0.38847178\n",
      "Iteration 83, loss = 0.08110535\n",
      "Iteration 8, loss = 0.36818944\n",
      "Iteration 84, loss = 0.08237685\n",
      "Iteration 85, loss = 0.07834896\n",
      "Iteration 86, loss = 0.07829303\n",
      "Iteration 9, loss = 0.35034822\n",
      "Iteration 10, loss = 0.33786813\n",
      "Iteration 11, loss = 0.32282847\n",
      "Iteration 87, loss = 0.07638691\n",
      "Iteration 12, loss = 0.31263873\n",
      "Iteration 13, loss = 0.30143198\n",
      "Iteration 88, loss = 0.07531590\n",
      "Iteration 89, loss = 0.07342953\n",
      "Iteration 14, loss = 0.29220902\n",
      "Iteration 90, loss = 0.07326598\n",
      "Iteration 15, loss = 0.28273200\n",
      "Iteration 91, loss = 0.07192677\n",
      "Iteration 16, loss = 0.27521560\n",
      "Iteration 92, loss = 0.07223832\n",
      "Iteration 93, loss = 0.07122239\n",
      "Iteration 17, loss = 0.26777038\n",
      "Iteration 94, loss = 0.06913683\n",
      "Iteration 95, loss = 0.06953898\n",
      "Iteration 18, loss = 0.26007300\n",
      "Iteration 96, loss = 0.06993074\n",
      "Iteration 97, loss = 0.06569115\n",
      "Iteration 98, loss = 0.06509958\n",
      "Iteration 19, loss = 0.25400894\n",
      "Iteration 99, loss = 0.06784723\n",
      "Iteration 100, loss = 0.06805593\n",
      "Iteration 20, loss = 0.24676011\n",
      "Iteration 21, loss = 0.24095459\n",
      "Iteration 22, loss = 0.23545754\n",
      "Iteration 23, loss = 0.23064618\n",
      "Iteration 24, loss = 0.22518865\n",
      "Iteration 25, loss = 0.21997308\n",
      "Iteration 26, loss = 0.21574438\n",
      "Iteration 27, loss = 0.21145482\n",
      "Iteration 28, loss = 0.21022153\n",
      "Iteration 29, loss = 0.20206851\n",
      "Iteration 1, loss = 1.03366690\n",
      "Iteration 30, loss = 0.19820407\n",
      "Iteration 31, loss = 0.19496858\n",
      "Iteration 32, loss = 0.19169346\n",
      "Iteration 33, loss = 0.18680891\n",
      "Iteration 2, loss = 0.72625266\n",
      "Iteration 34, loss = 0.18506449\n",
      "Iteration 3, loss = 0.59012766\n",
      "Iteration 35, loss = 0.18531049\n",
      "Iteration 4, loss = 0.51513624\n",
      "Iteration 5, loss = 0.46561547\n",
      "Iteration 36, loss = 0.18619405\n",
      "Iteration 37, loss = 0.17925171\n",
      "Iteration 38, loss = 0.17659700\n",
      "Iteration 6, loss = 0.43143046\n",
      "Iteration 39, loss = 0.17104019\n",
      "Iteration 7, loss = 0.40467208\n",
      "Iteration 8, loss = 0.38506259\n",
      "Iteration 40, loss = 0.16913784\n",
      "Iteration 41, loss = 0.16823562\n",
      "Iteration 42, loss = 0.15980018\n",
      "Iteration 9, loss = 0.36999432\n",
      "Iteration 43, loss = 0.15850309\n",
      "Iteration 44, loss = 0.15493838\n",
      "Iteration 45, loss = 0.15281430\n",
      "Iteration 10, loss = 0.35505683\n",
      "Iteration 46, loss = 0.15154337\n",
      "Iteration 11, loss = 0.34272939\n",
      "Iteration 47, loss = 0.15120070\n",
      "Iteration 12, loss = 0.33213978\n",
      "Iteration 13, loss = 0.32281251\n",
      "Iteration 14, loss = 0.31741070\n",
      "Iteration 48, loss = 0.14997194\n",
      "Iteration 15, loss = 0.30867760\n",
      "Iteration 49, loss = 0.15424776\n",
      "Iteration 50, loss = 0.14908543\n",
      "Iteration 51, loss = 0.13798634\n",
      "Iteration 16, loss = 0.29672135\n",
      "Iteration 52, loss = 0.14416417\n",
      "Iteration 53, loss = 0.14460038\n",
      "Iteration 54, loss = 0.14481038\n",
      "Iteration 17, loss = 0.29076055\n",
      "Iteration 18, loss = 0.28454075\n",
      "Iteration 19, loss = 0.27605344\n",
      "Iteration 55, loss = 0.13332326\n",
      "Iteration 20, loss = 0.27322458\n",
      "Iteration 21, loss = 0.26635338\n",
      "Iteration 56, loss = 0.13129852\n",
      "Iteration 57, loss = 0.13221616\n",
      "Iteration 58, loss = 0.13128386\n",
      "Iteration 22, loss = 0.26470486\n",
      "Iteration 23, loss = 0.25693811\n",
      "Iteration 59, loss = 0.12895892\n",
      "Iteration 24, loss = 0.25131621\n",
      "Iteration 60, loss = 0.12299577\n",
      "Iteration 25, loss = 0.24587112\n",
      "Iteration 61, loss = 0.12336180\n",
      "Iteration 62, loss = 0.12345876\n",
      "Iteration 26, loss = 0.24097069\n",
      "Iteration 63, loss = 0.11917549\n",
      "Iteration 64, loss = 0.12029331\n",
      "Iteration 65, loss = 0.11687515\n",
      "Iteration 27, loss = 0.23610941\n",
      "Iteration 66, loss = 0.11592072\n",
      "Iteration 28, loss = 0.22970591\n",
      "Iteration 29, loss = 0.22554481\n",
      "Iteration 30, loss = 0.22136835\n",
      "Iteration 67, loss = 0.11281831\n",
      "Iteration 68, loss = 0.11397940\n",
      "Iteration 69, loss = 0.10970738\n",
      "Iteration 31, loss = 0.21960810\n",
      "Iteration 70, loss = 0.11154150\n",
      "Iteration 71, loss = 0.10931608\n",
      "Iteration 32, loss = 0.21384630\n",
      "Iteration 72, loss = 0.10839920\n",
      "Iteration 33, loss = 0.21060112\n",
      "Iteration 73, loss = 0.10601382\n",
      "Iteration 34, loss = 0.20549630\n",
      "Iteration 74, loss = 0.10691041\n",
      "Iteration 35, loss = 0.20233454\n",
      "Iteration 75, loss = 0.10431936\n",
      "Iteration 36, loss = 0.20325616\n",
      "Iteration 37, loss = 0.19627208\n",
      "Iteration 38, loss = 0.19457897\n",
      "Iteration 76, loss = 0.10552292\n",
      "Iteration 77, loss = 0.10625327\n",
      "Iteration 39, loss = 0.18853781\n",
      "Iteration 78, loss = 0.10221598\n",
      "Iteration 40, loss = 0.18678486\n",
      "Iteration 79, loss = 0.10500834\n",
      "Iteration 41, loss = 0.18244025\n",
      "Iteration 42, loss = 0.17944058\n",
      "Iteration 43, loss = 0.17916973\n",
      "Iteration 80, loss = 0.09945612\n",
      "Iteration 81, loss = 0.10092546\n",
      "Iteration 82, loss = 0.09432960\n",
      "Iteration 44, loss = 0.17627782\n",
      "Iteration 45, loss = 0.17224707\n",
      "Iteration 83, loss = 0.09515723\n",
      "Iteration 46, loss = 0.17285442\n",
      "Iteration 47, loss = 0.16711699\n",
      "Iteration 84, loss = 0.09618430\n",
      "Iteration 85, loss = 0.09760730\n",
      "Iteration 48, loss = 0.16224463\n",
      "Iteration 86, loss = 0.09625011\n",
      "Iteration 87, loss = 0.09550368\n",
      "Iteration 88, loss = 0.09864943\n",
      "Iteration 49, loss = 0.16109956\n",
      "Iteration 89, loss = 0.09297714\n",
      "Iteration 90, loss = 0.09990049\n",
      "Iteration 91, loss = 0.08797053\n",
      "Iteration 50, loss = 0.15904431\n",
      "Iteration 92, loss = 0.08668694\n",
      "Iteration 51, loss = 0.15453698\n",
      "Iteration 93, loss = 0.08845706\n",
      "Iteration 52, loss = 0.15384033\n",
      "Iteration 94, loss = 0.08644878\n",
      "Iteration 53, loss = 0.14962768\n",
      "Iteration 95, loss = 0.08537234\n",
      "Iteration 54, loss = 0.14779453\n",
      "Iteration 96, loss = 0.08402501\n",
      "Iteration 55, loss = 0.14801358\n",
      "Iteration 97, loss = 0.08157547\n",
      "Iteration 98, loss = 0.08097416\n",
      "Iteration 56, loss = 0.14453315\n",
      "Iteration 99, loss = 0.08019090\n",
      "Iteration 100, loss = 0.08112048\n",
      "Iteration 57, loss = 0.14227297\n",
      "Iteration 58, loss = 0.13968907\n",
      "Iteration 59, loss = 0.14033015\n",
      "Iteration 60, loss = 0.13798059\n",
      "Iteration 61, loss = 0.13647644\n",
      "Iteration 62, loss = 0.13839869\n",
      "Iteration 63, loss = 0.13545104\n",
      "Iteration 1, loss = 1.16570579\n",
      "Iteration 64, loss = 0.13250915\n",
      "Iteration 2, loss = 0.79204987\n",
      "Iteration 65, loss = 0.13102942\n",
      "Iteration 66, loss = 0.12511644\n",
      "Iteration 67, loss = 0.12334608\n",
      "Iteration 3, loss = 0.61499815\n",
      "Iteration 68, loss = 0.13328927\n",
      "Iteration 4, loss = 0.51817821\n",
      "Iteration 5, loss = 0.46982817\n",
      "Iteration 6, loss = 0.42929515\n",
      "Iteration 7, loss = 0.40367177\n",
      "Iteration 69, loss = 0.13345147\n",
      "Iteration 70, loss = 0.12624917\n",
      "Iteration 71, loss = 0.12190206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72, loss = 0.12565859\n",
      "Iteration 8, loss = 0.38414899\n",
      "Iteration 73, loss = 0.12382176\n",
      "Iteration 74, loss = 0.12334728\n",
      "Iteration 75, loss = 0.11209780\n",
      "Iteration 9, loss = 0.36592639\n",
      "Iteration 10, loss = 0.35186515\n",
      "Iteration 11, loss = 0.33910048\n",
      "Iteration 76, loss = 0.11478233\n",
      "Iteration 12, loss = 0.32777256\n",
      "Iteration 77, loss = 0.11011754\n",
      "Iteration 78, loss = 0.10919606\n",
      "Iteration 79, loss = 0.10644770\n",
      "Iteration 13, loss = 0.31922747\n",
      "Iteration 14, loss = 0.30879234\n",
      "Iteration 80, loss = 0.10611323\n",
      "Iteration 15, loss = 0.30202681\n",
      "Iteration 16, loss = 0.29391924\n",
      "Iteration 81, loss = 0.10993779\n",
      "Iteration 17, loss = 0.28681121\n",
      "Iteration 18, loss = 0.28171101\n",
      "Iteration 82, loss = 0.10972484\n",
      "Iteration 19, loss = 0.27463250\n",
      "Iteration 20, loss = 0.26956548\n",
      "Iteration 21, loss = 0.26288874\n",
      "Iteration 83, loss = 0.10305457\n",
      "Iteration 22, loss = 0.25765091\n",
      "Iteration 84, loss = 0.10099373\n",
      "Iteration 23, loss = 0.25327436\n",
      "Iteration 85, loss = 0.10027948\n",
      "Iteration 86, loss = 0.10000207\n",
      "Iteration 24, loss = 0.24943628\n",
      "Iteration 87, loss = 0.10014013\n",
      "Iteration 88, loss = 0.10057606\n",
      "Iteration 89, loss = 0.09727135\n",
      "Iteration 25, loss = 0.24433610\n",
      "Iteration 26, loss = 0.24009168\n",
      "Iteration 27, loss = 0.23367559\n",
      "Iteration 90, loss = 0.09864286\n",
      "Iteration 28, loss = 0.23164706\n",
      "Iteration 91, loss = 0.09404023\n",
      "Iteration 92, loss = 0.09179394\n",
      "Iteration 93, loss = 0.08993822\n",
      "Iteration 29, loss = 0.22563426\n",
      "Iteration 30, loss = 0.22290324\n",
      "Iteration 31, loss = 0.21782927\n",
      "Iteration 94, loss = 0.08796159\n",
      "Iteration 95, loss = 0.08841784\n",
      "Iteration 32, loss = 0.21476701\n",
      "Iteration 96, loss = 0.08846416\n",
      "Iteration 33, loss = 0.21175795\n",
      "Iteration 97, loss = 0.08774566\n",
      "Iteration 98, loss = 0.08493838\n",
      "Iteration 34, loss = 0.20801287\n",
      "Iteration 99, loss = 0.08275372\n",
      "Iteration 35, loss = 0.21103062\n",
      "Iteration 100, loss = 0.08485653\n",
      "Iteration 36, loss = 0.19920320\n",
      "Iteration 101, loss = 0.08464246\n",
      "Iteration 102, loss = 0.08053783\n",
      "Iteration 37, loss = 0.20055560\n",
      "Iteration 103, loss = 0.08141330\n",
      "Iteration 38, loss = 0.19296283\n",
      "Iteration 104, loss = 0.07844823\n",
      "Iteration 39, loss = 0.18968430\n",
      "Iteration 40, loss = 0.18499099\n",
      "Iteration 105, loss = 0.07946819\n",
      "Iteration 106, loss = 0.07834548\n",
      "Iteration 107, loss = 0.07939390\n",
      "Iteration 41, loss = 0.18238665\n",
      "Iteration 108, loss = 0.07763586\n",
      "Iteration 109, loss = 0.07268428\n",
      "Iteration 42, loss = 0.18285025\n",
      "Iteration 43, loss = 0.17799963\n",
      "Iteration 110, loss = 0.07254737\n",
      "Iteration 44, loss = 0.17371820\n",
      "Iteration 111, loss = 0.07294914\n",
      "Iteration 45, loss = 0.17207623\n",
      "Iteration 46, loss = 0.17258239\n",
      "Iteration 47, loss = 0.16916797\n",
      "Iteration 112, loss = 0.07478752\n",
      "Iteration 48, loss = 0.16340803\n",
      "Iteration 113, loss = 0.07276021\n",
      "Iteration 114, loss = 0.06983625\n",
      "Iteration 115, loss = 0.07557734\n",
      "Iteration 49, loss = 0.16037239\n",
      "Iteration 50, loss = 0.16167769\n",
      "Iteration 51, loss = 0.15581239\n",
      "Iteration 116, loss = 0.07135396\n",
      "Iteration 117, loss = 0.07481770\n",
      "Iteration 118, loss = 0.07067331\n",
      "Iteration 52, loss = 0.15441968\n",
      "Iteration 119, loss = 0.06657551\n",
      "Iteration 53, loss = 0.15048676\n",
      "Iteration 120, loss = 0.07018716\n",
      "Iteration 54, loss = 0.14947604\n",
      "Iteration 121, loss = 0.06681358\n",
      "Iteration 55, loss = 0.14674284\n",
      "Iteration 122, loss = 0.06931367\n",
      "Iteration 56, loss = 0.14438759\n",
      "Iteration 57, loss = 0.14422500\n",
      "Iteration 58, loss = 0.13916688\n",
      "Iteration 123, loss = 0.06623017\n",
      "Iteration 59, loss = 0.14279441\n",
      "Iteration 60, loss = 0.14016206\n",
      "Iteration 61, loss = 0.13298347\n",
      "Iteration 124, loss = 0.06609319\n",
      "Iteration 62, loss = 0.13538355\n",
      "Iteration 63, loss = 0.13494573\n",
      "Iteration 64, loss = 0.13007250\n",
      "Iteration 125, loss = 0.06908813\n",
      "Iteration 65, loss = 0.12710021\n",
      "Iteration 66, loss = 0.12837791\n",
      "Iteration 67, loss = 0.12525441\n",
      "Iteration 126, loss = 0.06559623\n",
      "Iteration 68, loss = 0.12142869\n",
      "Iteration 69, loss = 0.12398241\n",
      "Iteration 127, loss = 0.06844438\n",
      "Iteration 128, loss = 0.06727881\n",
      "Iteration 70, loss = 0.11998469\n",
      "Iteration 129, loss = 0.06371993\n",
      "Iteration 71, loss = 0.12000397\n",
      "Iteration 130, loss = 0.06096665\n",
      "Iteration 72, loss = 0.11693284\n",
      "Iteration 73, loss = 0.12367779\n",
      "Iteration 131, loss = 0.06001814\n",
      "Iteration 74, loss = 0.11794786\n",
      "Iteration 132, loss = 0.05700250\n",
      "Iteration 75, loss = 0.11038114\n",
      "Iteration 133, loss = 0.06113176\n",
      "Iteration 76, loss = 0.11313838\n",
      "Iteration 134, loss = 0.05674795\n",
      "Iteration 135, loss = 0.05998856\n",
      "Iteration 77, loss = 0.11028918\n",
      "Iteration 136, loss = 0.05891192\n",
      "Iteration 137, loss = 0.05656555\n",
      "Iteration 78, loss = 0.10921281\n",
      "Iteration 138, loss = 0.05320349\n",
      "Iteration 79, loss = 0.10767030\n",
      "Iteration 139, loss = 0.05134928\n",
      "Iteration 80, loss = 0.10574945\n",
      "Iteration 81, loss = 0.10508336\n",
      "Iteration 140, loss = 0.05266367\n",
      "Iteration 82, loss = 0.10159086\n",
      "Iteration 141, loss = 0.05121561\n",
      "Iteration 142, loss = 0.05034554\n",
      "Iteration 83, loss = 0.10181875\n",
      "Iteration 143, loss = 0.04898303\n",
      "Iteration 144, loss = 0.05170051\n",
      "Iteration 84, loss = 0.10363941\n",
      "Iteration 145, loss = 0.05257413\n",
      "Iteration 146, loss = 0.05497980\n",
      "Iteration 85, loss = 0.10003709\n",
      "Iteration 86, loss = 0.09839442\n",
      "Iteration 147, loss = 0.04812010\n",
      "Iteration 148, loss = 0.04896728\n",
      "Iteration 149, loss = 0.04787790\n",
      "Iteration 87, loss = 0.09706706\n",
      "Iteration 150, loss = 0.04682054\n",
      "Iteration 88, loss = 0.09651658\n",
      "Iteration 89, loss = 0.09497501\n",
      "Iteration 90, loss = 0.09265074\n",
      "Iteration 151, loss = 0.04489104\n",
      "Iteration 91, loss = 0.09254863\n",
      "Iteration 152, loss = 0.04414137\n",
      "Iteration 92, loss = 0.09346630\n",
      "Iteration 153, loss = 0.04317343\n",
      "Iteration 154, loss = 0.04543123\n",
      "Iteration 93, loss = 0.09349397\n",
      "Iteration 155, loss = 0.04638983\n",
      "Iteration 94, loss = 0.09428885\n",
      "Iteration 95, loss = 0.09259312\n",
      "Iteration 156, loss = 0.04532946\n",
      "Iteration 96, loss = 0.09274579\n",
      "Iteration 157, loss = 0.04315812\n",
      "Iteration 158, loss = 0.04255698\n",
      "Iteration 97, loss = 0.08926562\n",
      "Iteration 159, loss = 0.04160410\n",
      "Iteration 98, loss = 0.09348977\n",
      "Iteration 99, loss = 0.09824890\n",
      "Iteration 160, loss = 0.03985693\n",
      "Iteration 100, loss = 0.09972092\n",
      "Iteration 161, loss = 0.03892611\n",
      "Iteration 162, loss = 0.03910551\n",
      "Iteration 163, loss = 0.04031906\n",
      "Iteration 164, loss = 0.04054449\n",
      "Iteration 165, loss = 0.03956621\n",
      "Iteration 166, loss = 0.03940482\n",
      "Iteration 167, loss = 0.03721574\n",
      "Iteration 168, loss = 0.03736350\n",
      "Iteration 169, loss = 0.03681605\n",
      "Iteration 170, loss = 0.03777003\n",
      "Iteration 171, loss = 0.03459437\n",
      "Iteration 172, loss = 0.03395261\n",
      "Iteration 173, loss = 0.03329061\n",
      "Iteration 174, loss = 0.03412854\n",
      "Iteration 175, loss = 0.03530882\n",
      "Iteration 176, loss = 0.03292162\n",
      "Iteration 1, loss = 1.15370569\n",
      "Iteration 2, loss = 0.79058185\n",
      "Iteration 177, loss = 0.03367674\n",
      "Iteration 3, loss = 0.61794179\n",
      "Iteration 178, loss = 0.03340403\n",
      "Iteration 179, loss = 0.03414609\n",
      "Iteration 4, loss = 0.52937189\n",
      "Iteration 180, loss = 0.03361987\n",
      "Iteration 5, loss = 0.47963586\n",
      "Iteration 6, loss = 0.44475596\n",
      "Iteration 181, loss = 0.03309184\n",
      "Iteration 7, loss = 0.42009799\n",
      "Iteration 182, loss = 0.03266013\n",
      "Iteration 8, loss = 0.39937751\n",
      "Iteration 183, loss = 0.03033983\n",
      "Iteration 9, loss = 0.38222509\n",
      "Iteration 184, loss = 0.02928991\n",
      "Iteration 10, loss = 0.36735163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.35463246\n",
      "Iteration 185, loss = 0.02856908\n",
      "Iteration 186, loss = 0.03222775\n",
      "Iteration 12, loss = 0.34340427\n",
      "Iteration 187, loss = 0.03018874\n",
      "Iteration 13, loss = 0.33399875\n",
      "Iteration 188, loss = 0.03009250\n",
      "Iteration 189, loss = 0.03207637\n",
      "Iteration 14, loss = 0.32390089\n",
      "Iteration 190, loss = 0.02810277\n",
      "Iteration 15, loss = 0.31582479\n",
      "Iteration 191, loss = 0.02706313\n",
      "Iteration 16, loss = 0.30752372\n",
      "Iteration 192, loss = 0.02799128\n",
      "Iteration 17, loss = 0.29943716\n",
      "Iteration 18, loss = 0.29521932\n",
      "Iteration 193, loss = 0.02835986\n",
      "Iteration 194, loss = 0.02925036\n",
      "Iteration 195, loss = 0.03154241\n",
      "Iteration 196, loss = 0.02906610\n",
      "Iteration 19, loss = 0.28612469\n",
      "Iteration 197, loss = 0.03008006\n",
      "Iteration 198, loss = 0.02847391\n",
      "Iteration 20, loss = 0.28089945\n",
      "Iteration 199, loss = 0.02738950\n",
      "Iteration 21, loss = 0.27432050\n",
      "Iteration 22, loss = 0.26897014\n",
      "Iteration 23, loss = 0.26206246\n",
      "Iteration 200, loss = 0.02494691\n",
      "Iteration 24, loss = 0.25899541\n",
      "Iteration 25, loss = 0.25502679\n",
      "Iteration 26, loss = 0.24839228\n",
      "Iteration 27, loss = 0.24360332\n",
      "Iteration 28, loss = 0.23770577\n",
      "Iteration 1, loss = 1.11713089\n",
      "Iteration 29, loss = 0.23714772\n",
      "Iteration 2, loss = 0.78823718\n",
      "Iteration 30, loss = 0.23658987\n",
      "Iteration 3, loss = 0.64269080\n",
      "Iteration 4, loss = 0.54806454\n",
      "Iteration 31, loss = 0.23058187\n",
      "Iteration 5, loss = 0.49748363\n",
      "Iteration 32, loss = 0.22491045\n",
      "Iteration 6, loss = 0.45100878\n",
      "Iteration 33, loss = 0.21798290\n",
      "Iteration 7, loss = 0.42054781\n",
      "Iteration 34, loss = 0.21709624\n",
      "Iteration 8, loss = 0.40037422\n",
      "Iteration 9, loss = 0.38240894\n",
      "Iteration 35, loss = 0.20950059\n",
      "Iteration 10, loss = 0.36666398\n",
      "Iteration 36, loss = 0.21105198\n",
      "Iteration 11, loss = 0.35187624\n",
      "Iteration 37, loss = 0.20492191\n",
      "Iteration 38, loss = 0.20147765\n",
      "Iteration 12, loss = 0.34205169\n",
      "Iteration 39, loss = 0.19726315\n",
      "Iteration 13, loss = 0.33073784\n",
      "Iteration 14, loss = 0.32163597\n",
      "Iteration 40, loss = 0.19380365\n",
      "Iteration 41, loss = 0.18986826\n",
      "Iteration 42, loss = 0.19060431\n",
      "Iteration 15, loss = 0.31306521\n",
      "Iteration 43, loss = 0.18529322\n",
      "Iteration 16, loss = 0.30534397\n",
      "Iteration 44, loss = 0.18310677\n",
      "Iteration 17, loss = 0.29836390\n",
      "Iteration 45, loss = 0.17747508\n",
      "Iteration 18, loss = 0.29236399\n",
      "Iteration 19, loss = 0.28631670\n",
      "Iteration 20, loss = 0.27918643\n",
      "Iteration 46, loss = 0.17594303\n",
      "Iteration 47, loss = 0.17081936\n",
      "Iteration 21, loss = 0.27366585\n",
      "Iteration 22, loss = 0.26700820\n",
      "Iteration 48, loss = 0.16974877\n",
      "Iteration 49, loss = 0.16531551\n",
      "Iteration 23, loss = 0.26307270\n",
      "Iteration 50, loss = 0.16333688\n",
      "Iteration 24, loss = 0.25521457\n",
      "Iteration 51, loss = 0.16023586\n",
      "Iteration 52, loss = 0.15807294\n",
      "Iteration 25, loss = 0.25407443\n",
      "Iteration 53, loss = 0.15467382\n",
      "Iteration 26, loss = 0.24987900\n",
      "Iteration 54, loss = 0.15290744\n",
      "Iteration 27, loss = 0.24665659\n",
      "Iteration 28, loss = 0.24275553\n",
      "Iteration 55, loss = 0.14905224\n",
      "Iteration 56, loss = 0.15130896\n",
      "Iteration 57, loss = 0.14581765\n",
      "Iteration 29, loss = 0.23633288\n",
      "Iteration 58, loss = 0.14498348\n",
      "Iteration 30, loss = 0.23322686\n",
      "Iteration 59, loss = 0.13862112\n",
      "Iteration 31, loss = 0.22652979\n",
      "Iteration 32, loss = 0.22384255\n",
      "Iteration 60, loss = 0.13946249\n",
      "Iteration 61, loss = 0.13527751\n",
      "Iteration 62, loss = 0.13431817\n",
      "Iteration 33, loss = 0.22033680\n",
      "Iteration 63, loss = 0.13394809\n",
      "Iteration 64, loss = 0.12964449\n",
      "Iteration 65, loss = 0.12770010\n",
      "Iteration 34, loss = 0.21563667\n",
      "Iteration 66, loss = 0.13034558\n",
      "Iteration 67, loss = 0.12455600\n",
      "Iteration 35, loss = 0.21262608\n",
      "Iteration 68, loss = 0.12206736\n",
      "Iteration 36, loss = 0.20868277\n",
      "Iteration 69, loss = 0.11984711\n",
      "Iteration 37, loss = 0.20535953\n",
      "Iteration 70, loss = 0.11954812\n",
      "Iteration 71, loss = 0.11768053\n",
      "Iteration 72, loss = 0.11513809\n",
      "Iteration 38, loss = 0.20280514\n",
      "Iteration 73, loss = 0.11431981\n",
      "Iteration 74, loss = 0.11173485\n",
      "Iteration 75, loss = 0.11176913\n",
      "Iteration 39, loss = 0.20046615\n",
      "Iteration 76, loss = 0.10893478\n",
      "Iteration 77, loss = 0.10757968\n",
      "Iteration 40, loss = 0.19750561\n",
      "Iteration 78, loss = 0.10881262\n",
      "Iteration 41, loss = 0.19745132\n",
      "Iteration 79, loss = 0.10822283\n",
      "Iteration 42, loss = 0.19474565\n",
      "Iteration 80, loss = 0.10337134\n",
      "Iteration 43, loss = 0.18931047\n",
      "Iteration 81, loss = 0.10693980\n",
      "Iteration 44, loss = 0.18284770\n",
      "Iteration 82, loss = 0.10145122\n",
      "Iteration 45, loss = 0.18139879\n",
      "Iteration 83, loss = 0.09987316\n",
      "Iteration 84, loss = 0.10168393\n",
      "Iteration 46, loss = 0.18086645\n",
      "Iteration 85, loss = 0.09750597\n",
      "Iteration 86, loss = 0.09576623\n",
      "Iteration 47, loss = 0.17694540\n",
      "Iteration 87, loss = 0.09598090\n",
      "Iteration 48, loss = 0.17513392\n",
      "Iteration 49, loss = 0.16749232\n",
      "Iteration 88, loss = 0.09576863\n",
      "Iteration 50, loss = 0.16906447\n",
      "Iteration 89, loss = 0.09652656\n",
      "Iteration 90, loss = 0.09006104\n",
      "Iteration 51, loss = 0.16706945\n",
      "Iteration 91, loss = 0.08928834\n",
      "Iteration 52, loss = 0.16423922\n",
      "Iteration 92, loss = 0.08899058\n",
      "Iteration 93, loss = 0.08655581\n",
      "Iteration 53, loss = 0.15820330\n",
      "Iteration 94, loss = 0.08772102\n",
      "Iteration 54, loss = 0.15665632\n",
      "Iteration 55, loss = 0.15816522\n",
      "Iteration 95, loss = 0.08837529\n",
      "Iteration 56, loss = 0.15064171\n",
      "Iteration 96, loss = 0.08275573\n",
      "Iteration 97, loss = 0.08200077\n",
      "Iteration 57, loss = 0.15164489\n",
      "Iteration 98, loss = 0.08185493\n",
      "Iteration 58, loss = 0.14720796\n",
      "Iteration 59, loss = 0.14290969\n",
      "Iteration 99, loss = 0.08107681\n",
      "Iteration 60, loss = 0.14459185\n",
      "Iteration 100, loss = 0.07868169\n",
      "Iteration 61, loss = 0.13978150\n",
      "Iteration 62, loss = 0.13735371\n",
      "Iteration 63, loss = 0.13733228\n",
      "Iteration 101, loss = 0.07813438\n",
      "Iteration 102, loss = 0.07741849\n",
      "Iteration 103, loss = 0.07772355\n",
      "Iteration 104, loss = 0.07804113\n",
      "Iteration 64, loss = 0.13642944\n",
      "Iteration 65, loss = 0.13366154\n",
      "Iteration 66, loss = 0.13027556\n",
      "Iteration 105, loss = 0.07979308\n",
      "Iteration 67, loss = 0.12730280\n",
      "Iteration 106, loss = 0.08025677\n",
      "Iteration 107, loss = 0.07584842\n",
      "Iteration 108, loss = 0.07097236\n",
      "Iteration 68, loss = 0.12727163\n",
      "Iteration 69, loss = 0.12594808\n",
      "Iteration 109, loss = 0.07194481\n",
      "Iteration 70, loss = 0.12447791\n",
      "Iteration 110, loss = 0.07157339\n",
      "Iteration 71, loss = 0.12086866\n",
      "Iteration 111, loss = 0.07148734\n",
      "Iteration 112, loss = 0.06670804\n",
      "Iteration 72, loss = 0.11962820\n",
      "Iteration 73, loss = 0.11822327\n",
      "Iteration 113, loss = 0.06614332\n",
      "Iteration 74, loss = 0.11541252\n",
      "Iteration 75, loss = 0.11398228\n",
      "Iteration 114, loss = 0.06694679\n",
      "Iteration 115, loss = 0.06741799\n",
      "Iteration 116, loss = 0.06495928\n",
      "Iteration 76, loss = 0.11467014\n",
      "Iteration 117, loss = 0.06289540\n",
      "Iteration 77, loss = 0.11475875\n",
      "Iteration 78, loss = 0.11025042\n",
      "Iteration 79, loss = 0.11012324\n",
      "Iteration 118, loss = 0.06430769\n",
      "Iteration 119, loss = 0.06238470\n",
      "Iteration 80, loss = 0.10843195\n",
      "Iteration 120, loss = 0.06075371\n",
      "Iteration 121, loss = 0.06263522\n",
      "Iteration 81, loss = 0.10515488\n",
      "Iteration 82, loss = 0.10373357\n",
      "Iteration 122, loss = 0.06325839\n",
      "Iteration 83, loss = 0.10265246\n",
      "Iteration 123, loss = 0.05912605\n",
      "Iteration 124, loss = 0.06238496\n",
      "Iteration 84, loss = 0.10611457\n",
      "Iteration 125, loss = 0.06156725\n",
      "Iteration 85, loss = 0.10286090\n",
      "Iteration 86, loss = 0.10525202\n",
      "Iteration 126, loss = 0.06086274\n",
      "Iteration 87, loss = 0.10320972\n",
      "Iteration 127, loss = 0.05850826\n",
      "Iteration 128, loss = 0.05598113\n",
      "Iteration 129, loss = 0.05361307\n",
      "Iteration 88, loss = 0.10005557\n",
      "Iteration 130, loss = 0.05569883\n",
      "Iteration 131, loss = 0.05971861\n",
      "Iteration 89, loss = 0.09994771\n",
      "Iteration 132, loss = 0.05770500\n",
      "Iteration 90, loss = 0.10260831\n",
      "Iteration 133, loss = 0.05357682\n",
      "Iteration 91, loss = 0.09818116\n",
      "Iteration 134, loss = 0.05216218\n",
      "Iteration 92, loss = 0.09241768\n",
      "Iteration 135, loss = 0.05341457\n",
      "Iteration 136, loss = 0.04956119\n",
      "Iteration 137, loss = 0.04943765\n",
      "Iteration 93, loss = 0.09149550\n",
      "Iteration 138, loss = 0.04920786\n",
      "Iteration 94, loss = 0.08952449\n",
      "Iteration 139, loss = 0.04786649\n",
      "Iteration 95, loss = 0.09094069\n",
      "Iteration 140, loss = 0.04774661\n",
      "Iteration 96, loss = 0.09049244\n",
      "Iteration 141, loss = 0.04722233\n",
      "Iteration 97, loss = 0.08663830\n",
      "Iteration 142, loss = 0.04925010\n",
      "Iteration 98, loss = 0.08681525\n",
      "Iteration 143, loss = 0.05153473\n",
      "Iteration 99, loss = 0.08906875\n",
      "Iteration 100, loss = 0.09308362\n",
      "Iteration 144, loss = 0.05178187\n",
      "Iteration 101, loss = 0.08446939\n",
      "Iteration 145, loss = 0.05408196\n",
      "Iteration 146, loss = 0.05393629\n",
      "Iteration 147, loss = 0.04879809\n",
      "Iteration 102, loss = 0.08183074\n",
      "Iteration 148, loss = 0.04588037\n",
      "Iteration 103, loss = 0.07831433\n",
      "Iteration 149, loss = 0.04420718\n",
      "Iteration 150, loss = 0.04281135\n",
      "Iteration 104, loss = 0.07821920\n",
      "Iteration 105, loss = 0.07793617\n",
      "Iteration 151, loss = 0.04458236\n",
      "Iteration 106, loss = 0.07905028\n",
      "Iteration 152, loss = 0.04154578\n",
      "Iteration 107, loss = 0.07678905\n",
      "Iteration 108, loss = 0.07803008\n",
      "Iteration 153, loss = 0.04285797\n",
      "Iteration 154, loss = 0.04344624\n",
      "Iteration 155, loss = 0.04117062\n",
      "Iteration 109, loss = 0.07601432\n",
      "Iteration 156, loss = 0.04222423\n",
      "Iteration 110, loss = 0.07480128\n",
      "Iteration 157, loss = 0.04007418\n",
      "Iteration 111, loss = 0.07287044\n",
      "Iteration 158, loss = 0.03987894\n",
      "Iteration 159, loss = 0.03810040\n",
      "Iteration 112, loss = 0.07811619\n",
      "Iteration 160, loss = 0.03865602Iteration 113, loss = 0.07800019\n",
      "\n",
      "Iteration 161, loss = 0.04022025\n",
      "Iteration 162, loss = 0.04047545\n",
      "Iteration 163, loss = 0.04037763\n",
      "Iteration 114, loss = 0.07566639\n",
      "Iteration 164, loss = 0.03626469\n",
      "Iteration 115, loss = 0.07287554\n",
      "Iteration 165, loss = 0.03529515\n",
      "Iteration 166, loss = 0.03568961\n",
      "Iteration 116, loss = 0.07403083\n",
      "Iteration 167, loss = 0.03416796\n",
      "Iteration 168, loss = 0.03356066\n",
      "Iteration 117, loss = 0.07885235\n",
      "Iteration 118, loss = 0.07175740\n",
      "Iteration 119, loss = 0.06718415\n",
      "Iteration 120, loss = 0.07360109\n",
      "Iteration 121, loss = 0.06807874\n",
      "Iteration 122, loss = 0.07155931\n",
      "Iteration 169, loss = 0.03340252\n",
      "Iteration 123, loss = 0.06411466\n",
      "Iteration 124, loss = 0.06282744\n",
      "Iteration 170, loss = 0.03343000\n",
      "Iteration 125, loss = 0.06224174\n",
      "Iteration 171, loss = 0.03281443\n",
      "Iteration 126, loss = 0.06084765\n",
      "Iteration 172, loss = 0.03641297\n",
      "Iteration 127, loss = 0.05982030\n",
      "Iteration 173, loss = 0.03773317\n",
      "Iteration 128, loss = 0.06012623\n",
      "Iteration 129, loss = 0.05788151\n",
      "Iteration 174, loss = 0.03469688\n",
      "Iteration 130, loss = 0.05793387\n",
      "Iteration 175, loss = 0.03158365\n",
      "Iteration 131, loss = 0.05654648\n",
      "Iteration 176, loss = 0.03061882\n",
      "Iteration 132, loss = 0.05651109\n",
      "Iteration 177, loss = 0.02972244\n",
      "Iteration 133, loss = 0.06179470\n",
      "Iteration 178, loss = 0.02982920\n",
      "Iteration 134, loss = 0.06019904\n",
      "Iteration 179, loss = 0.02985694\n",
      "Iteration 135, loss = 0.05826151\n",
      "Iteration 136, loss = 0.05781792\n",
      "Iteration 180, loss = 0.02986580\n",
      "Iteration 181, loss = 0.03080214\n",
      "Iteration 137, loss = 0.05359220\n",
      "Iteration 182, loss = 0.03224159\n",
      "Iteration 183, loss = 0.02935063\n",
      "Iteration 138, loss = 0.05242152\n",
      "Iteration 139, loss = 0.05338226\n",
      "Iteration 184, loss = 0.03107095\n",
      "Iteration 140, loss = 0.05357991\n",
      "Iteration 185, loss = 0.03093993\n",
      "Iteration 186, loss = 0.02874356\n",
      "Iteration 187, loss = 0.03070074\n",
      "Iteration 141, loss = 0.05381153\n",
      "Iteration 188, loss = 0.02692836\n",
      "Iteration 142, loss = 0.05153263\n",
      "Iteration 143, loss = 0.04956196\n",
      "Iteration 144, loss = 0.05022142\n",
      "Iteration 189, loss = 0.02671772\n",
      "Iteration 145, loss = 0.05229900\n",
      "Iteration 190, loss = 0.02947743\n",
      "Iteration 191, loss = 0.02563904\n",
      "Iteration 146, loss = 0.05138045\n",
      "Iteration 192, loss = 0.02637232\n",
      "Iteration 193, loss = 0.02523243\n",
      "Iteration 147, loss = 0.05122473\n",
      "Iteration 148, loss = 0.05845936\n",
      "Iteration 194, loss = 0.02465290\n",
      "Iteration 149, loss = 0.06342452\n",
      "Iteration 195, loss = 0.02455115\n",
      "Iteration 150, loss = 0.05549411\n",
      "Iteration 196, loss = 0.02465633\n",
      "Iteration 197, loss = 0.02414627\n",
      "Iteration 198, loss = 0.02400012\n",
      "Iteration 151, loss = 0.05886733\n",
      "Iteration 199, loss = 0.02659707\n",
      "Iteration 152, loss = 0.05363049\n",
      "Iteration 200, loss = 0.02295060\n",
      "Iteration 153, loss = 0.05135007\n",
      "Iteration 154, loss = 0.05769065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.37472236\n",
      "Iteration 2, loss = 0.88045922\n",
      "Iteration 3, loss = 0.66345049\n",
      "Iteration 4, loss = 0.56444806\n",
      "Iteration 1, loss = 0.92824820\n",
      "Iteration 2, loss = 0.67382182\n",
      "Iteration 5, loss = 0.50338934\n",
      "Iteration 3, loss = 0.55205098\n",
      "Iteration 4, loss = 0.49226593\n",
      "Iteration 5, loss = 0.44983516\n",
      "Iteration 6, loss = 0.46825749\n",
      "Iteration 6, loss = 0.42176807\n",
      "Iteration 7, loss = 0.43715241\n",
      "Iteration 7, loss = 0.39938753\n",
      "Iteration 8, loss = 0.41622741\n",
      "Iteration 9, loss = 0.39834738\n",
      "Iteration 8, loss = 0.38054073\n",
      "Iteration 10, loss = 0.38266495\n",
      "Iteration 9, loss = 0.36451820\n",
      "Iteration 11, loss = 0.36888536\n",
      "Iteration 12, loss = 0.35697639\n",
      "Iteration 10, loss = 0.35212557\n",
      "Iteration 13, loss = 0.34863439\n",
      "Iteration 11, loss = 0.34050058\n",
      "Iteration 14, loss = 0.33634493\n",
      "Iteration 15, loss = 0.32855822\n",
      "Iteration 16, loss = 0.32017500\n",
      "Iteration 12, loss = 0.33052023\n",
      "Iteration 17, loss = 0.31333889\n",
      "Iteration 13, loss = 0.32148896\n",
      "Iteration 18, loss = 0.30571651\n",
      "Iteration 14, loss = 0.31412119\n",
      "Iteration 19, loss = 0.29949276\n",
      "Iteration 20, loss = 0.29296916\n",
      "Iteration 15, loss = 0.30600485\n",
      "Iteration 16, loss = 0.29964950\n",
      "Iteration 21, loss = 0.28761690\n",
      "Iteration 17, loss = 0.29216147\n",
      "Iteration 18, loss = 0.28559042\n",
      "Iteration 19, loss = 0.28223088\n",
      "Iteration 22, loss = 0.28132400\n",
      "Iteration 23, loss = 0.27486392\n",
      "Iteration 20, loss = 0.27431153\n",
      "Iteration 21, loss = 0.27164209\n",
      "Iteration 24, loss = 0.27127862\n",
      "Iteration 25, loss = 0.26491941\n",
      "Iteration 22, loss = 0.26424173\n",
      "Iteration 26, loss = 0.26143910\n",
      "Iteration 23, loss = 0.25913046\n",
      "Iteration 27, loss = 0.25386827\n",
      "Iteration 24, loss = 0.26003542\n",
      "Iteration 25, loss = 0.25278721\n",
      "Iteration 28, loss = 0.25069438\n",
      "Iteration 26, loss = 0.25042005\n",
      "Iteration 29, loss = 0.24745442\n",
      "Iteration 30, loss = 0.24249381\n",
      "Iteration 27, loss = 0.24310398\n",
      "Iteration 31, loss = 0.23819812\n",
      "Iteration 28, loss = 0.23645753\n",
      "Iteration 32, loss = 0.23451160\n",
      "Iteration 29, loss = 0.23847750\n",
      "Iteration 33, loss = 0.23047993\n",
      "Iteration 30, loss = 0.22828439\n",
      "Iteration 34, loss = 0.22745388\n",
      "Iteration 31, loss = 0.22700226\n",
      "Iteration 35, loss = 0.22397194\n",
      "Iteration 36, loss = 0.22009267\n",
      "Iteration 32, loss = 0.22133900\n",
      "Iteration 33, loss = 0.21819616\n",
      "Iteration 37, loss = 0.21618097\n",
      "Iteration 34, loss = 0.21533895\n",
      "Iteration 38, loss = 0.21249905\n",
      "Iteration 35, loss = 0.21089111\n",
      "Iteration 39, loss = 0.20900294\n",
      "Iteration 40, loss = 0.20633539\n",
      "Iteration 36, loss = 0.20626591\n",
      "Iteration 41, loss = 0.20524921\n",
      "Iteration 37, loss = 0.20299047\n",
      "Iteration 42, loss = 0.20034472\n",
      "Iteration 38, loss = 0.20057191\n",
      "Iteration 43, loss = 0.19809834\n",
      "Iteration 39, loss = 0.19887306\n",
      "Iteration 44, loss = 0.19675744\n",
      "Iteration 45, loss = 0.19109499\n",
      "Iteration 46, loss = 0.19366662\n",
      "Iteration 40, loss = 0.19197196\n",
      "Iteration 41, loss = 0.18919014\n",
      "Iteration 47, loss = 0.18809778\n",
      "Iteration 42, loss = 0.18784519\n",
      "Iteration 48, loss = 0.18198440\n",
      "Iteration 43, loss = 0.18590542\n",
      "Iteration 49, loss = 0.18492448\n",
      "Iteration 44, loss = 0.18338017\n",
      "Iteration 45, loss = 0.17991191\n",
      "Iteration 50, loss = 0.18105107\n",
      "Iteration 46, loss = 0.17245889\n",
      "Iteration 51, loss = 0.17528888\n",
      "Iteration 47, loss = 0.17343320\n",
      "Iteration 52, loss = 0.17126541\n",
      "Iteration 53, loss = 0.17167537\n",
      "Iteration 54, loss = 0.16842446\n",
      "Iteration 48, loss = 0.16761849\n",
      "Iteration 55, loss = 0.16811274\n",
      "Iteration 49, loss = 0.16735623\n",
      "Iteration 56, loss = 0.16296762\n",
      "Iteration 50, loss = 0.16206451\n",
      "Iteration 57, loss = 0.16654889\n",
      "Iteration 51, loss = 0.15930244\n",
      "Iteration 58, loss = 0.16113633\n",
      "Iteration 52, loss = 0.15704063\n",
      "Iteration 59, loss = 0.15890339\n",
      "Iteration 53, loss = 0.15557830\n",
      "Iteration 60, loss = 0.15546503\n",
      "Iteration 61, loss = 0.15374922\n",
      "Iteration 62, loss = 0.14730212\n",
      "Iteration 54, loss = 0.15478698\n",
      "Iteration 63, loss = 0.15047964\n",
      "Iteration 64, loss = 0.14666110\n",
      "Iteration 65, loss = 0.14888138\n",
      "Iteration 55, loss = 0.15176283\n",
      "Iteration 56, loss = 0.14922017\n",
      "Iteration 66, loss = 0.14877975\n",
      "Iteration 57, loss = 0.14869072\n",
      "Iteration 67, loss = 0.14212650\n",
      "Iteration 68, loss = 0.13857385\n",
      "Iteration 69, loss = 0.13695233\n",
      "Iteration 58, loss = 0.14555513\n",
      "Iteration 70, loss = 0.13487310\n",
      "Iteration 59, loss = 0.14517261\n",
      "Iteration 60, loss = 0.14523179\n",
      "Iteration 71, loss = 0.13504054\n",
      "Iteration 61, loss = 0.14317879\n",
      "Iteration 72, loss = 0.13220062\n",
      "Iteration 73, loss = 0.12898881\n",
      "Iteration 74, loss = 0.12855538\n",
      "Iteration 62, loss = 0.14075330\n",
      "Iteration 63, loss = 0.14147948\n",
      "Iteration 75, loss = 0.12675598\n",
      "Iteration 64, loss = 0.13341633\n",
      "Iteration 76, loss = 0.12656703\n",
      "Iteration 77, loss = 0.12296044\n",
      "Iteration 78, loss = 0.12192884\n",
      "Iteration 65, loss = 0.13502646\n",
      "Iteration 79, loss = 0.11879013\n",
      "Iteration 80, loss = 0.12066760\n",
      "Iteration 81, loss = 0.11685421\n",
      "Iteration 82, loss = 0.11572646\n",
      "Iteration 66, loss = 0.13079864\n",
      "Iteration 83, loss = 0.11435267\n",
      "Iteration 67, loss = 0.12462472\n",
      "Iteration 68, loss = 0.12795160\n",
      "Iteration 84, loss = 0.11419736\n",
      "Iteration 85, loss = 0.11143249\n",
      "Iteration 86, loss = 0.10957305\n",
      "Iteration 69, loss = 0.12453029\n",
      "Iteration 87, loss = 0.11259205\n",
      "Iteration 88, loss = 0.11162978\n",
      "Iteration 70, loss = 0.12177007\n",
      "Iteration 89, loss = 0.11000847\n",
      "Iteration 90, loss = 0.10723798\n",
      "Iteration 91, loss = 0.10500179\n",
      "Iteration 71, loss = 0.11915283\n",
      "Iteration 92, loss = 0.10394832\n",
      "Iteration 72, loss = 0.12100191\n",
      "Iteration 93, loss = 0.10795461\n",
      "Iteration 73, loss = 0.11869823\n",
      "Iteration 94, loss = 0.10807843\n",
      "Iteration 74, loss = 0.11553984\n",
      "Iteration 95, loss = 0.10164242\n",
      "Iteration 75, loss = 0.11771495\n",
      "Iteration 96, loss = 0.10224326\n",
      "Iteration 76, loss = 0.11276207\n",
      "Iteration 97, loss = 0.10024487\n",
      "Iteration 77, loss = 0.11144766\n",
      "Iteration 98, loss = 0.09760976\n",
      "Iteration 99, loss = 0.09823976\n",
      "Iteration 78, loss = 0.11285079\n",
      "Iteration 100, loss = 0.09489596\n",
      "Iteration 101, loss = 0.09375818\n",
      "Iteration 102, loss = 0.09915761\n",
      "Iteration 79, loss = 0.10625539\n",
      "Iteration 80, loss = 0.10588754\n",
      "Iteration 103, loss = 0.09495616\n",
      "Iteration 104, loss = 0.09256713\n",
      "Iteration 81, loss = 0.10708408\n",
      "Iteration 82, loss = 0.10502935\n",
      "Iteration 83, loss = 0.10308761\n",
      "Iteration 105, loss = 0.09165909\n",
      "Iteration 84, loss = 0.10179848\n",
      "Iteration 106, loss = 0.09410117\n",
      "Iteration 107, loss = 0.09185084\n",
      "Iteration 85, loss = 0.09913561\n",
      "Iteration 108, loss = 0.09371889\n",
      "Iteration 86, loss = 0.09852528\n",
      "Iteration 109, loss = 0.08565759\n",
      "Iteration 87, loss = 0.09579267\n",
      "Iteration 88, loss = 0.09677235\n",
      "Iteration 110, loss = 0.08681989\n",
      "Iteration 89, loss = 0.09465809\n",
      "Iteration 90, loss = 0.09473767\n",
      "Iteration 111, loss = 0.08884504\n",
      "Iteration 91, loss = 0.09677509\n",
      "Iteration 92, loss = 0.09109975\n",
      "Iteration 112, loss = 0.08582630\n",
      "Iteration 93, loss = 0.09061372\n",
      "Iteration 113, loss = 0.08802067\n",
      "Iteration 114, loss = 0.08377229\n",
      "Iteration 94, loss = 0.08907142\n",
      "Iteration 115, loss = 0.08125885\n",
      "Iteration 95, loss = 0.08788922\n",
      "Iteration 96, loss = 0.08901959\n",
      "Iteration 116, loss = 0.08055855\n",
      "Iteration 97, loss = 0.08579106\n",
      "Iteration 117, loss = 0.08123966\n",
      "Iteration 98, loss = 0.08545382\n",
      "Iteration 118, loss = 0.08568292\n",
      "Iteration 119, loss = 0.08803169\n",
      "Iteration 120, loss = 0.07918878\n",
      "Iteration 99, loss = 0.08932365\n",
      "Iteration 121, loss = 0.07924908\n",
      "Iteration 122, loss = 0.07858928\n",
      "Iteration 100, loss = 0.09573287\n",
      "Iteration 101, loss = 0.08808690\n",
      "Iteration 102, loss = 0.08638056\n",
      "Iteration 123, loss = 0.07491062\n",
      "Iteration 124, loss = 0.07397011\n",
      "Iteration 103, loss = 0.08137856\n",
      "Iteration 125, loss = 0.07466222\n",
      "Iteration 104, loss = 0.08019410\n",
      "Iteration 126, loss = 0.07594311\n",
      "Iteration 105, loss = 0.08229257\n",
      "Iteration 106, loss = 0.07800207\n",
      "Iteration 127, loss = 0.07433599\n",
      "Iteration 107, loss = 0.07903007\n",
      "Iteration 108, loss = 0.07852075\n",
      "Iteration 128, loss = 0.07051212\n",
      "Iteration 109, loss = 0.07851924\n",
      "Iteration 129, loss = 0.07099006\n",
      "Iteration 130, loss = 0.07077270\n",
      "Iteration 110, loss = 0.07633923\n",
      "Iteration 131, loss = 0.07185665\n",
      "Iteration 111, loss = 0.07367113\n",
      "Iteration 132, loss = 0.07070291\n",
      "Iteration 112, loss = 0.07034236\n",
      "Iteration 133, loss = 0.07090618\n",
      "Iteration 134, loss = 0.07188174\n",
      "Iteration 113, loss = 0.07531319\n",
      "Iteration 135, loss = 0.07553171\n",
      "Iteration 114, loss = 0.07587739\n",
      "Iteration 115, loss = 0.07446143\n",
      "Iteration 136, loss = 0.08105024\n",
      "Iteration 116, loss = 0.07455611\n",
      "Iteration 137, loss = 0.07624271\n",
      "Iteration 117, loss = 0.06782348\n",
      "Iteration 138, loss = 0.07390686\n",
      "Iteration 118, loss = 0.06844360\n",
      "Iteration 139, loss = 0.07308042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 119, loss = 0.06747270\n",
      "Iteration 120, loss = 0.06843418\n",
      "Iteration 121, loss = 0.06550771\n",
      "Iteration 122, loss = 0.06587793\n",
      "Iteration 123, loss = 0.06510152\n",
      "Iteration 124, loss = 0.06698286\n",
      "Iteration 125, loss = 0.06967497\n",
      "Iteration 126, loss = 0.06966059\n",
      "Iteration 127, loss = 0.06331702\n",
      "Iteration 128, loss = 0.05857776\n",
      "Iteration 129, loss = 0.06239795\n",
      "Iteration 1, loss = 1.26785932\n",
      "Iteration 2, loss = 0.80326112\n",
      "Iteration 130, loss = 0.06092551\n",
      "Iteration 3, loss = 0.62459770\n",
      "Iteration 131, loss = 0.06174029\n",
      "Iteration 132, loss = 0.07001134\n",
      "Iteration 133, loss = 0.06586503\n",
      "Iteration 4, loss = 0.53291499\n",
      "Iteration 134, loss = 0.06674462\n",
      "Iteration 5, loss = 0.47876724\n",
      "Iteration 135, loss = 0.06474265\n",
      "Iteration 6, loss = 0.44216646\n",
      "Iteration 136, loss = 0.06176295\n",
      "Iteration 7, loss = 0.41552640\n",
      "Iteration 8, loss = 0.39470498\n",
      "Iteration 9, loss = 0.37865534\n",
      "Iteration 10, loss = 0.36425025\n",
      "Iteration 137, loss = 0.06856786\n",
      "Iteration 11, loss = 0.35330565\n",
      "Iteration 138, loss = 0.06082939\n",
      "Iteration 12, loss = 0.34173439\n",
      "Iteration 139, loss = 0.06000940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.33601471\n",
      "Iteration 14, loss = 0.32335880\n",
      "Iteration 15, loss = 0.31528141\n",
      "Iteration 16, loss = 0.30804312\n",
      "Iteration 17, loss = 0.29871945\n",
      "Iteration 18, loss = 0.29375178\n",
      "Iteration 19, loss = 0.28583124\n",
      "Iteration 20, loss = 0.27929320\n",
      "Iteration 21, loss = 0.27377004\n",
      "Iteration 22, loss = 0.27172687\n",
      "Iteration 23, loss = 0.26316720\n",
      "Iteration 24, loss = 0.25784116\n",
      "Iteration 25, loss = 0.25584793\n",
      "Iteration 1, loss = 0.86924994\n",
      "Iteration 26, loss = 0.25279250\n",
      "Iteration 27, loss = 0.24041162\n",
      "Iteration 2, loss = 0.65543403\n",
      "Iteration 28, loss = 0.24530822\n",
      "Iteration 29, loss = 0.23816164\n",
      "Iteration 30, loss = 0.23161765\n",
      "Iteration 3, loss = 0.54637042\n",
      "Iteration 31, loss = 0.23034257\n",
      "Iteration 4, loss = 0.48847068\n",
      "Iteration 5, loss = 0.45584128\n",
      "Iteration 32, loss = 0.22509559\n",
      "Iteration 33, loss = 0.22038069\n",
      "Iteration 6, loss = 0.42126785\n",
      "Iteration 34, loss = 0.21584729\n",
      "Iteration 35, loss = 0.21302112\n",
      "Iteration 7, loss = 0.39756135\n",
      "Iteration 36, loss = 0.20894863\n",
      "Iteration 8, loss = 0.37736897\n",
      "Iteration 9, loss = 0.36159718\n",
      "Iteration 37, loss = 0.20827865\n",
      "Iteration 10, loss = 0.34929995\n",
      "Iteration 38, loss = 0.20129373\n",
      "Iteration 39, loss = 0.19905911\n",
      "Iteration 11, loss = 0.33636145\n",
      "Iteration 40, loss = 0.19745577\n",
      "Iteration 12, loss = 0.32793546\n",
      "Iteration 13, loss = 0.31822445\n",
      "Iteration 41, loss = 0.19133160\n",
      "Iteration 14, loss = 0.30820472\n",
      "Iteration 15, loss = 0.29953213\n",
      "Iteration 42, loss = 0.19686994\n",
      "Iteration 16, loss = 0.29578182\n",
      "Iteration 43, loss = 0.19168782\n",
      "Iteration 17, loss = 0.28643855\n",
      "Iteration 44, loss = 0.19109588\n",
      "Iteration 18, loss = 0.27867342\n",
      "Iteration 45, loss = 0.17957265\n",
      "Iteration 46, loss = 0.18138103\n",
      "Iteration 47, loss = 0.17521360\n",
      "Iteration 19, loss = 0.27537063\n",
      "Iteration 48, loss = 0.17206387\n",
      "Iteration 20, loss = 0.26637500\n",
      "Iteration 49, loss = 0.16979560\n",
      "Iteration 21, loss = 0.26117823\n",
      "Iteration 22, loss = 0.25524121\n",
      "Iteration 50, loss = 0.16685502\n",
      "Iteration 51, loss = 0.16545551\n",
      "Iteration 23, loss = 0.24961116\n",
      "Iteration 52, loss = 0.16318887\n",
      "Iteration 24, loss = 0.24618062\n",
      "Iteration 53, loss = 0.15904798\n",
      "Iteration 25, loss = 0.24623809\n",
      "Iteration 54, loss = 0.15782113\n",
      "Iteration 55, loss = 0.15540866\n",
      "Iteration 56, loss = 0.15163948\n",
      "Iteration 57, loss = 0.15152937\n",
      "Iteration 26, loss = 0.23639690\n",
      "Iteration 58, loss = 0.14898870\n",
      "Iteration 27, loss = 0.23389585\n",
      "Iteration 59, loss = 0.14746591\n",
      "Iteration 28, loss = 0.22804992\n",
      "Iteration 60, loss = 0.14347439\n",
      "Iteration 29, loss = 0.22758933\n",
      "Iteration 61, loss = 0.14271678\n",
      "Iteration 30, loss = 0.22066051\n",
      "Iteration 62, loss = 0.14081679\n",
      "Iteration 31, loss = 0.21671427\n",
      "Iteration 32, loss = 0.20919730\n",
      "Iteration 63, loss = 0.14820243\n",
      "Iteration 33, loss = 0.20889079\n",
      "Iteration 64, loss = 0.14973980\n",
      "Iteration 34, loss = 0.20290064\n",
      "Iteration 65, loss = 0.14172604\n",
      "Iteration 66, loss = 0.13568709\n",
      "Iteration 35, loss = 0.19977876\n",
      "Iteration 67, loss = 0.13285400\n",
      "Iteration 68, loss = 0.13627096\n",
      "Iteration 69, loss = 0.14514201\n",
      "Iteration 36, loss = 0.19721709\n",
      "Iteration 37, loss = 0.19540498\n",
      "Iteration 38, loss = 0.18909714\n",
      "Iteration 70, loss = 0.14501061\n",
      "Iteration 71, loss = 0.13241091\n",
      "Iteration 72, loss = 0.12594387\n",
      "Iteration 39, loss = 0.18728355\n",
      "Iteration 73, loss = 0.13243116\n",
      "Iteration 40, loss = 0.18635717\n",
      "Iteration 41, loss = 0.18149748\n",
      "Iteration 42, loss = 0.17707285\n",
      "Iteration 43, loss = 0.17366262\n",
      "Iteration 74, loss = 0.12687583\n",
      "Iteration 44, loss = 0.17072216\n",
      "Iteration 75, loss = 0.11935177\n",
      "Iteration 76, loss = 0.11802055\n",
      "Iteration 45, loss = 0.17077667\n",
      "Iteration 77, loss = 0.11785639\n",
      "Iteration 46, loss = 0.16329461\n",
      "Iteration 47, loss = 0.16242075\n",
      "Iteration 48, loss = 0.16301274\n",
      "Iteration 78, loss = 0.11495382\n",
      "Iteration 79, loss = 0.11324581\n",
      "Iteration 49, loss = 0.15715547\n",
      "Iteration 50, loss = 0.15501641\n",
      "Iteration 51, loss = 0.15457770\n",
      "Iteration 52, loss = 0.15083117\n",
      "Iteration 53, loss = 0.14823884\n",
      "Iteration 80, loss = 0.11059203\n",
      "Iteration 81, loss = 0.11189389\n",
      "Iteration 54, loss = 0.14579196\n",
      "Iteration 82, loss = 0.11055441\n",
      "Iteration 55, loss = 0.14377935\n",
      "Iteration 83, loss = 0.10888930\n",
      "Iteration 84, loss = 0.10666123\n",
      "Iteration 85, loss = 0.10697395\n",
      "Iteration 86, loss = 0.11357599\n",
      "Iteration 56, loss = 0.14125823\n",
      "Iteration 87, loss = 0.10389477\n",
      "Iteration 57, loss = 0.13930020\n",
      "Iteration 58, loss = 0.13721179\n",
      "Iteration 88, loss = 0.10023609\n",
      "Iteration 59, loss = 0.13663962\n",
      "Iteration 89, loss = 0.10093020\n",
      "Iteration 60, loss = 0.13509789\n",
      "Iteration 90, loss = 0.10021322\n",
      "Iteration 61, loss = 0.13254206\n",
      "Iteration 91, loss = 0.09806690\n",
      "Iteration 92, loss = 0.09691776\n",
      "Iteration 62, loss = 0.12965519\n",
      "Iteration 93, loss = 0.09710643\n",
      "Iteration 63, loss = 0.12933005\n",
      "Iteration 94, loss = 0.09597215\n",
      "Iteration 95, loss = 0.09625880\n",
      "Iteration 64, loss = 0.12687602\n",
      "Iteration 65, loss = 0.12695239\n",
      "Iteration 96, loss = 0.09464429\n",
      "Iteration 66, loss = 0.12685413\n",
      "Iteration 97, loss = 0.09539288\n",
      "Iteration 67, loss = 0.12339177\n",
      "Iteration 98, loss = 0.09418542\n",
      "Iteration 68, loss = 0.12134324\n",
      "Iteration 99, loss = 0.08922481\n",
      "Iteration 69, loss = 0.12110728\n",
      "Iteration 100, loss = 0.08833606\n",
      "Iteration 70, loss = 0.11994597\n",
      "Iteration 71, loss = 0.11855448\n",
      "Iteration 101, loss = 0.08693541\n",
      "Iteration 72, loss = 0.12191257\n",
      "Iteration 102, loss = 0.08524666\n",
      "Iteration 73, loss = 0.11750784\n",
      "Iteration 74, loss = 0.11460581\n",
      "Iteration 103, loss = 0.08436805\n",
      "Iteration 104, loss = 0.08221674\n",
      "Iteration 75, loss = 0.11066355\n",
      "Iteration 105, loss = 0.08417280\n",
      "Iteration 106, loss = 0.08298475\n",
      "Iteration 76, loss = 0.11302087\n",
      "Iteration 107, loss = 0.07976807\n",
      "Iteration 108, loss = 0.07899146\n",
      "Iteration 77, loss = 0.10983241\n",
      "Iteration 109, loss = 0.07913528\n",
      "Iteration 78, loss = 0.10751067\n",
      "Iteration 110, loss = 0.08291792\n",
      "Iteration 79, loss = 0.10613268\n",
      "Iteration 111, loss = 0.08404278\n",
      "Iteration 80, loss = 0.10672246\n",
      "Iteration 112, loss = 0.08122171\n",
      "Iteration 81, loss = 0.10580985\n",
      "Iteration 113, loss = 0.07701544\n",
      "Iteration 82, loss = 0.10196748\n",
      "Iteration 114, loss = 0.07598715\n",
      "Iteration 83, loss = 0.10324947\n",
      "Iteration 84, loss = 0.10458280\n",
      "Iteration 115, loss = 0.07448867\n",
      "Iteration 116, loss = 0.07251863\n",
      "Iteration 85, loss = 0.09948137\n",
      "Iteration 117, loss = 0.07378932\n",
      "Iteration 86, loss = 0.10197218\n",
      "Iteration 118, loss = 0.07120402\n",
      "Iteration 87, loss = 0.09783423\n",
      "Iteration 88, loss = 0.09558539\n",
      "Iteration 119, loss = 0.06953686\n",
      "Iteration 89, loss = 0.09418123\n",
      "Iteration 90, loss = 0.09261799\n",
      "Iteration 91, loss = 0.09313602\n",
      "Iteration 92, loss = 0.09514036\n",
      "Iteration 120, loss = 0.06868603\n",
      "Iteration 93, loss = 0.09708837\n",
      "Iteration 121, loss = 0.07005799\n",
      "Iteration 122, loss = 0.06932890\n",
      "Iteration 94, loss = 0.09076515\n",
      "Iteration 95, loss = 0.09230780\n",
      "Iteration 96, loss = 0.08756108\n",
      "Iteration 97, loss = 0.08644673\n",
      "Iteration 123, loss = 0.06924197\n",
      "Iteration 124, loss = 0.06701155\n",
      "Iteration 125, loss = 0.06526183\n",
      "Iteration 98, loss = 0.08702481\n",
      "Iteration 126, loss = 0.06413965\n",
      "Iteration 99, loss = 0.08457639\n",
      "Iteration 127, loss = 0.06666714\n",
      "Iteration 128, loss = 0.06423136\n",
      "Iteration 100, loss = 0.08582814\n",
      "Iteration 129, loss = 0.06544761\n",
      "Iteration 101, loss = 0.08442614\n",
      "Iteration 130, loss = 0.06970257\n",
      "Iteration 131, loss = 0.07007570\n",
      "Iteration 132, loss = 0.06453215\n",
      "Iteration 102, loss = 0.08198065\n",
      "Iteration 103, loss = 0.08158965\n",
      "Iteration 104, loss = 0.08445751\n",
      "Iteration 133, loss = 0.06282191\n",
      "Iteration 105, loss = 0.07847068\n",
      "Iteration 134, loss = 0.06317561\n",
      "Iteration 135, loss = 0.06533863\n",
      "Iteration 136, loss = 0.06324167\n",
      "Iteration 137, loss = 0.06296030\n",
      "Iteration 106, loss = 0.08000319\n",
      "Iteration 138, loss = 0.05543389\n",
      "Iteration 139, loss = 0.06012785\n",
      "Iteration 140, loss = 0.05432173\n",
      "Iteration 107, loss = 0.07648246\n",
      "Iteration 141, loss = 0.06157358\n",
      "Iteration 108, loss = 0.08028568\n",
      "Iteration 142, loss = 0.06136310\n",
      "Iteration 109, loss = 0.07814125\n",
      "Iteration 143, loss = 0.05528331\n",
      "Iteration 110, loss = 0.07449623\n",
      "Iteration 144, loss = 0.05278142\n",
      "Iteration 111, loss = 0.07810423\n",
      "Iteration 145, loss = 0.05397141\n",
      "Iteration 112, loss = 0.07527537\n",
      "Iteration 146, loss = 0.05446285\n",
      "Iteration 113, loss = 0.07332987\n",
      "Iteration 147, loss = 0.05388449\n",
      "Iteration 114, loss = 0.07452042\n",
      "Iteration 115, loss = 0.07173316\n",
      "Iteration 116, loss = 0.07034170\n",
      "Iteration 148, loss = 0.05164463\n",
      "Iteration 117, loss = 0.07226719\n",
      "Iteration 149, loss = 0.05160338\n",
      "Iteration 118, loss = 0.07549421\n",
      "Iteration 119, loss = 0.07284740\n",
      "Iteration 120, loss = 0.06935262\n",
      "Iteration 150, loss = 0.04920975\n",
      "Iteration 121, loss = 0.06873837\n",
      "Iteration 151, loss = 0.05383648\n",
      "Iteration 122, loss = 0.07128114\n",
      "Iteration 152, loss = 0.05197235\n",
      "Iteration 153, loss = 0.05194837\n",
      "Iteration 123, loss = 0.06861553\n",
      "Iteration 154, loss = 0.04940816\n",
      "Iteration 124, loss = 0.06542432\n",
      "Iteration 125, loss = 0.06468830\n",
      "Iteration 126, loss = 0.06261432\n",
      "Iteration 155, loss = 0.04940800\n",
      "Iteration 156, loss = 0.04922994\n",
      "Iteration 127, loss = 0.06363998\n",
      "Iteration 157, loss = 0.04679910\n",
      "Iteration 128, loss = 0.06173886\n",
      "Iteration 158, loss = 0.04644208\n",
      "Iteration 129, loss = 0.06339380\n",
      "Iteration 130, loss = 0.06529313\n",
      "Iteration 159, loss = 0.04324926\n",
      "Iteration 131, loss = 0.06203961\n",
      "Iteration 160, loss = 0.04318418\n",
      "Iteration 161, loss = 0.04831237\n",
      "Iteration 132, loss = 0.06592342\n",
      "Iteration 162, loss = 0.04525297Iteration 133, loss = 0.06352569\n",
      "\n",
      "Iteration 134, loss = 0.06915605\n",
      "Iteration 135, loss = 0.06876137\n",
      "Iteration 163, loss = 0.05463621\n",
      "Iteration 164, loss = 0.05247905\n",
      "Iteration 165, loss = 0.05663362\n",
      "Iteration 136, loss = 0.06452563\n",
      "Iteration 137, loss = 0.07580759\n",
      "Iteration 138, loss = 0.06839770\n",
      "Iteration 139, loss = 0.06322086\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 166, loss = 0.04997506\n",
      "Iteration 167, loss = 0.04304622\n",
      "Iteration 168, loss = 0.04119271\n",
      "Iteration 169, loss = 0.04516837\n",
      "Iteration 170, loss = 0.04718225\n",
      "Iteration 171, loss = 0.04102067\n",
      "Iteration 172, loss = 0.04104527\n",
      "Iteration 173, loss = 0.04509049\n",
      "Iteration 174, loss = 0.03927389\n",
      "Iteration 175, loss = 0.04300834\n",
      "Iteration 176, loss = 0.04279588\n",
      "Iteration 177, loss = 0.04200954\n",
      "Iteration 178, loss = 0.04531798\n",
      "Iteration 179, loss = 0.04374158\n",
      "Iteration 180, loss = 0.03714552\n",
      "Iteration 181, loss = 0.04036710\n",
      "Iteration 182, loss = 0.04605313\n",
      "Iteration 183, loss = 0.04546411\n",
      "Iteration 1, loss = 1.16534871\n",
      "Iteration 184, loss = 0.04879702\n",
      "Iteration 2, loss = 0.82209850\n",
      "Iteration 185, loss = 0.04631665\n",
      "Iteration 3, loss = 0.64247519\n",
      "Iteration 186, loss = 0.04958079\n",
      "Iteration 4, loss = 0.55022935\n",
      "Iteration 5, loss = 0.49037626\n",
      "Iteration 6, loss = 0.44758396\n",
      "Iteration 7, loss = 0.41805601\n",
      "Iteration 187, loss = 0.04073093\n",
      "Iteration 8, loss = 0.39576023\n",
      "Iteration 9, loss = 0.37443814\n",
      "Iteration 10, loss = 0.35877200\n",
      "Iteration 188, loss = 0.03894149\n",
      "Iteration 189, loss = 0.03799581\n",
      "Iteration 190, loss = 0.03300920\n",
      "Iteration 11, loss = 0.34477581\n",
      "Iteration 191, loss = 0.03547070\n",
      "Iteration 12, loss = 0.33571450\n",
      "Iteration 192, loss = 0.03211335\n",
      "Iteration 193, loss = 0.03264284\n",
      "Iteration 194, loss = 0.03358662\n",
      "Iteration 13, loss = 0.32259848\n",
      "Iteration 195, loss = 0.03348006\n",
      "Iteration 14, loss = 0.31284550\n",
      "Iteration 15, loss = 0.30486306\n",
      "Iteration 196, loss = 0.03059877\n",
      "Iteration 16, loss = 0.29564501\n",
      "Iteration 197, loss = 0.03038514\n",
      "Iteration 17, loss = 0.28823125\n",
      "Iteration 198, loss = 0.02870348\n",
      "Iteration 199, loss = 0.02958744\n",
      "Iteration 200, loss = 0.02889038\n",
      "Iteration 18, loss = 0.28193836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.27323430\n",
      "Iteration 20, loss = 0.26704649\n",
      "Iteration 21, loss = 0.26386219\n",
      "Iteration 22, loss = 0.25497534\n",
      "Iteration 23, loss = 0.25127361\n",
      "Iteration 1, loss = 1.22805147\n",
      "Iteration 24, loss = 0.24423968\n",
      "Iteration 2, loss = 0.84108521\n",
      "Iteration 3, loss = 0.64785124\n",
      "Iteration 25, loss = 0.24094632\n",
      "Iteration 26, loss = 0.23535153\n",
      "Iteration 4, loss = 0.55433593\n",
      "Iteration 27, loss = 0.22917199\n",
      "Iteration 5, loss = 0.48420361\n",
      "Iteration 6, loss = 0.44350132\n",
      "Iteration 28, loss = 0.22564679\n",
      "Iteration 7, loss = 0.40974242\n",
      "Iteration 29, loss = 0.22061434\n",
      "Iteration 8, loss = 0.38678290\n",
      "Iteration 30, loss = 0.21574119\n",
      "Iteration 9, loss = 0.36799915\n",
      "Iteration 31, loss = 0.21189489\n",
      "Iteration 10, loss = 0.35354704\n",
      "Iteration 32, loss = 0.20913446\n",
      "Iteration 11, loss = 0.33821682\n",
      "Iteration 33, loss = 0.20286186\n",
      "Iteration 12, loss = 0.32721537\n",
      "Iteration 34, loss = 0.20149840\n",
      "Iteration 13, loss = 0.31424666\n",
      "Iteration 35, loss = 0.19416328\n",
      "Iteration 14, loss = 0.30463517\n",
      "Iteration 36, loss = 0.19340536\n",
      "Iteration 15, loss = 0.29587884\n",
      "Iteration 37, loss = 0.18785373\n",
      "Iteration 16, loss = 0.28818206\n",
      "Iteration 38, loss = 0.18540549\n",
      "Iteration 39, loss = 0.18041201\n",
      "Iteration 17, loss = 0.27876821\n",
      "Iteration 40, loss = 0.17638626\n",
      "Iteration 18, loss = 0.27386831\n",
      "Iteration 19, loss = 0.26600775\n",
      "Iteration 41, loss = 0.17376838\n",
      "Iteration 20, loss = 0.25925376\n",
      "Iteration 21, loss = 0.25283395\n",
      "Iteration 42, loss = 0.17147912\n",
      "Iteration 22, loss = 0.24825271\n",
      "Iteration 43, loss = 0.16510305\n",
      "Iteration 44, loss = 0.16438773\n",
      "Iteration 23, loss = 0.24263628\n",
      "Iteration 45, loss = 0.16116991\n",
      "Iteration 24, loss = 0.23833844\n",
      "Iteration 46, loss = 0.15868580\n",
      "Iteration 25, loss = 0.23279337\n",
      "Iteration 26, loss = 0.22589899\n",
      "Iteration 47, loss = 0.15446453\n",
      "Iteration 27, loss = 0.22372853\n",
      "Iteration 48, loss = 0.15286812\n",
      "Iteration 28, loss = 0.21797311\n",
      "Iteration 49, loss = 0.15172401\n",
      "Iteration 29, loss = 0.21311576\n",
      "Iteration 50, loss = 0.14739155\n",
      "Iteration 30, loss = 0.21007017\n",
      "Iteration 51, loss = 0.14717898\n",
      "Iteration 31, loss = 0.20502565\n",
      "Iteration 52, loss = 0.14196746\n",
      "Iteration 53, loss = 0.13750218\n",
      "Iteration 32, loss = 0.20428396\n",
      "Iteration 54, loss = 0.13913221\n",
      "Iteration 33, loss = 0.19805328\n",
      "Iteration 55, loss = 0.13402204\n",
      "Iteration 56, loss = 0.13446475\n",
      "Iteration 34, loss = 0.19418665\n",
      "Iteration 57, loss = 0.12922014\n",
      "Iteration 58, loss = 0.12697115\n",
      "Iteration 35, loss = 0.19352107\n",
      "Iteration 59, loss = 0.12412667\n",
      "Iteration 60, loss = 0.12203755\n",
      "Iteration 36, loss = 0.18852217\n",
      "Iteration 37, loss = 0.18442138\n",
      "Iteration 38, loss = 0.18534144\n",
      "Iteration 39, loss = 0.17952937\n",
      "Iteration 61, loss = 0.12122501\n",
      "Iteration 62, loss = 0.11713689\n",
      "Iteration 63, loss = 0.11677789\n",
      "Iteration 40, loss = 0.17490524\n",
      "Iteration 64, loss = 0.11488230\n",
      "Iteration 65, loss = 0.11267697\n",
      "Iteration 66, loss = 0.10985863\n",
      "Iteration 41, loss = 0.17108409\n",
      "Iteration 67, loss = 0.10660364\n",
      "Iteration 42, loss = 0.16926771\n",
      "Iteration 68, loss = 0.10543000\n",
      "Iteration 69, loss = 0.10386128\n",
      "Iteration 43, loss = 0.17099069\n",
      "Iteration 70, loss = 0.10586767\n",
      "Iteration 44, loss = 0.16461633\n",
      "Iteration 45, loss = 0.16145016\n",
      "Iteration 71, loss = 0.10662641\n",
      "Iteration 46, loss = 0.16049210\n",
      "Iteration 47, loss = 0.15537480\n",
      "Iteration 72, loss = 0.10079934\n",
      "Iteration 48, loss = 0.15309900\n",
      "Iteration 73, loss = 0.10100766\n",
      "Iteration 49, loss = 0.15837172\n",
      "Iteration 74, loss = 0.10575919\n",
      "Iteration 50, loss = 0.15328781\n",
      "Iteration 51, loss = 0.14740475\n",
      "Iteration 75, loss = 0.09639570\n",
      "Iteration 52, loss = 0.14855104\n",
      "Iteration 53, loss = 0.14589130\n",
      "Iteration 54, loss = 0.14006595\n",
      "Iteration 55, loss = 0.14032430\n",
      "Iteration 76, loss = 0.09250902\n",
      "Iteration 56, loss = 0.13601907\n",
      "Iteration 57, loss = 0.13489678\n",
      "Iteration 58, loss = 0.13253018\n",
      "Iteration 77, loss = 0.09694859\n",
      "Iteration 59, loss = 0.13143554\n",
      "Iteration 78, loss = 0.09121862\n",
      "Iteration 79, loss = 0.09088472\n",
      "Iteration 80, loss = 0.08954384\n",
      "Iteration 81, loss = 0.08511648\n",
      "Iteration 60, loss = 0.12906227\n",
      "Iteration 82, loss = 0.08506974\n",
      "Iteration 61, loss = 0.12769731\n",
      "Iteration 83, loss = 0.08273338\n",
      "Iteration 62, loss = 0.12768963\n",
      "Iteration 84, loss = 0.08432974\n",
      "Iteration 63, loss = 0.12575830\n",
      "Iteration 85, loss = 0.08091742\n",
      "Iteration 64, loss = 0.12593283\n",
      "Iteration 86, loss = 0.07972604\n",
      "Iteration 65, loss = 0.12345949\n",
      "Iteration 87, loss = 0.07863491\n",
      "Iteration 66, loss = 0.11952583\n",
      "Iteration 88, loss = 0.07703182\n",
      "Iteration 67, loss = 0.12037661\n",
      "Iteration 89, loss = 0.07569670\n",
      "Iteration 68, loss = 0.12270362\n",
      "Iteration 69, loss = 0.11630309\n",
      "Iteration 70, loss = 0.11560407\n",
      "Iteration 90, loss = 0.07641972\n",
      "Iteration 71, loss = 0.11533973\n",
      "Iteration 91, loss = 0.07509583\n",
      "Iteration 92, loss = 0.07749446\n",
      "Iteration 93, loss = 0.07599049\n",
      "Iteration 72, loss = 0.11193730\n",
      "Iteration 73, loss = 0.11036173\n",
      "Iteration 94, loss = 0.06997526\n",
      "Iteration 74, loss = 0.10974115\n",
      "Iteration 95, loss = 0.07019737\n",
      "Iteration 75, loss = 0.10722630\n",
      "Iteration 96, loss = 0.06883447\n",
      "Iteration 76, loss = 0.11062921\n",
      "Iteration 77, loss = 0.11089901\n",
      "Iteration 97, loss = 0.06680795\n",
      "Iteration 78, loss = 0.10690461\n",
      "Iteration 98, loss = 0.06547788\n",
      "Iteration 79, loss = 0.10204815\n",
      "Iteration 99, loss = 0.06720001\n",
      "Iteration 80, loss = 0.10256137\n",
      "Iteration 81, loss = 0.10171585\n",
      "Iteration 100, loss = 0.06639681\n",
      "Iteration 82, loss = 0.10012613\n",
      "Iteration 83, loss = 0.09734016\n",
      "Iteration 101, loss = 0.06498375\n",
      "Iteration 102, loss = 0.06888586\n",
      "Iteration 103, loss = 0.06228840\n",
      "Iteration 84, loss = 0.09815991\n",
      "Iteration 104, loss = 0.06257070\n",
      "Iteration 85, loss = 0.09745250\n",
      "Iteration 105, loss = 0.05847845\n",
      "Iteration 86, loss = 0.09785599\n",
      "Iteration 106, loss = 0.06278135\n",
      "Iteration 87, loss = 0.09318716\n",
      "Iteration 107, loss = 0.05891896\n",
      "Iteration 88, loss = 0.09627075\n",
      "Iteration 108, loss = 0.06312359\n",
      "Iteration 89, loss = 0.09499871\n",
      "Iteration 90, loss = 0.09631380\n",
      "Iteration 91, loss = 0.09464191\n",
      "Iteration 109, loss = 0.05939106\n",
      "Iteration 92, loss = 0.09996068\n",
      "Iteration 110, loss = 0.05926290\n",
      "Iteration 93, loss = 0.09843534\n",
      "Iteration 111, loss = 0.05441713\n",
      "Iteration 94, loss = 0.09368672\n",
      "Iteration 112, loss = 0.05383541\n",
      "Iteration 95, loss = 0.09599456\n",
      "Iteration 113, loss = 0.05751561\n",
      "Iteration 96, loss = 0.09054048\n",
      "Iteration 114, loss = 0.05926239\n",
      "Iteration 97, loss = 0.08389207\n",
      "Iteration 115, loss = 0.05877002\n",
      "Iteration 98, loss = 0.08800082\n",
      "Iteration 99, loss = 0.08316968\n",
      "Iteration 100, loss = 0.08342257\n",
      "Iteration 116, loss = 0.05952076\n",
      "Iteration 101, loss = 0.08389449\n",
      "Iteration 102, loss = 0.08192368\n",
      "Iteration 117, loss = 0.05329107\n",
      "Iteration 103, loss = 0.08147744\n",
      "Iteration 104, loss = 0.08095636\n",
      "Iteration 118, loss = 0.05111876\n",
      "Iteration 119, loss = 0.05272575\n",
      "Iteration 105, loss = 0.08112664\n",
      "Iteration 120, loss = 0.05300180\n",
      "Iteration 106, loss = 0.07844432\n",
      "Iteration 107, loss = 0.08192673\n",
      "Iteration 121, loss = 0.05092931\n",
      "Iteration 108, loss = 0.08470542\n",
      "Iteration 122, loss = 0.05084447\n",
      "Iteration 109, loss = 0.07961919\n",
      "Iteration 123, loss = 0.04566479\n",
      "Iteration 110, loss = 0.07790214\n",
      "Iteration 124, loss = 0.04717381\n",
      "Iteration 111, loss = 0.07854340\n",
      "Iteration 125, loss = 0.04713450\n",
      "Iteration 126, loss = 0.04458914\n",
      "Iteration 127, loss = 0.04411635\n",
      "Iteration 128, loss = 0.04397357\n",
      "Iteration 112, loss = 0.07683687\n",
      "Iteration 129, loss = 0.04227857\n",
      "Iteration 130, loss = 0.04207688\n",
      "Iteration 113, loss = 0.07354788\n",
      "Iteration 131, loss = 0.04193054\n",
      "Iteration 114, loss = 0.07165908\n",
      "Iteration 115, loss = 0.07487316\n",
      "Iteration 132, loss = 0.04014147\n",
      "Iteration 133, loss = 0.04036851\n",
      "Iteration 134, loss = 0.03962998\n",
      "Iteration 116, loss = 0.07314849\n",
      "Iteration 117, loss = 0.06957824\n",
      "Iteration 135, loss = 0.03987909\n",
      "Iteration 118, loss = 0.06931010\n",
      "Iteration 136, loss = 0.03829751\n",
      "Iteration 119, loss = 0.07023088\n",
      "Iteration 137, loss = 0.03789767\n",
      "Iteration 120, loss = 0.07032947\n",
      "Iteration 138, loss = 0.03839228\n",
      "Iteration 121, loss = 0.06759687\n",
      "Iteration 139, loss = 0.03784277\n",
      "Iteration 122, loss = 0.06711678\n",
      "Iteration 123, loss = 0.07091487\n",
      "Iteration 140, loss = 0.03689034\n",
      "Iteration 124, loss = 0.07721347\n",
      "Iteration 141, loss = 0.03588953\n",
      "Iteration 125, loss = 0.07124294\n",
      "Iteration 142, loss = 0.03792314\n",
      "Iteration 143, loss = 0.03774217\n",
      "Iteration 126, loss = 0.06556990\n",
      "Iteration 144, loss = 0.03834424\n",
      "Iteration 127, loss = 0.06614018\n",
      "Iteration 128, loss = 0.06250413\n",
      "Iteration 145, loss = 0.03869799\n",
      "Iteration 129, loss = 0.06401731\n",
      "Iteration 146, loss = 0.04083403\n",
      "Iteration 147, loss = 0.03748643\n",
      "Iteration 130, loss = 0.06251905\n",
      "Iteration 131, loss = 0.06036536\n",
      "Iteration 148, loss = 0.03573678\n",
      "Iteration 132, loss = 0.05937719\n",
      "Iteration 149, loss = 0.03691397\n",
      "Iteration 133, loss = 0.06186573\n",
      "Iteration 150, loss = 0.03451183\n",
      "Iteration 151, loss = 0.03341545\n",
      "Iteration 134, loss = 0.06177835\n",
      "Iteration 135, loss = 0.06001556\n",
      "Iteration 152, loss = 0.03301669\n",
      "Iteration 136, loss = 0.05794135\n",
      "Iteration 153, loss = 0.03222174\n",
      "Iteration 154, loss = 0.03223633\n",
      "Iteration 137, loss = 0.05763981\n",
      "Iteration 155, loss = 0.02997221\n",
      "Iteration 138, loss = 0.05653069\n",
      "Iteration 139, loss = 0.05840833\n",
      "Iteration 156, loss = 0.03014054\n",
      "Iteration 140, loss = 0.05793454\n",
      "Iteration 157, loss = 0.03032549\n",
      "Iteration 158, loss = 0.02930532\n",
      "Iteration 141, loss = 0.05666328\n",
      "Iteration 159, loss = 0.02870268Iteration 142, loss = 0.06269744\n",
      "\n",
      "Iteration 143, loss = 0.06073918\n",
      "Iteration 160, loss = 0.03082422\n",
      "Iteration 144, loss = 0.05851935\n",
      "Iteration 161, loss = 0.03062338\n",
      "Iteration 162, loss = 0.03429044\n",
      "Iteration 145, loss = 0.05800797\n",
      "Iteration 163, loss = 0.03336868\n",
      "Iteration 146, loss = 0.05998876\n",
      "Iteration 147, loss = 0.06388536\n",
      "Iteration 164, loss = 0.03932495\n",
      "Iteration 148, loss = 0.06128669\n",
      "Iteration 165, loss = 0.02986347\n",
      "Iteration 166, loss = 0.03186517\n",
      "Iteration 167, loss = 0.02642133\n",
      "Iteration 149, loss = 0.05474166\n",
      "Iteration 150, loss = 0.05376837\n",
      "Iteration 168, loss = 0.02751679\n",
      "Iteration 169, loss = 0.02545389\n",
      "Iteration 151, loss = 0.05304410\n",
      "Iteration 170, loss = 0.02478036\n",
      "Iteration 171, loss = 0.02413140\n",
      "Iteration 172, loss = 0.02417901\n",
      "Iteration 173, loss = 0.02422307\n",
      "Iteration 152, loss = 0.05294138\n",
      "Iteration 174, loss = 0.02451924\n",
      "Iteration 153, loss = 0.05227910\n",
      "Iteration 154, loss = 0.05128431\n",
      "Iteration 175, loss = 0.02513552\n",
      "Iteration 176, loss = 0.02395947\n",
      "Iteration 177, loss = 0.02364111\n",
      "Iteration 178, loss = 0.02283542\n",
      "Iteration 155, loss = 0.04879246\n",
      "Iteration 179, loss = 0.02285458\n",
      "Iteration 180, loss = 0.02166531\n",
      "Iteration 156, loss = 0.04851972\n",
      "Iteration 181, loss = 0.02125354\n",
      "Iteration 182, loss = 0.02144590\n",
      "Iteration 183, loss = 0.02101126\n",
      "Iteration 157, loss = 0.05023444\n",
      "Iteration 184, loss = 0.02094157\n",
      "Iteration 158, loss = 0.04863598\n",
      "Iteration 185, loss = 0.02019436\n",
      "Iteration 159, loss = 0.04926408\n",
      "Iteration 186, loss = 0.02038522\n",
      "Iteration 160, loss = 0.05220739\n",
      "Iteration 187, loss = 0.01996646\n",
      "Iteration 161, loss = 0.05018339\n",
      "Iteration 162, loss = 0.04641886\n",
      "Iteration 188, loss = 0.01924444\n",
      "Iteration 163, loss = 0.04877480\n",
      "Iteration 189, loss = 0.01934647\n",
      "Iteration 164, loss = 0.04613030\n",
      "Iteration 190, loss = 0.02058689\n",
      "Iteration 165, loss = 0.04569269\n",
      "Iteration 191, loss = 0.02024042\n",
      "Iteration 192, loss = 0.02207246\n",
      "Iteration 166, loss = 0.04297430\n",
      "Iteration 193, loss = 0.01905272\n",
      "Iteration 194, loss = 0.01907363\n",
      "Iteration 167, loss = 0.04395795\n",
      "Iteration 168, loss = 0.04327549\n",
      "Iteration 195, loss = 0.01875003\n",
      "Iteration 169, loss = 0.04229378\n",
      "Iteration 196, loss = 0.01768739\n",
      "Iteration 170, loss = 0.04289362\n",
      "Iteration 197, loss = 0.01739060\n",
      "Iteration 171, loss = 0.04244875\n",
      "Iteration 198, loss = 0.01726804\n",
      "Iteration 172, loss = 0.04040585\n",
      "Iteration 199, loss = 0.01745961\n",
      "Iteration 173, loss = 0.03976153\n",
      "Iteration 174, loss = 0.03991637\n",
      "Iteration 175, loss = 0.04117329\n",
      "Iteration 200, loss = 0.01926627\n",
      "Iteration 176, loss = 0.03957046\n",
      "Iteration 177, loss = 0.03975419\n",
      "Iteration 178, loss = 0.04274079\n",
      "Iteration 179, loss = 0.03954103\n",
      "Iteration 180, loss = 0.04103409\n",
      "Iteration 181, loss = 0.03896061\n",
      "Iteration 182, loss = 0.03746152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 183, loss = 0.03746982\n",
      "Iteration 184, loss = 0.03924258\n",
      "Iteration 185, loss = 0.04016677\n",
      "Iteration 186, loss = 0.03794728\n",
      "Iteration 187, loss = 0.03804491\n",
      "Iteration 1, loss = 0.94169897\n",
      "Iteration 188, loss = 0.03841217\n",
      "Iteration 2, loss = 0.70433693\n",
      "Iteration 3, loss = 0.57207190\n",
      "Iteration 4, loss = 0.49481866\n",
      "Iteration 189, loss = 0.04408863\n",
      "Iteration 5, loss = 0.44816865\n",
      "Iteration 190, loss = 0.03924969\n",
      "Iteration 6, loss = 0.41405596\n",
      "Iteration 7, loss = 0.38875558\n",
      "Iteration 8, loss = 0.37079258\n",
      "Iteration 9, loss = 0.35274606\n",
      "Iteration 191, loss = 0.03846016\n",
      "Iteration 10, loss = 0.34065809\n",
      "Iteration 192, loss = 0.03849363\n",
      "Iteration 193, loss = 0.04041630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.32632323\n",
      "Iteration 12, loss = 0.31715782\n",
      "Iteration 13, loss = 0.30738396\n",
      "Iteration 14, loss = 0.30099589\n",
      "Iteration 15, loss = 0.29299252\n",
      "Iteration 16, loss = 0.28467463\n",
      "Iteration 17, loss = 0.27737255\n",
      "Iteration 18, loss = 0.27061776\n",
      "Iteration 19, loss = 0.26338010\n",
      "Iteration 20, loss = 0.26120239\n",
      "Iteration 21, loss = 0.25504130\n",
      "Iteration 22, loss = 0.24816383\n",
      "Iteration 23, loss = 0.24301925\n",
      "Iteration 1, loss = 0.92265067\n",
      "Iteration 24, loss = 0.23732363\n",
      "Iteration 25, loss = 0.23527057\n",
      "Iteration 2, loss = 0.63120279\n",
      "Iteration 26, loss = 0.23243574\n",
      "Iteration 3, loss = 0.52378911\n",
      "Iteration 27, loss = 0.22250238\n",
      "Iteration 4, loss = 0.46690050\n",
      "Iteration 28, loss = 0.22283189\n",
      "Iteration 5, loss = 0.42765132\n",
      "Iteration 29, loss = 0.22112072\n",
      "Iteration 6, loss = 0.39932317\n",
      "Iteration 30, loss = 0.21357905\n",
      "Iteration 7, loss = 0.37911265\n",
      "Iteration 31, loss = 0.21171997\n",
      "Iteration 8, loss = 0.36510393\n",
      "Iteration 32, loss = 0.20496595\n",
      "Iteration 9, loss = 0.35614830\n",
      "Iteration 10, loss = 0.33727479\n",
      "Iteration 11, loss = 0.32997584\n",
      "Iteration 12, loss = 0.31871152\n",
      "Iteration 33, loss = 0.20216397\n",
      "Iteration 34, loss = 0.19867280\n",
      "Iteration 35, loss = 0.19487755\n",
      "Iteration 13, loss = 0.30913373\n",
      "Iteration 36, loss = 0.19041368\n",
      "Iteration 14, loss = 0.30558521\n",
      "Iteration 37, loss = 0.18934241\n",
      "Iteration 15, loss = 0.29589763\n",
      "Iteration 38, loss = 0.18329474\n",
      "Iteration 16, loss = 0.28714243\n",
      "Iteration 39, loss = 0.18379724\n",
      "Iteration 17, loss = 0.28059584\n",
      "Iteration 40, loss = 0.18149038\n",
      "Iteration 41, loss = 0.17569933\n",
      "Iteration 18, loss = 0.27330794\n",
      "Iteration 42, loss = 0.17352305\n",
      "Iteration 19, loss = 0.26617185\n",
      "Iteration 20, loss = 0.26223961\n",
      "Iteration 43, loss = 0.17191752\n",
      "Iteration 44, loss = 0.16755727\n",
      "Iteration 45, loss = 0.16547511\n",
      "Iteration 21, loss = 0.25691150\n",
      "Iteration 22, loss = 0.24929264\n",
      "Iteration 46, loss = 0.16121807\n",
      "Iteration 23, loss = 0.24629293\n",
      "Iteration 47, loss = 0.15884780\n",
      "Iteration 48, loss = 0.16258640\n",
      "Iteration 24, loss = 0.23949788\n",
      "Iteration 25, loss = 0.23571981\n",
      "Iteration 49, loss = 0.15625614\n",
      "Iteration 26, loss = 0.23055154\n",
      "Iteration 50, loss = 0.15103949\n",
      "Iteration 51, loss = 0.14995317\n",
      "Iteration 52, loss = 0.14556881\n",
      "Iteration 27, loss = 0.22814575\n",
      "Iteration 53, loss = 0.14296388\n",
      "Iteration 54, loss = 0.14452720\n",
      "Iteration 55, loss = 0.13845243\n",
      "Iteration 56, loss = 0.13777785\n",
      "Iteration 28, loss = 0.22466733\n",
      "Iteration 29, loss = 0.21694068\n",
      "Iteration 30, loss = 0.21720269\n",
      "Iteration 57, loss = 0.13703632\n",
      "Iteration 31, loss = 0.21236774\n",
      "Iteration 32, loss = 0.20708586\n",
      "Iteration 58, loss = 0.13334962\n",
      "Iteration 33, loss = 0.20568674\n",
      "Iteration 59, loss = 0.13300345\n",
      "Iteration 34, loss = 0.19968829\n",
      "Iteration 60, loss = 0.12891463\n",
      "Iteration 61, loss = 0.12725808\n",
      "Iteration 35, loss = 0.19590254\n",
      "Iteration 62, loss = 0.12580014\n",
      "Iteration 63, loss = 0.12525599\n",
      "Iteration 64, loss = 0.12302051\n",
      "Iteration 36, loss = 0.19895853\n",
      "Iteration 65, loss = 0.12756313\n",
      "Iteration 37, loss = 0.19269335\n",
      "Iteration 38, loss = 0.19034215\n",
      "Iteration 39, loss = 0.18333731\n",
      "Iteration 40, loss = 0.18576552\n",
      "Iteration 41, loss = 0.18316166\n",
      "Iteration 42, loss = 0.17353769\n",
      "Iteration 66, loss = 0.12144484\n",
      "Iteration 43, loss = 0.17511559\n",
      "Iteration 67, loss = 0.12073858\n",
      "Iteration 68, loss = 0.11528530\n",
      "Iteration 69, loss = 0.11582261\n",
      "Iteration 44, loss = 0.16892679\n",
      "Iteration 45, loss = 0.16588672\n",
      "Iteration 70, loss = 0.11294231\n",
      "Iteration 46, loss = 0.16395275\n",
      "Iteration 71, loss = 0.11303850\n",
      "Iteration 47, loss = 0.16394482\n",
      "Iteration 72, loss = 0.11215991\n",
      "Iteration 73, loss = 0.11150042\n",
      "Iteration 74, loss = 0.10873115\n",
      "Iteration 48, loss = 0.15942454\n",
      "Iteration 49, loss = 0.15635127\n",
      "Iteration 50, loss = 0.15867906\n",
      "Iteration 75, loss = 0.10873663\n",
      "Iteration 51, loss = 0.15202010\n",
      "Iteration 52, loss = 0.15102334\n",
      "Iteration 76, loss = 0.10413135\n",
      "Iteration 53, loss = 0.14597530\n",
      "Iteration 77, loss = 0.10671431\n",
      "Iteration 78, loss = 0.10368316\n",
      "Iteration 54, loss = 0.14692875\n",
      "Iteration 79, loss = 0.10362758\n",
      "Iteration 55, loss = 0.14149166\n",
      "Iteration 56, loss = 0.14005004\n",
      "Iteration 80, loss = 0.10156598\n",
      "Iteration 57, loss = 0.13692336\n",
      "Iteration 81, loss = 0.09892685\n",
      "Iteration 82, loss = 0.10030198\n",
      "Iteration 58, loss = 0.13918936\n",
      "Iteration 59, loss = 0.13395150\n",
      "Iteration 60, loss = 0.13120585\n",
      "Iteration 83, loss = 0.10139740\n",
      "Iteration 61, loss = 0.12920699\n",
      "Iteration 62, loss = 0.13011410\n",
      "Iteration 84, loss = 0.10120522\n",
      "Iteration 85, loss = 0.09758733\n",
      "Iteration 86, loss = 0.09337902\n",
      "Iteration 63, loss = 0.12635048\n",
      "Iteration 64, loss = 0.12524180\n",
      "Iteration 87, loss = 0.09242636\n",
      "Iteration 65, loss = 0.12213152\n",
      "Iteration 66, loss = 0.12363361\n",
      "Iteration 88, loss = 0.08994559\n",
      "Iteration 89, loss = 0.08951904\n",
      "Iteration 67, loss = 0.12226455\n",
      "Iteration 90, loss = 0.08848211\n",
      "Iteration 68, loss = 0.11691973\n",
      "Iteration 91, loss = 0.08865902\n",
      "Iteration 69, loss = 0.11796278\n",
      "Iteration 70, loss = 0.11660395\n",
      "Iteration 71, loss = 0.11358371\n",
      "Iteration 92, loss = 0.08685433\n",
      "Iteration 93, loss = 0.08844353\n",
      "Iteration 94, loss = 0.08781858\n",
      "Iteration 72, loss = 0.11216002\n",
      "Iteration 73, loss = 0.11205929\n",
      "Iteration 74, loss = 0.10871482\n",
      "Iteration 95, loss = 0.08900052\n",
      "Iteration 75, loss = 0.10864855\n",
      "Iteration 76, loss = 0.10811265\n",
      "Iteration 96, loss = 0.08806403\n",
      "Iteration 77, loss = 0.10842534\n",
      "Iteration 97, loss = 0.08623919\n",
      "Iteration 98, loss = 0.08318912\n",
      "Iteration 78, loss = 0.10811697\n",
      "Iteration 99, loss = 0.08110049\n",
      "Iteration 100, loss = 0.08337827\n",
      "Iteration 79, loss = 0.10568678\n",
      "Iteration 80, loss = 0.10179289\n",
      "Iteration 101, loss = 0.07764903\n",
      "Iteration 81, loss = 0.10291794\n",
      "Iteration 102, loss = 0.08196065\n",
      "Iteration 103, loss = 0.07900983\n",
      "Iteration 104, loss = 0.08540427\n",
      "Iteration 82, loss = 0.09959342\n",
      "Iteration 105, loss = 0.07958776\n",
      "Iteration 106, loss = 0.08701886\n",
      "Iteration 83, loss = 0.09624735\n",
      "Iteration 107, loss = 0.08778376\n",
      "Iteration 84, loss = 0.10226725\n",
      "Iteration 108, loss = 0.07808645\n",
      "Iteration 85, loss = 0.10612922\n",
      "Iteration 86, loss = 0.09986760\n",
      "Iteration 87, loss = 0.09765122\n",
      "Iteration 109, loss = 0.07545700\n",
      "Iteration 88, loss = 0.09270309\n",
      "Iteration 110, loss = 0.07767861\n",
      "Iteration 89, loss = 0.09356026\n",
      "Iteration 90, loss = 0.09143025\n",
      "Iteration 111, loss = 0.07381354\n",
      "Iteration 112, loss = 0.06800484\n",
      "Iteration 91, loss = 0.09181278\n",
      "Iteration 113, loss = 0.07067225\n",
      "Iteration 114, loss = 0.06940853\n",
      "Iteration 92, loss = 0.08904615\n",
      "Iteration 93, loss = 0.09323751\n",
      "Iteration 94, loss = 0.08837339\n",
      "Iteration 115, loss = 0.07329474\n",
      "Iteration 95, loss = 0.08325478\n",
      "Iteration 116, loss = 0.07075627\n",
      "Iteration 117, loss = 0.06937463\n",
      "Iteration 118, loss = 0.06811199\n",
      "Iteration 96, loss = 0.09026565\n",
      "Iteration 97, loss = 0.08485844\n",
      "Iteration 119, loss = 0.06643848\n",
      "Iteration 98, loss = 0.08274609\n",
      "Iteration 99, loss = 0.08156447\n",
      "Iteration 120, loss = 0.06425110\n",
      "Iteration 121, loss = 0.06424361\n",
      "Iteration 100, loss = 0.08048099\n",
      "Iteration 122, loss = 0.06195547\n",
      "Iteration 101, loss = 0.07769555\n",
      "Iteration 123, loss = 0.06017255\n",
      "Iteration 102, loss = 0.07740226\n",
      "Iteration 103, loss = 0.07843952\n",
      "Iteration 124, loss = 0.06034782\n",
      "Iteration 125, loss = 0.06071333\n",
      "Iteration 104, loss = 0.07819821\n",
      "Iteration 126, loss = 0.06255510\n",
      "Iteration 127, loss = 0.05863074\n",
      "Iteration 105, loss = 0.07397963\n",
      "Iteration 106, loss = 0.07207220\n",
      "Iteration 128, loss = 0.05726202\n",
      "Iteration 107, loss = 0.07502539\n",
      "Iteration 129, loss = 0.05734852\n",
      "Iteration 130, loss = 0.05647771\n",
      "Iteration 108, loss = 0.07271543\n",
      "Iteration 131, loss = 0.05739736\n",
      "Iteration 109, loss = 0.07167658\n",
      "Iteration 110, loss = 0.07020323\n",
      "Iteration 111, loss = 0.06760531\n",
      "Iteration 132, loss = 0.06007155\n",
      "Iteration 112, loss = 0.06967812\n",
      "Iteration 133, loss = 0.05849711\n",
      "Iteration 134, loss = 0.05722835\n",
      "Iteration 113, loss = 0.06642391\n",
      "Iteration 135, loss = 0.05691973\n",
      "Iteration 114, loss = 0.06514807\n",
      "Iteration 136, loss = 0.06560880\n",
      "Iteration 115, loss = 0.06662016\n",
      "Iteration 116, loss = 0.06538503\n",
      "Iteration 117, loss = 0.06427235\n",
      "Iteration 137, loss = 0.06977191\n",
      "Iteration 138, loss = 0.07017097\n",
      "Iteration 118, loss = 0.06267569\n",
      "Iteration 139, loss = 0.06487728\n",
      "Iteration 140, loss = 0.05550030\n",
      "Iteration 119, loss = 0.06405053\n",
      "Iteration 141, loss = 0.05219739\n",
      "Iteration 120, loss = 0.06217139\n",
      "Iteration 121, loss = 0.06067479\n",
      "Iteration 122, loss = 0.05971052\n",
      "Iteration 142, loss = 0.05503916\n",
      "Iteration 123, loss = 0.05979198\n",
      "Iteration 124, loss = 0.06039736\n",
      "Iteration 143, loss = 0.05176229\n",
      "Iteration 125, loss = 0.05959328\n",
      "Iteration 144, loss = 0.04908451\n",
      "Iteration 126, loss = 0.05880530\n",
      "Iteration 145, loss = 0.04926246\n",
      "Iteration 127, loss = 0.06276530\n",
      "Iteration 146, loss = 0.04672269\n",
      "Iteration 147, loss = 0.04460659\n",
      "Iteration 128, loss = 0.05751848\n",
      "Iteration 148, loss = 0.04592414\n",
      "Iteration 129, loss = 0.05869540\n",
      "Iteration 149, loss = 0.04387247\n",
      "Iteration 130, loss = 0.05603336\n",
      "Iteration 131, loss = 0.05463295\n",
      "Iteration 132, loss = 0.05230932\n",
      "Iteration 150, loss = 0.04426768\n",
      "Iteration 133, loss = 0.05242842\n",
      "Iteration 134, loss = 0.05352654\n",
      "Iteration 151, loss = 0.04512914\n",
      "Iteration 135, loss = 0.05059349\n",
      "Iteration 136, loss = 0.05182899\n",
      "Iteration 137, loss = 0.05340742\n",
      "Iteration 152, loss = 0.04357557\n",
      "Iteration 138, loss = 0.05216096\n",
      "Iteration 153, loss = 0.04393688\n",
      "Iteration 154, loss = 0.04511334\n",
      "Iteration 139, loss = 0.05284817\n",
      "Iteration 155, loss = 0.04503556\n",
      "Iteration 156, loss = 0.04768448\n",
      "Iteration 140, loss = 0.04908880\n",
      "Iteration 157, loss = 0.04416744\n",
      "Iteration 141, loss = 0.04734118\n",
      "Iteration 158, loss = 0.04214123\n",
      "Iteration 159, loss = 0.04339091\n",
      "Iteration 142, loss = 0.04734691\n",
      "Iteration 160, loss = 0.04194179\n",
      "Iteration 143, loss = 0.04686656\n",
      "Iteration 161, loss = 0.03888103\n",
      "Iteration 162, loss = 0.03978987\n",
      "Iteration 163, loss = 0.03669284\n",
      "Iteration 164, loss = 0.03829450\n",
      "Iteration 144, loss = 0.04558813\n",
      "Iteration 165, loss = 0.03932516\n",
      "Iteration 166, loss = 0.03782175\n",
      "Iteration 145, loss = 0.04462695\n",
      "Iteration 146, loss = 0.04410829\n",
      "Iteration 167, loss = 0.03554961\n",
      "Iteration 147, loss = 0.04408925\n",
      "Iteration 168, loss = 0.03492778\n",
      "Iteration 169, loss = 0.03509205\n",
      "Iteration 170, loss = 0.03482555\n",
      "Iteration 148, loss = 0.04386309\n",
      "Iteration 149, loss = 0.04417041\n",
      "Iteration 171, loss = 0.03594789\n",
      "Iteration 150, loss = 0.04373279\n",
      "Iteration 172, loss = 0.03666203\n",
      "Iteration 173, loss = 0.03445382\n",
      "Iteration 151, loss = 0.04782976\n",
      "Iteration 174, loss = 0.03468416\n",
      "Iteration 152, loss = 0.04645242\n",
      "Iteration 175, loss = 0.03563470\n",
      "Iteration 153, loss = 0.04629440\n",
      "Iteration 154, loss = 0.04316286\n",
      "Iteration 176, loss = 0.03417586\n",
      "Iteration 177, loss = 0.03348507\n",
      "Iteration 178, loss = 0.03619981\n",
      "Iteration 179, loss = 0.03374986\n",
      "Iteration 180, loss = 0.03459991\n",
      "Iteration 155, loss = 0.04309146\n",
      "Iteration 156, loss = 0.04412382\n",
      "Iteration 181, loss = 0.03277573\n",
      "Iteration 182, loss = 0.03008929\n",
      "Iteration 183, loss = 0.03080729\n",
      "Iteration 157, loss = 0.04218008\n",
      "Iteration 184, loss = 0.03011170\n",
      "Iteration 158, loss = 0.04282019\n",
      "Iteration 159, loss = 0.03925085\n",
      "Iteration 160, loss = 0.03813540\n",
      "Iteration 185, loss = 0.03124945\n",
      "Iteration 186, loss = 0.02948921\n",
      "Iteration 187, loss = 0.02832705\n",
      "Iteration 161, loss = 0.03662777\n",
      "Iteration 188, loss = 0.02789904\n",
      "Iteration 162, loss = 0.03818530\n",
      "Iteration 189, loss = 0.02924377\n",
      "Iteration 163, loss = 0.03790875\n",
      "Iteration 190, loss = 0.02759651\n",
      "Iteration 164, loss = 0.03820558\n",
      "Iteration 191, loss = 0.02692689\n",
      "Iteration 192, loss = 0.02695891\n",
      "Iteration 165, loss = 0.03562877\n",
      "Iteration 193, loss = 0.02697934\n",
      "Iteration 166, loss = 0.03477838\n",
      "Iteration 194, loss = 0.02876540\n",
      "Iteration 167, loss = 0.03575034\n",
      "Iteration 195, loss = 0.02769202\n",
      "Iteration 168, loss = 0.03570945\n",
      "Iteration 196, loss = 0.02752950\n",
      "Iteration 169, loss = 0.03701579\n",
      "Iteration 197, loss = 0.02799616\n",
      "Iteration 170, loss = 0.03979898\n",
      "Iteration 198, loss = 0.02923791\n",
      "Iteration 171, loss = 0.03852694\n",
      "Iteration 172, loss = 0.03419403\n",
      "Iteration 199, loss = 0.02919034\n",
      "Iteration 173, loss = 0.03677828\n",
      "Iteration 200, loss = 0.02732791\n",
      "Iteration 174, loss = 0.03622519\n",
      "Iteration 175, loss = 0.03642074\n",
      "Iteration 176, loss = 0.03502467\n",
      "Iteration 177, loss = 0.03436897\n",
      "Iteration 178, loss = 0.03192577\n",
      "Iteration 179, loss = 0.03266393\n",
      "Iteration 180, loss = 0.03624315\n",
      "Iteration 1, loss = 1.11405911\n",
      "Iteration 181, loss = 0.03516636\n",
      "Iteration 2, loss = 0.80150190\n",
      "Iteration 3, loss = 0.63415868\n",
      "Iteration 182, loss = 0.03120116\n",
      "Iteration 4, loss = 0.54633107\n",
      "Iteration 183, loss = 0.03234105\n",
      "Iteration 5, loss = 0.49510056\n",
      "Iteration 184, loss = 0.03248406\n",
      "Iteration 6, loss = 0.45866712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.42855020\n",
      "Iteration 185, loss = 0.02970541\n",
      "Iteration 186, loss = 0.02933743\n",
      "Iteration 187, loss = 0.03167004\n",
      "Iteration 8, loss = 0.40689936\n",
      "Iteration 9, loss = 0.38977830\n",
      "Iteration 188, loss = 0.02908809\n",
      "Iteration 10, loss = 0.37473194\n",
      "Iteration 11, loss = 0.36059715\n",
      "Iteration 12, loss = 0.34833905\n",
      "Iteration 13, loss = 0.33736988\n",
      "Iteration 189, loss = 0.02912639\n",
      "Iteration 14, loss = 0.32693110\n",
      "Iteration 15, loss = 0.31828146\n",
      "Iteration 16, loss = 0.30945431\n",
      "Iteration 190, loss = 0.02603455\n",
      "Iteration 17, loss = 0.30062601\n",
      "Iteration 18, loss = 0.29399813\n",
      "Iteration 191, loss = 0.02489424\n",
      "Iteration 19, loss = 0.28641112\n",
      "Iteration 192, loss = 0.02763332\n",
      "Iteration 20, loss = 0.27956300\n",
      "Iteration 193, loss = 0.02497601\n",
      "Iteration 21, loss = 0.27282852\n",
      "Iteration 194, loss = 0.02443945\n",
      "Iteration 22, loss = 0.26696610\n",
      "Iteration 195, loss = 0.02345271\n",
      "Iteration 196, loss = 0.02284977\n",
      "Iteration 197, loss = 0.02193151\n",
      "Iteration 23, loss = 0.26031266\n",
      "Iteration 198, loss = 0.02195200\n",
      "Iteration 24, loss = 0.25522670\n",
      "Iteration 199, loss = 0.02313183\n",
      "Iteration 25, loss = 0.25002304\n",
      "Iteration 26, loss = 0.24549456\n",
      "Iteration 200, loss = 0.02339730\n",
      "Iteration 201, loss = 0.02285670\n",
      "Iteration 202, loss = 0.02440034\n",
      "Iteration 27, loss = 0.24025043\n",
      "Iteration 203, loss = 0.02459673\n",
      "Iteration 28, loss = 0.23722478\n",
      "Iteration 29, loss = 0.23112927\n",
      "Iteration 204, loss = 0.02116577\n",
      "Iteration 30, loss = 0.22822290\n",
      "Iteration 205, loss = 0.02025545\n",
      "Iteration 206, loss = 0.01913923\n",
      "Iteration 31, loss = 0.22271807\n",
      "Iteration 207, loss = 0.01919055\n",
      "Iteration 32, loss = 0.21818456\n",
      "Iteration 33, loss = 0.21292743\n",
      "Iteration 208, loss = 0.01961080\n",
      "Iteration 34, loss = 0.20880235\n",
      "Iteration 209, loss = 0.01870340\n",
      "Iteration 35, loss = 0.20594052\n",
      "Iteration 210, loss = 0.02028761\n",
      "Iteration 211, loss = 0.01923260\n",
      "Iteration 36, loss = 0.20220470\n",
      "Iteration 37, loss = 0.20222256\n",
      "Iteration 212, loss = 0.02068994\n",
      "Iteration 38, loss = 0.19564526\n",
      "Iteration 213, loss = 0.02050976\n",
      "Iteration 214, loss = 0.02197535\n",
      "Iteration 39, loss = 0.19079676\n",
      "Iteration 215, loss = 0.02356025\n",
      "Iteration 216, loss = 0.02067312\n",
      "Iteration 40, loss = 0.19106019\n",
      "Iteration 41, loss = 0.18561857\n",
      "Iteration 217, loss = 0.01882134\n",
      "Iteration 218, loss = 0.01800316\n",
      "Iteration 42, loss = 0.18115702\n",
      "Iteration 219, loss = 0.01705588\n",
      "Iteration 43, loss = 0.17941849\n",
      "Iteration 44, loss = 0.17512672\n",
      "Iteration 220, loss = 0.01660224\n",
      "Iteration 45, loss = 0.16974661\n",
      "Iteration 46, loss = 0.17054094\n",
      "Iteration 47, loss = 0.16730411\n",
      "Iteration 221, loss = 0.01600493\n",
      "Iteration 48, loss = 0.17031964\n",
      "Iteration 222, loss = 0.01669760\n",
      "Iteration 223, loss = 0.01579736\n",
      "Iteration 49, loss = 0.16443750\n",
      "Iteration 224, loss = 0.01579712\n",
      "Iteration 225, loss = 0.01656764\n",
      "Iteration 50, loss = 0.16453044\n",
      "Iteration 226, loss = 0.01773885\n",
      "Iteration 51, loss = 0.15431245\n",
      "Iteration 227, loss = 0.01643105\n",
      "Iteration 52, loss = 0.15673682\n",
      "Iteration 228, loss = 0.01689335\n",
      "Iteration 53, loss = 0.14872257\n",
      "Iteration 229, loss = 0.01565350\n",
      "Iteration 230, loss = 0.01553686\n",
      "Iteration 54, loss = 0.14878909\n",
      "Iteration 231, loss = 0.01451447\n",
      "Iteration 232, loss = 0.01403349\n",
      "Iteration 233, loss = 0.01621184\n",
      "Iteration 234, loss = 0.01503236\n",
      "Iteration 55, loss = 0.14550432\n",
      "Iteration 235, loss = 0.01369110\n",
      "Iteration 236, loss = 0.01388248\n",
      "Iteration 56, loss = 0.14496979\n",
      "Iteration 237, loss = 0.01440398\n",
      "Iteration 238, loss = 0.01353780\n",
      "Iteration 239, loss = 0.01301646\n",
      "Iteration 240, loss = 0.01279414\n",
      "Iteration 57, loss = 0.14292888\n",
      "Iteration 241, loss = 0.01265473\n",
      "Iteration 58, loss = 0.14003636\n",
      "Iteration 242, loss = 0.01345672\n",
      "Iteration 243, loss = 0.01529915\n",
      "Iteration 244, loss = 0.01506430\n",
      "Iteration 59, loss = 0.14385465\n",
      "Iteration 245, loss = 0.01406128\n",
      "Iteration 60, loss = 0.13498684\n",
      "Iteration 61, loss = 0.13557684\n",
      "Iteration 62, loss = 0.13232374\n",
      "Iteration 246, loss = 0.01347196\n",
      "Iteration 63, loss = 0.13093780\n",
      "Iteration 64, loss = 0.12899540\n",
      "Iteration 247, loss = 0.01394082\n",
      "Iteration 65, loss = 0.12366908\n",
      "Iteration 248, loss = 0.01488349\n",
      "Iteration 66, loss = 0.12341844\n",
      "Iteration 249, loss = 0.01374143\n",
      "Iteration 67, loss = 0.11995890\n",
      "Iteration 250, loss = 0.01280796\n",
      "Iteration 68, loss = 0.11695668\n",
      "Iteration 251, loss = 0.01289800\n",
      "Iteration 69, loss = 0.11954138\n",
      "Iteration 70, loss = 0.11527938\n",
      "Iteration 252, loss = 0.01336383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 71, loss = 0.11241117\n",
      "Iteration 72, loss = 0.11382226\n",
      "Iteration 73, loss = 0.10895810\n",
      "Iteration 74, loss = 0.10910918\n",
      "Iteration 75, loss = 0.11083693\n",
      "Iteration 76, loss = 0.11572796\n",
      "Iteration 77, loss = 0.10583517\n",
      "Iteration 78, loss = 0.10880184\n",
      "Iteration 1, loss = 1.32692936\n",
      "Iteration 79, loss = 0.10348948\n",
      "Iteration 80, loss = 0.10246015\n",
      "Iteration 2, loss = 0.87099484\n",
      "Iteration 81, loss = 0.09958028\n",
      "Iteration 3, loss = 0.66410406\n",
      "Iteration 82, loss = 0.09848661\n",
      "Iteration 4, loss = 0.56404270\n",
      "Iteration 83, loss = 0.09772420\n",
      "Iteration 5, loss = 0.50234975\n",
      "Iteration 84, loss = 0.09584200\n",
      "Iteration 6, loss = 0.46542128\n",
      "Iteration 85, loss = 0.09447598\n",
      "Iteration 7, loss = 0.43496588\n",
      "Iteration 86, loss = 0.09381166\n",
      "Iteration 8, loss = 0.41440665\n",
      "Iteration 87, loss = 0.09075479\n",
      "Iteration 9, loss = 0.39380846\n",
      "Iteration 88, loss = 0.08980341\n",
      "Iteration 89, loss = 0.08906895\n",
      "Iteration 90, loss = 0.08775806\n",
      "Iteration 91, loss = 0.08621973\n",
      "Iteration 10, loss = 0.37853757\n",
      "Iteration 11, loss = 0.36415400\n",
      "Iteration 92, loss = 0.08537000\n",
      "Iteration 12, loss = 0.35118820\n",
      "Iteration 13, loss = 0.34069494\n",
      "Iteration 93, loss = 0.08552911\n",
      "Iteration 14, loss = 0.33040528\n",
      "Iteration 15, loss = 0.32104952\n",
      "Iteration 94, loss = 0.08330914\n",
      "Iteration 16, loss = 0.31145873\n",
      "Iteration 95, loss = 0.08523851\n",
      "Iteration 17, loss = 0.30296887\n",
      "Iteration 96, loss = 0.08620333\n",
      "Iteration 18, loss = 0.29536700\n",
      "Iteration 97, loss = 0.08791997\n",
      "Iteration 98, loss = 0.08501617\n",
      "Iteration 99, loss = 0.08228571\n",
      "Iteration 19, loss = 0.29012546\n",
      "Iteration 100, loss = 0.07919119\n",
      "Iteration 20, loss = 0.28462319\n",
      "Iteration 101, loss = 0.07563121\n",
      "Iteration 21, loss = 0.27580571\n",
      "Iteration 22, loss = 0.26891735\n",
      "Iteration 23, loss = 0.27018608\n",
      "Iteration 102, loss = 0.07612078\n",
      "Iteration 103, loss = 0.07419279\n",
      "Iteration 24, loss = 0.25812370\n",
      "Iteration 104, loss = 0.07340011\n",
      "Iteration 105, loss = 0.07426536\n",
      "Iteration 25, loss = 0.25245542\n",
      "Iteration 106, loss = 0.07322794\n",
      "Iteration 26, loss = 0.25247152\n",
      "Iteration 107, loss = 0.07324285\n",
      "Iteration 27, loss = 0.24410365\n",
      "Iteration 108, loss = 0.06944325\n",
      "Iteration 109, loss = 0.06964963\n",
      "Iteration 28, loss = 0.23909009\n",
      "Iteration 110, loss = 0.07015539\n",
      "Iteration 29, loss = 0.23412821\n",
      "Iteration 111, loss = 0.06873850\n",
      "Iteration 112, loss = 0.07055024\n",
      "Iteration 30, loss = 0.23003628\n",
      "Iteration 113, loss = 0.06742587\n",
      "Iteration 31, loss = 0.22467154\n",
      "Iteration 114, loss = 0.06527798\n",
      "Iteration 32, loss = 0.22365237\n",
      "Iteration 33, loss = 0.22024681\n",
      "Iteration 115, loss = 0.06458115\n",
      "Iteration 34, loss = 0.21260554\n",
      "Iteration 35, loss = 0.20878476\n",
      "Iteration 116, loss = 0.06380697\n",
      "Iteration 36, loss = 0.20531712\n",
      "Iteration 37, loss = 0.19991388\n",
      "Iteration 117, loss = 0.06201907\n",
      "Iteration 38, loss = 0.19700338\n",
      "Iteration 118, loss = 0.06084006\n",
      "Iteration 119, loss = 0.06128964\n",
      "Iteration 120, loss = 0.05999544\n",
      "Iteration 121, loss = 0.06062059\n",
      "Iteration 39, loss = 0.19491557\n",
      "Iteration 122, loss = 0.05920183\n",
      "Iteration 123, loss = 0.05830520\n",
      "Iteration 40, loss = 0.19076457\n",
      "Iteration 124, loss = 0.05784839\n",
      "Iteration 125, loss = 0.05620339\n",
      "Iteration 41, loss = 0.18696972\n",
      "Iteration 126, loss = 0.05602333\n",
      "Iteration 42, loss = 0.18449331\n",
      "Iteration 127, loss = 0.05791008\n",
      "Iteration 43, loss = 0.18043041\n",
      "Iteration 128, loss = 0.05811224\n",
      "Iteration 129, loss = 0.05591286\n",
      "Iteration 44, loss = 0.17729426\n",
      "Iteration 130, loss = 0.05446073\n",
      "Iteration 131, loss = 0.05268464\n",
      "Iteration 45, loss = 0.17407589\n",
      "Iteration 132, loss = 0.05552897\n",
      "Iteration 46, loss = 0.17071758\n",
      "Iteration 133, loss = 0.05592906\n",
      "Iteration 47, loss = 0.16748757\n",
      "Iteration 134, loss = 0.05547522\n",
      "Iteration 48, loss = 0.16620154\n",
      "Iteration 135, loss = 0.05451900\n",
      "Iteration 136, loss = 0.05890039\n",
      "Iteration 49, loss = 0.16915635\n",
      "Iteration 50, loss = 0.16054512\n",
      "Iteration 51, loss = 0.16016815\n",
      "Iteration 52, loss = 0.15464917\n",
      "Iteration 137, loss = 0.05406189\n",
      "Iteration 53, loss = 0.15465813\n",
      "Iteration 54, loss = 0.14890631\n",
      "Iteration 138, loss = 0.05505261\n",
      "Iteration 55, loss = 0.14605994\n",
      "Iteration 139, loss = 0.05126378\n",
      "Iteration 140, loss = 0.05270293\n",
      "Iteration 56, loss = 0.14496154\n",
      "Iteration 141, loss = 0.05113964\n",
      "Iteration 57, loss = 0.14270941\n",
      "Iteration 58, loss = 0.14368957\n",
      "Iteration 59, loss = 0.14147998\n",
      "Iteration 142, loss = 0.04840598\n",
      "Iteration 60, loss = 0.13581897\n",
      "Iteration 61, loss = 0.13598047\n",
      "Iteration 62, loss = 0.13548374\n",
      "Iteration 143, loss = 0.04845297\n",
      "Iteration 63, loss = 0.13059217\n",
      "Iteration 144, loss = 0.04422250\n",
      "Iteration 64, loss = 0.12821826\n",
      "Iteration 145, loss = 0.04490012\n",
      "Iteration 65, loss = 0.12704780\n",
      "Iteration 146, loss = 0.04598559\n",
      "Iteration 66, loss = 0.12351780\n",
      "Iteration 147, loss = 0.04435042\n",
      "Iteration 148, loss = 0.04351298\n",
      "Iteration 67, loss = 0.12217037\n",
      "Iteration 149, loss = 0.04307814\n",
      "Iteration 68, loss = 0.12068061\n",
      "Iteration 150, loss = 0.04304290\n",
      "Iteration 151, loss = 0.04319376\n",
      "Iteration 152, loss = 0.04161595\n",
      "Iteration 153, loss = 0.04163850\n",
      "Iteration 69, loss = 0.12080480\n",
      "Iteration 70, loss = 0.12067869\n",
      "Iteration 71, loss = 0.11668786\n",
      "Iteration 154, loss = 0.04185576\n",
      "Iteration 72, loss = 0.11937083\n",
      "Iteration 155, loss = 0.03953455\n",
      "Iteration 156, loss = 0.04012643\n",
      "Iteration 73, loss = 0.11679561\n",
      "Iteration 157, loss = 0.04001100\n",
      "Iteration 74, loss = 0.11526744\n",
      "Iteration 158, loss = 0.03968619\n",
      "Iteration 75, loss = 0.11356571\n",
      "Iteration 76, loss = 0.10845707\n",
      "Iteration 77, loss = 0.11198881\n",
      "Iteration 159, loss = 0.04086412\n",
      "Iteration 160, loss = 0.03991477\n",
      "Iteration 78, loss = 0.10608537\n",
      "Iteration 161, loss = 0.03876074\n",
      "Iteration 162, loss = 0.03741583\n",
      "Iteration 163, loss = 0.03758884\n",
      "Iteration 79, loss = 0.10569116\n",
      "Iteration 164, loss = 0.03477963\n",
      "Iteration 165, loss = 0.03478833\n",
      "Iteration 80, loss = 0.10389724\n",
      "Iteration 166, loss = 0.03757946\n",
      "Iteration 167, loss = 0.03897334\n",
      "Iteration 168, loss = 0.04236794\n",
      "Iteration 169, loss = 0.03554116\n",
      "Iteration 170, loss = 0.03484018\n",
      "Iteration 81, loss = 0.10873438\n",
      "Iteration 171, loss = 0.03344401\n",
      "Iteration 82, loss = 0.10468561\n",
      "Iteration 172, loss = 0.03335440\n",
      "Iteration 83, loss = 0.10384166\n",
      "Iteration 173, loss = 0.03518417\n",
      "Iteration 84, loss = 0.09877666\n",
      "Iteration 85, loss = 0.09826242\n",
      "Iteration 86, loss = 0.10112927\n",
      "Iteration 174, loss = 0.03310962\n",
      "Iteration 87, loss = 0.09431030\n",
      "Iteration 175, loss = 0.03239318\n",
      "Iteration 88, loss = 0.09223338\n",
      "Iteration 176, loss = 0.03274772\n",
      "Iteration 89, loss = 0.09392332\n",
      "Iteration 177, loss = 0.03162665\n",
      "Iteration 178, loss = 0.03173999\n",
      "Iteration 179, loss = 0.03144384\n",
      "Iteration 90, loss = 0.09062611\n",
      "Iteration 180, loss = 0.03117545\n",
      "Iteration 181, loss = 0.03670155\n",
      "Iteration 182, loss = 0.03338048\n",
      "Iteration 183, loss = 0.02996979\n",
      "Iteration 91, loss = 0.08994181\n",
      "Iteration 92, loss = 0.08943837\n",
      "Iteration 93, loss = 0.08786263\n",
      "Iteration 94, loss = 0.08673947\n",
      "Iteration 184, loss = 0.02900577\n",
      "Iteration 185, loss = 0.02781840\n",
      "Iteration 95, loss = 0.08855935\n",
      "Iteration 186, loss = 0.02876923\n",
      "Iteration 187, loss = 0.02767192\n",
      "Iteration 96, loss = 0.08637568\n",
      "Iteration 97, loss = 0.08544133\n",
      "Iteration 188, loss = 0.02737656\n",
      "Iteration 98, loss = 0.08427633\n",
      "Iteration 99, loss = 0.08490096\n",
      "Iteration 189, loss = 0.02860218\n",
      "Iteration 190, loss = 0.02821897\n",
      "Iteration 100, loss = 0.08220140\n",
      "Iteration 191, loss = 0.02687120\n",
      "Iteration 101, loss = 0.07991886\n",
      "Iteration 102, loss = 0.08232845\n",
      "Iteration 192, loss = 0.02751383\n",
      "Iteration 103, loss = 0.08222741\n",
      "Iteration 104, loss = 0.07612525\n",
      "Iteration 193, loss = 0.02620248\n",
      "Iteration 105, loss = 0.07523534\n",
      "Iteration 106, loss = 0.07659631\n",
      "Iteration 194, loss = 0.02632946\n",
      "Iteration 107, loss = 0.07363426\n",
      "Iteration 108, loss = 0.07626745\n",
      "Iteration 195, loss = 0.02635461\n",
      "Iteration 196, loss = 0.02542838\n",
      "Iteration 197, loss = 0.02779556\n",
      "Iteration 109, loss = 0.07580204\n",
      "Iteration 110, loss = 0.07279200\n",
      "Iteration 198, loss = 0.02714003\n",
      "Iteration 111, loss = 0.07908370\n",
      "Iteration 199, loss = 0.03175884\n",
      "Iteration 112, loss = 0.07378261\n",
      "Iteration 200, loss = 0.03109010\n",
      "Iteration 201, loss = 0.03372865\n",
      "Iteration 113, loss = 0.07251853\n",
      "Iteration 114, loss = 0.07151077\n",
      "Iteration 202, loss = 0.03528267\n",
      "Iteration 115, loss = 0.06713436\n",
      "Iteration 203, loss = 0.03568527\n",
      "Iteration 116, loss = 0.06908680\n",
      "Iteration 117, loss = 0.06541338\n",
      "Iteration 118, loss = 0.06527527\n",
      "Iteration 119, loss = 0.06557917\n",
      "Iteration 204, loss = 0.02883195\n",
      "Iteration 120, loss = 0.06312987\n",
      "Iteration 121, loss = 0.06430538\n",
      "Iteration 205, loss = 0.02858199\n",
      "Iteration 206, loss = 0.02549357\n",
      "Iteration 122, loss = 0.06390038\n",
      "Iteration 207, loss = 0.02389118\n",
      "Iteration 123, loss = 0.06507979\n",
      "Iteration 124, loss = 0.06375523\n",
      "Iteration 125, loss = 0.05958662\n",
      "Iteration 208, loss = 0.02353719\n",
      "Iteration 126, loss = 0.05910493\n",
      "Iteration 209, loss = 0.02268306\n",
      "Iteration 210, loss = 0.02314101\n",
      "Iteration 127, loss = 0.05801729\n",
      "Iteration 128, loss = 0.05991244\n",
      "Iteration 211, loss = 0.02384797\n",
      "Iteration 129, loss = 0.05777692\n",
      "Iteration 130, loss = 0.05593752\n",
      "Iteration 212, loss = 0.02133211\n",
      "Iteration 131, loss = 0.05688621\n",
      "Iteration 213, loss = 0.02243981\n",
      "Iteration 132, loss = 0.05907071\n",
      "Iteration 214, loss = 0.02230468\n",
      "Iteration 133, loss = 0.05775503\n",
      "Iteration 134, loss = 0.05433068\n",
      "Iteration 215, loss = 0.02248837\n",
      "Iteration 216, loss = 0.02191974\n",
      "Iteration 135, loss = 0.05417563\n",
      "Iteration 217, loss = 0.02306914\n",
      "Iteration 218, loss = 0.02117214\n",
      "Iteration 136, loss = 0.05555999\n",
      "Iteration 219, loss = 0.02126108\n",
      "Iteration 137, loss = 0.05312853\n",
      "Iteration 220, loss = 0.02042533\n",
      "Iteration 138, loss = 0.05151072\n",
      "Iteration 139, loss = 0.05154936\n",
      "Iteration 221, loss = 0.02297257\n",
      "Iteration 140, loss = 0.05037628\n",
      "Iteration 222, loss = 0.02138482\n",
      "Iteration 223, loss = 0.02216985\n",
      "Iteration 141, loss = 0.05230077\n",
      "Iteration 224, loss = 0.02251251\n",
      "Iteration 142, loss = 0.06271016\n",
      "Iteration 143, loss = 0.06086720\n",
      "Iteration 225, loss = 0.02380777\n",
      "Iteration 144, loss = 0.06835841\n",
      "Iteration 226, loss = 0.02105736\n",
      "Iteration 145, loss = 0.06328887\n",
      "Iteration 227, loss = 0.02128260\n",
      "Iteration 146, loss = 0.05360384\n",
      "Iteration 147, loss = 0.05323007\n",
      "Iteration 228, loss = 0.02072426\n",
      "Iteration 148, loss = 0.04766808\n",
      "Iteration 229, loss = 0.01829259\n",
      "Iteration 149, loss = 0.04554565\n",
      "Iteration 230, loss = 0.01884990\n",
      "Iteration 150, loss = 0.04580417\n",
      "Iteration 231, loss = 0.01884796\n",
      "Iteration 151, loss = 0.05050442\n",
      "Iteration 232, loss = 0.01907351\n",
      "Iteration 152, loss = 0.04769613\n",
      "Iteration 233, loss = 0.01848241\n",
      "Iteration 234, loss = 0.01889551\n",
      "Iteration 235, loss = 0.01777894\n",
      "Iteration 153, loss = 0.04846903\n",
      "Iteration 236, loss = 0.01942751\n",
      "Iteration 154, loss = 0.04577948\n",
      "Iteration 237, loss = 0.01906636\n",
      "Iteration 155, loss = 0.04401975\n",
      "Iteration 238, loss = 0.01739976\n",
      "Iteration 239, loss = 0.01611026\n",
      "Iteration 156, loss = 0.04549218\n",
      "Iteration 240, loss = 0.01807511\n",
      "Iteration 157, loss = 0.04321277\n",
      "Iteration 158, loss = 0.04040236\n",
      "Iteration 241, loss = 0.01843177\n",
      "Iteration 159, loss = 0.04095426\n",
      "Iteration 160, loss = 0.04112734\n",
      "Iteration 242, loss = 0.01918846\n",
      "Iteration 161, loss = 0.04027672\n",
      "Iteration 243, loss = 0.01815061\n",
      "Iteration 244, loss = 0.01987943\n",
      "Iteration 162, loss = 0.04123844\n",
      "Iteration 245, loss = 0.01736264\n",
      "Iteration 163, loss = 0.04194488\n",
      "Iteration 246, loss = 0.01685994\n",
      "Iteration 164, loss = 0.03774639\n",
      "Iteration 165, loss = 0.03969854\n",
      "Iteration 247, loss = 0.01612879\n",
      "Iteration 248, loss = 0.01684920\n",
      "Iteration 166, loss = 0.04180728\n",
      "Iteration 249, loss = 0.01736106\n",
      "Iteration 250, loss = 0.01578632\n",
      "Iteration 251, loss = 0.01490440\n",
      "Iteration 167, loss = 0.03817038\n",
      "Iteration 252, loss = 0.01524741\n",
      "Iteration 168, loss = 0.04060245\n",
      "Iteration 169, loss = 0.03813472\n",
      "Iteration 170, loss = 0.04113822\n",
      "Iteration 253, loss = 0.01477407\n",
      "Iteration 254, loss = 0.01472181\n",
      "Iteration 171, loss = 0.03912041\n",
      "Iteration 255, loss = 0.01664677\n",
      "Iteration 256, loss = 0.01489864\n",
      "Iteration 172, loss = 0.03408828\n",
      "Iteration 173, loss = 0.03678821\n",
      "Iteration 174, loss = 0.03591199\n",
      "Iteration 257, loss = 0.01531493\n",
      "Iteration 258, loss = 0.01529773\n",
      "Iteration 259, loss = 0.01498786\n",
      "Iteration 175, loss = 0.04112831\n",
      "Iteration 260, loss = 0.01463686\n",
      "Iteration 176, loss = 0.03986172\n",
      "Iteration 261, loss = 0.01454599\n",
      "Iteration 177, loss = 0.03680928\n",
      "Iteration 178, loss = 0.03259209\n",
      "Iteration 179, loss = 0.03321220\n",
      "Iteration 180, loss = 0.03146734\n",
      "Iteration 262, loss = 0.01947414\n",
      "Iteration 263, loss = 0.01841697\n",
      "Iteration 181, loss = 0.03136827\n",
      "Iteration 264, loss = 0.01965703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 182, loss = 0.03046392\n",
      "Iteration 183, loss = 0.03079426\n",
      "Iteration 184, loss = 0.03062118\n",
      "Iteration 185, loss = 0.03186203\n",
      "Iteration 186, loss = 0.03085155\n",
      "Iteration 187, loss = 0.03136635\n",
      "Iteration 188, loss = 0.02901379\n",
      "Iteration 189, loss = 0.02908093\n",
      "Iteration 1, loss = 1.30415112\n",
      "Iteration 2, loss = 0.90456347\n",
      "Iteration 3, loss = 0.69972905\n",
      "Iteration 4, loss = 0.58955799\n",
      "Iteration 5, loss = 0.52219697\n",
      "Iteration 190, loss = 0.02932617\n",
      "Iteration 6, loss = 0.47880350\n",
      "Iteration 191, loss = 0.03014418\n",
      "Iteration 7, loss = 0.44346088\n",
      "Iteration 192, loss = 0.03069830\n",
      "Iteration 8, loss = 0.41797498\n",
      "Iteration 193, loss = 0.03264749\n",
      "Iteration 9, loss = 0.39703014\n",
      "Iteration 194, loss = 0.02850783\n",
      "Iteration 195, loss = 0.04164762\n",
      "Iteration 196, loss = 0.03348833\n",
      "Iteration 10, loss = 0.37811477\n",
      "Iteration 197, loss = 0.03183678\n",
      "Iteration 11, loss = 0.36659036\n",
      "Iteration 198, loss = 0.03011053\n",
      "Iteration 12, loss = 0.35527910\n",
      "Iteration 13, loss = 0.34264237\n",
      "Iteration 199, loss = 0.02713408\n",
      "Iteration 14, loss = 0.33318668\n",
      "Iteration 200, loss = 0.02606150\n",
      "Iteration 15, loss = 0.32307174\n",
      "Iteration 201, loss = 0.02806883\n",
      "Iteration 16, loss = 0.31797398\n",
      "Iteration 202, loss = 0.02737392\n",
      "Iteration 17, loss = 0.30916864\n",
      "Iteration 203, loss = 0.02535908\n",
      "Iteration 18, loss = 0.30143099\n",
      "Iteration 204, loss = 0.02438438\n",
      "Iteration 19, loss = 0.29536914\n",
      "Iteration 205, loss = 0.02473162\n",
      "Iteration 20, loss = 0.28949192\n",
      "Iteration 21, loss = 0.28438656\n",
      "Iteration 206, loss = 0.02563833\n",
      "Iteration 22, loss = 0.27783014\n",
      "Iteration 207, loss = 0.02448225\n",
      "Iteration 23, loss = 0.27232307\n",
      "Iteration 208, loss = 0.02440161\n",
      "Iteration 209, loss = 0.02351839\n",
      "Iteration 24, loss = 0.26666945\n",
      "Iteration 25, loss = 0.26303207\n",
      "Iteration 210, loss = 0.02383020\n",
      "Iteration 26, loss = 0.25747979\n",
      "Iteration 27, loss = 0.25618421\n",
      "Iteration 28, loss = 0.25072838\n",
      "Iteration 211, loss = 0.02257188\n",
      "Iteration 29, loss = 0.24702862\n",
      "Iteration 212, loss = 0.02327584\n",
      "Iteration 30, loss = 0.24230635\n",
      "Iteration 213, loss = 0.02511660\n",
      "Iteration 31, loss = 0.23765760\n",
      "Iteration 214, loss = 0.02880243\n",
      "Iteration 32, loss = 0.23708696\n",
      "Iteration 215, loss = 0.02688659\n",
      "Iteration 216, loss = 0.02955797\n",
      "Iteration 217, loss = 0.02245573\n",
      "Iteration 33, loss = 0.23044784\n",
      "Iteration 218, loss = 0.02379373\n",
      "Iteration 219, loss = 0.02168759\n",
      "Iteration 220, loss = 0.02087701\n",
      "Iteration 34, loss = 0.22993030\n",
      "Iteration 221, loss = 0.02033999\n",
      "Iteration 222, loss = 0.02279109\n",
      "Iteration 223, loss = 0.02411265\n",
      "Iteration 224, loss = 0.02062263\n",
      "Iteration 225, loss = 0.02490388\n",
      "Iteration 35, loss = 0.22481643\n",
      "Iteration 226, loss = 0.02202927\n",
      "Iteration 36, loss = 0.22079142\n",
      "Iteration 227, loss = 0.02602621\n",
      "Iteration 228, loss = 0.02636627\n",
      "Iteration 37, loss = 0.21916403\n",
      "Iteration 229, loss = 0.02596215\n",
      "Iteration 38, loss = 0.21276370\n",
      "Iteration 230, loss = 0.02093428\n",
      "Iteration 39, loss = 0.21107958\n",
      "Iteration 231, loss = 0.02207975\n",
      "Iteration 40, loss = 0.20745883\n",
      "Iteration 232, loss = 0.02108424\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.20288184\n",
      "Iteration 42, loss = 0.20383204\n",
      "Iteration 43, loss = 0.19970464\n",
      "Iteration 44, loss = 0.19668185\n",
      "Iteration 45, loss = 0.19523568\n",
      "Iteration 46, loss = 0.18920850\n",
      "Iteration 47, loss = 0.19280272\n",
      "Iteration 48, loss = 0.18582122\n",
      "Iteration 49, loss = 0.18327423\n",
      "Iteration 50, loss = 0.17901043\n",
      "Iteration 51, loss = 0.17562710\n",
      "Iteration 52, loss = 0.17498283\n",
      "Iteration 53, loss = 0.17316494\n",
      "Iteration 54, loss = 0.17004111\n",
      "Iteration 55, loss = 0.16982848\n",
      "Iteration 1, loss = 0.92550019\n",
      "Iteration 56, loss = 0.17093389\n",
      "Iteration 2, loss = 0.69285682\n",
      "Iteration 3, loss = 0.58165862\n",
      "Iteration 57, loss = 0.16308837\n",
      "Iteration 4, loss = 0.51502529\n",
      "Iteration 58, loss = 0.16479858\n",
      "Iteration 59, loss = 0.15815265\n",
      "Iteration 5, loss = 0.47140842\n",
      "Iteration 60, loss = 0.15806918\n",
      "Iteration 6, loss = 0.44034390\n",
      "Iteration 7, loss = 0.41566998\n",
      "Iteration 8, loss = 0.39350870\n",
      "Iteration 9, loss = 0.37809258\n",
      "Iteration 61, loss = 0.15405550\n",
      "Iteration 10, loss = 0.36247474\n",
      "Iteration 62, loss = 0.15193226\n",
      "Iteration 11, loss = 0.35290118\n",
      "Iteration 12, loss = 0.33954844\n",
      "Iteration 13, loss = 0.33112014\n",
      "Iteration 63, loss = 0.14920256\n",
      "Iteration 14, loss = 0.32116560\n",
      "Iteration 15, loss = 0.31495969\n",
      "Iteration 16, loss = 0.30694369\n",
      "Iteration 64, loss = 0.15201267\n",
      "Iteration 65, loss = 0.15291191\n",
      "Iteration 17, loss = 0.29998367\n",
      "Iteration 66, loss = 0.14976861\n",
      "Iteration 18, loss = 0.29290999\n",
      "Iteration 19, loss = 0.29081852\n",
      "Iteration 67, loss = 0.14635656\n",
      "Iteration 20, loss = 0.27996716\n",
      "Iteration 68, loss = 0.15195999\n",
      "Iteration 21, loss = 0.27796860\n",
      "Iteration 69, loss = 0.14695718\n",
      "Iteration 70, loss = 0.13852929\n",
      "Iteration 71, loss = 0.13536560\n",
      "Iteration 22, loss = 0.27218566\n",
      "Iteration 72, loss = 0.14115965\n",
      "Iteration 73, loss = 0.13375048\n",
      "Iteration 74, loss = 0.13414587\n",
      "Iteration 75, loss = 0.13134838\n",
      "Iteration 23, loss = 0.26643153\n",
      "Iteration 76, loss = 0.13052002\n",
      "Iteration 77, loss = 0.12761868\n",
      "Iteration 78, loss = 0.12550553\n",
      "Iteration 24, loss = 0.26357207\n",
      "Iteration 25, loss = 0.25750305\n",
      "Iteration 79, loss = 0.13000245\n",
      "Iteration 26, loss = 0.25448975\n",
      "Iteration 80, loss = 0.12839739\n",
      "Iteration 81, loss = 0.12037000\n",
      "Iteration 27, loss = 0.24933980\n",
      "Iteration 82, loss = 0.12130322\n",
      "Iteration 28, loss = 0.24429385\n",
      "Iteration 29, loss = 0.24020797\n",
      "Iteration 83, loss = 0.11912854\n",
      "Iteration 84, loss = 0.12331851\n",
      "Iteration 85, loss = 0.12219391\n",
      "Iteration 30, loss = 0.23309130\n",
      "Iteration 86, loss = 0.11878447\n",
      "Iteration 31, loss = 0.23135756\n",
      "Iteration 87, loss = 0.11447171\n",
      "Iteration 32, loss = 0.22617380\n",
      "Iteration 88, loss = 0.11128584\n",
      "Iteration 89, loss = 0.11235158\n",
      "Iteration 33, loss = 0.22232354\n",
      "Iteration 90, loss = 0.10898335\n",
      "Iteration 91, loss = 0.10871191\n",
      "Iteration 34, loss = 0.21739910\n",
      "Iteration 92, loss = 0.11119302\n",
      "Iteration 35, loss = 0.21666043\n",
      "Iteration 93, loss = 0.11300612\n",
      "Iteration 94, loss = 0.11051017\n",
      "Iteration 36, loss = 0.21224884\n",
      "Iteration 95, loss = 0.10891892\n",
      "Iteration 37, loss = 0.20661537\n",
      "Iteration 96, loss = 0.10323414\n",
      "Iteration 38, loss = 0.20858700\n",
      "Iteration 97, loss = 0.10102335\n",
      "Iteration 39, loss = 0.20814759\n",
      "Iteration 98, loss = 0.10572493\n",
      "Iteration 40, loss = 0.19638555\n",
      "Iteration 99, loss = 0.10185917\n",
      "Iteration 41, loss = 0.20054471\n",
      "Iteration 100, loss = 0.09905047\n",
      "Iteration 42, loss = 0.19251231\n",
      "Iteration 101, loss = 0.09808028\n",
      "Iteration 43, loss = 0.19190222\n",
      "Iteration 102, loss = 0.09769770\n",
      "Iteration 103, loss = 0.09585895\n",
      "Iteration 104, loss = 0.09537007\n",
      "Iteration 44, loss = 0.18963252\n",
      "Iteration 105, loss = 0.09779501\n",
      "Iteration 106, loss = 0.10289403\n",
      "Iteration 107, loss = 0.09731067\n",
      "Iteration 45, loss = 0.18639440\n",
      "Iteration 108, loss = 0.09239510\n",
      "Iteration 46, loss = 0.18216449\n",
      "Iteration 47, loss = 0.17559705\n",
      "Iteration 109, loss = 0.09123612\n",
      "Iteration 48, loss = 0.17786008\n",
      "Iteration 110, loss = 0.09002734\n",
      "Iteration 111, loss = 0.08957818\n",
      "Iteration 49, loss = 0.17400147\n",
      "Iteration 112, loss = 0.09082311\n",
      "Iteration 50, loss = 0.16863416\n",
      "Iteration 51, loss = 0.16876958\n",
      "Iteration 113, loss = 0.09364051\n",
      "Iteration 52, loss = 0.16406914\n",
      "Iteration 114, loss = 0.08905002\n",
      "Iteration 53, loss = 0.16300742\n",
      "Iteration 115, loss = 0.08732653\n",
      "Iteration 116, loss = 0.08444120\n",
      "Iteration 54, loss = 0.16090665\n",
      "Iteration 55, loss = 0.15453683\n",
      "Iteration 117, loss = 0.08546290\n",
      "Iteration 56, loss = 0.15319516\n",
      "Iteration 57, loss = 0.15280162\n",
      "Iteration 118, loss = 0.08291675\n",
      "Iteration 119, loss = 0.08628919\n",
      "Iteration 120, loss = 0.08309174\n",
      "Iteration 58, loss = 0.15002049\n",
      "Iteration 59, loss = 0.15141081\n",
      "Iteration 121, loss = 0.08146477\n",
      "Iteration 60, loss = 0.14581858\n",
      "Iteration 122, loss = 0.08381000\n",
      "Iteration 61, loss = 0.14771146\n",
      "Iteration 123, loss = 0.08381978\n",
      "Iteration 124, loss = 0.07931086\n",
      "Iteration 62, loss = 0.14124893\n",
      "Iteration 63, loss = 0.13926496\n",
      "Iteration 125, loss = 0.07861774\n",
      "Iteration 64, loss = 0.14066691\n",
      "Iteration 65, loss = 0.13534994\n",
      "Iteration 126, loss = 0.07895173\n",
      "Iteration 127, loss = 0.07884668\n",
      "Iteration 128, loss = 0.08087463\n",
      "Iteration 66, loss = 0.13322984\n",
      "Iteration 67, loss = 0.13280454\n",
      "Iteration 68, loss = 0.13215075\n",
      "Iteration 129, loss = 0.08276151\n",
      "Iteration 69, loss = 0.13356664\n",
      "Iteration 130, loss = 0.08074659\n",
      "Iteration 70, loss = 0.13092850\n",
      "Iteration 131, loss = 0.08180451\n",
      "Iteration 71, loss = 0.12587107\n",
      "Iteration 132, loss = 0.07738280\n",
      "Iteration 72, loss = 0.12280909\n",
      "Iteration 133, loss = 0.07653966\n",
      "Iteration 134, loss = 0.07325187\n",
      "Iteration 135, loss = 0.07059786\n",
      "Iteration 73, loss = 0.12554271\n",
      "Iteration 136, loss = 0.07252988\n",
      "Iteration 74, loss = 0.12225969\n",
      "Iteration 137, loss = 0.07170474\n",
      "Iteration 138, loss = 0.07167147\n",
      "Iteration 75, loss = 0.11874231\n",
      "Iteration 76, loss = 0.11668501\n",
      "Iteration 139, loss = 0.07188370\n",
      "Iteration 140, loss = 0.06779774\n",
      "Iteration 141, loss = 0.06714657\n",
      "Iteration 77, loss = 0.11590596\n",
      "Iteration 142, loss = 0.06852703\n",
      "Iteration 143, loss = 0.06660280\n",
      "Iteration 144, loss = 0.06642598\n",
      "Iteration 145, loss = 0.06588943\n",
      "Iteration 78, loss = 0.11374698\n",
      "Iteration 146, loss = 0.06379009\n",
      "Iteration 147, loss = 0.06628075\n",
      "Iteration 148, loss = 0.06601396\n",
      "Iteration 149, loss = 0.06328373\n",
      "Iteration 79, loss = 0.11500601\n",
      "Iteration 150, loss = 0.06383051\n",
      "Iteration 80, loss = 0.11374531\n",
      "Iteration 151, loss = 0.06599860\n",
      "Iteration 81, loss = 0.11150372\n",
      "Iteration 152, loss = 0.06674007\n",
      "Iteration 82, loss = 0.10853557\n",
      "Iteration 153, loss = 0.06205602\n",
      "Iteration 83, loss = 0.10793543\n",
      "Iteration 154, loss = 0.06367555\n",
      "Iteration 84, loss = 0.10537143\n",
      "Iteration 155, loss = 0.06210048\n",
      "Iteration 85, loss = 0.10292827\n",
      "Iteration 156, loss = 0.06140166\n",
      "Iteration 86, loss = 0.11227339\n",
      "Iteration 157, loss = 0.05975782\n",
      "Iteration 87, loss = 0.10263861\n",
      "Iteration 158, loss = 0.05832356\n",
      "Iteration 88, loss = 0.10190538\n",
      "Iteration 159, loss = 0.05799224\n",
      "Iteration 89, loss = 0.09892131\n",
      "Iteration 90, loss = 0.10258714\n",
      "Iteration 160, loss = 0.05715607\n",
      "Iteration 91, loss = 0.09905006\n",
      "Iteration 92, loss = 0.09849838\n",
      "Iteration 161, loss = 0.05603542\n",
      "Iteration 93, loss = 0.09555874\n",
      "Iteration 162, loss = 0.05586416\n",
      "Iteration 94, loss = 0.09400184\n",
      "Iteration 163, loss = 0.05560788\n",
      "Iteration 95, loss = 0.09265220\n",
      "Iteration 164, loss = 0.05532221\n",
      "Iteration 96, loss = 0.09334519\n",
      "Iteration 165, loss = 0.05471741\n",
      "Iteration 166, loss = 0.05352984\n",
      "Iteration 97, loss = 0.09017850\n",
      "Iteration 98, loss = 0.08863315\n",
      "Iteration 167, loss = 0.05261099\n",
      "Iteration 99, loss = 0.09108633\n",
      "Iteration 168, loss = 0.05522520\n",
      "Iteration 169, loss = 0.05228020\n",
      "Iteration 100, loss = 0.09121902\n",
      "Iteration 170, loss = 0.05189285\n",
      "Iteration 101, loss = 0.08566601\n",
      "Iteration 171, loss = 0.05100958\n",
      "Iteration 102, loss = 0.08449490\n",
      "Iteration 172, loss = 0.05386483\n",
      "Iteration 103, loss = 0.08316862\n",
      "Iteration 104, loss = 0.08338743\n",
      "Iteration 173, loss = 0.05323925\n",
      "Iteration 105, loss = 0.08141958\n",
      "Iteration 174, loss = 0.05674906\n",
      "Iteration 106, loss = 0.08094357\n",
      "Iteration 175, loss = 0.05352644\n",
      "Iteration 107, loss = 0.07899393\n",
      "Iteration 176, loss = 0.05069388\n",
      "Iteration 108, loss = 0.08380192\n",
      "Iteration 109, loss = 0.08213959\n",
      "Iteration 177, loss = 0.05038044\n",
      "Iteration 110, loss = 0.07787608\n",
      "Iteration 178, loss = 0.05195577\n",
      "Iteration 179, loss = 0.05096971\n",
      "Iteration 111, loss = 0.07925988\n",
      "Iteration 180, loss = 0.05298892\n",
      "Iteration 112, loss = 0.08347325\n",
      "Iteration 181, loss = 0.05536738\n",
      "Iteration 113, loss = 0.07750128\n",
      "Iteration 114, loss = 0.07650191\n",
      "Iteration 182, loss = 0.05407784\n",
      "Iteration 183, loss = 0.05255704\n",
      "Iteration 115, loss = 0.07640437\n",
      "Iteration 184, loss = 0.05256221\n",
      "Iteration 116, loss = 0.07139351\n",
      "Iteration 185, loss = 0.05025363\n",
      "Iteration 117, loss = 0.07101125\n",
      "Iteration 118, loss = 0.07094460\n",
      "Iteration 186, loss = 0.04555521\n",
      "Iteration 119, loss = 0.07296118\n",
      "Iteration 120, loss = 0.06900645\n",
      "Iteration 121, loss = 0.06619093\n",
      "Iteration 187, loss = 0.04448812\n",
      "Iteration 122, loss = 0.06887075\n",
      "Iteration 123, loss = 0.07173557\n",
      "Iteration 188, loss = 0.04371342\n",
      "Iteration 189, loss = 0.04472345\n",
      "Iteration 190, loss = 0.04339641\n",
      "Iteration 191, loss = 0.04415988\n",
      "Iteration 124, loss = 0.06832035\n",
      "Iteration 192, loss = 0.04222375\n",
      "Iteration 125, loss = 0.06484356\n",
      "Iteration 193, loss = 0.04235822\n",
      "Iteration 194, loss = 0.04279109\n",
      "Iteration 126, loss = 0.06448235\n",
      "Iteration 127, loss = 0.06441913\n",
      "Iteration 195, loss = 0.04317858\n",
      "Iteration 128, loss = 0.06362044\n",
      "Iteration 196, loss = 0.04248500\n",
      "Iteration 129, loss = 0.06561464\n",
      "Iteration 197, loss = 0.04442171\n",
      "Iteration 130, loss = 0.07031479\n",
      "Iteration 198, loss = 0.04990623\n",
      "Iteration 131, loss = 0.07491328\n",
      "Iteration 199, loss = 0.04517134\n",
      "Iteration 200, loss = 0.05235392\n",
      "Iteration 132, loss = 0.07132660\n",
      "Iteration 201, loss = 0.05361907\n",
      "Iteration 133, loss = 0.06649676\n",
      "Iteration 202, loss = 0.04316816\n",
      "Iteration 134, loss = 0.06759102\n",
      "Iteration 203, loss = 0.04387229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 135, loss = 0.06262144\n",
      "Iteration 136, loss = 0.05813132\n",
      "Iteration 137, loss = 0.05921110\n",
      "Iteration 138, loss = 0.05993482\n",
      "Iteration 139, loss = 0.05675286\n",
      "Iteration 140, loss = 0.05519522\n",
      "Iteration 1, loss = 1.07236371\n",
      "Iteration 141, loss = 0.05275429\n",
      "Iteration 2, loss = 0.78827877\n",
      "Iteration 142, loss = 0.05665482\n",
      "Iteration 143, loss = 0.05578063\n",
      "Iteration 3, loss = 0.63511734\n",
      "Iteration 144, loss = 0.05780091\n",
      "Iteration 4, loss = 0.54910101\n",
      "Iteration 145, loss = 0.05123297\n",
      "Iteration 5, loss = 0.49380683\n",
      "Iteration 6, loss = 0.45272201\n",
      "Iteration 146, loss = 0.04934372\n",
      "Iteration 7, loss = 0.42391064\n",
      "Iteration 147, loss = 0.04871168\n",
      "Iteration 148, loss = 0.04789137\n",
      "Iteration 8, loss = 0.40141437\n",
      "Iteration 9, loss = 0.37979963\n",
      "Iteration 10, loss = 0.36628967\n",
      "Iteration 149, loss = 0.04677880\n",
      "Iteration 11, loss = 0.35198084\n",
      "Iteration 150, loss = 0.04700674\n",
      "Iteration 151, loss = 0.04516152\n",
      "Iteration 12, loss = 0.34212403\n",
      "Iteration 152, loss = 0.04574796\n",
      "Iteration 13, loss = 0.33026936\n",
      "Iteration 14, loss = 0.31896357\n",
      "Iteration 153, loss = 0.04604693\n",
      "Iteration 15, loss = 0.31144584\n",
      "Iteration 154, loss = 0.04624938\n",
      "Iteration 16, loss = 0.30410929\n",
      "Iteration 155, loss = 0.04769368\n",
      "Iteration 17, loss = 0.29458445\n",
      "Iteration 156, loss = 0.04449319\n",
      "Iteration 157, loss = 0.04323315\n",
      "Iteration 18, loss = 0.28741165\n",
      "Iteration 158, loss = 0.04388486\n",
      "Iteration 159, loss = 0.04217333\n",
      "Iteration 19, loss = 0.28126824\n",
      "Iteration 160, loss = 0.04286478\n",
      "Iteration 161, loss = 0.04197731\n",
      "Iteration 162, loss = 0.04172375\n",
      "Iteration 20, loss = 0.27471073\n",
      "Iteration 21, loss = 0.26843310\n",
      "Iteration 163, loss = 0.04238752\n",
      "Iteration 22, loss = 0.26375302\n",
      "Iteration 164, loss = 0.04036035\n",
      "Iteration 23, loss = 0.25927909\n",
      "Iteration 165, loss = 0.03887559\n",
      "Iteration 166, loss = 0.03925824\n",
      "Iteration 24, loss = 0.25814653\n",
      "Iteration 167, loss = 0.04015483\n",
      "Iteration 25, loss = 0.25617446\n",
      "Iteration 26, loss = 0.25366305\n",
      "Iteration 27, loss = 0.24167159\n",
      "Iteration 168, loss = 0.03855521\n",
      "Iteration 169, loss = 0.03971230\n",
      "Iteration 170, loss = 0.03816045\n",
      "Iteration 28, loss = 0.23882450\n",
      "Iteration 171, loss = 0.03879374\n",
      "Iteration 29, loss = 0.23180296\n",
      "Iteration 172, loss = 0.03802122\n",
      "Iteration 30, loss = 0.22736270\n",
      "Iteration 173, loss = 0.04160427\n",
      "Iteration 31, loss = 0.22395113\n",
      "Iteration 174, loss = 0.04410155\n",
      "Iteration 32, loss = 0.22233082\n",
      "Iteration 33, loss = 0.21725370\n",
      "Iteration 175, loss = 0.04406139\n",
      "Iteration 34, loss = 0.21278120\n",
      "Iteration 176, loss = 0.04719537\n",
      "Iteration 35, loss = 0.21136871\n",
      "Iteration 177, loss = 0.04442318\n",
      "Iteration 178, loss = 0.03765678\n",
      "Iteration 36, loss = 0.20475851\n",
      "Iteration 37, loss = 0.20509565\n",
      "Iteration 38, loss = 0.19912569\n",
      "Iteration 179, loss = 0.03778818\n",
      "Iteration 39, loss = 0.19522890\n",
      "Iteration 40, loss = 0.19214370\n",
      "Iteration 41, loss = 0.19221604\n",
      "Iteration 180, loss = 0.04066656\n",
      "Iteration 42, loss = 0.18714730\n",
      "Iteration 43, loss = 0.18501670\n",
      "Iteration 181, loss = 0.04324300\n",
      "Iteration 44, loss = 0.18284022\n",
      "Iteration 45, loss = 0.17764991\n",
      "Iteration 182, loss = 0.04369424\n",
      "Iteration 46, loss = 0.17455801\n",
      "Iteration 183, loss = 0.03847156\n",
      "Iteration 184, loss = 0.03512560\n",
      "Iteration 47, loss = 0.17208725\n",
      "Iteration 185, loss = 0.03453202\n",
      "Iteration 48, loss = 0.16999829\n",
      "Iteration 49, loss = 0.16759112\n",
      "Iteration 186, loss = 0.03597734\n",
      "Iteration 50, loss = 0.16946266\n",
      "Iteration 187, loss = 0.03572622\n",
      "Iteration 51, loss = 0.16775935\n",
      "Iteration 188, loss = 0.03185729\n",
      "Iteration 52, loss = 0.15929707\n",
      "Iteration 189, loss = 0.03243775\n",
      "Iteration 53, loss = 0.15995360\n",
      "Iteration 190, loss = 0.02953545\n",
      "Iteration 54, loss = 0.15582820\n",
      "Iteration 191, loss = 0.03053028\n",
      "Iteration 55, loss = 0.15272123\n",
      "Iteration 192, loss = 0.03022162\n",
      "Iteration 56, loss = 0.15655453\n",
      "Iteration 193, loss = 0.03114374\n",
      "Iteration 194, loss = 0.03073521\n",
      "Iteration 57, loss = 0.15571955\n",
      "Iteration 195, loss = 0.03069930\n",
      "Iteration 196, loss = 0.03024867\n",
      "Iteration 58, loss = 0.15034330\n",
      "Iteration 59, loss = 0.14875458\n",
      "Iteration 197, loss = 0.02804288\n",
      "Iteration 60, loss = 0.14673012\n",
      "Iteration 61, loss = 0.14314693\n",
      "Iteration 198, loss = 0.02702883\n",
      "Iteration 62, loss = 0.13861696\n",
      "Iteration 199, loss = 0.03002290\n",
      "Iteration 200, loss = 0.02947945\n",
      "Iteration 63, loss = 0.13870082\n",
      "Iteration 201, loss = 0.03071284\n",
      "Iteration 64, loss = 0.13501251\n",
      "Iteration 202, loss = 0.03079805\n",
      "Iteration 65, loss = 0.13507005\n",
      "Iteration 203, loss = 0.02786032\n",
      "Iteration 66, loss = 0.13381303\n",
      "Iteration 204, loss = 0.02783102\n",
      "Iteration 67, loss = 0.12879081\n",
      "Iteration 205, loss = 0.02574885\n",
      "Iteration 68, loss = 0.12931498\n",
      "Iteration 69, loss = 0.12775089\n",
      "Iteration 70, loss = 0.12874663\n",
      "Iteration 206, loss = 0.02690843\n",
      "Iteration 71, loss = 0.12526877\n",
      "Iteration 207, loss = 0.02445300\n",
      "Iteration 72, loss = 0.12398123\n",
      "Iteration 208, loss = 0.02473541\n",
      "Iteration 209, loss = 0.02391819\n",
      "Iteration 73, loss = 0.12304543\n",
      "Iteration 210, loss = 0.02402297Iteration 74, loss = 0.11932375\n",
      "\n",
      "Iteration 75, loss = 0.11975326\n",
      "Iteration 211, loss = 0.02446883\n",
      "Iteration 76, loss = 0.11761983\n",
      "Iteration 212, loss = 0.02379406\n",
      "Iteration 77, loss = 0.11522749\n",
      "Iteration 213, loss = 0.02262586\n",
      "Iteration 78, loss = 0.11377182\n",
      "Iteration 214, loss = 0.02573823\n",
      "Iteration 79, loss = 0.11276985\n",
      "Iteration 80, loss = 0.11218312\n",
      "Iteration 215, loss = 0.02856807\n",
      "Iteration 81, loss = 0.11239210\n",
      "Iteration 82, loss = 0.11079995\n",
      "Iteration 83, loss = 0.10903902\n",
      "Iteration 216, loss = 0.02326066\n",
      "Iteration 84, loss = 0.10555530\n",
      "Iteration 85, loss = 0.10430218\n",
      "Iteration 86, loss = 0.10665152\n",
      "Iteration 217, loss = 0.02244389\n",
      "Iteration 87, loss = 0.10784154\n",
      "Iteration 218, loss = 0.02284145\n",
      "Iteration 219, loss = 0.02286631\n",
      "Iteration 88, loss = 0.10167704\n",
      "Iteration 220, loss = 0.02442552\n",
      "Iteration 89, loss = 0.10203985\n",
      "Iteration 221, loss = 0.02362256\n",
      "Iteration 90, loss = 0.09945607\n",
      "Iteration 222, loss = 0.02577606\n",
      "Iteration 223, loss = 0.02395330\n",
      "Iteration 224, loss = 0.02239502\n",
      "Iteration 225, loss = 0.02182693\n",
      "Iteration 91, loss = 0.09656179\n",
      "Iteration 226, loss = 0.02610674\n",
      "Iteration 92, loss = 0.10095716\n",
      "Iteration 93, loss = 0.09667995\n",
      "Iteration 227, loss = 0.02432831\n",
      "Iteration 94, loss = 0.09504465\n",
      "Iteration 95, loss = 0.09220427\n",
      "Iteration 228, loss = 0.02174867\n",
      "Iteration 229, loss = 0.02166092\n",
      "Iteration 230, loss = 0.02319771\n",
      "Iteration 96, loss = 0.09099671\n",
      "Iteration 231, loss = 0.02011433\n",
      "Iteration 232, loss = 0.02083195Iteration 97, loss = 0.09297090\n",
      "\n",
      "Iteration 98, loss = 0.08947293\n",
      "Iteration 233, loss = 0.02207872\n",
      "Iteration 99, loss = 0.09183727\n",
      "Iteration 234, loss = 0.02256229\n",
      "Iteration 100, loss = 0.08729849\n",
      "Iteration 235, loss = 0.02180584\n",
      "Iteration 236, loss = 0.02095165\n",
      "Iteration 101, loss = 0.08657120\n",
      "Iteration 237, loss = 0.02088726\n",
      "Iteration 238, loss = 0.01900964\n",
      "Iteration 239, loss = 0.01777603\n",
      "Iteration 102, loss = 0.08473264\n",
      "Iteration 240, loss = 0.01855619\n",
      "Iteration 103, loss = 0.08513207\n",
      "Iteration 104, loss = 0.08342151\n",
      "Iteration 241, loss = 0.01993729\n",
      "Iteration 105, loss = 0.08484774\n",
      "Iteration 242, loss = 0.01866928\n",
      "Iteration 106, loss = 0.08453606\n",
      "Iteration 243, loss = 0.01732094\n",
      "Iteration 244, loss = 0.01809534\n",
      "Iteration 107, loss = 0.07917097\n",
      "Iteration 108, loss = 0.08266223\n",
      "Iteration 109, loss = 0.08178716\n",
      "Iteration 110, loss = 0.08286802\n",
      "Iteration 245, loss = 0.01658748\n",
      "Iteration 111, loss = 0.08717343\n",
      "Iteration 246, loss = 0.01667547\n",
      "Iteration 112, loss = 0.09024540\n",
      "Iteration 247, loss = 0.01645370\n",
      "Iteration 113, loss = 0.08157474\n",
      "Iteration 248, loss = 0.01772189\n",
      "Iteration 114, loss = 0.08120666\n",
      "Iteration 115, loss = 0.07943744\n",
      "Iteration 249, loss = 0.01713433\n",
      "Iteration 116, loss = 0.07867431\n",
      "Iteration 117, loss = 0.07520419\n",
      "Iteration 250, loss = 0.01758491\n",
      "Iteration 251, loss = 0.01691547\n",
      "Iteration 252, loss = 0.01709161\n",
      "Iteration 118, loss = 0.07180797\n",
      "Iteration 253, loss = 0.01600297\n",
      "Iteration 119, loss = 0.07287271\n",
      "Iteration 120, loss = 0.07290486\n",
      "Iteration 254, loss = 0.01559581\n",
      "Iteration 121, loss = 0.07520983\n",
      "Iteration 255, loss = 0.01683995\n",
      "Iteration 256, loss = 0.01795897\n",
      "Iteration 122, loss = 0.06996545\n",
      "Iteration 257, loss = 0.01646865\n",
      "Iteration 123, loss = 0.07007847\n",
      "Iteration 124, loss = 0.06816003\n",
      "Iteration 258, loss = 0.02040010\n",
      "Iteration 125, loss = 0.06736395\n",
      "Iteration 259, loss = 0.01697041\n",
      "Iteration 126, loss = 0.06967318\n",
      "Iteration 260, loss = 0.01712895\n",
      "Iteration 261, loss = 0.01456875\n",
      "Iteration 127, loss = 0.06899333\n",
      "Iteration 128, loss = 0.06598308\n",
      "Iteration 262, loss = 0.01451141\n",
      "Iteration 129, loss = 0.06526441\n",
      "Iteration 130, loss = 0.06306127\n",
      "Iteration 263, loss = 0.01475214\n",
      "Iteration 264, loss = 0.01484915\n",
      "Iteration 265, loss = 0.01449545\n",
      "Iteration 131, loss = 0.06371509\n",
      "Iteration 266, loss = 0.01585809\n",
      "Iteration 132, loss = 0.06717440\n",
      "Iteration 133, loss = 0.06322318\n",
      "Iteration 267, loss = 0.01995802\n",
      "Iteration 134, loss = 0.06167991\n",
      "Iteration 268, loss = 0.02259322\n",
      "Iteration 269, loss = 0.02536329\n",
      "Iteration 135, loss = 0.05958548\n",
      "Iteration 270, loss = 0.02384733\n",
      "Iteration 136, loss = 0.05884025\n",
      "Iteration 271, loss = 0.01952159\n",
      "Iteration 137, loss = 0.05914100\n",
      "Iteration 272, loss = 0.01816762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 138, loss = 0.06106663\n",
      "Iteration 139, loss = 0.05657419\n",
      "Iteration 140, loss = 0.06180300\n",
      "Iteration 141, loss = 0.05839706\n",
      "Iteration 142, loss = 0.05446489\n",
      "Iteration 143, loss = 0.05349030\n",
      "Iteration 144, loss = 0.05394523\n",
      "Iteration 1, loss = 1.49882123\n",
      "Iteration 2, loss = 0.96784043\n",
      "Iteration 145, loss = 0.05657022\n",
      "Iteration 3, loss = 0.71362950\n",
      "Iteration 146, loss = 0.05213066\n",
      "Iteration 4, loss = 0.59860014\n",
      "Iteration 5, loss = 0.52083414\n",
      "Iteration 147, loss = 0.05380371\n",
      "Iteration 6, loss = 0.47698526\n",
      "Iteration 148, loss = 0.05704289\n",
      "Iteration 7, loss = 0.44231796\n",
      "Iteration 149, loss = 0.05284185\n",
      "Iteration 8, loss = 0.41781772\n",
      "Iteration 150, loss = 0.05241252\n",
      "Iteration 9, loss = 0.39610288\n",
      "Iteration 151, loss = 0.04906195\n",
      "Iteration 10, loss = 0.38048972\n",
      "Iteration 11, loss = 0.36634809\n",
      "Iteration 12, loss = 0.35268396\n",
      "Iteration 13, loss = 0.34266487\n",
      "Iteration 152, loss = 0.04784989\n",
      "Iteration 14, loss = 0.33232061\n",
      "Iteration 153, loss = 0.05003164\n",
      "Iteration 15, loss = 0.32385366\n",
      "Iteration 154, loss = 0.05070515\n",
      "Iteration 16, loss = 0.31589921\n",
      "Iteration 155, loss = 0.04912985\n",
      "Iteration 17, loss = 0.30861866\n",
      "Iteration 18, loss = 0.30127054\n",
      "Iteration 19, loss = 0.29430569\n",
      "Iteration 156, loss = 0.04594645\n",
      "Iteration 20, loss = 0.28855406\n",
      "Iteration 157, loss = 0.04689774\n",
      "Iteration 158, loss = 0.04792593\n",
      "Iteration 21, loss = 0.28509169\n",
      "Iteration 159, loss = 0.05234627\n",
      "Iteration 22, loss = 0.27879410\n",
      "Iteration 23, loss = 0.27316572\n",
      "Iteration 24, loss = 0.26742952\n",
      "Iteration 160, loss = 0.04772521\n",
      "Iteration 161, loss = 0.05040982\n",
      "Iteration 25, loss = 0.26299693\n",
      "Iteration 162, loss = 0.05714674\n",
      "Iteration 163, loss = 0.04707835\n",
      "Iteration 26, loss = 0.25771986\n",
      "Iteration 27, loss = 0.25390549\n",
      "Iteration 28, loss = 0.25145456\n",
      "Iteration 164, loss = 0.04628009\n",
      "Iteration 29, loss = 0.24689357\n",
      "Iteration 30, loss = 0.24208550\n",
      "Iteration 31, loss = 0.23816292\n",
      "Iteration 32, loss = 0.23669354\n",
      "Iteration 165, loss = 0.05146998\n",
      "Iteration 33, loss = 0.23026027\n",
      "Iteration 166, loss = 0.04518388\n",
      "Iteration 167, loss = 0.04593790\n",
      "Iteration 34, loss = 0.22653437\n",
      "Iteration 35, loss = 0.22409518\n",
      "Iteration 36, loss = 0.22080382\n",
      "Iteration 37, loss = 0.21557728\n",
      "Iteration 168, loss = 0.04227281\n",
      "Iteration 169, loss = 0.04528428\n",
      "Iteration 170, loss = 0.04051815\n",
      "Iteration 38, loss = 0.21201248\n",
      "Iteration 171, loss = 0.04321545\n",
      "Iteration 39, loss = 0.20822576\n",
      "Iteration 40, loss = 0.20564380\n",
      "Iteration 41, loss = 0.20197702\n",
      "Iteration 172, loss = 0.04348295\n",
      "Iteration 42, loss = 0.20120088\n",
      "Iteration 173, loss = 0.04710664\n",
      "Iteration 43, loss = 0.19551451\n",
      "Iteration 174, loss = 0.04257272\n",
      "Iteration 44, loss = 0.19321139\n",
      "Iteration 45, loss = 0.19075667\n",
      "Iteration 175, loss = 0.04009421\n",
      "Iteration 46, loss = 0.18739955\n",
      "Iteration 176, loss = 0.03781054\n",
      "Iteration 47, loss = 0.18499268\n",
      "Iteration 48, loss = 0.18196842\n",
      "Iteration 177, loss = 0.04302430\n",
      "Iteration 49, loss = 0.17756786\n",
      "Iteration 50, loss = 0.17792586\n",
      "Iteration 51, loss = 0.17422479\n",
      "Iteration 52, loss = 0.17171011\n",
      "Iteration 53, loss = 0.17076019\n",
      "Iteration 178, loss = 0.04384770\n",
      "Iteration 54, loss = 0.16689768\n",
      "Iteration 55, loss = 0.16445447\n",
      "Iteration 179, loss = 0.05124555\n",
      "Iteration 56, loss = 0.16197490\n",
      "Iteration 57, loss = 0.16291244\n",
      "Iteration 58, loss = 0.15963544\n",
      "Iteration 59, loss = 0.15627368\n",
      "Iteration 180, loss = 0.04887369\n",
      "Iteration 60, loss = 0.15288650\n",
      "Iteration 61, loss = 0.15370443\n",
      "Iteration 62, loss = 0.16242522\n",
      "Iteration 181, loss = 0.04241368\n",
      "Iteration 182, loss = 0.03727759\n",
      "Iteration 63, loss = 0.14883257\n",
      "Iteration 183, loss = 0.03665562\n",
      "Iteration 64, loss = 0.14679736\n",
      "Iteration 184, loss = 0.03631832\n",
      "Iteration 65, loss = 0.14355680\n",
      "Iteration 66, loss = 0.15165067\n",
      "Iteration 185, loss = 0.03450543\n",
      "Iteration 67, loss = 0.15123233\n",
      "Iteration 186, loss = 0.03492018\n",
      "Iteration 68, loss = 0.14779870\n",
      "Iteration 187, loss = 0.03433408\n",
      "Iteration 69, loss = 0.14367029\n",
      "Iteration 70, loss = 0.13995099\n",
      "Iteration 71, loss = 0.13600319\n",
      "Iteration 188, loss = 0.03244673\n",
      "Iteration 72, loss = 0.13454871\n",
      "Iteration 73, loss = 0.13653916\n",
      "Iteration 189, loss = 0.03497639\n",
      "Iteration 74, loss = 0.13262615\n",
      "Iteration 190, loss = 0.03417540\n",
      "Iteration 75, loss = 0.12953111\n",
      "Iteration 76, loss = 0.12832140\n",
      "Iteration 191, loss = 0.03329718\n",
      "Iteration 77, loss = 0.12679285\n",
      "Iteration 192, loss = 0.03583949\n",
      "Iteration 78, loss = 0.12618854\n",
      "Iteration 79, loss = 0.12231723\n",
      "Iteration 80, loss = 0.12069425\n",
      "Iteration 193, loss = 0.03338986\n",
      "Iteration 81, loss = 0.11887713\n",
      "Iteration 82, loss = 0.11741222\n",
      "Iteration 83, loss = 0.11613832\n",
      "Iteration 194, loss = 0.03334032\n",
      "Iteration 84, loss = 0.11813259\n",
      "Iteration 195, loss = 0.03092013\n",
      "Iteration 85, loss = 0.11439533\n",
      "Iteration 196, loss = 0.03146713\n",
      "Iteration 86, loss = 0.11204311\n",
      "Iteration 87, loss = 0.11168918\n",
      "Iteration 197, loss = 0.03187367\n",
      "Iteration 198, loss = 0.03235078\n",
      "Iteration 199, loss = 0.02907715\n",
      "Iteration 88, loss = 0.11627699\n",
      "Iteration 200, loss = 0.02877715\n",
      "Iteration 201, loss = 0.03028235\n",
      "Iteration 89, loss = 0.10993194\n",
      "Iteration 90, loss = 0.10911507\n",
      "Iteration 91, loss = 0.10825069\n",
      "Iteration 202, loss = 0.02948036\n",
      "Iteration 92, loss = 0.10465935\n",
      "Iteration 203, loss = 0.02857063\n",
      "Iteration 204, loss = 0.02904139\n",
      "Iteration 205, loss = 0.02879826\n",
      "Iteration 93, loss = 0.10461171\n",
      "Iteration 206, loss = 0.02801211\n",
      "Iteration 94, loss = 0.10379510\n",
      "Iteration 95, loss = 0.10166350\n",
      "Iteration 207, loss = 0.02865694\n",
      "Iteration 96, loss = 0.10176364\n",
      "Iteration 97, loss = 0.10017300\n",
      "Iteration 208, loss = 0.02670709\n",
      "Iteration 209, loss = 0.02693484\n",
      "Iteration 210, loss = 0.02685891\n",
      "Iteration 98, loss = 0.10115491\n",
      "Iteration 211, loss = 0.02759353\n",
      "Iteration 99, loss = 0.09974251\n",
      "Iteration 100, loss = 0.09660249\n",
      "Iteration 212, loss = 0.02702621\n",
      "Iteration 101, loss = 0.10071717\n",
      "Iteration 213, loss = 0.02833489\n",
      "Iteration 214, loss = 0.02909095\n",
      "Iteration 215, loss = 0.02561519\n",
      "Iteration 102, loss = 0.10420209\n",
      "Iteration 103, loss = 0.09865162\n",
      "Iteration 216, loss = 0.02640016\n",
      "Iteration 104, loss = 0.09643889\n",
      "Iteration 105, loss = 0.10042798\n",
      "Iteration 217, loss = 0.02678735\n",
      "Iteration 218, loss = 0.02585714\n",
      "Iteration 219, loss = 0.02675554\n",
      "Iteration 106, loss = 0.09819806\n",
      "Iteration 107, loss = 0.09479771\n",
      "Iteration 108, loss = 0.09668344\n",
      "Iteration 220, loss = 0.02672438\n",
      "Iteration 109, loss = 0.09425814\n",
      "Iteration 221, loss = 0.02773513\n",
      "Iteration 222, loss = 0.02590814\n",
      "Iteration 110, loss = 0.08767620\n",
      "Iteration 111, loss = 0.08719520\n",
      "Iteration 223, loss = 0.02397602\n",
      "Iteration 112, loss = 0.09792847\n",
      "Iteration 113, loss = 0.10275536\n",
      "Iteration 224, loss = 0.02436691\n",
      "Iteration 225, loss = 0.02422484\n",
      "Iteration 114, loss = 0.09643162\n",
      "Iteration 226, loss = 0.02299143\n",
      "Iteration 227, loss = 0.02268246\n",
      "Iteration 115, loss = 0.09052693\n",
      "Iteration 228, loss = 0.02309625\n",
      "Iteration 116, loss = 0.08617477\n",
      "Iteration 117, loss = 0.08705198\n",
      "Iteration 118, loss = 0.08398949\n",
      "Iteration 229, loss = 0.02233294\n",
      "Iteration 119, loss = 0.08608327\n",
      "Iteration 120, loss = 0.08182403\n",
      "Iteration 121, loss = 0.07838906\n",
      "Iteration 122, loss = 0.07870394\n",
      "Iteration 230, loss = 0.02308265\n",
      "Iteration 123, loss = 0.07714761\n",
      "Iteration 124, loss = 0.07578043\n",
      "Iteration 125, loss = 0.07768109\n",
      "Iteration 231, loss = 0.02245290\n",
      "Iteration 126, loss = 0.07901963\n",
      "Iteration 127, loss = 0.07992424\n",
      "Iteration 232, loss = 0.02346315\n",
      "Iteration 128, loss = 0.07918928\n",
      "Iteration 233, loss = 0.02289284\n",
      "Iteration 129, loss = 0.07378045\n",
      "Iteration 234, loss = 0.02501689\n",
      "Iteration 130, loss = 0.07646038\n",
      "Iteration 131, loss = 0.07894050\n",
      "Iteration 132, loss = 0.07245511\n",
      "Iteration 235, loss = 0.02540327\n",
      "Iteration 133, loss = 0.07352465\n",
      "Iteration 236, loss = 0.02298178\n",
      "Iteration 134, loss = 0.07066100\n",
      "Iteration 237, loss = 0.02345312\n",
      "Iteration 135, loss = 0.07276148\n",
      "Iteration 238, loss = 0.02533396\n",
      "Iteration 136, loss = 0.07520172\n",
      "Iteration 137, loss = 0.07403752\n",
      "Iteration 138, loss = 0.07810977\n",
      "Iteration 239, loss = 0.02317731\n",
      "Iteration 240, loss = 0.02072196\n",
      "Iteration 139, loss = 0.07867115\n",
      "Iteration 241, loss = 0.02528553\n",
      "Iteration 242, loss = 0.02131943\n",
      "Iteration 140, loss = 0.07196229\n",
      "Iteration 243, loss = 0.02071146\n",
      "Iteration 141, loss = 0.07261403\n",
      "Iteration 244, loss = 0.01813356\n",
      "Iteration 142, loss = 0.07174764\n",
      "Iteration 143, loss = 0.07048861\n",
      "Iteration 245, loss = 0.01836678\n",
      "Iteration 144, loss = 0.06497749\n",
      "Iteration 145, loss = 0.06263499\n",
      "Iteration 146, loss = 0.06257043\n",
      "Iteration 246, loss = 0.01792519\n",
      "Iteration 247, loss = 0.02048343\n",
      "Iteration 147, loss = 0.06325130\n",
      "Iteration 248, loss = 0.01901226\n",
      "Iteration 148, loss = 0.06514940\n",
      "Iteration 149, loss = 0.06699145\n",
      "Iteration 249, loss = 0.01887502\n",
      "Iteration 150, loss = 0.06496131\n",
      "Iteration 250, loss = 0.02081063\n",
      "Iteration 251, loss = 0.01862405\n",
      "Iteration 151, loss = 0.06479359\n",
      "Iteration 252, loss = 0.02209771\n",
      "Iteration 253, loss = 0.02045469\n",
      "Iteration 254, loss = 0.02316685\n",
      "Iteration 152, loss = 0.06336066\n",
      "Iteration 255, loss = 0.01972081\n",
      "Iteration 256, loss = 0.01902237\n",
      "Iteration 153, loss = 0.06022407\n",
      "Iteration 257, loss = 0.02052910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 154, loss = 0.05802812\n",
      "Iteration 155, loss = 0.05769419\n",
      "Iteration 156, loss = 0.05743336\n",
      "Iteration 157, loss = 0.05786694\n",
      "Iteration 158, loss = 0.05671089\n",
      "Iteration 159, loss = 0.05765521\n",
      "Iteration 160, loss = 0.05354563\n",
      "Iteration 161, loss = 0.05529352\n",
      "Iteration 162, loss = 0.05522612\n",
      "Iteration 163, loss = 0.05394973\n",
      "Iteration 164, loss = 0.05131911\n",
      "Iteration 1, loss = 1.46473391\n",
      "Iteration 165, loss = 0.05318416\n",
      "Iteration 166, loss = 0.05407233\n",
      "Iteration 167, loss = 0.05602611\n",
      "Iteration 168, loss = 0.05376332\n",
      "Iteration 2, loss = 0.89882979\n",
      "Iteration 169, loss = 0.05261131\n",
      "Iteration 170, loss = 0.05285944\n",
      "Iteration 171, loss = 0.05572601\n",
      "Iteration 3, loss = 0.66031865\n",
      "Iteration 4, loss = 0.55044103\n",
      "Iteration 5, loss = 0.48053571\n",
      "Iteration 172, loss = 0.05470620\n",
      "Iteration 6, loss = 0.44297218\n",
      "Iteration 173, loss = 0.06108459\n",
      "Iteration 174, loss = 0.06489824\n",
      "Iteration 7, loss = 0.41083048\n",
      "Iteration 175, loss = 0.05345103\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.38983897\n",
      "Iteration 9, loss = 0.37038707\n",
      "Iteration 10, loss = 0.35671985\n",
      "Iteration 11, loss = 0.34320097\n",
      "Iteration 12, loss = 0.33235399\n",
      "Iteration 13, loss = 0.32171225\n",
      "Iteration 14, loss = 0.31342552\n",
      "Iteration 15, loss = 0.30211492\n",
      "Iteration 16, loss = 0.29650194\n",
      "Iteration 17, loss = 0.28685778\n",
      "Iteration 1, loss = 1.23049745\n",
      "Iteration 2, loss = 0.76255325\n",
      "Iteration 3, loss = 0.58462600\n",
      "Iteration 4, loss = 0.49036791\n",
      "Iteration 18, loss = 0.28100323\n",
      "Iteration 19, loss = 0.27787865\n",
      "Iteration 20, loss = 0.26768362\n",
      "Iteration 21, loss = 0.26149993\n",
      "Iteration 5, loss = 0.45116412\n",
      "Iteration 6, loss = 0.40875981\n",
      "Iteration 7, loss = 0.38755548\n",
      "Iteration 22, loss = 0.25886586\n",
      "Iteration 8, loss = 0.36542740\n",
      "Iteration 23, loss = 0.25096903\n",
      "Iteration 24, loss = 0.24559585\n",
      "Iteration 9, loss = 0.34894468\n",
      "Iteration 25, loss = 0.23870891\n",
      "Iteration 10, loss = 0.33545494\n",
      "Iteration 11, loss = 0.32230363\n",
      "Iteration 26, loss = 0.23458394\n",
      "Iteration 12, loss = 0.31226923\n",
      "Iteration 13, loss = 0.30246657\n",
      "Iteration 27, loss = 0.22995533\n",
      "Iteration 14, loss = 0.29271736\n",
      "Iteration 28, loss = 0.22415090\n",
      "Iteration 15, loss = 0.28526963\n",
      "Iteration 29, loss = 0.21962079\n",
      "Iteration 16, loss = 0.27710679\n",
      "Iteration 30, loss = 0.21626120\n",
      "Iteration 17, loss = 0.27128809\n",
      "Iteration 31, loss = 0.21200463\n",
      "Iteration 32, loss = 0.21075376\n",
      "Iteration 18, loss = 0.26400753\n",
      "Iteration 33, loss = 0.20613665\n",
      "Iteration 19, loss = 0.25871354\n",
      "Iteration 20, loss = 0.25157432\n",
      "Iteration 34, loss = 0.20060187\n",
      "Iteration 21, loss = 0.24584820\n",
      "Iteration 22, loss = 0.24062521\n",
      "Iteration 35, loss = 0.19771187\n",
      "Iteration 23, loss = 0.23636945\n",
      "Iteration 36, loss = 0.19162268\n",
      "Iteration 37, loss = 0.19207982\n",
      "Iteration 24, loss = 0.23145060\n",
      "Iteration 38, loss = 0.18463065\n",
      "Iteration 25, loss = 0.22708129\n",
      "Iteration 26, loss = 0.22241594\n",
      "Iteration 39, loss = 0.18148784\n",
      "Iteration 40, loss = 0.17911651\n",
      "Iteration 27, loss = 0.21722777\n",
      "Iteration 41, loss = 0.17396746\n",
      "Iteration 28, loss = 0.21327579\n",
      "Iteration 42, loss = 0.17227435\n",
      "Iteration 29, loss = 0.20960751\n",
      "Iteration 30, loss = 0.20597889\n",
      "Iteration 43, loss = 0.16968039\n",
      "Iteration 31, loss = 0.20123838\n",
      "Iteration 44, loss = 0.16526770\n",
      "Iteration 32, loss = 0.19761412\n",
      "Iteration 45, loss = 0.16382791\n",
      "Iteration 33, loss = 0.19684720\n",
      "Iteration 46, loss = 0.15989711\n",
      "Iteration 34, loss = 0.19151017\n",
      "Iteration 47, loss = 0.15434914\n",
      "Iteration 35, loss = 0.19078296\n",
      "Iteration 48, loss = 0.15412223\n",
      "Iteration 36, loss = 0.18571122\n",
      "Iteration 49, loss = 0.15388577\n",
      "Iteration 37, loss = 0.18344366\n",
      "Iteration 50, loss = 0.15023492\n",
      "Iteration 38, loss = 0.17837758\n",
      "Iteration 51, loss = 0.14365975\n",
      "Iteration 39, loss = 0.17601925\n",
      "Iteration 40, loss = 0.17280970\n",
      "Iteration 52, loss = 0.14236573\n",
      "Iteration 53, loss = 0.13952205\n",
      "Iteration 54, loss = 0.13668134\n",
      "Iteration 41, loss = 0.16894440\n",
      "Iteration 55, loss = 0.13270704\n",
      "Iteration 56, loss = 0.13245066\n",
      "Iteration 42, loss = 0.16672198\n",
      "Iteration 57, loss = 0.12959169\n",
      "Iteration 43, loss = 0.16401349\n",
      "Iteration 58, loss = 0.12758076\n",
      "Iteration 44, loss = 0.16271828\n",
      "Iteration 59, loss = 0.12636661\n",
      "Iteration 45, loss = 0.15832093\n",
      "Iteration 46, loss = 0.15826069\n",
      "Iteration 60, loss = 0.12003748\n",
      "Iteration 47, loss = 0.15320651\n",
      "Iteration 48, loss = 0.15253039\n",
      "Iteration 61, loss = 0.12087923\n",
      "Iteration 49, loss = 0.15021207\n",
      "Iteration 62, loss = 0.11756041\n",
      "Iteration 63, loss = 0.11867254\n",
      "Iteration 64, loss = 0.12018854\n",
      "Iteration 50, loss = 0.14883311\n",
      "Iteration 65, loss = 0.11582981\n",
      "Iteration 51, loss = 0.14547225\n",
      "Iteration 52, loss = 0.14362878\n",
      "Iteration 66, loss = 0.11104076\n",
      "Iteration 53, loss = 0.14261098\n",
      "Iteration 67, loss = 0.10753979\n",
      "Iteration 68, loss = 0.10579482\n",
      "Iteration 54, loss = 0.14218118\n",
      "Iteration 69, loss = 0.10452680\n",
      "Iteration 70, loss = 0.10273402\n",
      "Iteration 55, loss = 0.13712433\n",
      "Iteration 56, loss = 0.13716952\n",
      "Iteration 71, loss = 0.10091949\n",
      "Iteration 57, loss = 0.13642851\n",
      "Iteration 72, loss = 0.09928004\n",
      "Iteration 58, loss = 0.13239706\n",
      "Iteration 73, loss = 0.09743588\n",
      "Iteration 59, loss = 0.13302346\n",
      "Iteration 74, loss = 0.09482187\n",
      "Iteration 60, loss = 0.13068342\n",
      "Iteration 61, loss = 0.12821456\n",
      "Iteration 62, loss = 0.12736214\n",
      "Iteration 63, loss = 0.12410129\n",
      "Iteration 75, loss = 0.09577036\n",
      "Iteration 64, loss = 0.12262673\n",
      "Iteration 65, loss = 0.12322647\n",
      "Iteration 76, loss = 0.09348720\n",
      "Iteration 66, loss = 0.12671817\n",
      "Iteration 77, loss = 0.09586760\n",
      "Iteration 67, loss = 0.12103620\n",
      "Iteration 78, loss = 0.08942347\n",
      "Iteration 68, loss = 0.11834332\n",
      "Iteration 79, loss = 0.09012510\n",
      "Iteration 80, loss = 0.09630374\n",
      "Iteration 81, loss = 0.09193013\n",
      "Iteration 69, loss = 0.11832982\n",
      "Iteration 82, loss = 0.08886174\n",
      "Iteration 70, loss = 0.11618226\n",
      "Iteration 71, loss = 0.11285788\n",
      "Iteration 72, loss = 0.11186093\n",
      "Iteration 83, loss = 0.08157646\n",
      "Iteration 84, loss = 0.08422095\n",
      "Iteration 85, loss = 0.08253183\n",
      "Iteration 73, loss = 0.11154560\n",
      "Iteration 86, loss = 0.08072934\n",
      "Iteration 74, loss = 0.11057938\n",
      "Iteration 87, loss = 0.08073237\n",
      "Iteration 75, loss = 0.11457364\n",
      "Iteration 76, loss = 0.10709553\n",
      "Iteration 88, loss = 0.07689886\n",
      "Iteration 77, loss = 0.10656999\n",
      "Iteration 89, loss = 0.07546046\n",
      "Iteration 90, loss = 0.07402379\n",
      "Iteration 78, loss = 0.10633917\n",
      "Iteration 91, loss = 0.07528924\n",
      "Iteration 79, loss = 0.10320666\n",
      "Iteration 92, loss = 0.07952225\n",
      "Iteration 80, loss = 0.10352793\n",
      "Iteration 93, loss = 0.07460346\n",
      "Iteration 81, loss = 0.10230393\n",
      "Iteration 94, loss = 0.07156825\n",
      "Iteration 95, loss = 0.06853368\n",
      "Iteration 82, loss = 0.10053564\n",
      "Iteration 96, loss = 0.06869327\n",
      "Iteration 83, loss = 0.10009642\n",
      "Iteration 84, loss = 0.10051736\n",
      "Iteration 97, loss = 0.06821457\n",
      "Iteration 85, loss = 0.09945979\n",
      "Iteration 86, loss = 0.09618013\n",
      "Iteration 98, loss = 0.06727872\n",
      "Iteration 87, loss = 0.09543470\n",
      "Iteration 99, loss = 0.06796294\n",
      "Iteration 88, loss = 0.09535989\n",
      "Iteration 100, loss = 0.06737678\n",
      "Iteration 89, loss = 0.09310327\n",
      "Iteration 101, loss = 0.06791739\n",
      "Iteration 102, loss = 0.06571999\n",
      "Iteration 103, loss = 0.06153608\n",
      "Iteration 90, loss = 0.09230885\n",
      "Iteration 104, loss = 0.06099288\n",
      "Iteration 105, loss = 0.06064991\n",
      "Iteration 91, loss = 0.09226678\n",
      "Iteration 92, loss = 0.08960966\n",
      "Iteration 106, loss = 0.06321458\n",
      "Iteration 93, loss = 0.09011750\n",
      "Iteration 94, loss = 0.09000964\n",
      "Iteration 107, loss = 0.06370041\n",
      "Iteration 95, loss = 0.09033149\n",
      "Iteration 96, loss = 0.08567191\n",
      "Iteration 108, loss = 0.06346122\n",
      "Iteration 109, loss = 0.06021241\n",
      "Iteration 97, loss = 0.08693005\n",
      "Iteration 110, loss = 0.05780906\n",
      "Iteration 111, loss = 0.05544995\n",
      "Iteration 98, loss = 0.08587217\n",
      "Iteration 112, loss = 0.05636140\n",
      "Iteration 99, loss = 0.08310654\n",
      "Iteration 113, loss = 0.05914806\n",
      "Iteration 100, loss = 0.08705865\n",
      "Iteration 114, loss = 0.05683303\n",
      "Iteration 101, loss = 0.08594601\n",
      "Iteration 115, loss = 0.05287207\n",
      "Iteration 102, loss = 0.08037506\n",
      "Iteration 116, loss = 0.05231476\n",
      "Iteration 103, loss = 0.08171550\n",
      "Iteration 104, loss = 0.08369783Iteration 117, loss = 0.05246239\n",
      "\n",
      "Iteration 105, loss = 0.07911920\n",
      "Iteration 118, loss = 0.05060322\n",
      "Iteration 119, loss = 0.05241033\n",
      "Iteration 106, loss = 0.07925634\n",
      "Iteration 120, loss = 0.04920451\n",
      "Iteration 107, loss = 0.07778291\n",
      "Iteration 121, loss = 0.04963995\n",
      "Iteration 108, loss = 0.07999585\n",
      "Iteration 122, loss = 0.04927532\n",
      "Iteration 109, loss = 0.07613483\n",
      "Iteration 123, loss = 0.04791658\n",
      "Iteration 110, loss = 0.07508584\n",
      "Iteration 124, loss = 0.04694058\n",
      "Iteration 111, loss = 0.07528706\n",
      "Iteration 112, loss = 0.07508455\n",
      "Iteration 113, loss = 0.07599333\n",
      "Iteration 125, loss = 0.04458711\n",
      "Iteration 126, loss = 0.04449647\n",
      "Iteration 114, loss = 0.07479204\n",
      "Iteration 127, loss = 0.04393406\n",
      "Iteration 115, loss = 0.07741368\n",
      "Iteration 128, loss = 0.04508032\n",
      "Iteration 116, loss = 0.07365797\n",
      "Iteration 129, loss = 0.04279851\n",
      "Iteration 130, loss = 0.04319172\n",
      "Iteration 131, loss = 0.04369342\n",
      "Iteration 117, loss = 0.07058160\n",
      "Iteration 118, loss = 0.06836127\n",
      "Iteration 132, loss = 0.04527965\n",
      "Iteration 119, loss = 0.06908171\n",
      "Iteration 133, loss = 0.04992492\n",
      "Iteration 120, loss = 0.06717409\n",
      "Iteration 134, loss = 0.05113975\n",
      "Iteration 121, loss = 0.06689434\n",
      "Iteration 135, loss = 0.05047800\n",
      "Iteration 122, loss = 0.07125053\n",
      "Iteration 136, loss = 0.04673057\n",
      "Iteration 137, loss = 0.04588269\n",
      "Iteration 138, loss = 0.04094221\n",
      "Iteration 123, loss = 0.07296935\n",
      "Iteration 139, loss = 0.03876658\n",
      "Iteration 124, loss = 0.06840338\n",
      "Iteration 125, loss = 0.06541459\n",
      "Iteration 140, loss = 0.03715858\n",
      "Iteration 141, loss = 0.03701384\n",
      "Iteration 142, loss = 0.03676536\n",
      "Iteration 126, loss = 0.06414200\n",
      "Iteration 143, loss = 0.03547019\n",
      "Iteration 127, loss = 0.06433280\n",
      "Iteration 128, loss = 0.06419955\n",
      "Iteration 129, loss = 0.06382246\n",
      "Iteration 144, loss = 0.03574306\n",
      "Iteration 145, loss = 0.03930461\n",
      "Iteration 130, loss = 0.06456366\n",
      "Iteration 146, loss = 0.03872318\n",
      "Iteration 131, loss = 0.06131712\n",
      "Iteration 147, loss = 0.03781289\n",
      "Iteration 132, loss = 0.06103952\n",
      "Iteration 133, loss = 0.06115784\n",
      "Iteration 148, loss = 0.03488187\n",
      "Iteration 134, loss = 0.05872265\n",
      "Iteration 135, loss = 0.05740643\n",
      "Iteration 149, loss = 0.03370364\n",
      "Iteration 136, loss = 0.05665261\n",
      "Iteration 150, loss = 0.03262805\n",
      "Iteration 151, loss = 0.03252107\n",
      "Iteration 137, loss = 0.05739936\n",
      "Iteration 152, loss = 0.03235078\n",
      "Iteration 138, loss = 0.05726331\n",
      "Iteration 139, loss = 0.05549097\n",
      "Iteration 153, loss = 0.03173263\n",
      "Iteration 140, loss = 0.05968823\n",
      "Iteration 154, loss = 0.03039621\n",
      "Iteration 141, loss = 0.05812444\n",
      "Iteration 155, loss = 0.03090972\n",
      "Iteration 142, loss = 0.05897510\n",
      "Iteration 156, loss = 0.02997750\n",
      "Iteration 157, loss = 0.02945861\n",
      "Iteration 143, loss = 0.05513911\n",
      "Iteration 144, loss = 0.05632233\n",
      "Iteration 158, loss = 0.03217777\n",
      "Iteration 145, loss = 0.05715363\n",
      "Iteration 159, loss = 0.03330071\n",
      "Iteration 146, loss = 0.05441124\n",
      "Iteration 160, loss = 0.03403648\n",
      "Iteration 161, loss = 0.02996159\n",
      "Iteration 147, loss = 0.05134279\n",
      "Iteration 162, loss = 0.02939104\n",
      "Iteration 148, loss = 0.05164040\n",
      "Iteration 163, loss = 0.02730327\n",
      "Iteration 149, loss = 0.05043196\n",
      "Iteration 150, loss = 0.05096813\n",
      "Iteration 164, loss = 0.02690801\n",
      "Iteration 151, loss = 0.05251807\n",
      "Iteration 165, loss = 0.02783177\n",
      "Iteration 152, loss = 0.05083090\n",
      "Iteration 166, loss = 0.02770086\n",
      "Iteration 153, loss = 0.04987407\n",
      "Iteration 167, loss = 0.02704438\n",
      "Iteration 154, loss = 0.04798365\n",
      "Iteration 168, loss = 0.02588223\n",
      "Iteration 155, loss = 0.04652039\n",
      "Iteration 169, loss = 0.02607992\n",
      "Iteration 170, loss = 0.02576537\n",
      "Iteration 156, loss = 0.04678314\n",
      "Iteration 171, loss = 0.02570212\n",
      "Iteration 172, loss = 0.02596900\n",
      "Iteration 157, loss = 0.04842340\n",
      "Iteration 173, loss = 0.02546072\n",
      "Iteration 174, loss = 0.02502237\n",
      "Iteration 158, loss = 0.04739951\n",
      "Iteration 175, loss = 0.02324839\n",
      "Iteration 159, loss = 0.04625956\n",
      "Iteration 176, loss = 0.02407359\n",
      "Iteration 177, loss = 0.02351625\n",
      "Iteration 178, loss = 0.02430351\n",
      "Iteration 179, loss = 0.02287463\n",
      "Iteration 160, loss = 0.04483656\n",
      "Iteration 180, loss = 0.02233634\n",
      "Iteration 161, loss = 0.04350738\n",
      "Iteration 181, loss = 0.02155883\n",
      "Iteration 162, loss = 0.04267112\n",
      "Iteration 182, loss = 0.02118342\n",
      "Iteration 163, loss = 0.04356679\n",
      "Iteration 183, loss = 0.02129746\n",
      "Iteration 164, loss = 0.04447109\n",
      "Iteration 184, loss = 0.02091870\n",
      "Iteration 185, loss = 0.02141085\n",
      "Iteration 165, loss = 0.04840196\n",
      "Iteration 166, loss = 0.04710145\n",
      "Iteration 167, loss = 0.04799474\n",
      "Iteration 186, loss = 0.02062105\n",
      "Iteration 168, loss = 0.04655680\n",
      "Iteration 187, loss = 0.02166111\n",
      "Iteration 188, loss = 0.02042138\n",
      "Iteration 189, loss = 0.02187633\n",
      "Iteration 190, loss = 0.02339109\n",
      "Iteration 191, loss = 0.02080607\n",
      "Iteration 169, loss = 0.04189936\n",
      "Iteration 192, loss = 0.01925820\n",
      "Iteration 193, loss = 0.01912597\n",
      "Iteration 170, loss = 0.05110790\n",
      "Iteration 194, loss = 0.01873723\n",
      "Iteration 171, loss = 0.04529514\n",
      "Iteration 195, loss = 0.01822774\n",
      "Iteration 172, loss = 0.04112290\n",
      "Iteration 173, loss = 0.04102738\n",
      "Iteration 174, loss = 0.04116226\n",
      "Iteration 196, loss = 0.01805658\n",
      "Iteration 197, loss = 0.01841799\n",
      "Iteration 198, loss = 0.01828605\n",
      "Iteration 175, loss = 0.04036478\n",
      "Iteration 199, loss = 0.01907009\n",
      "Iteration 200, loss = 0.01989082\n",
      "Iteration 201, loss = 0.02009432\n",
      "Iteration 176, loss = 0.03740709\n",
      "Iteration 177, loss = 0.03838098\n",
      "Iteration 202, loss = 0.01983846\n",
      "Iteration 178, loss = 0.04247002\n",
      "Iteration 203, loss = 0.01991246\n",
      "Iteration 204, loss = 0.01700707\n",
      "Iteration 179, loss = 0.04416005\n",
      "Iteration 205, loss = 0.01700601\n",
      "Iteration 180, loss = 0.03715392\n",
      "Iteration 206, loss = 0.01781934\n",
      "Iteration 207, loss = 0.01780458\n",
      "Iteration 208, loss = 0.01589140\n",
      "Iteration 209, loss = 0.01561007\n",
      "Iteration 181, loss = 0.03600892\n",
      "Iteration 210, loss = 0.01498035\n",
      "Iteration 211, loss = 0.01631781\n",
      "Iteration 212, loss = 0.01546920\n",
      "Iteration 182, loss = 0.03633215\n",
      "Iteration 213, loss = 0.01546212\n",
      "Iteration 183, loss = 0.03475167\n",
      "Iteration 214, loss = 0.01507101\n",
      "Iteration 215, loss = 0.01468706Iteration 184, loss = 0.03679336\n",
      "\n",
      "Iteration 185, loss = 0.03666806\n",
      "Iteration 216, loss = 0.01483646\n",
      "Iteration 186, loss = 0.03847108\n",
      "Iteration 187, loss = 0.03886772\n",
      "Iteration 217, loss = 0.01387603\n",
      "Iteration 188, loss = 0.04015228\n",
      "Iteration 218, loss = 0.01443131\n",
      "Iteration 219, loss = 0.01360578\n",
      "Iteration 189, loss = 0.03460186\n",
      "Iteration 220, loss = 0.01353519\n",
      "Iteration 221, loss = 0.01305611\n",
      "Iteration 190, loss = 0.03361504\n",
      "Iteration 191, loss = 0.03321136\n",
      "Iteration 222, loss = 0.01291560\n",
      "Iteration 192, loss = 0.03118318\n",
      "Iteration 223, loss = 0.01308305\n",
      "Iteration 224, loss = 0.01307036\n",
      "Iteration 225, loss = 0.01276998\n",
      "Iteration 226, loss = 0.01277182\n",
      "Iteration 193, loss = 0.03144070\n",
      "Iteration 227, loss = 0.01230151\n",
      "Iteration 194, loss = 0.03200112\n",
      "Iteration 228, loss = 0.01215879\n",
      "Iteration 229, loss = 0.01194152\n",
      "Iteration 195, loss = 0.03134196\n",
      "Iteration 230, loss = 0.01180553\n",
      "Iteration 231, loss = 0.01276566\n",
      "Iteration 232, loss = 0.01220038\n",
      "Iteration 196, loss = 0.03053135\n",
      "Iteration 233, loss = 0.01205956\n",
      "Iteration 234, loss = 0.01199109\n",
      "Iteration 235, loss = 0.01164429\n",
      "Iteration 197, loss = 0.02951832\n",
      "Iteration 236, loss = 0.01289389\n",
      "Iteration 198, loss = 0.03085441\n",
      "Iteration 237, loss = 0.01294202\n",
      "Iteration 238, loss = 0.01252831\n",
      "Iteration 199, loss = 0.03005888\n",
      "Iteration 239, loss = 0.01287780\n",
      "Iteration 200, loss = 0.03004588\n",
      "Iteration 240, loss = 0.01352573\n",
      "Iteration 201, loss = 0.03203824\n",
      "Iteration 202, loss = 0.03015806\n",
      "Iteration 241, loss = 0.01259776\n",
      "Iteration 203, loss = 0.02997922\n",
      "Iteration 242, loss = 0.01234033\n",
      "Iteration 243, loss = 0.01154539\n",
      "Iteration 204, loss = 0.03035498\n",
      "Iteration 244, loss = 0.01257590\n",
      "Iteration 205, loss = 0.02910153\n",
      "Iteration 245, loss = 0.01250501\n",
      "Iteration 246, loss = 0.01056719\n",
      "Iteration 206, loss = 0.02797381\n",
      "Iteration 247, loss = 0.01088701\n",
      "Iteration 248, loss = 0.01087963\n",
      "Iteration 207, loss = 0.02801390\n",
      "Iteration 249, loss = 0.01077005\n",
      "Iteration 208, loss = 0.02987971\n",
      "Iteration 250, loss = 0.00965970\n",
      "Iteration 209, loss = 0.03389891\n",
      "Iteration 210, loss = 0.03539309\n",
      "Iteration 211, loss = 0.03433438\n",
      "Iteration 251, loss = 0.00955870\n",
      "Iteration 252, loss = 0.00934121\n",
      "Iteration 253, loss = 0.00941877\n",
      "Iteration 254, loss = 0.00899542\n",
      "Iteration 212, loss = 0.03739356\n",
      "Iteration 255, loss = 0.00882481\n",
      "Iteration 256, loss = 0.00927097\n",
      "Iteration 213, loss = 0.02713047\n",
      "Iteration 257, loss = 0.00889436\n",
      "Iteration 214, loss = 0.02720202\n",
      "Iteration 258, loss = 0.00953215\n",
      "Iteration 215, loss = 0.02967715\n",
      "Iteration 259, loss = 0.00897414\n",
      "Iteration 216, loss = 0.02660292\n",
      "Iteration 260, loss = 0.00904548\n",
      "Iteration 261, loss = 0.00867183\n",
      "Iteration 217, loss = 0.02450413\n",
      "Iteration 262, loss = 0.00857993\n",
      "Iteration 218, loss = 0.02482063\n",
      "Iteration 219, loss = 0.02379168\n",
      "Iteration 263, loss = 0.00816105\n",
      "Iteration 220, loss = 0.02410763\n",
      "Iteration 264, loss = 0.00842430\n",
      "Iteration 265, loss = 0.00830699\n",
      "Iteration 266, loss = 0.00790993\n",
      "Iteration 267, loss = 0.00800223\n",
      "Iteration 221, loss = 0.02392651\n",
      "Iteration 268, loss = 0.00798869\n",
      "Iteration 269, loss = 0.00908560\n",
      "Iteration 270, loss = 0.00863838\n",
      "Iteration 222, loss = 0.02545497\n",
      "Iteration 223, loss = 0.02342404\n",
      "Iteration 271, loss = 0.00911093\n",
      "Iteration 224, loss = 0.02258626\n",
      "Iteration 225, loss = 0.02545730\n",
      "Iteration 272, loss = 0.00747270\n",
      "Iteration 273, loss = 0.00786303\n",
      "Iteration 226, loss = 0.02524737\n",
      "Iteration 274, loss = 0.00754878\n",
      "Iteration 227, loss = 0.02501393\n",
      "Iteration 275, loss = 0.00836540\n",
      "Iteration 228, loss = 0.02241229\n",
      "Iteration 276, loss = 0.00821599\n",
      "Iteration 229, loss = 0.02335836\n",
      "Iteration 277, loss = 0.00756492\n",
      "Iteration 278, loss = 0.00779506\n",
      "Iteration 230, loss = 0.02417794\n",
      "Iteration 279, loss = 0.00711185\n",
      "Iteration 280, loss = 0.00708614\n",
      "Iteration 281, loss = 0.00691387\n",
      "Iteration 231, loss = 0.02561357\n",
      "Iteration 282, loss = 0.00678295\n",
      "Iteration 232, loss = 0.02526518\n",
      "Iteration 233, loss = 0.02308318\n",
      "Iteration 283, loss = 0.00667443\n",
      "Iteration 234, loss = 0.02265033\n",
      "Iteration 284, loss = 0.00683774\n",
      "Iteration 235, loss = 0.02238062\n",
      "Iteration 285, loss = 0.00649025\n",
      "Iteration 236, loss = 0.02391132\n",
      "Iteration 286, loss = 0.00652529\n",
      "Iteration 237, loss = 0.02247988\n",
      "Iteration 238, loss = 0.02148102\n",
      "Iteration 287, loss = 0.00653097\n",
      "Iteration 239, loss = 0.01971361\n",
      "Iteration 288, loss = 0.00642816\n",
      "Iteration 289, loss = 0.00634752\n",
      "Iteration 240, loss = 0.02007409\n",
      "Iteration 290, loss = 0.00640865\n",
      "Iteration 241, loss = 0.01914275\n",
      "Iteration 242, loss = 0.01887047\n",
      "Iteration 291, loss = 0.00696727\n",
      "Iteration 243, loss = 0.01932709\n",
      "Iteration 292, loss = 0.00813101\n",
      "Iteration 293, loss = 0.00740984\n",
      "Iteration 244, loss = 0.01915153\n",
      "Iteration 294, loss = 0.00653611\n",
      "Iteration 245, loss = 0.01849268\n",
      "Iteration 295, loss = 0.00579601\n",
      "Iteration 246, loss = 0.01879479\n",
      "Iteration 296, loss = 0.00616554\n",
      "Iteration 297, loss = 0.00631443\n",
      "Iteration 247, loss = 0.01879165\n",
      "Iteration 248, loss = 0.01891718\n",
      "Iteration 249, loss = 0.01997433\n",
      "Iteration 298, loss = 0.00573885\n",
      "Iteration 250, loss = 0.01752037\n",
      "Iteration 299, loss = 0.00561917\n",
      "Iteration 300, loss = 0.00561772\n",
      "Iteration 301, loss = 0.00564274\n",
      "Iteration 251, loss = 0.01884579\n",
      "Iteration 302, loss = 0.00552048\n",
      "Iteration 252, loss = 0.01969858\n",
      "Iteration 253, loss = 0.01989953\n",
      "Iteration 303, loss = 0.00566175\n",
      "Iteration 304, loss = 0.00536986\n",
      "Iteration 305, loss = 0.00551372Iteration 254, loss = 0.02134718\n",
      "\n",
      "Iteration 255, loss = 0.02025044\n",
      "Iteration 306, loss = 0.00563230\n",
      "Iteration 256, loss = 0.01994268\n",
      "Iteration 307, loss = 0.00535990\n",
      "Iteration 257, loss = 0.01874143\n",
      "Iteration 258, loss = 0.01864498\n",
      "Iteration 308, loss = 0.00525191\n",
      "Iteration 309, loss = 0.00525446\n",
      "Iteration 259, loss = 0.01972309\n",
      "Iteration 310, loss = 0.00514583\n",
      "Iteration 260, loss = 0.01885904\n",
      "Iteration 311, loss = 0.00515069\n",
      "Iteration 261, loss = 0.01761060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 312, loss = 0.00495687\n",
      "Iteration 313, loss = 0.00513078\n",
      "Iteration 314, loss = 0.00488872\n",
      "Iteration 315, loss = 0.00495005\n",
      "Iteration 316, loss = 0.00487819\n",
      "Iteration 317, loss = 0.00521268\n",
      "Iteration 318, loss = 0.00531976\n",
      "Iteration 319, loss = 0.00495871\n",
      "Iteration 320, loss = 0.00507738\n",
      "Iteration 1, loss = 1.39205873\n",
      "Iteration 2, loss = 0.96717799\n",
      "Iteration 321, loss = 0.00504728\n",
      "Iteration 322, loss = 0.00498872\n",
      "Iteration 3, loss = 0.73262894\n",
      "Iteration 323, loss = 0.00487310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.62223705\n",
      "Iteration 5, loss = 0.54279227\n",
      "Iteration 6, loss = 0.49525040\n",
      "Iteration 7, loss = 0.45945620\n",
      "Iteration 8, loss = 0.43006467\n",
      "Iteration 9, loss = 0.40825183\n",
      "Iteration 10, loss = 0.39106517\n",
      "Iteration 11, loss = 0.37658144\n",
      "Iteration 12, loss = 0.36477328\n",
      "Iteration 13, loss = 0.35192236\n",
      "Iteration 14, loss = 0.34060373\n",
      "Iteration 15, loss = 0.33167758\n",
      "Iteration 16, loss = 0.32271201\n",
      "Iteration 1, loss = 1.37034607\n",
      "Iteration 17, loss = 0.31473631\n",
      "Iteration 2, loss = 0.88549934\n",
      "Iteration 18, loss = 0.30670894\n",
      "Iteration 19, loss = 0.29948936\n",
      "Iteration 20, loss = 0.29324010\n",
      "Iteration 3, loss = 0.67717002\n",
      "Iteration 21, loss = 0.28862967\n",
      "Iteration 22, loss = 0.28091419\n",
      "Iteration 4, loss = 0.57813647\n",
      "Iteration 5, loss = 0.51456762\n",
      "Iteration 6, loss = 0.47546963\n",
      "Iteration 23, loss = 0.28015525\n",
      "Iteration 24, loss = 0.27137699\n",
      "Iteration 25, loss = 0.26840578\n",
      "Iteration 7, loss = 0.44493308\n",
      "Iteration 26, loss = 0.26073449\n",
      "Iteration 8, loss = 0.41832775\n",
      "Iteration 27, loss = 0.25652159\n",
      "Iteration 9, loss = 0.40047947\n",
      "Iteration 28, loss = 0.25100444\n",
      "Iteration 29, loss = 0.24528730\n",
      "Iteration 10, loss = 0.38004373\n",
      "Iteration 30, loss = 0.24233444\n",
      "Iteration 11, loss = 0.36515391\n",
      "Iteration 31, loss = 0.23668085\n",
      "Iteration 12, loss = 0.35481470\n",
      "Iteration 32, loss = 0.23388137\n",
      "Iteration 13, loss = 0.34237885\n",
      "Iteration 14, loss = 0.32986106\n",
      "Iteration 33, loss = 0.22922438\n",
      "Iteration 15, loss = 0.31902859\n",
      "Iteration 16, loss = 0.31154966\n",
      "Iteration 34, loss = 0.22582454\n",
      "Iteration 35, loss = 0.22467394\n",
      "Iteration 17, loss = 0.30301022\n",
      "Iteration 36, loss = 0.21680868\n",
      "Iteration 18, loss = 0.29597625\n",
      "Iteration 37, loss = 0.21371951\n",
      "Iteration 19, loss = 0.28786728\n",
      "Iteration 38, loss = 0.21017679\n",
      "Iteration 20, loss = 0.27979079\n",
      "Iteration 39, loss = 0.20610759\n",
      "Iteration 40, loss = 0.20473808\n",
      "Iteration 21, loss = 0.27611607\n",
      "Iteration 41, loss = 0.20119873\n",
      "Iteration 22, loss = 0.26847745\n",
      "Iteration 42, loss = 0.19742869\n",
      "Iteration 23, loss = 0.26383710\n",
      "Iteration 43, loss = 0.19546098\n",
      "Iteration 24, loss = 0.25622407\n",
      "Iteration 44, loss = 0.19020497\n",
      "Iteration 45, loss = 0.18865234\n",
      "Iteration 25, loss = 0.25253896\n",
      "Iteration 26, loss = 0.24765894\n",
      "Iteration 27, loss = 0.24443607\n",
      "Iteration 46, loss = 0.18354661\n",
      "Iteration 28, loss = 0.23890194\n",
      "Iteration 47, loss = 0.18293260\n",
      "Iteration 48, loss = 0.17786678\n",
      "Iteration 29, loss = 0.23350047\n",
      "Iteration 30, loss = 0.23359994\n",
      "Iteration 49, loss = 0.17641884\n",
      "Iteration 31, loss = 0.23048135\n",
      "Iteration 32, loss = 0.22525027\n",
      "Iteration 50, loss = 0.17231580\n",
      "Iteration 51, loss = 0.16904456\n",
      "Iteration 52, loss = 0.16708189\n",
      "Iteration 33, loss = 0.22223440\n",
      "Iteration 53, loss = 0.16350548\n",
      "Iteration 34, loss = 0.21545445\n",
      "Iteration 54, loss = 0.16379062\n",
      "Iteration 35, loss = 0.21090160\n",
      "Iteration 36, loss = 0.20970090\n",
      "Iteration 37, loss = 0.20669379\n",
      "Iteration 55, loss = 0.15833962\n",
      "Iteration 38, loss = 0.20234171\n",
      "Iteration 39, loss = 0.19923842\n",
      "Iteration 56, loss = 0.15781131\n",
      "Iteration 57, loss = 0.15520875\n",
      "Iteration 40, loss = 0.19354564\n",
      "Iteration 58, loss = 0.15510308\n",
      "Iteration 41, loss = 0.19315864\n",
      "Iteration 42, loss = 0.19052813\n",
      "Iteration 59, loss = 0.14892780\n",
      "Iteration 43, loss = 0.18628411\n",
      "Iteration 60, loss = 0.14638865\n",
      "Iteration 44, loss = 0.18321240\n",
      "Iteration 45, loss = 0.18000015\n",
      "Iteration 61, loss = 0.14569337\n",
      "Iteration 46, loss = 0.17622209\n",
      "Iteration 62, loss = 0.14260904\n",
      "Iteration 47, loss = 0.17509597\n",
      "Iteration 63, loss = 0.14201142\n",
      "Iteration 48, loss = 0.17016305\n",
      "Iteration 49, loss = 0.16804123\n",
      "Iteration 64, loss = 0.13768550\n",
      "Iteration 50, loss = 0.16567176\n",
      "Iteration 51, loss = 0.16179143\n",
      "Iteration 65, loss = 0.13847049\n",
      "Iteration 52, loss = 0.16181735\n",
      "Iteration 66, loss = 0.13471399\n",
      "Iteration 53, loss = 0.15693448\n",
      "Iteration 67, loss = 0.13151186\n",
      "Iteration 54, loss = 0.15552334\n",
      "Iteration 55, loss = 0.15414943\n",
      "Iteration 68, loss = 0.13218692\n",
      "Iteration 69, loss = 0.13095192\n",
      "Iteration 56, loss = 0.15175552\n",
      "Iteration 70, loss = 0.12711485\n",
      "Iteration 71, loss = 0.12591338\n",
      "Iteration 57, loss = 0.14862787\n",
      "Iteration 72, loss = 0.12826222\n",
      "Iteration 73, loss = 0.12160007\n",
      "Iteration 58, loss = 0.14767074\n",
      "Iteration 74, loss = 0.12035035\n",
      "Iteration 59, loss = 0.14668969\n",
      "Iteration 75, loss = 0.11878259\n",
      "Iteration 76, loss = 0.11977638\n",
      "Iteration 77, loss = 0.11985220\n",
      "Iteration 78, loss = 0.11446985\n",
      "Iteration 79, loss = 0.11234769\n",
      "Iteration 60, loss = 0.14304303\n",
      "Iteration 80, loss = 0.11185241\n",
      "Iteration 61, loss = 0.13987454\n",
      "Iteration 62, loss = 0.13782905\n",
      "Iteration 81, loss = 0.10843166\n",
      "Iteration 63, loss = 0.13514199\n",
      "Iteration 82, loss = 0.11128607\n",
      "Iteration 64, loss = 0.13751744\n",
      "Iteration 83, loss = 0.11330720\n",
      "Iteration 84, loss = 0.10702545\n",
      "Iteration 65, loss = 0.13682292\n",
      "Iteration 85, loss = 0.10599875\n",
      "Iteration 66, loss = 0.14065045\n",
      "Iteration 86, loss = 0.10391208\n",
      "Iteration 67, loss = 0.13069242\n",
      "Iteration 87, loss = 0.10454683\n",
      "Iteration 68, loss = 0.12570073\n",
      "Iteration 88, loss = 0.10304153\n",
      "Iteration 69, loss = 0.13388886\n",
      "Iteration 89, loss = 0.09893838\n",
      "Iteration 90, loss = 0.09942514\n",
      "Iteration 70, loss = 0.12587369\n",
      "Iteration 91, loss = 0.09811919\n",
      "Iteration 71, loss = 0.12372627\n",
      "Iteration 72, loss = 0.12123770\n",
      "Iteration 73, loss = 0.11993275\n",
      "Iteration 92, loss = 0.10079776\n",
      "Iteration 74, loss = 0.11806162\n",
      "Iteration 93, loss = 0.10078573\n",
      "Iteration 94, loss = 0.09352856\n",
      "Iteration 75, loss = 0.11479237\n",
      "Iteration 95, loss = 0.09694002\n",
      "Iteration 76, loss = 0.11623155\n",
      "Iteration 77, loss = 0.11215885\n",
      "Iteration 96, loss = 0.09833733\n",
      "Iteration 78, loss = 0.11250452\n",
      "Iteration 97, loss = 0.09407615\n",
      "Iteration 79, loss = 0.11069152\n",
      "Iteration 98, loss = 0.09143546\n",
      "Iteration 80, loss = 0.10838996\n",
      "Iteration 99, loss = 0.09162827\n",
      "Iteration 81, loss = 0.10750311\n",
      "Iteration 100, loss = 0.08886735\n",
      "Iteration 82, loss = 0.10697358\n",
      "Iteration 83, loss = 0.10479102\n",
      "Iteration 101, loss = 0.08842513\n",
      "Iteration 84, loss = 0.10250688\n",
      "Iteration 102, loss = 0.08798589\n",
      "Iteration 85, loss = 0.10181992\n",
      "Iteration 103, loss = 0.08322483\n",
      "Iteration 104, loss = 0.08926606\n",
      "Iteration 86, loss = 0.10162390\n",
      "Iteration 105, loss = 0.08300175\n",
      "Iteration 87, loss = 0.09877874\n",
      "Iteration 106, loss = 0.08146962\n",
      "Iteration 88, loss = 0.10134423\n",
      "Iteration 107, loss = 0.08333453\n",
      "Iteration 108, loss = 0.08168038\n",
      "Iteration 89, loss = 0.09899652\n",
      "Iteration 109, loss = 0.08728983\n",
      "Iteration 90, loss = 0.09697295\n",
      "Iteration 91, loss = 0.09578341\n",
      "Iteration 110, loss = 0.08682794\n",
      "Iteration 92, loss = 0.09678021\n",
      "Iteration 111, loss = 0.08472459\n",
      "Iteration 112, loss = 0.08509429\n",
      "Iteration 93, loss = 0.09752283\n",
      "Iteration 113, loss = 0.08293284\n",
      "Iteration 94, loss = 0.09228535\n",
      "Iteration 95, loss = 0.09148916\n",
      "Iteration 114, loss = 0.07565425\n",
      "Iteration 96, loss = 0.08963975\n",
      "Iteration 115, loss = 0.07819580\n",
      "Iteration 97, loss = 0.08905183\n",
      "Iteration 116, loss = 0.07689470\n",
      "Iteration 98, loss = 0.08895074\n",
      "Iteration 117, loss = 0.07153495\n",
      "Iteration 99, loss = 0.08879167\n",
      "Iteration 118, loss = 0.07213454\n",
      "Iteration 100, loss = 0.08848519\n",
      "Iteration 119, loss = 0.07072382\n",
      "Iteration 101, loss = 0.08491400\n",
      "Iteration 102, loss = 0.08522622\n",
      "Iteration 120, loss = 0.07126422\n",
      "Iteration 103, loss = 0.08523594\n",
      "Iteration 121, loss = 0.07117015\n",
      "Iteration 122, loss = 0.07050472\n",
      "Iteration 123, loss = 0.06855364\n",
      "Iteration 104, loss = 0.08473676\n",
      "Iteration 124, loss = 0.07281426\n",
      "Iteration 125, loss = 0.06961729\n",
      "Iteration 105, loss = 0.08207685\n",
      "Iteration 126, loss = 0.06741540\n",
      "Iteration 106, loss = 0.08047994\n",
      "Iteration 127, loss = 0.06513754\n",
      "Iteration 107, loss = 0.08017084\n",
      "Iteration 128, loss = 0.06397428\n",
      "Iteration 108, loss = 0.07740640\n",
      "Iteration 129, loss = 0.06320402\n",
      "Iteration 130, loss = 0.06395340\n",
      "Iteration 109, loss = 0.08028431\n",
      "Iteration 131, loss = 0.06429573\n",
      "Iteration 110, loss = 0.07813357\n",
      "Iteration 132, loss = 0.06281931\n",
      "Iteration 111, loss = 0.07548193\n",
      "Iteration 133, loss = 0.06228490\n",
      "Iteration 134, loss = 0.06103528\n",
      "Iteration 112, loss = 0.07662903\n",
      "Iteration 113, loss = 0.07936781\n",
      "Iteration 114, loss = 0.08165269\n",
      "Iteration 135, loss = 0.06024115\n",
      "Iteration 115, loss = 0.07865160\n",
      "Iteration 136, loss = 0.05993441\n",
      "Iteration 137, loss = 0.05761312\n",
      "Iteration 138, loss = 0.06023041\n",
      "Iteration 116, loss = 0.07931566\n",
      "Iteration 117, loss = 0.07764377\n",
      "Iteration 139, loss = 0.05939809\n",
      "Iteration 118, loss = 0.08080800\n",
      "Iteration 140, loss = 0.05869544\n",
      "Iteration 119, loss = 0.07100882\n",
      "Iteration 141, loss = 0.06115200\n",
      "Iteration 120, loss = 0.07221708\n",
      "Iteration 121, loss = 0.06729210\n",
      "Iteration 122, loss = 0.06680191\n",
      "Iteration 142, loss = 0.05694956\n",
      "Iteration 123, loss = 0.06758788\n",
      "Iteration 124, loss = 0.06866118\n",
      "Iteration 125, loss = 0.06628321\n",
      "Iteration 143, loss = 0.05468194\n",
      "Iteration 126, loss = 0.06664634\n",
      "Iteration 127, loss = 0.06391024\n",
      "Iteration 128, loss = 0.06227445\n",
      "Iteration 144, loss = 0.05318684\n",
      "Iteration 129, loss = 0.06038765\n",
      "Iteration 130, loss = 0.06551628\n",
      "Iteration 145, loss = 0.05573466\n",
      "Iteration 131, loss = 0.07048466\n",
      "Iteration 132, loss = 0.07959378\n",
      "Iteration 146, loss = 0.05507554\n",
      "Iteration 133, loss = 0.08058626\n",
      "Iteration 147, loss = 0.05587340\n",
      "Iteration 148, loss = 0.05481085\n",
      "Iteration 134, loss = 0.07162943\n",
      "Iteration 149, loss = 0.05206154\n",
      "Iteration 135, loss = 0.06581350\n",
      "Iteration 150, loss = 0.05185231\n",
      "Iteration 136, loss = 0.05888909\n",
      "Iteration 151, loss = 0.05382835\n",
      "Iteration 137, loss = 0.05780067\n",
      "Iteration 152, loss = 0.05141668\n",
      "Iteration 138, loss = 0.05639165\n",
      "Iteration 153, loss = 0.05327012\n",
      "Iteration 139, loss = 0.05486894\n",
      "Iteration 154, loss = 0.05246185\n",
      "Iteration 140, loss = 0.05372210\n",
      "Iteration 155, loss = 0.05121156\n",
      "Iteration 141, loss = 0.05388903\n",
      "Iteration 142, loss = 0.05397893\n",
      "Iteration 156, loss = 0.04810246\n",
      "Iteration 157, loss = 0.05154789\n",
      "Iteration 158, loss = 0.05384181\n",
      "Iteration 143, loss = 0.05502122\n",
      "Iteration 159, loss = 0.05348969\n",
      "Iteration 160, loss = 0.05607359\n",
      "Iteration 144, loss = 0.05349987\n",
      "Iteration 145, loss = 0.05464501\n",
      "Iteration 146, loss = 0.05465311\n",
      "Iteration 161, loss = 0.05186026\n",
      "Iteration 147, loss = 0.05254369\n",
      "Iteration 162, loss = 0.04746451\n",
      "Iteration 148, loss = 0.05019397\n",
      "Iteration 163, loss = 0.04536116\n",
      "Iteration 149, loss = 0.04898800\n",
      "Iteration 164, loss = 0.04434395\n",
      "Iteration 150, loss = 0.04762011\n",
      "Iteration 165, loss = 0.04297544\n",
      "Iteration 166, loss = 0.04449664\n",
      "Iteration 151, loss = 0.04769078\n",
      "Iteration 167, loss = 0.04316196\n",
      "Iteration 168, loss = 0.04262237\n",
      "Iteration 152, loss = 0.04989579\n",
      "Iteration 169, loss = 0.04110961\n",
      "Iteration 170, loss = 0.04256857\n",
      "Iteration 153, loss = 0.04970669\n",
      "Iteration 154, loss = 0.04805968\n",
      "Iteration 171, loss = 0.04214620\n",
      "Iteration 155, loss = 0.04680926\n",
      "Iteration 172, loss = 0.03921502\n",
      "Iteration 173, loss = 0.03989570\n",
      "Iteration 156, loss = 0.04922890\n",
      "Iteration 174, loss = 0.04227915\n",
      "Iteration 157, loss = 0.05123568\n",
      "Iteration 175, loss = 0.04344355\n",
      "Iteration 158, loss = 0.04606026\n",
      "Iteration 176, loss = 0.04434813\n",
      "Iteration 159, loss = 0.04532717\n",
      "Iteration 177, loss = 0.04595689\n",
      "Iteration 160, loss = 0.04357011\n",
      "Iteration 178, loss = 0.04273198\n",
      "Iteration 179, loss = 0.03737023\n",
      "Iteration 161, loss = 0.04199108\n",
      "Iteration 180, loss = 0.03468987\n",
      "Iteration 162, loss = 0.04285883\n",
      "Iteration 181, loss = 0.03489247\n",
      "Iteration 163, loss = 0.04470774\n",
      "Iteration 182, loss = 0.03480141\n",
      "Iteration 164, loss = 0.04110820\n",
      "Iteration 183, loss = 0.03471413\n",
      "Iteration 165, loss = 0.04104401\n",
      "Iteration 184, loss = 0.03306998\n",
      "Iteration 185, loss = 0.03557537\n",
      "Iteration 186, loss = 0.03425151\n",
      "Iteration 166, loss = 0.04109108\n",
      "Iteration 187, loss = 0.03273504\n",
      "Iteration 167, loss = 0.04087524\n",
      "Iteration 168, loss = 0.03949132\n",
      "Iteration 188, loss = 0.03273486\n",
      "Iteration 169, loss = 0.03964866\n",
      "Iteration 170, loss = 0.04039291\n",
      "Iteration 189, loss = 0.03157477\n",
      "Iteration 171, loss = 0.03918713\n",
      "Iteration 190, loss = 0.03151989\n",
      "Iteration 172, loss = 0.03858211\n",
      "Iteration 191, loss = 0.03252319\n",
      "Iteration 173, loss = 0.03720161\n",
      "Iteration 192, loss = 0.03275768\n",
      "Iteration 193, loss = 0.03241562\n",
      "Iteration 174, loss = 0.03878115\n",
      "Iteration 194, loss = 0.03187874\n",
      "Iteration 175, loss = 0.03690496\n",
      "Iteration 195, loss = 0.03077211\n",
      "Iteration 196, loss = 0.02956528\n",
      "Iteration 197, loss = 0.02955612\n",
      "Iteration 176, loss = 0.03764718\n",
      "Iteration 198, loss = 0.02829631\n",
      "Iteration 199, loss = 0.02885231\n",
      "Iteration 177, loss = 0.03667713\n",
      "Iteration 200, loss = 0.02794098\n",
      "Iteration 201, loss = 0.02730693\n",
      "Iteration 202, loss = 0.02691399\n",
      "Iteration 178, loss = 0.03614803\n",
      "Iteration 203, loss = 0.02749308\n",
      "Iteration 179, loss = 0.03597768\n",
      "Iteration 204, loss = 0.02942789\n",
      "Iteration 180, loss = 0.03458454\n",
      "Iteration 181, loss = 0.03509722\n",
      "Iteration 182, loss = 0.03463048\n",
      "Iteration 205, loss = 0.02735235\n",
      "Iteration 206, loss = 0.02704478\n",
      "Iteration 183, loss = 0.03367137\n",
      "Iteration 207, loss = 0.02565147\n",
      "Iteration 184, loss = 0.03241149\n",
      "Iteration 185, loss = 0.03355412\n",
      "Iteration 208, loss = 0.02778367\n",
      "Iteration 186, loss = 0.03675396\n",
      "Iteration 209, loss = 0.02835780\n",
      "Iteration 187, loss = 0.03179647\n",
      "Iteration 210, loss = 0.03069283\n",
      "Iteration 188, loss = 0.03756461\n",
      "Iteration 211, loss = 0.02737408\n",
      "Iteration 189, loss = 0.03333604\n",
      "Iteration 190, loss = 0.03481099\n",
      "Iteration 212, loss = 0.03166390\n",
      "Iteration 213, loss = 0.03092420\n",
      "Iteration 191, loss = 0.03184808\n",
      "Iteration 214, loss = 0.03054293\n",
      "Iteration 192, loss = 0.02958046\n",
      "Iteration 215, loss = 0.03001180\n",
      "Iteration 193, loss = 0.03019930\n",
      "Iteration 194, loss = 0.02922847\n",
      "Iteration 216, loss = 0.03154587\n",
      "Iteration 217, loss = 0.02982753\n",
      "Iteration 195, loss = 0.02760995\n",
      "Iteration 218, loss = 0.03052546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 196, loss = 0.02757854\n",
      "Iteration 197, loss = 0.02769675\n",
      "Iteration 198, loss = 0.02787245\n",
      "Iteration 199, loss = 0.02726273\n",
      "Iteration 200, loss = 0.02630268\n",
      "Iteration 201, loss = 0.02674825\n",
      "Iteration 202, loss = 0.02672999\n",
      "Iteration 203, loss = 0.02519556\n",
      "Iteration 204, loss = 0.02480352\n",
      "Iteration 1, loss = 1.39592126\n",
      "Iteration 205, loss = 0.02512651\n",
      "Iteration 2, loss = 0.83445175\n",
      "Iteration 206, loss = 0.02400955\n",
      "Iteration 3, loss = 0.62325769\n",
      "Iteration 4, loss = 0.53208773\n",
      "Iteration 207, loss = 0.02412427\n",
      "Iteration 5, loss = 0.47864705\n",
      "Iteration 208, loss = 0.02464245\n",
      "Iteration 6, loss = 0.44300217\n",
      "Iteration 209, loss = 0.02584277\n",
      "Iteration 7, loss = 0.41800700\n",
      "Iteration 8, loss = 0.39743461\n",
      "Iteration 9, loss = 0.37849038\n",
      "Iteration 210, loss = 0.02421312\n",
      "Iteration 10, loss = 0.36695304\n",
      "Iteration 211, loss = 0.02450559\n",
      "Iteration 11, loss = 0.35405197\n",
      "Iteration 212, loss = 0.02423758\n",
      "Iteration 12, loss = 0.34544155\n",
      "Iteration 13, loss = 0.33369896\n",
      "Iteration 14, loss = 0.32482432\n",
      "Iteration 213, loss = 0.02456799\n",
      "Iteration 15, loss = 0.31662832\n",
      "Iteration 16, loss = 0.31201586\n",
      "Iteration 17, loss = 0.30305520\n",
      "Iteration 214, loss = 0.02507473\n",
      "Iteration 18, loss = 0.29427522\n",
      "Iteration 19, loss = 0.28803057\n",
      "Iteration 215, loss = 0.03019979\n",
      "Iteration 20, loss = 0.28162207\n",
      "Iteration 21, loss = 0.27660424\n",
      "Iteration 216, loss = 0.02814875\n",
      "Iteration 217, loss = 0.02982410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.27042962\n",
      "Iteration 23, loss = 0.26652913\n",
      "Iteration 24, loss = 0.26017475\n",
      "Iteration 25, loss = 0.25615408\n",
      "Iteration 26, loss = 0.25368463\n",
      "Iteration 27, loss = 0.24629211\n",
      "Iteration 28, loss = 0.24214199\n",
      "Iteration 29, loss = 0.23945173\n",
      "Iteration 1, loss = 1.18331715\n",
      "Iteration 30, loss = 0.23323235\n",
      "Iteration 2, loss = 0.79916855\n",
      "Iteration 31, loss = 0.22947999\n",
      "Iteration 3, loss = 0.62707809\n",
      "Iteration 4, loss = 0.54800642\n",
      "Iteration 5, loss = 0.49490457\n",
      "Iteration 32, loss = 0.22690591\n",
      "Iteration 6, loss = 0.46060749\n",
      "Iteration 33, loss = 0.22628413\n",
      "Iteration 7, loss = 0.43046474\n",
      "Iteration 34, loss = 0.21912737\n",
      "Iteration 8, loss = 0.40716962\n",
      "Iteration 35, loss = 0.21577101\n",
      "Iteration 9, loss = 0.39133653\n",
      "Iteration 36, loss = 0.21524783\n",
      "Iteration 10, loss = 0.37433766\n",
      "Iteration 11, loss = 0.36262908\n",
      "Iteration 12, loss = 0.35077901\n",
      "Iteration 37, loss = 0.21430973\n",
      "Iteration 13, loss = 0.33940236\n",
      "Iteration 14, loss = 0.32994059\n",
      "Iteration 15, loss = 0.31996020\n",
      "Iteration 16, loss = 0.31357803\n",
      "Iteration 38, loss = 0.20377755\n",
      "Iteration 39, loss = 0.20076788\n",
      "Iteration 40, loss = 0.19849537\n",
      "Iteration 17, loss = 0.30552054\n",
      "Iteration 18, loss = 0.29691226\n",
      "Iteration 19, loss = 0.29194044\n",
      "Iteration 20, loss = 0.28539437\n",
      "Iteration 41, loss = 0.19479275\n",
      "Iteration 21, loss = 0.27901767\n",
      "Iteration 22, loss = 0.27260300\n",
      "Iteration 42, loss = 0.19086131\n",
      "Iteration 23, loss = 0.26648825\n",
      "Iteration 43, loss = 0.18698389\n",
      "Iteration 44, loss = 0.18745710\n",
      "Iteration 24, loss = 0.26144756\n",
      "Iteration 45, loss = 0.18911485\n",
      "Iteration 25, loss = 0.25684792\n",
      "Iteration 26, loss = 0.25126189\n",
      "Iteration 46, loss = 0.17827620\n",
      "Iteration 47, loss = 0.17722528\n",
      "Iteration 48, loss = 0.17442215\n",
      "Iteration 27, loss = 0.24577456\n",
      "Iteration 49, loss = 0.16811217\n",
      "Iteration 50, loss = 0.16817640\n",
      "Iteration 28, loss = 0.24104760\n",
      "Iteration 29, loss = 0.23754239\n",
      "Iteration 30, loss = 0.23409285\n",
      "Iteration 51, loss = 0.16445703\n",
      "Iteration 52, loss = 0.15986134\n",
      "Iteration 53, loss = 0.15848639\n",
      "Iteration 31, loss = 0.23083991\n",
      "Iteration 54, loss = 0.15595407\n",
      "Iteration 32, loss = 0.22487510\n",
      "Iteration 33, loss = 0.22081369\n",
      "Iteration 55, loss = 0.15557913\n",
      "Iteration 34, loss = 0.21935452\n",
      "Iteration 56, loss = 0.15266398\n",
      "Iteration 57, loss = 0.14822677\n",
      "Iteration 35, loss = 0.21712031\n",
      "Iteration 58, loss = 0.14727639\n",
      "Iteration 36, loss = 0.21109281\n",
      "Iteration 59, loss = 0.14505908\n",
      "Iteration 37, loss = 0.20852621\n",
      "Iteration 38, loss = 0.20440457\n",
      "Iteration 60, loss = 0.14217233\n",
      "Iteration 39, loss = 0.19993426\n",
      "Iteration 61, loss = 0.13999079\n",
      "Iteration 62, loss = 0.13965092\n",
      "Iteration 63, loss = 0.14021647\n",
      "Iteration 40, loss = 0.19597705\n",
      "Iteration 41, loss = 0.19379078\n",
      "Iteration 42, loss = 0.19069740\n",
      "Iteration 64, loss = 0.13957270\n",
      "Iteration 43, loss = 0.18911385\n",
      "Iteration 44, loss = 0.18531379\n",
      "Iteration 45, loss = 0.17997837\n",
      "Iteration 65, loss = 0.14098831\n",
      "Iteration 46, loss = 0.18208596\n",
      "Iteration 47, loss = 0.17830712\n",
      "Iteration 48, loss = 0.17213382\n",
      "Iteration 66, loss = 0.13513094\n",
      "Iteration 49, loss = 0.17179065\n",
      "Iteration 50, loss = 0.16838240\n",
      "Iteration 67, loss = 0.12762728\n",
      "Iteration 68, loss = 0.13182955\n",
      "Iteration 51, loss = 0.16464285\n",
      "Iteration 69, loss = 0.12839441\n",
      "Iteration 52, loss = 0.16256598\n",
      "Iteration 53, loss = 0.15988493\n",
      "Iteration 54, loss = 0.15789446\n",
      "Iteration 70, loss = 0.12777683\n",
      "Iteration 55, loss = 0.15573924\n",
      "Iteration 56, loss = 0.15019190\n",
      "Iteration 57, loss = 0.14907361\n",
      "Iteration 58, loss = 0.15040088\n",
      "Iteration 71, loss = 0.12489101\n",
      "Iteration 72, loss = 0.12130507\n",
      "Iteration 59, loss = 0.14463568\n",
      "Iteration 73, loss = 0.12039059\n",
      "Iteration 60, loss = 0.14428880\n",
      "Iteration 61, loss = 0.14337556\n",
      "Iteration 62, loss = 0.14038765\n",
      "Iteration 63, loss = 0.13593024\n",
      "Iteration 74, loss = 0.11995011\n",
      "Iteration 64, loss = 0.13445352\n",
      "Iteration 75, loss = 0.11633754\n",
      "Iteration 65, loss = 0.13727317\n",
      "Iteration 76, loss = 0.11566530\n",
      "Iteration 77, loss = 0.11489147\n",
      "Iteration 78, loss = 0.11316847\n",
      "Iteration 66, loss = 0.13317934\n",
      "Iteration 79, loss = 0.11198991\n",
      "Iteration 67, loss = 0.12839503\n",
      "Iteration 80, loss = 0.11026881\n",
      "Iteration 68, loss = 0.12893065\n",
      "Iteration 81, loss = 0.11272673\n",
      "Iteration 69, loss = 0.13125853\n",
      "Iteration 70, loss = 0.13159068\n",
      "Iteration 71, loss = 0.12824321\n",
      "Iteration 82, loss = 0.10430578\n",
      "Iteration 72, loss = 0.12587515\n",
      "Iteration 83, loss = 0.10601807\n",
      "Iteration 73, loss = 0.11797611\n",
      "Iteration 84, loss = 0.10600911\n",
      "Iteration 85, loss = 0.10346700\n",
      "Iteration 74, loss = 0.11987239\n",
      "Iteration 86, loss = 0.10115574\n",
      "Iteration 87, loss = 0.10034120\n",
      "Iteration 75, loss = 0.11626668\n",
      "Iteration 88, loss = 0.09721792\n",
      "Iteration 89, loss = 0.10221833\n",
      "Iteration 90, loss = 0.11083469\n",
      "Iteration 76, loss = 0.11456076\n",
      "Iteration 91, loss = 0.10191490\n",
      "Iteration 92, loss = 0.10285053\n",
      "Iteration 77, loss = 0.11500621\n",
      "Iteration 93, loss = 0.09364116\n",
      "Iteration 94, loss = 0.09392520\n",
      "Iteration 95, loss = 0.09285403\n",
      "Iteration 96, loss = 0.09193558\n",
      "Iteration 78, loss = 0.11329472\n",
      "Iteration 79, loss = 0.12374572\n",
      "Iteration 80, loss = 0.11955276\n",
      "Iteration 81, loss = 0.11754575\n",
      "Iteration 97, loss = 0.09225975\n",
      "Iteration 82, loss = 0.11043408\n",
      "Iteration 98, loss = 0.09034787\n",
      "Iteration 83, loss = 0.10213706\n",
      "Iteration 99, loss = 0.08771478\n",
      "Iteration 84, loss = 0.10906408\n",
      "Iteration 100, loss = 0.08608158\n",
      "Iteration 85, loss = 0.10241153\n",
      "Iteration 101, loss = 0.08504686\n",
      "Iteration 86, loss = 0.10121463\n",
      "Iteration 102, loss = 0.08370022\n",
      "Iteration 87, loss = 0.09853155\n",
      "Iteration 103, loss = 0.08222523\n",
      "Iteration 88, loss = 0.09828039\n",
      "Iteration 104, loss = 0.08049567\n",
      "Iteration 89, loss = 0.09645337\n",
      "Iteration 105, loss = 0.07949656\n",
      "Iteration 106, loss = 0.07950072\n",
      "Iteration 90, loss = 0.09558408\n",
      "Iteration 107, loss = 0.08177335\n",
      "Iteration 91, loss = 0.09518304\n",
      "Iteration 92, loss = 0.09418830\n",
      "Iteration 108, loss = 0.07862651\n",
      "Iteration 93, loss = 0.09715031\n",
      "Iteration 109, loss = 0.08233454\n",
      "Iteration 110, loss = 0.08022662\n",
      "Iteration 94, loss = 0.09565664\n",
      "Iteration 111, loss = 0.07376464\n",
      "Iteration 95, loss = 0.09098252\n",
      "Iteration 96, loss = 0.08860831\n",
      "Iteration 112, loss = 0.07636406\n",
      "Iteration 97, loss = 0.08815811\n",
      "Iteration 113, loss = 0.07364248\n",
      "Iteration 114, loss = 0.07267787\n",
      "Iteration 98, loss = 0.08816354\n",
      "Iteration 115, loss = 0.07105466\n",
      "Iteration 99, loss = 0.08474742\n",
      "Iteration 100, loss = 0.08633211\n",
      "Iteration 116, loss = 0.07028458\n",
      "Iteration 101, loss = 0.08584151\n",
      "Iteration 117, loss = 0.07008082\n",
      "Iteration 118, loss = 0.06811266\n",
      "Iteration 102, loss = 0.08530412\n",
      "Iteration 119, loss = 0.06867862\n",
      "Iteration 103, loss = 0.08428774\n",
      "Iteration 104, loss = 0.08439041\n",
      "Iteration 120, loss = 0.06864518\n",
      "Iteration 105, loss = 0.08070242\n",
      "Iteration 106, loss = 0.08130871\n",
      "Iteration 121, loss = 0.06709170\n",
      "Iteration 122, loss = 0.06603779\n",
      "Iteration 107, loss = 0.07899527\n",
      "Iteration 123, loss = 0.06514675\n",
      "Iteration 108, loss = 0.08066251\n",
      "Iteration 124, loss = 0.06332776\n",
      "Iteration 109, loss = 0.07963128\n",
      "Iteration 110, loss = 0.07942890\n",
      "Iteration 125, loss = 0.06326817\n",
      "Iteration 111, loss = 0.07463213\n",
      "Iteration 126, loss = 0.06465880\n",
      "Iteration 112, loss = 0.07624605\n",
      "Iteration 127, loss = 0.06878579\n",
      "Iteration 113, loss = 0.07146190\n",
      "Iteration 128, loss = 0.06637838\n",
      "Iteration 114, loss = 0.07184452\n",
      "Iteration 129, loss = 0.06853044\n",
      "Iteration 115, loss = 0.07122637\n",
      "Iteration 130, loss = 0.06278651\n",
      "Iteration 116, loss = 0.07150619\n",
      "Iteration 131, loss = 0.05996922\n",
      "Iteration 117, loss = 0.07309961\n",
      "Iteration 132, loss = 0.05955786\n",
      "Iteration 118, loss = 0.07524031\n",
      "Iteration 133, loss = 0.05703088\n",
      "Iteration 119, loss = 0.07023631\n",
      "Iteration 134, loss = 0.05734690\n",
      "Iteration 120, loss = 0.06948072\n",
      "Iteration 135, loss = 0.05917247\n",
      "Iteration 121, loss = 0.06931664\n",
      "Iteration 136, loss = 0.05799899\n",
      "Iteration 137, loss = 0.05725583\n",
      "Iteration 122, loss = 0.06542878\n",
      "Iteration 138, loss = 0.05608299\n",
      "Iteration 139, loss = 0.05683314\n",
      "Iteration 140, loss = 0.05588447\n",
      "Iteration 123, loss = 0.06766526\n",
      "Iteration 124, loss = 0.06733081\n",
      "Iteration 141, loss = 0.05456508\n",
      "Iteration 125, loss = 0.07294050\n",
      "Iteration 126, loss = 0.07333763\n",
      "Iteration 127, loss = 0.06826575\n",
      "Iteration 128, loss = 0.06867372\n",
      "Iteration 142, loss = 0.05348370\n",
      "Iteration 129, loss = 0.07294126\n",
      "Iteration 130, loss = 0.06346139\n",
      "Iteration 143, loss = 0.05311680Iteration 131, loss = 0.06532038\n",
      "\n",
      "Iteration 132, loss = 0.06442282\n",
      "Iteration 133, loss = 0.06262611\n",
      "Iteration 134, loss = 0.05729943\n",
      "Iteration 144, loss = 0.05205327\n",
      "Iteration 135, loss = 0.06107124\n",
      "Iteration 136, loss = 0.05695351\n",
      "Iteration 137, loss = 0.05327314\n",
      "Iteration 138, loss = 0.05562258\n",
      "Iteration 145, loss = 0.04917341\n",
      "Iteration 139, loss = 0.05454379\n",
      "Iteration 140, loss = 0.05250088\n",
      "Iteration 146, loss = 0.05272622\n",
      "Iteration 141, loss = 0.05185436\n",
      "Iteration 147, loss = 0.04964958\n",
      "Iteration 142, loss = 0.05435732\n",
      "Iteration 148, loss = 0.04856984\n",
      "Iteration 149, loss = 0.04982157\n",
      "Iteration 150, loss = 0.05060926\n",
      "Iteration 143, loss = 0.05605418\n",
      "Iteration 151, loss = 0.04685211\n",
      "Iteration 144, loss = 0.05737855\n",
      "Iteration 145, loss = 0.05332673\n",
      "Iteration 146, loss = 0.04882115\n",
      "Iteration 152, loss = 0.04775897\n",
      "Iteration 153, loss = 0.04936145\n",
      "Iteration 154, loss = 0.04710465\n",
      "Iteration 147, loss = 0.05209735\n",
      "Iteration 155, loss = 0.04785038\n",
      "Iteration 148, loss = 0.05509542\n",
      "Iteration 149, loss = 0.05375655\n",
      "Iteration 150, loss = 0.04823963\n",
      "Iteration 156, loss = 0.04644867\n",
      "Iteration 157, loss = 0.04482546\n",
      "Iteration 151, loss = 0.04684578\n",
      "Iteration 158, loss = 0.04811335\n",
      "Iteration 159, loss = 0.04876897\n",
      "Iteration 152, loss = 0.04789970\n",
      "Iteration 153, loss = 0.04863880\n",
      "Iteration 160, loss = 0.05086038\n",
      "Iteration 154, loss = 0.04482662\n",
      "Iteration 161, loss = 0.05406110\n",
      "Iteration 155, loss = 0.04339170\n",
      "Iteration 162, loss = 0.06228899\n",
      "Iteration 156, loss = 0.04240849\n",
      "Iteration 157, loss = 0.04438998\n",
      "Iteration 158, loss = 0.04316616\n",
      "Iteration 163, loss = 0.06571259\n",
      "Iteration 159, loss = 0.04308633\n",
      "Iteration 164, loss = 0.06870545\n",
      "Iteration 160, loss = 0.04306153\n",
      "Iteration 165, loss = 0.04715307\n",
      "Iteration 161, loss = 0.04281025\n",
      "Iteration 166, loss = 0.04838197\n",
      "Iteration 167, loss = 0.04927596\n",
      "Iteration 162, loss = 0.04169082\n",
      "Iteration 168, loss = 0.04556373\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 163, loss = 0.03946910\n",
      "Iteration 164, loss = 0.03925694\n",
      "Iteration 165, loss = 0.03846202\n",
      "Iteration 166, loss = 0.03889155\n",
      "Iteration 167, loss = 0.03890239\n",
      "Iteration 168, loss = 0.03852024\n",
      "Iteration 169, loss = 0.04019291\n",
      "Iteration 170, loss = 0.04043143\n",
      "Iteration 171, loss = 0.03851034\n",
      "Iteration 1, loss = 0.91494930\n",
      "Iteration 172, loss = 0.03921826\n",
      "Iteration 173, loss = 0.04034261\n",
      "Iteration 174, loss = 0.04388856\n",
      "Iteration 2, loss = 0.64500309\n",
      "Iteration 175, loss = 0.03740211\n",
      "Iteration 3, loss = 0.54378258\n",
      "Iteration 176, loss = 0.03604484\n",
      "Iteration 4, loss = 0.48747612\n",
      "Iteration 177, loss = 0.03475709\n",
      "Iteration 5, loss = 0.44638216\n",
      "Iteration 178, loss = 0.03450630\n",
      "Iteration 6, loss = 0.41835389\n",
      "Iteration 179, loss = 0.03657754\n",
      "Iteration 7, loss = 0.39419901\n",
      "Iteration 180, loss = 0.03484279\n",
      "Iteration 181, loss = 0.03344117\n",
      "Iteration 8, loss = 0.37627813\n",
      "Iteration 9, loss = 0.36242645\n",
      "Iteration 182, loss = 0.03395741\n",
      "Iteration 10, loss = 0.34895756\n",
      "Iteration 183, loss = 0.03222949\n",
      "Iteration 11, loss = 0.33835022\n",
      "Iteration 184, loss = 0.03047094\n",
      "Iteration 12, loss = 0.32907777\n",
      "Iteration 185, loss = 0.03126771\n",
      "Iteration 13, loss = 0.31878405\n",
      "Iteration 186, loss = 0.03122405\n",
      "Iteration 14, loss = 0.31107377\n",
      "Iteration 187, loss = 0.03148659\n",
      "Iteration 15, loss = 0.30252438\n",
      "Iteration 188, loss = 0.02971397\n",
      "Iteration 16, loss = 0.29631176\n",
      "Iteration 189, loss = 0.03067069\n",
      "Iteration 190, loss = 0.03136328\n",
      "Iteration 191, loss = 0.03049750\n",
      "Iteration 17, loss = 0.28906960\n",
      "Iteration 192, loss = 0.03036284\n",
      "Iteration 18, loss = 0.28385135\n",
      "Iteration 193, loss = 0.03044345\n",
      "Iteration 19, loss = 0.27670872\n",
      "Iteration 194, loss = 0.02906904\n",
      "Iteration 195, loss = 0.02778204\n",
      "Iteration 20, loss = 0.27197975\n",
      "Iteration 196, loss = 0.02867956\n",
      "Iteration 21, loss = 0.26885495\n",
      "Iteration 197, loss = 0.02902289\n",
      "Iteration 198, loss = 0.02973815\n",
      "Iteration 22, loss = 0.26271184\n",
      "Iteration 199, loss = 0.02780938\n",
      "Iteration 23, loss = 0.25762023\n",
      "Iteration 24, loss = 0.25236886\n",
      "Iteration 200, loss = 0.02886847\n",
      "Iteration 25, loss = 0.24614499\n",
      "Iteration 201, loss = 0.02979097\n",
      "Iteration 202, loss = 0.02775539\n",
      "Iteration 26, loss = 0.24512780\n",
      "Iteration 27, loss = 0.23802299\n",
      "Iteration 203, loss = 0.03029188\n",
      "Iteration 28, loss = 0.23467061\n",
      "Iteration 204, loss = 0.03868759\n",
      "Iteration 29, loss = 0.23198923\n",
      "Iteration 205, loss = 0.04079208\n",
      "Iteration 30, loss = 0.22776781\n",
      "Iteration 206, loss = 0.03954105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.22245613\n",
      "Iteration 32, loss = 0.22211274\n",
      "Iteration 33, loss = 0.21472404\n",
      "Iteration 34, loss = 0.21310822\n",
      "Iteration 35, loss = 0.20786304\n",
      "Iteration 36, loss = 0.20665711\n",
      "Iteration 37, loss = 0.20362417\n",
      "Iteration 1, loss = 1.27036260\n",
      "Iteration 38, loss = 0.19938758\n",
      "Iteration 2, loss = 0.81991165\n",
      "Iteration 39, loss = 0.19637096\n",
      "Iteration 3, loss = 0.64123150\n",
      "Iteration 40, loss = 0.19319535\n",
      "Iteration 4, loss = 0.55082607\n",
      "Iteration 41, loss = 0.18955778\n",
      "Iteration 5, loss = 0.49037126\n",
      "Iteration 6, loss = 0.45082617\n",
      "Iteration 42, loss = 0.18534690\n",
      "Iteration 43, loss = 0.18255978\n",
      "Iteration 44, loss = 0.18037912\n",
      "Iteration 7, loss = 0.42426879\n",
      "Iteration 45, loss = 0.17831418\n",
      "Iteration 8, loss = 0.40288083\n",
      "Iteration 9, loss = 0.38536437\n",
      "Iteration 10, loss = 0.36941897\n",
      "Iteration 46, loss = 0.17604192\n",
      "Iteration 11, loss = 0.35621862\n",
      "Iteration 47, loss = 0.17268200\n",
      "Iteration 48, loss = 0.16860608\n",
      "Iteration 49, loss = 0.16901330\n",
      "Iteration 12, loss = 0.34485214\n",
      "Iteration 13, loss = 0.33530030\n",
      "Iteration 50, loss = 0.16665249\n",
      "Iteration 14, loss = 0.32620493\n",
      "Iteration 51, loss = 0.16283089\n",
      "Iteration 15, loss = 0.31890806\n",
      "Iteration 16, loss = 0.31156120\n",
      "Iteration 17, loss = 0.30565055\n",
      "Iteration 52, loss = 0.15953032\n",
      "Iteration 18, loss = 0.29707156\n",
      "Iteration 53, loss = 0.15743412\n",
      "Iteration 19, loss = 0.29162997\n",
      "Iteration 54, loss = 0.15488439\n",
      "Iteration 20, loss = 0.28550882\n",
      "Iteration 55, loss = 0.15260486\n",
      "Iteration 21, loss = 0.28047544\n",
      "Iteration 56, loss = 0.15037693\n",
      "Iteration 22, loss = 0.27584113\n",
      "Iteration 57, loss = 0.15074958\n",
      "Iteration 23, loss = 0.27187154\n",
      "Iteration 58, loss = 0.14836186\n",
      "Iteration 59, loss = 0.14732109\n",
      "Iteration 60, loss = 0.14539429\n",
      "Iteration 24, loss = 0.26562584\n",
      "Iteration 25, loss = 0.26733637\n",
      "Iteration 61, loss = 0.14189560\n",
      "Iteration 26, loss = 0.25772536\n",
      "Iteration 62, loss = 0.13984614\n",
      "Iteration 63, loss = 0.13874064\n",
      "Iteration 27, loss = 0.25363225\n",
      "Iteration 28, loss = 0.25421538\n",
      "Iteration 29, loss = 0.24677215\n",
      "Iteration 64, loss = 0.14028215\n",
      "Iteration 65, loss = 0.13395736\n",
      "Iteration 66, loss = 0.13057944\n",
      "Iteration 30, loss = 0.24052777\n",
      "Iteration 31, loss = 0.24069549\n",
      "Iteration 32, loss = 0.23546302\n",
      "Iteration 67, loss = 0.12989834\n",
      "Iteration 33, loss = 0.23072135\n",
      "Iteration 68, loss = 0.12820547\n",
      "Iteration 69, loss = 0.12685979\n",
      "Iteration 34, loss = 0.22633602\n",
      "Iteration 35, loss = 0.22504838\n",
      "Iteration 70, loss = 0.12277532\n",
      "Iteration 71, loss = 0.12503137\n",
      "Iteration 36, loss = 0.22110292\n",
      "Iteration 72, loss = 0.12213952\n",
      "Iteration 37, loss = 0.21706625\n",
      "Iteration 73, loss = 0.12312091\n",
      "Iteration 38, loss = 0.21368125\n",
      "Iteration 74, loss = 0.11854498\n",
      "Iteration 39, loss = 0.21056008\n",
      "Iteration 75, loss = 0.12087876\n",
      "Iteration 40, loss = 0.20696758\n",
      "Iteration 76, loss = 0.11563920\n",
      "Iteration 41, loss = 0.20372485\n",
      "Iteration 42, loss = 0.20226470\n",
      "Iteration 43, loss = 0.19903713\n",
      "Iteration 44, loss = 0.19585708\n",
      "Iteration 45, loss = 0.19174648\n",
      "Iteration 77, loss = 0.11383533\n",
      "Iteration 46, loss = 0.18995430\n",
      "Iteration 47, loss = 0.18899854\n",
      "Iteration 78, loss = 0.11256737\n",
      "Iteration 48, loss = 0.18753394\n",
      "Iteration 49, loss = 0.17908251\n",
      "Iteration 50, loss = 0.17965427\n",
      "Iteration 79, loss = 0.11242716\n",
      "Iteration 80, loss = 0.11552395\n",
      "Iteration 81, loss = 0.11313114\n",
      "Iteration 51, loss = 0.17530105\n",
      "Iteration 82, loss = 0.11369730\n",
      "Iteration 83, loss = 0.11469105\n",
      "Iteration 52, loss = 0.17303100\n",
      "Iteration 84, loss = 0.11016701\n",
      "Iteration 53, loss = 0.16805604\n",
      "Iteration 54, loss = 0.16810859\n",
      "Iteration 55, loss = 0.16488372\n",
      "Iteration 85, loss = 0.10620543\n",
      "Iteration 56, loss = 0.16090796\n",
      "Iteration 86, loss = 0.10373128\n",
      "Iteration 87, loss = 0.10189295\n",
      "Iteration 57, loss = 0.16064239\n",
      "Iteration 88, loss = 0.09970122\n",
      "Iteration 58, loss = 0.16034893\n",
      "Iteration 89, loss = 0.10329541\n",
      "Iteration 59, loss = 0.15749018\n",
      "Iteration 90, loss = 0.10128466\n",
      "Iteration 60, loss = 0.15770139\n",
      "Iteration 61, loss = 0.15429959\n",
      "Iteration 62, loss = 0.15269814\n",
      "Iteration 91, loss = 0.10020095\n",
      "Iteration 63, loss = 0.14517677\n",
      "Iteration 92, loss = 0.09597269\n",
      "Iteration 93, loss = 0.09615044\n",
      "Iteration 64, loss = 0.14895385\n",
      "Iteration 94, loss = 0.09767953\n",
      "Iteration 65, loss = 0.14355732\n",
      "Iteration 95, loss = 0.09734079\n",
      "Iteration 66, loss = 0.14064124\n",
      "Iteration 96, loss = 0.10458746\n",
      "Iteration 67, loss = 0.14078956\n",
      "Iteration 68, loss = 0.13634617\n",
      "Iteration 69, loss = 0.13624435\n",
      "Iteration 97, loss = 0.09436097\n",
      "Iteration 98, loss = 0.09187485\n",
      "Iteration 99, loss = 0.09833650\n",
      "Iteration 70, loss = 0.13576538\n",
      "Iteration 71, loss = 0.13437886\n",
      "Iteration 72, loss = 0.13551723\n",
      "Iteration 100, loss = 0.09321966\n",
      "Iteration 101, loss = 0.08978640\n",
      "Iteration 73, loss = 0.14126348\n",
      "Iteration 102, loss = 0.08780027\n",
      "Iteration 103, loss = 0.08676884\n",
      "Iteration 74, loss = 0.13418968\n",
      "Iteration 75, loss = 0.13109276\n",
      "Iteration 76, loss = 0.12934010\n",
      "Iteration 104, loss = 0.08538919\n",
      "Iteration 77, loss = 0.12263371\n",
      "Iteration 105, loss = 0.08356880\n",
      "Iteration 106, loss = 0.08516087\n",
      "Iteration 107, loss = 0.08459719\n",
      "Iteration 78, loss = 0.12278559\n",
      "Iteration 79, loss = 0.12486455\n",
      "Iteration 108, loss = 0.08157319\n",
      "Iteration 80, loss = 0.12252988\n",
      "Iteration 109, loss = 0.08328668\n",
      "Iteration 81, loss = 0.11664704\n",
      "Iteration 110, loss = 0.08148762\n",
      "Iteration 82, loss = 0.11652354\n",
      "Iteration 83, loss = 0.11761188\n",
      "Iteration 111, loss = 0.08170741\n",
      "Iteration 84, loss = 0.11951512\n",
      "Iteration 112, loss = 0.07896749\n",
      "Iteration 113, loss = 0.07653186\n",
      "Iteration 85, loss = 0.11896812\n",
      "Iteration 114, loss = 0.07665485\n",
      "Iteration 86, loss = 0.11294217\n",
      "Iteration 87, loss = 0.11577634\n",
      "Iteration 115, loss = 0.07373909\n",
      "Iteration 88, loss = 0.10974404\n",
      "Iteration 89, loss = 0.10726099\n",
      "Iteration 116, loss = 0.07698214\n",
      "Iteration 117, loss = 0.07520711\n",
      "Iteration 90, loss = 0.10402660\n",
      "Iteration 118, loss = 0.07629702\n",
      "Iteration 91, loss = 0.10444221\n",
      "Iteration 92, loss = 0.10158035\n",
      "Iteration 119, loss = 0.07443081\n",
      "Iteration 93, loss = 0.10272418\n",
      "Iteration 94, loss = 0.09937191\n",
      "Iteration 120, loss = 0.07204488\n",
      "Iteration 121, loss = 0.06944545\n",
      "Iteration 95, loss = 0.10070532\n",
      "Iteration 122, loss = 0.07177037\n",
      "Iteration 96, loss = 0.09756240\n",
      "Iteration 123, loss = 0.07171218\n",
      "Iteration 97, loss = 0.09601041\n",
      "Iteration 98, loss = 0.09663040\n",
      "Iteration 124, loss = 0.06847806\n",
      "Iteration 99, loss = 0.09396350\n",
      "Iteration 125, loss = 0.07134127\n",
      "Iteration 100, loss = 0.09133435\n",
      "Iteration 126, loss = 0.06951421\n",
      "Iteration 101, loss = 0.09131705\n",
      "Iteration 127, loss = 0.07575677\n",
      "Iteration 102, loss = 0.09221824\n",
      "Iteration 128, loss = 0.08366686\n",
      "Iteration 103, loss = 0.08928215\n",
      "Iteration 129, loss = 0.07591332\n",
      "Iteration 104, loss = 0.08796803\n",
      "Iteration 130, loss = 0.07134898\n",
      "Iteration 131, loss = 0.06841084\n",
      "Iteration 105, loss = 0.08622468\n",
      "Iteration 132, loss = 0.07381569\n",
      "Iteration 106, loss = 0.08670623\n",
      "Iteration 133, loss = 0.06728190\n",
      "Iteration 107, loss = 0.08952377\n",
      "Iteration 108, loss = 0.08542106\n",
      "Iteration 109, loss = 0.08516524\n",
      "Iteration 110, loss = 0.08244271\n",
      "Iteration 134, loss = 0.06686924\n",
      "Iteration 135, loss = 0.06644854\n",
      "Iteration 111, loss = 0.08473754\n",
      "Iteration 112, loss = 0.08675204\n",
      "Iteration 136, loss = 0.06594350\n",
      "Iteration 137, loss = 0.07040477\n",
      "Iteration 113, loss = 0.08087542\n",
      "Iteration 114, loss = 0.07933368\n",
      "Iteration 138, loss = 0.07462937\n",
      "Iteration 115, loss = 0.07865560\n",
      "Iteration 139, loss = 0.07139539\n",
      "Iteration 116, loss = 0.07807729\n",
      "Iteration 140, loss = 0.06581526\n",
      "Iteration 117, loss = 0.07751935\n",
      "Iteration 141, loss = 0.06672682\n",
      "Iteration 118, loss = 0.07784705\n",
      "Iteration 142, loss = 0.06205851\n",
      "Iteration 119, loss = 0.07627904\n",
      "Iteration 120, loss = 0.07424484\n",
      "Iteration 121, loss = 0.07668000\n",
      "Iteration 143, loss = 0.06300914\n",
      "Iteration 122, loss = 0.07506205\n",
      "Iteration 144, loss = 0.05736897\n",
      "Iteration 145, loss = 0.05764316\n",
      "Iteration 123, loss = 0.07265968\n",
      "Iteration 146, loss = 0.06047360\n",
      "Iteration 147, loss = 0.06057970\n",
      "Iteration 148, loss = 0.05699826\n",
      "Iteration 124, loss = 0.07506212\n",
      "Iteration 149, loss = 0.05538294\n",
      "Iteration 150, loss = 0.05737316\n",
      "Iteration 125, loss = 0.06984045\n",
      "Iteration 151, loss = 0.05522797\n",
      "Iteration 126, loss = 0.06925006\n",
      "Iteration 127, loss = 0.06937023\n",
      "Iteration 152, loss = 0.05397436\n",
      "Iteration 153, loss = 0.05255088\n",
      "Iteration 128, loss = 0.06869170\n",
      "Iteration 154, loss = 0.05271867\n",
      "Iteration 129, loss = 0.07224146\n",
      "Iteration 155, loss = 0.05447543\n",
      "Iteration 130, loss = 0.07182176\n",
      "Iteration 131, loss = 0.06632865\n",
      "Iteration 156, loss = 0.06377049\n",
      "Iteration 157, loss = 0.05882959\n",
      "Iteration 132, loss = 0.06604469\n",
      "Iteration 158, loss = 0.05330503\n",
      "Iteration 159, loss = 0.05211207\n",
      "Iteration 133, loss = 0.06667942\n",
      "Iteration 134, loss = 0.06781465\n",
      "Iteration 135, loss = 0.06614733\n",
      "Iteration 160, loss = 0.05187379\n",
      "Iteration 136, loss = 0.06165102\n",
      "Iteration 161, loss = 0.05005670\n",
      "Iteration 162, loss = 0.05587463\n",
      "Iteration 163, loss = 0.05734757\n",
      "Iteration 137, loss = 0.06097933\n",
      "Iteration 138, loss = 0.06470811\n",
      "Iteration 139, loss = 0.06314190\n",
      "Iteration 164, loss = 0.05104314\n",
      "Iteration 140, loss = 0.06178053\n",
      "Iteration 165, loss = 0.04772670\n",
      "Iteration 166, loss = 0.04751329\n",
      "Iteration 141, loss = 0.06713075\n",
      "Iteration 167, loss = 0.04604979\n",
      "Iteration 142, loss = 0.07337803\n",
      "Iteration 168, loss = 0.04726022\n",
      "Iteration 143, loss = 0.06991259\n",
      "Iteration 169, loss = 0.04636294\n",
      "Iteration 170, loss = 0.04376652\n",
      "Iteration 171, loss = 0.04331443\n",
      "Iteration 144, loss = 0.06769858\n",
      "Iteration 172, loss = 0.04353441\n",
      "Iteration 145, loss = 0.06597706\n",
      "Iteration 146, loss = 0.06107272\n",
      "Iteration 173, loss = 0.04252969\n",
      "Iteration 174, loss = 0.04303418\n",
      "Iteration 147, loss = 0.05637823\n",
      "Iteration 175, loss = 0.04207431\n",
      "Iteration 148, loss = 0.05285344\n",
      "Iteration 176, loss = 0.04330455\n",
      "Iteration 149, loss = 0.05562413\n",
      "Iteration 150, loss = 0.05323848\n",
      "Iteration 177, loss = 0.04216867\n",
      "Iteration 151, loss = 0.05143845\n",
      "Iteration 152, loss = 0.05179238\n",
      "Iteration 153, loss = 0.04932114\n",
      "Iteration 178, loss = 0.04255160\n",
      "Iteration 154, loss = 0.04942111\n",
      "Iteration 179, loss = 0.04198261\n",
      "Iteration 155, loss = 0.04972153\n",
      "Iteration 180, loss = 0.04095920\n",
      "Iteration 156, loss = 0.05348005\n",
      "Iteration 181, loss = 0.03848152\n",
      "Iteration 157, loss = 0.04825905\n",
      "Iteration 182, loss = 0.03890105\n",
      "Iteration 158, loss = 0.04765884\n",
      "Iteration 183, loss = 0.03903947\n",
      "Iteration 184, loss = 0.03879710\n",
      "Iteration 159, loss = 0.04951881\n",
      "Iteration 185, loss = 0.03904447\n",
      "Iteration 160, loss = 0.05193937\n",
      "Iteration 186, loss = 0.03920787\n",
      "Iteration 161, loss = 0.04950491\n",
      "Iteration 187, loss = 0.04382275\n",
      "Iteration 188, loss = 0.04357914\n",
      "Iteration 162, loss = 0.04881556\n",
      "Iteration 189, loss = 0.04577349\n",
      "Iteration 163, loss = 0.04473197\n",
      "Iteration 164, loss = 0.04849643\n",
      "Iteration 190, loss = 0.04690875\n",
      "Iteration 165, loss = 0.04669275\n",
      "Iteration 166, loss = 0.04451420\n",
      "Iteration 167, loss = 0.04362273\n",
      "Iteration 168, loss = 0.04183401\n",
      "Iteration 191, loss = 0.04305686\n",
      "Iteration 169, loss = 0.04183394\n",
      "Iteration 192, loss = 0.04124091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 170, loss = 0.04296911\n",
      "Iteration 171, loss = 0.04245207\n",
      "Iteration 172, loss = 0.04153617\n",
      "Iteration 173, loss = 0.04291518\n",
      "Iteration 174, loss = 0.04027305\n",
      "Iteration 175, loss = 0.04134361\n",
      "Iteration 176, loss = 0.04098592\n",
      "Iteration 177, loss = 0.04000606\n",
      "Iteration 178, loss = 0.03991927\n",
      "Iteration 179, loss = 0.03982798\n",
      "Iteration 180, loss = 0.04434825\n",
      "Iteration 1, loss = 1.38307407\n",
      "Iteration 181, loss = 0.04337102\n",
      "Iteration 2, loss = 0.89264800\n",
      "Iteration 182, loss = 0.05212126\n",
      "Iteration 3, loss = 0.71353277\n",
      "Iteration 4, loss = 0.59554211\n",
      "Iteration 183, loss = 0.04879730\n",
      "Iteration 5, loss = 0.52576758\n",
      "Iteration 184, loss = 0.04765804\n",
      "Iteration 6, loss = 0.47531736\n",
      "Iteration 185, loss = 0.04479814\n",
      "Iteration 186, loss = 0.03765149\n",
      "Iteration 7, loss = 0.44383248\n",
      "Iteration 187, loss = 0.03913549\n",
      "Iteration 8, loss = 0.41736265\n",
      "Iteration 188, loss = 0.03582568\n",
      "Iteration 9, loss = 0.39670123\n",
      "Iteration 189, loss = 0.03516991\n",
      "Iteration 10, loss = 0.38206484\n",
      "Iteration 190, loss = 0.03350148\n",
      "Iteration 11, loss = 0.36541942\n",
      "Iteration 191, loss = 0.03354856\n",
      "Iteration 12, loss = 0.35413512\n",
      "Iteration 192, loss = 0.03276744\n",
      "Iteration 13, loss = 0.34139039\n",
      "Iteration 193, loss = 0.03384424\n",
      "Iteration 14, loss = 0.33296216\n",
      "Iteration 194, loss = 0.03427711\n",
      "Iteration 15, loss = 0.32288112\n",
      "Iteration 195, loss = 0.03405648\n",
      "Iteration 16, loss = 0.31474013\n",
      "Iteration 196, loss = 0.03303006\n",
      "Iteration 17, loss = 0.30562474\n",
      "Iteration 197, loss = 0.03287846\n",
      "Iteration 18, loss = 0.29959126\n",
      "Iteration 198, loss = 0.03066225\n",
      "Iteration 19, loss = 0.29178522\n",
      "Iteration 199, loss = 0.03130179\n",
      "Iteration 20, loss = 0.28611209\n",
      "Iteration 200, loss = 0.03509929\n",
      "Iteration 21, loss = 0.28006452\n",
      "Iteration 201, loss = 0.03649450\n",
      "Iteration 22, loss = 0.27342199\n",
      "Iteration 23, loss = 0.27178395\n",
      "Iteration 24, loss = 0.26318646\n",
      "Iteration 202, loss = 0.03624261\n",
      "Iteration 203, loss = 0.03207233\n",
      "Iteration 25, loss = 0.25948714\n",
      "Iteration 204, loss = 0.03094363\n",
      "Iteration 205, loss = 0.03022432\n",
      "Iteration 26, loss = 0.25575535\n",
      "Iteration 206, loss = 0.02849553\n",
      "Iteration 27, loss = 0.25203888\n",
      "Iteration 207, loss = 0.02693179\n",
      "Iteration 28, loss = 0.24507738\n",
      "Iteration 29, loss = 0.23912899\n",
      "Iteration 208, loss = 0.02831839\n",
      "Iteration 209, loss = 0.02825537\n",
      "Iteration 30, loss = 0.23816465\n",
      "Iteration 210, loss = 0.03353638\n",
      "Iteration 31, loss = 0.23149874\n",
      "Iteration 211, loss = 0.03272714\n",
      "Iteration 32, loss = 0.22933097\n",
      "Iteration 212, loss = 0.02893944\n",
      "Iteration 33, loss = 0.22423445\n",
      "Iteration 213, loss = 0.02964113\n",
      "Iteration 34, loss = 0.21913150\n",
      "Iteration 214, loss = 0.02692081\n",
      "Iteration 35, loss = 0.21550351\n",
      "Iteration 215, loss = 0.02716568\n",
      "Iteration 36, loss = 0.21138551\n",
      "Iteration 216, loss = 0.02601553\n",
      "Iteration 217, loss = 0.02495782\n",
      "Iteration 37, loss = 0.20685020\n",
      "Iteration 218, loss = 0.02522516\n",
      "Iteration 219, loss = 0.02786939\n",
      "Iteration 38, loss = 0.20652664\n",
      "Iteration 220, loss = 0.02499207\n",
      "Iteration 221, loss = 0.02377557\n",
      "Iteration 222, loss = 0.02477056\n",
      "Iteration 39, loss = 0.20105605\n",
      "Iteration 223, loss = 0.02307102\n",
      "Iteration 40, loss = 0.19850999\n",
      "Iteration 41, loss = 0.19470280\n",
      "Iteration 224, loss = 0.02589904\n",
      "Iteration 42, loss = 0.19118447\n",
      "Iteration 225, loss = 0.02619689\n",
      "Iteration 226, loss = 0.02662776\n",
      "Iteration 227, loss = 0.02708292\n",
      "Iteration 43, loss = 0.19051813\n",
      "Iteration 228, loss = 0.02426616\n",
      "Iteration 229, loss = 0.02412668\n",
      "Iteration 230, loss = 0.02256560\n",
      "Iteration 44, loss = 0.18564229\n",
      "Iteration 45, loss = 0.18304061\n",
      "Iteration 231, loss = 0.02315890\n",
      "Iteration 46, loss = 0.18098701\n",
      "Iteration 232, loss = 0.02346485\n",
      "Iteration 47, loss = 0.17895216\n",
      "Iteration 233, loss = 0.02219542\n",
      "Iteration 48, loss = 0.17510761\n",
      "Iteration 234, loss = 0.02108607\n",
      "Iteration 49, loss = 0.17208530\n",
      "Iteration 235, loss = 0.02186143\n",
      "Iteration 50, loss = 0.17119328\n",
      "Iteration 51, loss = 0.17173054\n",
      "Iteration 236, loss = 0.02108555\n",
      "Iteration 52, loss = 0.16401953\n",
      "Iteration 53, loss = 0.16071919\n",
      "Iteration 54, loss = 0.15677873\n",
      "Iteration 237, loss = 0.02120782\n",
      "Iteration 238, loss = 0.02183710\n",
      "Iteration 55, loss = 0.15686727\n",
      "Iteration 239, loss = 0.02287602\n",
      "Iteration 240, loss = 0.02109002\n",
      "Iteration 241, loss = 0.02035308\n",
      "Iteration 56, loss = 0.16312615\n",
      "Iteration 242, loss = 0.02296249\n",
      "Iteration 57, loss = 0.15946167\n",
      "Iteration 58, loss = 0.15172097\n",
      "Iteration 59, loss = 0.14621213\n",
      "Iteration 243, loss = 0.01996497\n",
      "Iteration 60, loss = 0.14713234\n",
      "Iteration 244, loss = 0.02207115\n",
      "Iteration 61, loss = 0.14245412\n",
      "Iteration 245, loss = 0.01985387\n",
      "Iteration 62, loss = 0.13899337\n",
      "Iteration 246, loss = 0.01993996\n",
      "Iteration 63, loss = 0.13996677\n",
      "Iteration 247, loss = 0.01861540\n",
      "Iteration 248, loss = 0.01906414\n",
      "Iteration 64, loss = 0.13613679\n",
      "Iteration 249, loss = 0.02099919\n",
      "Iteration 250, loss = 0.01819365\n",
      "Iteration 65, loss = 0.13529135\n",
      "Iteration 251, loss = 0.01886905\n",
      "Iteration 252, loss = 0.01808262\n",
      "Iteration 66, loss = 0.13081013\n",
      "Iteration 67, loss = 0.13155007\n",
      "Iteration 253, loss = 0.01764872\n",
      "Iteration 68, loss = 0.12916611\n",
      "Iteration 254, loss = 0.01803781\n",
      "Iteration 255, loss = 0.01767161\n",
      "Iteration 69, loss = 0.12727065\n",
      "Iteration 256, loss = 0.01823648\n",
      "Iteration 70, loss = 0.12526579\n",
      "Iteration 71, loss = 0.12540835\n",
      "Iteration 257, loss = 0.01902125\n",
      "Iteration 72, loss = 0.12276187\n",
      "Iteration 73, loss = 0.12160028\n",
      "Iteration 258, loss = 0.02240471\n",
      "Iteration 259, loss = 0.02315959\n",
      "Iteration 74, loss = 0.11940434\n",
      "Iteration 75, loss = 0.11769498\n",
      "Iteration 260, loss = 0.02599527\n",
      "Iteration 76, loss = 0.11621820\n",
      "Iteration 77, loss = 0.11499384\n",
      "Iteration 261, loss = 0.02262700\n",
      "Iteration 262, loss = 0.01838827\n",
      "Iteration 263, loss = 0.01940470\n",
      "Iteration 78, loss = 0.11226193\n",
      "Iteration 79, loss = 0.11423703\n",
      "Iteration 80, loss = 0.11220914\n",
      "Iteration 81, loss = 0.11154954\n",
      "Iteration 82, loss = 0.10625629\n",
      "Iteration 264, loss = 0.01659300\n",
      "Iteration 83, loss = 0.10680453\n",
      "Iteration 84, loss = 0.10875223\n",
      "Iteration 85, loss = 0.10341166\n",
      "Iteration 86, loss = 0.10532503\n",
      "Iteration 265, loss = 0.01587618\n",
      "Iteration 87, loss = 0.10243899\n",
      "Iteration 266, loss = 0.01605431\n",
      "Iteration 88, loss = 0.10533700\n",
      "Iteration 267, loss = 0.01692684\n",
      "Iteration 89, loss = 0.10751047\n",
      "Iteration 268, loss = 0.01605132\n",
      "Iteration 90, loss = 0.10268197\n",
      "Iteration 269, loss = 0.01592459\n",
      "Iteration 270, loss = 0.01600326\n",
      "Iteration 91, loss = 0.10533917\n",
      "Iteration 271, loss = 0.01548187\n",
      "Iteration 272, loss = 0.01489725\n",
      "Iteration 92, loss = 0.09890697\n",
      "Iteration 93, loss = 0.09870717\n",
      "Iteration 273, loss = 0.01605743\n",
      "Iteration 94, loss = 0.09652788\n",
      "Iteration 95, loss = 0.09310390\n",
      "Iteration 274, loss = 0.01475843\n",
      "Iteration 275, loss = 0.01635876\n",
      "Iteration 96, loss = 0.09306927\n",
      "Iteration 276, loss = 0.01464838\n",
      "Iteration 97, loss = 0.09377233\n",
      "Iteration 277, loss = 0.01590193\n",
      "Iteration 98, loss = 0.09127582\n",
      "Iteration 99, loss = 0.08835298\n",
      "Iteration 278, loss = 0.01495760\n",
      "Iteration 100, loss = 0.08733536\n",
      "Iteration 279, loss = 0.01461337\n",
      "Iteration 280, loss = 0.01417855\n",
      "Iteration 281, loss = 0.01429005\n",
      "Iteration 101, loss = 0.08924796\n",
      "Iteration 102, loss = 0.08510590\n",
      "Iteration 282, loss = 0.01654572\n",
      "Iteration 103, loss = 0.08496700\n",
      "Iteration 104, loss = 0.08698230\n",
      "Iteration 283, loss = 0.01466467\n",
      "Iteration 105, loss = 0.08436412\n",
      "Iteration 284, loss = 0.01502411\n",
      "Iteration 285, loss = 0.01514473\n",
      "Iteration 106, loss = 0.08201707\n",
      "Iteration 107, loss = 0.08052223\n",
      "Iteration 286, loss = 0.01591969\n",
      "Iteration 108, loss = 0.08386319\n",
      "Iteration 287, loss = 0.01506063\n",
      "Iteration 288, loss = 0.01378887\n",
      "Iteration 109, loss = 0.08555392\n",
      "Iteration 289, loss = 0.01559569\n",
      "Iteration 110, loss = 0.09034606\n",
      "Iteration 290, loss = 0.01280040\n",
      "Iteration 111, loss = 0.09183401\n",
      "Iteration 112, loss = 0.08192120\n",
      "Iteration 291, loss = 0.01479569\n",
      "Iteration 113, loss = 0.07858575\n",
      "Iteration 292, loss = 0.01428297\n",
      "Iteration 293, loss = 0.01242346\n",
      "Iteration 114, loss = 0.07571627\n",
      "Iteration 115, loss = 0.07568854\n",
      "Iteration 294, loss = 0.01271817\n",
      "Iteration 116, loss = 0.07468538\n",
      "Iteration 295, loss = 0.01435319\n",
      "Iteration 117, loss = 0.07135724\n",
      "Iteration 296, loss = 0.01504237\n",
      "Iteration 118, loss = 0.07257102\n",
      "Iteration 297, loss = 0.01226096\n",
      "Iteration 119, loss = 0.07222249\n",
      "Iteration 298, loss = 0.01373020\n",
      "Iteration 120, loss = 0.07093071\n",
      "Iteration 121, loss = 0.06892441\n",
      "Iteration 299, loss = 0.01229747\n",
      "Iteration 122, loss = 0.06903724\n",
      "Iteration 123, loss = 0.06832420\n",
      "Iteration 300, loss = 0.01252639\n",
      "Iteration 124, loss = 0.06615019\n",
      "Iteration 301, loss = 0.01231318\n",
      "Iteration 302, loss = 0.01180336\n",
      "Iteration 125, loss = 0.06843534\n",
      "Iteration 303, loss = 0.01149799\n",
      "Iteration 304, loss = 0.01225252\n",
      "Iteration 126, loss = 0.06660041\n",
      "Iteration 305, loss = 0.01252106\n",
      "Iteration 127, loss = 0.06822162\n",
      "Iteration 128, loss = 0.06721076\n",
      "Iteration 129, loss = 0.07155302\n",
      "Iteration 130, loss = 0.06987062\n",
      "Iteration 306, loss = 0.01209168\n",
      "Iteration 131, loss = 0.06998896\n",
      "Iteration 132, loss = 0.06883955\n",
      "Iteration 307, loss = 0.01198958\n",
      "Iteration 133, loss = 0.06915654\n",
      "Iteration 134, loss = 0.07052586\n",
      "Iteration 308, loss = 0.01373108\n",
      "Iteration 135, loss = 0.06990423\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 309, loss = 0.01442943\n",
      "Iteration 310, loss = 0.01132232\n",
      "Iteration 311, loss = 0.01205999\n",
      "Iteration 312, loss = 0.01187711\n",
      "Iteration 313, loss = 0.01089689\n",
      "Iteration 314, loss = 0.01087284\n",
      "Iteration 315, loss = 0.01178003\n",
      "Iteration 1, loss = 1.47862990\n",
      "Iteration 2, loss = 0.90518606\n",
      "Iteration 316, loss = 0.01348885\n",
      "Iteration 3, loss = 0.68730411\n",
      "Iteration 317, loss = 0.01190923\n",
      "Iteration 4, loss = 0.56023818\n",
      "Iteration 318, loss = 0.01314808\n",
      "Iteration 5, loss = 0.50587074\n",
      "Iteration 319, loss = 0.01071271\n",
      "Iteration 6, loss = 0.46652919\n",
      "Iteration 7, loss = 0.43688841\n",
      "Iteration 8, loss = 0.41485104\n",
      "Iteration 320, loss = 0.01025062\n",
      "Iteration 9, loss = 0.39857651\n",
      "Iteration 321, loss = 0.00994521\n",
      "Iteration 10, loss = 0.38162499\n",
      "Iteration 322, loss = 0.01072051\n",
      "Iteration 11, loss = 0.36802007\n",
      "Iteration 12, loss = 0.35758468\n",
      "Iteration 323, loss = 0.01209319\n",
      "Iteration 13, loss = 0.34603498\n",
      "Iteration 324, loss = 0.01199574\n",
      "Iteration 14, loss = 0.33770590\n",
      "Iteration 325, loss = 0.01066019\n",
      "Iteration 15, loss = 0.32811803\n",
      "Iteration 326, loss = 0.01706905\n",
      "Iteration 16, loss = 0.32135927\n",
      "Iteration 327, loss = 0.01596731\n",
      "Iteration 17, loss = 0.31391514\n",
      "Iteration 328, loss = 0.01629349\n",
      "Iteration 329, loss = 0.01540233\n",
      "Iteration 18, loss = 0.30577441\n",
      "Iteration 330, loss = 0.01697077\n",
      "Iteration 19, loss = 0.29925393\n",
      "Iteration 20, loss = 0.29317030\n",
      "Iteration 21, loss = 0.28938784\n",
      "Iteration 331, loss = 0.02186444\n",
      "Iteration 22, loss = 0.28098153\n",
      "Iteration 23, loss = 0.27893339\n",
      "Iteration 332, loss = 0.01979234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.27120803\n",
      "Iteration 25, loss = 0.26583210\n",
      "Iteration 26, loss = 0.26115780\n",
      "Iteration 27, loss = 0.25592473\n",
      "Iteration 1, loss = 1.21188952\n",
      "Iteration 2, loss = 0.81648744\n",
      "Iteration 28, loss = 0.25212830\n",
      "Iteration 29, loss = 0.24723803\n",
      "Iteration 3, loss = 0.63619543\n",
      "Iteration 4, loss = 0.53503482\n",
      "Iteration 5, loss = 0.48354090\n",
      "Iteration 6, loss = 0.44455598\n",
      "Iteration 7, loss = 0.41465006\n",
      "Iteration 30, loss = 0.24372350\n",
      "Iteration 8, loss = 0.38972912\n",
      "Iteration 31, loss = 0.23862864\n",
      "Iteration 32, loss = 0.23643177\n",
      "Iteration 9, loss = 0.37191161\n",
      "Iteration 33, loss = 0.23166137\n",
      "Iteration 10, loss = 0.35775331\n",
      "Iteration 34, loss = 0.22801238\n",
      "Iteration 11, loss = 0.34303873\n",
      "Iteration 12, loss = 0.33101693\n",
      "Iteration 35, loss = 0.22389248\n",
      "Iteration 36, loss = 0.22196481\n",
      "Iteration 37, loss = 0.21736872\n",
      "Iteration 13, loss = 0.31950787\n",
      "Iteration 38, loss = 0.21535143\n",
      "Iteration 39, loss = 0.21373158\n",
      "Iteration 40, loss = 0.20619115\n",
      "Iteration 41, loss = 0.20736375\n",
      "Iteration 14, loss = 0.30869963\n",
      "Iteration 42, loss = 0.20253590\n",
      "Iteration 43, loss = 0.19861177\n",
      "Iteration 44, loss = 0.19856974\n",
      "Iteration 45, loss = 0.19472902\n",
      "Iteration 15, loss = 0.30120183\n",
      "Iteration 46, loss = 0.19248535\n",
      "Iteration 16, loss = 0.29370905\n",
      "Iteration 47, loss = 0.19612941\n",
      "Iteration 17, loss = 0.28416226\n",
      "Iteration 48, loss = 0.19058628\n",
      "Iteration 18, loss = 0.27774333\n",
      "Iteration 49, loss = 0.18477591\n",
      "Iteration 19, loss = 0.27030653\n",
      "Iteration 50, loss = 0.18058519\n",
      "Iteration 51, loss = 0.18183394\n",
      "Iteration 20, loss = 0.26297682\n",
      "Iteration 52, loss = 0.17636833\n",
      "Iteration 21, loss = 0.25659461\n",
      "Iteration 53, loss = 0.17164888\n",
      "Iteration 54, loss = 0.16975503\n",
      "Iteration 22, loss = 0.25072582\n",
      "Iteration 55, loss = 0.16769540\n",
      "Iteration 56, loss = 0.16436661\n",
      "Iteration 23, loss = 0.24644860\n",
      "Iteration 24, loss = 0.24020352\n",
      "Iteration 57, loss = 0.16167038\n",
      "Iteration 58, loss = 0.15963105\n",
      "Iteration 59, loss = 0.15774454\n",
      "Iteration 25, loss = 0.23537296\n",
      "Iteration 26, loss = 0.23019692\n",
      "Iteration 60, loss = 0.15570641\n",
      "Iteration 27, loss = 0.22635482\n",
      "Iteration 28, loss = 0.22142257\n",
      "Iteration 61, loss = 0.15255215\n",
      "Iteration 62, loss = 0.15380857\n",
      "Iteration 29, loss = 0.21867887\n",
      "Iteration 63, loss = 0.15378948\n",
      "Iteration 64, loss = 0.15017511\n",
      "Iteration 30, loss = 0.21453867\n",
      "Iteration 31, loss = 0.20645715\n",
      "Iteration 32, loss = 0.20431780\n",
      "Iteration 65, loss = 0.14310861\n",
      "Iteration 33, loss = 0.19913196\n",
      "Iteration 66, loss = 0.14101056\n",
      "Iteration 67, loss = 0.14088636\n",
      "Iteration 68, loss = 0.13811754\n",
      "Iteration 34, loss = 0.19460089\n",
      "Iteration 35, loss = 0.19198483\n",
      "Iteration 36, loss = 0.18864160\n",
      "Iteration 69, loss = 0.13841845\n",
      "Iteration 37, loss = 0.18332462\n",
      "Iteration 38, loss = 0.18314979\n",
      "Iteration 70, loss = 0.13512011\n",
      "Iteration 71, loss = 0.13249909\n",
      "Iteration 39, loss = 0.18024813\n",
      "Iteration 72, loss = 0.13123218\n",
      "Iteration 73, loss = 0.12891845\n",
      "Iteration 74, loss = 0.12904030\n",
      "Iteration 40, loss = 0.17772873\n",
      "Iteration 75, loss = 0.12740677\n",
      "Iteration 41, loss = 0.16963279\n",
      "Iteration 76, loss = 0.12798216\n",
      "Iteration 42, loss = 0.16739940\n",
      "Iteration 43, loss = 0.16584326\n",
      "Iteration 44, loss = 0.15973928\n",
      "Iteration 77, loss = 0.13172040\n",
      "Iteration 78, loss = 0.12324472\n",
      "Iteration 79, loss = 0.12090913\n",
      "Iteration 45, loss = 0.15716829\n",
      "Iteration 80, loss = 0.12423564\n",
      "Iteration 46, loss = 0.15380141\n",
      "Iteration 47, loss = 0.14957704\n",
      "Iteration 81, loss = 0.11915038\n",
      "Iteration 82, loss = 0.11638598\n",
      "Iteration 48, loss = 0.15134304\n",
      "Iteration 83, loss = 0.11601845\n",
      "Iteration 84, loss = 0.12731823\n",
      "Iteration 85, loss = 0.13657756\n",
      "Iteration 49, loss = 0.14730921\n",
      "Iteration 86, loss = 0.12985782\n",
      "Iteration 87, loss = 0.12270945\n",
      "Iteration 88, loss = 0.11746085\n",
      "Iteration 50, loss = 0.14244013\n",
      "Iteration 89, loss = 0.11429530\n",
      "Iteration 90, loss = 0.10368282\n",
      "Iteration 91, loss = 0.11004452\n",
      "Iteration 51, loss = 0.14090437\n",
      "Iteration 92, loss = 0.10555973\n",
      "Iteration 52, loss = 0.13899355\n",
      "Iteration 53, loss = 0.13779200\n",
      "Iteration 93, loss = 0.10289483\n",
      "Iteration 54, loss = 0.14090667\n",
      "Iteration 94, loss = 0.10328132\n",
      "Iteration 95, loss = 0.10001233\n",
      "Iteration 96, loss = 0.09926197\n",
      "Iteration 55, loss = 0.13481239\n",
      "Iteration 97, loss = 0.10249689\n",
      "Iteration 56, loss = 0.13434902\n",
      "Iteration 57, loss = 0.12767284\n",
      "Iteration 58, loss = 0.12627722\n",
      "Iteration 98, loss = 0.10275678\n",
      "Iteration 59, loss = 0.12231705\n",
      "Iteration 60, loss = 0.12368991\n",
      "Iteration 61, loss = 0.11855934\n",
      "Iteration 99, loss = 0.09654400\n",
      "Iteration 62, loss = 0.11342283\n",
      "Iteration 100, loss = 0.09577337\n",
      "Iteration 63, loss = 0.11274107\n",
      "Iteration 101, loss = 0.09677490\n",
      "Iteration 64, loss = 0.11531597\n",
      "Iteration 65, loss = 0.11366431\n",
      "Iteration 102, loss = 0.09302472\n",
      "Iteration 66, loss = 0.11231891\n",
      "Iteration 103, loss = 0.09195724\n",
      "Iteration 104, loss = 0.09226824\n",
      "Iteration 105, loss = 0.09208148\n",
      "Iteration 106, loss = 0.09463419\n",
      "Iteration 67, loss = 0.10612550\n",
      "Iteration 107, loss = 0.09065128\n",
      "Iteration 108, loss = 0.08801422\n",
      "Iteration 109, loss = 0.08760591\n",
      "Iteration 110, loss = 0.08896849\n",
      "Iteration 68, loss = 0.10559926\n",
      "Iteration 111, loss = 0.08755316\n",
      "Iteration 69, loss = 0.10292333\n",
      "Iteration 112, loss = 0.08709994\n",
      "Iteration 113, loss = 0.08796620\n",
      "Iteration 70, loss = 0.10241274\n",
      "Iteration 114, loss = 0.08848161\n",
      "Iteration 71, loss = 0.10188759\n",
      "Iteration 72, loss = 0.09968314\n",
      "Iteration 73, loss = 0.09768948\n",
      "Iteration 115, loss = 0.08616166\n",
      "Iteration 116, loss = 0.08862766\n",
      "Iteration 117, loss = 0.08555711\n",
      "Iteration 74, loss = 0.09673968\n",
      "Iteration 118, loss = 0.08503506\n",
      "Iteration 75, loss = 0.09364987\n",
      "Iteration 119, loss = 0.08848113\n",
      "Iteration 76, loss = 0.09318065\n",
      "Iteration 120, loss = 0.08541797\n",
      "Iteration 77, loss = 0.09056894\n",
      "Iteration 121, loss = 0.08331847\n",
      "Iteration 78, loss = 0.08842601\n",
      "Iteration 122, loss = 0.08198197\n",
      "Iteration 79, loss = 0.08864158\n",
      "Iteration 123, loss = 0.08829268\n",
      "Iteration 124, loss = 0.09232213\n",
      "Iteration 80, loss = 0.08504583\n",
      "Iteration 81, loss = 0.08408414\n",
      "Iteration 82, loss = 0.08506762\n",
      "Iteration 125, loss = 0.08485815\n",
      "Iteration 83, loss = 0.08234581\n",
      "Iteration 84, loss = 0.08202282\n",
      "Iteration 85, loss = 0.08015734\n",
      "Iteration 86, loss = 0.07900072\n",
      "Iteration 126, loss = 0.07763386\n",
      "Iteration 87, loss = 0.08203249\n",
      "Iteration 127, loss = 0.08065993\n",
      "Iteration 88, loss = 0.07710215\n",
      "Iteration 128, loss = 0.07407582\n",
      "Iteration 89, loss = 0.07806362\n",
      "Iteration 129, loss = 0.07439718\n",
      "Iteration 90, loss = 0.07396507\n",
      "Iteration 130, loss = 0.07185495\n",
      "Iteration 91, loss = 0.07557865\n",
      "Iteration 131, loss = 0.07254471\n",
      "Iteration 92, loss = 0.07489996\n",
      "Iteration 132, loss = 0.07476756\n",
      "Iteration 93, loss = 0.07529222\n",
      "Iteration 133, loss = 0.07076633\n",
      "Iteration 94, loss = 0.07817183\n",
      "Iteration 134, loss = 0.07007404\n",
      "Iteration 95, loss = 0.07191634\n",
      "Iteration 96, loss = 0.07112562\n",
      "Iteration 135, loss = 0.06749062\n",
      "Iteration 97, loss = 0.07078239\n",
      "Iteration 136, loss = 0.06633043\n",
      "Iteration 98, loss = 0.06791191\n",
      "Iteration 137, loss = 0.07062813\n",
      "Iteration 138, loss = 0.06686045\n",
      "Iteration 139, loss = 0.06711869\n",
      "Iteration 99, loss = 0.06573695\n",
      "Iteration 140, loss = 0.06948035\n",
      "Iteration 100, loss = 0.06342019\n",
      "Iteration 101, loss = 0.06545928\n",
      "Iteration 141, loss = 0.06461485\n",
      "Iteration 102, loss = 0.06292440\n",
      "Iteration 142, loss = 0.06331293\n",
      "Iteration 143, loss = 0.06500977\n",
      "Iteration 144, loss = 0.06468292\n",
      "Iteration 103, loss = 0.06357216\n",
      "Iteration 145, loss = 0.06149661\n",
      "Iteration 104, loss = 0.06200376\n",
      "Iteration 146, loss = 0.06103194\n",
      "Iteration 105, loss = 0.06302356\n",
      "Iteration 147, loss = 0.06010634\n",
      "Iteration 106, loss = 0.06178460\n",
      "Iteration 148, loss = 0.06015160\n",
      "Iteration 149, loss = 0.05848511\n",
      "Iteration 107, loss = 0.06243423\n",
      "Iteration 150, loss = 0.06019855\n",
      "Iteration 151, loss = 0.05978155\n",
      "Iteration 108, loss = 0.05967242\n",
      "Iteration 152, loss = 0.05869750\n",
      "Iteration 109, loss = 0.05705234\n",
      "Iteration 153, loss = 0.05998648\n",
      "Iteration 110, loss = 0.05566628\n",
      "Iteration 154, loss = 0.06230607\n",
      "Iteration 155, loss = 0.06150574\n",
      "Iteration 111, loss = 0.05390773\n",
      "Iteration 156, loss = 0.05908452\n",
      "Iteration 157, loss = 0.05633570\n",
      "Iteration 158, loss = 0.06408351\n",
      "Iteration 112, loss = 0.05330118\n",
      "Iteration 159, loss = 0.06657855\n",
      "Iteration 113, loss = 0.05400321\n",
      "Iteration 114, loss = 0.05102550\n",
      "Iteration 160, loss = 0.06481128\n",
      "Iteration 115, loss = 0.05213313\n",
      "Iteration 161, loss = 0.06064900\n",
      "Iteration 162, loss = 0.05510185\n",
      "Iteration 163, loss = 0.05333407\n",
      "Iteration 116, loss = 0.05117342\n",
      "Iteration 117, loss = 0.05070750\n",
      "Iteration 164, loss = 0.05733522\n",
      "Iteration 118, loss = 0.05007263\n",
      "Iteration 119, loss = 0.04936986\n",
      "Iteration 165, loss = 0.05159161\n",
      "Iteration 166, loss = 0.05213575\n",
      "Iteration 120, loss = 0.04876439\n",
      "Iteration 167, loss = 0.05170519\n",
      "Iteration 121, loss = 0.04839436\n",
      "Iteration 168, loss = 0.05037352\n",
      "Iteration 122, loss = 0.04848752\n",
      "Iteration 123, loss = 0.05074749\n",
      "Iteration 169, loss = 0.05070062\n",
      "Iteration 124, loss = 0.04792160\n",
      "Iteration 170, loss = 0.04889501\n",
      "Iteration 171, loss = 0.04956270\n",
      "Iteration 125, loss = 0.04510599\n",
      "Iteration 126, loss = 0.04398544Iteration 172, loss = 0.04904941\n",
      "\n",
      "Iteration 127, loss = 0.04413372\n",
      "Iteration 173, loss = 0.04782414\n",
      "Iteration 128, loss = 0.04240133\n",
      "Iteration 174, loss = 0.04654745\n",
      "Iteration 129, loss = 0.04413181\n",
      "Iteration 175, loss = 0.04606251\n",
      "Iteration 130, loss = 0.04200024\n",
      "Iteration 176, loss = 0.04828496\n",
      "Iteration 131, loss = 0.04134839\n",
      "Iteration 132, loss = 0.04715624\n",
      "Iteration 133, loss = 0.04518861\n",
      "Iteration 177, loss = 0.04760705\n",
      "Iteration 134, loss = 0.04002058\n",
      "Iteration 178, loss = 0.04796888\n",
      "Iteration 179, loss = 0.04526518\n",
      "Iteration 135, loss = 0.03982777\n",
      "Iteration 136, loss = 0.03809945\n",
      "Iteration 137, loss = 0.03914700\n",
      "Iteration 180, loss = 0.04579551\n",
      "Iteration 138, loss = 0.03652800\n",
      "Iteration 181, loss = 0.04482036\n",
      "Iteration 139, loss = 0.03739502\n",
      "Iteration 182, loss = 0.04547436\n",
      "Iteration 140, loss = 0.03675680\n",
      "Iteration 183, loss = 0.04657424\n",
      "Iteration 141, loss = 0.03598941\n",
      "Iteration 184, loss = 0.04776250\n",
      "Iteration 142, loss = 0.03573745\n",
      "Iteration 185, loss = 0.04324354\n",
      "Iteration 143, loss = 0.03620421\n",
      "Iteration 186, loss = 0.04284789\n",
      "Iteration 144, loss = 0.03482850\n",
      "Iteration 187, loss = 0.04374959\n",
      "Iteration 145, loss = 0.03642334\n",
      "Iteration 188, loss = 0.04255390\n",
      "Iteration 146, loss = 0.03495401\n",
      "Iteration 147, loss = 0.03495388\n",
      "Iteration 189, loss = 0.04619464\n",
      "Iteration 148, loss = 0.03462797\n",
      "Iteration 190, loss = 0.04546140\n",
      "Iteration 149, loss = 0.03486165\n",
      "Iteration 191, loss = 0.04757413\n",
      "Iteration 150, loss = 0.03321980\n",
      "Iteration 192, loss = 0.04731006\n",
      "Iteration 193, loss = 0.04827948\n",
      "Iteration 151, loss = 0.03111310\n",
      "Iteration 194, loss = 0.04909983\n",
      "Iteration 195, loss = 0.05434021\n",
      "Iteration 152, loss = 0.03246234\n",
      "Iteration 196, loss = 0.04492878\n",
      "Iteration 197, loss = 0.04535815\n",
      "Iteration 153, loss = 0.03185728\n",
      "Iteration 154, loss = 0.03092502\n",
      "Iteration 198, loss = 0.04026843\n",
      "Iteration 155, loss = 0.03102960\n",
      "Iteration 156, loss = 0.03032146\n",
      "Iteration 199, loss = 0.04148710\n",
      "Iteration 200, loss = 0.03923824\n",
      "Iteration 201, loss = 0.04140117\n",
      "Iteration 157, loss = 0.03002765\n",
      "Iteration 202, loss = 0.03915046\n",
      "Iteration 203, loss = 0.03796810\n",
      "Iteration 204, loss = 0.03736372\n",
      "Iteration 158, loss = 0.02930439\n",
      "Iteration 205, loss = 0.03703847\n",
      "Iteration 206, loss = 0.03744093\n",
      "Iteration 207, loss = 0.03971148\n",
      "Iteration 159, loss = 0.02886228\n",
      "Iteration 208, loss = 0.03986775\n",
      "Iteration 209, loss = 0.04079539\n",
      "Iteration 210, loss = 0.04321275\n",
      "Iteration 160, loss = 0.02889315\n",
      "Iteration 211, loss = 0.04375505\n",
      "Iteration 161, loss = 0.03031530\n",
      "Iteration 212, loss = 0.03987208\n",
      "Iteration 213, loss = 0.04195490\n",
      "Iteration 162, loss = 0.02911340\n",
      "Iteration 163, loss = 0.02981031\n",
      "Iteration 214, loss = 0.03486162\n",
      "Iteration 215, loss = 0.03632844\n",
      "Iteration 164, loss = 0.02687228\n",
      "Iteration 216, loss = 0.03508184\n",
      "Iteration 165, loss = 0.02678420\n",
      "Iteration 166, loss = 0.02590527\n",
      "Iteration 167, loss = 0.02710293\n",
      "Iteration 217, loss = 0.03560060\n",
      "Iteration 218, loss = 0.03551236\n",
      "Iteration 168, loss = 0.02598294\n",
      "Iteration 219, loss = 0.03370569\n",
      "Iteration 169, loss = 0.02538719\n",
      "Iteration 220, loss = 0.03237883\n",
      "Iteration 170, loss = 0.02471468\n",
      "Iteration 171, loss = 0.02415078\n",
      "Iteration 221, loss = 0.03209660\n",
      "Iteration 172, loss = 0.02431718\n",
      "Iteration 222, loss = 0.03167430\n",
      "Iteration 223, loss = 0.03338359\n",
      "Iteration 173, loss = 0.02386586\n",
      "Iteration 174, loss = 0.02482742\n",
      "Iteration 224, loss = 0.03266713\n",
      "Iteration 175, loss = 0.02449352\n",
      "Iteration 176, loss = 0.02303547\n",
      "Iteration 225, loss = 0.03187120\n",
      "Iteration 177, loss = 0.02382136\n",
      "Iteration 226, loss = 0.03283889\n",
      "Iteration 178, loss = 0.02264595\n",
      "Iteration 227, loss = 0.03422717\n",
      "Iteration 179, loss = 0.02246163\n",
      "Iteration 228, loss = 0.03347775\n",
      "Iteration 229, loss = 0.03137684\n",
      "Iteration 180, loss = 0.02489977\n",
      "Iteration 230, loss = 0.03234921\n",
      "Iteration 181, loss = 0.02757916\n",
      "Iteration 182, loss = 0.02808877\n",
      "Iteration 231, loss = 0.03084157\n",
      "Iteration 183, loss = 0.02578947\n",
      "Iteration 232, loss = 0.03002852\n",
      "Iteration 184, loss = 0.02566560\n",
      "Iteration 233, loss = 0.02829741\n",
      "Iteration 234, loss = 0.02825433\n",
      "Iteration 185, loss = 0.02124977\n",
      "Iteration 186, loss = 0.02379158\n",
      "Iteration 187, loss = 0.02287938\n",
      "Iteration 235, loss = 0.02831048\n",
      "Iteration 188, loss = 0.02043193\n",
      "Iteration 236, loss = 0.02764168\n",
      "Iteration 237, loss = 0.02976744\n",
      "Iteration 189, loss = 0.01864775\n",
      "Iteration 238, loss = 0.03038715\n",
      "Iteration 190, loss = 0.02010278\n",
      "Iteration 191, loss = 0.01871217\n",
      "Iteration 239, loss = 0.03215404\n",
      "Iteration 192, loss = 0.01851715\n",
      "Iteration 240, loss = 0.02802894\n",
      "Iteration 193, loss = 0.01947436\n",
      "Iteration 241, loss = 0.02812392\n",
      "Iteration 242, loss = 0.02710177\n",
      "Iteration 194, loss = 0.01932609\n",
      "Iteration 195, loss = 0.01856871\n",
      "Iteration 196, loss = 0.01689669\n",
      "Iteration 243, loss = 0.02619484\n",
      "Iteration 197, loss = 0.01748942\n",
      "Iteration 244, loss = 0.02609242\n",
      "Iteration 245, loss = 0.02719140\n",
      "Iteration 198, loss = 0.01781344\n",
      "Iteration 246, loss = 0.02970002\n",
      "Iteration 247, loss = 0.03523549\n",
      "Iteration 248, loss = 0.03016557\n",
      "Iteration 199, loss = 0.01744731\n",
      "Iteration 249, loss = 0.02833972\n",
      "Iteration 250, loss = 0.02581934\n",
      "Iteration 200, loss = 0.01742834\n",
      "Iteration 251, loss = 0.02924617\n",
      "Iteration 252, loss = 0.02902503\n",
      "Iteration 253, loss = 0.02669333\n",
      "Iteration 201, loss = 0.01642957\n",
      "Iteration 254, loss = 0.02513437\n",
      "Iteration 255, loss = 0.02477285\n",
      "Iteration 202, loss = 0.01623440\n",
      "Iteration 256, loss = 0.02427734\n",
      "Iteration 203, loss = 0.01593490\n",
      "Iteration 257, loss = 0.02501496\n",
      "Iteration 258, loss = 0.02293309\n",
      "Iteration 259, loss = 0.02423621\n",
      "Iteration 204, loss = 0.01596169\n",
      "Iteration 260, loss = 0.02296023\n",
      "Iteration 205, loss = 0.01546758\n",
      "Iteration 261, loss = 0.02362486\n",
      "Iteration 206, loss = 0.01659290\n",
      "Iteration 207, loss = 0.01538522\n",
      "Iteration 208, loss = 0.01575381\n",
      "Iteration 262, loss = 0.02350776\n",
      "Iteration 263, loss = 0.02287291\n",
      "Iteration 264, loss = 0.02240051\n",
      "Iteration 209, loss = 0.01484007\n",
      "Iteration 265, loss = 0.02384262Iteration 210, loss = 0.01453005\n",
      "\n",
      "Iteration 211, loss = 0.01450025\n",
      "Iteration 266, loss = 0.02550681\n",
      "Iteration 267, loss = 0.02493131\n",
      "Iteration 212, loss = 0.01508361\n",
      "Iteration 268, loss = 0.02201309\n",
      "Iteration 213, loss = 0.01492058\n",
      "Iteration 269, loss = 0.02309453\n",
      "Iteration 214, loss = 0.01580700\n",
      "Iteration 270, loss = 0.02150728\n",
      "Iteration 215, loss = 0.01589827\n",
      "Iteration 271, loss = 0.02231540\n",
      "Iteration 216, loss = 0.01661687\n",
      "Iteration 272, loss = 0.02182942\n",
      "Iteration 217, loss = 0.01549873\n",
      "Iteration 273, loss = 0.02165531\n",
      "Iteration 218, loss = 0.01588769\n",
      "Iteration 274, loss = 0.02172522\n",
      "Iteration 219, loss = 0.01442126\n",
      "Iteration 275, loss = 0.02356185\n",
      "Iteration 220, loss = 0.01461525\n",
      "Iteration 276, loss = 0.02226770\n",
      "Iteration 221, loss = 0.01454561\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 277, loss = 0.02475587\n",
      "Iteration 278, loss = 0.02521383\n",
      "Iteration 279, loss = 0.02291676\n",
      "Iteration 280, loss = 0.02249719\n",
      "Iteration 281, loss = 0.02065458\n",
      "Iteration 282, loss = 0.02244312\n",
      "Iteration 283, loss = 0.02322310\n",
      "Iteration 284, loss = 0.02070164\n",
      "Iteration 285, loss = 0.01959748\n",
      "Iteration 286, loss = 0.02263811\n",
      "Iteration 1, loss = 1.21966829\n",
      "Iteration 287, loss = 0.01883516\n",
      "Iteration 288, loss = 0.02269095\n",
      "Iteration 2, loss = 0.83838268\n",
      "Iteration 289, loss = 0.01963752\n",
      "Iteration 290, loss = 0.02641475\n",
      "Iteration 3, loss = 0.64530205\n",
      "Iteration 291, loss = 0.02240136\n",
      "Iteration 4, loss = 0.55272723\n",
      "Iteration 5, loss = 0.48780569\n",
      "Iteration 292, loss = 0.02030585\n",
      "Iteration 6, loss = 0.44523947\n",
      "Iteration 293, loss = 0.01973581\n",
      "Iteration 7, loss = 0.41210242\n",
      "Iteration 8, loss = 0.38778165\n",
      "Iteration 294, loss = 0.02057260\n",
      "Iteration 9, loss = 0.36943627\n",
      "Iteration 10, loss = 0.35063129\n",
      "Iteration 295, loss = 0.02568159\n",
      "Iteration 11, loss = 0.33738206\n",
      "Iteration 296, loss = 0.02466187\n",
      "Iteration 297, loss = 0.02077027\n",
      "Iteration 12, loss = 0.32395586\n",
      "Iteration 298, loss = 0.01860113\n",
      "Iteration 13, loss = 0.31262713\n",
      "Iteration 299, loss = 0.01792507\n",
      "Iteration 14, loss = 0.30358284\n",
      "Iteration 300, loss = 0.01753647\n",
      "Iteration 15, loss = 0.29407668\n",
      "Iteration 16, loss = 0.28548109\n",
      "Iteration 17, loss = 0.27874777\n",
      "Iteration 18, loss = 0.27067743\n",
      "Iteration 301, loss = 0.01706118\n",
      "Iteration 19, loss = 0.26454839\n",
      "Iteration 20, loss = 0.25766601\n",
      "Iteration 302, loss = 0.01733490\n",
      "Iteration 21, loss = 0.25200610\n",
      "Iteration 303, loss = 0.01780642\n",
      "Iteration 22, loss = 0.24486327\n",
      "Iteration 304, loss = 0.02018036\n",
      "Iteration 23, loss = 0.24187546\n",
      "Iteration 24, loss = 0.23285493\n",
      "Iteration 305, loss = 0.02014567\n",
      "Iteration 25, loss = 0.23254488\n",
      "Iteration 26, loss = 0.22424379\n",
      "Iteration 306, loss = 0.01659426\n",
      "Iteration 307, loss = 0.01677643\n",
      "Iteration 308, loss = 0.01814641\n",
      "Iteration 27, loss = 0.22074075\n",
      "Iteration 28, loss = 0.21503412\n",
      "Iteration 29, loss = 0.21056159\n",
      "Iteration 309, loss = 0.01745410\n",
      "Iteration 30, loss = 0.20667200\n",
      "Iteration 310, loss = 0.01673829\n",
      "Iteration 31, loss = 0.20434805\n",
      "Iteration 311, loss = 0.01716982\n",
      "Iteration 312, loss = 0.01663629\n",
      "Iteration 313, loss = 0.01676617\n",
      "Iteration 32, loss = 0.20024563\n",
      "Iteration 314, loss = 0.01690861\n",
      "Iteration 33, loss = 0.19788544\n",
      "Iteration 34, loss = 0.19216336\n",
      "Iteration 35, loss = 0.18907966\n",
      "Iteration 36, loss = 0.18389704\n",
      "Iteration 315, loss = 0.01689703\n",
      "Iteration 37, loss = 0.18064424\n",
      "Iteration 38, loss = 0.17866462\n",
      "Iteration 316, loss = 0.01707045\n",
      "Iteration 317, loss = 0.01981648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.17471719\n",
      "Iteration 40, loss = 0.17270244\n",
      "Iteration 41, loss = 0.17358147\n",
      "Iteration 42, loss = 0.16716484\n",
      "Iteration 43, loss = 0.16337572\n",
      "Iteration 44, loss = 0.16685890\n",
      "Iteration 45, loss = 0.16028913\n",
      "Iteration 46, loss = 0.15674086\n",
      "Iteration 47, loss = 0.15475544\n",
      "Iteration 1, loss = 1.05503461\n",
      "Iteration 48, loss = 0.15027072\n",
      "Iteration 49, loss = 0.14869902\n",
      "Iteration 50, loss = 0.15168637\n",
      "Iteration 2, loss = 0.74577968\n",
      "Iteration 51, loss = 0.14216873\n",
      "Iteration 3, loss = 0.59388720\n",
      "Iteration 4, loss = 0.51214114\n",
      "Iteration 52, loss = 0.14501445\n",
      "Iteration 53, loss = 0.14350703\n",
      "Iteration 5, loss = 0.45523507\n",
      "Iteration 54, loss = 0.13677437\n",
      "Iteration 55, loss = 0.13761499\n",
      "Iteration 6, loss = 0.41715630\n",
      "Iteration 7, loss = 0.39165513\n",
      "Iteration 8, loss = 0.36897431\n",
      "Iteration 56, loss = 0.13523254\n",
      "Iteration 9, loss = 0.35087644\n",
      "Iteration 57, loss = 0.13159749\n",
      "Iteration 58, loss = 0.12964463\n",
      "Iteration 59, loss = 0.12807975\n",
      "Iteration 10, loss = 0.34065187\n",
      "Iteration 60, loss = 0.12872018\n",
      "Iteration 61, loss = 0.12547381\n",
      "Iteration 11, loss = 0.32740216\n",
      "Iteration 62, loss = 0.12550952\n",
      "Iteration 12, loss = 0.31807228\n",
      "Iteration 13, loss = 0.30647318\n",
      "Iteration 63, loss = 0.12417869\n",
      "Iteration 14, loss = 0.29865724\n",
      "Iteration 15, loss = 0.29152142\n",
      "Iteration 64, loss = 0.12156415\n",
      "Iteration 16, loss = 0.28382132\n",
      "Iteration 17, loss = 0.27579222\n",
      "Iteration 18, loss = 0.26891716\n",
      "Iteration 65, loss = 0.11894170\n",
      "Iteration 66, loss = 0.12292514\n",
      "Iteration 19, loss = 0.26550503\n",
      "Iteration 67, loss = 0.12205043\n",
      "Iteration 20, loss = 0.26011568\n",
      "Iteration 21, loss = 0.25771943\n",
      "Iteration 68, loss = 0.11688746\n",
      "Iteration 22, loss = 0.25090312\n",
      "Iteration 69, loss = 0.11610943\n",
      "Iteration 70, loss = 0.11358962\n",
      "Iteration 23, loss = 0.24389200\n",
      "Iteration 71, loss = 0.10966030\n",
      "Iteration 24, loss = 0.24373485\n",
      "Iteration 25, loss = 0.23771003\n",
      "Iteration 72, loss = 0.11114827\n",
      "Iteration 26, loss = 0.22735434\n",
      "Iteration 27, loss = 0.22979430\n",
      "Iteration 73, loss = 0.10794551\n",
      "Iteration 74, loss = 0.10573391\n",
      "Iteration 75, loss = 0.10536191\n",
      "Iteration 76, loss = 0.10366846\n",
      "Iteration 28, loss = 0.22121114\n",
      "Iteration 77, loss = 0.10333504\n",
      "Iteration 78, loss = 0.10588295\n",
      "Iteration 79, loss = 0.10398204\n",
      "Iteration 29, loss = 0.21540784\n",
      "Iteration 30, loss = 0.21106350\n",
      "Iteration 31, loss = 0.20856125\n",
      "Iteration 80, loss = 0.10440948\n",
      "Iteration 32, loss = 0.20357267\n",
      "Iteration 81, loss = 0.09891546\n",
      "Iteration 82, loss = 0.09987437\n",
      "Iteration 33, loss = 0.20352675\n",
      "Iteration 83, loss = 0.09764256\n",
      "Iteration 84, loss = 0.09546104\n",
      "Iteration 85, loss = 0.09561615\n",
      "Iteration 34, loss = 0.19774677\n",
      "Iteration 35, loss = 0.19654555\n",
      "Iteration 86, loss = 0.09321914\n",
      "Iteration 36, loss = 0.19041057\n",
      "Iteration 87, loss = 0.09156012\n",
      "Iteration 37, loss = 0.18925209\n",
      "Iteration 88, loss = 0.09201614\n",
      "Iteration 89, loss = 0.09094775\n",
      "Iteration 90, loss = 0.08998886\n",
      "Iteration 38, loss = 0.18282558\n",
      "Iteration 39, loss = 0.18436562\n",
      "Iteration 40, loss = 0.18052691\n",
      "Iteration 91, loss = 0.08960579\n",
      "Iteration 92, loss = 0.09073949\n",
      "Iteration 41, loss = 0.17808252\n",
      "Iteration 93, loss = 0.09382364\n",
      "Iteration 94, loss = 0.08807561\n",
      "Iteration 42, loss = 0.17224033\n",
      "Iteration 43, loss = 0.16959761\n",
      "Iteration 44, loss = 0.16531281\n",
      "Iteration 95, loss = 0.08861205\n",
      "Iteration 45, loss = 0.16562163\n",
      "Iteration 96, loss = 0.08679163\n",
      "Iteration 46, loss = 0.16318311\n",
      "Iteration 97, loss = 0.08636279\n",
      "Iteration 98, loss = 0.08256994\n",
      "Iteration 47, loss = 0.16095604\n",
      "Iteration 48, loss = 0.15322002\n",
      "Iteration 99, loss = 0.08081405\n",
      "Iteration 49, loss = 0.15303360\n",
      "Iteration 50, loss = 0.15055743\n",
      "Iteration 100, loss = 0.08002696\n",
      "Iteration 101, loss = 0.08206177\n",
      "Iteration 51, loss = 0.14952613\n",
      "Iteration 102, loss = 0.08640285\n",
      "Iteration 103, loss = 0.08223169\n",
      "Iteration 52, loss = 0.14588142\n",
      "Iteration 53, loss = 0.14309130\n",
      "Iteration 104, loss = 0.07834963\n",
      "Iteration 54, loss = 0.14352152\n",
      "Iteration 55, loss = 0.13876248\n",
      "Iteration 105, loss = 0.07713628\n",
      "Iteration 56, loss = 0.13947544\n",
      "Iteration 57, loss = 0.13444001\n",
      "Iteration 106, loss = 0.07988587\n",
      "Iteration 58, loss = 0.13250804\n",
      "Iteration 59, loss = 0.13297407\n",
      "Iteration 107, loss = 0.07691299\n",
      "Iteration 60, loss = 0.13721943\n",
      "Iteration 108, loss = 0.07359820\n",
      "Iteration 61, loss = 0.13325037\n",
      "Iteration 109, loss = 0.07363178\n",
      "Iteration 62, loss = 0.13382729\n",
      "Iteration 63, loss = 0.12569430\n",
      "Iteration 110, loss = 0.07421013\n",
      "Iteration 64, loss = 0.12158329\n",
      "Iteration 65, loss = 0.12763691\n",
      "Iteration 111, loss = 0.07389352\n",
      "Iteration 112, loss = 0.06996481\n",
      "Iteration 66, loss = 0.12392197\n",
      "Iteration 113, loss = 0.06990508\n",
      "Iteration 67, loss = 0.11903446\n",
      "Iteration 114, loss = 0.06986034\n",
      "Iteration 68, loss = 0.11669087\n",
      "Iteration 115, loss = 0.06875169\n",
      "Iteration 116, loss = 0.06710222\n",
      "Iteration 69, loss = 0.11678675\n",
      "Iteration 117, loss = 0.06646218\n",
      "Iteration 70, loss = 0.11441284\n",
      "Iteration 118, loss = 0.06737330\n",
      "Iteration 71, loss = 0.11188762\n",
      "Iteration 119, loss = 0.06786255\n",
      "Iteration 120, loss = 0.06825394\n",
      "Iteration 121, loss = 0.07150708\n",
      "Iteration 72, loss = 0.11056117\n",
      "Iteration 122, loss = 0.07720128\n",
      "Iteration 73, loss = 0.11213760\n",
      "Iteration 74, loss = 0.11025375\n",
      "Iteration 75, loss = 0.10725480\n",
      "Iteration 123, loss = 0.08124327\n",
      "Iteration 76, loss = 0.10926187\n",
      "Iteration 77, loss = 0.10807612\n",
      "Iteration 78, loss = 0.10275776\n",
      "Iteration 124, loss = 0.07595848\n",
      "Iteration 79, loss = 0.10221373\n",
      "Iteration 125, loss = 0.06840896\n",
      "Iteration 80, loss = 0.10090824\n",
      "Iteration 81, loss = 0.09940778\n",
      "Iteration 82, loss = 0.09764496\n",
      "Iteration 126, loss = 0.06487933\n",
      "Iteration 83, loss = 0.09986591\n",
      "Iteration 127, loss = 0.06410198\n",
      "Iteration 128, loss = 0.06572664\n",
      "Iteration 129, loss = 0.07183640\n",
      "Iteration 130, loss = 0.07505209\n",
      "Iteration 84, loss = 0.09806165\n",
      "Iteration 131, loss = 0.07225708\n",
      "Iteration 85, loss = 0.09606001\n",
      "Iteration 132, loss = 0.07168098\n",
      "Iteration 86, loss = 0.09443661\n",
      "Iteration 133, loss = 0.06163921\n",
      "Iteration 87, loss = 0.09277775\n",
      "Iteration 134, loss = 0.05921335\n",
      "Iteration 135, loss = 0.05860481\n",
      "Iteration 88, loss = 0.09248613\n",
      "Iteration 136, loss = 0.05716196\n",
      "Iteration 89, loss = 0.09289215\n",
      "Iteration 90, loss = 0.09194900\n",
      "Iteration 91, loss = 0.08820569\n",
      "Iteration 137, loss = 0.05672940\n",
      "Iteration 92, loss = 0.09144574\n",
      "Iteration 138, loss = 0.05446366\n",
      "Iteration 93, loss = 0.08963989\n",
      "Iteration 94, loss = 0.09188796\n",
      "Iteration 95, loss = 0.10043859\n",
      "Iteration 139, loss = 0.05562036\n",
      "Iteration 140, loss = 0.06060267\n",
      "Iteration 96, loss = 0.09123561\n",
      "Iteration 141, loss = 0.06174490\n",
      "Iteration 97, loss = 0.08589958\n",
      "Iteration 98, loss = 0.08502223\n",
      "Iteration 142, loss = 0.05962871\n",
      "Iteration 99, loss = 0.08176593\n",
      "Iteration 143, loss = 0.06107355\n",
      "Iteration 144, loss = 0.06760249\n",
      "Iteration 100, loss = 0.08109900\n",
      "Iteration 145, loss = 0.05800741\n",
      "Iteration 146, loss = 0.05544416\n",
      "Iteration 101, loss = 0.08296629\n",
      "Iteration 147, loss = 0.05480210\n",
      "Iteration 102, loss = 0.08453615\n",
      "Iteration 103, loss = 0.08318415\n",
      "Iteration 148, loss = 0.05180463\n",
      "Iteration 149, loss = 0.04795118\n",
      "Iteration 104, loss = 0.07813941\n",
      "Iteration 150, loss = 0.05000472\n",
      "Iteration 105, loss = 0.08112314\n",
      "Iteration 106, loss = 0.08771764\n",
      "Iteration 151, loss = 0.04839143\n",
      "Iteration 107, loss = 0.10146035\n",
      "Iteration 152, loss = 0.04783895\n",
      "Iteration 108, loss = 0.09712881\n",
      "Iteration 109, loss = 0.09083626\n",
      "Iteration 110, loss = 0.08954666\n",
      "Iteration 153, loss = 0.04599312\n",
      "Iteration 111, loss = 0.08241086\n",
      "Iteration 112, loss = 0.07860511\n",
      "Iteration 113, loss = 0.07332379\n",
      "Iteration 154, loss = 0.04636033\n",
      "Iteration 114, loss = 0.06967888\n",
      "Iteration 155, loss = 0.04588953\n",
      "Iteration 115, loss = 0.06982155\n",
      "Iteration 156, loss = 0.04655272\n",
      "Iteration 116, loss = 0.06957124\n",
      "Iteration 157, loss = 0.04479796\n",
      "Iteration 117, loss = 0.06746447\n",
      "Iteration 158, loss = 0.04558752\n",
      "Iteration 159, loss = 0.04507778\n",
      "Iteration 160, loss = 0.04250877\n",
      "Iteration 118, loss = 0.06680147\n",
      "Iteration 161, loss = 0.04309959\n",
      "Iteration 119, loss = 0.06669821\n",
      "Iteration 162, loss = 0.04358384\n",
      "Iteration 120, loss = 0.06479488\n",
      "Iteration 163, loss = 0.04084997\n",
      "Iteration 121, loss = 0.06655741\n",
      "Iteration 164, loss = 0.04105715\n",
      "Iteration 122, loss = 0.06152418\n",
      "Iteration 123, loss = 0.06639217\n",
      "Iteration 165, loss = 0.04210484\n",
      "Iteration 166, loss = 0.04108362\n",
      "Iteration 124, loss = 0.06378537\n",
      "Iteration 167, loss = 0.04304314\n",
      "Iteration 125, loss = 0.06465726\n",
      "Iteration 126, loss = 0.06502327\n",
      "Iteration 168, loss = 0.04080788\n",
      "Iteration 127, loss = 0.07155071\n",
      "Iteration 169, loss = 0.04333454\n",
      "Iteration 128, loss = 0.06319215\n",
      "Iteration 170, loss = 0.04574201\n",
      "Iteration 129, loss = 0.05880533\n",
      "Iteration 171, loss = 0.04543146\n",
      "Iteration 130, loss = 0.05742811\n",
      "Iteration 172, loss = 0.04186746\n",
      "Iteration 173, loss = 0.04225805\n",
      "Iteration 131, loss = 0.05709043\n",
      "Iteration 174, loss = 0.03885911\n",
      "Iteration 175, loss = 0.03789411\n",
      "Iteration 176, loss = 0.03717858\n",
      "Iteration 132, loss = 0.05623322\n",
      "Iteration 177, loss = 0.03475011\n",
      "Iteration 133, loss = 0.05530749\n",
      "Iteration 178, loss = 0.03601077\n",
      "Iteration 134, loss = 0.05645196\n",
      "Iteration 179, loss = 0.03809375\n",
      "Iteration 135, loss = 0.05379716\n",
      "Iteration 136, loss = 0.05387351\n",
      "Iteration 180, loss = 0.03450359\n",
      "Iteration 181, loss = 0.03495006\n",
      "Iteration 137, loss = 0.05422817\n",
      "Iteration 138, loss = 0.05184683\n",
      "Iteration 182, loss = 0.03505067\n",
      "Iteration 139, loss = 0.05128792\n",
      "Iteration 140, loss = 0.05224045\n",
      "Iteration 183, loss = 0.03409657\n",
      "Iteration 141, loss = 0.05033149\n",
      "Iteration 184, loss = 0.03320903\n",
      "Iteration 142, loss = 0.05158847\n",
      "Iteration 185, loss = 0.03379029\n",
      "Iteration 143, loss = 0.05089580\n",
      "Iteration 144, loss = 0.04971536\n",
      "Iteration 186, loss = 0.03623389\n",
      "Iteration 145, loss = 0.04841834\n",
      "Iteration 146, loss = 0.04758162\n",
      "Iteration 187, loss = 0.03768086\n",
      "Iteration 147, loss = 0.05009909\n",
      "Iteration 188, loss = 0.03458118\n",
      "Iteration 148, loss = 0.05028230\n",
      "Iteration 189, loss = 0.03009316\n",
      "Iteration 149, loss = 0.05805376\n",
      "Iteration 190, loss = 0.03359708\n",
      "Iteration 150, loss = 0.04904565\n",
      "Iteration 191, loss = 0.03192133\n",
      "Iteration 151, loss = 0.04733196\n",
      "Iteration 192, loss = 0.03102430\n",
      "Iteration 193, loss = 0.03170340\n",
      "Iteration 152, loss = 0.04663305\n",
      "Iteration 194, loss = 0.03229576\n",
      "Iteration 195, loss = 0.03466876\n",
      "Iteration 153, loss = 0.04382159\n",
      "Iteration 154, loss = 0.04685277\n",
      "Iteration 155, loss = 0.04477613\n",
      "Iteration 196, loss = 0.03276123\n",
      "Iteration 156, loss = 0.04596101\n",
      "Iteration 197, loss = 0.03437686\n",
      "Iteration 198, loss = 0.03252774\n",
      "Iteration 199, loss = 0.03516689\n",
      "Iteration 200, loss = 0.02868701\n",
      "Iteration 157, loss = 0.04277450\n",
      "Iteration 201, loss = 0.02835598\n",
      "Iteration 158, loss = 0.04385663\n",
      "Iteration 159, loss = 0.04291569\n",
      "Iteration 160, loss = 0.04430232\n",
      "Iteration 202, loss = 0.02896360\n",
      "Iteration 161, loss = 0.04145773\n",
      "Iteration 203, loss = 0.02727300\n",
      "Iteration 162, loss = 0.04368333\n",
      "Iteration 163, loss = 0.04019981\n",
      "Iteration 204, loss = 0.02715949\n",
      "Iteration 205, loss = 0.02856849\n",
      "Iteration 164, loss = 0.03967573\n",
      "Iteration 206, loss = 0.02658165\n",
      "Iteration 165, loss = 0.04033439\n",
      "Iteration 166, loss = 0.03886439\n",
      "Iteration 167, loss = 0.03892824\n",
      "Iteration 168, loss = 0.03892065\n",
      "Iteration 207, loss = 0.02710870\n",
      "Iteration 169, loss = 0.03711254\n",
      "Iteration 208, loss = 0.02655387\n",
      "Iteration 170, loss = 0.03689252\n",
      "Iteration 209, loss = 0.02533740\n",
      "Iteration 171, loss = 0.03801910\n",
      "Iteration 210, loss = 0.02515089\n",
      "Iteration 172, loss = 0.03607143\n",
      "Iteration 173, loss = 0.03463457\n",
      "Iteration 211, loss = 0.02497859\n",
      "Iteration 174, loss = 0.03551887\n",
      "Iteration 175, loss = 0.03492590\n",
      "Iteration 176, loss = 0.03452570\n",
      "Iteration 212, loss = 0.02479703\n",
      "Iteration 177, loss = 0.03295399\n",
      "Iteration 213, loss = 0.02491470\n",
      "Iteration 214, loss = 0.02645902\n",
      "Iteration 178, loss = 0.03232897\n",
      "Iteration 179, loss = 0.03335666\n",
      "Iteration 180, loss = 0.03334936\n",
      "Iteration 215, loss = 0.02501069\n",
      "Iteration 181, loss = 0.03110020\n",
      "Iteration 216, loss = 0.02600446\n",
      "Iteration 182, loss = 0.03100258\n",
      "Iteration 217, loss = 0.02468647\n",
      "Iteration 183, loss = 0.03131561\n",
      "Iteration 218, loss = 0.02370364\n",
      "Iteration 184, loss = 0.03116651\n",
      "Iteration 219, loss = 0.02641466\n",
      "Iteration 185, loss = 0.03289303\n",
      "Iteration 220, loss = 0.02286916\n",
      "Iteration 221, loss = 0.02309555\n",
      "Iteration 186, loss = 0.03069329\n",
      "Iteration 222, loss = 0.02342446\n",
      "Iteration 187, loss = 0.02974301\n",
      "Iteration 223, loss = 0.02759403\n",
      "Iteration 188, loss = 0.02939921\n",
      "Iteration 224, loss = 0.02895249\n",
      "Iteration 189, loss = 0.02878774\n",
      "Iteration 225, loss = 0.02656755\n",
      "Iteration 226, loss = 0.02317540\n",
      "Iteration 190, loss = 0.02851065\n",
      "Iteration 227, loss = 0.02784887\n",
      "Iteration 191, loss = 0.02784931\n",
      "Iteration 192, loss = 0.02979620\n",
      "Iteration 193, loss = 0.02975704\n",
      "Iteration 228, loss = 0.02390242\n",
      "Iteration 229, loss = 0.02626279\n",
      "Iteration 230, loss = 0.02703065\n",
      "Iteration 194, loss = 0.03024557\n",
      "Iteration 195, loss = 0.02810285\n",
      "Iteration 231, loss = 0.02449145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 196, loss = 0.02712029\n",
      "Iteration 197, loss = 0.02730127\n",
      "Iteration 198, loss = 0.02771360\n",
      "Iteration 199, loss = 0.02884620\n",
      "Iteration 200, loss = 0.02805325\n",
      "Iteration 201, loss = 0.02644553\n",
      "Iteration 202, loss = 0.02605810\n",
      "Iteration 203, loss = 0.02734254\n",
      "Iteration 204, loss = 0.02649559\n",
      "Iteration 205, loss = 0.02578976\n",
      "Iteration 206, loss = 0.02429620\n",
      "Iteration 1, loss = 21.14146174\n",
      "Iteration 207, loss = 0.02463254\n",
      "Iteration 2, loss = 19.64688637\n",
      "Iteration 208, loss = 0.02370500\n",
      "Iteration 209, loss = 0.02376924\n",
      "Iteration 210, loss = 0.02229408\n",
      "Iteration 3, loss = 11.10128978\n",
      "Iteration 4, loss = 9.50399985\n",
      "Iteration 211, loss = 0.02213172\n",
      "Iteration 5, loss = 8.73010113\n",
      "Iteration 6, loss = 7.53020791\n",
      "Iteration 212, loss = 0.02404638\n",
      "Iteration 7, loss = 7.49159761\n",
      "Iteration 8, loss = 7.39161963\n",
      "Iteration 9, loss = 5.62376884\n",
      "Iteration 213, loss = 0.02240644\n",
      "Iteration 10, loss = 6.08553218\n",
      "Iteration 11, loss = 6.50275679\n",
      "Iteration 214, loss = 0.02259316\n",
      "Iteration 12, loss = 6.26518716\n",
      "Iteration 215, loss = 0.02303447\n",
      "Iteration 216, loss = 0.02148577\n",
      "Iteration 217, loss = 0.02140935\n",
      "Iteration 218, loss = 0.02115946\n",
      "Iteration 13, loss = 4.83292452\n",
      "Iteration 219, loss = 0.02037649\n",
      "Iteration 14, loss = 4.80084585\n",
      "Iteration 220, loss = 0.02028520\n",
      "Iteration 15, loss = 4.89706093\n",
      "Iteration 16, loss = 5.01684451\n",
      "Iteration 221, loss = 0.02135918\n",
      "Iteration 17, loss = 3.93326115\n",
      "Iteration 18, loss = 4.61700569\n",
      "Iteration 222, loss = 0.02211802\n",
      "Iteration 19, loss = 5.23735712\n",
      "Iteration 223, loss = 0.02217475\n",
      "Iteration 20, loss = 4.98450833\n",
      "Iteration 21, loss = 4.15139708\n",
      "Iteration 224, loss = 0.02268126\n",
      "Iteration 22, loss = 4.22662972\n",
      "Iteration 23, loss = 4.04710119\n",
      "Iteration 225, loss = 0.02186911\n",
      "Iteration 24, loss = 3.91939831\n",
      "Iteration 226, loss = 0.02241237\n",
      "Iteration 227, loss = 0.02006952\n",
      "Iteration 25, loss = 3.52740152\n",
      "Iteration 26, loss = 3.79155080\n",
      "Iteration 228, loss = 0.01956431\n",
      "Iteration 27, loss = 3.56844114\n",
      "Iteration 229, loss = 0.01974966\n",
      "Iteration 28, loss = 2.99520885\n",
      "Iteration 230, loss = 0.01813485\n",
      "Iteration 231, loss = 0.01795791\n",
      "Iteration 232, loss = 0.01852555\n",
      "Iteration 29, loss = 3.39682093\n",
      "Iteration 30, loss = 3.15299502\n",
      "Iteration 233, loss = 0.01911276\n",
      "Iteration 31, loss = 3.73365636\n",
      "Iteration 234, loss = 0.01953731\n",
      "Iteration 32, loss = 4.22995082\n",
      "Iteration 235, loss = 0.01909472\n",
      "Iteration 33, loss = 3.58663558\n",
      "Iteration 236, loss = 0.01915470\n",
      "Iteration 34, loss = 3.50835821\n",
      "Iteration 35, loss = 4.09337263\n",
      "Iteration 237, loss = 0.01808318\n",
      "Iteration 36, loss = 3.23996644\n",
      "Iteration 37, loss = 3.24079679\n",
      "Iteration 238, loss = 0.01646737\n",
      "Iteration 38, loss = 2.23934431\n",
      "Iteration 39, loss = 2.37817941\n",
      "Iteration 40, loss = 2.23588367\n",
      "Iteration 41, loss = 2.01786839\n",
      "Iteration 239, loss = 0.01614162\n",
      "Iteration 42, loss = 2.72818724\n",
      "Iteration 240, loss = 0.01582774\n",
      "Iteration 43, loss = 2.60656494\n",
      "Iteration 241, loss = 0.01694404\n",
      "Iteration 242, loss = 0.01680091\n",
      "Iteration 243, loss = 0.01982412\n",
      "Iteration 44, loss = 2.43472222\n",
      "Iteration 244, loss = 0.01932978\n",
      "Iteration 45, loss = 2.07231659\n",
      "Iteration 46, loss = 2.08411843\n",
      "Iteration 47, loss = 1.52247521\n",
      "Iteration 245, loss = 0.02202250\n",
      "Iteration 48, loss = 1.83032581\n",
      "Iteration 246, loss = 0.02245881\n",
      "Iteration 49, loss = 1.80632416\n",
      "Iteration 247, loss = 0.02439015\n",
      "Iteration 50, loss = 2.03034849\n",
      "Iteration 248, loss = 0.02230274\n",
      "Iteration 51, loss = 2.10063760\n",
      "Iteration 249, loss = 0.01700925\n",
      "Iteration 250, loss = 0.01926421\n",
      "Iteration 52, loss = 2.21111677\n",
      "Iteration 251, loss = 0.01812098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 2.26818151\n",
      "Iteration 54, loss = 4.30732565\n",
      "Iteration 55, loss = 4.25188146\n",
      "Iteration 56, loss = 3.88121732\n",
      "Iteration 57, loss = 2.96896357\n",
      "Iteration 58, loss = 3.32170154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.68688388\n",
      "Iteration 2, loss = 9.47179275\n",
      "Iteration 3, loss = 11.32492062\n",
      "Iteration 4, loss = 10.80036809\n",
      "Iteration 5, loss = 8.01886744\n",
      "Iteration 6, loss = 7.18163827\n",
      "Iteration 7, loss = 8.82585225\n",
      "Iteration 8, loss = 9.39686967\n",
      "Iteration 1, loss = 18.02038950\n",
      "Iteration 9, loss = 8.58383749\n",
      "Iteration 10, loss = 8.84483925\n",
      "Iteration 11, loss = 6.08882787\n",
      "Iteration 2, loss = 13.11673716\n",
      "Iteration 12, loss = 4.83886860\n",
      "Iteration 13, loss = 4.68812107\n",
      "Iteration 3, loss = 11.56056398\n",
      "Iteration 4, loss = 9.53364758\n",
      "Iteration 14, loss = 4.14068947\n",
      "Iteration 5, loss = 6.90334269\n",
      "Iteration 15, loss = 5.01356377\n",
      "Iteration 6, loss = 7.75345509\n",
      "Iteration 16, loss = 5.07549431\n",
      "Iteration 7, loss = 6.58725510\n",
      "Iteration 17, loss = 4.27590265\n",
      "Iteration 18, loss = 4.99020604\n",
      "Iteration 8, loss = 7.38587590\n",
      "Iteration 19, loss = 3.99725563\n",
      "Iteration 9, loss = 7.07333638\n",
      "Iteration 10, loss = 7.02639498\n",
      "Iteration 20, loss = 3.28104298\n",
      "Iteration 11, loss = 5.58714614\n",
      "Iteration 21, loss = 3.15257209\n",
      "Iteration 12, loss = 6.14504793\n",
      "Iteration 22, loss = 3.25583643\n",
      "Iteration 13, loss = 5.02954868\n",
      "Iteration 23, loss = 3.66814648\n",
      "Iteration 14, loss = 4.44579530\n",
      "Iteration 24, loss = 3.13914182\n",
      "Iteration 15, loss = 4.09945527\n",
      "Iteration 16, loss = 4.82861129\n",
      "Iteration 25, loss = 3.10956532\n",
      "Iteration 17, loss = 4.38486913\n",
      "Iteration 26, loss = 2.54822968\n",
      "Iteration 27, loss = 2.16592192\n",
      "Iteration 28, loss = 1.99567503\n",
      "Iteration 18, loss = 4.60467568\n",
      "Iteration 29, loss = 2.06779943\n",
      "Iteration 19, loss = 5.18174430\n",
      "Iteration 30, loss = 2.59310349\n",
      "Iteration 20, loss = 4.41744862\n",
      "Iteration 31, loss = 2.98961529\n",
      "Iteration 21, loss = 3.31787468\n",
      "Iteration 32, loss = 2.67599452\n",
      "Iteration 22, loss = 3.12253586\n",
      "Iteration 23, loss = 3.79109237\n",
      "Iteration 24, loss = 3.26703359\n",
      "Iteration 33, loss = 3.80115016\n",
      "Iteration 34, loss = 2.79107366\n",
      "Iteration 25, loss = 4.42925713\n",
      "Iteration 35, loss = 3.38639638\n",
      "Iteration 36, loss = 2.41879047\n",
      "Iteration 37, loss = 2.43592181\n",
      "Iteration 26, loss = 4.36184490\n",
      "Iteration 38, loss = 2.20154758\n",
      "Iteration 39, loss = 2.25643865\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 3.40703725\n",
      "Iteration 28, loss = 3.08737524\n",
      "Iteration 29, loss = 2.95209578\n",
      "Iteration 30, loss = 3.57059083\n",
      "Iteration 31, loss = 3.29070764\n",
      "Iteration 32, loss = 2.82158376\n",
      "Iteration 33, loss = 2.81382930\n",
      "Iteration 34, loss = 3.41480198\n",
      "Iteration 35, loss = 3.03244796\n",
      "Iteration 36, loss = 2.71609969\n",
      "Iteration 37, loss = 3.51326172\n",
      "Iteration 38, loss = 4.18541314\n",
      "Iteration 1, loss = 17.34499713\n",
      "Iteration 39, loss = 4.48840941\n",
      "Iteration 2, loss = 19.46817385\n",
      "Iteration 40, loss = 4.02364890\n",
      "Iteration 3, loss = 15.83204435\n",
      "Iteration 41, loss = 3.29088076\n",
      "Iteration 4, loss = 9.96086156\n",
      "Iteration 5, loss = 9.73743944\n",
      "Iteration 6, loss = 9.86686408\n",
      "Iteration 42, loss = 4.03081103\n",
      "Iteration 7, loss = 11.33025912\n",
      "Iteration 43, loss = 3.14595153\n",
      "Iteration 8, loss = 6.98839734\n",
      "Iteration 44, loss = 2.96880804\n",
      "Iteration 45, loss = 2.45549994\n",
      "Iteration 46, loss = 2.26715713\n",
      "Iteration 47, loss = 2.63233309\n",
      "Iteration 9, loss = 7.57839093\n",
      "Iteration 48, loss = 2.48485605\n",
      "Iteration 10, loss = 5.28204144\n",
      "Iteration 49, loss = 2.64687466\n",
      "Iteration 11, loss = 5.34771774\n",
      "Iteration 50, loss = 2.78358042\n",
      "Iteration 51, loss = 1.96970571\n",
      "Iteration 12, loss = 8.34835096\n",
      "Iteration 52, loss = 2.17684353\n",
      "Iteration 53, loss = 3.20354597\n",
      "Iteration 13, loss = 5.76883479\n",
      "Iteration 54, loss = 2.22629617\n",
      "Iteration 14, loss = 6.32838327\n",
      "Iteration 15, loss = 5.26365979\n",
      "Iteration 55, loss = 2.06771155\n",
      "Iteration 16, loss = 3.89569491\n",
      "Iteration 17, loss = 4.47202399\n",
      "Iteration 18, loss = 3.78598692\n",
      "Iteration 56, loss = 2.18103241\n",
      "Iteration 19, loss = 4.72403130\n",
      "Iteration 20, loss = 4.53466810\n",
      "Iteration 21, loss = 5.48067433\n",
      "Iteration 57, loss = 2.06582013\n",
      "Iteration 22, loss = 8.08976557\n",
      "Iteration 23, loss = 6.33756359\n",
      "Iteration 58, loss = 2.29591744\n",
      "Iteration 24, loss = 8.64770171\n",
      "Iteration 25, loss = 7.16903379\n",
      "Iteration 26, loss = 5.63490987\n",
      "Iteration 59, loss = 2.77504799\n",
      "Iteration 27, loss = 5.07900135\n",
      "Iteration 60, loss = 2.37946252\n",
      "Iteration 28, loss = 3.53613685\n",
      "Iteration 61, loss = 2.63822144\n",
      "Iteration 29, loss = 3.87080527\n",
      "Iteration 62, loss = 2.39870443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 3.76130672\n",
      "Iteration 31, loss = 3.50964362\n",
      "Iteration 32, loss = 4.83005734\n",
      "Iteration 33, loss = 4.23161092\n",
      "Iteration 34, loss = 3.04532938\n",
      "Iteration 35, loss = 3.75060430\n",
      "Iteration 36, loss = 2.81445600\n",
      "Iteration 37, loss = 4.17316490\n",
      "Iteration 38, loss = 2.78175688\n",
      "Iteration 39, loss = 3.15181796\n",
      "Iteration 40, loss = 2.93306914\n",
      "Iteration 1, loss = 15.53539464\n",
      "Iteration 2, loss = 15.35478177\n",
      "Iteration 41, loss = 2.76993875\n",
      "Iteration 3, loss = 14.61662675\n",
      "Iteration 4, loss = 9.41074182\n",
      "Iteration 42, loss = 2.65844668\n",
      "Iteration 5, loss = 9.30375797\n",
      "Iteration 43, loss = 3.22801290\n",
      "Iteration 6, loss = 9.35337391\n",
      "Iteration 7, loss = 6.53855473\n",
      "Iteration 44, loss = 2.00717402\n",
      "Iteration 45, loss = 2.18789604\n",
      "Iteration 8, loss = 5.95863567\n",
      "Iteration 46, loss = 3.08514946\n",
      "Iteration 9, loss = 5.79981311\n",
      "Iteration 10, loss = 5.73883601\n",
      "Iteration 11, loss = 4.60369109\n",
      "Iteration 12, loss = 4.83401259\n",
      "Iteration 13, loss = 6.71246180\n",
      "Iteration 14, loss = 7.68358478\n",
      "Iteration 15, loss = 5.86809548\n",
      "Iteration 47, loss = 4.50255501\n",
      "Iteration 16, loss = 7.11958204\n",
      "Iteration 48, loss = 2.80480089\n",
      "Iteration 17, loss = 4.36571626\n",
      "Iteration 18, loss = 3.59664728\n",
      "Iteration 49, loss = 2.75233762\n",
      "Iteration 19, loss = 4.29457777\n",
      "Iteration 20, loss = 4.70487337\n",
      "Iteration 50, loss = 2.34074422\n",
      "Iteration 21, loss = 3.59263308\n",
      "Iteration 22, loss = 3.52130382\n",
      "Iteration 51, loss = 2.27485248\n",
      "Iteration 52, loss = 2.11634330\n",
      "Iteration 53, loss = 2.58426525\n",
      "Iteration 23, loss = 3.58232160\n",
      "Iteration 24, loss = 3.56324311\n",
      "Iteration 54, loss = 2.49851516\n",
      "Iteration 25, loss = 3.92626610\n",
      "Iteration 26, loss = 3.34530029\n",
      "Iteration 55, loss = 2.31081219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 3.01515724\n",
      "Iteration 28, loss = 3.44568656\n",
      "Iteration 29, loss = 3.07547041\n",
      "Iteration 30, loss = 2.80866723\n",
      "Iteration 31, loss = 2.95116196\n",
      "Iteration 32, loss = 3.03114482\n",
      "Iteration 33, loss = 3.52479120\n",
      "Iteration 34, loss = 3.65240138\n",
      "Iteration 35, loss = 3.59452749\n",
      "Iteration 36, loss = 4.19101824\n",
      "Iteration 37, loss = 3.22652081\n",
      "Iteration 1, loss = 15.27463100\n",
      "Iteration 2, loss = 16.37178854\n",
      "Iteration 38, loss = 2.39938342\n",
      "Iteration 3, loss = 9.73303760\n",
      "Iteration 39, loss = 2.58872783\n",
      "Iteration 40, loss = 2.35449100\n",
      "Iteration 4, loss = 9.45517263\n",
      "Iteration 41, loss = 1.96336616\n",
      "Iteration 5, loss = 7.71367388\n",
      "Iteration 6, loss = 9.20154146\n",
      "Iteration 42, loss = 2.34408404\n",
      "Iteration 43, loss = 2.29644395\n",
      "Iteration 7, loss = 7.87588724\n",
      "Iteration 44, loss = 2.33376687\n",
      "Iteration 45, loss = 2.04847547\n",
      "Iteration 8, loss = 8.61653458\n",
      "Iteration 9, loss = 6.78827906\n",
      "Iteration 10, loss = 5.85903529\n",
      "Iteration 46, loss = 2.54817013\n",
      "Iteration 11, loss = 5.77530819\n",
      "Iteration 47, loss = 2.17845618\n",
      "Iteration 48, loss = 2.20929287\n",
      "Iteration 49, loss = 2.06034539\n",
      "Iteration 12, loss = 4.77288814\n",
      "Iteration 50, loss = 2.17581363\n",
      "Iteration 51, loss = 2.07911357\n",
      "Iteration 52, loss = 2.13855625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 6.71064805\n",
      "Iteration 14, loss = 5.26271660\n",
      "Iteration 15, loss = 8.13207440\n",
      "Iteration 16, loss = 9.67336934\n",
      "Iteration 17, loss = 9.44664410\n",
      "Iteration 18, loss = 9.61597926\n",
      "Iteration 19, loss = 6.25531431\n",
      "Iteration 20, loss = 5.72549946\n",
      "Iteration 1, loss = 16.53961822\n",
      "Iteration 21, loss = 5.84005731\n",
      "Iteration 22, loss = 5.76259110\n",
      "Iteration 2, loss = 15.69600511\n",
      "Iteration 23, loss = 5.18667312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 11.59959182\n",
      "Iteration 4, loss = 8.15956825\n",
      "Iteration 5, loss = 10.15790785\n",
      "Iteration 6, loss = 8.45896779\n",
      "Iteration 7, loss = 12.85424473\n",
      "Iteration 8, loss = 9.81335158\n",
      "Iteration 9, loss = 6.91911807\n",
      "Iteration 10, loss = 6.98838862\n",
      "Iteration 11, loss = 6.82312294\n",
      "Iteration 12, loss = 6.61805545\n",
      "Iteration 13, loss = 4.45244878\n",
      "Iteration 1, loss = 18.16434086\n",
      "Iteration 14, loss = 5.23237375\n",
      "Iteration 2, loss = 18.72580624\n",
      "Iteration 15, loss = 4.05254647\n",
      "Iteration 3, loss = 12.46065892\n",
      "Iteration 16, loss = 3.91829210\n",
      "Iteration 4, loss = 9.80774220\n",
      "Iteration 17, loss = 4.42968742\n",
      "Iteration 5, loss = 7.34747446\n",
      "Iteration 18, loss = 4.55388257\n",
      "Iteration 6, loss = 7.16376813\n",
      "Iteration 7, loss = 6.96456740\n",
      "Iteration 19, loss = 3.65558808\n",
      "Iteration 20, loss = 4.71275418\n",
      "Iteration 8, loss = 5.61016139\n",
      "Iteration 9, loss = 6.67823903\n",
      "Iteration 10, loss = 6.72651381\n",
      "Iteration 21, loss = 4.28106167\n",
      "Iteration 22, loss = 4.26608808\n",
      "Iteration 11, loss = 5.54187429\n",
      "Iteration 23, loss = 3.99051908\n",
      "Iteration 24, loss = 3.60620315\n",
      "Iteration 25, loss = 3.84227448\n",
      "Iteration 12, loss = 4.70286300\n",
      "Iteration 26, loss = 3.22354544\n",
      "Iteration 13, loss = 3.94545979\n",
      "Iteration 27, loss = 3.61705858\n",
      "Iteration 28, loss = 3.00923395\n",
      "Iteration 29, loss = 2.80700447\n",
      "Iteration 14, loss = 3.87577328\n",
      "Iteration 30, loss = 2.53484242\n",
      "Iteration 15, loss = 3.99450607\n",
      "Iteration 16, loss = 3.65163237\n",
      "Iteration 17, loss = 3.33018419\n",
      "Iteration 31, loss = 2.72466752\n",
      "Iteration 32, loss = 2.94787204\n",
      "Iteration 33, loss = 2.77529300\n",
      "Iteration 34, loss = 3.42209748\n",
      "Iteration 18, loss = 2.96694770\n",
      "Iteration 35, loss = 2.93980898\n",
      "Iteration 19, loss = 3.31035466\n",
      "Iteration 36, loss = 4.33930986\n",
      "Iteration 37, loss = 4.44997212\n",
      "Iteration 20, loss = 3.40796966\n",
      "Iteration 38, loss = 3.56321818\n",
      "Iteration 21, loss = 3.06632115\n",
      "Iteration 39, loss = 3.70472050\n",
      "Iteration 40, loss = 2.69531556\n",
      "Iteration 41, loss = 2.55966897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 5.65870008\n",
      "Iteration 23, loss = 3.95767007\n",
      "Iteration 24, loss = 3.66422520\n",
      "Iteration 25, loss = 3.42297372\n",
      "Iteration 26, loss = 3.77136029\n",
      "Iteration 27, loss = 2.92981722\n",
      "Iteration 1, loss = 17.50311734\n",
      "Iteration 28, loss = 2.92812762\n",
      "Iteration 2, loss = 19.50900536\n",
      "Iteration 29, loss = 2.51938534\n",
      "Iteration 30, loss = 2.84385511\n",
      "Iteration 31, loss = 2.72728305\n",
      "Iteration 3, loss = 12.69789399\n",
      "Iteration 32, loss = 3.14280384\n",
      "Iteration 33, loss = 2.51688372\n",
      "Iteration 4, loss = 9.76257303\n",
      "Iteration 5, loss = 7.08295871\n",
      "Iteration 6, loss = 6.09707470\n",
      "Iteration 34, loss = 2.51717425\n",
      "Iteration 7, loss = 6.22627985\n",
      "Iteration 35, loss = 3.32426098\n",
      "Iteration 8, loss = 6.47433264\n",
      "Iteration 36, loss = 3.23946142\n",
      "Iteration 9, loss = 7.77771020\n",
      "Iteration 37, loss = 2.04477758\n",
      "Iteration 38, loss = 2.76374300\n",
      "Iteration 10, loss = 11.75714555\n",
      "Iteration 39, loss = 2.20445286\n",
      "Iteration 11, loss = 5.97667091\n",
      "Iteration 12, loss = 5.94134811\n",
      "Iteration 40, loss = 1.79702455\n",
      "Iteration 41, loss = 1.50050663\n",
      "Iteration 42, loss = 1.54081664\n",
      "Iteration 13, loss = 5.08215869\n",
      "Iteration 43, loss = 1.58110924\n",
      "Iteration 44, loss = 1.50959347\n",
      "Iteration 45, loss = 1.30816901\n",
      "Iteration 14, loss = 3.66870873\n",
      "Iteration 15, loss = 6.20141343\n",
      "Iteration 16, loss = 3.19521431\n",
      "Iteration 46, loss = 1.77977219\n",
      "Iteration 17, loss = 3.05100979\n",
      "Iteration 47, loss = 1.77587675\n",
      "Iteration 48, loss = 1.34703608\n",
      "Iteration 18, loss = 3.32192079\n",
      "Iteration 49, loss = 1.09263998\n",
      "Iteration 19, loss = 3.79536820\n",
      "Iteration 50, loss = 1.82047644\n",
      "Iteration 20, loss = 3.14187375\n",
      "Iteration 21, loss = 2.40674634\n",
      "Iteration 22, loss = 2.90385169\n",
      "Iteration 51, loss = 1.53735800\n",
      "Iteration 52, loss = 1.45341025\n",
      "Iteration 53, loss = 1.78503977\n",
      "Iteration 23, loss = 2.52274251\n",
      "Iteration 54, loss = 2.42866054\n",
      "Iteration 24, loss = 2.75193453\n",
      "Iteration 25, loss = 2.55154862\n",
      "Iteration 26, loss = 2.50178447\n",
      "Iteration 55, loss = 2.56836302\n",
      "Iteration 56, loss = 2.01249192\n",
      "Iteration 57, loss = 1.18720132\n",
      "Iteration 27, loss = 3.22111684\n",
      "Iteration 58, loss = 1.46789961\n",
      "Iteration 28, loss = 2.68346873\n",
      "Iteration 59, loss = 1.33214491\n",
      "Iteration 29, loss = 2.88538226\n",
      "Iteration 30, loss = 2.35156573\n",
      "Iteration 60, loss = 0.90130627\n",
      "Iteration 61, loss = 1.00570256\n",
      "Iteration 62, loss = 0.87326439\n",
      "Iteration 31, loss = 2.48776758\n",
      "Iteration 63, loss = 1.25112140\n",
      "Iteration 32, loss = 3.48600232\n",
      "Iteration 64, loss = 0.94224751\n",
      "Iteration 33, loss = 2.69410354\n",
      "Iteration 65, loss = 1.09647276\n",
      "Iteration 34, loss = 3.09284731\n",
      "Iteration 66, loss = 1.62349670\n",
      "Iteration 35, loss = 2.46200736\n",
      "Iteration 67, loss = 1.43406436\n",
      "Iteration 36, loss = 2.80587616\n",
      "Iteration 68, loss = 1.14965633\n",
      "Iteration 69, loss = 1.04291651\n",
      "Iteration 37, loss = 2.78441784\n",
      "Iteration 70, loss = 1.43128888\n",
      "Iteration 71, loss = 1.43162601\n",
      "Iteration 72, loss = 1.22835900\n",
      "Iteration 38, loss = 2.47502789\n",
      "Iteration 73, loss = 0.77880269\n",
      "Iteration 39, loss = 3.08817437\n",
      "Iteration 40, loss = 2.66042171\n",
      "Iteration 41, loss = 2.90994208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 74, loss = 0.77941040\n",
      "Iteration 75, loss = 0.99221604\n",
      "Iteration 76, loss = 1.07057772\n",
      "Iteration 77, loss = 1.06903910\n",
      "Iteration 78, loss = 0.76809410\n",
      "Iteration 79, loss = 0.66292343\n",
      "Iteration 80, loss = 1.02577761\n",
      "Iteration 81, loss = 0.72510644\n",
      "Iteration 82, loss = 0.79962474\n",
      "Iteration 1, loss = 16.36631030\n",
      "Iteration 83, loss = 0.47413541\n",
      "Iteration 84, loss = 0.53887056\n",
      "Iteration 2, loss = 14.64423264\n",
      "Iteration 85, loss = 0.58995730\n",
      "Iteration 3, loss = 13.01075389\n",
      "Iteration 4, loss = 10.07937993\n",
      "Iteration 86, loss = 0.49873037\n",
      "Iteration 5, loss = 7.65908359\n",
      "Iteration 6, loss = 9.54457078\n",
      "Iteration 87, loss = 0.56111874\n",
      "Iteration 88, loss = 0.53673069\n",
      "Iteration 89, loss = 1.13605245\n",
      "Iteration 7, loss = 6.31772193\n",
      "Iteration 90, loss = 1.00038937\n",
      "Iteration 8, loss = 6.70995293\n",
      "Iteration 9, loss = 6.72315979\n",
      "Iteration 10, loss = 6.09963716\n",
      "Iteration 91, loss = 0.88550416\n",
      "Iteration 92, loss = 0.55457230\n",
      "Iteration 11, loss = 5.04114948\n",
      "Iteration 93, loss = 1.20103489\n",
      "Iteration 94, loss = 0.98955389\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 5.19086186\n",
      "Iteration 13, loss = 5.80957056\n",
      "Iteration 14, loss = 7.39736974\n",
      "Iteration 15, loss = 5.38672511\n",
      "Iteration 16, loss = 4.67507928\n",
      "Iteration 17, loss = 4.21094154\n",
      "Iteration 18, loss = 3.94397264\n",
      "Iteration 19, loss = 3.42678242\n",
      "Iteration 20, loss = 3.05785027\n",
      "Iteration 21, loss = 4.21857887\n",
      "Iteration 22, loss = 3.46414064\n",
      "Iteration 23, loss = 4.14249693\n",
      "Iteration 24, loss = 3.69479253\n",
      "Iteration 25, loss = 2.80944540\n",
      "Iteration 26, loss = 2.31884697\n",
      "Iteration 1, loss = 17.81159297\n",
      "Iteration 27, loss = 2.65383623\n",
      "Iteration 2, loss = 16.62255387\n",
      "Iteration 28, loss = 2.24224420\n",
      "Iteration 3, loss = 11.72649758\n",
      "Iteration 4, loss = 9.98487511\n",
      "Iteration 29, loss = 2.99004651\n",
      "Iteration 5, loss = 9.12312726\n",
      "Iteration 30, loss = 3.21760581\n",
      "Iteration 31, loss = 3.17953114\n",
      "Iteration 6, loss = 6.62241259\n",
      "Iteration 32, loss = 2.90241972\n",
      "Iteration 7, loss = 7.22195946\n",
      "Iteration 33, loss = 3.07119987\n",
      "Iteration 8, loss = 7.54427416\n",
      "Iteration 34, loss = 3.20058833\n",
      "Iteration 9, loss = 6.03395977\n",
      "Iteration 35, loss = 1.86978148\n",
      "Iteration 10, loss = 6.37766492\n",
      "Iteration 36, loss = 2.27699253\n",
      "Iteration 11, loss = 4.77335904\n",
      "Iteration 37, loss = 2.50857537\n",
      "Iteration 12, loss = 4.98908376\n",
      "Iteration 38, loss = 2.61548755\n",
      "Iteration 13, loss = 4.86445866\n",
      "Iteration 39, loss = 2.32887256\n",
      "Iteration 14, loss = 5.36250585\n",
      "Iteration 40, loss = 2.45208239\n",
      "Iteration 41, loss = 2.93793242\n",
      "Iteration 42, loss = 3.14012838\n",
      "Iteration 15, loss = 6.21703420\n",
      "Iteration 43, loss = 2.40354378\n",
      "Iteration 16, loss = 5.27111901\n",
      "Iteration 17, loss = 4.42382822\n",
      "Iteration 44, loss = 3.07770000\n",
      "Iteration 45, loss = 2.33608575\n",
      "Iteration 18, loss = 4.56247349\n",
      "Iteration 46, loss = 1.93030722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 3.53284931\n",
      "Iteration 20, loss = 4.03086306\n",
      "Iteration 21, loss = 4.03431769\n",
      "Iteration 22, loss = 4.16789449\n",
      "Iteration 23, loss = 4.15099872\n",
      "Iteration 24, loss = 4.58847323\n",
      "Iteration 25, loss = 3.48349833\n",
      "Iteration 26, loss = 3.43130949\n",
      "Iteration 1, loss = 20.68733318\n",
      "Iteration 2, loss = 15.70148508\n",
      "Iteration 27, loss = 2.80088622\n",
      "Iteration 28, loss = 2.70577030\n",
      "Iteration 3, loss = 14.88680410\n",
      "Iteration 29, loss = 3.21006064\n",
      "Iteration 4, loss = 8.69858778\n",
      "Iteration 5, loss = 8.51059941\n",
      "Iteration 30, loss = 3.33370397\n",
      "Iteration 31, loss = 2.69569097\n",
      "Iteration 6, loss = 8.87161885\n",
      "Iteration 32, loss = 2.88770089\n",
      "Iteration 7, loss = 7.72136617\n",
      "Iteration 33, loss = 2.37976649\n",
      "Iteration 8, loss = 6.97337821\n",
      "Iteration 9, loss = 7.25035413\n",
      "Iteration 34, loss = 3.50020685\n",
      "Iteration 10, loss = 5.84966687\n",
      "Iteration 35, loss = 3.35370490\n",
      "Iteration 36, loss = 3.79157870\n",
      "Iteration 11, loss = 4.56632209\n",
      "Iteration 37, loss = 3.16890624\n",
      "Iteration 12, loss = 5.54445734\n",
      "Iteration 38, loss = 2.59705681\n",
      "Iteration 13, loss = 4.97507263\n",
      "Iteration 39, loss = 2.28440312\n",
      "Iteration 14, loss = 4.61434688\n",
      "Iteration 40, loss = 2.27466417\n",
      "Iteration 15, loss = 5.29366551\n",
      "Iteration 41, loss = 2.70250812\n",
      "Iteration 16, loss = 4.45874552\n",
      "Iteration 17, loss = 4.66794037\n",
      "Iteration 42, loss = 2.37749791\n",
      "Iteration 18, loss = 3.86779117\n",
      "Iteration 43, loss = 2.16540088\n",
      "Iteration 19, loss = 4.00840117\n",
      "Iteration 20, loss = 4.00358897\n",
      "Iteration 44, loss = 2.10206114\n",
      "Iteration 21, loss = 2.74957977\n",
      "Iteration 22, loss = 2.92031033\n",
      "Iteration 45, loss = 2.19928423\n",
      "Iteration 23, loss = 2.87509700\n",
      "Iteration 46, loss = 3.39649513\n",
      "Iteration 24, loss = 2.52737054\n",
      "Iteration 47, loss = 2.07951218\n",
      "Iteration 25, loss = 2.82346355\n",
      "Iteration 48, loss = 2.90818790\n",
      "Iteration 26, loss = 2.24941262\n",
      "Iteration 27, loss = 3.31116835\n",
      "Iteration 28, loss = 2.97681410\n",
      "Iteration 29, loss = 3.77799942\n",
      "Iteration 49, loss = 3.13349773\n",
      "Iteration 30, loss = 3.39914325\n",
      "Iteration 31, loss = 2.74191266\n",
      "Iteration 32, loss = 2.34849577\n",
      "Iteration 50, loss = 3.22143294\n",
      "Iteration 33, loss = 2.77856424\n",
      "Iteration 34, loss = 2.01382166\n",
      "Iteration 51, loss = 3.55485889\n",
      "Iteration 52, loss = 2.68818292\n",
      "Iteration 53, loss = 2.60568956\n",
      "Iteration 54, loss = 2.20423565\n",
      "Iteration 55, loss = 2.38231439\n",
      "Iteration 35, loss = 2.54434352\n",
      "Iteration 36, loss = 2.69975168\n",
      "Iteration 56, loss = 1.93164178\n",
      "Iteration 37, loss = 2.64389546\n",
      "Iteration 57, loss = 2.81735786\n",
      "Iteration 58, loss = 3.08246108\n",
      "Iteration 38, loss = 2.00582016\n",
      "Iteration 39, loss = 2.17161510\n",
      "Iteration 59, loss = 3.11842641\n",
      "Iteration 40, loss = 2.21647188\n",
      "Iteration 60, loss = 2.94304787\n",
      "Iteration 41, loss = 2.14625099\n",
      "Iteration 61, loss = 2.32584424\n",
      "Iteration 62, loss = 1.80967983\n",
      "Iteration 42, loss = 2.72541268\n",
      "Iteration 43, loss = 2.50481494\n",
      "Iteration 63, loss = 2.32292139\n",
      "Iteration 44, loss = 2.24103077\n",
      "Iteration 64, loss = 1.82413443\n",
      "Iteration 45, loss = 2.37323626\n",
      "Iteration 65, loss = 1.73479270\n",
      "Iteration 66, loss = 2.44547708\n",
      "Iteration 46, loss = 3.21746528\n",
      "Iteration 67, loss = 2.18305109\n",
      "Iteration 47, loss = 3.50215234\n",
      "Iteration 48, loss = 2.52307447\n",
      "Iteration 68, loss = 2.25351978\n",
      "Iteration 49, loss = 2.67810870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 69, loss = 1.44367009\n",
      "Iteration 70, loss = 1.51503526\n",
      "Iteration 71, loss = 1.89655962\n",
      "Iteration 72, loss = 2.12727442\n",
      "Iteration 73, loss = 2.26377065\n",
      "Iteration 74, loss = 2.51889569\n",
      "Iteration 75, loss = 2.46294280\n",
      "Iteration 1, loss = 17.00953750\n",
      "Iteration 2, loss = 13.75153898\n",
      "Iteration 76, loss = 2.90690877\n",
      "Iteration 3, loss = 10.58427678\n",
      "Iteration 77, loss = 2.58414166\n",
      "Iteration 4, loss = 8.75432521\n",
      "Iteration 78, loss = 2.13504083\n",
      "Iteration 5, loss = 7.57853227\n",
      "Iteration 6, loss = 6.34386454\n",
      "Iteration 7, loss = 9.38069821\n",
      "Iteration 79, loss = 1.94596968\n",
      "Iteration 8, loss = 6.63371688\n",
      "Iteration 9, loss = 9.22998189\n",
      "Iteration 80, loss = 1.65601217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 5.72740026\n",
      "Iteration 11, loss = 6.21982120\n",
      "Iteration 12, loss = 6.18489705\n",
      "Iteration 13, loss = 5.29805410\n",
      "Iteration 14, loss = 5.68317790\n",
      "Iteration 15, loss = 5.05967169\n",
      "Iteration 16, loss = 4.59622049\n",
      "Iteration 17, loss = 4.81490639\n",
      "Iteration 18, loss = 4.94557177\n",
      "Iteration 19, loss = 3.66439059\n",
      "Iteration 20, loss = 4.07234745\n",
      "Iteration 21, loss = 3.65541611\n",
      "Iteration 1, loss = 13.65175890\n",
      "Iteration 22, loss = 3.72851764\n",
      "Iteration 2, loss = 14.76394434\n",
      "Iteration 23, loss = 3.28644260\n",
      "Iteration 3, loss = 13.52008511\n",
      "Iteration 24, loss = 3.00964445\n",
      "Iteration 25, loss = 2.81232844\n",
      "Iteration 4, loss = 12.32593555\n",
      "Iteration 26, loss = 2.96301441\n",
      "Iteration 27, loss = 2.80607151\n",
      "Iteration 5, loss = 10.64303543\n",
      "Iteration 28, loss = 3.52156527\n",
      "Iteration 6, loss = 10.61707934\n",
      "Iteration 29, loss = 2.34921041\n",
      "Iteration 7, loss = 11.14356349\n",
      "Iteration 30, loss = 3.17548933\n",
      "Iteration 8, loss = 6.64094471\n",
      "Iteration 31, loss = 2.25662425\n",
      "Iteration 32, loss = 3.19364944\n",
      "Iteration 33, loss = 3.59057688\n",
      "Iteration 9, loss = 6.17172897\n",
      "Iteration 34, loss = 3.80615554\n",
      "Iteration 10, loss = 5.16934142\n",
      "Iteration 35, loss = 2.88717269\n",
      "Iteration 11, loss = 4.92642531\n",
      "Iteration 36, loss = 2.85594413\n",
      "Iteration 37, loss = 2.75667889\n",
      "Iteration 12, loss = 5.02382559\n",
      "Iteration 38, loss = 3.64352774\n",
      "Iteration 39, loss = 3.16238595\n",
      "Iteration 40, loss = 2.60279938\n",
      "Iteration 13, loss = 4.33105361\n",
      "Iteration 41, loss = 1.96405549\n",
      "Iteration 14, loss = 4.54819693\n",
      "Iteration 42, loss = 2.04601610\n",
      "Iteration 15, loss = 4.22192469\n",
      "Iteration 43, loss = 1.91393403\n",
      "Iteration 44, loss = 2.23762044\n",
      "Iteration 16, loss = 3.49997970\n",
      "Iteration 45, loss = 1.78052346\n",
      "Iteration 46, loss = 1.48466619\n",
      "Iteration 17, loss = 5.10091359\n",
      "Iteration 47, loss = 1.70464803\n",
      "Iteration 18, loss = 4.78936402\n",
      "Iteration 48, loss = 1.70410804\n",
      "Iteration 49, loss = 1.54193197\n",
      "Iteration 50, loss = 1.69109999\n",
      "Iteration 19, loss = 4.82529485\n",
      "Iteration 51, loss = 2.08636052\n",
      "Iteration 20, loss = 4.31554949\n",
      "Iteration 52, loss = 1.97269096\n",
      "Iteration 53, loss = 1.80703830\n",
      "Iteration 21, loss = 5.34899347\n",
      "Iteration 54, loss = 2.09704491\n",
      "Iteration 22, loss = 3.85306862\n",
      "Iteration 55, loss = 1.33088455\n",
      "Iteration 23, loss = 4.46398633\n",
      "Iteration 24, loss = 4.62480717\n",
      "Iteration 56, loss = 1.53418284\n",
      "Iteration 25, loss = 4.46360978\n",
      "Iteration 26, loss = 4.33633856\n",
      "Iteration 27, loss = 3.54391972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 2.07121250\n",
      "Iteration 58, loss = 1.63873497\n",
      "Iteration 59, loss = 1.46670217\n",
      "Iteration 60, loss = 1.87180496\n",
      "Iteration 61, loss = 1.79306574\n",
      "Iteration 62, loss = 1.56823033\n",
      "Iteration 63, loss = 2.03359994\n",
      "Iteration 1, loss = 17.31356336\n",
      "Iteration 64, loss = 1.81343553\n",
      "Iteration 65, loss = 1.53308634\n",
      "Iteration 2, loss = 16.87990450\n",
      "Iteration 66, loss = 2.10734265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 9.12148781\n",
      "Iteration 4, loss = 8.93523545\n",
      "Iteration 5, loss = 7.76911224\n",
      "Iteration 6, loss = 8.41646324\n",
      "Iteration 7, loss = 5.81834770\n",
      "Iteration 1, loss = 15.59389192\n",
      "Iteration 2, loss = 13.51169446\n",
      "Iteration 8, loss = 4.54254296\n",
      "Iteration 3, loss = 17.05458393\n",
      "Iteration 9, loss = 6.61508434\n",
      "Iteration 4, loss = 8.00731091\n",
      "Iteration 10, loss = 8.03966347\n",
      "Iteration 5, loss = 6.60982176\n",
      "Iteration 11, loss = 7.74359482\n",
      "Iteration 12, loss = 5.48847117\n",
      "Iteration 6, loss = 11.13573186\n",
      "Iteration 13, loss = 5.15967449\n",
      "Iteration 7, loss = 6.48245511\n",
      "Iteration 14, loss = 4.96528648\n",
      "Iteration 8, loss = 5.75039929\n",
      "Iteration 15, loss = 5.06343900\n",
      "Iteration 9, loss = 4.89906644\n",
      "Iteration 16, loss = 3.71310621\n",
      "Iteration 10, loss = 4.49698404\n",
      "Iteration 11, loss = 4.10873474\n",
      "Iteration 17, loss = 3.19385105\n",
      "Iteration 12, loss = 4.29381215\n",
      "Iteration 13, loss = 3.96525022\n",
      "Iteration 14, loss = 5.24100606\n",
      "Iteration 18, loss = 3.82947302\n",
      "Iteration 15, loss = 7.09041579\n",
      "Iteration 19, loss = 3.55619255\n",
      "Iteration 20, loss = 3.51134392\n",
      "Iteration 16, loss = 4.42432829\n",
      "Iteration 21, loss = 3.32390100\n",
      "Iteration 22, loss = 3.71576527\n",
      "Iteration 17, loss = 4.23313994\n",
      "Iteration 23, loss = 3.40026587\n",
      "Iteration 18, loss = 4.30158029\n",
      "Iteration 24, loss = 2.89489596\n",
      "Iteration 19, loss = 4.07325658\n",
      "Iteration 25, loss = 3.37339587\n",
      "Iteration 20, loss = 3.46229967\n",
      "Iteration 26, loss = 2.64675368\n",
      "Iteration 21, loss = 2.80672017\n",
      "Iteration 27, loss = 3.30657240\n",
      "Iteration 22, loss = 2.70473287\n",
      "Iteration 28, loss = 3.52986759\n",
      "Iteration 23, loss = 2.59614833\n",
      "Iteration 29, loss = 4.25724663\n",
      "Iteration 24, loss = 3.07245108\n",
      "Iteration 30, loss = 3.30393336\n",
      "Iteration 25, loss = 3.05506859\n",
      "Iteration 26, loss = 3.14491930\n",
      "Iteration 31, loss = 2.65648948\n",
      "Iteration 27, loss = 3.03149464\n",
      "Iteration 32, loss = 2.35869570\n",
      "Iteration 28, loss = 3.00329910\n",
      "Iteration 33, loss = 2.20439471\n",
      "Iteration 29, loss = 3.49185063\n",
      "Iteration 34, loss = 2.85699846\n",
      "Iteration 30, loss = 3.43326237\n",
      "Iteration 35, loss = 2.07069867\n",
      "Iteration 31, loss = 2.61157947\n",
      "Iteration 36, loss = 1.91454742\n",
      "Iteration 32, loss = 2.27140414\n",
      "Iteration 33, loss = 2.27798604\n",
      "Iteration 34, loss = 2.17188333\n",
      "Iteration 37, loss = 1.79682892\n",
      "Iteration 35, loss = 2.58266821\n",
      "Iteration 38, loss = 3.09745428\n",
      "Iteration 36, loss = 2.19818057\n",
      "Iteration 39, loss = 2.98609178\n",
      "Iteration 37, loss = 1.99012991\n",
      "Iteration 40, loss = 1.98500071\n",
      "Iteration 38, loss = 1.76647805\n",
      "Iteration 39, loss = 1.92624543\n",
      "Iteration 41, loss = 1.89577680\n",
      "Iteration 40, loss = 2.21970551\n",
      "Iteration 42, loss = 2.15061186\n",
      "Iteration 41, loss = 2.63755407\n",
      "Iteration 43, loss = 1.97859945\n",
      "Iteration 42, loss = 2.94073576\n",
      "Iteration 43, loss = 2.78218792\n",
      "Iteration 44, loss = 2.52845580\n",
      "Iteration 44, loss = 1.62982022\n",
      "Iteration 45, loss = 2.80301088\n",
      "Iteration 45, loss = 3.66581478\n",
      "Iteration 46, loss = 3.14978134\n",
      "Iteration 46, loss = 2.30590404\n",
      "Iteration 47, loss = 3.24112867\n",
      "Iteration 47, loss = 1.99243858\n",
      "Iteration 48, loss = 3.44644134\n",
      "Iteration 48, loss = 2.05259437\n",
      "Iteration 49, loss = 2.82222742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 2.48595322\n",
      "Iteration 50, loss = 2.67305661\n",
      "Iteration 51, loss = 1.86451124\n",
      "Iteration 52, loss = 2.06358479\n",
      "Iteration 53, loss = 2.05769838\n",
      "Iteration 54, loss = 1.79452412\n",
      "Iteration 55, loss = 1.96522415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.86816928\n",
      "Iteration 2, loss = 15.96460723\n",
      "Iteration 3, loss = 11.96862840\n",
      "Iteration 1, loss = 17.43924323\n",
      "Iteration 2, loss = 14.27523825\n",
      "Iteration 4, loss = 10.32352997\n",
      "Iteration 3, loss = 11.67404893\n",
      "Iteration 4, loss = 8.78095600\n",
      "Iteration 5, loss = 6.63293783\n",
      "Iteration 6, loss = 6.50911519\n",
      "Iteration 5, loss = 7.87439655\n",
      "Iteration 7, loss = 6.19447428\n",
      "Iteration 6, loss = 9.48343027\n",
      "Iteration 8, loss = 5.47865507\n",
      "Iteration 7, loss = 6.64046886\n",
      "Iteration 8, loss = 7.10553122\n",
      "Iteration 9, loss = 10.41691329\n",
      "Iteration 10, loss = 8.72114456\n",
      "Iteration 9, loss = 8.46391796\n",
      "Iteration 11, loss = 7.77269228\n",
      "Iteration 12, loss = 6.81618336\n",
      "Iteration 13, loss = 5.94832502\n",
      "Iteration 10, loss = 6.80101353\n",
      "Iteration 14, loss = 9.10077242\n",
      "Iteration 15, loss = 7.95451886\n",
      "Iteration 16, loss = 7.01582107\n",
      "Iteration 17, loss = 7.21552041\n",
      "Iteration 11, loss = 8.89339337\n",
      "Iteration 18, loss = 6.19622008\n",
      "Iteration 12, loss = 5.94058217\n",
      "Iteration 13, loss = 4.89041455\n",
      "Iteration 19, loss = 9.48182546\n",
      "Iteration 14, loss = 4.28578927\n",
      "Iteration 20, loss = 8.60437626\n",
      "Iteration 15, loss = 3.62342393\n",
      "Iteration 21, loss = 5.32221109\n",
      "Iteration 22, loss = 4.79997504\n",
      "Iteration 23, loss = 4.27673124\n",
      "Iteration 16, loss = 3.29854019\n",
      "Iteration 24, loss = 4.11168687\n",
      "Iteration 25, loss = 3.74509938\n",
      "Iteration 17, loss = 2.75800507\n",
      "Iteration 26, loss = 4.04370500\n",
      "Iteration 18, loss = 3.18054272\n",
      "Iteration 27, loss = 3.96202725\n",
      "Iteration 19, loss = 3.86373939\n",
      "Iteration 28, loss = 3.24006173\n",
      "Iteration 20, loss = 4.47253163\n",
      "Iteration 21, loss = 3.49374910\n",
      "Iteration 29, loss = 2.98661821\n",
      "Iteration 22, loss = 3.52393091\n",
      "Iteration 30, loss = 2.78090536\n",
      "Iteration 23, loss = 2.73516847\n",
      "Iteration 31, loss = 2.72384387\n",
      "Iteration 24, loss = 3.35860469\n",
      "Iteration 32, loss = 2.40949822\n",
      "Iteration 33, loss = 4.24226487\n",
      "Iteration 34, loss = 2.40202216\n",
      "Iteration 25, loss = 2.51002941\n",
      "Iteration 26, loss = 2.43096355\n",
      "Iteration 35, loss = 3.47725342Iteration 27, loss = 2.11280492\n",
      "\n",
      "Iteration 28, loss = 2.35935540\n",
      "Iteration 36, loss = 2.66995383\n",
      "Iteration 37, loss = 2.47684841\n",
      "Iteration 29, loss = 2.62606914\n",
      "Iteration 38, loss = 2.09417431\n",
      "Iteration 30, loss = 2.15111887\n",
      "Iteration 31, loss = 2.11469362\n",
      "Iteration 39, loss = 2.56961671\n",
      "Iteration 32, loss = 1.63930365\n",
      "Iteration 40, loss = 2.60602671\n",
      "Iteration 33, loss = 1.53782813\n",
      "Iteration 41, loss = 2.50864546\n",
      "Iteration 34, loss = 1.78737619\n",
      "Iteration 42, loss = 2.58842549\n",
      "Iteration 35, loss = 1.82906300\n",
      "Iteration 43, loss = 2.56534791\n",
      "Iteration 36, loss = 2.00369492\n",
      "Iteration 44, loss = 2.26143361\n",
      "Iteration 37, loss = 1.79224243\n",
      "Iteration 45, loss = 1.95244181\n",
      "Iteration 38, loss = 1.84452157\n",
      "Iteration 39, loss = 1.66646018\n",
      "Iteration 46, loss = 2.33738067\n",
      "Iteration 40, loss = 1.79918156\n",
      "Iteration 47, loss = 2.13279678\n",
      "Iteration 48, loss = 2.11478651\n",
      "Iteration 49, loss = 1.91356758\n",
      "Iteration 41, loss = 2.09453425\n",
      "Iteration 50, loss = 1.57145370\n",
      "Iteration 51, loss = 1.43615070\n",
      "Iteration 52, loss = 1.72777607\n",
      "Iteration 42, loss = 2.20674266\n",
      "Iteration 53, loss = 1.67861204\n",
      "Iteration 54, loss = 2.14670927\n",
      "Iteration 43, loss = 1.66999244\n",
      "Iteration 55, loss = 2.29309959\n",
      "Iteration 44, loss = 1.57580196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 2.74417353\n",
      "Iteration 57, loss = 2.00948560\n",
      "Iteration 58, loss = 2.14348586\n",
      "Iteration 59, loss = 3.10783840\n",
      "Iteration 60, loss = 3.08302621\n",
      "Iteration 61, loss = 2.71600538\n",
      "Iteration 62, loss = 2.27182193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.47903990\n",
      "Iteration 2, loss = 14.53667248\n",
      "Iteration 3, loss = 11.40648503\n",
      "Iteration 4, loss = 10.08788656\n",
      "Iteration 5, loss = 7.27882938\n",
      "Iteration 1, loss = 14.86629771\n",
      "Iteration 6, loss = 7.33188677\n",
      "Iteration 2, loss = 15.61417276\n",
      "Iteration 7, loss = 6.24586743\n",
      "Iteration 8, loss = 7.99279364\n",
      "Iteration 9, loss = 5.74302675\n",
      "Iteration 3, loss = 11.22440824\n",
      "Iteration 10, loss = 3.91225277\n",
      "Iteration 4, loss = 7.21821527\n",
      "Iteration 5, loss = 7.83636818\n",
      "Iteration 11, loss = 5.49514908\n",
      "Iteration 12, loss = 4.28025854\n",
      "Iteration 6, loss = 8.92642209\n",
      "Iteration 13, loss = 4.30122012\n",
      "Iteration 7, loss = 6.62230656\n",
      "Iteration 14, loss = 3.92698607\n",
      "Iteration 8, loss = 8.42421303\n",
      "Iteration 15, loss = 3.65396541\n",
      "Iteration 9, loss = 7.16377751\n",
      "Iteration 16, loss = 4.15451625\n",
      "Iteration 17, loss = 5.61109896\n",
      "Iteration 10, loss = 5.91937768\n",
      "Iteration 18, loss = 4.00648272\n",
      "Iteration 11, loss = 6.08355349\n",
      "Iteration 19, loss = 3.05187314\n",
      "Iteration 12, loss = 4.91024219\n",
      "Iteration 13, loss = 4.72275999\n",
      "Iteration 20, loss = 3.41564662\n",
      "Iteration 14, loss = 4.80477979\n",
      "Iteration 21, loss = 3.50110420\n",
      "Iteration 22, loss = 4.20954047\n",
      "Iteration 15, loss = 8.40823979\n",
      "Iteration 16, loss = 6.00122723\n",
      "Iteration 23, loss = 3.48433028\n",
      "Iteration 17, loss = 5.39064502\n",
      "Iteration 18, loss = 6.20592175\n",
      "Iteration 24, loss = 2.94815031\n",
      "Iteration 19, loss = 5.06063063\n",
      "Iteration 25, loss = 3.12992981\n",
      "Iteration 26, loss = 2.98596054\n",
      "Iteration 20, loss = 4.19834217\n",
      "Iteration 27, loss = 3.24236655\n",
      "Iteration 21, loss = 3.88833416\n",
      "Iteration 22, loss = 3.61725958\n",
      "Iteration 28, loss = 2.86194204\n",
      "Iteration 23, loss = 3.20780841Iteration 29, loss = 4.66506105\n",
      "\n",
      "Iteration 30, loss = 3.85251713\n",
      "Iteration 31, loss = 3.28637019\n",
      "Iteration 24, loss = 3.20274895\n",
      "Iteration 25, loss = 4.32273381\n",
      "Iteration 32, loss = 2.95076340\n",
      "Iteration 26, loss = 3.83802252\n",
      "Iteration 27, loss = 3.82926446\n",
      "Iteration 28, loss = 3.65679561\n",
      "Iteration 33, loss = 3.87074898\n",
      "Iteration 29, loss = 3.70344251\n",
      "Iteration 30, loss = 2.60083094\n",
      "Iteration 34, loss = 3.01323166\n",
      "Iteration 31, loss = 2.62834201\n",
      "Iteration 32, loss = 1.99936542\n",
      "Iteration 33, loss = 2.24411674\n",
      "Iteration 35, loss = 3.40690580\n",
      "Iteration 36, loss = 3.79380439\n",
      "Iteration 34, loss = 1.83158090\n",
      "Iteration 37, loss = 2.91617072\n",
      "Iteration 35, loss = 2.74958931\n",
      "Iteration 36, loss = 2.35883042\n",
      "Iteration 38, loss = 2.24281233\n",
      "Iteration 37, loss = 2.15939785\n",
      "Iteration 39, loss = 2.01990882\n",
      "Iteration 40, loss = 2.05206932\n",
      "Iteration 38, loss = 2.10595023\n",
      "Iteration 41, loss = 3.16340310\n",
      "Iteration 42, loss = 3.13366485\n",
      "Iteration 39, loss = 1.81352993\n",
      "Iteration 40, loss = 1.52243110\n",
      "Iteration 43, loss = 2.33131318\n",
      "Iteration 41, loss = 1.27804704\n",
      "Iteration 44, loss = 2.43414768\n",
      "Iteration 42, loss = 1.81879091\n",
      "Iteration 45, loss = 4.49131084\n",
      "Iteration 43, loss = 1.46855888\n",
      "Iteration 46, loss = 3.84695696\n",
      "Iteration 44, loss = 2.27889876\n",
      "Iteration 47, loss = 5.47271390\n",
      "Iteration 48, loss = 2.85801253\n",
      "Iteration 49, loss = 3.72896836\n",
      "Iteration 45, loss = 2.17932689\n",
      "Iteration 50, loss = 4.23719853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 2.21198980\n",
      "Iteration 47, loss = 1.92405095\n",
      "Iteration 48, loss = 2.00686868\n",
      "Iteration 49, loss = 2.01350633\n",
      "Iteration 50, loss = 2.38387170\n",
      "Iteration 51, loss = 1.88030318\n",
      "Iteration 52, loss = 1.97687698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.62816682\n",
      "Iteration 1, loss = 17.12550767\n",
      "Iteration 2, loss = 15.05627781\n",
      "Iteration 2, loss = 12.77545817\n",
      "Iteration 3, loss = 14.98741979\n",
      "Iteration 3, loss = 14.10998326\n",
      "Iteration 4, loss = 13.03477633\n",
      "Iteration 4, loss = 11.45466274\n",
      "Iteration 5, loss = 7.23634842\n",
      "Iteration 6, loss = 8.50914924\n",
      "Iteration 5, loss = 10.41419882\n",
      "Iteration 7, loss = 8.51215605\n",
      "Iteration 6, loss = 8.71095942\n",
      "Iteration 8, loss = 7.63381925\n",
      "Iteration 7, loss = 6.93935357\n",
      "Iteration 9, loss = 7.48690276\n",
      "Iteration 8, loss = 7.92947909\n",
      "Iteration 9, loss = 7.62844989\n",
      "Iteration 10, loss = 5.61091341\n",
      "Iteration 10, loss = 6.01832116\n",
      "Iteration 11, loss = 7.47532309\n",
      "Iteration 11, loss = 6.43929443\n",
      "Iteration 12, loss = 5.32943653\n",
      "Iteration 13, loss = 4.69210394\n",
      "Iteration 12, loss = 6.99616681\n",
      "Iteration 13, loss = 5.05729917\n",
      "Iteration 14, loss = 5.18765731\n",
      "Iteration 14, loss = 5.67323238\n",
      "Iteration 15, loss = 4.90903608\n",
      "Iteration 15, loss = 4.78365482\n",
      "Iteration 16, loss = 4.56014474\n",
      "Iteration 16, loss = 5.09318329\n",
      "Iteration 17, loss = 4.57485095\n",
      "Iteration 17, loss = 6.29288098\n",
      "Iteration 18, loss = 5.73018919\n",
      "Iteration 19, loss = 6.05587682\n",
      "Iteration 18, loss = 9.66067387\n",
      "Iteration 20, loss = 4.65921119\n",
      "Iteration 19, loss = 6.29509356\n",
      "Iteration 20, loss = 5.17269022\n",
      "Iteration 21, loss = 4.08294761\n",
      "Iteration 21, loss = 4.77479149\n",
      "Iteration 22, loss = 3.69547845\n",
      "Iteration 23, loss = 3.95682248\n",
      "Iteration 22, loss = 4.68992793\n",
      "Iteration 24, loss = 3.77327232\n",
      "Iteration 23, loss = 5.12494790\n",
      "Iteration 24, loss = 4.60823732\n",
      "Iteration 25, loss = 3.97337237\n",
      "Iteration 25, loss = 4.15178823\n",
      "Iteration 26, loss = 3.47870600\n",
      "Iteration 26, loss = 2.96690260\n",
      "Iteration 27, loss = 4.09378278\n",
      "Iteration 28, loss = 3.76845980\n",
      "Iteration 27, loss = 3.50757678\n",
      "Iteration 28, loss = 2.79663929\n",
      "Iteration 29, loss = 3.25405120\n",
      "Iteration 30, loss = 2.84801974\n",
      "Iteration 29, loss = 3.53766533\n",
      "Iteration 30, loss = 3.32580071\n",
      "Iteration 31, loss = 4.26600300\n",
      "Iteration 31, loss = 2.97513052\n",
      "Iteration 32, loss = 2.98264783\n",
      "Iteration 32, loss = 4.00239830\n",
      "Iteration 33, loss = 2.39818968\n",
      "Iteration 33, loss = 3.11145844\n",
      "Iteration 34, loss = 3.43239129\n",
      "Iteration 34, loss = 3.02078871\n",
      "Iteration 35, loss = 3.22994911\n",
      "Iteration 35, loss = 3.21375084\n",
      "Iteration 36, loss = 3.50751388\n",
      "Iteration 36, loss = 3.61558093\n",
      "Iteration 37, loss = 3.43425092\n",
      "Iteration 37, loss = 3.60371753\n",
      "Iteration 38, loss = 2.85629933\n",
      "Iteration 38, loss = 3.17700327\n",
      "Iteration 39, loss = 2.46397654\n",
      "Iteration 39, loss = 3.38238360\n",
      "Iteration 40, loss = 2.49531606\n",
      "Iteration 40, loss = 4.73217016\n",
      "Iteration 41, loss = 3.53442597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 3.01879838\n",
      "Iteration 42, loss = 2.85914411\n",
      "Iteration 43, loss = 2.94572666\n",
      "Iteration 44, loss = 2.73327963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 13.79602197\n",
      "Iteration 2, loss = 13.78052296\n",
      "Iteration 3, loss = 10.87125450\n",
      "Iteration 1, loss = 21.25022469\n",
      "Iteration 4, loss = 12.15315691\n",
      "Iteration 2, loss = 21.86081853\n",
      "Iteration 5, loss = 7.77100226\n",
      "Iteration 6, loss = 7.93894938\n",
      "Iteration 3, loss = 12.54073227\n",
      "Iteration 7, loss = 11.12626092\n",
      "Iteration 4, loss = 7.84732110\n",
      "Iteration 5, loss = 8.13333293\n",
      "Iteration 8, loss = 9.69023466\n",
      "Iteration 6, loss = 9.35968837\n",
      "Iteration 7, loss = 7.01627407\n",
      "Iteration 9, loss = 9.21785381\n",
      "Iteration 8, loss = 5.96793011\n",
      "Iteration 10, loss = 6.85189541\n",
      "Iteration 11, loss = 6.78869572\n",
      "Iteration 12, loss = 5.11207048\n",
      "Iteration 9, loss = 5.85989433\n",
      "Iteration 10, loss = 6.76959574\n",
      "Iteration 11, loss = 7.33995633\n",
      "Iteration 13, loss = 6.06568178\n",
      "Iteration 12, loss = 5.35847215\n",
      "Iteration 14, loss = 4.68529398\n",
      "Iteration 13, loss = 5.27673722\n",
      "Iteration 14, loss = 5.79402102\n",
      "Iteration 15, loss = 4.87047122\n",
      "Iteration 16, loss = 4.48501252\n",
      "Iteration 17, loss = 5.59462254\n",
      "Iteration 15, loss = 5.41134511\n",
      "Iteration 18, loss = 5.42046833\n",
      "Iteration 16, loss = 5.91753329\n",
      "Iteration 17, loss = 4.72771044\n",
      "Iteration 18, loss = 5.24664969\n",
      "Iteration 19, loss = 3.47716689\n",
      "Iteration 19, loss = 5.11790951\n",
      "Iteration 20, loss = 4.79706649\n",
      "Iteration 21, loss = 4.66764091\n",
      "Iteration 22, loss = 3.81617302\n",
      "Iteration 20, loss = 3.62737059\n",
      "Iteration 23, loss = 3.95784608\n",
      "Iteration 24, loss = 4.61314902\n",
      "Iteration 21, loss = 4.39004717\n",
      "Iteration 25, loss = 3.54413027\n",
      "Iteration 22, loss = 3.83015895\n",
      "Iteration 26, loss = 3.39402964\n",
      "Iteration 23, loss = 3.50524383\n",
      "Iteration 27, loss = 4.61424103\n",
      "Iteration 28, loss = 6.50451706\n",
      "Iteration 24, loss = 3.70602147\n",
      "Iteration 29, loss = 6.87680085\n",
      "Iteration 25, loss = 3.52295589\n",
      "Iteration 30, loss = 6.03640875\n",
      "Iteration 26, loss = 3.04420566\n",
      "Iteration 31, loss = 5.23096642\n",
      "Iteration 27, loss = 2.70683192\n",
      "Iteration 32, loss = 4.05085003\n",
      "Iteration 33, loss = 3.88359421\n",
      "Iteration 34, loss = 4.20966400\n",
      "Iteration 28, loss = 3.12615088\n",
      "Iteration 35, loss = 3.20722003\n",
      "Iteration 29, loss = 3.29534528\n",
      "Iteration 36, loss = 3.73915440\n",
      "Iteration 30, loss = 2.89658064\n",
      "Iteration 37, loss = 3.47919172\n",
      "Iteration 38, loss = 3.50424804\n",
      "Iteration 39, loss = 2.80221106\n",
      "Iteration 31, loss = 3.01504794\n",
      "Iteration 40, loss = 2.93292879\n",
      "Iteration 41, loss = 2.74489904\n",
      "Iteration 32, loss = 2.64075640\n",
      "Iteration 42, loss = 3.29265603\n",
      "Iteration 33, loss = 2.83818986\n",
      "Iteration 43, loss = 2.73602799\n",
      "Iteration 34, loss = 2.55583815\n",
      "Iteration 44, loss = 2.71259662\n",
      "Iteration 35, loss = 3.69988402\n",
      "Iteration 45, loss = 2.71717797\n",
      "Iteration 36, loss = 3.96195112\n",
      "Iteration 46, loss = 2.95104321\n",
      "Iteration 37, loss = 3.38163694\n",
      "Iteration 47, loss = 3.63575538\n",
      "Iteration 48, loss = 3.77212517\n",
      "Iteration 38, loss = 3.54279208\n",
      "Iteration 49, loss = 3.34449330\n",
      "Iteration 39, loss = 3.19640067\n",
      "Iteration 50, loss = 3.14172154\n",
      "Iteration 40, loss = 2.72895468\n",
      "Iteration 51, loss = 3.14660196\n",
      "Iteration 41, loss = 2.69294098\n",
      "Iteration 52, loss = 3.47227030\n",
      "Iteration 42, loss = 2.91745781\n",
      "Iteration 53, loss = 2.76591398\n",
      "Iteration 43, loss = 2.57297464\n",
      "Iteration 54, loss = 2.78639759\n",
      "Iteration 44, loss = 1.96312947\n",
      "Iteration 55, loss = 3.22615134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 2.49138633\n",
      "Iteration 46, loss = 2.16896226\n",
      "Iteration 47, loss = 2.84519441\n",
      "Iteration 48, loss = 3.20020049\n",
      "Iteration 49, loss = 2.37882375\n",
      "Iteration 50, loss = 1.94059148\n",
      "Iteration 51, loss = 2.74626418\n",
      "Iteration 1, loss = 19.21871670\n",
      "Iteration 52, loss = 2.63550485\n",
      "Iteration 2, loss = 15.03904159\n",
      "Iteration 53, loss = 2.26685525\n",
      "Iteration 54, loss = 2.03116761\n",
      "Iteration 3, loss = 11.32685411\n",
      "Iteration 55, loss = 2.03387185\n",
      "Iteration 56, loss = 2.50540383\n",
      "Iteration 4, loss = 9.45651632\n",
      "Iteration 57, loss = 2.02462384\n",
      "Iteration 5, loss = 7.19196732\n",
      "Iteration 58, loss = 2.24522902\n",
      "Iteration 6, loss = 8.61236598\n",
      "Iteration 59, loss = 2.11462235\n",
      "Iteration 7, loss = 7.60493993\n",
      "Iteration 8, loss = 9.00073582\n",
      "Iteration 60, loss = 2.17527522\n",
      "Iteration 9, loss = 5.86212575\n",
      "Iteration 61, loss = 2.54336250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 5.68127767\n",
      "Iteration 11, loss = 4.77135763\n",
      "Iteration 12, loss = 5.21038771\n",
      "Iteration 13, loss = 6.01241598\n",
      "Iteration 14, loss = 5.80932063\n",
      "Iteration 15, loss = 5.91751065\n",
      "Iteration 16, loss = 4.49360448\n",
      "Iteration 17, loss = 4.23800938\n",
      "Iteration 18, loss = 3.79327595\n",
      "Iteration 19, loss = 5.58597072\n",
      "Iteration 1, loss = 21.30250021\n",
      "Iteration 20, loss = 5.15073136\n",
      "Iteration 21, loss = 3.87705445\n",
      "Iteration 2, loss = 17.78084359\n",
      "Iteration 3, loss = 11.55585168\n",
      "Iteration 22, loss = 4.51649566\n",
      "Iteration 4, loss = 10.31724809\n",
      "Iteration 23, loss = 4.65663518\n",
      "Iteration 5, loss = 8.52892362\n",
      "Iteration 24, loss = 4.84572941\n",
      "Iteration 25, loss = 4.50135610\n",
      "Iteration 26, loss = 4.32550334\n",
      "Iteration 6, loss = 7.43494766\n",
      "Iteration 7, loss = 10.49545108\n",
      "Iteration 27, loss = 5.36242175\n",
      "Iteration 8, loss = 7.60025670\n",
      "Iteration 28, loss = 4.10096402\n",
      "Iteration 9, loss = 6.12375712\n",
      "Iteration 29, loss = 3.80386752\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 7.52447650\n",
      "Iteration 11, loss = 5.85206958\n",
      "Iteration 12, loss = 7.03465968\n",
      "Iteration 13, loss = 6.72016053\n",
      "Iteration 14, loss = 5.93938658\n",
      "Iteration 1, loss = 16.85742256\n",
      "Iteration 15, loss = 6.42170093\n",
      "Iteration 16, loss = 3.98122092\n",
      "Iteration 2, loss = 14.56823766\n",
      "Iteration 17, loss = 3.51453665\n",
      "Iteration 3, loss = 10.22841716\n",
      "Iteration 4, loss = 9.10304700\n",
      "Iteration 5, loss = 8.17330590\n",
      "Iteration 6, loss = 8.81254606\n",
      "Iteration 18, loss = 3.67077579\n",
      "Iteration 19, loss = 4.12166653\n",
      "Iteration 7, loss = 10.78443071\n",
      "Iteration 20, loss = 3.79873444\n",
      "Iteration 21, loss = 3.63696784\n",
      "Iteration 22, loss = 3.38667440\n",
      "Iteration 23, loss = 3.40164017\n",
      "Iteration 8, loss = 7.65365684\n",
      "Iteration 24, loss = 4.47459562\n",
      "Iteration 25, loss = 4.24232892\n",
      "Iteration 9, loss = 8.10010669\n",
      "Iteration 26, loss = 2.65586724\n",
      "Iteration 10, loss = 7.50950848\n",
      "Iteration 27, loss = 3.11306521\n",
      "Iteration 11, loss = 5.08589418\n",
      "Iteration 28, loss = 2.56021248\n",
      "Iteration 12, loss = 5.11706905\n",
      "Iteration 29, loss = 2.95632409\n",
      "Iteration 13, loss = 5.15714720\n",
      "Iteration 30, loss = 2.98964628\n",
      "Iteration 14, loss = 7.09072366\n",
      "Iteration 31, loss = 2.37505437\n",
      "Iteration 15, loss = 5.26769777\n",
      "Iteration 32, loss = 2.59351508\n",
      "Iteration 16, loss = 5.36767276\n",
      "Iteration 33, loss = 3.73703377\n",
      "Iteration 17, loss = 4.01517580\n",
      "Iteration 34, loss = 3.22577309\n",
      "Iteration 18, loss = 3.47470671\n",
      "Iteration 19, loss = 3.61802371\n",
      "Iteration 35, loss = 4.60063456\n",
      "Iteration 20, loss = 3.13258389\n",
      "Iteration 36, loss = 2.92630197\n",
      "Iteration 21, loss = 3.31062531\n",
      "Iteration 37, loss = 2.54314951\n",
      "Iteration 38, loss = 2.46551402\n",
      "Iteration 22, loss = 3.96268500\n",
      "Iteration 39, loss = 2.33028635\n",
      "Iteration 23, loss = 2.83009164\n",
      "Iteration 24, loss = 2.82875340\n",
      "Iteration 40, loss = 3.18331165\n",
      "Iteration 25, loss = 3.49350298\n",
      "Iteration 26, loss = 3.48959549\n",
      "Iteration 41, loss = 2.12688389\n",
      "Iteration 42, loss = 2.10245961\n",
      "Iteration 43, loss = 2.28051180\n",
      "Iteration 27, loss = 3.50344334\n",
      "Iteration 28, loss = 3.65168008\n",
      "Iteration 44, loss = 3.03076690\n",
      "Iteration 29, loss = 2.93788001\n",
      "Iteration 45, loss = 3.17965888\n",
      "Iteration 30, loss = 3.28623641\n",
      "Iteration 46, loss = 2.38414402\n",
      "Iteration 31, loss = 3.09576265\n",
      "Iteration 47, loss = 2.80613554\n",
      "Iteration 32, loss = 3.63637285\n",
      "Iteration 33, loss = 3.09316055\n",
      "Iteration 48, loss = 2.24226231\n",
      "Iteration 34, loss = 3.60013771\n",
      "Iteration 49, loss = 2.38787420\n",
      "Iteration 50, loss = 2.44909930\n",
      "Iteration 35, loss = 4.01667085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 2.39309395\n",
      "Iteration 52, loss = 2.85509579\n",
      "Iteration 53, loss = 2.58956147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 14.23177171\n",
      "Iteration 2, loss = 14.43205830\n",
      "Iteration 1, loss = 18.20974745\n",
      "Iteration 3, loss = 13.59617728\n",
      "Iteration 2, loss = 14.52484447\n",
      "Iteration 4, loss = 9.88223538\n",
      "Iteration 3, loss = 14.33910127\n",
      "Iteration 5, loss = 8.32598837\n",
      "Iteration 4, loss = 8.46750072\n",
      "Iteration 6, loss = 7.26536727\n",
      "Iteration 5, loss = 9.22526005\n",
      "Iteration 7, loss = 8.35710189\n",
      "Iteration 8, loss = 7.06667141\n",
      "Iteration 6, loss = 12.04645464\n",
      "Iteration 9, loss = 5.89993916\n",
      "Iteration 10, loss = 6.77655968\n",
      "Iteration 7, loss = 7.44453068\n",
      "Iteration 11, loss = 3.84455672\n",
      "Iteration 8, loss = 5.67670431\n",
      "Iteration 12, loss = 4.41220878\n",
      "Iteration 13, loss = 4.00814713\n",
      "Iteration 9, loss = 7.20470499\n",
      "Iteration 10, loss = 6.05357800\n",
      "Iteration 14, loss = 4.27935485\n",
      "Iteration 11, loss = 7.90114851\n",
      "Iteration 15, loss = 3.33407485\n",
      "Iteration 12, loss = 5.44362527\n",
      "Iteration 16, loss = 3.46190866\n",
      "Iteration 13, loss = 5.30196154\n",
      "Iteration 17, loss = 3.50439714\n",
      "Iteration 14, loss = 5.61507867\n",
      "Iteration 18, loss = 3.88355907\n",
      "Iteration 15, loss = 7.28679812\n",
      "Iteration 16, loss = 3.65518615\n",
      "Iteration 17, loss = 3.69073566\n",
      "Iteration 18, loss = 4.07898170\n",
      "Iteration 19, loss = 4.04294234\n",
      "Iteration 19, loss = 3.77622102\n",
      "Iteration 20, loss = 3.91539274\n",
      "Iteration 20, loss = 3.95266111\n",
      "Iteration 21, loss = 3.56452293\n",
      "Iteration 22, loss = 3.58624919\n",
      "Iteration 21, loss = 4.05636906\n",
      "Iteration 22, loss = 3.93840672\n",
      "Iteration 23, loss = 3.22983917\n",
      "Iteration 23, loss = 3.22934346\n",
      "Iteration 24, loss = 3.20658994\n",
      "Iteration 24, loss = 3.77071948\n",
      "Iteration 25, loss = 3.43883948\n",
      "Iteration 25, loss = 2.75940348\n",
      "Iteration 26, loss = 3.07862403\n",
      "Iteration 27, loss = 2.29399903\n",
      "Iteration 26, loss = 3.49166199\n",
      "Iteration 27, loss = 3.50125526\n",
      "Iteration 28, loss = 2.77441213\n",
      "Iteration 29, loss = 2.22869843\n",
      "Iteration 30, loss = 2.24097964\n",
      "Iteration 31, loss = 2.16944992\n",
      "Iteration 28, loss = 3.58503086\n",
      "Iteration 32, loss = 2.19731003\n",
      "Iteration 29, loss = 3.26739675\n",
      "Iteration 33, loss = 2.13583559\n",
      "Iteration 30, loss = 3.82270072\n",
      "Iteration 34, loss = 2.26769860\n",
      "Iteration 31, loss = 3.27049636\n",
      "Iteration 35, loss = 2.90775345\n",
      "Iteration 32, loss = 3.24824369\n",
      "Iteration 36, loss = 2.36290855\n",
      "Iteration 33, loss = 2.58862584\n",
      "Iteration 34, loss = 2.68369791\n",
      "Iteration 37, loss = 2.61628707\n",
      "Iteration 38, loss = 2.29964884\n",
      "Iteration 39, loss = 1.89734618\n",
      "Iteration 35, loss = 3.10184558\n",
      "Iteration 40, loss = 2.55921544\n",
      "Iteration 36, loss = 2.46381086\n",
      "Iteration 37, loss = 2.06562337\n",
      "Iteration 41, loss = 2.53712173\n",
      "Iteration 38, loss = 2.26548918\n",
      "Iteration 42, loss = 2.87133743\n",
      "Iteration 43, loss = 3.05578145\n",
      "Iteration 39, loss = 2.62004810\n",
      "Iteration 44, loss = 2.66586428\n",
      "Iteration 40, loss = 2.98251570\n",
      "Iteration 41, loss = 3.72129408\n",
      "Iteration 45, loss = 2.40172019\n",
      "Iteration 42, loss = 3.05379179\n",
      "Iteration 46, loss = 2.20823478\n",
      "Iteration 47, loss = 1.86254254\n",
      "Iteration 43, loss = 2.62243327\n",
      "Iteration 48, loss = 1.82946667\n",
      "Iteration 44, loss = 2.74848773\n",
      "Iteration 45, loss = 2.42569075\n",
      "Iteration 49, loss = 1.89563566\n",
      "Iteration 46, loss = 2.18205515\n",
      "Iteration 50, loss = 1.52598908\n",
      "Iteration 51, loss = 1.34888310\n",
      "Iteration 52, loss = 1.60115271\n",
      "Iteration 47, loss = 3.06544952\n",
      "Iteration 53, loss = 1.61342119\n",
      "Iteration 54, loss = 1.27646318\n",
      "Iteration 55, loss = 1.24586583\n",
      "Iteration 56, loss = 1.39372770\n",
      "Iteration 48, loss = 3.55657401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 1.28360728\n",
      "Iteration 58, loss = 1.40384420\n",
      "Iteration 59, loss = 1.48603187\n",
      "Iteration 60, loss = 1.38902780\n",
      "Iteration 61, loss = 1.43428499\n",
      "Iteration 62, loss = 1.53246352\n",
      "Iteration 63, loss = 1.41493558\n",
      "Iteration 64, loss = 1.22522989\n",
      "Iteration 65, loss = 1.06927141\n",
      "Iteration 66, loss = 1.59795130\n",
      "Iteration 67, loss = 1.85926033\n",
      "Iteration 1, loss = 15.41197695\n",
      "Iteration 68, loss = 1.79871039\n",
      "Iteration 2, loss = 14.72492012\n",
      "Iteration 3, loss = 14.19087284\n",
      "Iteration 69, loss = 1.37140760\n",
      "Iteration 4, loss = 8.24383619\n",
      "Iteration 70, loss = 1.04270841\n",
      "Iteration 5, loss = 6.65040702\n",
      "Iteration 71, loss = 1.47259785\n",
      "Iteration 72, loss = 1.31287269\n",
      "Iteration 73, loss = 1.15433893\n",
      "Iteration 6, loss = 7.68335674\n",
      "Iteration 74, loss = 1.46134082\n",
      "Iteration 7, loss = 7.65527934\n",
      "Iteration 75, loss = 1.78335732\n",
      "Iteration 76, loss = 2.59930566\n",
      "Iteration 8, loss = 4.88032759\n",
      "Iteration 77, loss = 2.52407903\n",
      "Iteration 78, loss = 2.31434337\n",
      "Iteration 9, loss = 4.96892767\n",
      "Iteration 79, loss = 3.68208186\n",
      "Iteration 10, loss = 4.80113058\n",
      "Iteration 11, loss = 5.68437363\n",
      "Iteration 80, loss = 2.86900841\n",
      "Iteration 12, loss = 4.70677529\n",
      "Iteration 13, loss = 7.31362174\n",
      "Iteration 81, loss = 2.53367532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 7.26036136\n",
      "Iteration 15, loss = 5.62244505\n",
      "Iteration 16, loss = 4.40170727\n",
      "Iteration 1, loss = 14.41973229\n",
      "Iteration 17, loss = 3.63214529\n",
      "Iteration 2, loss = 11.91861108\n",
      "Iteration 3, loss = 9.82599728\n",
      "Iteration 4, loss = 8.25842671\n",
      "Iteration 18, loss = 4.76018048\n",
      "Iteration 5, loss = 8.09989647\n",
      "Iteration 6, loss = 7.22984410\n",
      "Iteration 7, loss = 7.92232959\n",
      "Iteration 19, loss = 3.99650419\n",
      "Iteration 8, loss = 9.79445511\n",
      "Iteration 9, loss = 8.50500854\n",
      "Iteration 20, loss = 3.63815929\n",
      "Iteration 10, loss = 7.56770732\n",
      "Iteration 11, loss = 6.38502648\n",
      "Iteration 12, loss = 6.31459062\n",
      "Iteration 13, loss = 6.04709925\n",
      "Iteration 14, loss = 4.13694924\n",
      "Iteration 21, loss = 3.71401144\n",
      "Iteration 15, loss = 4.08519304\n",
      "Iteration 16, loss = 4.52838294\n",
      "Iteration 17, loss = 5.08503169\n",
      "Iteration 18, loss = 3.81979235\n",
      "Iteration 22, loss = 2.82916652\n",
      "Iteration 19, loss = 3.62019651\n",
      "Iteration 20, loss = 3.94525299\n",
      "Iteration 21, loss = 5.99388319\n",
      "Iteration 22, loss = 4.15079188\n",
      "Iteration 23, loss = 3.98419924\n",
      "Iteration 23, loss = 3.46902656\n",
      "Iteration 24, loss = 3.14584030\n",
      "Iteration 24, loss = 3.07928232\n",
      "Iteration 25, loss = 4.66469542\n",
      "Iteration 26, loss = 3.00816218\n",
      "Iteration 25, loss = 3.66726277\n",
      "Iteration 26, loss = 3.47324095\n",
      "Iteration 27, loss = 3.47935018\n",
      "Iteration 27, loss = 2.84832468\n",
      "Iteration 28, loss = 3.06424901\n",
      "Iteration 29, loss = 3.24378127\n",
      "Iteration 28, loss = 2.69086961\n",
      "Iteration 30, loss = 2.63060881\n",
      "Iteration 29, loss = 3.92330563\n",
      "Iteration 31, loss = 2.41814859\n",
      "Iteration 30, loss = 4.56626322\n",
      "Iteration 32, loss = 4.74769650\n",
      "Iteration 31, loss = 3.11059487\n",
      "Iteration 33, loss = 3.84117429\n",
      "Iteration 34, loss = 3.21496934\n",
      "Iteration 35, loss = 2.73313360\n",
      "Iteration 36, loss = 2.55253322\n",
      "Iteration 32, loss = 3.78540437\n",
      "Iteration 37, loss = 2.39888890\n",
      "Iteration 38, loss = 3.15481866\n",
      "Iteration 39, loss = 2.86356706\n",
      "Iteration 33, loss = 3.28471542\n",
      "Iteration 40, loss = 2.52788786\n",
      "Iteration 41, loss = 2.45469669\n",
      "Iteration 42, loss = 2.92846795\n",
      "Iteration 43, loss = 2.46032743\n",
      "Iteration 44, loss = 2.04959168\n",
      "Iteration 34, loss = 2.25212875\n",
      "Iteration 45, loss = 2.79855818\n",
      "Iteration 46, loss = 2.79231686\n",
      "Iteration 47, loss = 2.78905873\n",
      "Iteration 35, loss = 1.98675104\n",
      "Iteration 48, loss = 2.83566253\n",
      "Iteration 49, loss = 2.68634070\n",
      "Iteration 36, loss = 2.76500866\n",
      "Iteration 50, loss = 2.84460893\n",
      "Iteration 51, loss = 2.76901131\n",
      "Iteration 52, loss = 2.09185760\n",
      "Iteration 37, loss = 2.54679093\n",
      "Iteration 53, loss = 2.08739118\n",
      "Iteration 38, loss = 2.92115037\n",
      "Iteration 54, loss = 2.07437509\n",
      "Iteration 55, loss = 2.03892628\n",
      "Iteration 56, loss = 1.87302591\n",
      "Iteration 39, loss = 2.54639969\n",
      "Iteration 57, loss = 2.01550598\n",
      "Iteration 58, loss = 2.07092960\n",
      "Iteration 59, loss = 2.35332218\n",
      "Iteration 40, loss = 2.44878653\n",
      "Iteration 60, loss = 2.68765832\n",
      "Iteration 61, loss = 2.28544234\n",
      "Iteration 62, loss = 1.78535516\n",
      "Iteration 41, loss = 2.53297865\n",
      "Iteration 42, loss = 1.94824347\n",
      "Iteration 43, loss = 2.17301368\n",
      "Iteration 63, loss = 2.74486842\n",
      "Iteration 44, loss = 1.67378542\n",
      "Iteration 64, loss = 1.61560534\n",
      "Iteration 45, loss = 2.30006901\n",
      "Iteration 65, loss = 1.60022338\n",
      "Iteration 46, loss = 1.63616300\n",
      "Iteration 66, loss = 1.57745074\n",
      "Iteration 67, loss = 1.57000773\n",
      "Iteration 47, loss = 1.86288588\n",
      "Iteration 68, loss = 2.22750653\n",
      "Iteration 69, loss = 1.26780118\n",
      "Iteration 70, loss = 1.51466226\n",
      "Iteration 48, loss = 2.04349690\n",
      "Iteration 71, loss = 1.51150311\n",
      "Iteration 49, loss = 2.05797765\n",
      "Iteration 50, loss = 1.98134407\n",
      "Iteration 72, loss = 1.81639089\n",
      "Iteration 51, loss = 1.65502542\n",
      "Iteration 73, loss = 1.54275390\n",
      "Iteration 74, loss = 1.57402567\n",
      "Iteration 75, loss = 1.53446609Iteration 52, loss = 1.36977973\n",
      "\n",
      "Iteration 53, loss = 1.34791474\n",
      "Iteration 54, loss = 1.35750297\n",
      "Iteration 55, loss = 1.64024889\n",
      "Iteration 76, loss = 1.61393537\n",
      "Iteration 77, loss = 1.42496143\n",
      "Iteration 78, loss = 1.44508855\n",
      "Iteration 56, loss = 1.81371812\n",
      "Iteration 79, loss = 1.75723554\n",
      "Iteration 57, loss = 1.39563240\n",
      "Iteration 58, loss = 1.06239833\n",
      "Iteration 59, loss = 1.59861889\n",
      "Iteration 80, loss = 1.95101278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 1.57617110\n",
      "Iteration 61, loss = 1.34301609\n",
      "Iteration 62, loss = 1.31449689\n",
      "Iteration 63, loss = 2.35598008\n",
      "Iteration 64, loss = 2.07198587\n",
      "Iteration 65, loss = 1.99853608\n",
      "Iteration 66, loss = 1.58600205\n",
      "Iteration 67, loss = 2.57058769\n",
      "Iteration 68, loss = 2.20204462\n",
      "Iteration 69, loss = 1.79889050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.75523653\n",
      "Iteration 2, loss = 18.14903636\n",
      "Iteration 3, loss = 15.43954713\n",
      "Iteration 4, loss = 10.82048910\n",
      "Iteration 5, loss = 8.82926651\n",
      "Iteration 6, loss = 8.90448030\n",
      "Iteration 7, loss = 7.37200829\n",
      "Iteration 8, loss = 7.60970794\n",
      "Iteration 9, loss = 6.71250279\n",
      "Iteration 10, loss = 6.87183527\n",
      "Iteration 11, loss = 5.46833700\n",
      "Iteration 12, loss = 8.42441432\n",
      "Iteration 1, loss = 16.24293076\n",
      "Iteration 13, loss = 7.22215325\n",
      "Iteration 2, loss = 13.14307522\n",
      "Iteration 14, loss = 4.71773467\n",
      "Iteration 3, loss = 10.07264801\n",
      "Iteration 15, loss = 4.87996934\n",
      "Iteration 4, loss = 9.61469396\n",
      "Iteration 16, loss = 4.91395913\n",
      "Iteration 17, loss = 4.69912741\n",
      "Iteration 5, loss = 9.11629279\n",
      "Iteration 6, loss = 9.55864154\n",
      "Iteration 7, loss = 8.23362887\n",
      "Iteration 18, loss = 4.56134656\n",
      "Iteration 8, loss = 8.98528319\n",
      "Iteration 9, loss = 7.26615910\n",
      "Iteration 19, loss = 4.63639929\n",
      "Iteration 10, loss = 7.52823715\n",
      "Iteration 20, loss = 4.08982885\n",
      "Iteration 21, loss = 4.78359764\n",
      "Iteration 11, loss = 5.82815131\n",
      "Iteration 22, loss = 4.22089359\n",
      "Iteration 12, loss = 5.93086594\n",
      "Iteration 23, loss = 3.55778356\n",
      "Iteration 13, loss = 4.80855019\n",
      "Iteration 24, loss = 3.08493077\n",
      "Iteration 14, loss = 5.62556485\n",
      "Iteration 25, loss = 3.19550901\n",
      "Iteration 15, loss = 4.19805368\n",
      "Iteration 26, loss = 3.06061473\n",
      "Iteration 27, loss = 2.63591756\n",
      "Iteration 16, loss = 4.72891907\n",
      "Iteration 17, loss = 4.84471845\n",
      "Iteration 18, loss = 4.01871102\n",
      "Iteration 28, loss = 2.45244407\n",
      "Iteration 19, loss = 3.18359627\n",
      "Iteration 29, loss = 2.73662217\n",
      "Iteration 20, loss = 3.30483479\n",
      "Iteration 30, loss = 2.61771728\n",
      "Iteration 21, loss = 4.23973062\n",
      "Iteration 31, loss = 2.21080365\n",
      "Iteration 22, loss = 3.77399341\n",
      "Iteration 32, loss = 3.04797631\n",
      "Iteration 23, loss = 3.55903234\n",
      "Iteration 24, loss = 2.94598419\n",
      "Iteration 25, loss = 2.52159734\n",
      "Iteration 33, loss = 2.50780197\n",
      "Iteration 26, loss = 2.96395867\n",
      "Iteration 27, loss = 3.12868287\n",
      "Iteration 34, loss = 2.88020817\n",
      "Iteration 28, loss = 2.44163210\n",
      "Iteration 29, loss = 2.40606342\n",
      "Iteration 35, loss = 3.54993809\n",
      "Iteration 30, loss = 2.63160781\n",
      "Iteration 36, loss = 2.60531580\n",
      "Iteration 31, loss = 2.17221115\n",
      "Iteration 32, loss = 2.71766870\n",
      "Iteration 37, loss = 2.77429385\n",
      "Iteration 33, loss = 2.61478021\n",
      "Iteration 38, loss = 2.78444022\n",
      "Iteration 34, loss = 2.83027863\n",
      "Iteration 35, loss = 2.89421398\n",
      "Iteration 39, loss = 2.52340927\n",
      "Iteration 36, loss = 2.37491663\n",
      "Iteration 40, loss = 2.50677959\n",
      "Iteration 37, loss = 2.31366061\n",
      "Iteration 41, loss = 2.79467381\n",
      "Iteration 42, loss = 2.31324348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 2.42184522\n",
      "Iteration 39, loss = 2.57333581\n",
      "Iteration 40, loss = 1.93037830\n",
      "Iteration 41, loss = 2.27797789\n",
      "Iteration 42, loss = 2.23488133\n",
      "Iteration 43, loss = 2.02235952\n",
      "Iteration 44, loss = 2.01966866\n",
      "Iteration 45, loss = 2.36404139\n",
      "Iteration 46, loss = 2.53714171\n",
      "Iteration 47, loss = 2.40591157\n",
      "Iteration 48, loss = 2.33258066\n",
      "Iteration 1, loss = 17.43308211\n",
      "Iteration 2, loss = 17.43598787\n",
      "Iteration 49, loss = 1.62489748\n",
      "Iteration 3, loss = 12.19421342\n",
      "Iteration 50, loss = 1.76548241\n",
      "Iteration 4, loss = 11.14858408\n",
      "Iteration 51, loss = 1.66293964\n",
      "Iteration 5, loss = 8.21936979\n",
      "Iteration 52, loss = 1.55454942\n",
      "Iteration 6, loss = 8.69889500\n",
      "Iteration 7, loss = 9.36093442\n",
      "Iteration 53, loss = 1.48945869\n",
      "Iteration 8, loss = 10.16743550\n",
      "Iteration 54, loss = 1.60436728\n",
      "Iteration 55, loss = 1.55854573\n",
      "Iteration 9, loss = 8.20329240\n",
      "Iteration 56, loss = 1.97164032\n",
      "Iteration 10, loss = 7.48493431\n",
      "Iteration 57, loss = 2.81970425\n",
      "Iteration 11, loss = 7.74205404\n",
      "Iteration 58, loss = 2.43945355\n",
      "Iteration 12, loss = 6.04217744\n",
      "Iteration 59, loss = 3.37488211\n",
      "Iteration 60, loss = 2.31084035\n",
      "Iteration 13, loss = 7.04061048\n",
      "Iteration 61, loss = 2.15582449\n",
      "Iteration 14, loss = 6.90237078\n",
      "Iteration 62, loss = 2.18070296\n",
      "Iteration 15, loss = 4.77831815\n",
      "Iteration 63, loss = 4.50953196\n",
      "Iteration 64, loss = 3.20443883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 4.77465870\n",
      "Iteration 17, loss = 4.85653739\n",
      "Iteration 18, loss = 5.53629928\n",
      "Iteration 19, loss = 5.59737915\n",
      "Iteration 1, loss = 17.85072948\n",
      "Iteration 2, loss = 16.30812089\n",
      "Iteration 20, loss = 4.60651680\n",
      "Iteration 21, loss = 5.72482757\n",
      "Iteration 3, loss = 14.61433480\n",
      "Iteration 22, loss = 4.97618035\n",
      "Iteration 23, loss = 5.00501536\n",
      "Iteration 4, loss = 9.83813050\n",
      "Iteration 24, loss = 4.15303657\n",
      "Iteration 5, loss = 6.53706367\n",
      "Iteration 6, loss = 7.42485958\n",
      "Iteration 25, loss = 5.49370959\n",
      "Iteration 7, loss = 8.21683197\n",
      "Iteration 26, loss = 4.16630974\n",
      "Iteration 8, loss = 5.95216767\n",
      "Iteration 27, loss = 4.63657617\n",
      "Iteration 9, loss = 5.48369387\n",
      "Iteration 28, loss = 4.30470677\n",
      "Iteration 29, loss = 4.24305739\n",
      "Iteration 10, loss = 6.07572171\n",
      "Iteration 30, loss = 3.91844035\n",
      "Iteration 11, loss = 5.36077614\n",
      "Iteration 12, loss = 6.20132519\n",
      "Iteration 13, loss = 5.19250530\n",
      "Iteration 31, loss = 4.49762779\n",
      "Iteration 32, loss = 4.36331765\n",
      "Iteration 14, loss = 5.67497112\n",
      "Iteration 33, loss = 4.22443571\n",
      "Iteration 34, loss = 4.11965588\n",
      "Iteration 15, loss = 5.71533938\n",
      "Iteration 16, loss = 5.62511650\n",
      "Iteration 35, loss = 4.01401057\n",
      "Iteration 17, loss = 3.75300376\n",
      "Iteration 36, loss = 3.41166317\n",
      "Iteration 18, loss = 3.75693754\n",
      "Iteration 37, loss = 4.47253911\n",
      "Iteration 19, loss = 4.35711992\n",
      "Iteration 38, loss = 3.73858617\n",
      "Iteration 20, loss = 4.22157771\n",
      "Iteration 39, loss = 3.03566531\n",
      "Iteration 21, loss = 4.57611092\n",
      "Iteration 40, loss = 3.15753542\n",
      "Iteration 22, loss = 3.31738565\n",
      "Iteration 41, loss = 3.36402283\n",
      "Iteration 23, loss = 2.98157128\n",
      "Iteration 42, loss = 3.19597729\n",
      "Iteration 24, loss = 4.14109847\n",
      "Iteration 43, loss = 3.20026031\n",
      "Iteration 25, loss = 3.98153593\n",
      "Iteration 44, loss = 2.94881446\n",
      "Iteration 26, loss = 3.14665454\n",
      "Iteration 27, loss = 3.12153129\n",
      "Iteration 45, loss = 3.06132900\n",
      "Iteration 46, loss = 3.07682050\n",
      "Iteration 47, loss = 3.43701092\n",
      "Iteration 28, loss = 2.45158743\n",
      "Iteration 29, loss = 2.59263559\n",
      "Iteration 48, loss = 3.32222047\n",
      "Iteration 30, loss = 2.94573836\n",
      "Iteration 31, loss = 2.62816132\n",
      "Iteration 49, loss = 3.18769001\n",
      "Iteration 50, loss = 2.96256825\n",
      "Iteration 32, loss = 3.13549174\n",
      "Iteration 51, loss = 2.58201987\n",
      "Iteration 33, loss = 3.77726335\n",
      "Iteration 52, loss = 2.57872383\n",
      "Iteration 34, loss = 3.63683140\n",
      "Iteration 35, loss = 3.90020652\n",
      "Iteration 36, loss = 3.10406329\n",
      "Iteration 37, loss = 3.59124242\n",
      "Iteration 53, loss = 1.85267983\n",
      "Iteration 54, loss = 2.17737198\n",
      "Iteration 38, loss = 3.55873972\n",
      "Iteration 55, loss = 1.70093851\n",
      "Iteration 39, loss = 3.54376036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 1.98450324\n",
      "Iteration 57, loss = 1.74385271\n",
      "Iteration 58, loss = 1.71808598\n",
      "Iteration 59, loss = 1.51791487\n",
      "Iteration 60, loss = 1.76465436\n",
      "Iteration 61, loss = 1.72404636\n",
      "Iteration 62, loss = 2.61916861\n",
      "Iteration 63, loss = 1.96016301\n",
      "Iteration 64, loss = 1.82276361\n",
      "Iteration 1, loss = 15.54573133\n",
      "Iteration 2, loss = 16.57061223\n",
      "Iteration 65, loss = 1.45089813\n",
      "Iteration 66, loss = 1.28684128\n",
      "Iteration 3, loss = 11.95381074\n",
      "Iteration 67, loss = 1.48688195\n",
      "Iteration 68, loss = 1.50922975\n",
      "Iteration 4, loss = 11.35822493\n",
      "Iteration 5, loss = 7.34982903\n",
      "Iteration 69, loss = 1.86600314\n",
      "Iteration 70, loss = 1.61830525\n",
      "Iteration 6, loss = 8.81213409\n",
      "Iteration 71, loss = 2.02722250\n",
      "Iteration 72, loss = 1.87725010\n",
      "Iteration 7, loss = 8.62672304\n",
      "Iteration 73, loss = 2.18317123\n",
      "Iteration 74, loss = 1.99983269\n",
      "Iteration 8, loss = 8.34604473\n",
      "Iteration 9, loss = 6.83387188\n",
      "Iteration 10, loss = 7.43181135\n",
      "Iteration 11, loss = 6.75405764\n",
      "Iteration 75, loss = 2.17221159\n",
      "Iteration 12, loss = 6.25233480\n",
      "Iteration 13, loss = 5.80198456\n",
      "Iteration 76, loss = 1.79504403\n",
      "Iteration 14, loss = 5.24479310\n",
      "Iteration 77, loss = 1.85282260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 4.47691864\n",
      "Iteration 16, loss = 4.61482577\n",
      "Iteration 17, loss = 4.97929407\n",
      "Iteration 18, loss = 7.56914281\n",
      "Iteration 19, loss = 5.33685241\n",
      "Iteration 20, loss = 4.72324885\n",
      "Iteration 21, loss = 4.28294122\n",
      "Iteration 22, loss = 4.72449817\n",
      "Iteration 23, loss = 7.02139217\n",
      "Iteration 1, loss = 18.05969965\n",
      "Iteration 24, loss = 4.26714279\n",
      "Iteration 2, loss = 14.41059843\n",
      "Iteration 25, loss = 4.56063933\n",
      "Iteration 3, loss = 13.61919058\n",
      "Iteration 26, loss = 4.38168515\n",
      "Iteration 27, loss = 4.44530253\n",
      "Iteration 4, loss = 12.55010166\n",
      "Iteration 5, loss = 8.28365666\n",
      "Iteration 28, loss = 4.62772775\n",
      "Iteration 6, loss = 6.49592487\n",
      "Iteration 7, loss = 7.23352068\n",
      "Iteration 29, loss = 3.43209043\n",
      "Iteration 30, loss = 3.06914677\n",
      "Iteration 8, loss = 9.82099012\n",
      "Iteration 31, loss = 3.60502029\n",
      "Iteration 9, loss = 7.61961710\n",
      "Iteration 10, loss = 6.36202667\n",
      "Iteration 32, loss = 3.20288732\n",
      "Iteration 11, loss = 5.37822251\n",
      "Iteration 33, loss = 3.47277047\n",
      "Iteration 34, loss = 3.53718214\n",
      "Iteration 12, loss = 4.67729314\n",
      "Iteration 35, loss = 3.55524445\n",
      "Iteration 13, loss = 4.41093358\n",
      "Iteration 36, loss = 3.28918271\n",
      "Iteration 14, loss = 3.75131843\n",
      "Iteration 15, loss = 4.22186456\n",
      "Iteration 37, loss = 3.00243568\n",
      "Iteration 38, loss = 3.72609220\n",
      "Iteration 16, loss = 4.51878433\n",
      "Iteration 39, loss = 6.72287460\n",
      "Iteration 17, loss = 4.30154818\n",
      "Iteration 40, loss = 4.39310480\n",
      "Iteration 18, loss = 4.22622759\n",
      "Iteration 41, loss = 3.46244740\n",
      "Iteration 19, loss = 4.20807667\n",
      "Iteration 42, loss = 3.24485425\n",
      "Iteration 20, loss = 4.21951674\n",
      "Iteration 43, loss = 2.51482761\n",
      "Iteration 44, loss = 2.33477209\n",
      "Iteration 21, loss = 3.65046953\n",
      "Iteration 45, loss = 2.49621185\n",
      "Iteration 22, loss = 2.88221651\n",
      "Iteration 23, loss = 3.46392757\n",
      "Iteration 46, loss = 2.34280257\n",
      "Iteration 24, loss = 3.04266169\n",
      "Iteration 47, loss = 2.95567051\n",
      "Iteration 48, loss = 2.16109330\n",
      "Iteration 25, loss = 3.23634243\n",
      "Iteration 49, loss = 2.48131737\n",
      "Iteration 26, loss = 2.72309367\n",
      "Iteration 27, loss = 2.94286106\n",
      "Iteration 50, loss = 2.64606316\n",
      "Iteration 28, loss = 3.29002075\n",
      "Iteration 51, loss = 3.24535735\n",
      "Iteration 52, loss = 2.68344580\n",
      "Iteration 29, loss = 2.95954660\n",
      "Iteration 53, loss = 2.38568501\n",
      "Iteration 30, loss = 2.62199802\n",
      "Iteration 31, loss = 3.12676048\n",
      "Iteration 54, loss = 2.26376872\n",
      "Iteration 32, loss = 2.57156085\n",
      "Iteration 55, loss = 1.82523230\n",
      "Iteration 33, loss = 2.45836645\n",
      "Iteration 56, loss = 1.77883556\n",
      "Iteration 57, loss = 1.60438341\n",
      "Iteration 58, loss = 2.05053205\n",
      "Iteration 34, loss = 2.76628322\n",
      "Iteration 59, loss = 2.35047640\n",
      "Iteration 60, loss = 2.20240832\n",
      "Iteration 35, loss = 2.74994800\n",
      "Iteration 61, loss = 3.72666473\n",
      "Iteration 36, loss = 2.46193661\n",
      "Iteration 62, loss = 4.66649349\n",
      "Iteration 37, loss = 2.29614089\n",
      "Iteration 63, loss = 2.84458686\n",
      "Iteration 64, loss = 2.26443495\n",
      "Iteration 38, loss = 2.40372874\n",
      "Iteration 65, loss = 2.84221275\n",
      "Iteration 39, loss = 2.09287599\n",
      "Iteration 40, loss = 1.94853958\n",
      "Iteration 66, loss = 2.13518429\n",
      "Iteration 67, loss = 2.05871247\n",
      "Iteration 41, loss = 1.91130890\n",
      "Iteration 68, loss = 1.94617249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 1.74623309\n",
      "Iteration 43, loss = 2.26203044\n",
      "Iteration 44, loss = 2.27471883\n",
      "Iteration 45, loss = 2.52615793\n",
      "Iteration 46, loss = 2.32062029\n",
      "Iteration 47, loss = 4.30354365\n",
      "Iteration 48, loss = 3.35562108\n",
      "Iteration 49, loss = 2.59786788\n",
      "Iteration 50, loss = 2.38427932\n",
      "Iteration 51, loss = 2.31560442\n",
      "Iteration 52, loss = 2.85495755\n",
      "Iteration 53, loss = 2.43287192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.96982329\n",
      "Iteration 2, loss = 14.56637758\n",
      "Iteration 3, loss = 13.47887640\n",
      "Iteration 4, loss = 11.29464582\n",
      "Iteration 5, loss = 9.14953801\n",
      "Iteration 6, loss = 7.14223274\n",
      "Iteration 7, loss = 6.78891445\n",
      "Iteration 1, loss = 22.17075525\n",
      "Iteration 8, loss = 6.71578841\n",
      "Iteration 2, loss = 15.46823782\n",
      "Iteration 9, loss = 6.72596709\n",
      "Iteration 3, loss = 9.95232734\n",
      "Iteration 10, loss = 7.46769651\n",
      "Iteration 11, loss = 4.78361030\n",
      "Iteration 12, loss = 4.37958436\n",
      "Iteration 4, loss = 6.36680608\n",
      "Iteration 5, loss = 7.81759876\n",
      "Iteration 6, loss = 7.66912721\n",
      "Iteration 13, loss = 4.35196480\n",
      "Iteration 14, loss = 5.04697328\n",
      "Iteration 15, loss = 4.23243115\n",
      "Iteration 16, loss = 3.75734862\n",
      "Iteration 7, loss = 7.11064329\n",
      "Iteration 8, loss = 4.96994647\n",
      "Iteration 9, loss = 5.25048736\n",
      "Iteration 17, loss = 3.38706984\n",
      "Iteration 18, loss = 3.14729248\n",
      "Iteration 19, loss = 2.86806499\n",
      "Iteration 20, loss = 2.80597829\n",
      "Iteration 10, loss = 9.92922810\n",
      "Iteration 21, loss = 3.35654360\n",
      "Iteration 22, loss = 2.65323352\n",
      "Iteration 23, loss = 3.70933167\n",
      "Iteration 11, loss = 5.41168776\n",
      "Iteration 24, loss = 3.12144371\n",
      "Iteration 25, loss = 2.88178851\n",
      "Iteration 12, loss = 4.83403228\n",
      "Iteration 26, loss = 2.93310384\n",
      "Iteration 13, loss = 4.12325581\n",
      "Iteration 27, loss = 2.21377671\n",
      "Iteration 14, loss = 5.11694486\n",
      "Iteration 15, loss = 7.78825254\n",
      "Iteration 28, loss = 2.53145756\n",
      "Iteration 16, loss = 11.35130607\n",
      "Iteration 17, loss = 8.87523471\n",
      "Iteration 29, loss = 2.56439927\n",
      "Iteration 18, loss = 6.99938371\n",
      "Iteration 19, loss = 6.94562107\n",
      "Iteration 30, loss = 2.13016720\n",
      "Iteration 20, loss = 4.37200852\n",
      "Iteration 31, loss = 1.83079978\n",
      "Iteration 21, loss = 7.02382833\n",
      "Iteration 22, loss = 3.54309443\n",
      "Iteration 23, loss = 3.92878523\n",
      "Iteration 32, loss = 2.82697503\n",
      "Iteration 24, loss = 3.52759000\n",
      "Iteration 33, loss = 2.68212945\n",
      "Iteration 34, loss = 2.76048084\n",
      "Iteration 25, loss = 3.63223553\n",
      "Iteration 35, loss = 2.34454428\n",
      "Iteration 36, loss = 2.10553751\n",
      "Iteration 26, loss = 3.47696854\n",
      "Iteration 37, loss = 1.86448082\n",
      "Iteration 27, loss = 2.53660257\n",
      "Iteration 38, loss = 2.21445697\n",
      "Iteration 28, loss = 3.67464827\n",
      "Iteration 39, loss = 2.13745533\n",
      "Iteration 29, loss = 3.27705664\n",
      "Iteration 40, loss = 2.13872370\n",
      "Iteration 30, loss = 3.03300009\n",
      "Iteration 31, loss = 2.77505020\n",
      "Iteration 41, loss = 2.35416186\n",
      "Iteration 42, loss = 2.04697714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 2.80307845\n",
      "Iteration 33, loss = 2.93567173\n",
      "Iteration 34, loss = 3.09713075\n",
      "Iteration 35, loss = 3.51054865\n",
      "Iteration 36, loss = 2.39135792\n",
      "Iteration 37, loss = 2.15146635\n",
      "Iteration 38, loss = 2.57674828\n",
      "Iteration 39, loss = 2.36806894\n",
      "Iteration 1, loss = 16.99457684\n",
      "Iteration 40, loss = 2.18936018\n",
      "Iteration 2, loss = 11.57527838\n",
      "Iteration 41, loss = 2.68935849\n",
      "Iteration 3, loss = 11.98732744\n",
      "Iteration 42, loss = 2.90684368\n",
      "Iteration 43, loss = 2.73872139\n",
      "Iteration 4, loss = 15.08587650\n",
      "Iteration 5, loss = 10.05901683\n",
      "Iteration 44, loss = 2.55981953\n",
      "Iteration 6, loss = 7.68089989\n",
      "Iteration 45, loss = 2.52424864\n",
      "Iteration 7, loss = 7.22576161\n",
      "Iteration 46, loss = 2.18924659\n",
      "Iteration 47, loss = 1.87593476\n",
      "Iteration 8, loss = 7.54684159\n",
      "Iteration 48, loss = 1.74125832\n",
      "Iteration 9, loss = 6.38144490\n",
      "Iteration 10, loss = 5.45707485\n",
      "Iteration 11, loss = 5.00551395\n",
      "Iteration 49, loss = 1.79493093\n",
      "Iteration 12, loss = 6.35305868\n",
      "Iteration 50, loss = 1.57060160\n",
      "Iteration 13, loss = 4.80825784\n",
      "Iteration 51, loss = 1.74651485\n",
      "Iteration 14, loss = 5.20037275\n",
      "Iteration 52, loss = 2.29720397\n",
      "Iteration 15, loss = 5.50277841\n",
      "Iteration 53, loss = 2.02510709\n",
      "Iteration 16, loss = 4.65804674\n",
      "Iteration 17, loss = 4.23925001\n",
      "Iteration 18, loss = 8.61582800\n",
      "Iteration 54, loss = 1.58947644\n",
      "Iteration 55, loss = 1.62487946\n",
      "Iteration 19, loss = 5.68930580\n",
      "Iteration 56, loss = 1.25151708\n",
      "Iteration 20, loss = 4.54412361\n",
      "Iteration 57, loss = 1.27738014\n",
      "Iteration 21, loss = 3.63521677\n",
      "Iteration 58, loss = 1.44563331\n",
      "Iteration 22, loss = 3.82562309\n",
      "Iteration 59, loss = 1.33797890\n",
      "Iteration 23, loss = 3.37260944\n",
      "Iteration 60, loss = 2.02663823\n",
      "Iteration 24, loss = 3.35012828\n",
      "Iteration 61, loss = 1.75107745\n",
      "Iteration 25, loss = 3.16270925\n",
      "Iteration 62, loss = 1.57768949\n",
      "Iteration 26, loss = 3.50461795\n",
      "Iteration 27, loss = 3.39623112\n",
      "Iteration 63, loss = 1.63737804\n",
      "Iteration 64, loss = 1.00058277\n",
      "Iteration 28, loss = 3.91674236\n",
      "Iteration 65, loss = 1.11714886\n",
      "Iteration 29, loss = 3.89543191\n",
      "Iteration 30, loss = 3.36865312\n",
      "Iteration 66, loss = 1.87487765\n",
      "Iteration 31, loss = 3.09470128\n",
      "Iteration 32, loss = 2.72836898\n",
      "Iteration 67, loss = 1.83205710\n",
      "Iteration 33, loss = 2.68798350\n",
      "Iteration 68, loss = 1.86048035\n",
      "Iteration 34, loss = 3.19085221\n",
      "Iteration 69, loss = 1.75719192\n",
      "Iteration 70, loss = 1.99161533\n",
      "Iteration 35, loss = 3.70986121\n",
      "Iteration 36, loss = 3.00655062\n",
      "Iteration 71, loss = 1.60179962\n",
      "Iteration 37, loss = 2.80940569\n",
      "Iteration 72, loss = 1.82298010\n",
      "Iteration 38, loss = 2.72247486\n",
      "Iteration 73, loss = 1.66973460\n",
      "Iteration 39, loss = 2.38876100\n",
      "Iteration 40, loss = 2.39898120\n",
      "Iteration 41, loss = 3.00716763\n",
      "Iteration 74, loss = 1.48571779\n",
      "Iteration 75, loss = 1.32981546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 3.17125858\n",
      "Iteration 43, loss = 3.00515190\n",
      "Iteration 44, loss = 2.29520248\n",
      "Iteration 45, loss = 2.26072498\n",
      "Iteration 46, loss = 2.42472963\n",
      "Iteration 1, loss = 19.12609557\n",
      "Iteration 47, loss = 2.71991216\n",
      "Iteration 2, loss = 15.32923856\n",
      "Iteration 48, loss = 2.85401724\n",
      "Iteration 3, loss = 13.75199376\n",
      "Iteration 4, loss = 12.96813553\n",
      "Iteration 49, loss = 2.00504426\n",
      "Iteration 5, loss = 15.63559468\n",
      "Iteration 6, loss = 13.08393927\n",
      "Iteration 50, loss = 2.16922410\n",
      "Iteration 7, loss = 15.69431577\n",
      "Iteration 8, loss = 18.91863301\n",
      "Iteration 51, loss = 1.98979521\n",
      "Iteration 9, loss = 20.97141552\n",
      "Iteration 52, loss = 2.25785608\n",
      "Iteration 10, loss = 16.79772303\n",
      "Iteration 53, loss = 2.30652941\n",
      "Iteration 11, loss = 15.35312539\n",
      "Iteration 54, loss = 2.20113516\n",
      "Iteration 55, loss = 1.88752221\n",
      "Iteration 12, loss = 15.92039401\n",
      "Iteration 56, loss = 2.04675228\n",
      "Iteration 57, loss = 1.92177855\n",
      "Iteration 13, loss = 16.15640157\n",
      "Iteration 58, loss = 1.53701682\n",
      "Iteration 14, loss = 17.12388285\n",
      "Iteration 59, loss = 1.60706088\n",
      "Iteration 60, loss = 1.51275893\n",
      "Iteration 15, loss = 19.33955604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 61, loss = 2.06883666\n",
      "Iteration 62, loss = 2.19871991\n",
      "Iteration 63, loss = 2.18271514\n",
      "Iteration 64, loss = 1.91019878\n",
      "Iteration 65, loss = 2.41311898\n",
      "Iteration 66, loss = 1.84870112\n",
      "Iteration 67, loss = 1.99003826\n",
      "Iteration 1, loss = 18.02636585\n",
      "Iteration 68, loss = 2.19756754\n",
      "Iteration 69, loss = 2.04391554\n",
      "Iteration 2, loss = 15.17359571\n",
      "Iteration 70, loss = 2.12851157\n",
      "Iteration 3, loss = 15.26839181\n",
      "Iteration 4, loss = 13.29808886\n",
      "Iteration 71, loss = 2.04206531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 14.33986415\n",
      "Iteration 6, loss = 15.42579640\n",
      "Iteration 7, loss = 17.32390496\n",
      "Iteration 8, loss = 14.70170999\n",
      "Iteration 9, loss = 15.03477416\n",
      "Iteration 10, loss = 14.27214050\n",
      "Iteration 11, loss = 14.94764537\n",
      "Iteration 1, loss = 17.04010328\n",
      "Iteration 12, loss = 13.65379213\n",
      "Iteration 2, loss = 19.56394395\n",
      "Iteration 13, loss = 14.52452098\n",
      "Iteration 14, loss = 13.88773175\n",
      "Iteration 15, loss = 13.95165842\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 15.59902258\n",
      "Iteration 4, loss = 11.03678777\n",
      "Iteration 5, loss = 16.12050071\n",
      "Iteration 6, loss = 12.95617928\n",
      "Iteration 1, loss = 16.72374846\n",
      "Iteration 7, loss = 14.68800782\n",
      "Iteration 2, loss = 17.63489407\n",
      "Iteration 8, loss = 14.75216816\n",
      "Iteration 3, loss = 11.05613670\n",
      "Iteration 9, loss = 16.82019902\n",
      "Iteration 4, loss = 14.12552888\n",
      "Iteration 10, loss = 16.38750431\n",
      "Iteration 5, loss = 16.04744194\n",
      "Iteration 6, loss = 17.71202032\n",
      "Iteration 11, loss = 16.95543723\n",
      "Iteration 7, loss = 14.61749003\n",
      "Iteration 8, loss = 14.95912753\n",
      "Iteration 12, loss = 16.79270923\n",
      "Iteration 9, loss = 14.10839250\n",
      "Iteration 13, loss = 16.08236696\n",
      "Iteration 14, loss = 16.29177563\n",
      "Iteration 10, loss = 13.84347962\n",
      "Iteration 15, loss = 17.41734300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 14.91500228\n",
      "Iteration 12, loss = 16.54132329\n",
      "Iteration 13, loss = 21.00022307\n",
      "Iteration 14, loss = 16.18952275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.90352508\n",
      "Iteration 2, loss = 15.34653688\n",
      "Iteration 3, loss = 14.73370579\n",
      "Iteration 4, loss = 11.90241764\n",
      "Iteration 5, loss = 13.43675293\n",
      "Iteration 6, loss = 11.20552518\n",
      "Iteration 7, loss = 13.14053314\n",
      "Iteration 1, loss = 14.91453144\n",
      "Iteration 2, loss = 16.01116838\n",
      "Iteration 8, loss = 15.16412113\n",
      "Iteration 3, loss = 14.70777630\n",
      "Iteration 4, loss = 11.27356783\n",
      "Iteration 5, loss = 14.57955335\n",
      "Iteration 6, loss = 13.60154685\n",
      "Iteration 9, loss = 14.54631309\n",
      "Iteration 10, loss = 14.74484273\n",
      "Iteration 7, loss = 15.11997135\n",
      "Iteration 8, loss = 15.12823936\n",
      "Iteration 9, loss = 14.02605057\n",
      "Iteration 11, loss = 14.09178433\n",
      "Iteration 12, loss = 13.65085577\n",
      "Iteration 10, loss = 15.34017741\n",
      "Iteration 13, loss = 13.42072525\n",
      "Iteration 14, loss = 12.94272126\n",
      "Iteration 11, loss = 16.54488746\n",
      "Iteration 12, loss = 17.21126490\n",
      "Iteration 15, loss = 13.20725255\n",
      "Iteration 13, loss = 16.41799199\n",
      "Iteration 16, loss = 12.74392203\n",
      "Iteration 17, loss = 13.71656297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 16.40936762\n",
      "Iteration 15, loss = 15.88051551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.58387172\n",
      "Iteration 2, loss = 20.30130806\n",
      "Iteration 3, loss = 14.48259393\n",
      "Iteration 1, loss = 19.23590370\n",
      "Iteration 4, loss = 14.98721598\n",
      "Iteration 5, loss = 14.36799352\n",
      "Iteration 2, loss = 16.27981045\n",
      "Iteration 6, loss = 14.78785678\n",
      "Iteration 7, loss = 15.43739936\n",
      "Iteration 3, loss = 13.87731835\n",
      "Iteration 8, loss = 15.72418269\n",
      "Iteration 4, loss = 17.57507652\n",
      "Iteration 9, loss = 14.45111662\n",
      "Iteration 5, loss = 14.28017718\n",
      "Iteration 6, loss = 14.65694262\n",
      "Iteration 10, loss = 15.63793856\n",
      "Iteration 7, loss = 13.78029265\n",
      "Iteration 11, loss = 17.46637388\n",
      "Iteration 8, loss = 12.10393052\n",
      "Iteration 9, loss = 11.84709742\n",
      "Iteration 12, loss = 17.47156165\n",
      "Iteration 10, loss = 11.77586748\n",
      "Iteration 13, loss = 15.53047454\n",
      "Iteration 14, loss = 17.28771010\n",
      "Iteration 11, loss = 13.13118352\n",
      "Iteration 12, loss = 11.76643446\n",
      "Iteration 15, loss = 16.72681069\n",
      "Iteration 16, loss = 18.46802200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 11.57641771\n",
      "Iteration 14, loss = 11.54637513\n",
      "Iteration 15, loss = 11.72834509\n",
      "Iteration 16, loss = 11.98550513\n",
      "Iteration 17, loss = 11.74891221\n",
      "Iteration 18, loss = 10.96535500\n",
      "Iteration 19, loss = 11.02951080\n",
      "Iteration 20, loss = 11.07671099\n",
      "Iteration 21, loss = 11.14483980\n",
      "Iteration 22, loss = 10.89410428\n",
      "Iteration 23, loss = 10.92261803\n",
      "Iteration 24, loss = 11.22345186\n",
      "Iteration 25, loss = 11.28199756\n",
      "Iteration 26, loss = 11.14423602\n",
      "Iteration 1, loss = 19.95794921\n",
      "Iteration 27, loss = 11.01301262\n",
      "Iteration 2, loss = 17.29916001\n",
      "Iteration 28, loss = 10.50533023\n",
      "Iteration 3, loss = 14.95195547\n",
      "Iteration 29, loss = 10.85603037\n",
      "Iteration 4, loss = 10.89625838\n",
      "Iteration 30, loss = 10.84045457\n",
      "Iteration 5, loss = 11.93488939\n",
      "Iteration 31, loss = 10.65952985\n",
      "Iteration 32, loss = 10.00483132\n",
      "Iteration 33, loss = 10.75844225\n",
      "Iteration 6, loss = 13.95736134\n",
      "Iteration 34, loss = 11.00301729\n",
      "Iteration 7, loss = 16.04868643\n",
      "Iteration 8, loss = 14.68526308\n",
      "Iteration 35, loss = 11.56295599\n",
      "Iteration 9, loss = 15.06881369\n",
      "Iteration 36, loss = 11.25603467\n",
      "Iteration 10, loss = 15.65738924\n",
      "Iteration 11, loss = 15.26484690\n",
      "Iteration 37, loss = 11.73263498\n",
      "Iteration 12, loss = 14.92290612\n",
      "Iteration 38, loss = 11.07102376\n",
      "Iteration 13, loss = 14.11943370\n",
      "Iteration 39, loss = 11.08977060\n",
      "Iteration 40, loss = 10.56981711\n",
      "Iteration 41, loss = 10.33180059\n",
      "Iteration 42, loss = 10.59868611\n",
      "Iteration 14, loss = 14.15460245\n",
      "Iteration 43, loss = 10.18790954\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 14.78542534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.76760846\n",
      "Iteration 2, loss = 20.72901084\n",
      "Iteration 1, loss = 17.26443992\n",
      "Iteration 2, loss = 18.70175191\n",
      "Iteration 3, loss = 16.82534995\n",
      "Iteration 3, loss = 13.99959667\n",
      "Iteration 4, loss = 11.82982689\n",
      "Iteration 4, loss = 16.23698458\n",
      "Iteration 5, loss = 14.82064500\n",
      "Iteration 5, loss = 17.41104285\n",
      "Iteration 6, loss = 14.68928972\n",
      "Iteration 7, loss = 15.36769590\n",
      "Iteration 6, loss = 13.20765805\n",
      "Iteration 8, loss = 15.40034871\n",
      "Iteration 9, loss = 14.45959038\n",
      "Iteration 10, loss = 14.66022502\n",
      "Iteration 11, loss = 14.56800307\n",
      "Iteration 7, loss = 14.61191626\n",
      "Iteration 12, loss = 15.43177808\n",
      "Iteration 8, loss = 17.69435311\n",
      "Iteration 9, loss = 16.84755488\n",
      "Iteration 10, loss = 16.56432750\n",
      "Iteration 13, loss = 16.40379999\n",
      "Iteration 11, loss = 15.52308507\n",
      "Iteration 12, loss = 15.54996985\n",
      "Iteration 14, loss = 15.68064290\n",
      "Iteration 15, loss = 14.62191457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 16.45458204\n",
      "Iteration 14, loss = 17.59510433\n",
      "Iteration 15, loss = 16.23117583\n",
      "Iteration 16, loss = 16.74765012\n",
      "Iteration 17, loss = 16.52858185\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 21.03783204\n",
      "Iteration 2, loss = 25.59659294\n",
      "Iteration 1, loss = 15.97617317\n",
      "Iteration 2, loss = 14.47350993\n",
      "Iteration 3, loss = 14.49820808\n",
      "Iteration 3, loss = 17.89937583\n",
      "Iteration 4, loss = 11.04956746\n",
      "Iteration 4, loss = 14.22172682\n",
      "Iteration 5, loss = 15.52521501\n",
      "Iteration 6, loss = 19.75066402\n",
      "Iteration 7, loss = 16.00579995\n",
      "Iteration 8, loss = 14.48952098\n",
      "Iteration 5, loss = 13.85786156\n",
      "Iteration 6, loss = 14.64840709\n",
      "Iteration 7, loss = 13.98799943\n",
      "Iteration 9, loss = 16.08958351\n",
      "Iteration 10, loss = 15.62547361\n",
      "Iteration 11, loss = 16.82460964\n",
      "Iteration 12, loss = 17.47370395\n",
      "Iteration 13, loss = 18.45623479\n",
      "Iteration 14, loss = 17.71857971\n",
      "Iteration 15, loss = 15.04567993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 16.27671777\n",
      "Iteration 9, loss = 20.42392761\n",
      "Iteration 10, loss = 18.38751844\n",
      "Iteration 11, loss = 16.66502543\n",
      "Iteration 1, loss = 17.05071393\n",
      "Iteration 2, loss = 17.58385136\n",
      "Iteration 3, loss = 16.43323581\n",
      "Iteration 4, loss = 16.57459839\n",
      "Iteration 12, loss = 15.35132550\n",
      "Iteration 5, loss = 15.24493205\n",
      "Iteration 6, loss = 15.13648386\n",
      "Iteration 7, loss = 15.80570775\n",
      "Iteration 8, loss = 14.93082130\n",
      "Iteration 9, loss = 15.87147591\n",
      "Iteration 13, loss = 15.92103379Iteration 10, loss = 18.02261250\n",
      "\n",
      "Iteration 11, loss = 18.41892009\n",
      "Iteration 12, loss = 15.73737533\n",
      "Iteration 13, loss = 16.30982681\n",
      "Iteration 14, loss = 16.91472122\n",
      "Iteration 15, loss = 16.25412832\n",
      "Iteration 14, loss = 14.78534103\n",
      "Iteration 16, loss = 16.45996575\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 14.84507813\n",
      "Iteration 16, loss = 14.61890668\n",
      "Iteration 17, loss = 15.12104241\n",
      "Iteration 18, loss = 16.00573172\n",
      "Iteration 19, loss = 16.65228674\n",
      "Iteration 20, loss = 15.84099838\n",
      "Iteration 21, loss = 14.34839411\n",
      "Iteration 22, loss = 13.56425976\n",
      "Iteration 23, loss = 14.19854848\n",
      "Iteration 24, loss = 15.82065224\n",
      "Iteration 1, loss = 19.68146208\n",
      "Iteration 2, loss = 23.79031394\n",
      "Iteration 3, loss = 14.65966380\n",
      "Iteration 25, loss = 15.76173943\n",
      "Iteration 4, loss = 10.85276008\n",
      "Iteration 5, loss = 10.83736157\n",
      "Iteration 26, loss = 14.33505732\n",
      "Iteration 6, loss = 17.25367175\n",
      "Iteration 27, loss = 13.66657244\n",
      "Iteration 7, loss = 14.37332745\n",
      "Iteration 28, loss = 14.01969004\n",
      "Iteration 29, loss = 13.23774865\n",
      "Iteration 8, loss = 14.54319712\n",
      "Iteration 30, loss = 14.68397912\n",
      "Iteration 31, loss = 13.52681174\n",
      "Iteration 9, loss = 13.60797710\n",
      "Iteration 10, loss = 13.64811445\n",
      "Iteration 32, loss = 13.78350466\n",
      "Iteration 33, loss = 14.24218455\n",
      "Iteration 11, loss = 14.05162564\n",
      "Iteration 34, loss = 13.75891146\n",
      "Iteration 12, loss = 14.89838137\n",
      "Iteration 35, loss = 14.17558435\n",
      "Iteration 13, loss = 14.62066380\n",
      "Iteration 36, loss = 14.82530623\n",
      "Iteration 37, loss = 14.25810084\n",
      "Iteration 14, loss = 13.56250860\n",
      "Iteration 38, loss = 13.32983577\n",
      "Iteration 39, loss = 13.89189723\n",
      "Iteration 15, loss = 13.99625564\n",
      "Iteration 16, loss = 13.00638989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 13.86859805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.86055205\n",
      "Iteration 2, loss = 15.81958059\n",
      "Iteration 3, loss = 16.34438470\n",
      "Iteration 4, loss = 15.87063423\n",
      "Iteration 5, loss = 12.63719371\n",
      "Iteration 6, loss = 11.14554788\n",
      "Iteration 7, loss = 13.97231445\n",
      "Iteration 8, loss = 16.08360886\n",
      "Iteration 9, loss = 14.41226652\n",
      "Iteration 1, loss = 18.70496287\n",
      "Iteration 10, loss = 17.24487160\n",
      "Iteration 2, loss = 15.78477000\n",
      "Iteration 11, loss = 16.87911968\n",
      "Iteration 3, loss = 18.43724171\n",
      "Iteration 4, loss = 13.79460971\n",
      "Iteration 12, loss = 15.88405154\n",
      "Iteration 5, loss = 14.36291597\n",
      "Iteration 6, loss = 15.93112536\n",
      "Iteration 13, loss = 14.85305468\n",
      "Iteration 14, loss = 14.71719524\n",
      "Iteration 7, loss = 17.51054372\n",
      "Iteration 15, loss = 15.13622446\n",
      "Iteration 8, loss = 15.58698223\n",
      "Iteration 16, loss = 14.08803184\n",
      "Iteration 9, loss = 15.23354453\n",
      "Iteration 17, loss = 14.87752922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 16.49182139\n",
      "Iteration 11, loss = 17.46211678\n",
      "Iteration 12, loss = 20.39697688\n",
      "Iteration 13, loss = 18.54301219\n",
      "Iteration 14, loss = 18.36262975\n",
      "Iteration 1, loss = 20.25059426\n",
      "Iteration 15, loss = 16.98904903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 15.60020294\n",
      "Iteration 3, loss = 17.11880397\n",
      "Iteration 4, loss = 10.81658681\n",
      "Iteration 5, loss = 13.97675952\n",
      "Iteration 6, loss = 11.86683533\n",
      "Iteration 7, loss = 11.28349479\n",
      "Iteration 8, loss = 16.46938995\n",
      "Iteration 9, loss = 14.29879508\n",
      "Iteration 10, loss = 14.51611503\n",
      "Iteration 11, loss = 14.13196282\n",
      "Iteration 1, loss = 16.02020266\n",
      "Iteration 12, loss = 14.01145896\n",
      "Iteration 2, loss = 15.49858515\n",
      "Iteration 3, loss = 13.23919723\n",
      "Iteration 13, loss = 14.29307392\n",
      "Iteration 14, loss = 13.64142907\n",
      "Iteration 4, loss = 12.37938180\n",
      "Iteration 15, loss = 15.14716556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 11.00820374\n",
      "Iteration 6, loss = 11.53066631\n",
      "Iteration 7, loss = 10.91022533\n",
      "Iteration 8, loss = 12.02259451\n",
      "Iteration 9, loss = 13.78318214\n",
      "Iteration 1, loss = 21.41035479\n",
      "Iteration 10, loss = 13.88855284\n",
      "Iteration 2, loss = 26.10538348\n",
      "Iteration 3, loss = 16.85695390\n",
      "Iteration 11, loss = 15.18968653\n",
      "Iteration 4, loss = 16.62432299\n",
      "Iteration 12, loss = 13.30955029\n",
      "Iteration 13, loss = 13.50085913\n",
      "Iteration 14, loss = 13.68777160\n",
      "Iteration 15, loss = 13.26857264\n",
      "Iteration 5, loss = 12.70505730\n",
      "Iteration 16, loss = 13.06053039\n",
      "Iteration 17, loss = 13.28964270\n",
      "Iteration 6, loss = 11.80698027\n",
      "Iteration 18, loss = 14.14898425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 13.03264234\n",
      "Iteration 8, loss = 14.25974495\n",
      "Iteration 9, loss = 13.93760152\n",
      "Iteration 10, loss = 15.15397465\n",
      "Iteration 11, loss = 14.43374322\n",
      "Iteration 12, loss = 14.74891499\n",
      "Iteration 13, loss = 14.80435265\n",
      "Iteration 14, loss = 14.98116715\n",
      "Iteration 15, loss = 15.74641656\n",
      "Iteration 16, loss = 15.60823167\n",
      "Iteration 1, loss = 18.65659965\n",
      "Iteration 17, loss = 15.71123087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 17.06252654\n",
      "Iteration 3, loss = 13.07638923\n",
      "Iteration 4, loss = 14.33462746\n",
      "Iteration 5, loss = 16.20626902\n",
      "Iteration 6, loss = 13.62127770\n",
      "Iteration 7, loss = 17.47175222\n",
      "Iteration 8, loss = 15.35369726\n",
      "Iteration 1, loss = 17.63237617\n",
      "Iteration 2, loss = 20.18731650\n",
      "Iteration 3, loss = 16.84089713\n",
      "Iteration 9, loss = 19.40071723\n",
      "Iteration 10, loss = 18.65957609\n",
      "Iteration 4, loss = 15.34398244\n",
      "Iteration 11, loss = 16.66602834\n",
      "Iteration 12, loss = 17.74572427\n",
      "Iteration 5, loss = 14.14492652\n",
      "Iteration 13, loss = 19.52807628\n",
      "Iteration 6, loss = 14.31913124\n",
      "Iteration 7, loss = 16.10339567\n",
      "Iteration 14, loss = 17.05941341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 16.85200865\n",
      "Iteration 9, loss = 14.27007763\n",
      "Iteration 10, loss = 16.81047267\n",
      "Iteration 11, loss = 15.00103798\n",
      "Iteration 12, loss = 13.46296946\n",
      "Iteration 13, loss = 15.36737219\n",
      "Iteration 1, loss = 17.74813841\n",
      "Iteration 14, loss = 16.04613439\n",
      "Iteration 2, loss = 16.15425059\n",
      "Iteration 15, loss = 15.48224053\n",
      "Iteration 16, loss = 14.49031315\n",
      "Iteration 3, loss = 13.92723967\n",
      "Iteration 17, loss = 13.50599674\n",
      "Iteration 18, loss = 18.45777728\n",
      "Iteration 4, loss = 17.37720271\n",
      "Iteration 5, loss = 17.46324419\n",
      "Iteration 6, loss = 18.76839816\n",
      "Iteration 19, loss = 14.60721414\n",
      "Iteration 7, loss = 15.15934687\n",
      "Iteration 20, loss = 14.87380093\n",
      "Iteration 8, loss = 14.15232638\n",
      "Iteration 9, loss = 14.68905101\n",
      "Iteration 21, loss = 13.57447272\n",
      "Iteration 10, loss = 15.75571846\n",
      "Iteration 22, loss = 13.64747963\n",
      "Iteration 23, loss = 13.25171593\n",
      "Iteration 11, loss = 14.85601911\n",
      "Iteration 24, loss = 14.09836198\n",
      "Iteration 12, loss = 15.23133342\n",
      "Iteration 25, loss = 15.52887738\n",
      "Iteration 13, loss = 14.20399159\n",
      "Iteration 26, loss = 14.79435341\n",
      "Iteration 14, loss = 15.56437159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 13.83568306\n",
      "Iteration 28, loss = 13.99797204\n",
      "Iteration 29, loss = 13.24842738\n",
      "Iteration 30, loss = 14.80392194\n",
      "Iteration 31, loss = 14.38739701\n",
      "Iteration 32, loss = 13.92672146\n",
      "Iteration 33, loss = 13.50319962\n",
      "Iteration 34, loss = 13.75211412\n",
      "Iteration 35, loss = 13.80063876\n",
      "Iteration 36, loss = 13.54529559\n",
      "Iteration 37, loss = 13.08109592\n",
      "Iteration 38, loss = 13.07461233\n",
      "Iteration 1, loss = 18.50460056\n",
      "Iteration 39, loss = 13.52659007\n",
      "Iteration 2, loss = 16.44046975\n",
      "Iteration 3, loss = 16.26949903\n",
      "Iteration 4, loss = 13.42028938\n",
      "Iteration 40, loss = 13.50046205\n",
      "Iteration 5, loss = 11.69788672\n",
      "Iteration 6, loss = 12.30966973\n",
      "Iteration 41, loss = 13.74454434\n",
      "Iteration 7, loss = 12.14411230\n",
      "Iteration 42, loss = 12.88609011\n",
      "Iteration 8, loss = 14.31193655\n",
      "Iteration 43, loss = 13.47556490\n",
      "Iteration 9, loss = 14.71028647\n",
      "Iteration 44, loss = 13.08165908\n",
      "Iteration 10, loss = 16.18460651\n",
      "Iteration 11, loss = 15.08779334\n",
      "Iteration 12, loss = 14.12565101\n",
      "Iteration 13, loss = 14.33187924\n",
      "Iteration 45, loss = 12.96347446\n",
      "Iteration 14, loss = 14.06042652\n",
      "Iteration 46, loss = 13.47165888Iteration 15, loss = 13.42671716\n",
      "\n",
      "Iteration 47, loss = 13.82478687\n",
      "Iteration 16, loss = 13.33938837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 12.99981181\n",
      "Iteration 49, loss = 13.04624766\n",
      "Iteration 50, loss = 13.17903374\n",
      "Iteration 51, loss = 12.58516720\n",
      "Iteration 52, loss = 12.80022578\n",
      "Iteration 53, loss = 12.67886027\n",
      "Iteration 54, loss = 12.38546895\n",
      "Iteration 55, loss = 13.80902812\n",
      "Iteration 56, loss = 14.09060171\n",
      "Iteration 57, loss = 13.66198077\n",
      "Iteration 1, loss = 19.62486625\n",
      "Iteration 58, loss = 14.17315495\n",
      "Iteration 2, loss = 17.64544465\n",
      "Iteration 59, loss = 13.11429601\n",
      "Iteration 60, loss = 12.91836252\n",
      "Iteration 3, loss = 16.63581181\n",
      "Iteration 61, loss = 13.58524648\n",
      "Iteration 4, loss = 13.86685405\n",
      "Iteration 5, loss = 14.83454619\n",
      "Iteration 62, loss = 14.52457156\n",
      "Iteration 6, loss = 17.52344318\n",
      "Iteration 63, loss = 14.21109287\n",
      "Iteration 7, loss = 16.23683524\n",
      "Iteration 64, loss = 14.01552955\n",
      "Iteration 8, loss = 13.51929772\n",
      "Iteration 65, loss = 13.39093879\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 14.39391380\n",
      "Iteration 10, loss = 13.73612106\n",
      "Iteration 11, loss = 14.58161574\n",
      "Iteration 12, loss = 14.84942698\n",
      "Iteration 13, loss = 14.67032221\n",
      "Iteration 14, loss = 14.93192667\n",
      "Iteration 15, loss = 14.62495026\n",
      "Iteration 16, loss = 14.49600835\n",
      "Iteration 17, loss = 14.43771957\n",
      "Iteration 1, loss = 18.06101636\n",
      "Iteration 18, loss = 14.30024021\n",
      "Iteration 2, loss = 18.52921869\n",
      "Iteration 19, loss = 14.37668975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 17.03746548\n",
      "Iteration 4, loss = 15.16760314\n",
      "Iteration 5, loss = 12.49504695\n",
      "Iteration 6, loss = 13.04225599\n",
      "Iteration 7, loss = 14.25315206\n",
      "Iteration 8, loss = 13.17043350\n",
      "Iteration 9, loss = 14.85822237\n",
      "Iteration 10, loss = 14.61903803\n",
      "Iteration 11, loss = 13.92551968\n",
      "Iteration 12, loss = 14.56905938\n",
      "Iteration 1, loss = 16.68654734\n",
      "Iteration 2, loss = 17.19241441\n",
      "Iteration 3, loss = 15.84618773\n",
      "Iteration 4, loss = 12.83352777\n",
      "Iteration 13, loss = 15.26470243\n",
      "Iteration 5, loss = 15.64147323\n",
      "Iteration 14, loss = 14.78339921\n",
      "Iteration 15, loss = 14.86461375\n",
      "Iteration 6, loss = 15.81623486\n",
      "Iteration 7, loss = 16.46492100\n",
      "Iteration 16, loss = 15.29116421\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 15.94765933\n",
      "Iteration 9, loss = 14.59905210\n",
      "Iteration 1, loss = 17.02301539\n",
      "Iteration 10, loss = 15.18635989\n",
      "Iteration 2, loss = 15.87756598\n",
      "Iteration 3, loss = 17.15570796\n",
      "Iteration 4, loss = 13.53104401\n",
      "Iteration 5, loss = 12.59641089\n",
      "Iteration 6, loss = 11.86019765\n",
      "Iteration 7, loss = 14.30879409\n",
      "Iteration 8, loss = 14.20714123\n",
      "Iteration 9, loss = 14.35628299\n",
      "Iteration 10, loss = 13.68111316\n",
      "Iteration 11, loss = 13.77745841\n",
      "Iteration 12, loss = 16.47389288\n",
      "Iteration 13, loss = 15.80101410\n",
      "Iteration 14, loss = 15.48399801\n",
      "Iteration 15, loss = 14.28274996\n",
      "Iteration 16, loss = 13.84753089\n",
      "Iteration 17, loss = 14.10283964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 15.81385665\n",
      "Iteration 12, loss = 15.85699630\n",
      "Iteration 13, loss = 15.81372841\n",
      "Iteration 14, loss = 17.67865794\n",
      "Iteration 15, loss = 16.47525035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.34151836\n",
      "Iteration 2, loss = 16.03082972\n",
      "Iteration 3, loss = 16.80773458\n",
      "Iteration 4, loss = 14.65467583\n",
      "Iteration 5, loss = 12.48070982\n",
      "Iteration 6, loss = 13.31737746\n",
      "Iteration 7, loss = 14.13138753\n",
      "Iteration 8, loss = 20.40345830\n",
      "Iteration 9, loss = 18.12251900\n",
      "Iteration 10, loss = 20.08873010\n",
      "Iteration 11, loss = 15.91067849\n",
      "Iteration 12, loss = 16.60535580\n",
      "Iteration 13, loss = 16.61304310\n",
      "Iteration 14, loss = 16.28912445\n",
      "Iteration 15, loss = 16.47082085\n",
      "Iteration 16, loss = 17.06897966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.24516280\n",
      "Iteration 2, loss = 13.39309465\n",
      "Iteration 3, loss = 17.26983124\n",
      "Iteration 4, loss = 14.85945896\n",
      "Iteration 5, loss = 11.86133115\n",
      "Iteration 6, loss = 15.64536003\n",
      "Iteration 7, loss = 13.91294257\n",
      "Iteration 1, loss = 20.64196828\n",
      "Iteration 8, loss = 13.64416444\n",
      "Iteration 2, loss = 19.77759703\n",
      "Iteration 9, loss = 15.81919729\n",
      "Iteration 3, loss = 15.74309493\n",
      "Iteration 4, loss = 14.73658294\n",
      "Iteration 10, loss = 14.86193192\n",
      "Iteration 5, loss = 14.11796860\n",
      "Iteration 11, loss = 15.43012178\n",
      "Iteration 6, loss = 17.07255919\n",
      "Iteration 12, loss = 16.44421892\n",
      "Iteration 7, loss = 16.37214941\n",
      "Iteration 13, loss = 15.30223791\n",
      "Iteration 8, loss = 17.20477771\n",
      "Iteration 14, loss = 14.77826376\n",
      "Iteration 9, loss = 17.97690291\n",
      "Iteration 15, loss = 14.53464969\n",
      "Iteration 16, loss = 15.64576863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 17.74923982\n",
      "Iteration 11, loss = 17.50591110\n",
      "Iteration 12, loss = 17.36100361\n",
      "Iteration 13, loss = 17.36151469\n",
      "Iteration 14, loss = 16.11995554\n",
      "Iteration 15, loss = 16.78298545\n",
      "Iteration 16, loss = 17.04794170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.89826767\n",
      "Iteration 2, loss = 16.89647576\n",
      "Iteration 3, loss = 13.13793757\n",
      "Iteration 4, loss = 10.88152800\n",
      "Iteration 5, loss = 11.27341230\n",
      "Iteration 6, loss = 12.73516217\n",
      "Iteration 7, loss = 14.33585626\n",
      "Iteration 1, loss = 16.89699132\n",
      "Iteration 2, loss = 15.39652967\n",
      "Iteration 8, loss = 14.42913038\n",
      "Iteration 9, loss = 12.71709423\n",
      "Iteration 10, loss = 12.70186911\n",
      "Iteration 3, loss = 14.80050295\n",
      "Iteration 11, loss = 13.96818605\n",
      "Iteration 12, loss = 13.81192552\n",
      "Iteration 4, loss = 13.88409751\n",
      "Iteration 13, loss = 13.73164605\n",
      "Iteration 5, loss = 14.82204597\n",
      "Iteration 14, loss = 15.01247706\n",
      "Iteration 6, loss = 14.45354600\n",
      "Iteration 15, loss = 15.90143372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 13.68017683\n",
      "Iteration 8, loss = 15.90867328\n",
      "Iteration 9, loss = 17.35963437\n",
      "Iteration 10, loss = 14.67095564\n",
      "Iteration 11, loss = 16.69461694\n",
      "Iteration 12, loss = 16.30929194\n",
      "Iteration 13, loss = 15.58175395\n",
      "Iteration 14, loss = 20.65819660\n",
      "Iteration 15, loss = 18.71286760\n",
      "Iteration 16, loss = 20.43248403\n",
      "Iteration 17, loss = 17.43745672\n",
      "Iteration 18, loss = 16.81922482\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.96940428\n",
      "Iteration 2, loss = 16.80875629\n",
      "Iteration 3, loss = 11.77754422\n",
      "Iteration 4, loss = 11.88001954\n",
      "Iteration 5, loss = 17.01314528\n",
      "Iteration 6, loss = 16.02837699\n",
      "Iteration 7, loss = 17.59378453\n",
      "Iteration 8, loss = 16.22525140\n",
      "Iteration 9, loss = 15.92007138\n",
      "Iteration 1, loss = 16.34822235\n",
      "Iteration 10, loss = 17.81826020\n",
      "Iteration 11, loss = 17.21757912\n",
      "Iteration 12, loss = 18.59174717\n",
      "Iteration 13, loss = 16.19088670\n",
      "Iteration 2, loss = 14.26116779\n",
      "Iteration 14, loss = 16.08279918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 13.62658014\n",
      "Iteration 4, loss = 11.32822656\n",
      "Iteration 5, loss = 13.02722561\n",
      "Iteration 6, loss = 13.27029103\n",
      "Iteration 7, loss = 19.28658034\n",
      "Iteration 8, loss = 15.04013850\n",
      "Iteration 9, loss = 15.19018792\n",
      "Iteration 10, loss = 20.55220274\n",
      "Iteration 11, loss = 14.84730910\n",
      "Iteration 1, loss = 20.12625434\n",
      "Iteration 12, loss = 16.63028771\n",
      "Iteration 2, loss = 20.57658838\n",
      "Iteration 13, loss = 15.35022376\n",
      "Iteration 3, loss = 16.32375291\n",
      "Iteration 14, loss = 15.90748365\n",
      "Iteration 4, loss = 14.59800456\n",
      "Iteration 15, loss = 15.94109181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 12.72539672\n",
      "Iteration 6, loss = 16.26165684\n",
      "Iteration 7, loss = 14.93051581\n",
      "Iteration 8, loss = 13.01681449\n",
      "Iteration 9, loss = 12.65032319\n",
      "Iteration 10, loss = 13.09838522\n",
      "Iteration 11, loss = 13.59542605\n",
      "Iteration 1, loss = 17.88609570\n",
      "Iteration 12, loss = 13.14999116\n",
      "Iteration 13, loss = 12.49664756\n",
      "Iteration 14, loss = 12.51342041\n",
      "Iteration 2, loss = 16.04931620\n",
      "Iteration 15, loss = 12.81768658\n",
      "Iteration 16, loss = 13.61044539\n",
      "Iteration 3, loss = 14.03610045\n",
      "Iteration 17, loss = 12.85118151\n",
      "Iteration 18, loss = 11.56874918\n",
      "Iteration 4, loss = 13.33387073\n",
      "Iteration 19, loss = 12.63596527\n",
      "Iteration 5, loss = 14.54265360\n",
      "Iteration 6, loss = 14.95145991\n",
      "Iteration 20, loss = 12.25232744\n",
      "Iteration 7, loss = 18.09009086\n",
      "Iteration 21, loss = 12.10231693\n",
      "Iteration 8, loss = 15.69203321\n",
      "Iteration 9, loss = 15.64032753\n",
      "Iteration 10, loss = 16.36216588\n",
      "Iteration 22, loss = 11.87128834\n",
      "Iteration 23, loss = 12.03383547\n",
      "Iteration 11, loss = 19.58679338\n",
      "Iteration 24, loss = 11.88687454\n",
      "Iteration 12, loss = 17.38517218\n",
      "Iteration 13, loss = 17.12367200\n",
      "Iteration 25, loss = 11.74078126\n",
      "Iteration 14, loss = 17.36316666\n",
      "Iteration 15, loss = 15.55806454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 11.83126617\n",
      "Iteration 27, loss = 11.37281405\n",
      "Iteration 28, loss = 12.36325171\n",
      "Iteration 29, loss = 11.70541559\n",
      "Iteration 30, loss = 11.82980552\n",
      "Iteration 31, loss = 13.45183921\n",
      "Iteration 32, loss = 12.40480820\n",
      "Iteration 33, loss = 12.49257675\n",
      "Iteration 34, loss = 11.83233584\n",
      "Iteration 35, loss = 11.64737401\n",
      "Iteration 36, loss = 11.42171197\n",
      "Iteration 37, loss = 10.92425655\n",
      "Iteration 1, loss = 21.02843980\n",
      "Iteration 2, loss = 23.59331743\n",
      "Iteration 3, loss = 15.25424375\n",
      "Iteration 38, loss = 11.61219587\n",
      "Iteration 4, loss = 15.70232948\n",
      "Iteration 39, loss = 10.92589946\n",
      "Iteration 40, loss = 11.65517245\n",
      "Iteration 5, loss = 13.54214400\n",
      "Iteration 6, loss = 13.64978462\n",
      "Iteration 41, loss = 12.38270172\n",
      "Iteration 7, loss = 15.19437857\n",
      "Iteration 8, loss = 17.29173215\n",
      "Iteration 9, loss = 13.65764418\n",
      "Iteration 42, loss = 12.51819818\n",
      "Iteration 10, loss = 14.55727992\n",
      "Iteration 43, loss = 11.71242990\n",
      "Iteration 44, loss = 11.57564558\n",
      "Iteration 11, loss = 16.43766094\n",
      "Iteration 45, loss = 11.19377240\n",
      "Iteration 46, loss = 11.29606316\n",
      "Iteration 12, loss = 15.85581394\n",
      "Iteration 13, loss = 15.42558784\n",
      "Iteration 47, loss = 11.08680503\n",
      "Iteration 14, loss = 15.69732443\n",
      "Iteration 48, loss = 10.99845139\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 15.54648879\n",
      "Iteration 16, loss = 16.13014184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.91159451\n",
      "Iteration 2, loss = 13.71243548\n",
      "Iteration 3, loss = 15.85775033\n",
      "Iteration 4, loss = 12.71617695\n",
      "Iteration 5, loss = 13.02038776\n",
      "Iteration 6, loss = 14.76399123\n",
      "Iteration 7, loss = 13.41755733\n",
      "Iteration 1, loss = 18.91409199\n",
      "Iteration 8, loss = 13.75601027\n",
      "Iteration 2, loss = 16.95368381\n",
      "Iteration 9, loss = 15.01943201\n",
      "Iteration 3, loss = 14.59638210\n",
      "Iteration 10, loss = 15.68284627\n",
      "Iteration 4, loss = 11.77691158\n",
      "Iteration 11, loss = 15.30659594\n",
      "Iteration 5, loss = 13.35852647\n",
      "Iteration 12, loss = 16.06079667\n",
      "Iteration 6, loss = 12.28936368\n",
      "Iteration 13, loss = 15.02790826\n",
      "Iteration 7, loss = 14.53061966\n",
      "Iteration 8, loss = 14.59915683\n",
      "Iteration 9, loss = 14.47471785\n",
      "Iteration 14, loss = 15.27571986\n",
      "Iteration 10, loss = 12.78276480\n",
      "Iteration 11, loss = 14.51904040\n",
      "Iteration 15, loss = 14.89450348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 15.79047467\n",
      "Iteration 13, loss = 14.67398759\n",
      "Iteration 14, loss = 14.71494565\n",
      "Iteration 15, loss = 17.97514363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02780207\n",
      "Iteration 2, loss = 0.54535454\n",
      "Iteration 3, loss = 0.46331301\n",
      "Iteration 4, loss = 0.41843679\n",
      "Iteration 5, loss = 0.39966234\n",
      "Iteration 6, loss = 0.34815442\n",
      "Iteration 7, loss = 0.32656316\n",
      "Iteration 1, loss = 0.91281676\n",
      "Iteration 8, loss = 0.29770662\n",
      "Iteration 2, loss = 0.50822561\n",
      "Iteration 9, loss = 0.27900058\n",
      "Iteration 3, loss = 0.41541329\n",
      "Iteration 10, loss = 0.27180144\n",
      "Iteration 4, loss = 0.37704398\n",
      "Iteration 11, loss = 0.25310774\n",
      "Iteration 5, loss = 0.34196688\n",
      "Iteration 12, loss = 0.24222153\n",
      "Iteration 6, loss = 0.31488799\n",
      "Iteration 13, loss = 0.24214551\n",
      "Iteration 7, loss = 0.29769062\n",
      "Iteration 14, loss = 0.23646491\n",
      "Iteration 15, loss = 0.22119401\n",
      "Iteration 16, loss = 0.20692016\n",
      "Iteration 17, loss = 0.19177343\n",
      "Iteration 8, loss = 0.28400271\n",
      "Iteration 18, loss = 0.18853416\n",
      "Iteration 19, loss = 0.18708190\n",
      "Iteration 9, loss = 0.26812007\n",
      "Iteration 20, loss = 0.18379934\n",
      "Iteration 10, loss = 0.25248962\n",
      "Iteration 21, loss = 0.17513239\n",
      "Iteration 11, loss = 0.23546564\n",
      "Iteration 22, loss = 0.17856125\n",
      "Iteration 12, loss = 0.22148676\n",
      "Iteration 23, loss = 0.15283476\n",
      "Iteration 13, loss = 0.21189564\n",
      "Iteration 24, loss = 0.14945246\n",
      "Iteration 25, loss = 0.14505407\n",
      "Iteration 14, loss = 0.19823852\n",
      "Iteration 26, loss = 0.14073516\n",
      "Iteration 27, loss = 0.12712036\n",
      "Iteration 28, loss = 0.13375089\n",
      "Iteration 29, loss = 0.12278464\n",
      "Iteration 15, loss = 0.19626312\n",
      "Iteration 30, loss = 0.11959670\n",
      "Iteration 31, loss = 0.11360031\n",
      "Iteration 32, loss = 0.11615065\n",
      "Iteration 16, loss = 0.18804467\n",
      "Iteration 17, loss = 0.16899523\n",
      "Iteration 33, loss = 0.12170762\n",
      "Iteration 18, loss = 0.16610730\n",
      "Iteration 19, loss = 0.17482627\n",
      "Iteration 34, loss = 0.11479783\n",
      "Iteration 20, loss = 0.18040843\n",
      "Iteration 35, loss = 0.10819355\n",
      "Iteration 21, loss = 0.15899577\n",
      "Iteration 36, loss = 0.12775824\n",
      "Iteration 22, loss = 0.14438285\n",
      "Iteration 37, loss = 0.10431908\n",
      "Iteration 23, loss = 0.14501513\n",
      "Iteration 38, loss = 0.11399669\n",
      "Iteration 24, loss = 0.13618794\n",
      "Iteration 25, loss = 0.13020659\n",
      "Iteration 26, loss = 0.12178931\n",
      "Iteration 39, loss = 0.13555804\n",
      "Iteration 27, loss = 0.12295070\n",
      "Iteration 28, loss = 0.12569838\n",
      "Iteration 40, loss = 0.13243068\n",
      "Iteration 29, loss = 0.15577159\n",
      "Iteration 41, loss = 0.11598917\n",
      "Iteration 30, loss = 0.18548725\n",
      "Iteration 31, loss = 0.14765224\n",
      "Iteration 42, loss = 0.15373927\n",
      "Iteration 32, loss = 0.12963351\n",
      "Iteration 33, loss = 0.10969778\n",
      "Iteration 34, loss = 0.09835185\n",
      "Iteration 35, loss = 0.09140122\n",
      "Iteration 43, loss = 0.13499060\n",
      "Iteration 36, loss = 0.09350515\n",
      "Iteration 37, loss = 0.10059101\n",
      "Iteration 38, loss = 0.09603912\n",
      "Iteration 39, loss = 0.10614308\n",
      "Iteration 40, loss = 0.10752142\n",
      "Iteration 41, loss = 0.10012907\n",
      "Iteration 44, loss = 0.11634448\n",
      "Iteration 42, loss = 0.09709739\n",
      "Iteration 45, loss = 0.12111565\n",
      "Iteration 43, loss = 0.10706822\n",
      "Iteration 44, loss = 0.09103481\n",
      "Iteration 46, loss = 0.11356169\n",
      "Iteration 45, loss = 0.08019937\n",
      "Iteration 47, loss = 0.10898785\n",
      "Iteration 46, loss = 0.09537179\n",
      "Iteration 48, loss = 0.08501582\n",
      "Iteration 47, loss = 0.08265153\n",
      "Iteration 49, loss = 0.08770497\n",
      "Iteration 48, loss = 0.07786283\n",
      "Iteration 50, loss = 0.08385438\n",
      "Iteration 49, loss = 0.07467184\n",
      "Iteration 50, loss = 0.07282464\n",
      "Iteration 51, loss = 0.08688881\n",
      "Iteration 51, loss = 0.07528789\n",
      "Iteration 52, loss = 0.06698499\n",
      "Iteration 52, loss = 0.09137506\n",
      "Iteration 53, loss = 0.07018024\n",
      "Iteration 53, loss = 0.07961513\n",
      "Iteration 54, loss = 0.05463640\n",
      "Iteration 54, loss = 0.06932105\n",
      "Iteration 55, loss = 0.05814830\n",
      "Iteration 56, loss = 0.05163704\n",
      "Iteration 57, loss = 0.04979182\n",
      "Iteration 55, loss = 0.07434339\n",
      "Iteration 58, loss = 0.04785656\n",
      "Iteration 59, loss = 0.04422681\n",
      "Iteration 56, loss = 0.06418971\n",
      "Iteration 60, loss = 0.05223239\n",
      "Iteration 57, loss = 0.06545029\n",
      "Iteration 61, loss = 0.05830458\n",
      "Iteration 58, loss = 0.06152429\n",
      "Iteration 62, loss = 0.05472020\n",
      "Iteration 63, loss = 0.04936850\n",
      "Iteration 59, loss = 0.07304613\n",
      "Iteration 64, loss = 0.05119606\n",
      "Iteration 65, loss = 0.04877607\n",
      "Iteration 66, loss = 0.05290980\n",
      "Iteration 67, loss = 0.04416617\n",
      "Iteration 68, loss = 0.04472149\n",
      "Iteration 69, loss = 0.04758125\n",
      "Iteration 70, loss = 0.04807890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 0.06962306\n",
      "Iteration 61, loss = 0.05920595\n",
      "Iteration 62, loss = 0.05729205\n",
      "Iteration 63, loss = 0.05852294\n",
      "Iteration 64, loss = 0.06420582\n",
      "Iteration 65, loss = 0.06102739\n",
      "Iteration 66, loss = 0.06151326\n",
      "Iteration 67, loss = 0.06783394\n",
      "Iteration 1, loss = 1.05513238\n",
      "Iteration 2, loss = 0.52210515\n",
      "Iteration 3, loss = 0.42713644\n",
      "Iteration 68, loss = 0.06241261\n",
      "Iteration 4, loss = 0.39285266\n",
      "Iteration 5, loss = 0.34874036\n",
      "Iteration 6, loss = 0.32446191\n",
      "Iteration 69, loss = 0.04909501\n",
      "Iteration 7, loss = 0.30225693\n",
      "Iteration 8, loss = 0.28968816\n",
      "Iteration 70, loss = 0.05071250\n",
      "Iteration 71, loss = 0.05179167\n",
      "Iteration 9, loss = 0.27294788\n",
      "Iteration 72, loss = 0.05674437\n",
      "Iteration 10, loss = 0.27450913\n",
      "Iteration 73, loss = 0.06409972\n",
      "Iteration 74, loss = 0.07680294\n",
      "Iteration 11, loss = 0.24456747\n",
      "Iteration 75, loss = 0.06472011\n",
      "Iteration 12, loss = 0.23139360\n",
      "Iteration 76, loss = 0.05114280\n",
      "Iteration 77, loss = 0.04323904\n",
      "Iteration 13, loss = 0.22795658\n",
      "Iteration 78, loss = 0.04681751\n",
      "Iteration 14, loss = 0.22097232\n",
      "Iteration 79, loss = 0.03910030\n",
      "Iteration 15, loss = 0.20696617\n",
      "Iteration 80, loss = 0.04713970\n",
      "Iteration 16, loss = 0.20068645\n",
      "Iteration 81, loss = 0.03965872\n",
      "Iteration 17, loss = 0.18217557\n",
      "Iteration 82, loss = 0.03118775\n",
      "Iteration 18, loss = 0.16849671\n",
      "Iteration 83, loss = 0.03578489\n",
      "Iteration 19, loss = 0.16466065\n",
      "Iteration 84, loss = 0.02931547\n",
      "Iteration 20, loss = 0.15642620\n",
      "Iteration 85, loss = 0.02427640\n",
      "Iteration 21, loss = 0.14410625\n",
      "Iteration 86, loss = 0.02639033\n",
      "Iteration 22, loss = 0.14127444\n",
      "Iteration 87, loss = 0.02499193\n",
      "Iteration 23, loss = 0.13432376\n",
      "Iteration 88, loss = 0.02677718\n",
      "Iteration 24, loss = 0.12407984\n",
      "Iteration 89, loss = 0.02478489\n",
      "Iteration 25, loss = 0.11945852\n",
      "Iteration 90, loss = 0.03909128\n",
      "Iteration 26, loss = 0.12250863\n",
      "Iteration 91, loss = 0.04187774\n",
      "Iteration 27, loss = 0.13513972\n",
      "Iteration 92, loss = 0.04525699\n",
      "Iteration 28, loss = 0.13449427\n",
      "Iteration 93, loss = 0.03248179\n",
      "Iteration 29, loss = 0.13222448\n",
      "Iteration 94, loss = 0.03726960\n",
      "Iteration 30, loss = 0.13688791\n",
      "Iteration 95, loss = 0.03229717\n",
      "Iteration 31, loss = 0.12124888\n",
      "Iteration 96, loss = 0.03257301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.15102165\n",
      "Iteration 33, loss = 0.15579633\n",
      "Iteration 34, loss = 0.13345502\n",
      "Iteration 35, loss = 0.12990648\n",
      "Iteration 36, loss = 0.11048591\n",
      "Iteration 37, loss = 0.09431828\n",
      "Iteration 38, loss = 0.09331407\n",
      "Iteration 39, loss = 0.09719194\n",
      "Iteration 40, loss = 0.11942394\n",
      "Iteration 41, loss = 0.11975519\n",
      "Iteration 42, loss = 0.09977421\n",
      "Iteration 43, loss = 0.10024762\n",
      "Iteration 44, loss = 0.09261771\n",
      "Iteration 45, loss = 0.10455717\n",
      "Iteration 46, loss = 0.08626539\n",
      "Iteration 47, loss = 0.08414234\n",
      "Iteration 48, loss = 0.07860707\n",
      "Iteration 49, loss = 0.08121083\n",
      "Iteration 1, loss = 0.96875710\n",
      "Iteration 2, loss = 0.50281093\n",
      "Iteration 50, loss = 0.09766123\n",
      "Iteration 3, loss = 0.41393321\n",
      "Iteration 51, loss = 0.08627776\n",
      "Iteration 4, loss = 0.38623115\n",
      "Iteration 52, loss = 0.08316405\n",
      "Iteration 5, loss = 0.35572861\n",
      "Iteration 53, loss = 0.08254181\n",
      "Iteration 6, loss = 0.33743750\n",
      "Iteration 54, loss = 0.06746936\n",
      "Iteration 7, loss = 0.31897386\n",
      "Iteration 55, loss = 0.09884541\n",
      "Iteration 8, loss = 0.29932685\n",
      "Iteration 56, loss = 0.08494097\n",
      "Iteration 9, loss = 0.27858246\n",
      "Iteration 57, loss = 0.07944967\n",
      "Iteration 10, loss = 0.27324920\n",
      "Iteration 58, loss = 0.06584647\n",
      "Iteration 11, loss = 0.25812572\n",
      "Iteration 59, loss = 0.06163725\n",
      "Iteration 12, loss = 0.24970837\n",
      "Iteration 60, loss = 0.05931481\n",
      "Iteration 13, loss = 0.23244573\n",
      "Iteration 61, loss = 0.06680023\n",
      "Iteration 14, loss = 0.21847505\n",
      "Iteration 62, loss = 0.09493636\n",
      "Iteration 63, loss = 0.07517865\n",
      "Iteration 15, loss = 0.21809144\n",
      "Iteration 64, loss = 0.07860406\n",
      "Iteration 16, loss = 0.20596360\n",
      "Iteration 65, loss = 0.07123394\n",
      "Iteration 17, loss = 0.19979424\n",
      "Iteration 66, loss = 0.06570548\n",
      "Iteration 18, loss = 0.19003144\n",
      "Iteration 67, loss = 0.06968871\n",
      "Iteration 19, loss = 0.19088419\n",
      "Iteration 68, loss = 0.07861424\n",
      "Iteration 20, loss = 0.17410609\n",
      "Iteration 69, loss = 0.06518452\n",
      "Iteration 21, loss = 0.16572784\n",
      "Iteration 70, loss = 0.06884755\n",
      "Iteration 22, loss = 0.17318945\n",
      "Iteration 23, loss = 0.17161061\n",
      "Iteration 71, loss = 0.07495321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.15148094\n",
      "Iteration 25, loss = 0.15967716\n",
      "Iteration 26, loss = 0.15644561\n",
      "Iteration 27, loss = 0.15016071\n",
      "Iteration 28, loss = 0.15394469\n",
      "Iteration 29, loss = 0.15159016\n",
      "Iteration 30, loss = 0.15427548\n",
      "Iteration 31, loss = 0.13835638\n",
      "Iteration 32, loss = 0.14096608\n",
      "Iteration 33, loss = 0.15228285\n",
      "Iteration 34, loss = 0.12578488\n",
      "Iteration 35, loss = 0.13776425\n",
      "Iteration 36, loss = 0.13502463\n",
      "Iteration 37, loss = 0.11285790\n",
      "Iteration 38, loss = 0.09961503\n",
      "Iteration 39, loss = 0.11261669\n",
      "Iteration 40, loss = 0.12813643\n",
      "Iteration 1, loss = 1.11127144\n",
      "Iteration 41, loss = 0.11148617\n",
      "Iteration 42, loss = 0.10077624\n",
      "Iteration 2, loss = 0.63480104\n",
      "Iteration 43, loss = 0.08250400\n",
      "Iteration 3, loss = 0.44693462\n",
      "Iteration 44, loss = 0.09680292\n",
      "Iteration 4, loss = 0.39408825\n",
      "Iteration 45, loss = 0.08754714\n",
      "Iteration 5, loss = 0.36392177\n",
      "Iteration 46, loss = 0.09775216\n",
      "Iteration 6, loss = 0.33910572\n",
      "Iteration 47, loss = 0.08996547\n",
      "Iteration 7, loss = 0.31655421\n",
      "Iteration 48, loss = 0.08420510\n",
      "Iteration 8, loss = 0.31017587\n",
      "Iteration 49, loss = 0.08073327\n",
      "Iteration 9, loss = 0.28910890\n",
      "Iteration 50, loss = 0.09031330\n",
      "Iteration 10, loss = 0.27265442\n",
      "Iteration 51, loss = 0.10930024\n",
      "Iteration 11, loss = 0.26263979\n",
      "Iteration 12, loss = 0.24917427\n",
      "Iteration 52, loss = 0.11371696\n",
      "Iteration 13, loss = 0.24128110\n",
      "Iteration 53, loss = 0.12564201\n",
      "Iteration 14, loss = 0.22369450\n",
      "Iteration 54, loss = 0.09358175\n",
      "Iteration 15, loss = 0.21267758\n",
      "Iteration 55, loss = 0.09210133\n",
      "Iteration 16, loss = 0.20773343\n",
      "Iteration 56, loss = 0.08182839\n",
      "Iteration 17, loss = 0.19818929\n",
      "Iteration 57, loss = 0.07460494\n",
      "Iteration 18, loss = 0.19197425\n",
      "Iteration 58, loss = 0.07688878\n",
      "Iteration 19, loss = 0.18445535\n",
      "Iteration 59, loss = 0.06928234\n",
      "Iteration 20, loss = 0.17787602\n",
      "Iteration 60, loss = 0.06954512\n",
      "Iteration 21, loss = 0.17672875\n",
      "Iteration 61, loss = 0.06333667\n",
      "Iteration 22, loss = 0.17527794\n",
      "Iteration 62, loss = 0.05561780\n",
      "Iteration 23, loss = 0.16252975\n",
      "Iteration 63, loss = 0.06771994\n",
      "Iteration 24, loss = 0.16422904\n",
      "Iteration 64, loss = 0.07165032\n",
      "Iteration 25, loss = 0.16073835\n",
      "Iteration 65, loss = 0.07312836\n",
      "Iteration 26, loss = 0.14600442\n",
      "Iteration 66, loss = 0.05406732\n",
      "Iteration 27, loss = 0.13453571\n",
      "Iteration 67, loss = 0.06072613\n",
      "Iteration 28, loss = 0.16478180\n",
      "Iteration 68, loss = 0.06395469\n",
      "Iteration 29, loss = 0.14423881\n",
      "Iteration 69, loss = 0.05762703\n",
      "Iteration 30, loss = 0.15576649\n",
      "Iteration 70, loss = 0.05925466\n",
      "Iteration 31, loss = 0.13030450\n",
      "Iteration 71, loss = 0.05911584\n",
      "Iteration 32, loss = 0.12360875\n",
      "Iteration 72, loss = 0.05908484\n",
      "Iteration 33, loss = 0.11926913\n",
      "Iteration 73, loss = 0.06874892\n",
      "Iteration 34, loss = 0.11109912\n",
      "Iteration 74, loss = 0.06651959\n",
      "Iteration 35, loss = 0.10150374\n",
      "Iteration 75, loss = 0.06919950\n",
      "Iteration 36, loss = 0.09541575\n",
      "Iteration 76, loss = 0.09280427\n",
      "Iteration 37, loss = 0.09701473\n",
      "Iteration 77, loss = 0.08375969\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.09768411\n",
      "Iteration 39, loss = 0.10311472\n",
      "Iteration 40, loss = 0.09935503\n",
      "Iteration 41, loss = 0.08352205\n",
      "Iteration 42, loss = 0.08358601\n",
      "Iteration 43, loss = 0.08420085\n",
      "Iteration 44, loss = 0.07962838\n",
      "Iteration 45, loss = 0.07251457\n",
      "Iteration 46, loss = 0.08207336\n",
      "Iteration 47, loss = 0.07951473\n",
      "Iteration 48, loss = 0.07015280\n",
      "Iteration 49, loss = 0.07255346\n",
      "Iteration 50, loss = 0.07038391\n",
      "Iteration 51, loss = 0.06972841\n",
      "Iteration 52, loss = 0.08228916\n",
      "Iteration 53, loss = 0.08508165\n",
      "Iteration 54, loss = 0.09788290\n",
      "Iteration 55, loss = 0.15476509\n",
      "Iteration 1, loss = 0.99200892\n",
      "Iteration 56, loss = 0.14580659\n",
      "Iteration 2, loss = 0.55827702\n",
      "Iteration 57, loss = 0.15738105\n",
      "Iteration 3, loss = 0.41347106\n",
      "Iteration 58, loss = 0.13613478\n",
      "Iteration 4, loss = 0.38327451\n",
      "Iteration 59, loss = 0.11748174\n",
      "Iteration 5, loss = 0.35630633\n",
      "Iteration 60, loss = 0.11559591\n",
      "Iteration 6, loss = 0.32346153\n",
      "Iteration 61, loss = 0.12361562\n",
      "Iteration 7, loss = 0.30471978\n",
      "Iteration 62, loss = 0.08605183\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.28438415\n",
      "Iteration 9, loss = 0.26571996\n",
      "Iteration 10, loss = 0.25570225\n",
      "Iteration 11, loss = 0.23618971\n",
      "Iteration 12, loss = 0.23170718\n",
      "Iteration 13, loss = 0.21889097\n",
      "Iteration 14, loss = 0.20915808\n",
      "Iteration 15, loss = 0.20752142\n",
      "Iteration 16, loss = 0.18821544\n",
      "Iteration 17, loss = 0.17924675\n",
      "Iteration 18, loss = 0.17344070\n",
      "Iteration 19, loss = 0.18640288\n",
      "Iteration 20, loss = 0.18490357\n",
      "Iteration 21, loss = 0.19032138\n",
      "Iteration 22, loss = 0.18036545\n",
      "Iteration 23, loss = 0.16306309\n",
      "Iteration 24, loss = 0.16124873\n",
      "Iteration 25, loss = 0.14567975\n",
      "Iteration 1, loss = 1.11515921\n",
      "Iteration 26, loss = 0.13776699\n",
      "Iteration 2, loss = 0.58748852\n",
      "Iteration 27, loss = 0.12657107\n",
      "Iteration 3, loss = 0.46590064\n",
      "Iteration 28, loss = 0.13006790\n",
      "Iteration 4, loss = 0.41857207\n",
      "Iteration 29, loss = 0.13179204\n",
      "Iteration 5, loss = 0.37521642\n",
      "Iteration 30, loss = 0.12828030\n",
      "Iteration 6, loss = 0.35944832\n",
      "Iteration 31, loss = 0.12853136\n",
      "Iteration 7, loss = 0.32751041\n",
      "Iteration 32, loss = 0.11782179\n",
      "Iteration 8, loss = 0.31226726\n",
      "Iteration 33, loss = 0.13614336\n",
      "Iteration 9, loss = 0.29629440\n",
      "Iteration 34, loss = 0.16128370\n",
      "Iteration 10, loss = 0.28187139\n",
      "Iteration 35, loss = 0.14577997\n",
      "Iteration 11, loss = 0.26786323\n",
      "Iteration 36, loss = 0.11690547\n",
      "Iteration 12, loss = 0.25611243\n",
      "Iteration 37, loss = 0.13528861\n",
      "Iteration 13, loss = 0.24291978\n",
      "Iteration 38, loss = 0.11951229\n",
      "Iteration 14, loss = 0.23836334\n",
      "Iteration 39, loss = 0.09735879\n",
      "Iteration 15, loss = 0.22317588\n",
      "Iteration 40, loss = 0.08884030\n",
      "Iteration 16, loss = 0.21482719\n",
      "Iteration 41, loss = 0.08831243\n",
      "Iteration 17, loss = 0.20528477\n",
      "Iteration 42, loss = 0.08286920\n",
      "Iteration 18, loss = 0.19980647\n",
      "Iteration 43, loss = 0.08179089\n",
      "Iteration 19, loss = 0.18859988\n",
      "Iteration 44, loss = 0.07612376\n",
      "Iteration 20, loss = 0.18606061\n",
      "Iteration 45, loss = 0.08047616\n",
      "Iteration 21, loss = 0.17739746\n",
      "Iteration 46, loss = 0.07817690\n",
      "Iteration 22, loss = 0.17436777\n",
      "Iteration 47, loss = 0.07824535\n",
      "Iteration 23, loss = 0.16648609\n",
      "Iteration 48, loss = 0.06865952\n",
      "Iteration 24, loss = 0.15922321\n",
      "Iteration 49, loss = 0.06467548\n",
      "Iteration 25, loss = 0.16262311\n",
      "Iteration 50, loss = 0.06895554\n",
      "Iteration 26, loss = 0.15181310\n",
      "Iteration 51, loss = 0.07287638\n",
      "Iteration 27, loss = 0.14258078\n",
      "Iteration 52, loss = 0.07035533\n",
      "Iteration 28, loss = 0.13603323\n",
      "Iteration 53, loss = 0.07919880\n",
      "Iteration 29, loss = 0.14175803\n",
      "Iteration 54, loss = 0.08226107\n",
      "Iteration 30, loss = 0.14249509\n",
      "Iteration 55, loss = 0.08674635\n",
      "Iteration 31, loss = 0.14187751\n",
      "Iteration 56, loss = 0.06440449\n",
      "Iteration 32, loss = 0.15369304\n",
      "Iteration 57, loss = 0.06426227\n",
      "Iteration 33, loss = 0.13084805\n",
      "Iteration 58, loss = 0.06019667\n",
      "Iteration 34, loss = 0.11938640\n",
      "Iteration 59, loss = 0.05648275\n",
      "Iteration 35, loss = 0.11561527\n",
      "Iteration 60, loss = 0.05018381\n",
      "Iteration 36, loss = 0.11685171\n",
      "Iteration 61, loss = 0.06579345\n",
      "Iteration 37, loss = 0.12220580\n",
      "Iteration 62, loss = 0.05889722\n",
      "Iteration 38, loss = 0.10564562\n",
      "Iteration 63, loss = 0.05545436\n",
      "Iteration 39, loss = 0.10873304\n",
      "Iteration 64, loss = 0.05109739\n",
      "Iteration 40, loss = 0.09974566\n",
      "Iteration 65, loss = 0.05774616\n",
      "Iteration 41, loss = 0.09622222\n",
      "Iteration 66, loss = 0.04754403\n",
      "Iteration 42, loss = 0.11852747\n",
      "Iteration 67, loss = 0.05678805\n",
      "Iteration 43, loss = 0.11114157\n",
      "Iteration 68, loss = 0.04359326\n",
      "Iteration 44, loss = 0.10137325\n",
      "Iteration 69, loss = 0.04768669\n",
      "Iteration 45, loss = 0.09736084\n",
      "Iteration 70, loss = 0.04588597\n",
      "Iteration 46, loss = 0.09166471\n",
      "Iteration 71, loss = 0.04647338\n",
      "Iteration 72, loss = 0.05382135\n",
      "Iteration 47, loss = 0.08149486\n",
      "Iteration 73, loss = 0.03649890\n",
      "Iteration 48, loss = 0.08872314\n",
      "Iteration 74, loss = 0.04305026\n",
      "Iteration 49, loss = 0.07644688\n",
      "Iteration 75, loss = 0.04603062\n",
      "Iteration 50, loss = 0.08634237\n",
      "Iteration 76, loss = 0.04056198\n",
      "Iteration 51, loss = 0.09831805\n",
      "Iteration 77, loss = 0.05119924\n",
      "Iteration 52, loss = 0.10854781\n",
      "Iteration 78, loss = 0.04582631\n",
      "Iteration 53, loss = 0.07992344\n",
      "Iteration 79, loss = 0.06532690\n",
      "Iteration 54, loss = 0.08172315\n",
      "Iteration 80, loss = 0.06165417\n",
      "Iteration 55, loss = 0.08431439\n",
      "Iteration 81, loss = 0.07744998\n",
      "Iteration 56, loss = 0.08687292\n",
      "Iteration 82, loss = 0.06048890\n",
      "Iteration 57, loss = 0.07371869\n",
      "Iteration 83, loss = 0.06536818\n",
      "Iteration 58, loss = 0.06363668\n",
      "Iteration 84, loss = 0.07049201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.07674493\n",
      "Iteration 60, loss = 0.06363619\n",
      "Iteration 61, loss = 0.08062746\n",
      "Iteration 62, loss = 0.07104784\n",
      "Iteration 63, loss = 0.11677755\n",
      "Iteration 64, loss = 0.13696643\n",
      "Iteration 65, loss = 0.16141127\n",
      "Iteration 66, loss = 0.18384506\n",
      "Iteration 67, loss = 0.24087489\n",
      "Iteration 68, loss = 0.18891651\n",
      "Iteration 69, loss = 0.15799044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96026600\n",
      "Iteration 2, loss = 0.53393857\n",
      "Iteration 3, loss = 0.40839866\n",
      "Iteration 4, loss = 0.36696325\n",
      "Iteration 5, loss = 0.34420122\n",
      "Iteration 6, loss = 0.31914091\n",
      "Iteration 7, loss = 0.28284568\n",
      "Iteration 8, loss = 0.25879106\n",
      "Iteration 9, loss = 0.25870680\n",
      "Iteration 10, loss = 0.23396831\n",
      "Iteration 1, loss = 0.95674133\n",
      "Iteration 11, loss = 0.22409848\n",
      "Iteration 2, loss = 0.48707056\n",
      "Iteration 12, loss = 0.21521308\n",
      "Iteration 3, loss = 0.38103338\n",
      "Iteration 13, loss = 0.19332074\n",
      "Iteration 4, loss = 0.34970110\n",
      "Iteration 14, loss = 0.18284554\n",
      "Iteration 5, loss = 0.31804383\n",
      "Iteration 15, loss = 0.17750847\n",
      "Iteration 16, loss = 0.15998640\n",
      "Iteration 6, loss = 0.28375292\n",
      "Iteration 17, loss = 0.15061397Iteration 7, loss = 0.26259781\n",
      "\n",
      "Iteration 18, loss = 0.14016940\n",
      "Iteration 8, loss = 0.25091275\n",
      "Iteration 9, loss = 0.24667195\n",
      "Iteration 19, loss = 0.13227229\n",
      "Iteration 10, loss = 0.22521188\n",
      "Iteration 20, loss = 0.12702413\n",
      "Iteration 11, loss = 0.22560112\n",
      "Iteration 21, loss = 0.13948193\n",
      "Iteration 12, loss = 0.20833763\n",
      "Iteration 22, loss = 0.13453727\n",
      "Iteration 13, loss = 0.18189038\n",
      "Iteration 23, loss = 0.13137941\n",
      "Iteration 14, loss = 0.17737702\n",
      "Iteration 24, loss = 0.11240526\n",
      "Iteration 25, loss = 0.11923801\n",
      "Iteration 15, loss = 0.16585443\n",
      "Iteration 16, loss = 0.16386848Iteration 26, loss = 0.11904240\n",
      "\n",
      "Iteration 27, loss = 0.14719479\n",
      "Iteration 17, loss = 0.15400489\n",
      "Iteration 28, loss = 0.13937224\n",
      "Iteration 18, loss = 0.14746630\n",
      "Iteration 29, loss = 0.10796418\n",
      "Iteration 19, loss = 0.15225328\n",
      "Iteration 30, loss = 0.11781273\n",
      "Iteration 20, loss = 0.13184185\n",
      "Iteration 31, loss = 0.11113006\n",
      "Iteration 21, loss = 0.13705979\n",
      "Iteration 32, loss = 0.09163344\n",
      "Iteration 22, loss = 0.13694105\n",
      "Iteration 33, loss = 0.08194651\n",
      "Iteration 23, loss = 0.12499014\n",
      "Iteration 34, loss = 0.07402533\n",
      "Iteration 24, loss = 0.13899836\n",
      "Iteration 35, loss = 0.08415720\n",
      "Iteration 25, loss = 0.12695825\n",
      "Iteration 36, loss = 0.08386277\n",
      "Iteration 26, loss = 0.10371685\n",
      "Iteration 37, loss = 0.08447405\n",
      "Iteration 27, loss = 0.12544874\n",
      "Iteration 38, loss = 0.07808403\n",
      "Iteration 28, loss = 0.11085500\n",
      "Iteration 39, loss = 0.06791843\n",
      "Iteration 29, loss = 0.11341969\n",
      "Iteration 40, loss = 0.06453133\n",
      "Iteration 30, loss = 0.11515902\n",
      "Iteration 41, loss = 0.06485406\n",
      "Iteration 31, loss = 0.10574476\n",
      "Iteration 42, loss = 0.06240241\n",
      "Iteration 32, loss = 0.09869743\n",
      "Iteration 43, loss = 0.06886207\n",
      "Iteration 33, loss = 0.09021353\n",
      "Iteration 44, loss = 0.05734752\n",
      "Iteration 34, loss = 0.08508431\n",
      "Iteration 45, loss = 0.05606055\n",
      "Iteration 35, loss = 0.09914207\n",
      "Iteration 46, loss = 0.06189559\n",
      "Iteration 36, loss = 0.08676593\n",
      "Iteration 47, loss = 0.07069606\n",
      "Iteration 37, loss = 0.08674701\n",
      "Iteration 48, loss = 0.06091324\n",
      "Iteration 38, loss = 0.08689719\n",
      "Iteration 49, loss = 0.06770501\n",
      "Iteration 39, loss = 0.10304577\n",
      "Iteration 50, loss = 0.04683233\n",
      "Iteration 40, loss = 0.09632448\n",
      "Iteration 51, loss = 0.04194171\n",
      "Iteration 41, loss = 0.10290585\n",
      "Iteration 52, loss = 0.04481476\n",
      "Iteration 42, loss = 0.08657752\n",
      "Iteration 53, loss = 0.04783346\n",
      "Iteration 43, loss = 0.10090972\n",
      "Iteration 54, loss = 0.07492597\n",
      "Iteration 44, loss = 0.08639223\n",
      "Iteration 55, loss = 0.05711949\n",
      "Iteration 45, loss = 0.08123749\n",
      "Iteration 56, loss = 0.06869412\n",
      "Iteration 46, loss = 0.09117011\n",
      "Iteration 57, loss = 0.05255205\n",
      "Iteration 47, loss = 0.10840107\n",
      "Iteration 58, loss = 0.04031503\n",
      "Iteration 48, loss = 0.09013901\n",
      "Iteration 59, loss = 0.04177233\n",
      "Iteration 49, loss = 0.13717119\n",
      "Iteration 60, loss = 0.03796201\n",
      "Iteration 50, loss = 0.11250933\n",
      "Iteration 61, loss = 0.04799703\n",
      "Iteration 51, loss = 0.09058807\n",
      "Iteration 62, loss = 0.04515854\n",
      "Iteration 52, loss = 0.08469390\n",
      "Iteration 63, loss = 0.03855029\n",
      "Iteration 53, loss = 0.08476613\n",
      "Iteration 64, loss = 0.04218632\n",
      "Iteration 54, loss = 0.07299728\n",
      "Iteration 65, loss = 0.03269505\n",
      "Iteration 55, loss = 0.06543032\n",
      "Iteration 66, loss = 0.04949602\n",
      "Iteration 56, loss = 0.06776192\n",
      "Iteration 67, loss = 0.03904278\n",
      "Iteration 57, loss = 0.05955850\n",
      "Iteration 68, loss = 0.03932464\n",
      "Iteration 58, loss = 0.06059227\n",
      "Iteration 69, loss = 0.03121217\n",
      "Iteration 59, loss = 0.06103464\n",
      "Iteration 70, loss = 0.03120966\n",
      "Iteration 60, loss = 0.08781732\n",
      "Iteration 71, loss = 0.02952426\n",
      "Iteration 61, loss = 0.07338744\n",
      "Iteration 72, loss = 0.02490938\n",
      "Iteration 62, loss = 0.06589511\n",
      "Iteration 73, loss = 0.03093763\n",
      "Iteration 63, loss = 0.07832384\n",
      "Iteration 74, loss = 0.03847633\n",
      "Iteration 64, loss = 0.06888864\n",
      "Iteration 75, loss = 0.05924333\n",
      "Iteration 65, loss = 0.05879229\n",
      "Iteration 76, loss = 0.03567027\n",
      "Iteration 66, loss = 0.06479989\n",
      "Iteration 77, loss = 0.06633474\n",
      "Iteration 67, loss = 0.06828983\n",
      "Iteration 78, loss = 0.10769758\n",
      "Iteration 68, loss = 0.15254211\n",
      "Iteration 79, loss = 0.13630819\n",
      "Iteration 69, loss = 0.15119148\n",
      "Iteration 80, loss = 0.11490104\n",
      "Iteration 70, loss = 0.13927217\n",
      "Iteration 81, loss = 0.08206975\n",
      "Iteration 71, loss = 0.11903668\n",
      "Iteration 82, loss = 0.07347922\n",
      "Iteration 72, loss = 0.08804645\n",
      "Iteration 83, loss = 0.04365168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 73, loss = 0.06369370\n",
      "Iteration 74, loss = 0.07070056\n",
      "Iteration 75, loss = 0.05168231\n",
      "Iteration 76, loss = 0.07035037\n",
      "Iteration 77, loss = 0.07243406\n",
      "Iteration 78, loss = 0.10350248\n",
      "Iteration 79, loss = 0.06539588\n",
      "Iteration 80, loss = 0.06292120\n",
      "Iteration 81, loss = 0.05350105\n",
      "Iteration 82, loss = 0.04417970\n",
      "Iteration 83, loss = 0.06091791\n",
      "Iteration 84, loss = 0.05276517\n",
      "Iteration 85, loss = 0.08308042\n",
      "Iteration 86, loss = 0.06186733\n",
      "Iteration 87, loss = 0.06522592\n",
      "Iteration 88, loss = 0.06778229\n",
      "Iteration 89, loss = 0.05585099\n",
      "Iteration 90, loss = 0.04549202\n",
      "Iteration 1, loss = 0.85898919\n",
      "Iteration 91, loss = 0.03517092\n",
      "Iteration 2, loss = 0.46030353\n",
      "Iteration 92, loss = 0.03457700\n",
      "Iteration 3, loss = 0.39911209\n",
      "Iteration 93, loss = 0.03794106\n",
      "Iteration 4, loss = 0.34918207\n",
      "Iteration 94, loss = 0.02925059\n",
      "Iteration 5, loss = 0.31818497\n",
      "Iteration 95, loss = 0.02935670\n",
      "Iteration 6, loss = 0.29462631\n",
      "Iteration 96, loss = 0.02976686\n",
      "Iteration 7, loss = 0.27725194\n",
      "Iteration 97, loss = 0.03062061\n",
      "Iteration 8, loss = 0.26331334\n",
      "Iteration 98, loss = 0.03010338\n",
      "Iteration 9, loss = 0.25638570\n",
      "Iteration 99, loss = 0.03369158\n",
      "Iteration 10, loss = 0.23512626\n",
      "Iteration 100, loss = 0.02950632\n",
      "Iteration 11, loss = 0.22971399\n",
      "Iteration 12, loss = 0.21629177\n",
      "Iteration 13, loss = 0.20158728\n",
      "Iteration 14, loss = 0.19425621\n",
      "Iteration 15, loss = 0.18132052\n",
      "Iteration 16, loss = 0.18089947\n",
      "Iteration 17, loss = 0.18469922\n",
      "Iteration 18, loss = 0.16820565\n",
      "Iteration 19, loss = 0.16309385\n",
      "Iteration 20, loss = 0.15739092\n",
      "Iteration 21, loss = 0.14403390\n",
      "Iteration 22, loss = 0.16318739\n",
      "Iteration 23, loss = 0.13921097\n",
      "Iteration 24, loss = 0.15831271\n",
      "Iteration 25, loss = 0.15762261\n",
      "Iteration 26, loss = 0.19183383\n",
      "Iteration 27, loss = 0.16818021\n",
      "Iteration 28, loss = 0.12525941\n",
      "Iteration 1, loss = 1.03986353\n",
      "Iteration 29, loss = 0.11822114\n",
      "Iteration 2, loss = 0.54148833\n",
      "Iteration 30, loss = 0.12058962\n",
      "Iteration 3, loss = 0.43256039\n",
      "Iteration 31, loss = 0.12471208\n",
      "Iteration 4, loss = 0.36802175\n",
      "Iteration 32, loss = 0.11584092\n",
      "Iteration 5, loss = 0.33795618\n",
      "Iteration 33, loss = 0.11561664\n",
      "Iteration 6, loss = 0.31804712\n",
      "Iteration 34, loss = 0.11859453\n",
      "Iteration 7, loss = 0.29584092\n",
      "Iteration 35, loss = 0.09692065\n",
      "Iteration 8, loss = 0.27543591\n",
      "Iteration 36, loss = 0.11300525\n",
      "Iteration 9, loss = 0.27474251\n",
      "Iteration 37, loss = 0.11322470\n",
      "Iteration 10, loss = 0.26104386\n",
      "Iteration 38, loss = 0.12340189\n",
      "Iteration 11, loss = 0.25177765\n",
      "Iteration 39, loss = 0.09087394\n",
      "Iteration 12, loss = 0.22532828\n",
      "Iteration 40, loss = 0.08976136\n",
      "Iteration 13, loss = 0.20998934\n",
      "Iteration 41, loss = 0.09287697\n",
      "Iteration 14, loss = 0.20447517\n",
      "Iteration 42, loss = 0.07501145\n",
      "Iteration 15, loss = 0.19964958\n",
      "Iteration 43, loss = 0.10349967\n",
      "Iteration 16, loss = 0.18070646\n",
      "Iteration 44, loss = 0.07761827\n",
      "Iteration 17, loss = 0.18385572\n",
      "Iteration 45, loss = 0.08668250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.17201585\n",
      "Iteration 46, loss = 0.10338396\n",
      "Iteration 19, loss = 0.17624171\n",
      "Iteration 47, loss = 0.09579716\n",
      "Iteration 20, loss = 0.16157745\n",
      "Iteration 48, loss = 0.08918930\n",
      "Iteration 21, loss = 0.16469709\n",
      "Iteration 49, loss = 0.11584089\n",
      "Iteration 22, loss = 0.14377059\n",
      "Iteration 50, loss = 0.09956449\n",
      "Iteration 23, loss = 0.14766129\n",
      "Iteration 51, loss = 0.10198452\n",
      "Iteration 24, loss = 0.13428478\n",
      "Iteration 52, loss = 0.11667433\n",
      "Iteration 25, loss = 0.13733938\n",
      "Iteration 53, loss = 0.07258077\n",
      "Iteration 26, loss = 0.14006139\n",
      "Iteration 54, loss = 0.09979103\n",
      "Iteration 27, loss = 0.13380285\n",
      "Iteration 55, loss = 0.08963656\n",
      "Iteration 28, loss = 0.13179915\n",
      "Iteration 56, loss = 0.07792832\n",
      "Iteration 29, loss = 0.11305975\n",
      "Iteration 57, loss = 0.07277048\n",
      "Iteration 30, loss = 0.12233353\n",
      "Iteration 58, loss = 0.06712768\n",
      "Iteration 31, loss = 0.12204702\n",
      "Iteration 59, loss = 0.06661423\n",
      "Iteration 32, loss = 0.12912645\n",
      "Iteration 60, loss = 0.10523448\n",
      "Iteration 33, loss = 0.11292652\n",
      "Iteration 61, loss = 0.07139656\n",
      "Iteration 34, loss = 0.09812721\n",
      "Iteration 62, loss = 0.07601108\n",
      "Iteration 35, loss = 0.09809958\n",
      "Iteration 63, loss = 0.07345265\n",
      "Iteration 36, loss = 0.11788585\n",
      "Iteration 64, loss = 0.07039272\n",
      "Iteration 37, loss = 0.10693967\n",
      "Iteration 65, loss = 0.07401112\n",
      "Iteration 38, loss = 0.10863241\n",
      "Iteration 66, loss = 0.09549688\n",
      "Iteration 39, loss = 0.10766771\n",
      "Iteration 67, loss = 0.07285002\n",
      "Iteration 40, loss = 0.11841602\n",
      "Iteration 68, loss = 0.05729409\n",
      "Iteration 41, loss = 0.11041660\n",
      "Iteration 69, loss = 0.05909214\n",
      "Iteration 70, loss = 0.04729825\n",
      "Iteration 42, loss = 0.16473373\n",
      "Iteration 71, loss = 0.04801253\n",
      "Iteration 43, loss = 0.14860187\n",
      "Iteration 72, loss = 0.04586394\n",
      "Iteration 44, loss = 0.10541800\n",
      "Iteration 73, loss = 0.04757708\n",
      "Iteration 45, loss = 0.11328196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 74, loss = 0.04926322\n",
      "Iteration 75, loss = 0.03714289\n",
      "Iteration 76, loss = 0.03966170\n",
      "Iteration 77, loss = 0.03950079\n",
      "Iteration 78, loss = 0.04716816\n",
      "Iteration 79, loss = 0.04051564\n",
      "Iteration 80, loss = 0.06170991\n",
      "Iteration 81, loss = 0.07292920\n",
      "Iteration 82, loss = 0.06867099\n",
      "Iteration 83, loss = 0.05659233\n",
      "Iteration 84, loss = 0.03987599\n",
      "Iteration 85, loss = 0.04011556\n",
      "Iteration 86, loss = 0.04989168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.80914774\n",
      "Iteration 2, loss = 0.45226059\n",
      "Iteration 3, loss = 0.38770347\n",
      "Iteration 4, loss = 0.34629324\n",
      "Iteration 5, loss = 0.31793953\n",
      "Iteration 6, loss = 0.30349610\n",
      "Iteration 7, loss = 0.28813996\n",
      "Iteration 8, loss = 0.27244035\n",
      "Iteration 9, loss = 0.25121179\n",
      "Iteration 10, loss = 0.24434977\n",
      "Iteration 11, loss = 0.22867552\n",
      "Iteration 12, loss = 0.22284923\n",
      "Iteration 13, loss = 0.21629303\n",
      "Iteration 1, loss = 1.00154532\n",
      "Iteration 14, loss = 0.21396856\n",
      "Iteration 2, loss = 0.59010730\n",
      "Iteration 15, loss = 0.20062694\n",
      "Iteration 3, loss = 0.45321355\n",
      "Iteration 16, loss = 0.19753761\n",
      "Iteration 4, loss = 0.39406314\n",
      "Iteration 17, loss = 0.20034336\n",
      "Iteration 5, loss = 0.35694158\n",
      "Iteration 18, loss = 0.19773711\n",
      "Iteration 6, loss = 0.33700103\n",
      "Iteration 19, loss = 0.17098969\n",
      "Iteration 7, loss = 0.30619360\n",
      "Iteration 20, loss = 0.15811831\n",
      "Iteration 8, loss = 0.28284793\n",
      "Iteration 21, loss = 0.14373349\n",
      "Iteration 9, loss = 0.26900842\n",
      "Iteration 22, loss = 0.15800149\n",
      "Iteration 10, loss = 0.26631324\n",
      "Iteration 23, loss = 0.13071508\n",
      "Iteration 11, loss = 0.24394443\n",
      "Iteration 24, loss = 0.13252069\n",
      "Iteration 12, loss = 0.24484739\n",
      "Iteration 25, loss = 0.13272243\n",
      "Iteration 13, loss = 0.22029566\n",
      "Iteration 26, loss = 0.12873012\n",
      "Iteration 14, loss = 0.21430094\n",
      "Iteration 27, loss = 0.11763438\n",
      "Iteration 15, loss = 0.21538926\n",
      "Iteration 28, loss = 0.13043894\n",
      "Iteration 16, loss = 0.19321680\n",
      "Iteration 29, loss = 0.11700045\n",
      "Iteration 17, loss = 0.18395960\n",
      "Iteration 30, loss = 0.11048890\n",
      "Iteration 18, loss = 0.16962777\n",
      "Iteration 31, loss = 0.12532377\n",
      "Iteration 19, loss = 0.16334064\n",
      "Iteration 32, loss = 0.11441702\n",
      "Iteration 20, loss = 0.16185459\n",
      "Iteration 33, loss = 0.11570467\n",
      "Iteration 21, loss = 0.15211814\n",
      "Iteration 34, loss = 0.11477399\n",
      "Iteration 22, loss = 0.15314237\n",
      "Iteration 35, loss = 0.09906406\n",
      "Iteration 23, loss = 0.14718978\n",
      "Iteration 36, loss = 0.09555657\n",
      "Iteration 24, loss = 0.13550225\n",
      "Iteration 37, loss = 0.09886777\n",
      "Iteration 25, loss = 0.13531121\n",
      "Iteration 38, loss = 0.09942956\n",
      "Iteration 26, loss = 0.12646689\n",
      "Iteration 39, loss = 0.09587541\n",
      "Iteration 27, loss = 0.11482900\n",
      "Iteration 40, loss = 0.08714897\n",
      "Iteration 28, loss = 0.13392357\n",
      "Iteration 41, loss = 0.07428960\n",
      "Iteration 29, loss = 0.13625006\n",
      "Iteration 42, loss = 0.08301979\n",
      "Iteration 30, loss = 0.11880907\n",
      "Iteration 43, loss = 0.07821735\n",
      "Iteration 31, loss = 0.11497468\n",
      "Iteration 44, loss = 0.10092498\n",
      "Iteration 32, loss = 0.12476718\n",
      "Iteration 45, loss = 0.11152869\n",
      "Iteration 33, loss = 0.11615736\n",
      "Iteration 46, loss = 0.14031607\n",
      "Iteration 34, loss = 0.10014593\n",
      "Iteration 47, loss = 0.13010706\n",
      "Iteration 35, loss = 0.12085778\n",
      "Iteration 48, loss = 0.13162692\n",
      "Iteration 36, loss = 0.13124904\n",
      "Iteration 49, loss = 0.10212234\n",
      "Iteration 37, loss = 0.14404051\n",
      "Iteration 50, loss = 0.07052860\n",
      "Iteration 38, loss = 0.13085593\n",
      "Iteration 51, loss = 0.07124758\n",
      "Iteration 39, loss = 0.11228442\n",
      "Iteration 52, loss = 0.06642972\n",
      "Iteration 40, loss = 0.10925655\n",
      "Iteration 53, loss = 0.06035434\n",
      "Iteration 41, loss = 0.09758180\n",
      "Iteration 54, loss = 0.05808316\n",
      "Iteration 42, loss = 0.08819631\n",
      "Iteration 55, loss = 0.06446700\n",
      "Iteration 43, loss = 0.07772367\n",
      "Iteration 56, loss = 0.07070679\n",
      "Iteration 44, loss = 0.08236981\n",
      "Iteration 57, loss = 0.07532690\n",
      "Iteration 45, loss = 0.08052342\n",
      "Iteration 58, loss = 0.07989206\n",
      "Iteration 46, loss = 0.07849190\n",
      "Iteration 59, loss = 0.08093896\n",
      "Iteration 47, loss = 0.08975811\n",
      "Iteration 60, loss = 0.10125331\n",
      "Iteration 48, loss = 0.11520498\n",
      "Iteration 61, loss = 0.09268861\n",
      "Iteration 49, loss = 0.07421844\n",
      "Iteration 62, loss = 0.08433602\n",
      "Iteration 50, loss = 0.10119335\n",
      "Iteration 63, loss = 0.06341917\n",
      "Iteration 51, loss = 0.08271956\n",
      "Iteration 64, loss = 0.06195038\n",
      "Iteration 52, loss = 0.07734312\n",
      "Iteration 65, loss = 0.06286817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.06607711\n",
      "Iteration 54, loss = 0.10536243\n",
      "Iteration 55, loss = 0.09943358\n",
      "Iteration 56, loss = 0.10469314\n",
      "Iteration 57, loss = 0.07126759\n",
      "Iteration 58, loss = 0.06454387\n",
      "Iteration 59, loss = 0.06325605\n",
      "Iteration 60, loss = 0.06301227\n",
      "Iteration 61, loss = 0.06226258\n",
      "Iteration 62, loss = 0.06820934\n",
      "Iteration 63, loss = 0.08073001\n",
      "Iteration 64, loss = 0.07875892\n",
      "Iteration 65, loss = 0.04962152\n",
      "Iteration 66, loss = 0.05356013\n",
      "Iteration 67, loss = 0.05091733\n",
      "Iteration 68, loss = 0.04139626\n",
      "Iteration 69, loss = 0.04043694\n",
      "Iteration 70, loss = 0.03951037\n",
      "Iteration 1, loss = 0.98459356\n",
      "Iteration 71, loss = 0.05223552\n",
      "Iteration 2, loss = 0.51928594\n",
      "Iteration 72, loss = 0.05476642\n",
      "Iteration 3, loss = 0.43100844\n",
      "Iteration 73, loss = 0.06060112\n",
      "Iteration 4, loss = 0.38575442\n",
      "Iteration 74, loss = 0.05528280\n",
      "Iteration 5, loss = 0.34510594\n",
      "Iteration 75, loss = 0.05210473\n",
      "Iteration 6, loss = 0.32380794\n",
      "Iteration 76, loss = 0.05192858\n",
      "Iteration 7, loss = 0.29926730\n",
      "Iteration 77, loss = 0.04918497\n",
      "Iteration 8, loss = 0.28418550\n",
      "Iteration 78, loss = 0.04595083\n",
      "Iteration 9, loss = 0.26679817\n",
      "Iteration 79, loss = 0.04402049\n",
      "Iteration 10, loss = 0.26106299\n",
      "Iteration 80, loss = 0.04391349\n",
      "Iteration 11, loss = 0.25264888\n",
      "Iteration 81, loss = 0.03889588\n",
      "Iteration 12, loss = 0.26033682\n",
      "Iteration 82, loss = 0.06116200\n",
      "Iteration 13, loss = 0.23183409\n",
      "Iteration 83, loss = 0.06817203\n",
      "Iteration 84, loss = 0.07774978\n",
      "Iteration 14, loss = 0.22776452\n",
      "Iteration 85, loss = 0.07388363\n",
      "Iteration 15, loss = 0.21377489\n",
      "Iteration 86, loss = 0.08905586\n",
      "Iteration 16, loss = 0.20599089\n",
      "Iteration 87, loss = 0.08792246\n",
      "Iteration 17, loss = 0.19190688\n",
      "Iteration 88, loss = 0.05855456\n",
      "Iteration 18, loss = 0.18629330\n",
      "Iteration 89, loss = 0.06296919\n",
      "Iteration 19, loss = 0.18506516\n",
      "Iteration 90, loss = 0.06109903\n",
      "Iteration 20, loss = 0.20772074\n",
      "Iteration 91, loss = 0.06699981\n",
      "Iteration 21, loss = 0.17848369\n",
      "Iteration 92, loss = 0.06916322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.16031182\n",
      "Iteration 23, loss = 0.15067735\n",
      "Iteration 24, loss = 0.17192375\n",
      "Iteration 25, loss = 0.16129851\n",
      "Iteration 26, loss = 0.15122288\n",
      "Iteration 27, loss = 0.14389850\n",
      "Iteration 28, loss = 0.14001592\n",
      "Iteration 29, loss = 0.11819354\n",
      "Iteration 30, loss = 0.13009512\n",
      "Iteration 31, loss = 0.12011650\n",
      "Iteration 32, loss = 0.11508396\n",
      "Iteration 33, loss = 0.13666500\n",
      "Iteration 34, loss = 0.11130606\n",
      "Iteration 35, loss = 0.11630873\n",
      "Iteration 36, loss = 0.10960731\n",
      "Iteration 37, loss = 0.10904943\n",
      "Iteration 38, loss = 0.14307036\n",
      "Iteration 39, loss = 0.16042882\n",
      "Iteration 1, loss = 0.97589555\n",
      "Iteration 40, loss = 0.12416971\n",
      "Iteration 2, loss = 0.51727086\n",
      "Iteration 41, loss = 0.14404527\n",
      "Iteration 3, loss = 0.43614379\n",
      "Iteration 42, loss = 0.10578610\n",
      "Iteration 4, loss = 0.39034232\n",
      "Iteration 43, loss = 0.09432829\n",
      "Iteration 5, loss = 0.35993834\n",
      "Iteration 44, loss = 0.09709217\n",
      "Iteration 6, loss = 0.32060745\n",
      "Iteration 45, loss = 0.10033017\n",
      "Iteration 7, loss = 0.30227741\n",
      "Iteration 46, loss = 0.08782262\n",
      "Iteration 8, loss = 0.29199517\n",
      "Iteration 47, loss = 0.08559801\n",
      "Iteration 9, loss = 0.27404764\n",
      "Iteration 48, loss = 0.11895983\n",
      "Iteration 10, loss = 0.26515196\n",
      "Iteration 49, loss = 0.11951842\n",
      "Iteration 11, loss = 0.25697868\n",
      "Iteration 50, loss = 0.19699498\n",
      "Iteration 12, loss = 0.25220277\n",
      "Iteration 51, loss = 0.15404472\n",
      "Iteration 13, loss = 0.23321799\n",
      "Iteration 52, loss = 0.11777910\n",
      "Iteration 14, loss = 0.22815851\n",
      "Iteration 53, loss = 0.10178311\n",
      "Iteration 15, loss = 0.22226051\n",
      "Iteration 54, loss = 0.09535650\n",
      "Iteration 16, loss = 0.22026487\n",
      "Iteration 55, loss = 0.10683992\n",
      "Iteration 17, loss = 0.21167729\n",
      "Iteration 56, loss = 0.08386622\n",
      "Iteration 18, loss = 0.19235976\n",
      "Iteration 57, loss = 0.08186049\n",
      "Iteration 19, loss = 0.18429702\n",
      "Iteration 58, loss = 0.07401079\n",
      "Iteration 20, loss = 0.16718720\n",
      "Iteration 59, loss = 0.07640839\n",
      "Iteration 21, loss = 0.19010938\n",
      "Iteration 60, loss = 0.07723680\n",
      "Iteration 22, loss = 0.18166572\n",
      "Iteration 61, loss = 0.07453917\n",
      "Iteration 23, loss = 0.18009489\n",
      "Iteration 62, loss = 0.06404437\n",
      "Iteration 24, loss = 0.16403934\n",
      "Iteration 63, loss = 0.05780188\n",
      "Iteration 25, loss = 0.15637658\n",
      "Iteration 64, loss = 0.06228805\n",
      "Iteration 26, loss = 0.15215711\n",
      "Iteration 65, loss = 0.06740436\n",
      "Iteration 27, loss = 0.13053840\n",
      "Iteration 66, loss = 0.07040920\n",
      "Iteration 28, loss = 0.12275996\n",
      "Iteration 67, loss = 0.06373430\n",
      "Iteration 29, loss = 0.12175500\n",
      "Iteration 68, loss = 0.06157030\n",
      "Iteration 30, loss = 0.11887025\n",
      "Iteration 69, loss = 0.07545016\n",
      "Iteration 31, loss = 0.13719710\n",
      "Iteration 70, loss = 0.06907214\n",
      "Iteration 32, loss = 0.13324475\n",
      "Iteration 71, loss = 0.07249421\n",
      "Iteration 33, loss = 0.14098068\n",
      "Iteration 72, loss = 0.07598279\n",
      "Iteration 34, loss = 0.13806088\n",
      "Iteration 73, loss = 0.07321311\n",
      "Iteration 35, loss = 0.15181769\n",
      "Iteration 74, loss = 0.06415572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.12192581\n",
      "Iteration 37, loss = 0.11465041\n",
      "Iteration 38, loss = 0.09881011\n",
      "Iteration 39, loss = 0.09965777\n",
      "Iteration 40, loss = 0.09600989\n",
      "Iteration 41, loss = 0.08852222\n",
      "Iteration 42, loss = 0.09095170\n",
      "Iteration 43, loss = 0.08643236\n",
      "Iteration 44, loss = 0.10184047\n",
      "Iteration 45, loss = 0.10102724\n",
      "Iteration 46, loss = 0.09053250\n",
      "Iteration 47, loss = 0.10498255\n",
      "Iteration 48, loss = 0.09365916\n",
      "Iteration 49, loss = 0.09793896\n",
      "Iteration 50, loss = 0.08470463\n",
      "Iteration 51, loss = 0.10984174\n",
      "Iteration 52, loss = 0.12216802\n",
      "Iteration 53, loss = 0.16038430\n",
      "Iteration 1, loss = 0.88691378\n",
      "Iteration 54, loss = 0.11381338\n",
      "Iteration 2, loss = 0.52227366\n",
      "Iteration 55, loss = 0.11077842\n",
      "Iteration 3, loss = 0.41756975\n",
      "Iteration 56, loss = 0.10281224\n",
      "Iteration 4, loss = 0.37405374\n",
      "Iteration 57, loss = 0.10354429\n",
      "Iteration 5, loss = 0.34512559\n",
      "Iteration 58, loss = 0.10925088\n",
      "Iteration 6, loss = 0.32136185\n",
      "Iteration 59, loss = 0.11708672\n",
      "Iteration 7, loss = 0.29825570\n",
      "Iteration 60, loss = 0.11872404\n",
      "Iteration 8, loss = 0.28046055\n",
      "Iteration 61, loss = 0.10142641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.26960681\n",
      "Iteration 10, loss = 0.25525748\n",
      "Iteration 11, loss = 0.24108082\n",
      "Iteration 12, loss = 0.22835604\n",
      "Iteration 13, loss = 0.21571224\n",
      "Iteration 14, loss = 0.20581985\n",
      "Iteration 15, loss = 0.20425204\n",
      "Iteration 16, loss = 0.19347500\n",
      "Iteration 17, loss = 0.17953931\n",
      "Iteration 18, loss = 0.17755846\n",
      "Iteration 19, loss = 0.19589736\n",
      "Iteration 20, loss = 0.20139116\n",
      "Iteration 21, loss = 0.17973904\n",
      "Iteration 22, loss = 0.18007317\n",
      "Iteration 23, loss = 0.14940475\n",
      "Iteration 24, loss = 0.15007945\n",
      "Iteration 25, loss = 0.13990399\n",
      "Iteration 26, loss = 0.14001210\n",
      "Iteration 1, loss = 0.96759948\n",
      "Iteration 27, loss = 0.13884157\n",
      "Iteration 2, loss = 0.55205872\n",
      "Iteration 28, loss = 0.12661610\n",
      "Iteration 3, loss = 0.42883567\n",
      "Iteration 29, loss = 0.13047348\n",
      "Iteration 4, loss = 0.38743240\n",
      "Iteration 30, loss = 0.12517426\n",
      "Iteration 5, loss = 0.36133494\n",
      "Iteration 31, loss = 0.12273405\n",
      "Iteration 6, loss = 0.32111096\n",
      "Iteration 32, loss = 0.13374978\n",
      "Iteration 7, loss = 0.30842885\n",
      "Iteration 33, loss = 0.13661774\n",
      "Iteration 8, loss = 0.28453956\n",
      "Iteration 34, loss = 0.13184421\n",
      "Iteration 9, loss = 0.27035164\n",
      "Iteration 35, loss = 0.11175837\n",
      "Iteration 10, loss = 0.25572357\n",
      "Iteration 36, loss = 0.09710818\n",
      "Iteration 11, loss = 0.24481202\n",
      "Iteration 37, loss = 0.09398592\n",
      "Iteration 12, loss = 0.23522393\n",
      "Iteration 38, loss = 0.08724828\n",
      "Iteration 13, loss = 0.23162358\n",
      "Iteration 39, loss = 0.08887366\n",
      "Iteration 14, loss = 0.21449396\n",
      "Iteration 40, loss = 0.09165110\n",
      "Iteration 15, loss = 0.21213366\n",
      "Iteration 41, loss = 0.08268013\n",
      "Iteration 16, loss = 0.19887363\n",
      "Iteration 42, loss = 0.09424680\n",
      "Iteration 17, loss = 0.19055155\n",
      "Iteration 43, loss = 0.08800485\n",
      "Iteration 18, loss = 0.19908521\n",
      "Iteration 44, loss = 0.09635211\n",
      "Iteration 19, loss = 0.18561360\n",
      "Iteration 45, loss = 0.08323330\n",
      "Iteration 20, loss = 0.17087907\n",
      "Iteration 46, loss = 0.09877794\n",
      "Iteration 21, loss = 0.17695679\n",
      "Iteration 47, loss = 0.10273597\n",
      "Iteration 22, loss = 0.17068685\n",
      "Iteration 48, loss = 0.09743369\n",
      "Iteration 23, loss = 0.15352659\n",
      "Iteration 49, loss = 0.09883836\n",
      "Iteration 24, loss = 0.14756091\n",
      "Iteration 50, loss = 0.07391712\n",
      "Iteration 25, loss = 0.14479381\n",
      "Iteration 51, loss = 0.08352070\n",
      "Iteration 26, loss = 0.16496694\n",
      "Iteration 52, loss = 0.10029941\n",
      "Iteration 27, loss = 0.16365071\n",
      "Iteration 53, loss = 0.10763662\n",
      "Iteration 28, loss = 0.15236609\n",
      "Iteration 54, loss = 0.11503153\n",
      "Iteration 29, loss = 0.13648061\n",
      "Iteration 55, loss = 0.17075388\n",
      "Iteration 30, loss = 0.12488925\n",
      "Iteration 56, loss = 0.13377562\n",
      "Iteration 31, loss = 0.11961906\n",
      "Iteration 57, loss = 0.10436133\n",
      "Iteration 32, loss = 0.12072538\n",
      "Iteration 58, loss = 0.08779912\n",
      "Iteration 33, loss = 0.11140755\n",
      "Iteration 59, loss = 0.08242195\n",
      "Iteration 34, loss = 0.10769892\n",
      "Iteration 60, loss = 0.08672912\n",
      "Iteration 35, loss = 0.10664387\n",
      "Iteration 61, loss = 0.06929675\n",
      "Iteration 36, loss = 0.09340724\n",
      "Iteration 62, loss = 0.06949160\n",
      "Iteration 37, loss = 0.09165565\n",
      "Iteration 63, loss = 0.06702101\n",
      "Iteration 38, loss = 0.10219156\n",
      "Iteration 64, loss = 0.07393077\n",
      "Iteration 39, loss = 0.10292164\n",
      "Iteration 65, loss = 0.05595175\n",
      "Iteration 40, loss = 0.10860969\n",
      "Iteration 66, loss = 0.05281749\n",
      "Iteration 41, loss = 0.10435289\n",
      "Iteration 67, loss = 0.04749620\n",
      "Iteration 42, loss = 0.10074798\n",
      "Iteration 68, loss = 0.04682096\n",
      "Iteration 43, loss = 0.10973642\n",
      "Iteration 69, loss = 0.04412954\n",
      "Iteration 44, loss = 0.10523513\n",
      "Iteration 70, loss = 0.04518196\n",
      "Iteration 45, loss = 0.09941800\n",
      "Iteration 71, loss = 0.04195803\n",
      "Iteration 46, loss = 0.09725876\n",
      "Iteration 72, loss = 0.03798987\n",
      "Iteration 47, loss = 0.10420232\n",
      "Iteration 73, loss = 0.05374310\n",
      "Iteration 48, loss = 0.09566838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 74, loss = 0.04526485\n",
      "Iteration 75, loss = 0.04826310\n",
      "Iteration 76, loss = 0.04237995\n",
      "Iteration 77, loss = 0.05392131\n",
      "Iteration 78, loss = 0.09323030\n",
      "Iteration 79, loss = 0.08911138\n",
      "Iteration 80, loss = 0.05580397\n",
      "Iteration 81, loss = 0.07622111\n",
      "Iteration 82, loss = 0.07240124\n",
      "Iteration 83, loss = 0.05672444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85089099\n",
      "Iteration 2, loss = 0.50110800\n",
      "Iteration 3, loss = 0.41663087\n",
      "Iteration 4, loss = 0.37691995\n",
      "Iteration 5, loss = 0.33224447\n",
      "Iteration 6, loss = 0.31542758\n",
      "Iteration 7, loss = 0.28976337\n",
      "Iteration 8, loss = 0.26740647\n",
      "Iteration 9, loss = 0.25217561\n",
      "Iteration 10, loss = 0.24192397\n",
      "Iteration 11, loss = 0.22895265\n",
      "Iteration 1, loss = 0.93687928\n",
      "Iteration 12, loss = 0.21724816\n",
      "Iteration 2, loss = 0.54761376\n",
      "Iteration 3, loss = 0.41165601\n",
      "Iteration 13, loss = 0.19549056\n",
      "Iteration 4, loss = 0.36514755\n",
      "Iteration 14, loss = 0.19261279\n",
      "Iteration 5, loss = 0.31736932\n",
      "Iteration 15, loss = 0.18162404\n",
      "Iteration 6, loss = 0.29245122\n",
      "Iteration 16, loss = 0.16811892\n",
      "Iteration 7, loss = 0.26517196\n",
      "Iteration 17, loss = 0.16311787\n",
      "Iteration 8, loss = 0.24415892\n",
      "Iteration 18, loss = 0.14518283\n",
      "Iteration 9, loss = 0.24107104\n",
      "Iteration 19, loss = 0.13988563\n",
      "Iteration 10, loss = 0.22769547\n",
      "Iteration 20, loss = 0.13103029\n",
      "Iteration 11, loss = 0.20493992\n",
      "Iteration 21, loss = 0.12390951\n",
      "Iteration 12, loss = 0.20234203\n",
      "Iteration 22, loss = 0.13672734\n",
      "Iteration 23, loss = 0.12766231\n",
      "Iteration 13, loss = 0.19126138\n",
      "Iteration 24, loss = 0.12706442\n",
      "Iteration 14, loss = 0.17647334\n",
      "Iteration 25, loss = 0.13175200\n",
      "Iteration 15, loss = 0.16834335\n",
      "Iteration 26, loss = 0.10861981\n",
      "Iteration 16, loss = 0.16824056\n",
      "Iteration 27, loss = 0.10911150\n",
      "Iteration 17, loss = 0.16453958\n",
      "Iteration 18, loss = 0.15216196\n",
      "Iteration 28, loss = 0.10696212\n",
      "Iteration 19, loss = 0.14742848\n",
      "Iteration 29, loss = 0.09792888\n",
      "Iteration 30, loss = 0.10687549\n",
      "Iteration 20, loss = 0.13699072\n",
      "Iteration 31, loss = 0.10366720\n",
      "Iteration 21, loss = 0.13507722\n",
      "Iteration 32, loss = 0.09074403\n",
      "Iteration 22, loss = 0.13908141\n",
      "Iteration 33, loss = 0.08884260\n",
      "Iteration 23, loss = 0.12673781\n",
      "Iteration 34, loss = 0.07405186\n",
      "Iteration 24, loss = 0.12301927\n",
      "Iteration 35, loss = 0.06952344\n",
      "Iteration 25, loss = 0.11394355\n",
      "Iteration 36, loss = 0.06435485\n",
      "Iteration 26, loss = 0.10886530\n",
      "Iteration 37, loss = 0.05979211\n",
      "Iteration 27, loss = 0.10752977\n",
      "Iteration 38, loss = 0.06418355\n",
      "Iteration 28, loss = 0.11311436\n",
      "Iteration 39, loss = 0.05852392\n",
      "Iteration 29, loss = 0.11331472\n",
      "Iteration 40, loss = 0.06959474\n",
      "Iteration 30, loss = 0.10595299\n",
      "Iteration 41, loss = 0.09402017\n",
      "Iteration 31, loss = 0.09519074\n",
      "Iteration 42, loss = 0.10931590\n",
      "Iteration 32, loss = 0.09846391\n",
      "Iteration 43, loss = 0.10618918\n",
      "Iteration 33, loss = 0.09182589\n",
      "Iteration 44, loss = 0.13156162\n",
      "Iteration 34, loss = 0.09237598\n",
      "Iteration 45, loss = 0.09306765\n",
      "Iteration 35, loss = 0.07982912\n",
      "Iteration 46, loss = 0.10366932\n",
      "Iteration 36, loss = 0.09942103\n",
      "Iteration 47, loss = 0.07370224\n",
      "Iteration 37, loss = 0.13209839\n",
      "Iteration 48, loss = 0.07077826\n",
      "Iteration 38, loss = 0.13472838\n",
      "Iteration 49, loss = 0.10811967\n",
      "Iteration 39, loss = 0.15471140\n",
      "Iteration 50, loss = 0.09905059\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.16466431\n",
      "Iteration 41, loss = 0.12841704\n",
      "Iteration 42, loss = 0.10678746\n",
      "Iteration 43, loss = 0.10924661\n",
      "Iteration 44, loss = 0.11294781\n",
      "Iteration 45, loss = 0.10908828\n",
      "Iteration 46, loss = 0.07381673\n",
      "Iteration 47, loss = 0.07378588\n",
      "Iteration 48, loss = 0.06155118\n",
      "Iteration 49, loss = 0.07904865\n",
      "Iteration 50, loss = 0.10577802\n",
      "Iteration 51, loss = 0.13082868\n",
      "Iteration 52, loss = 0.10906134\n",
      "Iteration 53, loss = 0.11345669\n",
      "Iteration 54, loss = 0.16546314\n",
      "Iteration 55, loss = 0.16721819\n",
      "Iteration 56, loss = 0.16620611\n",
      "Iteration 57, loss = 0.12096973\n",
      "Iteration 58, loss = 0.08527526\n",
      "Iteration 1, loss = 0.94370609\n",
      "Iteration 59, loss = 0.08459304\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.52391379\n",
      "Iteration 3, loss = 0.43509249\n",
      "Iteration 4, loss = 0.37757879\n",
      "Iteration 5, loss = 0.36186712\n",
      "Iteration 6, loss = 0.32741906\n",
      "Iteration 7, loss = 0.29359936\n",
      "Iteration 8, loss = 0.27621969\n",
      "Iteration 9, loss = 0.27308493\n",
      "Iteration 10, loss = 0.25291746\n",
      "Iteration 11, loss = 0.23860923\n",
      "Iteration 12, loss = 0.23096421\n",
      "Iteration 13, loss = 0.21824987\n",
      "Iteration 14, loss = 0.20861116\n",
      "Iteration 15, loss = 0.19963065\n",
      "Iteration 16, loss = 0.19321465\n",
      "Iteration 17, loss = 0.18443590\n",
      "Iteration 18, loss = 0.18268510\n",
      "Iteration 19, loss = 0.18313564\n",
      "Iteration 1, loss = 0.87704381\n",
      "Iteration 20, loss = 0.18770983\n",
      "Iteration 2, loss = 0.49100054\n",
      "Iteration 21, loss = 0.16771216\n",
      "Iteration 3, loss = 0.41508301\n",
      "Iteration 22, loss = 0.17275577\n",
      "Iteration 4, loss = 0.36993758\n",
      "Iteration 23, loss = 0.17654741\n",
      "Iteration 5, loss = 0.34509908\n",
      "Iteration 24, loss = 0.16297522\n",
      "Iteration 6, loss = 0.31520539\n",
      "Iteration 25, loss = 0.15717446\n",
      "Iteration 7, loss = 0.29046147\n",
      "Iteration 26, loss = 0.13720951\n",
      "Iteration 8, loss = 0.27684036\n",
      "Iteration 27, loss = 0.12219908\n",
      "Iteration 9, loss = 0.25632752\n",
      "Iteration 28, loss = 0.13055839\n",
      "Iteration 10, loss = 0.24352695\n",
      "Iteration 29, loss = 0.12979137\n",
      "Iteration 11, loss = 0.23548810\n",
      "Iteration 30, loss = 0.11699042\n",
      "Iteration 12, loss = 0.22234037\n",
      "Iteration 31, loss = 0.12359135\n",
      "Iteration 13, loss = 0.22560687\n",
      "Iteration 32, loss = 0.10478747\n",
      "Iteration 14, loss = 0.20533418\n",
      "Iteration 33, loss = 0.10432044\n",
      "Iteration 15, loss = 0.20130796\n",
      "Iteration 34, loss = 0.09283367\n",
      "Iteration 16, loss = 0.18909890\n",
      "Iteration 35, loss = 0.10684034\n",
      "Iteration 17, loss = 0.18727636\n",
      "Iteration 36, loss = 0.10647640\n",
      "Iteration 18, loss = 0.17917081\n",
      "Iteration 37, loss = 0.12055515\n",
      "Iteration 19, loss = 0.17162030\n",
      "Iteration 38, loss = 0.13647164\n",
      "Iteration 20, loss = 0.16495030\n",
      "Iteration 39, loss = 0.13342236\n",
      "Iteration 21, loss = 0.16422331\n",
      "Iteration 40, loss = 0.13816009\n",
      "Iteration 22, loss = 0.15227079\n",
      "Iteration 41, loss = 0.13671827\n",
      "Iteration 23, loss = 0.15629574\n",
      "Iteration 42, loss = 0.11674574\n",
      "Iteration 24, loss = 0.16406100\n",
      "Iteration 43, loss = 0.11782866\n",
      "Iteration 25, loss = 0.16714323\n",
      "Iteration 44, loss = 0.10129195\n",
      "Iteration 26, loss = 0.16058297\n",
      "Iteration 45, loss = 0.08440694\n",
      "Iteration 27, loss = 0.16601387\n",
      "Iteration 46, loss = 0.09861702\n",
      "Iteration 28, loss = 0.18450163\n",
      "Iteration 47, loss = 0.08728401\n",
      "Iteration 29, loss = 0.17937544\n",
      "Iteration 48, loss = 0.08641851\n",
      "Iteration 30, loss = 0.13832012\n",
      "Iteration 49, loss = 0.08864674\n",
      "Iteration 31, loss = 0.13980006\n",
      "Iteration 50, loss = 0.07304621\n",
      "Iteration 32, loss = 0.15225857\n",
      "Iteration 51, loss = 0.06211437\n",
      "Iteration 33, loss = 0.13453273\n",
      "Iteration 52, loss = 0.07060142\n",
      "Iteration 34, loss = 0.11811584\n",
      "Iteration 53, loss = 0.07041903\n",
      "Iteration 35, loss = 0.12450065\n",
      "Iteration 54, loss = 0.06924579\n",
      "Iteration 36, loss = 0.12212586\n",
      "Iteration 55, loss = 0.07081119\n",
      "Iteration 37, loss = 0.10499236\n",
      "Iteration 56, loss = 0.09551143\n",
      "Iteration 38, loss = 0.09715301\n",
      "Iteration 57, loss = 0.09243854\n",
      "Iteration 39, loss = 0.09113897\n",
      "Iteration 58, loss = 0.09138390\n",
      "Iteration 40, loss = 0.08795314\n",
      "Iteration 59, loss = 0.08446709\n",
      "Iteration 41, loss = 0.09039214\n",
      "Iteration 60, loss = 0.09104779\n",
      "Iteration 42, loss = 0.08819876\n",
      "Iteration 61, loss = 0.09860802\n",
      "Iteration 43, loss = 0.09344940\n",
      "Iteration 62, loss = 0.09984486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.08257408\n",
      "Iteration 45, loss = 0.09873830\n",
      "Iteration 46, loss = 0.07448099\n",
      "Iteration 47, loss = 0.07025709\n",
      "Iteration 48, loss = 0.08065684\n",
      "Iteration 49, loss = 0.08159505\n",
      "Iteration 50, loss = 0.08756370\n",
      "Iteration 51, loss = 0.07635915\n",
      "Iteration 52, loss = 0.07627811\n",
      "Iteration 53, loss = 0.07945739\n",
      "Iteration 54, loss = 0.07434000\n",
      "Iteration 55, loss = 0.07727074\n",
      "Iteration 56, loss = 0.07522992\n",
      "Iteration 57, loss = 0.11193745\n",
      "Iteration 58, loss = 0.06845441\n",
      "Iteration 59, loss = 0.08437769\n",
      "Iteration 60, loss = 0.09133172\n",
      "Iteration 61, loss = 0.07840220\n",
      "Iteration 1, loss = 0.96407860\n",
      "Iteration 62, loss = 0.07969228\n",
      "Iteration 2, loss = 0.53471557\n",
      "Iteration 63, loss = 0.07866971\n",
      "Iteration 3, loss = 0.43607051\n",
      "Iteration 64, loss = 0.08191315\n",
      "Iteration 4, loss = 0.40494138\n",
      "Iteration 65, loss = 0.08844397\n",
      "Iteration 5, loss = 0.36357378\n",
      "Iteration 66, loss = 0.06332298\n",
      "Iteration 6, loss = 0.33036014\n",
      "Iteration 67, loss = 0.05625147\n",
      "Iteration 7, loss = 0.29322193\n",
      "Iteration 68, loss = 0.05519830\n",
      "Iteration 8, loss = 0.28072452\n",
      "Iteration 69, loss = 0.04787639\n",
      "Iteration 9, loss = 0.26633630\n",
      "Iteration 70, loss = 0.04488553\n",
      "Iteration 10, loss = 0.24488074\n",
      "Iteration 71, loss = 0.04157109\n",
      "Iteration 11, loss = 0.24206003\n",
      "Iteration 72, loss = 0.03903439\n",
      "Iteration 12, loss = 0.22687616\n",
      "Iteration 73, loss = 0.04562721\n",
      "Iteration 13, loss = 0.22443909\n",
      "Iteration 74, loss = 0.06024549\n",
      "Iteration 14, loss = 0.23612629\n",
      "Iteration 75, loss = 0.05048464\n",
      "Iteration 15, loss = 0.21327089\n",
      "Iteration 76, loss = 0.06081299\n",
      "Iteration 16, loss = 0.19215263\n",
      "Iteration 77, loss = 0.07065284\n",
      "Iteration 17, loss = 0.19685905\n",
      "Iteration 78, loss = 0.06892385\n",
      "Iteration 18, loss = 0.17492101\n",
      "Iteration 79, loss = 0.08444301\n",
      "Iteration 19, loss = 0.18774428\n",
      "Iteration 80, loss = 0.06794470\n",
      "Iteration 20, loss = 0.16868296\n",
      "Iteration 81, loss = 0.05183166\n",
      "Iteration 21, loss = 0.16459334\n",
      "Iteration 82, loss = 0.04429209\n",
      "Iteration 22, loss = 0.14952313\n",
      "Iteration 83, loss = 0.04733789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.15643698\n",
      "Iteration 24, loss = 0.14636033\n",
      "Iteration 25, loss = 0.13585753\n",
      "Iteration 26, loss = 0.14175231\n",
      "Iteration 27, loss = 0.12414756\n",
      "Iteration 28, loss = 0.12507367\n",
      "Iteration 29, loss = 0.12145558\n",
      "Iteration 30, loss = 0.11352790\n",
      "Iteration 31, loss = 0.10365867\n",
      "Iteration 32, loss = 0.09872072\n",
      "Iteration 33, loss = 0.10280373\n",
      "Iteration 34, loss = 0.10755274\n",
      "Iteration 35, loss = 0.10104948\n",
      "Iteration 36, loss = 0.09814663\n",
      "Iteration 37, loss = 0.09797367\n",
      "Iteration 38, loss = 0.10230618\n",
      "Iteration 39, loss = 0.08923463\n",
      "Iteration 40, loss = 0.08350252\n",
      "Iteration 1, loss = 0.95956711\n",
      "Iteration 41, loss = 0.08078176\n",
      "Iteration 2, loss = 0.49895099\n",
      "Iteration 42, loss = 0.09047989\n",
      "Iteration 3, loss = 0.41439110\n",
      "Iteration 43, loss = 0.12304656\n",
      "Iteration 4, loss = 0.37873481\n",
      "Iteration 5, loss = 0.34422569\n",
      "Iteration 44, loss = 0.12952643\n",
      "Iteration 45, loss = 0.15394549\n",
      "Iteration 6, loss = 0.31604810\n",
      "Iteration 46, loss = 0.13503365\n",
      "Iteration 7, loss = 0.29491786\n",
      "Iteration 47, loss = 0.09075486\n",
      "Iteration 8, loss = 0.27492482\n",
      "Iteration 48, loss = 0.08288361\n",
      "Iteration 9, loss = 0.25948781\n",
      "Iteration 49, loss = 0.07267320\n",
      "Iteration 10, loss = 0.24805628\n",
      "Iteration 50, loss = 0.06793605\n",
      "Iteration 11, loss = 0.23762490\n",
      "Iteration 51, loss = 0.06535218\n",
      "Iteration 12, loss = 0.22554834\n",
      "Iteration 52, loss = 0.06816112\n",
      "Iteration 13, loss = 0.23477465\n",
      "Iteration 53, loss = 0.05502850\n",
      "Iteration 14, loss = 0.20190575\n",
      "Iteration 54, loss = 0.05340674\n",
      "Iteration 15, loss = 0.19808519\n",
      "Iteration 55, loss = 0.06405888\n",
      "Iteration 16, loss = 0.19832770\n",
      "Iteration 56, loss = 0.06285271\n",
      "Iteration 17, loss = 0.19662615\n",
      "Iteration 57, loss = 0.05643189\n",
      "Iteration 18, loss = 0.18395583\n",
      "Iteration 58, loss = 0.05965121\n",
      "Iteration 19, loss = 0.16601613\n",
      "Iteration 20, loss = 0.15582756Iteration 59, loss = 0.05828780\n",
      "\n",
      "Iteration 21, loss = 0.14785826Iteration 60, loss = 0.06514199\n",
      "\n",
      "Iteration 22, loss = 0.14156655\n",
      "Iteration 61, loss = 0.05373475\n",
      "Iteration 23, loss = 0.13109707\n",
      "Iteration 62, loss = 0.06555879\n",
      "Iteration 24, loss = 0.13567786\n",
      "Iteration 63, loss = 0.05422719\n",
      "Iteration 64, loss = 0.05659185\n",
      "Iteration 25, loss = 0.14127887\n",
      "Iteration 26, loss = 0.12462903\n",
      "Iteration 65, loss = 0.05172922\n",
      "Iteration 27, loss = 0.15074292\n",
      "Iteration 66, loss = 0.05285227\n",
      "Iteration 28, loss = 0.12325150\n",
      "Iteration 67, loss = 0.06638542\n",
      "Iteration 29, loss = 0.15129855\n",
      "Iteration 68, loss = 0.05158959\n",
      "Iteration 30, loss = 0.12863404\n",
      "Iteration 69, loss = 0.05687564\n",
      "Iteration 31, loss = 0.10410409\n",
      "Iteration 70, loss = 0.07432658\n",
      "Iteration 32, loss = 0.10243032\n",
      "Iteration 71, loss = 0.04240026\n",
      "Iteration 33, loss = 0.09129438\n",
      "Iteration 72, loss = 0.04696129\n",
      "Iteration 34, loss = 0.09368546\n",
      "Iteration 73, loss = 0.05878280\n",
      "Iteration 35, loss = 0.09751512\n",
      "Iteration 74, loss = 0.06305197\n",
      "Iteration 36, loss = 0.08233959\n",
      "Iteration 75, loss = 0.04913236\n",
      "Iteration 37, loss = 0.08834234\n",
      "Iteration 76, loss = 0.04638221\n",
      "Iteration 38, loss = 0.08599055\n",
      "Iteration 77, loss = 0.04362186\n",
      "Iteration 39, loss = 0.08897108\n",
      "Iteration 78, loss = 0.05558543\n",
      "Iteration 40, loss = 0.08204852\n",
      "Iteration 79, loss = 0.05296305\n",
      "Iteration 41, loss = 0.09091427\n",
      "Iteration 80, loss = 0.05373565\n",
      "Iteration 42, loss = 0.10928913\n",
      "Iteration 81, loss = 0.05538219\n",
      "Iteration 43, loss = 0.08636288\n",
      "Iteration 82, loss = 0.04071513\n",
      "Iteration 44, loss = 0.08497136\n",
      "Iteration 83, loss = 0.04021733\n",
      "Iteration 45, loss = 0.11024922\n",
      "Iteration 84, loss = 0.04503179\n",
      "Iteration 46, loss = 0.10384028\n",
      "Iteration 85, loss = 0.04503379\n",
      "Iteration 47, loss = 0.10151035\n",
      "Iteration 86, loss = 0.04021808\n",
      "Iteration 48, loss = 0.12565389\n",
      "Iteration 87, loss = 0.04121456\n",
      "Iteration 49, loss = 0.13736571\n",
      "Iteration 88, loss = 0.03759343\n",
      "Iteration 50, loss = 0.12792463\n",
      "Iteration 89, loss = 0.03417613\n",
      "Iteration 51, loss = 0.12331661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 90, loss = 0.02915795\n",
      "Iteration 91, loss = 0.02510704\n",
      "Iteration 92, loss = 0.03736501\n",
      "Iteration 93, loss = 0.03427329\n",
      "Iteration 94, loss = 0.03113027\n",
      "Iteration 95, loss = 0.03559840\n",
      "Iteration 96, loss = 0.03003405\n",
      "Iteration 97, loss = 0.02709864\n",
      "Iteration 98, loss = 0.02504954\n",
      "Iteration 99, loss = 0.02307162\n",
      "Iteration 100, loss = 0.02035412\n",
      "Iteration 101, loss = 0.02569137\n",
      "Iteration 102, loss = 0.05734158\n",
      "Iteration 103, loss = 0.04172906\n",
      "Iteration 104, loss = 0.05510924\n",
      "Iteration 105, loss = 0.05168715\n",
      "Iteration 106, loss = 0.02960423\n",
      "Iteration 107, loss = 0.02368586\n",
      "Iteration 1, loss = 0.99818425\n",
      "Iteration 108, loss = 0.03078003\n",
      "Iteration 2, loss = 0.57496460\n",
      "Iteration 109, loss = 0.02680289\n",
      "Iteration 3, loss = 0.45406339\n",
      "Iteration 110, loss = 0.03326782\n",
      "Iteration 4, loss = 0.40244855\n",
      "Iteration 111, loss = 0.03683945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.37108307\n",
      "Iteration 6, loss = 0.34693224\n",
      "Iteration 7, loss = 0.31840306\n",
      "Iteration 8, loss = 0.29950039\n",
      "Iteration 9, loss = 0.28204465\n",
      "Iteration 10, loss = 0.26835594\n",
      "Iteration 11, loss = 0.25259528\n",
      "Iteration 12, loss = 0.24969470\n",
      "Iteration 13, loss = 0.24294306\n",
      "Iteration 14, loss = 0.22419094\n",
      "Iteration 15, loss = 0.21514449\n",
      "Iteration 16, loss = 0.22180649\n",
      "Iteration 17, loss = 0.20852260\n",
      "Iteration 18, loss = 0.20485537\n",
      "Iteration 19, loss = 0.19086018\n",
      "Iteration 20, loss = 0.18147820\n",
      "Iteration 21, loss = 0.17217415\n",
      "Iteration 22, loss = 0.16577999\n",
      "Iteration 1, loss = 0.95086833\n",
      "Iteration 23, loss = 0.16860359\n",
      "Iteration 2, loss = 0.52022041\n",
      "Iteration 24, loss = 0.15366181\n",
      "Iteration 3, loss = 0.42971477\n",
      "Iteration 25, loss = 0.15411900\n",
      "Iteration 4, loss = 0.41188806\n",
      "Iteration 26, loss = 0.15379222\n",
      "Iteration 5, loss = 0.37578614\n",
      "Iteration 27, loss = 0.13748837\n",
      "Iteration 6, loss = 0.32968891\n",
      "Iteration 28, loss = 0.13395151\n",
      "Iteration 7, loss = 0.31877464\n",
      "Iteration 29, loss = 0.15472154\n",
      "Iteration 8, loss = 0.30572116\n",
      "Iteration 30, loss = 0.15246025\n",
      "Iteration 9, loss = 0.27954303\n",
      "Iteration 31, loss = 0.13829713\n",
      "Iteration 10, loss = 0.26742008\n",
      "Iteration 32, loss = 0.15999646\n",
      "Iteration 11, loss = 0.26052406\n",
      "Iteration 33, loss = 0.13161342\n",
      "Iteration 12, loss = 0.24531005\n",
      "Iteration 34, loss = 0.14201535\n",
      "Iteration 13, loss = 0.23489881\n",
      "Iteration 35, loss = 0.14241167\n",
      "Iteration 14, loss = 0.22175121\n",
      "Iteration 36, loss = 0.11654481\n",
      "Iteration 15, loss = 0.21802796\n",
      "Iteration 37, loss = 0.10597694\n",
      "Iteration 16, loss = 0.21429498\n",
      "Iteration 38, loss = 0.11986734\n",
      "Iteration 17, loss = 0.19506599\n",
      "Iteration 39, loss = 0.15182329\n",
      "Iteration 18, loss = 0.18747058\n",
      "Iteration 40, loss = 0.13453168\n",
      "Iteration 19, loss = 0.17562539\n",
      "Iteration 41, loss = 0.10777014\n",
      "Iteration 20, loss = 0.17812299\n",
      "Iteration 42, loss = 0.10121124\n",
      "Iteration 21, loss = 0.17843952\n",
      "Iteration 43, loss = 0.09652106\n",
      "Iteration 22, loss = 0.16353291\n",
      "Iteration 44, loss = 0.10544885\n",
      "Iteration 23, loss = 0.15315855\n",
      "Iteration 45, loss = 0.10649074\n",
      "Iteration 24, loss = 0.15057174\n",
      "Iteration 46, loss = 0.11505686\n",
      "Iteration 25, loss = 0.14882281\n",
      "Iteration 47, loss = 0.11904843\n",
      "Iteration 26, loss = 0.14734837\n",
      "Iteration 48, loss = 0.11882475\n",
      "Iteration 27, loss = 0.12773847\n",
      "Iteration 49, loss = 0.10376660\n",
      "Iteration 28, loss = 0.13632833\n",
      "Iteration 50, loss = 0.10134424\n",
      "Iteration 29, loss = 0.13058323\n",
      "Iteration 51, loss = 0.10238450\n",
      "Iteration 30, loss = 0.11513554\n",
      "Iteration 52, loss = 0.09513990\n",
      "Iteration 31, loss = 0.11768802\n",
      "Iteration 53, loss = 0.09659078\n",
      "Iteration 32, loss = 0.10364128\n",
      "Iteration 54, loss = 0.09200138\n",
      "Iteration 33, loss = 0.10037548\n",
      "Iteration 55, loss = 0.08436020\n",
      "Iteration 34, loss = 0.09890594\n",
      "Iteration 56, loss = 0.09070008\n",
      "Iteration 35, loss = 0.09464772\n",
      "Iteration 57, loss = 0.07891623\n",
      "Iteration 36, loss = 0.09716676\n",
      "Iteration 58, loss = 0.07873456\n",
      "Iteration 37, loss = 0.10639680\n",
      "Iteration 59, loss = 0.07171031\n",
      "Iteration 38, loss = 0.10844834\n",
      "Iteration 60, loss = 0.07173612\n",
      "Iteration 39, loss = 0.09978413\n",
      "Iteration 61, loss = 0.06351369\n",
      "Iteration 40, loss = 0.12154269\n",
      "Iteration 62, loss = 0.06610491\n",
      "Iteration 41, loss = 0.12175980\n",
      "Iteration 63, loss = 0.06598623\n",
      "Iteration 42, loss = 0.11698249\n",
      "Iteration 64, loss = 0.07019240\n",
      "Iteration 43, loss = 0.09765322\n",
      "Iteration 65, loss = 0.05775268\n",
      "Iteration 44, loss = 0.11174770\n",
      "Iteration 66, loss = 0.06879664\n",
      "Iteration 45, loss = 0.11038778\n",
      "Iteration 67, loss = 0.06962726\n",
      "Iteration 46, loss = 0.11910465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 68, loss = 0.07242845\n",
      "Iteration 69, loss = 0.06008147\n",
      "Iteration 70, loss = 0.05091170\n",
      "Iteration 71, loss = 0.05602822\n",
      "Iteration 72, loss = 0.06401749\n",
      "Iteration 73, loss = 0.07722294\n",
      "Iteration 74, loss = 0.08608944\n",
      "Iteration 75, loss = 0.08446379\n",
      "Iteration 76, loss = 0.08813100\n",
      "Iteration 77, loss = 0.07240159\n",
      "Iteration 78, loss = 0.07379870\n",
      "Iteration 79, loss = 0.06320473\n",
      "Iteration 80, loss = 0.06747710\n",
      "Iteration 81, loss = 0.05502693\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06299094\n",
      "Iteration 2, loss = 0.60393129\n",
      "Iteration 3, loss = 0.48001743\n",
      "Iteration 4, loss = 0.41002931\n",
      "Iteration 5, loss = 0.36759727\n",
      "Iteration 6, loss = 0.34897934\n",
      "Iteration 7, loss = 0.32071131\n",
      "Iteration 8, loss = 0.29914282\n",
      "Iteration 9, loss = 0.27388102\n",
      "Iteration 10, loss = 0.25992668\n",
      "Iteration 11, loss = 0.25040049\n",
      "Iteration 12, loss = 0.23744289\n",
      "Iteration 13, loss = 0.22993938\n",
      "Iteration 14, loss = 0.21715813\n",
      "Iteration 15, loss = 0.21450254\n",
      "Iteration 1, loss = 0.80378601\n",
      "Iteration 16, loss = 0.22747731\n",
      "Iteration 2, loss = 0.45310140\n",
      "Iteration 17, loss = 0.18660396\n",
      "Iteration 3, loss = 0.38639962\n",
      "Iteration 18, loss = 0.17990394\n",
      "Iteration 4, loss = 0.36232949\n",
      "Iteration 19, loss = 0.17069183\n",
      "Iteration 5, loss = 0.33244159\n",
      "Iteration 20, loss = 0.16410377\n",
      "Iteration 6, loss = 0.30022911\n",
      "Iteration 7, loss = 0.28828958\n",
      "Iteration 21, loss = 0.17118576\n",
      "Iteration 8, loss = 0.27660235\n",
      "Iteration 22, loss = 0.15321265\n",
      "Iteration 9, loss = 0.25263494\n",
      "Iteration 23, loss = 0.15344877\n",
      "Iteration 10, loss = 0.24152698\n",
      "Iteration 24, loss = 0.15209929\n",
      "Iteration 11, loss = 0.22813577\n",
      "Iteration 25, loss = 0.14024554\n",
      "Iteration 12, loss = 0.21560758\n",
      "Iteration 26, loss = 0.13221644\n",
      "Iteration 13, loss = 0.21209797\n",
      "Iteration 27, loss = 0.13321210\n",
      "Iteration 14, loss = 0.19352721\n",
      "Iteration 28, loss = 0.12595024\n",
      "Iteration 29, loss = 0.15003346\n",
      "Iteration 15, loss = 0.20278608\n",
      "Iteration 30, loss = 0.15347668\n",
      "Iteration 16, loss = 0.19638972\n",
      "Iteration 17, loss = 0.17886772\n",
      "Iteration 31, loss = 0.15087950\n",
      "Iteration 18, loss = 0.18443347\n",
      "Iteration 32, loss = 0.14328289\n",
      "Iteration 19, loss = 0.18442855\n",
      "Iteration 33, loss = 0.13817856\n",
      "Iteration 20, loss = 0.16024979\n",
      "Iteration 34, loss = 0.14425765\n",
      "Iteration 21, loss = 0.15596178\n",
      "Iteration 35, loss = 0.12918384\n",
      "Iteration 36, loss = 0.11250722\n",
      "Iteration 22, loss = 0.14854745\n",
      "Iteration 37, loss = 0.11874276\n",
      "Iteration 23, loss = 0.14000083\n",
      "Iteration 24, loss = 0.14267665\n",
      "Iteration 38, loss = 0.10665724\n",
      "Iteration 25, loss = 0.13955963\n",
      "Iteration 39, loss = 0.11051865\n",
      "Iteration 26, loss = 0.13192960\n",
      "Iteration 40, loss = 0.09200965\n",
      "Iteration 27, loss = 0.12411933\n",
      "Iteration 41, loss = 0.08597298\n",
      "Iteration 28, loss = 0.12932131\n",
      "Iteration 42, loss = 0.08268031\n",
      "Iteration 29, loss = 0.13426778\n",
      "Iteration 43, loss = 0.08178262\n",
      "Iteration 30, loss = 0.11788775\n",
      "Iteration 44, loss = 0.08862691\n",
      "Iteration 31, loss = 0.13263010\n",
      "Iteration 45, loss = 0.10327343\n",
      "Iteration 32, loss = 0.12138555\n",
      "Iteration 46, loss = 0.09013058\n",
      "Iteration 33, loss = 0.12361451\n",
      "Iteration 47, loss = 0.12926398\n",
      "Iteration 34, loss = 0.12178966\n",
      "Iteration 48, loss = 0.10506113\n",
      "Iteration 35, loss = 0.13388952\n",
      "Iteration 49, loss = 0.11358045\n",
      "Iteration 36, loss = 0.14103352\n",
      "Iteration 50, loss = 0.09630247\n",
      "Iteration 37, loss = 0.12677041\n",
      "Iteration 51, loss = 0.09593750\n",
      "Iteration 38, loss = 0.10870708\n",
      "Iteration 52, loss = 0.09366210\n",
      "Iteration 39, loss = 0.11598600\n",
      "Iteration 53, loss = 0.07801907\n",
      "Iteration 40, loss = 0.10685922\n",
      "Iteration 54, loss = 0.07723958\n",
      "Iteration 41, loss = 0.10128770\n",
      "Iteration 55, loss = 0.08826031\n",
      "Iteration 42, loss = 0.12939137\n",
      "Iteration 56, loss = 0.08309775\n",
      "Iteration 43, loss = 0.10937114\n",
      "Iteration 57, loss = 0.08198787\n",
      "Iteration 44, loss = 0.09093279\n",
      "Iteration 58, loss = 0.08475187\n",
      "Iteration 45, loss = 0.09748983\n",
      "Iteration 59, loss = 0.07098694\n",
      "Iteration 46, loss = 0.12573946\n",
      "Iteration 60, loss = 0.07206241\n",
      "Iteration 47, loss = 0.09815792\n",
      "Iteration 61, loss = 0.05798327\n",
      "Iteration 48, loss = 0.09225474\n",
      "Iteration 62, loss = 0.06060572\n",
      "Iteration 49, loss = 0.07875269\n",
      "Iteration 63, loss = 0.07848423\n",
      "Iteration 50, loss = 0.07592277\n",
      "Iteration 64, loss = 0.07215352\n",
      "Iteration 51, loss = 0.07071908\n",
      "Iteration 65, loss = 0.07600869\n",
      "Iteration 52, loss = 0.07336265\n",
      "Iteration 66, loss = 0.07699186\n",
      "Iteration 53, loss = 0.09587421\n",
      "Iteration 67, loss = 0.08105785\n",
      "Iteration 54, loss = 0.10107086\n",
      "Iteration 68, loss = 0.06789284\n",
      "Iteration 55, loss = 0.11901989\n",
      "Iteration 69, loss = 0.09497330\n",
      "Iteration 56, loss = 0.09443459\n",
      "Iteration 70, loss = 0.09629907\n",
      "Iteration 57, loss = 0.08231330\n",
      "Iteration 71, loss = 0.10416264\n",
      "Iteration 58, loss = 0.08091294\n",
      "Iteration 72, loss = 0.08838485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.07111566\n",
      "Iteration 60, loss = 0.06714664\n",
      "Iteration 61, loss = 0.06937463\n",
      "Iteration 62, loss = 0.06134696\n",
      "Iteration 63, loss = 0.06150593\n",
      "Iteration 64, loss = 0.05480955\n",
      "Iteration 65, loss = 0.07538497\n",
      "Iteration 66, loss = 0.07404740\n",
      "Iteration 67, loss = 0.06599352\n",
      "Iteration 68, loss = 0.09115273\n",
      "Iteration 69, loss = 0.10941985\n",
      "Iteration 70, loss = 0.08456390\n",
      "Iteration 71, loss = 0.08043823\n",
      "Iteration 72, loss = 0.08811599\n",
      "Iteration 73, loss = 0.09035268\n",
      "Iteration 74, loss = 0.07117141\n",
      "Iteration 75, loss = 0.08114899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08559197\n",
      "Iteration 2, loss = 0.49304439\n",
      "Iteration 3, loss = 0.40839949\n",
      "Iteration 4, loss = 0.37153544\n",
      "Iteration 5, loss = 0.32781970\n",
      "Iteration 6, loss = 0.30744648\n",
      "Iteration 7, loss = 0.28666312\n",
      "Iteration 8, loss = 0.27227770\n",
      "Iteration 9, loss = 0.25228899\n",
      "Iteration 10, loss = 0.25006649\n",
      "Iteration 11, loss = 0.24433722\n",
      "Iteration 12, loss = 0.22618211\n",
      "Iteration 13, loss = 0.21316601\n",
      "Iteration 14, loss = 0.20562364\n",
      "Iteration 15, loss = 0.19541413\n",
      "Iteration 16, loss = 0.17493839\n",
      "Iteration 17, loss = 0.17112449\n",
      "Iteration 1, loss = 0.86745450\n",
      "Iteration 18, loss = 0.15745379\n",
      "Iteration 2, loss = 0.46604509\n",
      "Iteration 19, loss = 0.15363251\n",
      "Iteration 3, loss = 0.37233175\n",
      "Iteration 20, loss = 0.14618332\n",
      "Iteration 4, loss = 0.32372438\n",
      "Iteration 21, loss = 0.13751869\n",
      "Iteration 5, loss = 0.29266722\n",
      "Iteration 22, loss = 0.12365062\n",
      "Iteration 6, loss = 0.28115965\n",
      "Iteration 23, loss = 0.11072454\n",
      "Iteration 7, loss = 0.26262857\n",
      "Iteration 24, loss = 0.11326168\n",
      "Iteration 8, loss = 0.25099200\n",
      "Iteration 25, loss = 0.10106763\n",
      "Iteration 9, loss = 0.24327954\n",
      "Iteration 26, loss = 0.09877134\n",
      "Iteration 10, loss = 0.24217393\n",
      "Iteration 27, loss = 0.09638539\n",
      "Iteration 11, loss = 0.23136861\n",
      "Iteration 28, loss = 0.09005762\n",
      "Iteration 12, loss = 0.20700988\n",
      "Iteration 29, loss = 0.08679815\n",
      "Iteration 13, loss = 0.19290893\n",
      "Iteration 30, loss = 0.08949217\n",
      "Iteration 14, loss = 0.18357212\n",
      "Iteration 31, loss = 0.08113321\n",
      "Iteration 15, loss = 0.17374441\n",
      "Iteration 32, loss = 0.09170004\n",
      "Iteration 16, loss = 0.16394219\n",
      "Iteration 33, loss = 0.07022907\n",
      "Iteration 17, loss = 0.15166134\n",
      "Iteration 34, loss = 0.08003735\n",
      "Iteration 18, loss = 0.14592075\n",
      "Iteration 35, loss = 0.07767116\n",
      "Iteration 19, loss = 0.13965566\n",
      "Iteration 36, loss = 0.06974236\n",
      "Iteration 20, loss = 0.13912325\n",
      "Iteration 37, loss = 0.07783307\n",
      "Iteration 21, loss = 0.13628011\n",
      "Iteration 38, loss = 0.09181768\n",
      "Iteration 22, loss = 0.15034346\n",
      "Iteration 39, loss = 0.08830802\n",
      "Iteration 23, loss = 0.13186883\n",
      "Iteration 40, loss = 0.10530843\n",
      "Iteration 24, loss = 0.13215517\n",
      "Iteration 41, loss = 0.10808379\n",
      "Iteration 25, loss = 0.11933448\n",
      "Iteration 42, loss = 0.08983026\n",
      "Iteration 26, loss = 0.12623450\n",
      "Iteration 43, loss = 0.09626977\n",
      "Iteration 27, loss = 0.12565952\n",
      "Iteration 44, loss = 0.07385911\n",
      "Iteration 28, loss = 0.12116902\n",
      "Iteration 45, loss = 0.07234714\n",
      "Iteration 29, loss = 0.11275887\n",
      "Iteration 46, loss = 0.10612802\n",
      "Iteration 30, loss = 0.10465362\n",
      "Iteration 47, loss = 0.08268312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.14709972\n",
      "Iteration 32, loss = 0.14637208\n",
      "Iteration 33, loss = 0.13314760\n",
      "Iteration 34, loss = 0.11847009\n",
      "Iteration 35, loss = 0.12347948\n",
      "Iteration 36, loss = 0.11084809\n",
      "Iteration 37, loss = 0.08980099\n",
      "Iteration 38, loss = 0.08754118\n",
      "Iteration 39, loss = 0.08227324\n",
      "Iteration 40, loss = 0.08719273\n",
      "Iteration 41, loss = 0.07849019\n",
      "Iteration 42, loss = 0.08026212\n",
      "Iteration 43, loss = 0.07781824\n",
      "Iteration 44, loss = 0.08255355\n",
      "Iteration 45, loss = 0.07457882\n",
      "Iteration 46, loss = 0.08584457\n",
      "Iteration 47, loss = 0.09063967\n",
      "Iteration 48, loss = 0.07492143\n",
      "Iteration 1, loss = 1.08163879\n",
      "Iteration 49, loss = 0.07988656\n",
      "Iteration 2, loss = 0.60056750\n",
      "Iteration 50, loss = 0.07550453\n",
      "Iteration 51, loss = 0.07846671\n",
      "Iteration 3, loss = 0.45059133\n",
      "Iteration 52, loss = 0.05850778\n",
      "Iteration 4, loss = 0.40496116\n",
      "Iteration 5, loss = 0.35729830\n",
      "Iteration 53, loss = 0.06335793\n",
      "Iteration 6, loss = 0.33100393\n",
      "Iteration 54, loss = 0.07645602\n",
      "Iteration 7, loss = 0.30106700\n",
      "Iteration 55, loss = 0.08267535\n",
      "Iteration 8, loss = 0.28038512\n",
      "Iteration 56, loss = 0.08880096\n",
      "Iteration 9, loss = 0.26544782\n",
      "Iteration 57, loss = 0.08427732\n",
      "Iteration 10, loss = 0.25900323\n",
      "Iteration 58, loss = 0.08196168\n",
      "Iteration 11, loss = 0.25667816\n",
      "Iteration 59, loss = 0.06710231\n",
      "Iteration 60, loss = 0.07185730\n",
      "Iteration 12, loss = 0.23943459\n",
      "Iteration 13, loss = 0.22463894Iteration 61, loss = 0.08795024\n",
      "\n",
      "Iteration 14, loss = 0.21451540\n",
      "Iteration 62, loss = 0.08679185\n",
      "Iteration 15, loss = 0.20542842\n",
      "Iteration 63, loss = 0.09185740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.19306358\n",
      "Iteration 17, loss = 0.20675072\n",
      "Iteration 18, loss = 0.17930066\n",
      "Iteration 19, loss = 0.16960256\n",
      "Iteration 20, loss = 0.16443088\n",
      "Iteration 21, loss = 0.15360809\n",
      "Iteration 22, loss = 0.14496889\n",
      "Iteration 23, loss = 0.13597727\n",
      "Iteration 24, loss = 0.15157646\n",
      "Iteration 25, loss = 0.15226177\n",
      "Iteration 26, loss = 0.15025518\n",
      "Iteration 27, loss = 0.15315039\n",
      "Iteration 28, loss = 0.16897107\n",
      "Iteration 29, loss = 0.15961144\n",
      "Iteration 30, loss = 0.12645549\n",
      "Iteration 31, loss = 0.11125656\n",
      "Iteration 32, loss = 0.11354582\n",
      "Iteration 33, loss = 0.11766108\n",
      "Iteration 1, loss = 0.90820782\n",
      "Iteration 34, loss = 0.10565279\n",
      "Iteration 2, loss = 0.53845453\n",
      "Iteration 35, loss = 0.10130196\n",
      "Iteration 3, loss = 0.42357115\n",
      "Iteration 36, loss = 0.09650664\n",
      "Iteration 4, loss = 0.37639824\n",
      "Iteration 37, loss = 0.12090925\n",
      "Iteration 5, loss = 0.34390210\n",
      "Iteration 38, loss = 0.09978199\n",
      "Iteration 6, loss = 0.31268067\n",
      "Iteration 39, loss = 0.11030899\n",
      "Iteration 7, loss = 0.29471875\n",
      "Iteration 40, loss = 0.10429435\n",
      "Iteration 8, loss = 0.27924657\n",
      "Iteration 41, loss = 0.09915930\n",
      "Iteration 9, loss = 0.26007928\n",
      "Iteration 42, loss = 0.10310101\n",
      "Iteration 10, loss = 0.25211152\n",
      "Iteration 43, loss = 0.09575909\n",
      "Iteration 11, loss = 0.23500789\n",
      "Iteration 44, loss = 0.07755668\n",
      "Iteration 12, loss = 0.22715448\n",
      "Iteration 45, loss = 0.07823900\n",
      "Iteration 13, loss = 0.23102613\n",
      "Iteration 46, loss = 0.08330977\n",
      "Iteration 14, loss = 0.20835607\n",
      "Iteration 47, loss = 0.08630405\n",
      "Iteration 15, loss = 0.20919515\n",
      "Iteration 48, loss = 0.06777054\n",
      "Iteration 16, loss = 0.19035988\n",
      "Iteration 49, loss = 0.08905410\n",
      "Iteration 17, loss = 0.18583851\n",
      "Iteration 50, loss = 0.07429415\n",
      "Iteration 18, loss = 0.18650652\n",
      "Iteration 51, loss = 0.06269309\n",
      "Iteration 19, loss = 0.17236489\n",
      "Iteration 52, loss = 0.07515887\n",
      "Iteration 20, loss = 0.16293302\n",
      "Iteration 53, loss = 0.09818802\n",
      "Iteration 21, loss = 0.14569154\n",
      "Iteration 54, loss = 0.08234878\n",
      "Iteration 22, loss = 0.15066376\n",
      "Iteration 55, loss = 0.06839949\n",
      "Iteration 23, loss = 0.14367660\n",
      "Iteration 56, loss = 0.06463620\n",
      "Iteration 24, loss = 0.12853005\n",
      "Iteration 57, loss = 0.06688467\n",
      "Iteration 25, loss = 0.13030257\n",
      "Iteration 58, loss = 0.08377072\n",
      "Iteration 26, loss = 0.14697366\n",
      "Iteration 59, loss = 0.07434554\n",
      "Iteration 27, loss = 0.12336701\n",
      "Iteration 60, loss = 0.06238708\n",
      "Iteration 28, loss = 0.15278294\n",
      "Iteration 61, loss = 0.06575839\n",
      "Iteration 29, loss = 0.12659767\n",
      "Iteration 62, loss = 0.06530933\n",
      "Iteration 30, loss = 0.12368131\n",
      "Iteration 63, loss = 0.07274003\n",
      "Iteration 31, loss = 0.12626810\n",
      "Iteration 64, loss = 0.08298646\n",
      "Iteration 32, loss = 0.11155952\n",
      "Iteration 65, loss = 0.09926560\n",
      "Iteration 33, loss = 0.12183945\n",
      "Iteration 66, loss = 0.09985965\n",
      "Iteration 34, loss = 0.15846570\n",
      "Iteration 67, loss = 0.08490145\n",
      "Iteration 35, loss = 0.12703120\n",
      "Iteration 68, loss = 0.05611664\n",
      "Iteration 36, loss = 0.12862272\n",
      "Iteration 69, loss = 0.05243332\n",
      "Iteration 37, loss = 0.11341927\n",
      "Iteration 70, loss = 0.05109874\n",
      "Iteration 38, loss = 0.11613350\n",
      "Iteration 71, loss = 0.04271595\n",
      "Iteration 39, loss = 0.10674797\n",
      "Iteration 72, loss = 0.04401739\n",
      "Iteration 40, loss = 0.10771722\n",
      "Iteration 73, loss = 0.03637172\n",
      "Iteration 41, loss = 0.11951118\n",
      "Iteration 74, loss = 0.04208906\n",
      "Iteration 42, loss = 0.14663360\n",
      "Iteration 75, loss = 0.05819480\n",
      "Iteration 43, loss = 0.10735502\n",
      "Iteration 76, loss = 0.05390725\n",
      "Iteration 44, loss = 0.11377110\n",
      "Iteration 77, loss = 0.06252646\n",
      "Iteration 45, loss = 0.08792367\n",
      "Iteration 78, loss = 0.09500221\n",
      "Iteration 46, loss = 0.08826310\n",
      "Iteration 79, loss = 0.10082346\n",
      "Iteration 47, loss = 0.09090722\n",
      "Iteration 80, loss = 0.09201722\n",
      "Iteration 48, loss = 0.08368426\n",
      "Iteration 81, loss = 0.09514637\n",
      "Iteration 49, loss = 0.06836554\n",
      "Iteration 82, loss = 0.05981295\n",
      "Iteration 50, loss = 0.06465097\n",
      "Iteration 83, loss = 0.04078970\n",
      "Iteration 51, loss = 0.07135594\n",
      "Iteration 52, loss = 0.07818545\n",
      "Iteration 84, loss = 0.05252816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.10045253\n",
      "Iteration 54, loss = 0.08626518\n",
      "Iteration 55, loss = 0.08544261\n",
      "Iteration 56, loss = 0.09121493\n",
      "Iteration 57, loss = 0.07027252\n",
      "Iteration 58, loss = 0.06403875\n",
      "Iteration 59, loss = 0.06151881\n",
      "Iteration 60, loss = 0.05726221\n",
      "Iteration 61, loss = 0.05478396\n",
      "Iteration 62, loss = 0.07315559\n",
      "Iteration 63, loss = 0.05036385\n",
      "Iteration 64, loss = 0.04720362\n",
      "Iteration 65, loss = 0.06083463\n",
      "Iteration 66, loss = 0.04819903\n",
      "Iteration 67, loss = 0.04419882\n",
      "Iteration 68, loss = 0.05751144\n",
      "Iteration 69, loss = 0.04489517\n",
      "Iteration 70, loss = 0.06086365\n",
      "Iteration 1, loss = 0.91509197\n",
      "Iteration 71, loss = 0.04784087\n",
      "Iteration 2, loss = 0.51575292\n",
      "Iteration 72, loss = 0.04025861\n",
      "Iteration 3, loss = 0.42087523\n",
      "Iteration 73, loss = 0.03064578\n",
      "Iteration 4, loss = 0.36390497\n",
      "Iteration 74, loss = 0.03394165\n",
      "Iteration 5, loss = 0.33201277\n",
      "Iteration 75, loss = 0.03236480\n",
      "Iteration 6, loss = 0.30998371\n",
      "Iteration 76, loss = 0.02990537\n",
      "Iteration 7, loss = 0.29085131\n",
      "Iteration 77, loss = 0.03506328\n",
      "Iteration 8, loss = 0.27393064\n",
      "Iteration 78, loss = 0.03377107\n",
      "Iteration 9, loss = 0.25918441\n",
      "Iteration 79, loss = 0.03139276\n",
      "Iteration 10, loss = 0.24208448\n",
      "Iteration 80, loss = 0.03074739\n",
      "Iteration 11, loss = 0.22698426\n",
      "Iteration 81, loss = 0.03304738\n",
      "Iteration 12, loss = 0.21445927\n",
      "Iteration 82, loss = 0.03252322\n",
      "Iteration 13, loss = 0.22018360\n",
      "Iteration 83, loss = 0.04058600\n",
      "Iteration 14, loss = 0.19929500\n",
      "Iteration 84, loss = 0.03455090\n",
      "Iteration 15, loss = 0.19659182\n",
      "Iteration 85, loss = 0.04942547\n",
      "Iteration 16, loss = 0.17330638\n",
      "Iteration 86, loss = 0.05930486\n",
      "Iteration 17, loss = 0.16355048\n",
      "Iteration 87, loss = 0.05004495\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.16372000\n",
      "Iteration 19, loss = 0.16699610\n",
      "Iteration 20, loss = 0.14855605\n",
      "Iteration 21, loss = 0.15715791\n",
      "Iteration 22, loss = 0.13499147\n",
      "Iteration 23, loss = 0.13288593\n",
      "Iteration 24, loss = 0.12960164\n",
      "Iteration 25, loss = 0.12875750\n",
      "Iteration 26, loss = 0.14049209\n",
      "Iteration 27, loss = 0.15214181\n",
      "Iteration 28, loss = 0.12074310\n",
      "Iteration 29, loss = 0.11118231\n",
      "Iteration 30, loss = 0.11112021\n",
      "Iteration 31, loss = 0.11069938\n",
      "Iteration 32, loss = 0.11482989\n",
      "Iteration 33, loss = 0.12393593\n",
      "Iteration 34, loss = 0.09819409\n",
      "Iteration 35, loss = 0.10367752\n",
      "Iteration 1, loss = 0.82795750\n",
      "Iteration 36, loss = 0.08776618\n",
      "Iteration 2, loss = 0.47384538\n",
      "Iteration 37, loss = 0.08266429\n",
      "Iteration 3, loss = 0.40284350\n",
      "Iteration 38, loss = 0.07885913\n",
      "Iteration 4, loss = 0.37291586\n",
      "Iteration 39, loss = 0.08132874\n",
      "Iteration 5, loss = 0.34116341\n",
      "Iteration 40, loss = 0.07990544\n",
      "Iteration 6, loss = 0.31503933\n",
      "Iteration 41, loss = 0.07385749\n",
      "Iteration 7, loss = 0.29239651\n",
      "Iteration 8, loss = 0.27740664\n",
      "Iteration 9, loss = 0.26373745\n",
      "Iteration 42, loss = 0.08153728\n",
      "Iteration 10, loss = 0.24348786\n",
      "Iteration 43, loss = 0.08223699\n",
      "Iteration 11, loss = 0.24523749\n",
      "Iteration 44, loss = 0.08002884\n",
      "Iteration 12, loss = 0.24584799\n",
      "Iteration 13, loss = 0.21977135\n",
      "Iteration 45, loss = 0.07786740\n",
      "Iteration 14, loss = 0.21178921\n",
      "Iteration 15, loss = 0.19503035\n",
      "Iteration 46, loss = 0.09189400\n",
      "Iteration 16, loss = 0.18819375\n",
      "Iteration 17, loss = 0.17141162\n",
      "Iteration 47, loss = 0.11327695\n",
      "Iteration 18, loss = 0.16960743\n",
      "Iteration 19, loss = 0.18488911\n",
      "Iteration 20, loss = 0.18461361\n",
      "Iteration 48, loss = 0.10468672\n",
      "Iteration 21, loss = 0.17638011\n",
      "Iteration 49, loss = 0.13376540\n",
      "Iteration 22, loss = 0.17900808\n",
      "Iteration 50, loss = 0.14989073\n",
      "Iteration 23, loss = 0.15367038\n",
      "Iteration 51, loss = 0.11341566\n",
      "Iteration 24, loss = 0.16746096\n",
      "Iteration 52, loss = 0.09803413\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.14777587\n",
      "Iteration 26, loss = 0.12472858\n",
      "Iteration 27, loss = 0.12449057\n",
      "Iteration 28, loss = 0.12917359\n",
      "Iteration 29, loss = 0.11489740\n",
      "Iteration 30, loss = 0.12307489\n",
      "Iteration 31, loss = 0.12149238\n",
      "Iteration 32, loss = 0.11048646\n",
      "Iteration 33, loss = 0.10071382\n",
      "Iteration 34, loss = 0.08991308\n",
      "Iteration 35, loss = 0.08335633\n",
      "Iteration 36, loss = 0.08636460\n",
      "Iteration 37, loss = 0.07971242\n",
      "Iteration 38, loss = 0.09098626\n",
      "Iteration 39, loss = 0.08049750\n",
      "Iteration 40, loss = 0.08904403\n",
      "Iteration 41, loss = 0.07162587\n",
      "Iteration 42, loss = 0.07440070\n",
      "Iteration 43, loss = 0.06964248\n",
      "Iteration 1, loss = 1.04163090\n",
      "Iteration 44, loss = 0.06994440\n",
      "Iteration 2, loss = 0.56257932\n",
      "Iteration 45, loss = 0.07714522\n",
      "Iteration 3, loss = 0.43434009\n",
      "Iteration 46, loss = 0.07864726\n",
      "Iteration 4, loss = 0.38668527\n",
      "Iteration 47, loss = 0.07205696\n",
      "Iteration 5, loss = 0.36782953\n",
      "Iteration 48, loss = 0.07923402\n",
      "Iteration 6, loss = 0.34002715\n",
      "Iteration 49, loss = 0.07871040\n",
      "Iteration 7, loss = 0.31349225\n",
      "Iteration 50, loss = 0.08308939\n",
      "Iteration 8, loss = 0.28853888\n",
      "Iteration 51, loss = 0.06713708\n",
      "Iteration 9, loss = 0.27964169\n",
      "Iteration 52, loss = 0.07884616\n",
      "Iteration 10, loss = 0.27188016\n",
      "Iteration 53, loss = 0.06584789\n",
      "Iteration 11, loss = 0.25434045\n",
      "Iteration 54, loss = 0.05736717\n",
      "Iteration 12, loss = 0.23868638\n",
      "Iteration 55, loss = 0.06141527\n",
      "Iteration 13, loss = 0.23050699\n",
      "Iteration 56, loss = 0.06106423\n",
      "Iteration 14, loss = 0.22215539\n",
      "Iteration 57, loss = 0.05190017\n",
      "Iteration 15, loss = 0.20958077\n",
      "Iteration 58, loss = 0.06304705\n",
      "Iteration 16, loss = 0.20639070\n",
      "Iteration 59, loss = 0.09380524\n",
      "Iteration 17, loss = 0.19110962\n",
      "Iteration 60, loss = 0.09047136\n",
      "Iteration 18, loss = 0.18696745\n",
      "Iteration 61, loss = 0.08030901\n",
      "Iteration 19, loss = 0.17378499\n",
      "Iteration 62, loss = 0.06951357\n",
      "Iteration 20, loss = 0.18396979\n",
      "Iteration 63, loss = 0.05427525\n",
      "Iteration 21, loss = 0.17703697\n",
      "Iteration 64, loss = 0.05501408\n",
      "Iteration 22, loss = 0.19682532\n",
      "Iteration 23, loss = 0.18998224\n",
      "Iteration 65, loss = 0.04644433\n",
      "Iteration 66, loss = 0.05716731Iteration 24, loss = 0.17115651\n",
      "\n",
      "Iteration 67, loss = 0.04359250\n",
      "Iteration 25, loss = 0.14893898\n",
      "Iteration 68, loss = 0.05309225\n",
      "Iteration 26, loss = 0.14555996\n",
      "Iteration 69, loss = 0.04225362\n",
      "Iteration 27, loss = 0.13936857\n",
      "Iteration 28, loss = 0.13231317\n",
      "Iteration 70, loss = 0.04670151\n",
      "Iteration 29, loss = 0.12973747\n",
      "Iteration 71, loss = 0.05030914\n",
      "Iteration 30, loss = 0.11955602\n",
      "Iteration 72, loss = 0.07773154\n",
      "Iteration 31, loss = 0.12527115\n",
      "Iteration 73, loss = 0.07182896\n",
      "Iteration 32, loss = 0.12745331\n",
      "Iteration 74, loss = 0.08564176\n",
      "Iteration 33, loss = 0.15646874\n",
      "Iteration 75, loss = 0.08582344\n",
      "Iteration 34, loss = 0.12637528\n",
      "Iteration 76, loss = 0.06927218\n",
      "Iteration 35, loss = 0.12497596\n",
      "Iteration 77, loss = 0.10802519\n",
      "Iteration 36, loss = 0.10706348\n",
      "Iteration 78, loss = 0.16691590\n",
      "Iteration 37, loss = 0.10591652\n",
      "Iteration 79, loss = 0.13446473\n",
      "Iteration 38, loss = 0.09989401\n",
      "Iteration 80, loss = 0.10224455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.10255249\n",
      "Iteration 40, loss = 0.10487441\n",
      "Iteration 41, loss = 0.10468174\n",
      "Iteration 42, loss = 0.12456180\n",
      "Iteration 43, loss = 0.11290435\n",
      "Iteration 44, loss = 0.13837945\n",
      "Iteration 45, loss = 0.15975434\n",
      "Iteration 46, loss = 0.11364880\n",
      "Iteration 47, loss = 0.10533730\n",
      "Iteration 48, loss = 0.09600880\n",
      "Iteration 49, loss = 0.09855271\n",
      "Iteration 50, loss = 0.09140653\n",
      "Iteration 51, loss = 0.10562572\n",
      "Iteration 52, loss = 0.08554042\n",
      "Iteration 53, loss = 0.09890750\n",
      "Iteration 54, loss = 0.08550740\n",
      "Iteration 55, loss = 0.07949153\n",
      "Iteration 56, loss = 0.07114518\n",
      "Iteration 57, loss = 0.06383131\n",
      "Iteration 1, loss = 0.83524494\n",
      "Iteration 58, loss = 0.07072973\n",
      "Iteration 2, loss = 0.53560528\n",
      "Iteration 59, loss = 0.07371785\n",
      "Iteration 3, loss = 0.43692267\n",
      "Iteration 60, loss = 0.08894208\n",
      "Iteration 4, loss = 0.39065613\n",
      "Iteration 61, loss = 0.07890568\n",
      "Iteration 5, loss = 0.34237438\n",
      "Iteration 62, loss = 0.06841800\n",
      "Iteration 6, loss = 0.31735681\n",
      "Iteration 63, loss = 0.07691575\n",
      "Iteration 7, loss = 0.31268944\n",
      "Iteration 64, loss = 0.08933849\n",
      "Iteration 8, loss = 0.29395747\n",
      "Iteration 65, loss = 0.09143976\n",
      "Iteration 9, loss = 0.27639886\n",
      "Iteration 66, loss = 0.11128406\n",
      "Iteration 10, loss = 0.26190081\n",
      "Iteration 67, loss = 0.15366058\n",
      "Iteration 11, loss = 0.24761755\n",
      "Iteration 68, loss = 0.15324986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.23433667\n",
      "Iteration 13, loss = 0.22127700\n",
      "Iteration 14, loss = 0.22834326\n",
      "Iteration 15, loss = 0.21278235\n",
      "Iteration 16, loss = 0.21210097\n",
      "Iteration 17, loss = 0.19795099\n",
      "Iteration 18, loss = 0.18594544\n",
      "Iteration 19, loss = 0.18346876\n",
      "Iteration 20, loss = 0.17642569\n",
      "Iteration 21, loss = 0.16745241\n",
      "Iteration 22, loss = 0.17036037\n",
      "Iteration 23, loss = 0.15523902\n",
      "Iteration 24, loss = 0.18636135\n",
      "Iteration 25, loss = 0.15858928\n",
      "Iteration 26, loss = 0.15381048\n",
      "Iteration 27, loss = 0.14111112\n",
      "Iteration 28, loss = 0.15052164\n",
      "Iteration 1, loss = 1.12800673\n",
      "Iteration 29, loss = 0.14785296\n",
      "Iteration 2, loss = 0.59008526\n",
      "Iteration 30, loss = 0.13148209\n",
      "Iteration 3, loss = 0.45025907\n",
      "Iteration 31, loss = 0.13145372\n",
      "Iteration 4, loss = 0.39854003\n",
      "Iteration 32, loss = 0.12506256\n",
      "Iteration 5, loss = 0.36527391\n",
      "Iteration 33, loss = 0.11556938\n",
      "Iteration 6, loss = 0.33016658\n",
      "Iteration 34, loss = 0.10337147\n",
      "Iteration 35, loss = 0.10764893\n",
      "Iteration 7, loss = 0.30741434\n",
      "Iteration 36, loss = 0.11687149\n",
      "Iteration 8, loss = 0.29471173\n",
      "Iteration 37, loss = 0.11705852\n",
      "Iteration 9, loss = 0.27670210\n",
      "Iteration 38, loss = 0.13121473\n",
      "Iteration 10, loss = 0.26135679\n",
      "Iteration 11, loss = 0.24950403\n",
      "Iteration 39, loss = 0.11414214\n",
      "Iteration 40, loss = 0.12082709\n",
      "Iteration 12, loss = 0.23881600\n",
      "Iteration 41, loss = 0.12551645\n",
      "Iteration 13, loss = 0.22185104\n",
      "Iteration 42, loss = 0.15153442\n",
      "Iteration 14, loss = 0.21218470\n",
      "Iteration 43, loss = 0.15891479\n",
      "Iteration 15, loss = 0.19701011\n",
      "Iteration 44, loss = 0.14122870\n",
      "Iteration 16, loss = 0.19541269\n",
      "Iteration 45, loss = 0.12779443\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.18741969\n",
      "Iteration 18, loss = 0.17709581\n",
      "Iteration 19, loss = 0.18069476\n",
      "Iteration 20, loss = 0.16968532\n",
      "Iteration 21, loss = 0.16197726\n",
      "Iteration 22, loss = 0.15015499\n",
      "Iteration 23, loss = 0.14061461\n",
      "Iteration 24, loss = 0.13651849\n",
      "Iteration 25, loss = 0.13395961\n",
      "Iteration 26, loss = 0.14446118\n",
      "Iteration 27, loss = 0.13285371\n",
      "Iteration 28, loss = 0.12616144\n",
      "Iteration 29, loss = 0.12417337\n",
      "Iteration 30, loss = 0.12088513\n",
      "Iteration 31, loss = 0.11846263\n",
      "Iteration 32, loss = 0.11048446\n",
      "Iteration 33, loss = 0.10462911\n",
      "Iteration 34, loss = 0.10586494\n",
      "Iteration 35, loss = 0.11012265\n",
      "Iteration 1, loss = 1.01409301\n",
      "Iteration 36, loss = 0.10652692\n",
      "Iteration 2, loss = 0.58610797\n",
      "Iteration 37, loss = 0.11504982\n",
      "Iteration 3, loss = 0.45467741\n",
      "Iteration 38, loss = 0.10160708\n",
      "Iteration 4, loss = 0.40124945\n",
      "Iteration 39, loss = 0.11274971\n",
      "Iteration 5, loss = 0.36933215\n",
      "Iteration 40, loss = 0.12134183\n",
      "Iteration 6, loss = 0.33232360\n",
      "Iteration 41, loss = 0.11814068\n",
      "Iteration 7, loss = 0.30396007\n",
      "Iteration 42, loss = 0.11084036\n",
      "Iteration 8, loss = 0.29325157\n",
      "Iteration 43, loss = 0.12978036\n",
      "Iteration 9, loss = 0.26990236\n",
      "Iteration 44, loss = 0.12177437\n",
      "Iteration 10, loss = 0.25113602\n",
      "Iteration 45, loss = 0.11579427\n",
      "Iteration 11, loss = 0.24668799\n",
      "Iteration 46, loss = 0.09691555\n",
      "Iteration 12, loss = 0.23150460\n",
      "Iteration 47, loss = 0.10906156\n",
      "Iteration 13, loss = 0.22325684\n",
      "Iteration 48, loss = 0.09284826\n",
      "Iteration 14, loss = 0.22665071\n",
      "Iteration 49, loss = 0.07815922\n",
      "Iteration 15, loss = 0.21448736\n",
      "Iteration 50, loss = 0.07715959\n",
      "Iteration 16, loss = 0.19013084\n",
      "Iteration 51, loss = 0.07124340\n",
      "Iteration 17, loss = 0.18355827\n",
      "Iteration 52, loss = 0.07068433\n",
      "Iteration 18, loss = 0.17325681\n",
      "Iteration 53, loss = 0.07963081\n",
      "Iteration 19, loss = 0.17643308\n",
      "Iteration 54, loss = 0.07813224\n",
      "Iteration 20, loss = 0.17679498\n",
      "Iteration 55, loss = 0.07990201\n",
      "Iteration 21, loss = 0.16458755\n",
      "Iteration 56, loss = 0.06908974\n",
      "Iteration 22, loss = 0.17306489\n",
      "Iteration 57, loss = 0.07633077\n",
      "Iteration 23, loss = 0.15544739\n",
      "Iteration 58, loss = 0.06180909\n",
      "Iteration 24, loss = 0.14404210\n",
      "Iteration 59, loss = 0.06725629\n",
      "Iteration 25, loss = 0.13758438\n",
      "Iteration 60, loss = 0.05882094\n",
      "Iteration 26, loss = 0.13307557\n",
      "Iteration 61, loss = 0.07071066\n",
      "Iteration 27, loss = 0.13945104\n",
      "Iteration 62, loss = 0.05533416\n",
      "Iteration 28, loss = 0.13435608\n",
      "Iteration 63, loss = 0.05048438\n",
      "Iteration 29, loss = 0.13482858\n",
      "Iteration 64, loss = 0.04954962\n",
      "Iteration 30, loss = 0.11818197\n",
      "Iteration 65, loss = 0.06296084\n",
      "Iteration 31, loss = 0.13395789\n",
      "Iteration 66, loss = 0.05669800\n",
      "Iteration 32, loss = 0.15485582\n",
      "Iteration 67, loss = 0.05053788\n",
      "Iteration 33, loss = 0.14244415\n",
      "Iteration 68, loss = 0.04925861\n",
      "Iteration 34, loss = 0.19706770\n",
      "Iteration 69, loss = 0.04265274\n",
      "Iteration 35, loss = 0.13520563\n",
      "Iteration 70, loss = 0.05266683\n",
      "Iteration 36, loss = 0.13724487\n",
      "Iteration 71, loss = 0.05067290\n",
      "Iteration 37, loss = 0.13910882\n",
      "Iteration 72, loss = 0.04190922\n",
      "Iteration 38, loss = 0.13426320\n",
      "Iteration 73, loss = 0.04708248\n",
      "Iteration 39, loss = 0.11650591\n",
      "Iteration 74, loss = 0.04439933\n",
      "Iteration 40, loss = 0.11193845\n",
      "Iteration 75, loss = 0.05316396\n",
      "Iteration 41, loss = 0.14167954\n",
      "Iteration 76, loss = 0.04649475\n",
      "Iteration 42, loss = 0.10051814\n",
      "Iteration 77, loss = 0.05687258\n",
      "Iteration 43, loss = 0.09595270\n",
      "Iteration 78, loss = 0.05949965\n",
      "Iteration 44, loss = 0.08957965\n",
      "Iteration 79, loss = 0.06497484\n",
      "Iteration 45, loss = 0.09394339\n",
      "Iteration 80, loss = 0.05286783\n",
      "Iteration 46, loss = 0.08384457\n",
      "Iteration 81, loss = 0.05617612\n",
      "Iteration 47, loss = 0.09063754\n",
      "Iteration 82, loss = 0.04667096\n",
      "Iteration 48, loss = 0.09055390\n",
      "Iteration 83, loss = 0.05503164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.08806668\n",
      "Iteration 50, loss = 0.09516948\n",
      "Iteration 51, loss = 0.10085020\n",
      "Iteration 52, loss = 0.09804846\n",
      "Iteration 53, loss = 0.09567887\n",
      "Iteration 54, loss = 0.08880957\n",
      "Iteration 55, loss = 0.08135980\n",
      "Iteration 56, loss = 0.07131729\n",
      "Iteration 57, loss = 0.10338548\n",
      "Iteration 58, loss = 0.08203530\n",
      "Iteration 59, loss = 0.10407338\n",
      "Iteration 60, loss = 0.09100142\n",
      "Iteration 61, loss = 0.09490296\n",
      "Iteration 62, loss = 0.08440308\n",
      "Iteration 63, loss = 0.08557848\n",
      "Iteration 64, loss = 0.07767750\n",
      "Iteration 65, loss = 0.08248966\n",
      "Iteration 66, loss = 0.06715690\n",
      "Iteration 1, loss = 0.95891185\n",
      "Iteration 67, loss = 0.07438351\n",
      "Iteration 2, loss = 0.52872118\n",
      "Iteration 68, loss = 0.07973782\n",
      "Iteration 3, loss = 0.40093207\n",
      "Iteration 69, loss = 0.07229820\n",
      "Iteration 4, loss = 0.38200549\n",
      "Iteration 70, loss = 0.08809600\n",
      "Iteration 5, loss = 0.37666023\n",
      "Iteration 71, loss = 0.06587585\n",
      "Iteration 6, loss = 0.36123993\n",
      "Iteration 72, loss = 0.07759092\n",
      "Iteration 7, loss = 0.32466045\n",
      "Iteration 73, loss = 0.07930610\n",
      "Iteration 8, loss = 0.30023986\n",
      "Iteration 74, loss = 0.06799259\n",
      "Iteration 9, loss = 0.26237728\n",
      "Iteration 75, loss = 0.07421200\n",
      "Iteration 10, loss = 0.24232143\n",
      "Iteration 76, loss = 0.06412758\n",
      "Iteration 11, loss = 0.25095628\n",
      "Iteration 77, loss = 0.06297905\n",
      "Iteration 12, loss = 0.24502955\n",
      "Iteration 78, loss = 0.06923028\n",
      "Iteration 13, loss = 0.21853380\n",
      "Iteration 79, loss = 0.06068205\n",
      "Iteration 14, loss = 0.20318836\n",
      "Iteration 80, loss = 0.07940307\n",
      "Iteration 15, loss = 0.19704619\n",
      "Iteration 81, loss = 0.08002571\n",
      "Iteration 16, loss = 0.17119619\n",
      "Iteration 82, loss = 0.11017010\n",
      "Iteration 17, loss = 0.16880562\n",
      "Iteration 83, loss = 0.09875385\n",
      "Iteration 18, loss = 0.15387071\n",
      "Iteration 84, loss = 0.07059072\n",
      "Iteration 19, loss = 0.14644703\n",
      "Iteration 85, loss = 0.08808012\n",
      "Iteration 20, loss = 0.15169187\n",
      "Iteration 86, loss = 0.07136248\n",
      "Iteration 21, loss = 0.15285140\n",
      "Iteration 87, loss = 0.05156706\n",
      "Iteration 22, loss = 0.12794713\n",
      "Iteration 88, loss = 0.04664736\n",
      "Iteration 23, loss = 0.12093142\n",
      "Iteration 89, loss = 0.04734449\n",
      "Iteration 24, loss = 0.12045247\n",
      "Iteration 90, loss = 0.03933990\n",
      "Iteration 25, loss = 0.11242082\n",
      "Iteration 91, loss = 0.03679297\n",
      "Iteration 26, loss = 0.11240523\n",
      "Iteration 92, loss = 0.03485645\n",
      "Iteration 27, loss = 0.11640554\n",
      "Iteration 93, loss = 0.03522291\n",
      "Iteration 28, loss = 0.09287329\n",
      "Iteration 94, loss = 0.05876970\n",
      "Iteration 29, loss = 0.09544637\n",
      "Iteration 95, loss = 0.05192435\n",
      "Iteration 30, loss = 0.08811153\n",
      "Iteration 96, loss = 0.05777234\n",
      "Iteration 31, loss = 0.07875852\n",
      "Iteration 97, loss = 0.03848005\n",
      "Iteration 32, loss = 0.07928434\n",
      "Iteration 98, loss = 0.04382116\n",
      "Iteration 33, loss = 0.06900100\n",
      "Iteration 99, loss = 0.06000840\n",
      "Iteration 34, loss = 0.06268242\n",
      "Iteration 100, loss = 0.04085182\n",
      "Iteration 35, loss = 0.06037080\n",
      "Iteration 101, loss = 0.03632508\n",
      "Iteration 36, loss = 0.06294673\n",
      "Iteration 102, loss = 0.04157178\n",
      "Iteration 37, loss = 0.05846044\n",
      "Iteration 103, loss = 0.04991811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.05975393\n",
      "Iteration 39, loss = 0.06159973\n",
      "Iteration 40, loss = 0.07486913\n",
      "Iteration 41, loss = 0.07394367\n",
      "Iteration 42, loss = 0.06442304\n",
      "Iteration 43, loss = 0.05544116\n",
      "Iteration 44, loss = 0.05344071\n",
      "Iteration 45, loss = 0.05327633\n",
      "Iteration 46, loss = 0.04733882\n",
      "Iteration 47, loss = 0.07300071\n",
      "Iteration 48, loss = 0.07284848\n",
      "Iteration 49, loss = 0.08616463\n",
      "Iteration 50, loss = 0.05946940\n",
      "Iteration 51, loss = 0.07230974\n",
      "Iteration 52, loss = 0.06583478\n",
      "Iteration 53, loss = 0.06687242\n",
      "Iteration 54, loss = 0.04373105\n",
      "Iteration 55, loss = 0.04554899\n",
      "Iteration 1, loss = 0.97186165\n",
      "Iteration 56, loss = 0.04594199\n",
      "Iteration 57, loss = 0.05079357\n",
      "Iteration 2, loss = 0.53217041\n",
      "Iteration 58, loss = 0.06553997\n",
      "Iteration 3, loss = 0.39781356\n",
      "Iteration 59, loss = 0.04313649\n",
      "Iteration 4, loss = 0.35124846\n",
      "Iteration 60, loss = 0.02891133\n",
      "Iteration 5, loss = 0.30942066\n",
      "Iteration 6, loss = 0.30115441\n",
      "Iteration 61, loss = 0.03632716\n",
      "Iteration 7, loss = 0.29186725\n",
      "Iteration 62, loss = 0.02882239\n",
      "Iteration 8, loss = 0.27012673\n",
      "Iteration 63, loss = 0.02256004\n",
      "Iteration 9, loss = 0.25264646\n",
      "Iteration 64, loss = 0.02326089\n",
      "Iteration 10, loss = 0.22530752\n",
      "Iteration 65, loss = 0.02438681\n",
      "Iteration 11, loss = 0.20827530\n",
      "Iteration 66, loss = 0.03050717\n",
      "Iteration 12, loss = 0.19686393\n",
      "Iteration 67, loss = 0.02853876\n",
      "Iteration 13, loss = 0.18742258\n",
      "Iteration 68, loss = 0.04908784\n",
      "Iteration 14, loss = 0.17946483\n",
      "Iteration 69, loss = 0.05060127\n",
      "Iteration 15, loss = 0.16745514\n",
      "Iteration 70, loss = 0.04799228\n",
      "Iteration 16, loss = 0.15871017\n",
      "Iteration 71, loss = 0.06637015\n",
      "Iteration 17, loss = 0.16267070\n",
      "Iteration 72, loss = 0.07166961\n",
      "Iteration 18, loss = 0.16171385\n",
      "Iteration 73, loss = 0.10149631\n",
      "Iteration 19, loss = 0.14789411\n",
      "Iteration 74, loss = 0.10908923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.13597261\n",
      "Iteration 21, loss = 0.13951914\n",
      "Iteration 22, loss = 0.14133342\n",
      "Iteration 23, loss = 0.12015092\n",
      "Iteration 24, loss = 0.11910472\n",
      "Iteration 25, loss = 0.11680543\n",
      "Iteration 26, loss = 0.11424499\n",
      "Iteration 27, loss = 0.11696329\n",
      "Iteration 28, loss = 0.12447217\n",
      "Iteration 29, loss = 0.12235527\n",
      "Iteration 30, loss = 0.11394897\n",
      "Iteration 31, loss = 0.13573304\n",
      "Iteration 32, loss = 0.10494734\n",
      "Iteration 33, loss = 0.10189444\n",
      "Iteration 34, loss = 0.09800838\n",
      "Iteration 35, loss = 0.10073601\n",
      "Iteration 36, loss = 0.09809619\n",
      "Iteration 37, loss = 0.09167164\n",
      "Iteration 1, loss = 0.93783269\n",
      "Iteration 38, loss = 0.08498671\n",
      "Iteration 39, loss = 0.08503867\n",
      "Iteration 2, loss = 0.55236637\n",
      "Iteration 40, loss = 0.08127861\n",
      "Iteration 3, loss = 0.41416305\n",
      "Iteration 41, loss = 0.08489775\n",
      "Iteration 4, loss = 0.36848884\n",
      "Iteration 42, loss = 0.09554271\n",
      "Iteration 5, loss = 0.33200996\n",
      "Iteration 43, loss = 0.10955219\n",
      "Iteration 6, loss = 0.30566288\n",
      "Iteration 44, loss = 0.12892989\n",
      "Iteration 7, loss = 0.28701322\n",
      "Iteration 45, loss = 0.11344241\n",
      "Iteration 8, loss = 0.26449507\n",
      "Iteration 46, loss = 0.09737814\n",
      "Iteration 9, loss = 0.24656280\n",
      "Iteration 47, loss = 0.08688462\n",
      "Iteration 10, loss = 0.24423619\n",
      "Iteration 48, loss = 0.10122705\n",
      "Iteration 11, loss = 0.22713842\n",
      "Iteration 49, loss = 0.08270029\n",
      "Iteration 12, loss = 0.22366852\n",
      "Iteration 50, loss = 0.06809806\n",
      "Iteration 13, loss = 0.20208687\n",
      "Iteration 51, loss = 0.06450379\n",
      "Iteration 14, loss = 0.19819750\n",
      "Iteration 15, loss = 0.18059976\n",
      "Iteration 52, loss = 0.06580608\n",
      "Iteration 53, loss = 0.08064837\n",
      "Iteration 16, loss = 0.17757077\n",
      "Iteration 54, loss = 0.07565442\n",
      "Iteration 17, loss = 0.16826795\n",
      "Iteration 55, loss = 0.06204280\n",
      "Iteration 18, loss = 0.17086108\n",
      "Iteration 56, loss = 0.06096898\n",
      "Iteration 19, loss = 0.16206229\n",
      "Iteration 57, loss = 0.05937453\n",
      "Iteration 20, loss = 0.15070385\n",
      "Iteration 58, loss = 0.05423194\n",
      "Iteration 21, loss = 0.14316563\n",
      "Iteration 59, loss = 0.05948287\n",
      "Iteration 22, loss = 0.13756983\n",
      "Iteration 60, loss = 0.07080932\n",
      "Iteration 23, loss = 0.13712027\n",
      "Iteration 61, loss = 0.05813899\n",
      "Iteration 24, loss = 0.13616052\n",
      "Iteration 62, loss = 0.06115677\n",
      "Iteration 25, loss = 0.14324003\n",
      "Iteration 63, loss = 0.05850680\n",
      "Iteration 26, loss = 0.12856110\n",
      "Iteration 64, loss = 0.07453251\n",
      "Iteration 27, loss = 0.13262391\n",
      "Iteration 65, loss = 0.08397880\n",
      "Iteration 28, loss = 0.13812520\n",
      "Iteration 66, loss = 0.11804970\n",
      "Iteration 29, loss = 0.14239504\n",
      "Iteration 67, loss = 0.07997503\n",
      "Iteration 30, loss = 0.12098328\n",
      "Iteration 68, loss = 0.06622940\n",
      "Iteration 31, loss = 0.12064613\n",
      "Iteration 69, loss = 0.06596556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.12178671\n",
      "Iteration 33, loss = 0.12471739\n",
      "Iteration 34, loss = 0.16311078\n",
      "Iteration 35, loss = 0.11445031\n",
      "Iteration 36, loss = 0.10525770\n",
      "Iteration 37, loss = 0.09646732\n",
      "Iteration 38, loss = 0.08766239\n",
      "Iteration 39, loss = 0.09365870\n",
      "Iteration 40, loss = 0.10140766\n",
      "Iteration 41, loss = 0.08847079\n",
      "Iteration 42, loss = 0.08038917\n",
      "Iteration 43, loss = 0.09052971\n",
      "Iteration 44, loss = 0.08339919\n",
      "Iteration 45, loss = 0.06991249\n",
      "Iteration 46, loss = 0.07581503\n",
      "Iteration 47, loss = 0.06654398\n",
      "Iteration 48, loss = 0.06363078\n",
      "Iteration 49, loss = 0.06089241\n",
      "Iteration 50, loss = 0.06453022\n",
      "Iteration 1, loss = 1.07822984\n",
      "Iteration 51, loss = 0.07621827\n",
      "Iteration 2, loss = 0.92728659\n",
      "Iteration 52, loss = 0.06565477\n",
      "Iteration 3, loss = 0.82939736\n",
      "Iteration 53, loss = 0.08847128\n",
      "Iteration 4, loss = 0.74161567\n",
      "Iteration 54, loss = 0.07562388\n",
      "Iteration 5, loss = 0.67394744\n",
      "Iteration 55, loss = 0.06633877\n",
      "Iteration 6, loss = 0.62206564\n",
      "Iteration 56, loss = 0.05899437\n",
      "Iteration 7, loss = 0.57793541\n",
      "Iteration 57, loss = 0.05102962\n",
      "Iteration 8, loss = 0.54359303\n",
      "Iteration 58, loss = 0.06227754\n",
      "Iteration 59, loss = 0.06521094\n",
      "Iteration 9, loss = 0.51438220\n",
      "Iteration 10, loss = 0.49287900\n",
      "Iteration 60, loss = 0.08107702\n",
      "Iteration 11, loss = 0.47224861\n",
      "Iteration 61, loss = 0.09060796\n",
      "Iteration 62, loss = 0.09049692\n",
      "Iteration 12, loss = 0.45443127\n",
      "Iteration 63, loss = 0.11810480\n",
      "Iteration 13, loss = 0.43940474\n",
      "Iteration 14, loss = 0.42628306\n",
      "Iteration 64, loss = 0.11336076\n",
      "Iteration 15, loss = 0.41577685\n",
      "Iteration 65, loss = 0.16989328\n",
      "Iteration 16, loss = 0.40319561\n",
      "Iteration 66, loss = 0.23006945\n",
      "Iteration 67, loss = 0.15578204Iteration 17, loss = 0.39474482\n",
      "\n",
      "Iteration 68, loss = 0.12774961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.38530947\n",
      "Iteration 19, loss = 0.37789975\n",
      "Iteration 20, loss = 0.37093969\n",
      "Iteration 21, loss = 0.36235029\n",
      "Iteration 22, loss = 0.35625497\n",
      "Iteration 23, loss = 0.34958395\n",
      "Iteration 24, loss = 0.34388497\n",
      "Iteration 25, loss = 0.33807192\n",
      "Iteration 26, loss = 0.33331071\n",
      "Iteration 27, loss = 0.32681847\n",
      "Iteration 28, loss = 0.32335465\n",
      "Iteration 29, loss = 0.31743026\n",
      "Iteration 30, loss = 0.31357052\n",
      "Iteration 31, loss = 0.30832225\n",
      "Iteration 32, loss = 0.30499999\n",
      "Iteration 33, loss = 0.29947852\n",
      "Iteration 34, loss = 0.29563509\n",
      "Iteration 35, loss = 0.29062009\n",
      "Iteration 1, loss = 1.05222196\n",
      "Iteration 36, loss = 0.28760959\n",
      "Iteration 2, loss = 0.89787417\n",
      "Iteration 37, loss = 0.28333764\n",
      "Iteration 3, loss = 0.80917583\n",
      "Iteration 38, loss = 0.28044006\n",
      "Iteration 4, loss = 0.73514536\n",
      "Iteration 39, loss = 0.27647313\n",
      "Iteration 5, loss = 0.66901120\n",
      "Iteration 40, loss = 0.27383000\n",
      "Iteration 6, loss = 0.61861475\n",
      "Iteration 41, loss = 0.26991927\n",
      "Iteration 7, loss = 0.57586158\n",
      "Iteration 42, loss = 0.26686166\n",
      "Iteration 8, loss = 0.54033223\n",
      "Iteration 43, loss = 0.26398145\n",
      "Iteration 9, loss = 0.51118038\n",
      "Iteration 44, loss = 0.26026315\n",
      "Iteration 10, loss = 0.48668503\n",
      "Iteration 45, loss = 0.25760163\n",
      "Iteration 11, loss = 0.46557646\n",
      "Iteration 46, loss = 0.25463943\n",
      "Iteration 12, loss = 0.44800033\n",
      "Iteration 47, loss = 0.25372171\n",
      "Iteration 13, loss = 0.43144691\n",
      "Iteration 48, loss = 0.24769534\n",
      "Iteration 14, loss = 0.41779569\n",
      "Iteration 49, loss = 0.24794959\n",
      "Iteration 15, loss = 0.40565104\n",
      "Iteration 50, loss = 0.24577487\n",
      "Iteration 51, loss = 0.24125504\n",
      "Iteration 16, loss = 0.39433598\n",
      "Iteration 52, loss = 0.23851073\n",
      "Iteration 17, loss = 0.38425444\n",
      "Iteration 53, loss = 0.23724494\n",
      "Iteration 18, loss = 0.37483957\n",
      "Iteration 19, loss = 0.36643292\n",
      "Iteration 54, loss = 0.23375916\n",
      "Iteration 20, loss = 0.35879410\n",
      "Iteration 55, loss = 0.23154585\n",
      "Iteration 21, loss = 0.35128950\n",
      "Iteration 56, loss = 0.22859560\n",
      "Iteration 22, loss = 0.34411128\n",
      "Iteration 57, loss = 0.22713829\n",
      "Iteration 23, loss = 0.33841840\n",
      "Iteration 58, loss = 0.22389003\n",
      "Iteration 59, loss = 0.22192743\n",
      "Iteration 24, loss = 0.33261487\n",
      "Iteration 25, loss = 0.32660928\n",
      "Iteration 60, loss = 0.21939765\n",
      "Iteration 26, loss = 0.32087484\n",
      "Iteration 61, loss = 0.21707104\n",
      "Iteration 27, loss = 0.31481698\n",
      "Iteration 62, loss = 0.21511933\n",
      "Iteration 28, loss = 0.30963623\n",
      "Iteration 63, loss = 0.21310959\n",
      "Iteration 29, loss = 0.30493430\n",
      "Iteration 64, loss = 0.21291819\n",
      "Iteration 30, loss = 0.30112379\n",
      "Iteration 65, loss = 0.21246812\n",
      "Iteration 31, loss = 0.29596041\n",
      "Iteration 66, loss = 0.21006159\n",
      "Iteration 32, loss = 0.29246112\n",
      "Iteration 67, loss = 0.20563248\n",
      "Iteration 68, loss = 0.20383229\n",
      "Iteration 33, loss = 0.28839951\n",
      "Iteration 69, loss = 0.20219204\n",
      "Iteration 34, loss = 0.28411486\n",
      "Iteration 70, loss = 0.19924451\n",
      "Iteration 35, loss = 0.27934912\n",
      "Iteration 71, loss = 0.19876188\n",
      "Iteration 36, loss = 0.27602000\n",
      "Iteration 37, loss = 0.27229945\n",
      "Iteration 72, loss = 0.19637736\n",
      "Iteration 73, loss = 0.19446736\n",
      "Iteration 38, loss = 0.26932038\n",
      "Iteration 74, loss = 0.19314589\n",
      "Iteration 39, loss = 0.26493480\n",
      "Iteration 75, loss = 0.18979992\n",
      "Iteration 40, loss = 0.26133018\n",
      "Iteration 41, loss = 0.25806119Iteration 76, loss = 0.19025388\n",
      "\n",
      "Iteration 77, loss = 0.18730585\n",
      "Iteration 42, loss = 0.25557069\n",
      "Iteration 78, loss = 0.18554686\n",
      "Iteration 43, loss = 0.25208854\n",
      "Iteration 44, loss = 0.24914986\n",
      "Iteration 79, loss = 0.18320562\n",
      "Iteration 45, loss = 0.24701398\n",
      "Iteration 80, loss = 0.18214817\n",
      "Iteration 46, loss = 0.24403701\n",
      "Iteration 81, loss = 0.18047183\n",
      "Iteration 47, loss = 0.24102872\n",
      "Iteration 82, loss = 0.17879038\n",
      "Iteration 48, loss = 0.23851562\n",
      "Iteration 83, loss = 0.17872077\n",
      "Iteration 49, loss = 0.23679589\n",
      "Iteration 84, loss = 0.17579846\n",
      "Iteration 50, loss = 0.23280784\n",
      "Iteration 85, loss = 0.17579271\n",
      "Iteration 51, loss = 0.23099303\n",
      "Iteration 86, loss = 0.17420788\n",
      "Iteration 52, loss = 0.22715469\n",
      "Iteration 87, loss = 0.17162798\n",
      "Iteration 53, loss = 0.22453430\n",
      "Iteration 88, loss = 0.16997629\n",
      "Iteration 54, loss = 0.22185088\n",
      "Iteration 89, loss = 0.17065705\n",
      "Iteration 55, loss = 0.21945011\n",
      "Iteration 90, loss = 0.16691159\n",
      "Iteration 91, loss = 0.16730253\n",
      "Iteration 56, loss = 0.21753041\n",
      "Iteration 57, loss = 0.21478413\n",
      "Iteration 92, loss = 0.16376996\n",
      "Iteration 58, loss = 0.21408506\n",
      "Iteration 93, loss = 0.16197256\n",
      "Iteration 59, loss = 0.21007696\n",
      "Iteration 94, loss = 0.16015757\n",
      "Iteration 60, loss = 0.20898225\n",
      "Iteration 95, loss = 0.15970298\n",
      "Iteration 96, loss = 0.15792433\n",
      "Iteration 61, loss = 0.20609125\n",
      "Iteration 97, loss = 0.15708941\n",
      "Iteration 62, loss = 0.20461609\n",
      "Iteration 63, loss = 0.20183165\n",
      "Iteration 98, loss = 0.15726112\n",
      "Iteration 64, loss = 0.20033831\n",
      "Iteration 99, loss = 0.15581255\n",
      "Iteration 65, loss = 0.19740681\n",
      "Iteration 100, loss = 0.15227700\n",
      "Iteration 66, loss = 0.19580689\n",
      "Iteration 67, loss = 0.19460549\n",
      "Iteration 68, loss = 0.19359389\n",
      "Iteration 69, loss = 0.19015346\n",
      "Iteration 70, loss = 0.18833609\n",
      "Iteration 71, loss = 0.18656468\n",
      "Iteration 72, loss = 0.18591351\n",
      "Iteration 73, loss = 0.18177835\n",
      "Iteration 74, loss = 0.18109594\n",
      "Iteration 75, loss = 0.17927183\n",
      "Iteration 76, loss = 0.17653884\n",
      "Iteration 77, loss = 0.17465459\n",
      "Iteration 78, loss = 0.17284537\n",
      "Iteration 79, loss = 0.17175449\n",
      "Iteration 80, loss = 0.17063995\n",
      "Iteration 81, loss = 0.16817117\n",
      "Iteration 82, loss = 0.16659450\n",
      "Iteration 83, loss = 0.16497001\n",
      "Iteration 1, loss = 1.21473224\n",
      "Iteration 2, loss = 1.03827321\n",
      "Iteration 84, loss = 0.16329815\n",
      "Iteration 85, loss = 0.16269792\n",
      "Iteration 3, loss = 0.91617033\n",
      "Iteration 86, loss = 0.15983435\n",
      "Iteration 4, loss = 0.81620894\n",
      "Iteration 87, loss = 0.15942643\n",
      "Iteration 5, loss = 0.73651763\n",
      "Iteration 88, loss = 0.15610660\n",
      "Iteration 6, loss = 0.67455820\n",
      "Iteration 89, loss = 0.15563030\n",
      "Iteration 7, loss = 0.62224448\n",
      "Iteration 90, loss = 0.15405995\n",
      "Iteration 8, loss = 0.58044709\n",
      "Iteration 9, loss = 0.54662972\n",
      "Iteration 91, loss = 0.15256834\n",
      "Iteration 10, loss = 0.51726722\n",
      "Iteration 92, loss = 0.15053473\n",
      "Iteration 11, loss = 0.49314522\n",
      "Iteration 93, loss = 0.15011146\n",
      "Iteration 12, loss = 0.47311037\n",
      "Iteration 94, loss = 0.14848627\n",
      "Iteration 13, loss = 0.45503105\n",
      "Iteration 95, loss = 0.14591356\n",
      "Iteration 14, loss = 0.44037656\n",
      "Iteration 96, loss = 0.14521731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.42636362\n",
      "Iteration 97, loss = 0.14363345\n",
      "Iteration 16, loss = 0.41380797\n",
      "Iteration 98, loss = 0.14234087\n",
      "Iteration 17, loss = 0.40283254\n",
      "Iteration 99, loss = 0.14209963\n",
      "Iteration 18, loss = 0.39332007\n",
      "Iteration 100, loss = 0.13913309\n",
      "Iteration 19, loss = 0.38385557\n",
      "Iteration 20, loss = 0.37527347\n",
      "Iteration 21, loss = 0.36758180\n",
      "Iteration 22, loss = 0.36030996\n",
      "Iteration 23, loss = 0.35359851\n",
      "Iteration 24, loss = 0.34672729\n",
      "Iteration 25, loss = 0.34154368\n",
      "Iteration 26, loss = 0.33443541\n",
      "Iteration 27, loss = 0.32975747\n",
      "Iteration 28, loss = 0.32536277\n",
      "Iteration 29, loss = 0.31915575\n",
      "Iteration 30, loss = 0.31534534\n",
      "Iteration 31, loss = 0.31077937\n",
      "Iteration 32, loss = 0.30621517\n",
      "Iteration 33, loss = 0.30123987\n",
      "Iteration 34, loss = 0.29722160\n",
      "Iteration 35, loss = 0.29348766\n",
      "Iteration 1, loss = 1.49145266\n",
      "Iteration 36, loss = 0.28920483\n",
      "Iteration 2, loss = 1.16997474\n",
      "Iteration 37, loss = 0.28508770\n",
      "Iteration 3, loss = 0.97720943\n",
      "Iteration 38, loss = 0.28222949\n",
      "Iteration 4, loss = 0.85025523\n",
      "Iteration 39, loss = 0.27831373\n",
      "Iteration 5, loss = 0.75265395\n",
      "Iteration 40, loss = 0.27536102\n",
      "Iteration 41, loss = 0.27208479\n",
      "Iteration 6, loss = 0.68123028\n",
      "Iteration 42, loss = 0.26901998\n",
      "Iteration 7, loss = 0.63085243\n",
      "Iteration 43, loss = 0.26510052\n",
      "Iteration 8, loss = 0.59191724\n",
      "Iteration 44, loss = 0.26232706\n",
      "Iteration 9, loss = 0.55806781\n",
      "Iteration 45, loss = 0.25959031\n",
      "Iteration 10, loss = 0.53171237\n",
      "Iteration 46, loss = 0.25615775\n",
      "Iteration 11, loss = 0.50907760\n",
      "Iteration 47, loss = 0.25447142\n",
      "Iteration 12, loss = 0.49014143\n",
      "Iteration 48, loss = 0.25041617\n",
      "Iteration 13, loss = 0.47322624\n",
      "Iteration 49, loss = 0.24811498\n",
      "Iteration 14, loss = 0.45802096\n",
      "Iteration 50, loss = 0.24605608\n",
      "Iteration 15, loss = 0.44410841\n",
      "Iteration 51, loss = 0.24218627\n",
      "Iteration 16, loss = 0.43297661\n",
      "Iteration 52, loss = 0.23992082\n",
      "Iteration 17, loss = 0.42137384\n",
      "Iteration 53, loss = 0.23728912\n",
      "Iteration 18, loss = 0.41146312\n",
      "Iteration 54, loss = 0.23456878\n",
      "Iteration 19, loss = 0.40180869\n",
      "Iteration 55, loss = 0.23245264\n",
      "Iteration 20, loss = 0.39327109\n",
      "Iteration 56, loss = 0.22981543\n",
      "Iteration 21, loss = 0.38545263\n",
      "Iteration 57, loss = 0.22693875\n",
      "Iteration 22, loss = 0.37808539\n",
      "Iteration 58, loss = 0.22454919\n",
      "Iteration 23, loss = 0.37092295\n",
      "Iteration 59, loss = 0.22207714\n",
      "Iteration 24, loss = 0.36421480\n",
      "Iteration 25, loss = 0.35806506\n",
      "Iteration 60, loss = 0.22024762\n",
      "Iteration 26, loss = 0.35160989\n",
      "Iteration 61, loss = 0.21763731\n",
      "Iteration 62, loss = 0.21600940\n",
      "Iteration 27, loss = 0.34678445\n",
      "Iteration 63, loss = 0.21324847\n",
      "Iteration 28, loss = 0.34116903\n",
      "Iteration 64, loss = 0.21105742\n",
      "Iteration 29, loss = 0.33570301\n",
      "Iteration 65, loss = 0.20953613\n",
      "Iteration 30, loss = 0.33203337\n",
      "Iteration 66, loss = 0.20649329\n",
      "Iteration 31, loss = 0.32680697\n",
      "Iteration 67, loss = 0.20476965\n",
      "Iteration 32, loss = 0.32298853\n",
      "Iteration 68, loss = 0.20271904\n",
      "Iteration 33, loss = 0.31857945\n",
      "Iteration 69, loss = 0.20125388\n",
      "Iteration 34, loss = 0.31391264\n",
      "Iteration 70, loss = 0.19844342\n",
      "Iteration 35, loss = 0.30978608\n",
      "Iteration 71, loss = 0.19677038\n",
      "Iteration 36, loss = 0.30587380\n",
      "Iteration 72, loss = 0.19462910\n",
      "Iteration 37, loss = 0.30306884\n",
      "Iteration 73, loss = 0.19265867\n",
      "Iteration 38, loss = 0.29850109\n",
      "Iteration 74, loss = 0.19059797\n",
      "Iteration 39, loss = 0.29530788\n",
      "Iteration 75, loss = 0.19118536\n",
      "Iteration 40, loss = 0.29214856\n",
      "Iteration 76, loss = 0.18636621\n",
      "Iteration 41, loss = 0.28852396\n",
      "Iteration 77, loss = 0.18517728\n",
      "Iteration 42, loss = 0.28535960\n",
      "Iteration 78, loss = 0.18378380\n",
      "Iteration 43, loss = 0.28256985\n",
      "Iteration 79, loss = 0.18105581\n",
      "Iteration 44, loss = 0.27914631\n",
      "Iteration 80, loss = 0.18122198\n",
      "Iteration 45, loss = 0.27634395\n",
      "Iteration 81, loss = 0.17712770\n",
      "Iteration 46, loss = 0.27361761\n",
      "Iteration 82, loss = 0.17637423\n",
      "Iteration 47, loss = 0.27025130\n",
      "Iteration 83, loss = 0.17476959\n",
      "Iteration 48, loss = 0.26855989\n",
      "Iteration 84, loss = 0.17286487\n",
      "Iteration 49, loss = 0.26523669\n",
      "Iteration 50, loss = 0.26247308\n",
      "Iteration 85, loss = 0.17041946\n",
      "Iteration 51, loss = 0.25949847\n",
      "Iteration 86, loss = 0.16902159\n",
      "Iteration 52, loss = 0.25696887\n",
      "Iteration 87, loss = 0.16693787\n",
      "Iteration 53, loss = 0.25482782\n",
      "Iteration 88, loss = 0.16559427\n",
      "Iteration 54, loss = 0.25205882\n",
      "Iteration 89, loss = 0.16389008\n",
      "Iteration 55, loss = 0.24981540\n",
      "Iteration 90, loss = 0.16294733\n",
      "Iteration 56, loss = 0.24847370\n",
      "Iteration 91, loss = 0.16099081\n",
      "Iteration 57, loss = 0.24578014\n",
      "Iteration 92, loss = 0.16043694\n",
      "Iteration 58, loss = 0.24312840\n",
      "Iteration 93, loss = 0.15790608\n",
      "Iteration 59, loss = 0.24063712\n",
      "Iteration 94, loss = 0.15468271\n",
      "Iteration 60, loss = 0.23824252\n",
      "Iteration 95, loss = 0.15458134\n",
      "Iteration 61, loss = 0.23619823\n",
      "Iteration 96, loss = 0.15163176\n",
      "Iteration 62, loss = 0.23503055\n",
      "Iteration 97, loss = 0.15266032\n",
      "Iteration 63, loss = 0.23362599\n",
      "Iteration 98, loss = 0.15018913\n",
      "Iteration 64, loss = 0.23215519\n",
      "Iteration 99, loss = 0.14880464\n",
      "Iteration 65, loss = 0.22912681\n",
      "Iteration 100, loss = 0.14775860\n",
      "Iteration 66, loss = 0.22557015\n",
      "Iteration 67, loss = 0.22402639\n",
      "Iteration 68, loss = 0.22295172\n",
      "Iteration 69, loss = 0.22031402\n",
      "Iteration 70, loss = 0.21934127\n",
      "Iteration 71, loss = 0.21752638\n",
      "Iteration 72, loss = 0.21548678\n",
      "Iteration 73, loss = 0.21258657\n",
      "Iteration 74, loss = 0.21122422\n",
      "Iteration 75, loss = 0.20958462\n",
      "Iteration 76, loss = 0.20823820\n",
      "Iteration 77, loss = 0.20664300\n",
      "Iteration 78, loss = 0.20429748\n",
      "Iteration 79, loss = 0.20226569\n",
      "Iteration 80, loss = 0.20022447\n",
      "Iteration 81, loss = 0.19893804\n",
      "Iteration 82, loss = 0.19689012\n",
      "Iteration 83, loss = 0.19640614\n",
      "Iteration 1, loss = 0.99092591\n",
      "Iteration 84, loss = 0.19441703\n",
      "Iteration 85, loss = 0.19414168\n",
      "Iteration 2, loss = 0.84713348\n",
      "Iteration 86, loss = 0.19076995\n",
      "Iteration 87, loss = 0.19130500\n",
      "Iteration 88, loss = 0.18987711\n",
      "Iteration 89, loss = 0.18737831\n",
      "Iteration 3, loss = 0.76858835\n",
      "Iteration 90, loss = 0.18429180\n",
      "Iteration 4, loss = 0.70215233\n",
      "Iteration 91, loss = 0.18550118\n",
      "Iteration 5, loss = 0.64427119\n",
      "Iteration 92, loss = 0.18160407\n",
      "Iteration 6, loss = 0.60117509\n",
      "Iteration 93, loss = 0.18098980\n",
      "Iteration 94, loss = 0.17905370\n",
      "Iteration 95, loss = 0.17780126\n",
      "Iteration 7, loss = 0.56583819\n",
      "Iteration 96, loss = 0.17589115\n",
      "Iteration 8, loss = 0.53610732\n",
      "Iteration 97, loss = 0.17475634\n",
      "Iteration 9, loss = 0.51105698\n",
      "Iteration 98, loss = 0.17336657\n",
      "Iteration 10, loss = 0.49033814\n",
      "Iteration 99, loss = 0.17223959\n",
      "Iteration 11, loss = 0.47203403\n",
      "Iteration 100, loss = 0.17146936\n",
      "Iteration 12, loss = 0.45602627\n",
      "Iteration 13, loss = 0.44124425\n",
      "Iteration 14, loss = 0.42881931\n",
      "Iteration 15, loss = 0.41748247\n",
      "Iteration 16, loss = 0.40623000\n",
      "Iteration 17, loss = 0.39677993\n",
      "Iteration 18, loss = 0.38777702\n",
      "Iteration 19, loss = 0.38021831\n",
      "Iteration 20, loss = 0.37199439\n",
      "Iteration 21, loss = 0.36532192\n",
      "Iteration 22, loss = 0.35836908\n",
      "Iteration 23, loss = 0.35256264\n",
      "Iteration 24, loss = 0.34632155\n",
      "Iteration 25, loss = 0.34107687\n",
      "Iteration 26, loss = 0.33590669\n",
      "Iteration 27, loss = 0.33062984\n",
      "Iteration 28, loss = 0.32661838\n",
      "Iteration 29, loss = 0.32086063\n",
      "Iteration 30, loss = 0.31689215\n",
      "Iteration 31, loss = 0.31292272\n",
      "Iteration 32, loss = 0.30838475\n",
      "Iteration 33, loss = 0.30482652\n",
      "Iteration 34, loss = 0.30070823\n",
      "Iteration 1, loss = 1.29062896\n",
      "Iteration 35, loss = 0.29753581\n",
      "Iteration 2, loss = 1.01968054\n",
      "Iteration 36, loss = 0.29437050\n",
      "Iteration 3, loss = 0.88334131\n",
      "Iteration 37, loss = 0.29040784\n",
      "Iteration 4, loss = 0.79080137\n",
      "Iteration 38, loss = 0.28833723\n",
      "Iteration 5, loss = 0.71470959\n",
      "Iteration 39, loss = 0.28394004\n",
      "Iteration 6, loss = 0.65601397\n",
      "Iteration 40, loss = 0.28126675\n",
      "Iteration 7, loss = 0.61250259\n",
      "Iteration 41, loss = 0.27797730\n",
      "Iteration 8, loss = 0.57644712\n",
      "Iteration 42, loss = 0.27475678\n",
      "Iteration 9, loss = 0.54672049\n",
      "Iteration 43, loss = 0.27200040\n",
      "Iteration 10, loss = 0.52196126\n",
      "Iteration 44, loss = 0.27060825\n",
      "Iteration 11, loss = 0.50063634\n",
      "Iteration 45, loss = 0.26631818\n",
      "Iteration 12, loss = 0.48278746\n",
      "Iteration 46, loss = 0.26368667\n",
      "Iteration 13, loss = 0.46470416\n",
      "Iteration 47, loss = 0.26066300\n",
      "Iteration 14, loss = 0.45105730\n",
      "Iteration 48, loss = 0.25845567\n",
      "Iteration 15, loss = 0.43780921\n",
      "Iteration 49, loss = 0.25541811\n",
      "Iteration 16, loss = 0.42545615\n",
      "Iteration 50, loss = 0.25346955\n",
      "Iteration 17, loss = 0.41570727\n",
      "Iteration 18, loss = 0.40551153\n",
      "Iteration 51, loss = 0.25058333\n",
      "Iteration 19, loss = 0.39561960\n",
      "Iteration 52, loss = 0.24869526\n",
      "Iteration 20, loss = 0.38803314\n",
      "Iteration 53, loss = 0.24727025\n",
      "Iteration 21, loss = 0.37946173\n",
      "Iteration 54, loss = 0.24400809\n",
      "Iteration 22, loss = 0.37182476\n",
      "Iteration 55, loss = 0.24251484\n",
      "Iteration 23, loss = 0.36483367\n",
      "Iteration 56, loss = 0.23963729\n",
      "Iteration 24, loss = 0.35853528\n",
      "Iteration 57, loss = 0.23867291\n",
      "Iteration 25, loss = 0.35209300\n",
      "Iteration 58, loss = 0.23526670\n",
      "Iteration 26, loss = 0.34677118\n",
      "Iteration 59, loss = 0.23261983\n",
      "Iteration 27, loss = 0.34078800\n",
      "Iteration 60, loss = 0.23099493\n",
      "Iteration 28, loss = 0.33500643\n",
      "Iteration 61, loss = 0.22822737\n",
      "Iteration 29, loss = 0.33060897\n",
      "Iteration 62, loss = 0.22625805\n",
      "Iteration 30, loss = 0.32629053\n",
      "Iteration 63, loss = 0.22442060\n",
      "Iteration 31, loss = 0.32124182\n",
      "Iteration 32, loss = 0.31594077\n",
      "Iteration 64, loss = 0.22224773\n",
      "Iteration 33, loss = 0.31179744\n",
      "Iteration 65, loss = 0.21967295\n",
      "Iteration 34, loss = 0.30763700\n",
      "Iteration 66, loss = 0.21788320\n",
      "Iteration 35, loss = 0.30347374\n",
      "Iteration 67, loss = 0.21665941\n",
      "Iteration 36, loss = 0.29926139\n",
      "Iteration 68, loss = 0.21493969\n",
      "Iteration 37, loss = 0.29571329\n",
      "Iteration 69, loss = 0.21217980\n",
      "Iteration 38, loss = 0.29197760\n",
      "Iteration 70, loss = 0.21018054\n",
      "Iteration 39, loss = 0.28830061\n",
      "Iteration 71, loss = 0.20860502\n",
      "Iteration 40, loss = 0.28503473\n",
      "Iteration 72, loss = 0.20726697\n",
      "Iteration 41, loss = 0.28124022\n",
      "Iteration 73, loss = 0.20487166\n",
      "Iteration 42, loss = 0.27879755\n",
      "Iteration 74, loss = 0.20336345\n",
      "Iteration 43, loss = 0.27471245\n",
      "Iteration 75, loss = 0.20158079\n",
      "Iteration 44, loss = 0.27146701\n",
      "Iteration 76, loss = 0.20045413\n",
      "Iteration 45, loss = 0.26867496\n",
      "Iteration 77, loss = 0.19797123\n",
      "Iteration 46, loss = 0.26589246\n",
      "Iteration 47, loss = 0.26365331\n",
      "Iteration 78, loss = 0.19672039\n",
      "Iteration 48, loss = 0.26013100\n",
      "Iteration 79, loss = 0.19478752\n",
      "Iteration 49, loss = 0.25754115\n",
      "Iteration 80, loss = 0.19243439\n",
      "Iteration 50, loss = 0.25433127\n",
      "Iteration 81, loss = 0.19176269\n",
      "Iteration 51, loss = 0.25210047\n",
      "Iteration 82, loss = 0.18879940\n",
      "Iteration 52, loss = 0.24910419\n",
      "Iteration 83, loss = 0.18747804\n",
      "Iteration 53, loss = 0.24553422\n",
      "Iteration 84, loss = 0.18648262\n",
      "Iteration 54, loss = 0.24365068\n",
      "Iteration 85, loss = 0.18416870\n",
      "Iteration 55, loss = 0.24030905\n",
      "Iteration 86, loss = 0.18291398\n",
      "Iteration 56, loss = 0.23827399\n",
      "Iteration 87, loss = 0.18074571\n",
      "Iteration 57, loss = 0.23558528\n",
      "Iteration 88, loss = 0.17863590\n",
      "Iteration 58, loss = 0.23413812\n",
      "Iteration 89, loss = 0.17893858\n",
      "Iteration 59, loss = 0.23174973\n",
      "Iteration 90, loss = 0.17696982\n",
      "Iteration 60, loss = 0.22828857\n",
      "Iteration 91, loss = 0.17426530\n",
      "Iteration 61, loss = 0.22731741\n",
      "Iteration 92, loss = 0.17317015\n",
      "Iteration 62, loss = 0.22348717\n",
      "Iteration 93, loss = 0.17085202\n",
      "Iteration 63, loss = 0.22258587\n",
      "Iteration 94, loss = 0.17062672\n",
      "Iteration 64, loss = 0.22067844\n",
      "Iteration 95, loss = 0.16900310\n",
      "Iteration 65, loss = 0.21833750\n",
      "Iteration 96, loss = 0.16689529\n",
      "Iteration 66, loss = 0.21518227\n",
      "Iteration 97, loss = 0.16635553\n",
      "Iteration 67, loss = 0.21291769\n",
      "Iteration 68, loss = 0.21059592\n",
      "Iteration 98, loss = 0.16260387\n",
      "Iteration 69, loss = 0.20842134\n",
      "Iteration 99, loss = 0.16265437\n",
      "Iteration 70, loss = 0.20754664\n",
      "Iteration 100, loss = 0.16143208\n",
      "Iteration 71, loss = 0.20397943\n",
      "Iteration 72, loss = 0.20211168\n",
      "Iteration 73, loss = 0.20063977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74, loss = 0.19908046\n",
      "Iteration 75, loss = 0.19845073\n",
      "Iteration 76, loss = 0.19494714\n",
      "Iteration 77, loss = 0.19279061\n",
      "Iteration 78, loss = 0.19153711\n",
      "Iteration 79, loss = 0.18904491\n",
      "Iteration 80, loss = 0.18701635\n",
      "Iteration 81, loss = 0.18638975\n",
      "Iteration 82, loss = 0.18303964\n",
      "Iteration 83, loss = 0.18214206\n",
      "Iteration 84, loss = 0.17986186\n",
      "Iteration 85, loss = 0.17754472\n",
      "Iteration 86, loss = 0.17647417\n",
      "Iteration 87, loss = 0.17355456\n",
      "Iteration 88, loss = 0.17343410\n",
      "Iteration 89, loss = 0.17223809\n",
      "Iteration 1, loss = 1.24776756\n",
      "Iteration 90, loss = 0.16921857\n",
      "Iteration 2, loss = 1.06746276\n",
      "Iteration 91, loss = 0.16752584\n",
      "Iteration 3, loss = 0.95397041\n",
      "Iteration 92, loss = 0.16608519\n",
      "Iteration 4, loss = 0.85853737\n",
      "Iteration 93, loss = 0.16366493\n",
      "Iteration 5, loss = 0.78120222\n",
      "Iteration 94, loss = 0.16320766\n",
      "Iteration 6, loss = 0.71905115\n",
      "Iteration 95, loss = 0.16036602\n",
      "Iteration 7, loss = 0.66672577\n",
      "Iteration 96, loss = 0.16030261\n",
      "Iteration 8, loss = 0.62385537\n",
      "Iteration 97, loss = 0.15786972\n",
      "Iteration 9, loss = 0.58718332\n",
      "Iteration 98, loss = 0.15716237\n",
      "Iteration 10, loss = 0.55711065\n",
      "Iteration 99, loss = 0.15545184\n",
      "Iteration 11, loss = 0.53210027\n",
      "Iteration 100, loss = 0.15383459\n",
      "Iteration 12, loss = 0.50968537\n",
      "Iteration 13, loss = 0.49023639\n",
      "Iteration 14, loss = 0.47334211\n",
      "Iteration 15, loss = 0.45842702\n",
      "Iteration 16, loss = 0.44460644\n",
      "Iteration 17, loss = 0.43194782\n",
      "Iteration 18, loss = 0.42126841\n",
      "Iteration 19, loss = 0.41080763\n",
      "Iteration 20, loss = 0.40113138\n",
      "Iteration 21, loss = 0.39192964\n",
      "Iteration 22, loss = 0.38532366\n",
      "Iteration 23, loss = 0.37577538\n",
      "Iteration 24, loss = 0.37092256\n",
      "Iteration 25, loss = 0.36288315\n",
      "Iteration 26, loss = 0.35730467\n",
      "Iteration 27, loss = 0.35190951\n",
      "Iteration 28, loss = 0.34504868\n",
      "Iteration 1, loss = 1.26102631\n",
      "Iteration 29, loss = 0.33909922\n",
      "Iteration 2, loss = 1.12895636\n",
      "Iteration 30, loss = 0.33458952\n",
      "Iteration 3, loss = 1.02898847\n",
      "Iteration 31, loss = 0.32937280\n",
      "Iteration 4, loss = 0.94353715\n",
      "Iteration 32, loss = 0.32447195\n",
      "Iteration 5, loss = 0.86737044\n",
      "Iteration 33, loss = 0.32004312\n",
      "Iteration 6, loss = 0.79691714\n",
      "Iteration 34, loss = 0.31625711\n",
      "Iteration 7, loss = 0.73504465\n",
      "Iteration 35, loss = 0.31127832\n",
      "Iteration 8, loss = 0.67758447\n",
      "Iteration 36, loss = 0.30865179\n",
      "Iteration 9, loss = 0.62846662\n",
      "Iteration 37, loss = 0.30326327\n",
      "Iteration 10, loss = 0.58722490\n",
      "Iteration 38, loss = 0.30056985\n",
      "Iteration 11, loss = 0.55121279\n",
      "Iteration 39, loss = 0.29513918\n",
      "Iteration 12, loss = 0.52216053\n",
      "Iteration 40, loss = 0.29290585\n",
      "Iteration 13, loss = 0.49679762\n",
      "Iteration 41, loss = 0.28888416\n",
      "Iteration 14, loss = 0.47513059\n",
      "Iteration 42, loss = 0.28549493\n",
      "Iteration 15, loss = 0.45479374\n",
      "Iteration 43, loss = 0.28318873\n",
      "Iteration 16, loss = 0.43874583\n",
      "Iteration 44, loss = 0.27962794\n",
      "Iteration 17, loss = 0.42315008\n",
      "Iteration 45, loss = 0.27601981\n",
      "Iteration 18, loss = 0.41033612\n",
      "Iteration 46, loss = 0.27272742\n",
      "Iteration 19, loss = 0.39814035\n",
      "Iteration 47, loss = 0.26927374\n",
      "Iteration 20, loss = 0.38692068\n",
      "Iteration 48, loss = 0.26719359\n",
      "Iteration 21, loss = 0.37664336\n",
      "Iteration 49, loss = 0.26456799\n",
      "Iteration 50, loss = 0.26154444\n",
      "Iteration 22, loss = 0.36728280\n",
      "Iteration 51, loss = 0.25965311\n",
      "Iteration 23, loss = 0.35889633\n",
      "Iteration 24, loss = 0.35165031\n",
      "Iteration 52, loss = 0.25531530\n",
      "Iteration 25, loss = 0.34376165\n",
      "Iteration 53, loss = 0.25453533\n",
      "Iteration 26, loss = 0.33639763\n",
      "Iteration 54, loss = 0.25156388\n",
      "Iteration 27, loss = 0.32942469\n",
      "Iteration 55, loss = 0.24795083\n",
      "Iteration 28, loss = 0.32418596\n",
      "Iteration 56, loss = 0.24659193\n",
      "Iteration 29, loss = 0.31750432\n",
      "Iteration 57, loss = 0.24385613\n",
      "Iteration 58, loss = 0.24073796\n",
      "Iteration 30, loss = 0.31153356\n",
      "Iteration 59, loss = 0.23927110\n",
      "Iteration 31, loss = 0.30573660\n",
      "Iteration 32, loss = 0.30090003\n",
      "Iteration 60, loss = 0.23668297\n",
      "Iteration 33, loss = 0.29686159\n",
      "Iteration 61, loss = 0.23395778\n",
      "Iteration 34, loss = 0.29222877\n",
      "Iteration 62, loss = 0.23174920\n",
      "Iteration 63, loss = 0.23043264\n",
      "Iteration 35, loss = 0.28676196\n",
      "Iteration 64, loss = 0.22731158\n",
      "Iteration 36, loss = 0.28261982\n",
      "Iteration 65, loss = 0.22546719\n",
      "Iteration 37, loss = 0.27862518\n",
      "Iteration 66, loss = 0.22435970\n",
      "Iteration 38, loss = 0.27435297\n",
      "Iteration 67, loss = 0.22084770\n",
      "Iteration 39, loss = 0.26979980\n",
      "Iteration 68, loss = 0.21967569\n",
      "Iteration 40, loss = 0.26682070\n",
      "Iteration 69, loss = 0.21834087\n",
      "Iteration 41, loss = 0.26268887\n",
      "Iteration 70, loss = 0.21525784\n",
      "Iteration 42, loss = 0.25860220\n",
      "Iteration 71, loss = 0.21368207\n",
      "Iteration 43, loss = 0.25513907\n",
      "Iteration 72, loss = 0.21212060\n",
      "Iteration 44, loss = 0.25181706\n",
      "Iteration 73, loss = 0.20999934\n",
      "Iteration 45, loss = 0.24876401\n",
      "Iteration 74, loss = 0.20708275\n",
      "Iteration 46, loss = 0.24862138\n",
      "Iteration 75, loss = 0.20545208\n",
      "Iteration 47, loss = 0.24585551\n",
      "Iteration 76, loss = 0.20458416\n",
      "Iteration 48, loss = 0.23897166\n",
      "Iteration 77, loss = 0.20221226\n",
      "Iteration 49, loss = 0.24005732\n",
      "Iteration 78, loss = 0.20156153\n",
      "Iteration 50, loss = 0.23580001\n",
      "Iteration 79, loss = 0.19885627\n",
      "Iteration 51, loss = 0.23236453\n",
      "Iteration 80, loss = 0.19720493\n",
      "Iteration 52, loss = 0.22764712\n",
      "Iteration 81, loss = 0.19447272\n",
      "Iteration 53, loss = 0.22500438\n",
      "Iteration 82, loss = 0.19280152\n",
      "Iteration 54, loss = 0.22382486\n",
      "Iteration 83, loss = 0.19194034\n",
      "Iteration 55, loss = 0.21991904\n",
      "Iteration 56, loss = 0.21801621Iteration 84, loss = 0.18950804\n",
      "\n",
      "Iteration 85, loss = 0.18811887\n",
      "Iteration 57, loss = 0.21458223\n",
      "Iteration 86, loss = 0.18657741\n",
      "Iteration 58, loss = 0.21412787\n",
      "Iteration 87, loss = 0.18540547\n",
      "Iteration 59, loss = 0.21058296\n",
      "Iteration 88, loss = 0.18328837\n",
      "Iteration 60, loss = 0.20811338\n",
      "Iteration 89, loss = 0.18174646\n",
      "Iteration 61, loss = 0.20510364\n",
      "Iteration 90, loss = 0.17969526\n",
      "Iteration 62, loss = 0.20421629\n",
      "Iteration 91, loss = 0.17819885\n",
      "Iteration 63, loss = 0.20086589\n",
      "Iteration 92, loss = 0.17935144\n",
      "Iteration 64, loss = 0.20016324\n",
      "Iteration 93, loss = 0.17719028\n",
      "Iteration 65, loss = 0.19566798\n",
      "Iteration 94, loss = 0.17496356\n",
      "Iteration 66, loss = 0.19599745\n",
      "Iteration 95, loss = 0.17375592\n",
      "Iteration 67, loss = 0.19331975\n",
      "Iteration 96, loss = 0.17047354\n",
      "Iteration 68, loss = 0.19135138\n",
      "Iteration 97, loss = 0.16841620\n",
      "Iteration 69, loss = 0.18798808\n",
      "Iteration 98, loss = 0.16850078\n",
      "Iteration 70, loss = 0.18496511\n",
      "Iteration 99, loss = 0.16622864\n",
      "Iteration 71, loss = 0.18329625\n",
      "Iteration 100, loss = 0.16658338\n",
      "Iteration 72, loss = 0.18230373\n",
      "Iteration 73, loss = 0.18175963\n",
      "Iteration 74, loss = 0.17627445\n",
      "Iteration 75, loss = 0.17557890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76, loss = 0.17209424\n",
      "Iteration 77, loss = 0.17165030\n",
      "Iteration 78, loss = 0.16770746\n",
      "Iteration 79, loss = 0.16723104\n",
      "Iteration 80, loss = 0.16490737\n",
      "Iteration 81, loss = 0.16296820\n",
      "Iteration 82, loss = 0.16107959\n",
      "Iteration 83, loss = 0.15897549\n",
      "Iteration 84, loss = 0.15729441\n",
      "Iteration 85, loss = 0.15494218\n",
      "Iteration 86, loss = 0.15460782\n",
      "Iteration 87, loss = 0.15081572\n",
      "Iteration 88, loss = 0.15010786\n",
      "Iteration 89, loss = 0.14863189\n",
      "Iteration 1, loss = 1.39321611\n",
      "Iteration 90, loss = 0.14622486\n",
      "Iteration 2, loss = 1.14903103\n",
      "Iteration 91, loss = 0.14550845\n",
      "Iteration 3, loss = 0.95433110\n",
      "Iteration 92, loss = 0.14447649\n",
      "Iteration 4, loss = 0.82452349\n",
      "Iteration 93, loss = 0.14324367\n",
      "Iteration 5, loss = 0.73908594\n",
      "Iteration 94, loss = 0.14275163\n",
      "Iteration 6, loss = 0.68042775\n",
      "Iteration 95, loss = 0.13980252\n",
      "Iteration 7, loss = 0.63037656\n",
      "Iteration 96, loss = 0.13768335\n",
      "Iteration 8, loss = 0.58589604\n",
      "Iteration 97, loss = 0.13872860\n",
      "Iteration 9, loss = 0.54843122\n",
      "Iteration 98, loss = 0.13561811\n",
      "Iteration 10, loss = 0.51695036\n",
      "Iteration 99, loss = 0.13376419\n",
      "Iteration 11, loss = 0.48955042\n",
      "Iteration 100, loss = 0.13195967\n",
      "Iteration 12, loss = 0.46646189\n",
      "Iteration 13, loss = 0.44719648\n",
      "Iteration 14, loss = 0.42934768\n",
      "Iteration 15, loss = 0.41351729\n",
      "Iteration 16, loss = 0.39958038\n",
      "Iteration 17, loss = 0.38715432\n",
      "Iteration 18, loss = 0.37558235\n",
      "Iteration 19, loss = 0.36550201\n",
      "Iteration 20, loss = 0.35545009\n",
      "Iteration 21, loss = 0.34769866\n",
      "Iteration 22, loss = 0.33918176\n",
      "Iteration 23, loss = 0.33149030\n",
      "Iteration 24, loss = 0.32418642\n",
      "Iteration 25, loss = 0.31814324\n",
      "Iteration 26, loss = 0.31194318\n",
      "Iteration 27, loss = 0.30562595\n",
      "Iteration 28, loss = 0.30010552\n",
      "Iteration 1, loss = 1.28164228\n",
      "Iteration 29, loss = 0.29536312\n",
      "Iteration 2, loss = 1.10515748\n",
      "Iteration 30, loss = 0.29054368\n",
      "Iteration 3, loss = 0.97724035\n",
      "Iteration 31, loss = 0.28495962\n",
      "Iteration 4, loss = 0.87985694\n",
      "Iteration 32, loss = 0.28106535\n",
      "Iteration 5, loss = 0.79944222\n",
      "Iteration 33, loss = 0.27628173\n",
      "Iteration 6, loss = 0.73129373\n",
      "Iteration 34, loss = 0.27174177\n",
      "Iteration 7, loss = 0.67344868\n",
      "Iteration 35, loss = 0.26759153\n",
      "Iteration 8, loss = 0.62532202\n",
      "Iteration 36, loss = 0.26356213\n",
      "Iteration 9, loss = 0.58405915\n",
      "Iteration 37, loss = 0.25999492\n",
      "Iteration 10, loss = 0.54916231\n",
      "Iteration 38, loss = 0.25660696\n",
      "Iteration 11, loss = 0.52055524\n",
      "Iteration 39, loss = 0.25344394\n",
      "Iteration 12, loss = 0.49539063\n",
      "Iteration 40, loss = 0.25048420\n",
      "Iteration 13, loss = 0.47297239\n",
      "Iteration 41, loss = 0.24594260\n",
      "Iteration 14, loss = 0.45458556\n",
      "Iteration 42, loss = 0.24427862\n",
      "Iteration 15, loss = 0.43847883\n",
      "Iteration 43, loss = 0.24023687\n",
      "Iteration 16, loss = 0.42318276\n",
      "Iteration 44, loss = 0.23686566\n",
      "Iteration 17, loss = 0.41025762\n",
      "Iteration 45, loss = 0.23418549\n",
      "Iteration 18, loss = 0.39951923\n",
      "Iteration 46, loss = 0.23066181\n",
      "Iteration 19, loss = 0.38813306\n",
      "Iteration 47, loss = 0.22811464\n",
      "Iteration 20, loss = 0.37877628\n",
      "Iteration 48, loss = 0.22529314\n",
      "Iteration 21, loss = 0.37080151\n",
      "Iteration 49, loss = 0.22239086\n",
      "Iteration 22, loss = 0.36278523\n",
      "Iteration 50, loss = 0.22033741\n",
      "Iteration 23, loss = 0.35549048\n",
      "Iteration 51, loss = 0.21754582\n",
      "Iteration 24, loss = 0.34900585\n",
      "Iteration 52, loss = 0.21581570\n",
      "Iteration 53, loss = 0.21323477\n",
      "Iteration 25, loss = 0.34203313\n",
      "Iteration 54, loss = 0.21038034\n",
      "Iteration 26, loss = 0.33651654\n",
      "Iteration 55, loss = 0.20837030\n",
      "Iteration 27, loss = 0.33035984\n",
      "Iteration 56, loss = 0.20590920\n",
      "Iteration 28, loss = 0.32550737\n",
      "Iteration 57, loss = 0.20318281\n",
      "Iteration 29, loss = 0.32056992\n",
      "Iteration 58, loss = 0.20154249\n",
      "Iteration 30, loss = 0.31526891\n",
      "Iteration 59, loss = 0.19904658\n",
      "Iteration 31, loss = 0.31052978\n",
      "Iteration 60, loss = 0.19787837\n",
      "Iteration 32, loss = 0.30627671\n",
      "Iteration 61, loss = 0.19439141\n",
      "Iteration 33, loss = 0.30245261\n",
      "Iteration 62, loss = 0.19331655\n",
      "Iteration 34, loss = 0.29821370\n",
      "Iteration 63, loss = 0.19067780\n",
      "Iteration 35, loss = 0.29376604\n",
      "Iteration 64, loss = 0.18926888\n",
      "Iteration 36, loss = 0.29015814\n",
      "Iteration 65, loss = 0.18671595\n",
      "Iteration 37, loss = 0.28646908\n",
      "Iteration 66, loss = 0.18470199\n",
      "Iteration 38, loss = 0.28306440\n",
      "Iteration 67, loss = 0.18363894\n",
      "Iteration 39, loss = 0.27976979\n",
      "Iteration 68, loss = 0.18108382\n",
      "Iteration 40, loss = 0.27609654\n",
      "Iteration 69, loss = 0.18052396\n",
      "Iteration 41, loss = 0.27229197\n",
      "Iteration 70, loss = 0.17736394\n",
      "Iteration 42, loss = 0.26928838\n",
      "Iteration 71, loss = 0.17621392\n",
      "Iteration 43, loss = 0.26623692\n",
      "Iteration 72, loss = 0.17545078\n",
      "Iteration 44, loss = 0.26308561\n",
      "Iteration 73, loss = 0.17299841\n",
      "Iteration 45, loss = 0.26117739\n",
      "Iteration 74, loss = 0.17196639\n",
      "Iteration 46, loss = 0.25735716\n",
      "Iteration 75, loss = 0.16965291\n",
      "Iteration 47, loss = 0.25458913\n",
      "Iteration 76, loss = 0.16835168\n",
      "Iteration 48, loss = 0.25294947\n",
      "Iteration 77, loss = 0.16680302\n",
      "Iteration 49, loss = 0.24878397\n",
      "Iteration 78, loss = 0.16400273\n",
      "Iteration 50, loss = 0.24735853\n",
      "Iteration 79, loss = 0.16293533\n",
      "Iteration 51, loss = 0.24409882\n",
      "Iteration 80, loss = 0.16111158\n",
      "Iteration 52, loss = 0.24177396\n",
      "Iteration 81, loss = 0.16022399\n",
      "Iteration 53, loss = 0.24010511\n",
      "Iteration 82, loss = 0.15914622\n",
      "Iteration 54, loss = 0.23678841\n",
      "Iteration 83, loss = 0.15703239\n",
      "Iteration 55, loss = 0.23387737\n",
      "Iteration 84, loss = 0.15605387\n",
      "Iteration 56, loss = 0.23175938\n",
      "Iteration 85, loss = 0.15333855\n",
      "Iteration 57, loss = 0.22897716\n",
      "Iteration 86, loss = 0.15658219\n",
      "Iteration 58, loss = 0.22734511\n",
      "Iteration 87, loss = 0.15208370\n",
      "Iteration 59, loss = 0.22592828\n",
      "Iteration 88, loss = 0.15118855\n",
      "Iteration 60, loss = 0.22261112\n",
      "Iteration 89, loss = 0.15106256\n",
      "Iteration 61, loss = 0.22113328\n",
      "Iteration 90, loss = 0.14631362\n",
      "Iteration 62, loss = 0.21933851\n",
      "Iteration 91, loss = 0.14801745\n",
      "Iteration 63, loss = 0.21691287\n",
      "Iteration 92, loss = 0.14386980\n",
      "Iteration 64, loss = 0.21534512\n",
      "Iteration 93, loss = 0.14454509\n",
      "Iteration 65, loss = 0.21327501\n",
      "Iteration 94, loss = 0.14369523\n",
      "Iteration 66, loss = 0.20989806\n",
      "Iteration 95, loss = 0.14054173\n",
      "Iteration 67, loss = 0.20825196\n",
      "Iteration 96, loss = 0.14112392\n",
      "Iteration 68, loss = 0.20635345\n",
      "Iteration 97, loss = 0.13913185\n",
      "Iteration 69, loss = 0.20411750\n",
      "Iteration 98, loss = 0.14081092\n",
      "Iteration 70, loss = 0.20270125\n",
      "Iteration 99, loss = 0.13664524\n",
      "Iteration 71, loss = 0.19990990\n",
      "Iteration 100, loss = 0.13518437\n",
      "Iteration 72, loss = 0.19791247\n",
      "Iteration 73, loss = 0.19724550\n",
      "Iteration 74, loss = 0.19455126\n",
      "Iteration 75, loss = 0.19367198\n",
      "Iteration 76, loss = 0.18994355\n",
      "Iteration 77, loss = 0.18925691\n",
      "Iteration 78, loss = 0.18731370\n",
      "Iteration 79, loss = 0.18544818\n",
      "Iteration 80, loss = 0.18299086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 81, loss = 0.18273325\n",
      "Iteration 82, loss = 0.17972798\n",
      "Iteration 83, loss = 0.17818426\n",
      "Iteration 84, loss = 0.17649044\n",
      "Iteration 85, loss = 0.17509564\n",
      "Iteration 86, loss = 0.17450275\n",
      "Iteration 87, loss = 0.17153994\n",
      "Iteration 88, loss = 0.17080197\n",
      "Iteration 89, loss = 0.16928346\n",
      "Iteration 90, loss = 0.16658524\n",
      "Iteration 1, loss = 1.36746144\n",
      "Iteration 91, loss = 0.16623903\n",
      "Iteration 2, loss = 1.12471638\n",
      "Iteration 92, loss = 0.16391022\n",
      "Iteration 3, loss = 0.96044763\n",
      "Iteration 93, loss = 0.16244829\n",
      "Iteration 4, loss = 0.84455353\n",
      "Iteration 94, loss = 0.16354169\n",
      "Iteration 5, loss = 0.75188277\n",
      "Iteration 95, loss = 0.15924180\n",
      "Iteration 6, loss = 0.68319843\n",
      "Iteration 96, loss = 0.15799491\n",
      "Iteration 7, loss = 0.62989242\n",
      "Iteration 97, loss = 0.15648403\n",
      "Iteration 8, loss = 0.58911624\n",
      "Iteration 98, loss = 0.15489265\n",
      "Iteration 9, loss = 0.55654613\n",
      "Iteration 99, loss = 0.15423025\n",
      "Iteration 100, loss = 0.15215588\n",
      "Iteration 10, loss = 0.53115391\n",
      "Iteration 11, loss = 0.51014933\n",
      "Iteration 12, loss = 0.49079768\n",
      "Iteration 13, loss = 0.47477711\n",
      "Iteration 14, loss = 0.46016658\n",
      "Iteration 15, loss = 0.44634456\n",
      "Iteration 16, loss = 0.43421067\n",
      "Iteration 17, loss = 0.42372896\n",
      "Iteration 18, loss = 0.41297073\n",
      "Iteration 19, loss = 0.40362626\n",
      "Iteration 20, loss = 0.39527112\n",
      "Iteration 21, loss = 0.38695590\n",
      "Iteration 22, loss = 0.37898154\n",
      "Iteration 23, loss = 0.37194079\n",
      "Iteration 24, loss = 0.36503640\n",
      "Iteration 25, loss = 0.35836641\n",
      "Iteration 26, loss = 0.35287463\n",
      "Iteration 27, loss = 0.34719553\n",
      "Iteration 1, loss = 1.22763283\n",
      "Iteration 28, loss = 0.34156339\n",
      "Iteration 2, loss = 1.06196011\n",
      "Iteration 29, loss = 0.33686324\n",
      "Iteration 3, loss = 0.94445410\n",
      "Iteration 30, loss = 0.33064393\n",
      "Iteration 4, loss = 0.84443535\n",
      "Iteration 31, loss = 0.32681682\n",
      "Iteration 5, loss = 0.76214573\n",
      "Iteration 32, loss = 0.32126380\n",
      "Iteration 6, loss = 0.69559660\n",
      "Iteration 33, loss = 0.31751335\n",
      "Iteration 7, loss = 0.64156649\n",
      "Iteration 34, loss = 0.31237912\n",
      "Iteration 8, loss = 0.59648792\n",
      "Iteration 35, loss = 0.30924021\n",
      "Iteration 9, loss = 0.55915397\n",
      "Iteration 36, loss = 0.30471845\n",
      "Iteration 10, loss = 0.52979146\n",
      "Iteration 37, loss = 0.30147147\n",
      "Iteration 11, loss = 0.50467726\n",
      "Iteration 38, loss = 0.29661893\n",
      "Iteration 12, loss = 0.48277160\n",
      "Iteration 39, loss = 0.29331284\n",
      "Iteration 40, loss = 0.28993780\n",
      "Iteration 41, loss = 0.28653047\n",
      "Iteration 13, loss = 0.46413377\n",
      "Iteration 14, loss = 0.44815977\n",
      "Iteration 42, loss = 0.28304286\n",
      "Iteration 43, loss = 0.28051095\n",
      "Iteration 15, loss = 0.43452880\n",
      "Iteration 16, loss = 0.42071250\n",
      "Iteration 44, loss = 0.27602594\n",
      "Iteration 17, loss = 0.40999552\n",
      "Iteration 45, loss = 0.27370559\n",
      "Iteration 46, loss = 0.27086019\n",
      "Iteration 18, loss = 0.39900342\n",
      "Iteration 47, loss = 0.26814078\n",
      "Iteration 19, loss = 0.39108032\n",
      "Iteration 48, loss = 0.26444925\n",
      "Iteration 20, loss = 0.38104011\n",
      "Iteration 49, loss = 0.26185642\n",
      "Iteration 50, loss = 0.25950270\n",
      "Iteration 21, loss = 0.37310159\n",
      "Iteration 22, loss = 0.36561808\n",
      "Iteration 51, loss = 0.25631777\n",
      "Iteration 23, loss = 0.35796684\n",
      "Iteration 24, loss = 0.35065662\n",
      "Iteration 25, loss = 0.34417705\n",
      "Iteration 26, loss = 0.33869017\n",
      "Iteration 27, loss = 0.33203369\n",
      "Iteration 28, loss = 0.32662023\n",
      "Iteration 29, loss = 0.32126242\n",
      "Iteration 52, loss = 0.25362222\n",
      "Iteration 30, loss = 0.31621568\n",
      "Iteration 53, loss = 0.25074356\n",
      "Iteration 31, loss = 0.31127094\n",
      "Iteration 54, loss = 0.24823947\n",
      "Iteration 32, loss = 0.30737789\n",
      "Iteration 55, loss = 0.24611835\n",
      "Iteration 33, loss = 0.30201284\n",
      "Iteration 56, loss = 0.24354542\n",
      "Iteration 34, loss = 0.29775196\n",
      "Iteration 57, loss = 0.24122349\n",
      "Iteration 35, loss = 0.29458731\n",
      "Iteration 58, loss = 0.23851689\n",
      "Iteration 36, loss = 0.28952725\n",
      "Iteration 59, loss = 0.23591570\n",
      "Iteration 37, loss = 0.28590585\n",
      "Iteration 60, loss = 0.23323463\n",
      "Iteration 38, loss = 0.28139407\n",
      "Iteration 61, loss = 0.23191522\n",
      "Iteration 39, loss = 0.27828025\n",
      "Iteration 40, loss = 0.27404743\n",
      "Iteration 41, loss = 0.27123521\n",
      "Iteration 42, loss = 0.26851153\n",
      "Iteration 62, loss = 0.22928361\n",
      "Iteration 43, loss = 0.26433642\n",
      "Iteration 44, loss = 0.26188678\n",
      "Iteration 63, loss = 0.22677836\n",
      "Iteration 45, loss = 0.25787156\n",
      "Iteration 64, loss = 0.22466687\n",
      "Iteration 46, loss = 0.25444595\n",
      "Iteration 65, loss = 0.22223804\n",
      "Iteration 47, loss = 0.25201189\n",
      "Iteration 66, loss = 0.21993590\n",
      "Iteration 48, loss = 0.24927200\n",
      "Iteration 67, loss = 0.21834466\n",
      "Iteration 49, loss = 0.24591116\n",
      "Iteration 68, loss = 0.21583725\n",
      "Iteration 50, loss = 0.24416168\n",
      "Iteration 69, loss = 0.21455385\n",
      "Iteration 51, loss = 0.24092252\n",
      "Iteration 70, loss = 0.21225737\n",
      "Iteration 52, loss = 0.23799389\n",
      "Iteration 71, loss = 0.21183577\n",
      "Iteration 53, loss = 0.23563799\n",
      "Iteration 72, loss = 0.20735696\n",
      "Iteration 54, loss = 0.23289552\n",
      "Iteration 73, loss = 0.20689554\n",
      "Iteration 55, loss = 0.23084571\n",
      "Iteration 74, loss = 0.20501456\n",
      "Iteration 56, loss = 0.22825490\n",
      "Iteration 75, loss = 0.20305264\n",
      "Iteration 57, loss = 0.22583101\n",
      "Iteration 76, loss = 0.20188521\n",
      "Iteration 58, loss = 0.22348625\n",
      "Iteration 77, loss = 0.19901250\n",
      "Iteration 59, loss = 0.22097552\n",
      "Iteration 78, loss = 0.19694668\n",
      "Iteration 60, loss = 0.21919699\n",
      "Iteration 79, loss = 0.19526503\n",
      "Iteration 61, loss = 0.21753547\n",
      "Iteration 80, loss = 0.19342333\n",
      "Iteration 62, loss = 0.21467720\n",
      "Iteration 81, loss = 0.19194485\n",
      "Iteration 63, loss = 0.21270914\n",
      "Iteration 82, loss = 0.19179323\n",
      "Iteration 64, loss = 0.21046938\n",
      "Iteration 83, loss = 0.18967098\n",
      "Iteration 65, loss = 0.20802583\n",
      "Iteration 84, loss = 0.18734650\n",
      "Iteration 66, loss = 0.20717782\n",
      "Iteration 85, loss = 0.18458219\n",
      "Iteration 67, loss = 0.20576417\n",
      "Iteration 86, loss = 0.18295007\n",
      "Iteration 68, loss = 0.20368911\n",
      "Iteration 87, loss = 0.18163947\n",
      "Iteration 69, loss = 0.20086389\n",
      "Iteration 88, loss = 0.17931157\n",
      "Iteration 70, loss = 0.19928365\n",
      "Iteration 89, loss = 0.17835179\n",
      "Iteration 71, loss = 0.19695127\n",
      "Iteration 90, loss = 0.17699068\n",
      "Iteration 72, loss = 0.19615405\n",
      "Iteration 91, loss = 0.17485427\n",
      "Iteration 73, loss = 0.19406923\n",
      "Iteration 92, loss = 0.17398118\n",
      "Iteration 74, loss = 0.19116199\n",
      "Iteration 93, loss = 0.17153378\n",
      "Iteration 75, loss = 0.18924192\n",
      "Iteration 94, loss = 0.17043928\n",
      "Iteration 76, loss = 0.18870339\n",
      "Iteration 95, loss = 0.16816547\n",
      "Iteration 77, loss = 0.18578903\n",
      "Iteration 96, loss = 0.16782442\n",
      "Iteration 78, loss = 0.18444265\n",
      "Iteration 97, loss = 0.16709781\n",
      "Iteration 79, loss = 0.18377203\n",
      "Iteration 98, loss = 0.16495021\n",
      "Iteration 80, loss = 0.18073327\n",
      "Iteration 99, loss = 0.16401498\n",
      "Iteration 81, loss = 0.18019382\n",
      "Iteration 100, loss = 0.16209382\n",
      "Iteration 82, loss = 0.17999576\n",
      "Iteration 101, loss = 0.15970583\n",
      "Iteration 83, loss = 0.17458040\n",
      "Iteration 102, loss = 0.15829092\n",
      "Iteration 84, loss = 0.17817090\n",
      "Iteration 103, loss = 0.15924195\n",
      "Iteration 85, loss = 0.17315505\n",
      "Iteration 104, loss = 0.15833337\n",
      "Iteration 86, loss = 0.17114291\n",
      "Iteration 105, loss = 0.15635115\n",
      "Iteration 87, loss = 0.16973180\n",
      "Iteration 106, loss = 0.15335116\n",
      "Iteration 88, loss = 0.16762904\n",
      "Iteration 107, loss = 0.15034785\n",
      "Iteration 89, loss = 0.16655623\n",
      "Iteration 108, loss = 0.14947934\n",
      "Iteration 90, loss = 0.16508322\n",
      "Iteration 109, loss = 0.14824803\n",
      "Iteration 91, loss = 0.16328107\n",
      "Iteration 110, loss = 0.14672504\n",
      "Iteration 92, loss = 0.16086858\n",
      "Iteration 111, loss = 0.14565467\n",
      "Iteration 93, loss = 0.16053668\n",
      "Iteration 112, loss = 0.14467002\n",
      "Iteration 94, loss = 0.15809768\n",
      "Iteration 113, loss = 0.14301565\n",
      "Iteration 95, loss = 0.15683238\n",
      "Iteration 114, loss = 0.14201073\n",
      "Iteration 96, loss = 0.15622783\n",
      "Iteration 115, loss = 0.14111080\n",
      "Iteration 97, loss = 0.15373066\n",
      "Iteration 116, loss = 0.13894399\n",
      "Iteration 98, loss = 0.15268864\n",
      "Iteration 117, loss = 0.13918554\n",
      "Iteration 99, loss = 0.15113758\n",
      "Iteration 118, loss = 0.13768430\n",
      "Iteration 100, loss = 0.14972079\n",
      "Iteration 119, loss = 0.13632589\n",
      "Iteration 101, loss = 0.14799940\n",
      "Iteration 120, loss = 0.13539050\n",
      "Iteration 102, loss = 0.14973709\n",
      "Iteration 121, loss = 0.13348020\n",
      "Iteration 103, loss = 0.14539390\n",
      "Iteration 122, loss = 0.13288680\n",
      "Iteration 104, loss = 0.14605702\n",
      "Iteration 123, loss = 0.13181712\n",
      "Iteration 105, loss = 0.14288045\n",
      "Iteration 106, loss = 0.14167402\n",
      "Iteration 124, loss = 0.13235561\n",
      "Iteration 107, loss = 0.14037897\n",
      "Iteration 125, loss = 0.13097064\n",
      "Iteration 108, loss = 0.13891731\n",
      "Iteration 126, loss = 0.13234625\n",
      "Iteration 109, loss = 0.13733905\n",
      "Iteration 127, loss = 0.12809672\n",
      "Iteration 110, loss = 0.13596277\n",
      "Iteration 128, loss = 0.13062047\n",
      "Iteration 129, loss = 0.12606991\n",
      "Iteration 111, loss = 0.13494474\n",
      "Iteration 112, loss = 0.13325873Iteration 130, loss = 0.12626417\n",
      "\n",
      "Iteration 113, loss = 0.13322594\n",
      "Iteration 131, loss = 0.12622686\n",
      "Iteration 114, loss = 0.13057350\n",
      "Iteration 132, loss = 0.12442729\n",
      "Iteration 115, loss = 0.13067955\n",
      "Iteration 133, loss = 0.12455554\n",
      "Iteration 116, loss = 0.12903372\n",
      "Iteration 134, loss = 0.12068484\n",
      "Iteration 117, loss = 0.13001375\n",
      "Iteration 135, loss = 0.12406259\n",
      "Iteration 118, loss = 0.12662111\n",
      "Iteration 136, loss = 0.12035437\n",
      "Iteration 119, loss = 0.12609692\n",
      "Iteration 137, loss = 0.11928615\n",
      "Iteration 120, loss = 0.12523086\n",
      "Iteration 138, loss = 0.11727278\n",
      "Iteration 121, loss = 0.12310124\n",
      "Iteration 139, loss = 0.11723289\n",
      "Iteration 140, loss = 0.11604464\n",
      "Iteration 122, loss = 0.12273601\n",
      "Iteration 141, loss = 0.11566528\n",
      "Iteration 123, loss = 0.12248346\n",
      "Iteration 142, loss = 0.11399105\n",
      "Iteration 124, loss = 0.12030045\n",
      "Iteration 143, loss = 0.11364589\n",
      "Iteration 125, loss = 0.12049902\n",
      "Iteration 144, loss = 0.11224352\n",
      "Iteration 126, loss = 0.11815037\n",
      "Iteration 145, loss = 0.11486349\n",
      "Iteration 127, loss = 0.11741728\n",
      "Iteration 146, loss = 0.11238305\n",
      "Iteration 128, loss = 0.11649450\n",
      "Iteration 147, loss = 0.10918012\n",
      "Iteration 129, loss = 0.11539149\n",
      "Iteration 148, loss = 0.10953929\n",
      "Iteration 130, loss = 0.11501257\n",
      "Iteration 149, loss = 0.10735017\n",
      "Iteration 131, loss = 0.11473043\n",
      "Iteration 150, loss = 0.10694338\n",
      "Iteration 132, loss = 0.11332986\n",
      "Iteration 151, loss = 0.10554801\n",
      "Iteration 133, loss = 0.11241929\n",
      "Iteration 152, loss = 0.10663331\n",
      "Iteration 134, loss = 0.11036120\n",
      "Iteration 153, loss = 0.10508304\n",
      "Iteration 135, loss = 0.11086258\n",
      "Iteration 154, loss = 0.10436681\n",
      "Iteration 136, loss = 0.10890466\n",
      "Iteration 155, loss = 0.10543430\n",
      "Iteration 137, loss = 0.10861644\n",
      "Iteration 156, loss = 0.10331356\n",
      "Iteration 138, loss = 0.10876028\n",
      "Iteration 157, loss = 0.10326326\n",
      "Iteration 139, loss = 0.10737162\n",
      "Iteration 158, loss = 0.10052063\n",
      "Iteration 140, loss = 0.10719396\n",
      "Iteration 159, loss = 0.10074136\n",
      "Iteration 141, loss = 0.10504440\n",
      "Iteration 160, loss = 0.09961364\n",
      "Iteration 142, loss = 0.10530090\n",
      "Iteration 161, loss = 0.09943505\n",
      "Iteration 143, loss = 0.10503905\n",
      "Iteration 162, loss = 0.09755539\n",
      "Iteration 144, loss = 0.10170611\n",
      "Iteration 163, loss = 0.09832730\n",
      "Iteration 145, loss = 0.10212415\n",
      "Iteration 164, loss = 0.09553883\n",
      "Iteration 146, loss = 0.10075063\n",
      "Iteration 165, loss = 0.09590403\n",
      "Iteration 147, loss = 0.09982872\n",
      "Iteration 166, loss = 0.10000389\n",
      "Iteration 148, loss = 0.09978548\n",
      "Iteration 167, loss = 0.10161583\n",
      "Iteration 149, loss = 0.09947571\n",
      "Iteration 168, loss = 0.09474314\n",
      "Iteration 150, loss = 0.09752241\n",
      "Iteration 169, loss = 0.09692094\n",
      "Iteration 151, loss = 0.09661235\n",
      "Iteration 170, loss = 0.09402952\n",
      "Iteration 152, loss = 0.09580285\n",
      "Iteration 171, loss = 0.09258270\n",
      "Iteration 153, loss = 0.09635527\n",
      "Iteration 172, loss = 0.09187619\n",
      "Iteration 154, loss = 0.09541604\n",
      "Iteration 173, loss = 0.09024670\n",
      "Iteration 155, loss = 0.09420652\n",
      "Iteration 174, loss = 0.09088518\n",
      "Iteration 156, loss = 0.09379819\n",
      "Iteration 175, loss = 0.08949495\n",
      "Iteration 157, loss = 0.09575758\n",
      "Iteration 176, loss = 0.08871377\n",
      "Iteration 158, loss = 0.09195614\n",
      "Iteration 177, loss = 0.09023390\n",
      "Iteration 159, loss = 0.09121601\n",
      "Iteration 178, loss = 0.08700328\n",
      "Iteration 160, loss = 0.09089943\n",
      "Iteration 179, loss = 0.08782588\n",
      "Iteration 161, loss = 0.09002089\n",
      "Iteration 180, loss = 0.08606885\n",
      "Iteration 162, loss = 0.09003190\n",
      "Iteration 181, loss = 0.08591415\n",
      "Iteration 163, loss = 0.08911249\n",
      "Iteration 182, loss = 0.08798405\n",
      "Iteration 164, loss = 0.08753108\n",
      "Iteration 183, loss = 0.08852094\n",
      "Iteration 165, loss = 0.08920087\n",
      "Iteration 184, loss = 0.08430422\n",
      "Iteration 166, loss = 0.08710178\n",
      "Iteration 185, loss = 0.08498355\n",
      "Iteration 167, loss = 0.08641853\n",
      "Iteration 186, loss = 0.08614112\n",
      "Iteration 168, loss = 0.08518461\n",
      "Iteration 187, loss = 0.08247384\n",
      "Iteration 169, loss = 0.08419526\n",
      "Iteration 188, loss = 0.08128358\n",
      "Iteration 170, loss = 0.08379698\n",
      "Iteration 189, loss = 0.08080242\n",
      "Iteration 171, loss = 0.08279642\n",
      "Iteration 190, loss = 0.08095426\n",
      "Iteration 172, loss = 0.08219033\n",
      "Iteration 191, loss = 0.08090444\n",
      "Iteration 173, loss = 0.08094346\n",
      "Iteration 192, loss = 0.07830610\n",
      "Iteration 174, loss = 0.08077687\n",
      "Iteration 193, loss = 0.07990019\n",
      "Iteration 175, loss = 0.08043184\n",
      "Iteration 194, loss = 0.07919239\n",
      "Iteration 176, loss = 0.08023918\n",
      "Iteration 195, loss = 0.07878637\n",
      "Iteration 177, loss = 0.07891583\n",
      "Iteration 196, loss = 0.07837782\n",
      "Iteration 178, loss = 0.07855394\n",
      "Iteration 197, loss = 0.07585655\n",
      "Iteration 179, loss = 0.07983113\n",
      "Iteration 198, loss = 0.07529896\n",
      "Iteration 180, loss = 0.07925983\n",
      "Iteration 199, loss = 0.07683782\n",
      "Iteration 181, loss = 0.07652291\n",
      "Iteration 200, loss = 0.07429195\n",
      "Iteration 182, loss = 0.07784715\n",
      "Iteration 183, loss = 0.07682740\n",
      "Iteration 184, loss = 0.07733131\n",
      "Iteration 185, loss = 0.07505010\n",
      "Iteration 186, loss = 0.07399344\n",
      "Iteration 187, loss = 0.07555689\n",
      "Iteration 188, loss = 0.07401403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 189, loss = 0.07206371\n",
      "Iteration 190, loss = 0.07301130\n",
      "Iteration 191, loss = 0.07042335\n",
      "Iteration 192, loss = 0.07089321\n",
      "Iteration 193, loss = 0.07121761\n",
      "Iteration 194, loss = 0.07023287\n",
      "Iteration 195, loss = 0.07024324\n",
      "Iteration 196, loss = 0.06859765\n",
      "Iteration 197, loss = 0.06870537\n",
      "Iteration 198, loss = 0.06834291\n",
      "Iteration 199, loss = 0.06820186\n",
      "Iteration 1, loss = 1.49053720\n",
      "Iteration 200, loss = 0.06602902\n",
      "Iteration 2, loss = 1.18775444\n",
      "Iteration 3, loss = 0.99940206\n",
      "Iteration 4, loss = 0.88137061\n",
      "Iteration 5, loss = 0.79327458\n",
      "Iteration 6, loss = 0.71846046\n",
      "Iteration 7, loss = 0.65624750\n",
      "Iteration 8, loss = 0.61113782\n",
      "Iteration 9, loss = 0.57497786\n",
      "Iteration 10, loss = 0.54593796\n",
      "Iteration 11, loss = 0.52050785\n",
      "Iteration 12, loss = 0.49842086\n",
      "Iteration 13, loss = 0.48095010\n",
      "Iteration 14, loss = 0.46590571\n",
      "Iteration 15, loss = 0.45181282\n",
      "Iteration 16, loss = 0.43928894\n",
      "Iteration 17, loss = 0.42867218\n",
      "Iteration 18, loss = 0.41807059\n",
      "Iteration 19, loss = 0.40864023\n",
      "Iteration 1, loss = 1.08722290\n",
      "Iteration 20, loss = 0.40022686\n",
      "Iteration 2, loss = 0.90900304\n",
      "Iteration 21, loss = 0.39217411\n",
      "Iteration 3, loss = 0.80138445\n",
      "Iteration 22, loss = 0.38458713\n",
      "Iteration 4, loss = 0.71925838\n",
      "Iteration 23, loss = 0.37746522\n",
      "Iteration 5, loss = 0.65868007\n",
      "Iteration 24, loss = 0.37199186\n",
      "Iteration 6, loss = 0.61118571\n",
      "Iteration 25, loss = 0.36383059\n",
      "Iteration 7, loss = 0.57180263\n",
      "Iteration 26, loss = 0.35857725\n",
      "Iteration 8, loss = 0.54066437\n",
      "Iteration 27, loss = 0.35286272\n",
      "Iteration 9, loss = 0.51445605\n",
      "Iteration 28, loss = 0.34716857\n",
      "Iteration 10, loss = 0.49046560\n",
      "Iteration 29, loss = 0.34194235\n",
      "Iteration 11, loss = 0.46951170\n",
      "Iteration 30, loss = 0.33650702\n",
      "Iteration 12, loss = 0.45113481\n",
      "Iteration 31, loss = 0.33237354\n",
      "Iteration 13, loss = 0.43588170\n",
      "Iteration 32, loss = 0.32694408\n",
      "Iteration 14, loss = 0.42195512\n",
      "Iteration 33, loss = 0.32328384\n",
      "Iteration 15, loss = 0.40927020\n",
      "Iteration 34, loss = 0.31835193\n",
      "Iteration 16, loss = 0.39839196\n",
      "Iteration 35, loss = 0.31445730\n",
      "Iteration 17, loss = 0.38922049\n",
      "Iteration 36, loss = 0.31054629\n",
      "Iteration 18, loss = 0.38077050\n",
      "Iteration 37, loss = 0.30615967\n",
      "Iteration 19, loss = 0.37224198\n",
      "Iteration 38, loss = 0.30470912\n",
      "Iteration 20, loss = 0.36396734\n",
      "Iteration 39, loss = 0.29793869\n",
      "Iteration 21, loss = 0.35784919\n",
      "Iteration 40, loss = 0.29734657\n",
      "Iteration 22, loss = 0.35128968\n",
      "Iteration 41, loss = 0.29278060\n",
      "Iteration 23, loss = 0.34515039\n",
      "Iteration 42, loss = 0.28849597\n",
      "Iteration 24, loss = 0.33918497\n",
      "Iteration 43, loss = 0.28474686\n",
      "Iteration 25, loss = 0.33384537\n",
      "Iteration 44, loss = 0.28147526\n",
      "Iteration 26, loss = 0.33017313\n",
      "Iteration 45, loss = 0.27807935\n",
      "Iteration 27, loss = 0.32441993\n",
      "Iteration 46, loss = 0.27539582\n",
      "Iteration 28, loss = 0.31963321\n",
      "Iteration 47, loss = 0.27256135\n",
      "Iteration 29, loss = 0.31519191\n",
      "Iteration 48, loss = 0.26916965\n",
      "Iteration 30, loss = 0.31065012\n",
      "Iteration 49, loss = 0.26640453\n",
      "Iteration 31, loss = 0.30619446\n",
      "Iteration 50, loss = 0.26410839\n",
      "Iteration 32, loss = 0.30229943\n",
      "Iteration 51, loss = 0.26079239\n",
      "Iteration 33, loss = 0.29826534\n",
      "Iteration 52, loss = 0.25779488\n",
      "Iteration 34, loss = 0.29449107\n",
      "Iteration 53, loss = 0.25522147\n",
      "Iteration 35, loss = 0.29107718\n",
      "Iteration 54, loss = 0.25350966\n",
      "Iteration 36, loss = 0.28787517\n",
      "Iteration 55, loss = 0.24992031\n",
      "Iteration 37, loss = 0.28607684\n",
      "Iteration 56, loss = 0.24797571\n",
      "Iteration 38, loss = 0.28025655\n",
      "Iteration 57, loss = 0.24511500\n",
      "Iteration 39, loss = 0.27862362\n",
      "Iteration 58, loss = 0.24259524\n",
      "Iteration 40, loss = 0.27405436\n",
      "Iteration 59, loss = 0.24062869\n",
      "Iteration 41, loss = 0.27209443\n",
      "Iteration 60, loss = 0.23824507\n",
      "Iteration 42, loss = 0.26857124\n",
      "Iteration 61, loss = 0.23525921\n",
      "Iteration 43, loss = 0.26610908\n",
      "Iteration 62, loss = 0.23317243\n",
      "Iteration 44, loss = 0.26323497\n",
      "Iteration 63, loss = 0.23130200\n",
      "Iteration 45, loss = 0.26000642\n",
      "Iteration 64, loss = 0.22877714\n",
      "Iteration 46, loss = 0.25703536\n",
      "Iteration 65, loss = 0.22765033\n",
      "Iteration 47, loss = 0.25401372\n",
      "Iteration 66, loss = 0.22612702\n",
      "Iteration 48, loss = 0.25234092\n",
      "Iteration 67, loss = 0.22255208\n",
      "Iteration 49, loss = 0.24936954\n",
      "Iteration 68, loss = 0.21987432\n",
      "Iteration 50, loss = 0.24636322\n",
      "Iteration 69, loss = 0.21811523\n",
      "Iteration 51, loss = 0.24401385\n",
      "Iteration 70, loss = 0.21672840\n",
      "Iteration 52, loss = 0.24202305\n",
      "Iteration 71, loss = 0.21444264\n",
      "Iteration 53, loss = 0.23933070\n",
      "Iteration 72, loss = 0.21315635\n",
      "Iteration 54, loss = 0.23748331\n",
      "Iteration 73, loss = 0.20980367\n",
      "Iteration 55, loss = 0.23574362\n",
      "Iteration 74, loss = 0.20823161\n",
      "Iteration 56, loss = 0.23227185\n",
      "Iteration 75, loss = 0.20637522\n",
      "Iteration 57, loss = 0.23096186\n",
      "Iteration 76, loss = 0.20407046\n",
      "Iteration 58, loss = 0.22884838\n",
      "Iteration 77, loss = 0.20224927\n",
      "Iteration 59, loss = 0.22643494\n",
      "Iteration 78, loss = 0.20021829\n",
      "Iteration 60, loss = 0.22391150\n",
      "Iteration 79, loss = 0.19869173\n",
      "Iteration 61, loss = 0.22284949\n",
      "Iteration 80, loss = 0.19657411\n",
      "Iteration 62, loss = 0.21964707\n",
      "Iteration 81, loss = 0.19395185\n",
      "Iteration 63, loss = 0.21741181\n",
      "Iteration 82, loss = 0.19438338\n",
      "Iteration 64, loss = 0.21540535\n",
      "Iteration 83, loss = 0.19063051\n",
      "Iteration 65, loss = 0.21535653\n",
      "Iteration 84, loss = 0.18874845\n",
      "Iteration 66, loss = 0.21134716\n",
      "Iteration 85, loss = 0.18736316\n",
      "Iteration 67, loss = 0.21083950\n",
      "Iteration 86, loss = 0.18553803\n",
      "Iteration 68, loss = 0.20885107\n",
      "Iteration 87, loss = 0.18320450\n",
      "Iteration 69, loss = 0.20705319\n",
      "Iteration 88, loss = 0.18270495\n",
      "Iteration 70, loss = 0.20469373\n",
      "Iteration 89, loss = 0.17969267\n",
      "Iteration 71, loss = 0.20260116\n",
      "Iteration 90, loss = 0.17917554\n",
      "Iteration 72, loss = 0.20092430\n",
      "Iteration 91, loss = 0.17655571\n",
      "Iteration 73, loss = 0.19893835\n",
      "Iteration 92, loss = 0.17529157\n",
      "Iteration 74, loss = 0.19892996\n",
      "Iteration 93, loss = 0.17403966\n",
      "Iteration 75, loss = 0.19546720\n",
      "Iteration 94, loss = 0.17296565\n",
      "Iteration 76, loss = 0.19511339\n",
      "Iteration 95, loss = 0.17007936\n",
      "Iteration 77, loss = 0.19213668\n",
      "Iteration 96, loss = 0.16797063\n",
      "Iteration 78, loss = 0.19037313\n",
      "Iteration 97, loss = 0.16735091\n",
      "Iteration 79, loss = 0.18990989\n",
      "Iteration 98, loss = 0.16459110\n",
      "Iteration 80, loss = 0.18806914\n",
      "Iteration 99, loss = 0.16386079\n",
      "Iteration 81, loss = 0.18812207\n",
      "Iteration 100, loss = 0.16369645\n",
      "Iteration 82, loss = 0.18595661\n",
      "Iteration 101, loss = 0.16203895\n",
      "Iteration 83, loss = 0.18392378\n",
      "Iteration 102, loss = 0.15960230\n",
      "Iteration 84, loss = 0.18066327\n",
      "Iteration 103, loss = 0.15728678\n",
      "Iteration 85, loss = 0.17912280\n",
      "Iteration 104, loss = 0.15566235\n",
      "Iteration 86, loss = 0.17727367\n",
      "Iteration 105, loss = 0.15489667\n",
      "Iteration 87, loss = 0.17534327\n",
      "Iteration 106, loss = 0.15378140\n",
      "Iteration 88, loss = 0.17443287\n",
      "Iteration 107, loss = 0.15146025\n",
      "Iteration 89, loss = 0.17282300\n",
      "Iteration 108, loss = 0.15006482\n",
      "Iteration 90, loss = 0.17057598\n",
      "Iteration 109, loss = 0.14870144\n",
      "Iteration 91, loss = 0.16972362\n",
      "Iteration 110, loss = 0.14836130\n",
      "Iteration 92, loss = 0.17089766\n",
      "Iteration 111, loss = 0.14508926\n",
      "Iteration 93, loss = 0.17013904\n",
      "Iteration 112, loss = 0.14504982\n",
      "Iteration 94, loss = 0.16800573\n",
      "Iteration 113, loss = 0.14293419\n",
      "Iteration 95, loss = 0.16553969\n",
      "Iteration 114, loss = 0.14113950\n",
      "Iteration 96, loss = 0.16272825\n",
      "Iteration 115, loss = 0.14048980\n",
      "Iteration 97, loss = 0.16118912\n",
      "Iteration 116, loss = 0.13968257\n",
      "Iteration 98, loss = 0.15989686\n",
      "Iteration 117, loss = 0.13901884\n",
      "Iteration 99, loss = 0.15777678\n",
      "Iteration 118, loss = 0.13703931\n",
      "Iteration 100, loss = 0.15716634\n",
      "Iteration 119, loss = 0.13514417\n",
      "Iteration 101, loss = 0.15547041\n",
      "Iteration 120, loss = 0.13397922\n",
      "Iteration 102, loss = 0.15365406\n",
      "Iteration 121, loss = 0.13364112\n",
      "Iteration 103, loss = 0.15425719\n",
      "Iteration 122, loss = 0.13158186\n",
      "Iteration 104, loss = 0.15081679\n",
      "Iteration 123, loss = 0.13150237\n",
      "Iteration 105, loss = 0.15079915\n",
      "Iteration 124, loss = 0.12942667\n",
      "Iteration 106, loss = 0.14914825\n",
      "Iteration 125, loss = 0.12897366\n",
      "Iteration 107, loss = 0.14730678\n",
      "Iteration 126, loss = 0.12909142\n",
      "Iteration 108, loss = 0.14654597\n",
      "Iteration 127, loss = 0.12672878\n",
      "Iteration 109, loss = 0.14608079\n",
      "Iteration 128, loss = 0.12591200\n",
      "Iteration 110, loss = 0.14450744\n",
      "Iteration 129, loss = 0.12487683\n",
      "Iteration 111, loss = 0.14402845\n",
      "Iteration 130, loss = 0.12399196\n",
      "Iteration 112, loss = 0.14229189\n",
      "Iteration 131, loss = 0.12417793\n",
      "Iteration 113, loss = 0.14133444\n",
      "Iteration 132, loss = 0.12325889\n",
      "Iteration 114, loss = 0.13884197\n",
      "Iteration 133, loss = 0.12291978\n",
      "Iteration 115, loss = 0.13890079\n",
      "Iteration 134, loss = 0.12292774\n",
      "Iteration 116, loss = 0.13600397\n",
      "Iteration 135, loss = 0.11942867\n",
      "Iteration 117, loss = 0.13735829\n",
      "Iteration 136, loss = 0.12079147\n",
      "Iteration 118, loss = 0.13260116\n",
      "Iteration 137, loss = 0.11845263\n",
      "Iteration 119, loss = 0.13482036\n",
      "Iteration 138, loss = 0.11889840\n",
      "Iteration 120, loss = 0.13410720\n",
      "Iteration 139, loss = 0.11650609\n",
      "Iteration 121, loss = 0.13088297\n",
      "Iteration 140, loss = 0.11526624\n",
      "Iteration 122, loss = 0.13083263\n",
      "Iteration 141, loss = 0.11402833\n",
      "Iteration 123, loss = 0.12935060\n",
      "Iteration 142, loss = 0.11310444\n",
      "Iteration 124, loss = 0.12700004\n",
      "Iteration 143, loss = 0.11112522\n",
      "Iteration 125, loss = 0.12656690\n",
      "Iteration 144, loss = 0.11215867\n",
      "Iteration 126, loss = 0.12507013\n",
      "Iteration 145, loss = 0.10993962\n",
      "Iteration 127, loss = 0.12730254\n",
      "Iteration 146, loss = 0.10942818\n",
      "Iteration 128, loss = 0.12590341\n",
      "Iteration 147, loss = 0.10876089\n",
      "Iteration 129, loss = 0.12305160\n",
      "Iteration 148, loss = 0.10799484\n",
      "Iteration 130, loss = 0.12216066\n",
      "Iteration 149, loss = 0.10912547\n",
      "Iteration 131, loss = 0.12113310\n",
      "Iteration 150, loss = 0.10802120\n",
      "Iteration 132, loss = 0.11963613\n",
      "Iteration 151, loss = 0.10611485\n",
      "Iteration 133, loss = 0.12103534\n",
      "Iteration 152, loss = 0.10708104\n",
      "Iteration 134, loss = 0.11827658\n",
      "Iteration 153, loss = 0.10484800\n",
      "Iteration 135, loss = 0.11650533\n",
      "Iteration 154, loss = 0.10378210\n",
      "Iteration 136, loss = 0.11740962\n",
      "Iteration 155, loss = 0.10333053\n",
      "Iteration 137, loss = 0.11514359\n",
      "Iteration 156, loss = 0.10203989\n",
      "Iteration 138, loss = 0.11412867\n",
      "Iteration 157, loss = 0.10191954\n",
      "Iteration 139, loss = 0.11256702\n",
      "Iteration 158, loss = 0.09995438\n",
      "Iteration 140, loss = 0.11283562\n",
      "Iteration 159, loss = 0.10279641\n",
      "Iteration 141, loss = 0.11173731\n",
      "Iteration 160, loss = 0.10389821\n",
      "Iteration 142, loss = 0.11177275\n",
      "Iteration 161, loss = 0.09675885\n",
      "Iteration 143, loss = 0.10992470\n",
      "Iteration 162, loss = 0.10036996\n",
      "Iteration 144, loss = 0.10971624\n",
      "Iteration 163, loss = 0.09810464\n",
      "Iteration 145, loss = 0.11059814\n",
      "Iteration 164, loss = 0.09495450\n",
      "Iteration 146, loss = 0.11035759\n",
      "Iteration 165, loss = 0.09485783\n",
      "Iteration 147, loss = 0.10647018\n",
      "Iteration 166, loss = 0.09407870\n",
      "Iteration 148, loss = 0.10716139\n",
      "Iteration 167, loss = 0.09341354\n",
      "Iteration 149, loss = 0.10630142\n",
      "Iteration 168, loss = 0.09335088\n",
      "Iteration 150, loss = 0.10408109\n",
      "Iteration 169, loss = 0.09208950\n",
      "Iteration 151, loss = 0.10346011\n",
      "Iteration 170, loss = 0.09157575\n",
      "Iteration 152, loss = 0.10296679\n",
      "Iteration 171, loss = 0.09102857\n",
      "Iteration 153, loss = 0.10407513\n",
      "Iteration 172, loss = 0.09023948\n",
      "Iteration 154, loss = 0.10154973\n",
      "Iteration 173, loss = 0.09083502\n",
      "Iteration 155, loss = 0.10331101\n",
      "Iteration 174, loss = 0.08995432\n",
      "Iteration 156, loss = 0.10192752\n",
      "Iteration 175, loss = 0.09008888\n",
      "Iteration 157, loss = 0.10142339\n",
      "Iteration 176, loss = 0.08897480\n",
      "Iteration 158, loss = 0.10076698\n",
      "Iteration 177, loss = 0.08774400\n",
      "Iteration 159, loss = 0.09894234\n",
      "Iteration 178, loss = 0.08640062\n",
      "Iteration 160, loss = 0.09818142\n",
      "Iteration 179, loss = 0.08627912\n",
      "Iteration 161, loss = 0.09712148\n",
      "Iteration 180, loss = 0.08523232\n",
      "Iteration 162, loss = 0.09585673\n",
      "Iteration 181, loss = 0.08543936\n",
      "Iteration 163, loss = 0.09561750\n",
      "Iteration 182, loss = 0.08388135\n",
      "Iteration 164, loss = 0.09471463\n",
      "Iteration 183, loss = 0.08377035\n",
      "Iteration 165, loss = 0.09371412\n",
      "Iteration 184, loss = 0.08278948\n",
      "Iteration 166, loss = 0.09345147\n",
      "Iteration 185, loss = 0.08272944\n",
      "Iteration 167, loss = 0.09226555\n",
      "Iteration 186, loss = 0.08103584\n",
      "Iteration 168, loss = 0.09252714\n",
      "Iteration 187, loss = 0.08209488\n",
      "Iteration 169, loss = 0.09245619\n",
      "Iteration 188, loss = 0.08263428\n",
      "Iteration 170, loss = 0.09035796\n",
      "Iteration 189, loss = 0.08044423\n",
      "Iteration 171, loss = 0.09188485\n",
      "Iteration 190, loss = 0.08036711\n",
      "Iteration 172, loss = 0.08975188\n",
      "Iteration 191, loss = 0.08057072\n",
      "Iteration 173, loss = 0.08850933\n",
      "Iteration 192, loss = 0.07899079\n",
      "Iteration 174, loss = 0.08922195\n",
      "Iteration 193, loss = 0.07835994\n",
      "Iteration 175, loss = 0.08799832\n",
      "Iteration 194, loss = 0.07772098\n",
      "Iteration 176, loss = 0.08712017\n",
      "Iteration 195, loss = 0.07669488\n",
      "Iteration 177, loss = 0.08712502\n",
      "Iteration 196, loss = 0.07759525\n",
      "Iteration 178, loss = 0.08809271\n",
      "Iteration 197, loss = 0.07635932\n",
      "Iteration 179, loss = 0.08740582\n",
      "Iteration 198, loss = 0.07521667\n",
      "Iteration 180, loss = 0.08482835\n",
      "Iteration 199, loss = 0.07491537\n",
      "Iteration 181, loss = 0.08658989\n",
      "Iteration 200, loss = 0.07578745\n",
      "Iteration 182, loss = 0.08297727\n",
      "Iteration 183, loss = 0.08406194\n",
      "Iteration 184, loss = 0.08368494\n",
      "Iteration 185, loss = 0.08154238\n",
      "Iteration 186, loss = 0.08218978\n",
      "Iteration 187, loss = 0.08181363\n",
      "Iteration 188, loss = 0.08022230\n",
      "Iteration 189, loss = 0.08009403\n",
      "Iteration 190, loss = 0.08011709\n",
      "Iteration 191, loss = 0.07827893\n",
      "Iteration 192, loss = 0.08004738\n",
      "Iteration 193, loss = 0.07721637\n",
      "Iteration 194, loss = 0.07830085\n",
      "Iteration 195, loss = 0.07603137\n",
      "Iteration 196, loss = 0.07743334\n",
      "Iteration 197, loss = 0.07925553\n",
      "Iteration 198, loss = 0.08156061\n",
      "Iteration 199, loss = 0.07601048\n",
      "Iteration 1, loss = 1.13886141\n",
      "Iteration 200, loss = 0.07859350\n",
      "Iteration 2, loss = 0.95972948\n",
      "Iteration 3, loss = 0.84233795\n",
      "Iteration 4, loss = 0.75135399\n",
      "Iteration 5, loss = 0.68508627\n",
      "Iteration 6, loss = 0.63231993\n",
      "Iteration 7, loss = 0.58790409\n",
      "Iteration 8, loss = 0.55159244\n",
      "Iteration 9, loss = 0.52145506\n",
      "Iteration 10, loss = 0.49613504\n",
      "Iteration 11, loss = 0.47470593\n",
      "Iteration 12, loss = 0.45538002\n",
      "Iteration 13, loss = 0.43893704\n",
      "Iteration 14, loss = 0.42444470\n",
      "Iteration 15, loss = 0.41257644\n",
      "Iteration 16, loss = 0.40063949\n",
      "Iteration 17, loss = 0.39098829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.38116011\n",
      "Iteration 19, loss = 0.37349465\n",
      "Iteration 1, loss = 1.29386704\n",
      "Iteration 20, loss = 0.36581468\n",
      "Iteration 2, loss = 1.09587138\n",
      "Iteration 21, loss = 0.35919624\n",
      "Iteration 3, loss = 0.96182348\n",
      "Iteration 22, loss = 0.35275236\n",
      "Iteration 4, loss = 0.85289180\n",
      "Iteration 23, loss = 0.34578128\n",
      "Iteration 5, loss = 0.76936795\n",
      "Iteration 24, loss = 0.33985339\n",
      "Iteration 6, loss = 0.70332544\n",
      "Iteration 25, loss = 0.33444523\n",
      "Iteration 7, loss = 0.65024306\n",
      "Iteration 26, loss = 0.32922334\n",
      "Iteration 8, loss = 0.60645240\n",
      "Iteration 27, loss = 0.32430223\n",
      "Iteration 9, loss = 0.57111845\n",
      "Iteration 28, loss = 0.31973008\n",
      "Iteration 10, loss = 0.54005156\n",
      "Iteration 29, loss = 0.31476597\n",
      "Iteration 11, loss = 0.51401452\n",
      "Iteration 30, loss = 0.31127284\n",
      "Iteration 12, loss = 0.49158864\n",
      "Iteration 31, loss = 0.30801764\n",
      "Iteration 13, loss = 0.47318072\n",
      "Iteration 32, loss = 0.30287841\n",
      "Iteration 14, loss = 0.45495317\n",
      "Iteration 33, loss = 0.29912609\n",
      "Iteration 15, loss = 0.43964636\n",
      "Iteration 34, loss = 0.29578809\n",
      "Iteration 16, loss = 0.42678888\n",
      "Iteration 35, loss = 0.29219636\n",
      "Iteration 17, loss = 0.41470657\n",
      "Iteration 36, loss = 0.28918723\n",
      "Iteration 18, loss = 0.40380784\n",
      "Iteration 37, loss = 0.28588668\n",
      "Iteration 19, loss = 0.39381736\n",
      "Iteration 38, loss = 0.28183146\n",
      "Iteration 20, loss = 0.38517495\n",
      "Iteration 39, loss = 0.27811451\n",
      "Iteration 21, loss = 0.37692204\n",
      "Iteration 40, loss = 0.27586602\n",
      "Iteration 22, loss = 0.36927201\n",
      "Iteration 41, loss = 0.27220502\n",
      "Iteration 23, loss = 0.36331882\n",
      "Iteration 42, loss = 0.27001274\n",
      "Iteration 24, loss = 0.35593726\n",
      "Iteration 43, loss = 0.26738808\n",
      "Iteration 25, loss = 0.34964999\n",
      "Iteration 44, loss = 0.26416682\n",
      "Iteration 26, loss = 0.34452664\n",
      "Iteration 45, loss = 0.26140295\n",
      "Iteration 27, loss = 0.33909750\n",
      "Iteration 46, loss = 0.25898551\n",
      "Iteration 28, loss = 0.33414632\n",
      "Iteration 47, loss = 0.25578493\n",
      "Iteration 29, loss = 0.32921919\n",
      "Iteration 48, loss = 0.25302740\n",
      "Iteration 30, loss = 0.32376862\n",
      "Iteration 49, loss = 0.25125382\n",
      "Iteration 31, loss = 0.31970253\n",
      "Iteration 50, loss = 0.24792193\n",
      "Iteration 32, loss = 0.31505954\n",
      "Iteration 51, loss = 0.24532089\n",
      "Iteration 33, loss = 0.31050165\n",
      "Iteration 52, loss = 0.24295229\n",
      "Iteration 34, loss = 0.30666793\n",
      "Iteration 53, loss = 0.24061306\n",
      "Iteration 35, loss = 0.30253488\n",
      "Iteration 54, loss = 0.23838057\n",
      "Iteration 36, loss = 0.29904610\n",
      "Iteration 55, loss = 0.23639084\n",
      "Iteration 37, loss = 0.29640920\n",
      "Iteration 56, loss = 0.23416801\n",
      "Iteration 38, loss = 0.29104646\n",
      "Iteration 57, loss = 0.23228825\n",
      "Iteration 39, loss = 0.28760242\n",
      "Iteration 58, loss = 0.22976817\n",
      "Iteration 40, loss = 0.28472324\n",
      "Iteration 59, loss = 0.22818593\n",
      "Iteration 41, loss = 0.28133233\n",
      "Iteration 60, loss = 0.22511673\n",
      "Iteration 42, loss = 0.27852042\n",
      "Iteration 61, loss = 0.22282665\n",
      "Iteration 43, loss = 0.27432941\n",
      "Iteration 62, loss = 0.22043134\n",
      "Iteration 44, loss = 0.27144748\n",
      "Iteration 63, loss = 0.21882762\n",
      "Iteration 45, loss = 0.26822656\n",
      "Iteration 64, loss = 0.21702900\n",
      "Iteration 46, loss = 0.26555163\n",
      "Iteration 65, loss = 0.21572443\n",
      "Iteration 47, loss = 0.26393733\n",
      "Iteration 66, loss = 0.21323359\n",
      "Iteration 48, loss = 0.25975324\n",
      "Iteration 67, loss = 0.21080469\n",
      "Iteration 49, loss = 0.25790412\n",
      "Iteration 68, loss = 0.21080209\n",
      "Iteration 50, loss = 0.25418219\n",
      "Iteration 69, loss = 0.20715295\n",
      "Iteration 51, loss = 0.25307538\n",
      "Iteration 70, loss = 0.20534901\n",
      "Iteration 52, loss = 0.24943091\n",
      "Iteration 71, loss = 0.20445287\n",
      "Iteration 53, loss = 0.24657753\n",
      "Iteration 72, loss = 0.20077079\n",
      "Iteration 54, loss = 0.24557996\n",
      "Iteration 73, loss = 0.19987269\n",
      "Iteration 55, loss = 0.24220687\n",
      "Iteration 74, loss = 0.19750887\n",
      "Iteration 56, loss = 0.23965653\n",
      "Iteration 75, loss = 0.19603339\n",
      "Iteration 57, loss = 0.23828386\n",
      "Iteration 76, loss = 0.19330722\n",
      "Iteration 58, loss = 0.23527681\n",
      "Iteration 77, loss = 0.19170937\n",
      "Iteration 59, loss = 0.23298829\n",
      "Iteration 78, loss = 0.19011727\n",
      "Iteration 60, loss = 0.23061876\n",
      "Iteration 79, loss = 0.18893680\n",
      "Iteration 61, loss = 0.22796458\n",
      "Iteration 80, loss = 0.18736986\n",
      "Iteration 62, loss = 0.22591001\n",
      "Iteration 81, loss = 0.18478559\n",
      "Iteration 63, loss = 0.22375466\n",
      "Iteration 82, loss = 0.18347069\n",
      "Iteration 64, loss = 0.22182543\n",
      "Iteration 83, loss = 0.18160774\n",
      "Iteration 65, loss = 0.22045970\n",
      "Iteration 84, loss = 0.17937368\n",
      "Iteration 66, loss = 0.21770028\n",
      "Iteration 85, loss = 0.17825694\n",
      "Iteration 67, loss = 0.21611541\n",
      "Iteration 86, loss = 0.17678316\n",
      "Iteration 68, loss = 0.21436447\n",
      "Iteration 87, loss = 0.17562223\n",
      "Iteration 69, loss = 0.21180215\n",
      "Iteration 88, loss = 0.17411067\n",
      "Iteration 70, loss = 0.21044214\n",
      "Iteration 89, loss = 0.17242179\n",
      "Iteration 71, loss = 0.20910196\n",
      "Iteration 90, loss = 0.17084146\n",
      "Iteration 72, loss = 0.20684397\n",
      "Iteration 91, loss = 0.16772796\n",
      "Iteration 73, loss = 0.20541188\n",
      "Iteration 92, loss = 0.16644617\n",
      "Iteration 74, loss = 0.20259503\n",
      "Iteration 93, loss = 0.16629134\n",
      "Iteration 75, loss = 0.20118686\n",
      "Iteration 94, loss = 0.16553679\n",
      "Iteration 76, loss = 0.19819225\n",
      "Iteration 95, loss = 0.16219031\n",
      "Iteration 77, loss = 0.19690319\n",
      "Iteration 96, loss = 0.16333748\n",
      "Iteration 78, loss = 0.19459224\n",
      "Iteration 97, loss = 0.16016665\n",
      "Iteration 79, loss = 0.19472235\n",
      "Iteration 98, loss = 0.15823094\n",
      "Iteration 80, loss = 0.19101705\n",
      "Iteration 99, loss = 0.15662570\n",
      "Iteration 81, loss = 0.19075276\n",
      "Iteration 100, loss = 0.15533321\n",
      "Iteration 82, loss = 0.18799209\n",
      "Iteration 101, loss = 0.15405230\n",
      "Iteration 83, loss = 0.18612238\n",
      "Iteration 102, loss = 0.15210145\n",
      "Iteration 84, loss = 0.18475125\n",
      "Iteration 103, loss = 0.15280424\n",
      "Iteration 85, loss = 0.18358680\n",
      "Iteration 104, loss = 0.15145936\n",
      "Iteration 86, loss = 0.18218634\n",
      "Iteration 105, loss = 0.14875872\n",
      "Iteration 87, loss = 0.17955724\n",
      "Iteration 106, loss = 0.14794966\n",
      "Iteration 88, loss = 0.17826568\n",
      "Iteration 107, loss = 0.14516063\n",
      "Iteration 89, loss = 0.17631082\n",
      "Iteration 108, loss = 0.14535241\n",
      "Iteration 90, loss = 0.17663293\n",
      "Iteration 109, loss = 0.14435937\n",
      "Iteration 91, loss = 0.17304890\n",
      "Iteration 110, loss = 0.14240903\n",
      "Iteration 92, loss = 0.17232224\n",
      "Iteration 111, loss = 0.14141442\n",
      "Iteration 93, loss = 0.17033709\n",
      "Iteration 112, loss = 0.14081633\n",
      "Iteration 94, loss = 0.16865344\n",
      "Iteration 113, loss = 0.13917378\n",
      "Iteration 95, loss = 0.16809944\n",
      "Iteration 114, loss = 0.13844647\n",
      "Iteration 96, loss = 0.16423854\n",
      "Iteration 115, loss = 0.13681816\n",
      "Iteration 97, loss = 0.16600154\n",
      "Iteration 116, loss = 0.13616859\n",
      "Iteration 98, loss = 0.16326740\n",
      "Iteration 117, loss = 0.13486292\n",
      "Iteration 99, loss = 0.16140199\n",
      "Iteration 118, loss = 0.13424242\n",
      "Iteration 100, loss = 0.15932792\n",
      "Iteration 119, loss = 0.13238999\n",
      "Iteration 101, loss = 0.15876038\n",
      "Iteration 120, loss = 0.13296834\n",
      "Iteration 102, loss = 0.15778410\n",
      "Iteration 121, loss = 0.13063395\n",
      "Iteration 103, loss = 0.15627036\n",
      "Iteration 122, loss = 0.13239146\n",
      "Iteration 104, loss = 0.15357440\n",
      "Iteration 123, loss = 0.12969545\n",
      "Iteration 105, loss = 0.15291541\n",
      "Iteration 124, loss = 0.12834020\n",
      "Iteration 106, loss = 0.15233660\n",
      "Iteration 125, loss = 0.12718767\n",
      "Iteration 126, loss = 0.12607926\n",
      "Iteration 107, loss = 0.14901159\n",
      "Iteration 108, loss = 0.15006060\n",
      "Iteration 127, loss = 0.12570989\n",
      "Iteration 128, loss = 0.12430265\n",
      "Iteration 109, loss = 0.14768998\n",
      "Iteration 129, loss = 0.12406154\n",
      "Iteration 110, loss = 0.14645118\n",
      "Iteration 130, loss = 0.12370111\n",
      "Iteration 111, loss = 0.14401496\n",
      "Iteration 131, loss = 0.12244410\n",
      "Iteration 112, loss = 0.14449942\n",
      "Iteration 132, loss = 0.12065657\n",
      "Iteration 113, loss = 0.14178850\n",
      "Iteration 133, loss = 0.12122239\n",
      "Iteration 114, loss = 0.14163689\n",
      "Iteration 134, loss = 0.11864972\n",
      "Iteration 115, loss = 0.13933303\n",
      "Iteration 135, loss = 0.12073450\n",
      "Iteration 116, loss = 0.13849609\n",
      "Iteration 136, loss = 0.12279974\n",
      "Iteration 117, loss = 0.13728568\n",
      "Iteration 118, loss = 0.13597235\n",
      "Iteration 137, loss = 0.11631617\n",
      "Iteration 119, loss = 0.13520273\n",
      "Iteration 138, loss = 0.11878886\n",
      "Iteration 120, loss = 0.13445146\n",
      "Iteration 139, loss = 0.11659493\n",
      "Iteration 121, loss = 0.13639963\n",
      "Iteration 140, loss = 0.11361259\n",
      "Iteration 122, loss = 0.13244945\n",
      "Iteration 141, loss = 0.11584032\n",
      "Iteration 123, loss = 0.13244125\n",
      "Iteration 142, loss = 0.11460343\n",
      "Iteration 124, loss = 0.13017834\n",
      "Iteration 143, loss = 0.11239531\n",
      "Iteration 125, loss = 0.13117174\n",
      "Iteration 144, loss = 0.11331718\n",
      "Iteration 126, loss = 0.12707346\n",
      "Iteration 145, loss = 0.10975942\n",
      "Iteration 146, loss = 0.10962583Iteration 127, loss = 0.12745655\n",
      "\n",
      "Iteration 147, loss = 0.11024876\n",
      "Iteration 128, loss = 0.12542694\n",
      "Iteration 129, loss = 0.12603393\n",
      "Iteration 148, loss = 0.10830127\n",
      "Iteration 130, loss = 0.12372049\n",
      "Iteration 149, loss = 0.10713004\n",
      "Iteration 150, loss = 0.10579046\n",
      "Iteration 131, loss = 0.12288239\n",
      "Iteration 151, loss = 0.10502031\n",
      "Iteration 132, loss = 0.12162632\n",
      "Iteration 152, loss = 0.10566637\n",
      "Iteration 133, loss = 0.12084824\n",
      "Iteration 153, loss = 0.10375629\n",
      "Iteration 134, loss = 0.12021566\n",
      "Iteration 135, loss = 0.11926751\n",
      "Iteration 154, loss = 0.10364870\n",
      "Iteration 155, loss = 0.10197882\n",
      "Iteration 136, loss = 0.11802218\n",
      "Iteration 156, loss = 0.10171915\n",
      "Iteration 137, loss = 0.11669669\n",
      "Iteration 138, loss = 0.11527772\n",
      "Iteration 157, loss = 0.10253881\n",
      "Iteration 139, loss = 0.11486759\n",
      "Iteration 158, loss = 0.10013266\n",
      "Iteration 140, loss = 0.11437550\n",
      "Iteration 159, loss = 0.09960468\n",
      "Iteration 141, loss = 0.11382007\n",
      "Iteration 160, loss = 0.09832730\n",
      "Iteration 142, loss = 0.11259517\n",
      "Iteration 161, loss = 0.09798527\n",
      "Iteration 143, loss = 0.11176522\n",
      "Iteration 162, loss = 0.09680025\n",
      "Iteration 144, loss = 0.11074744\n",
      "Iteration 163, loss = 0.09611269\n",
      "Iteration 145, loss = 0.11064867\n",
      "Iteration 164, loss = 0.09723621\n",
      "Iteration 146, loss = 0.10997034\n",
      "Iteration 165, loss = 0.09646449\n",
      "Iteration 147, loss = 0.11014137\n",
      "Iteration 166, loss = 0.09385837\n",
      "Iteration 167, loss = 0.09500048\n",
      "Iteration 148, loss = 0.10795222\n",
      "Iteration 168, loss = 0.09180045\n",
      "Iteration 149, loss = 0.10655055\n",
      "Iteration 169, loss = 0.09357351\n",
      "Iteration 150, loss = 0.10673950\n",
      "Iteration 170, loss = 0.09177928\n",
      "Iteration 151, loss = 0.11089791\n",
      "Iteration 171, loss = 0.09093346\n",
      "Iteration 152, loss = 0.10625042\n",
      "Iteration 172, loss = 0.09026020\n",
      "Iteration 153, loss = 0.10500228\n",
      "Iteration 173, loss = 0.08928536\n",
      "Iteration 154, loss = 0.10410066\n",
      "Iteration 174, loss = 0.08874591\n",
      "Iteration 155, loss = 0.10397706\n",
      "Iteration 175, loss = 0.08866172\n",
      "Iteration 156, loss = 0.10278550\n",
      "Iteration 176, loss = 0.08891396\n",
      "Iteration 157, loss = 0.09982461\n",
      "Iteration 177, loss = 0.08786776\n",
      "Iteration 158, loss = 0.10175462\n",
      "Iteration 178, loss = 0.08854257\n",
      "Iteration 159, loss = 0.09838036\n",
      "Iteration 179, loss = 0.08701982\n",
      "Iteration 160, loss = 0.09903945\n",
      "Iteration 180, loss = 0.08908565\n",
      "Iteration 161, loss = 0.09935842\n",
      "Iteration 181, loss = 0.08648549\n",
      "Iteration 162, loss = 0.09554381\n",
      "Iteration 182, loss = 0.08457499\n",
      "Iteration 163, loss = 0.09863173\n",
      "Iteration 183, loss = 0.08575448\n",
      "Iteration 164, loss = 0.09842620\n",
      "Iteration 184, loss = 0.08354698\n",
      "Iteration 165, loss = 0.09553578\n",
      "Iteration 185, loss = 0.08268845\n",
      "Iteration 166, loss = 0.09780992\n",
      "Iteration 186, loss = 0.08247427\n",
      "Iteration 167, loss = 0.09365422\n",
      "Iteration 187, loss = 0.08386484\n",
      "Iteration 168, loss = 0.09345942\n",
      "Iteration 188, loss = 0.07979121\n",
      "Iteration 169, loss = 0.09175758\n",
      "Iteration 189, loss = 0.08088376\n",
      "Iteration 170, loss = 0.09066315\n",
      "Iteration 190, loss = 0.08101009\n",
      "Iteration 171, loss = 0.09095662\n",
      "Iteration 191, loss = 0.07920218\n",
      "Iteration 172, loss = 0.09013177\n",
      "Iteration 192, loss = 0.08067930\n",
      "Iteration 173, loss = 0.08982995\n",
      "Iteration 193, loss = 0.07791545\n",
      "Iteration 174, loss = 0.08862451\n",
      "Iteration 194, loss = 0.08496774\n",
      "Iteration 175, loss = 0.08754084\n",
      "Iteration 195, loss = 0.07924996\n",
      "Iteration 176, loss = 0.08755956\n",
      "Iteration 196, loss = 0.07854825\n",
      "Iteration 177, loss = 0.08653140\n",
      "Iteration 197, loss = 0.07841800\n",
      "Iteration 178, loss = 0.08586424\n",
      "Iteration 198, loss = 0.07828356\n",
      "Iteration 179, loss = 0.08530577\n",
      "Iteration 199, loss = 0.07647487\n",
      "Iteration 180, loss = 0.08555252\n",
      "Iteration 200, loss = 0.07524086\n",
      "Iteration 181, loss = 0.08641130\n",
      "Iteration 182, loss = 0.08282332\n",
      "Iteration 183, loss = 0.08488401\n",
      "Iteration 184, loss = 0.08337692\n",
      "Iteration 185, loss = 0.08216946\n",
      "Iteration 186, loss = 0.08248200\n",
      "Iteration 187, loss = 0.08170033\n",
      "Iteration 188, loss = 0.08076064\n",
      "Iteration 189, loss = 0.07922549\n",
      "Iteration 190, loss = 0.07820506\n",
      "Iteration 191, loss = 0.07905095\n",
      "Iteration 192, loss = 0.07796854\n",
      "Iteration 193, loss = 0.07794425\n",
      "Iteration 194, loss = 0.07871020\n",
      "Iteration 195, loss = 0.07594352\n",
      "Iteration 196, loss = 0.07657895\n",
      "Iteration 197, loss = 0.07621567\n",
      "Iteration 198, loss = 0.07797953\n",
      "Iteration 1, loss = 1.12793753\n",
      "Iteration 199, loss = 0.07570934\n",
      "Iteration 2, loss = 0.95454749\n",
      "Iteration 200, loss = 0.07419467\n",
      "Iteration 3, loss = 0.83235438\n",
      "Iteration 4, loss = 0.73861944\n",
      "Iteration 5, loss = 0.67173014\n",
      "Iteration 6, loss = 0.61953459\n",
      "Iteration 7, loss = 0.57888864\n",
      "Iteration 8, loss = 0.54624622\n",
      "Iteration 9, loss = 0.51905512\n",
      "Iteration 10, loss = 0.49726424\n",
      "Iteration 11, loss = 0.47764224\n",
      "Iteration 12, loss = 0.46205015\n",
      "Iteration 13, loss = 0.44801783\n",
      "Iteration 14, loss = 0.43465151\n",
      "Iteration 15, loss = 0.42280341\n",
      "Iteration 16, loss = 0.41239787\n",
      "Iteration 17, loss = 0.40213926\n",
      "Iteration 18, loss = 0.39318952\n",
      "Iteration 19, loss = 0.38546302\n",
      "Iteration 20, loss = 0.37722973\n",
      "Iteration 1, loss = 1.17081205\n",
      "Iteration 21, loss = 0.36930214\n",
      "Iteration 2, loss = 1.00719626\n",
      "Iteration 3, loss = 0.88885983\n",
      "Iteration 22, loss = 0.36319724\n",
      "Iteration 23, loss = 0.35716624\n",
      "Iteration 4, loss = 0.78999145\n",
      "Iteration 24, loss = 0.34993691\n",
      "Iteration 5, loss = 0.71268461\n",
      "Iteration 25, loss = 0.34499221\n",
      "Iteration 6, loss = 0.65073928\n",
      "Iteration 26, loss = 0.33934689\n",
      "Iteration 7, loss = 0.59998746\n",
      "Iteration 27, loss = 0.33340721\n",
      "Iteration 8, loss = 0.56043963\n",
      "Iteration 28, loss = 0.32847546\n",
      "Iteration 9, loss = 0.52873047\n",
      "Iteration 29, loss = 0.32388885\n",
      "Iteration 10, loss = 0.50090409\n",
      "Iteration 30, loss = 0.31861394\n",
      "Iteration 11, loss = 0.47742742\n",
      "Iteration 31, loss = 0.31631156\n",
      "Iteration 12, loss = 0.45846958\n",
      "Iteration 32, loss = 0.31064476\n",
      "Iteration 13, loss = 0.44123847\n",
      "Iteration 33, loss = 0.30901594\n",
      "Iteration 14, loss = 0.42597580\n",
      "Iteration 34, loss = 0.30161611\n",
      "Iteration 15, loss = 0.41285129\n",
      "Iteration 35, loss = 0.29911699\n",
      "Iteration 16, loss = 0.40053209\n",
      "Iteration 36, loss = 0.29528754\n",
      "Iteration 17, loss = 0.38903679\n",
      "Iteration 37, loss = 0.29079258\n",
      "Iteration 18, loss = 0.37863456\n",
      "Iteration 38, loss = 0.28746906\n",
      "Iteration 19, loss = 0.36898938\n",
      "Iteration 39, loss = 0.28335070\n",
      "Iteration 20, loss = 0.36035172\n",
      "Iteration 40, loss = 0.28010878\n",
      "Iteration 21, loss = 0.35153831\n",
      "Iteration 41, loss = 0.27753762\n",
      "Iteration 22, loss = 0.34414729\n",
      "Iteration 42, loss = 0.27357822\n",
      "Iteration 23, loss = 0.33817048\n",
      "Iteration 43, loss = 0.27263008\n",
      "Iteration 24, loss = 0.33040785\n",
      "Iteration 44, loss = 0.26835086\n",
      "Iteration 25, loss = 0.32363541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.26592319\n",
      "Iteration 26, loss = 0.31697423\n",
      "Iteration 46, loss = 0.26283223\n",
      "Iteration 27, loss = 0.31061896\n",
      "Iteration 47, loss = 0.25975115\n",
      "Iteration 28, loss = 0.30531296\n",
      "Iteration 48, loss = 0.25770947\n",
      "Iteration 29, loss = 0.30004867\n",
      "Iteration 49, loss = 0.25500155\n",
      "Iteration 30, loss = 0.29484975\n",
      "Iteration 50, loss = 0.25192286\n",
      "Iteration 31, loss = 0.29011917\n",
      "Iteration 51, loss = 0.25113236\n",
      "Iteration 32, loss = 0.28509647\n",
      "Iteration 52, loss = 0.24636910\n",
      "Iteration 33, loss = 0.28013380\n",
      "Iteration 53, loss = 0.24560758\n",
      "Iteration 34, loss = 0.27582320\n",
      "Iteration 54, loss = 0.24409143\n",
      "Iteration 35, loss = 0.27196844\n",
      "Iteration 36, loss = 0.26804767\n",
      "Iteration 55, loss = 0.23961660\n",
      "Iteration 37, loss = 0.26366437\n",
      "Iteration 56, loss = 0.23819712\n",
      "Iteration 38, loss = 0.26024892\n",
      "Iteration 57, loss = 0.23519299\n",
      "Iteration 39, loss = 0.25586926\n",
      "Iteration 58, loss = 0.23238822\n",
      "Iteration 40, loss = 0.25299506\n",
      "Iteration 59, loss = 0.23070211\n",
      "Iteration 41, loss = 0.24885915\n",
      "Iteration 60, loss = 0.22829663\n",
      "Iteration 42, loss = 0.24571554\n",
      "Iteration 61, loss = 0.22577673\n",
      "Iteration 43, loss = 0.24212481\n",
      "Iteration 62, loss = 0.22386266\n",
      "Iteration 44, loss = 0.23924047\n",
      "Iteration 63, loss = 0.22178891\n",
      "Iteration 45, loss = 0.23572178\n",
      "Iteration 64, loss = 0.21980400\n",
      "Iteration 46, loss = 0.23248356\n",
      "Iteration 65, loss = 0.21740936\n",
      "Iteration 47, loss = 0.22973807\n",
      "Iteration 66, loss = 0.21681245\n",
      "Iteration 48, loss = 0.22649194\n",
      "Iteration 67, loss = 0.21504007\n",
      "Iteration 49, loss = 0.22334734\n",
      "Iteration 68, loss = 0.21266402\n",
      "Iteration 50, loss = 0.22139852\n",
      "Iteration 69, loss = 0.20967714\n",
      "Iteration 51, loss = 0.21807345\n",
      "Iteration 70, loss = 0.20873489\n",
      "Iteration 52, loss = 0.21601201\n",
      "Iteration 71, loss = 0.20582783\n",
      "Iteration 53, loss = 0.21260897\n",
      "Iteration 72, loss = 0.20407413\n",
      "Iteration 54, loss = 0.20985524\n",
      "Iteration 73, loss = 0.20278235\n",
      "Iteration 55, loss = 0.20767832\n",
      "Iteration 74, loss = 0.20049961\n",
      "Iteration 56, loss = 0.20405328\n",
      "Iteration 75, loss = 0.19900706\n",
      "Iteration 57, loss = 0.20205827\n",
      "Iteration 76, loss = 0.19690368\n",
      "Iteration 58, loss = 0.19897138\n",
      "Iteration 77, loss = 0.19572275\n",
      "Iteration 59, loss = 0.19659645\n",
      "Iteration 78, loss = 0.19491701\n",
      "Iteration 60, loss = 0.19400884\n",
      "Iteration 79, loss = 0.19199200\n",
      "Iteration 61, loss = 0.19154448\n",
      "Iteration 80, loss = 0.18994895\n",
      "Iteration 62, loss = 0.18991941\n",
      "Iteration 81, loss = 0.18853117\n",
      "Iteration 63, loss = 0.18705877\n",
      "Iteration 82, loss = 0.18699132\n",
      "Iteration 64, loss = 0.18552870\n",
      "Iteration 83, loss = 0.18523805\n",
      "Iteration 65, loss = 0.18304834\n",
      "Iteration 84, loss = 0.18363297\n",
      "Iteration 66, loss = 0.18083495\n",
      "Iteration 85, loss = 0.18291284\n",
      "Iteration 67, loss = 0.17817777\n",
      "Iteration 86, loss = 0.18141477\n",
      "Iteration 68, loss = 0.17619697\n",
      "Iteration 87, loss = 0.18041146\n",
      "Iteration 69, loss = 0.17459827\n",
      "Iteration 88, loss = 0.17689311\n",
      "Iteration 70, loss = 0.17155751\n",
      "Iteration 89, loss = 0.17496707\n",
      "Iteration 71, loss = 0.16987174\n",
      "Iteration 90, loss = 0.17469946\n",
      "Iteration 72, loss = 0.16763150\n",
      "Iteration 91, loss = 0.17187805\n",
      "Iteration 73, loss = 0.16476970\n",
      "Iteration 92, loss = 0.17066017\n",
      "Iteration 74, loss = 0.16360366\n",
      "Iteration 93, loss = 0.16805373\n",
      "Iteration 75, loss = 0.16084397\n",
      "Iteration 94, loss = 0.16784467\n",
      "Iteration 76, loss = 0.15983259\n",
      "Iteration 95, loss = 0.16535774\n",
      "Iteration 77, loss = 0.15646990\n",
      "Iteration 96, loss = 0.16380576\n",
      "Iteration 78, loss = 0.15565691\n",
      "Iteration 97, loss = 0.16297894\n",
      "Iteration 79, loss = 0.15305462\n",
      "Iteration 98, loss = 0.16018730\n",
      "Iteration 80, loss = 0.15161038\n",
      "Iteration 99, loss = 0.15999714\n",
      "Iteration 81, loss = 0.15060559\n",
      "Iteration 100, loss = 0.15988333\n",
      "Iteration 82, loss = 0.14747171\n",
      "Iteration 101, loss = 0.15671486\n",
      "Iteration 83, loss = 0.14548915\n",
      "Iteration 102, loss = 0.15594192\n",
      "Iteration 84, loss = 0.14476388\n",
      "Iteration 103, loss = 0.15385858\n",
      "Iteration 85, loss = 0.14299145\n",
      "Iteration 104, loss = 0.15288187\n",
      "Iteration 86, loss = 0.13983495\n",
      "Iteration 105, loss = 0.15162175\n",
      "Iteration 87, loss = 0.14090446\n",
      "Iteration 106, loss = 0.15017258\n",
      "Iteration 88, loss = 0.13717264\n",
      "Iteration 107, loss = 0.14952367\n",
      "Iteration 89, loss = 0.13691106\n",
      "Iteration 108, loss = 0.14760517\n",
      "Iteration 90, loss = 0.13346764\n",
      "Iteration 109, loss = 0.14695882\n",
      "Iteration 91, loss = 0.13307185\n",
      "Iteration 110, loss = 0.14605208\n",
      "Iteration 92, loss = 0.12954286\n",
      "Iteration 111, loss = 0.14542085\n",
      "Iteration 93, loss = 0.12940374\n",
      "Iteration 112, loss = 0.14526940\n",
      "Iteration 94, loss = 0.12970098\n",
      "Iteration 113, loss = 0.14099992\n",
      "Iteration 95, loss = 0.12893716\n",
      "Iteration 114, loss = 0.14284854\n",
      "Iteration 96, loss = 0.12620284\n",
      "Iteration 115, loss = 0.14089833\n",
      "Iteration 97, loss = 0.12270822\n",
      "Iteration 116, loss = 0.13861340\n",
      "Iteration 98, loss = 0.12184622\n",
      "Iteration 117, loss = 0.13877689\n",
      "Iteration 99, loss = 0.12234113\n",
      "Iteration 118, loss = 0.13761795\n",
      "Iteration 100, loss = 0.12184811\n",
      "Iteration 119, loss = 0.13612533\n",
      "Iteration 120, loss = 0.13480926\n",
      "Iteration 101, loss = 0.11890454\n",
      "Iteration 121, loss = 0.13378018\n",
      "Iteration 102, loss = 0.11675153\n",
      "Iteration 122, loss = 0.13369193\n",
      "Iteration 103, loss = 0.11547947\n",
      "Iteration 123, loss = 0.13117838\n",
      "Iteration 104, loss = 0.11488041\n",
      "Iteration 124, loss = 0.13100680\n",
      "Iteration 105, loss = 0.11692305\n",
      "Iteration 125, loss = 0.12986071\n",
      "Iteration 106, loss = 0.11495384\n",
      "Iteration 126, loss = 0.12841224\n",
      "Iteration 127, loss = 0.12779990\n",
      "Iteration 107, loss = 0.11131250\n",
      "Iteration 108, loss = 0.10980193\n",
      "Iteration 128, loss = 0.12820579\n",
      "Iteration 109, loss = 0.10799868\n",
      "Iteration 129, loss = 0.12468568\n",
      "Iteration 130, loss = 0.12508423\n",
      "Iteration 110, loss = 0.10696441\n",
      "Iteration 131, loss = 0.12355914\n",
      "Iteration 132, loss = 0.12369044\n",
      "Iteration 111, loss = 0.10555144\n",
      "Iteration 133, loss = 0.12088109\n",
      "Iteration 112, loss = 0.10492815\n",
      "Iteration 134, loss = 0.12189196\n",
      "Iteration 113, loss = 0.10521313\n",
      "Iteration 135, loss = 0.11948187\n",
      "Iteration 114, loss = 0.10299048\n",
      "Iteration 136, loss = 0.12073661\n",
      "Iteration 115, loss = 0.10294790\n",
      "Iteration 137, loss = 0.11851582\n",
      "Iteration 116, loss = 0.10100627\n",
      "Iteration 138, loss = 0.11773515\n",
      "Iteration 117, loss = 0.10026768\n",
      "Iteration 139, loss = 0.11836846\n",
      "Iteration 118, loss = 0.09797607\n",
      "Iteration 119, loss = 0.09631499\n",
      "Iteration 120, loss = 0.09651984\n",
      "Iteration 121, loss = 0.09410117\n",
      "Iteration 122, loss = 0.09575104\n",
      "Iteration 140, loss = 0.11643635\n",
      "Iteration 123, loss = 0.09371206\n",
      "Iteration 141, loss = 0.11627882\n",
      "Iteration 124, loss = 0.09257428\n",
      "Iteration 142, loss = 0.11524826\n",
      "Iteration 125, loss = 0.09143451\n",
      "Iteration 126, loss = 0.09197343\n",
      "Iteration 127, loss = 0.09342368\n",
      "Iteration 143, loss = 0.11390238\n",
      "Iteration 144, loss = 0.11516343\n",
      "Iteration 128, loss = 0.09206782\n",
      "Iteration 145, loss = 0.11328832\n",
      "Iteration 129, loss = 0.08958746\n",
      "Iteration 146, loss = 0.11388666\n",
      "Iteration 130, loss = 0.08875711\n",
      "Iteration 147, loss = 0.11076956\n",
      "Iteration 131, loss = 0.09028153\n",
      "Iteration 148, loss = 0.11421952\n",
      "Iteration 132, loss = 0.08444564\n",
      "Iteration 149, loss = 0.11099553\n",
      "Iteration 133, loss = 0.08658508\n",
      "Iteration 150, loss = 0.10806547\n",
      "Iteration 134, loss = 0.08393215\n",
      "Iteration 151, loss = 0.11007428\n",
      "Iteration 135, loss = 0.08316828\n",
      "Iteration 152, loss = 0.10793366\n",
      "Iteration 136, loss = 0.08191479\n",
      "Iteration 153, loss = 0.10805032\n",
      "Iteration 154, loss = 0.10475057\n",
      "Iteration 137, loss = 0.08163499\n",
      "Iteration 155, loss = 0.10722996\n",
      "Iteration 138, loss = 0.07973776\n",
      "Iteration 156, loss = 0.10645161\n",
      "Iteration 157, loss = 0.10808857\n",
      "Iteration 139, loss = 0.07929080\n",
      "Iteration 158, loss = 0.10255058\n",
      "Iteration 140, loss = 0.07946150\n",
      "Iteration 159, loss = 0.10322845\n",
      "Iteration 141, loss = 0.07764678\n",
      "Iteration 160, loss = 0.10267034\n",
      "Iteration 142, loss = 0.07772029\n",
      "Iteration 161, loss = 0.10174989\n",
      "Iteration 143, loss = 0.07644381\n",
      "Iteration 162, loss = 0.09989996\n",
      "Iteration 144, loss = 0.07711577\n",
      "Iteration 163, loss = 0.09931729\n",
      "Iteration 145, loss = 0.07511514\n",
      "Iteration 164, loss = 0.09915074\n",
      "Iteration 146, loss = 0.07450634\n",
      "Iteration 165, loss = 0.09744990\n",
      "Iteration 147, loss = 0.07489623\n",
      "Iteration 166, loss = 0.09817321\n",
      "Iteration 148, loss = 0.07434415\n",
      "Iteration 167, loss = 0.09853894\n",
      "Iteration 149, loss = 0.07336906\n",
      "Iteration 168, loss = 0.09598561\n",
      "Iteration 150, loss = 0.07141870\n",
      "Iteration 169, loss = 0.09671207\n",
      "Iteration 151, loss = 0.07120129\n",
      "Iteration 152, loss = 0.07003687\n",
      "Iteration 170, loss = 0.09640413\n",
      "Iteration 153, loss = 0.07152835\n",
      "Iteration 171, loss = 0.09298770\n",
      "Iteration 154, loss = 0.06945892\n",
      "Iteration 172, loss = 0.09685934\n",
      "Iteration 173, loss = 0.09169520\n",
      "Iteration 155, loss = 0.06836813\n",
      "Iteration 174, loss = 0.09381430\n",
      "Iteration 156, loss = 0.06836830\n",
      "Iteration 175, loss = 0.09461362\n",
      "Iteration 157, loss = 0.06655123\n",
      "Iteration 176, loss = 0.09194555\n",
      "Iteration 158, loss = 0.06670592\n",
      "Iteration 177, loss = 0.09172455\n",
      "Iteration 159, loss = 0.06552983\n",
      "Iteration 178, loss = 0.09060925\n",
      "Iteration 160, loss = 0.06567763\n",
      "Iteration 179, loss = 0.09061050\n",
      "Iteration 161, loss = 0.06628114\n",
      "Iteration 180, loss = 0.08908960\n",
      "Iteration 162, loss = 0.06403536\n",
      "Iteration 163, loss = 0.06591735\n",
      "Iteration 181, loss = 0.08858839\n",
      "Iteration 182, loss = 0.08746201\n",
      "Iteration 164, loss = 0.06410645\n",
      "Iteration 165, loss = 0.06347718\n",
      "Iteration 183, loss = 0.08732753\n",
      "Iteration 166, loss = 0.06280318\n",
      "Iteration 184, loss = 0.08592130\n",
      "Iteration 185, loss = 0.08679832\n",
      "Iteration 167, loss = 0.06171635\n",
      "Iteration 186, loss = 0.08519943\n",
      "Iteration 168, loss = 0.06010492\n",
      "Iteration 187, loss = 0.08509422\n",
      "Iteration 169, loss = 0.06058861\n",
      "Iteration 188, loss = 0.08389362\n",
      "Iteration 170, loss = 0.05940900\n",
      "Iteration 171, loss = 0.05962391\n",
      "Iteration 189, loss = 0.08695311\n",
      "Iteration 172, loss = 0.05982489\n",
      "Iteration 190, loss = 0.08419360\n",
      "Iteration 173, loss = 0.05864376\n",
      "Iteration 191, loss = 0.08438601\n",
      "Iteration 174, loss = 0.05685924Iteration 192, loss = 0.08383220\n",
      "\n",
      "Iteration 193, loss = 0.08327389\n",
      "Iteration 175, loss = 0.05857038\n",
      "Iteration 194, loss = 0.08086859\n",
      "Iteration 176, loss = 0.05725809\n",
      "Iteration 195, loss = 0.08363794\n",
      "Iteration 177, loss = 0.05738646\n",
      "Iteration 196, loss = 0.08298499\n",
      "Iteration 178, loss = 0.05536016\n",
      "Iteration 197, loss = 0.07963936\n",
      "Iteration 179, loss = 0.05563291\n",
      "Iteration 180, loss = 0.05864629\n",
      "Iteration 198, loss = 0.08241533\n",
      "Iteration 181, loss = 0.05582032\n",
      "Iteration 199, loss = 0.07860036\n",
      "Iteration 182, loss = 0.05478699\n",
      "Iteration 200, loss = 0.07877100\n",
      "Iteration 183, loss = 0.05321341\n",
      "Iteration 184, loss = 0.05282212\n",
      "Iteration 185, loss = 0.05180796\n",
      "Iteration 186, loss = 0.05197384\n",
      "Iteration 187, loss = 0.05080699\n",
      "Iteration 188, loss = 0.05044569\n",
      "Iteration 189, loss = 0.05004828\n",
      "Iteration 190, loss = 0.04982696\n",
      "Iteration 191, loss = 0.04919561\n",
      "Iteration 192, loss = 0.04794344\n",
      "Iteration 193, loss = 0.04825541\n",
      "Iteration 194, loss = 0.04749655\n",
      "Iteration 195, loss = 0.04728883\n",
      "Iteration 196, loss = 0.04662934\n",
      "Iteration 197, loss = 0.04642978\n",
      "Iteration 198, loss = 0.04635135\n",
      "Iteration 199, loss = 0.04611481\n",
      "Iteration 200, loss = 0.04517529\n",
      "Iteration 1, loss = 1.35732408\n",
      "Iteration 2, loss = 1.19154607\n",
      "Iteration 3, loss = 1.06170915\n",
      "Iteration 4, loss = 0.94948974\n",
      "Iteration 5, loss = 0.85591633\n",
      "Iteration 6, loss = 0.77797270\n",
      "Iteration 7, loss = 0.71130423\n",
      "Iteration 8, loss = 0.65518147\n",
      "Iteration 9, loss = 0.61066527\n",
      "Iteration 10, loss = 0.57290577\n",
      "Iteration 11, loss = 0.53996206\n",
      "Iteration 12, loss = 0.51399461\n",
      "Iteration 13, loss = 0.49060889\n",
      "Iteration 14, loss = 0.46941675\n",
      "Iteration 15, loss = 0.45099083\n",
      "Iteration 16, loss = 0.43546879\n",
      "Iteration 17, loss = 0.41998660\n",
      "Iteration 18, loss = 0.40800322\n",
      "Iteration 1, loss = 1.18340779\n",
      "Iteration 19, loss = 0.39530874\n",
      "Iteration 2, loss = 0.98057024\n",
      "Iteration 20, loss = 0.38482968\n",
      "Iteration 3, loss = 0.86159344\n",
      "Iteration 21, loss = 0.37459480\n",
      "Iteration 4, loss = 0.76847743\n",
      "Iteration 22, loss = 0.36479322\n",
      "Iteration 5, loss = 0.69335745\n",
      "Iteration 23, loss = 0.35702708\n",
      "Iteration 6, loss = 0.63950445\n",
      "Iteration 24, loss = 0.34796889\n",
      "Iteration 7, loss = 0.59625122\n",
      "Iteration 25, loss = 0.34205433\n",
      "Iteration 8, loss = 0.55928810\n",
      "Iteration 26, loss = 0.33367002\n",
      "Iteration 9, loss = 0.52992786\n",
      "Iteration 27, loss = 0.32793452\n",
      "Iteration 10, loss = 0.50664783\n",
      "Iteration 28, loss = 0.32155449\n",
      "Iteration 11, loss = 0.48512247\n",
      "Iteration 29, loss = 0.31509742\n",
      "Iteration 12, loss = 0.46662777\n",
      "Iteration 30, loss = 0.30980345\n",
      "Iteration 13, loss = 0.45078472\n",
      "Iteration 31, loss = 0.30453965\n",
      "Iteration 14, loss = 0.43653384\n",
      "Iteration 32, loss = 0.29856228\n",
      "Iteration 15, loss = 0.42434606\n",
      "Iteration 33, loss = 0.29318430\n",
      "Iteration 16, loss = 0.41415096\n",
      "Iteration 34, loss = 0.28883548\n",
      "Iteration 17, loss = 0.40227184\n",
      "Iteration 35, loss = 0.28383009\n",
      "Iteration 18, loss = 0.39387749\n",
      "Iteration 36, loss = 0.27998287\n",
      "Iteration 19, loss = 0.38509181\n",
      "Iteration 37, loss = 0.27503693\n",
      "Iteration 20, loss = 0.37653457\n",
      "Iteration 38, loss = 0.27246076\n",
      "Iteration 21, loss = 0.36932461\n",
      "Iteration 39, loss = 0.26749686\n",
      "Iteration 22, loss = 0.36241253\n",
      "Iteration 40, loss = 0.26392553\n",
      "Iteration 23, loss = 0.35528706\n",
      "Iteration 41, loss = 0.26023459\n",
      "Iteration 24, loss = 0.34971177\n",
      "Iteration 42, loss = 0.25687796\n",
      "Iteration 25, loss = 0.34311820\n",
      "Iteration 43, loss = 0.25378461\n",
      "Iteration 26, loss = 0.33762236\n",
      "Iteration 44, loss = 0.25009713\n",
      "Iteration 27, loss = 0.33224973\n",
      "Iteration 45, loss = 0.24643446\n",
      "Iteration 28, loss = 0.32773007\n",
      "Iteration 46, loss = 0.24355491\n",
      "Iteration 29, loss = 0.32384449\n",
      "Iteration 47, loss = 0.23976612\n",
      "Iteration 30, loss = 0.31767810\n",
      "Iteration 48, loss = 0.23776757\n",
      "Iteration 31, loss = 0.31414995\n",
      "Iteration 49, loss = 0.23406955\n",
      "Iteration 32, loss = 0.30895642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.23118732\n",
      "Iteration 33, loss = 0.30432746\n",
      "Iteration 51, loss = 0.22831650\n",
      "Iteration 34, loss = 0.30066548\n",
      "Iteration 52, loss = 0.22522617\n",
      "Iteration 35, loss = 0.29632724\n",
      "Iteration 53, loss = 0.22366427\n",
      "Iteration 36, loss = 0.29338633\n",
      "Iteration 54, loss = 0.22082574\n",
      "Iteration 37, loss = 0.28895344\n",
      "Iteration 55, loss = 0.21722952\n",
      "Iteration 38, loss = 0.28533518\n",
      "Iteration 56, loss = 0.21603670\n",
      "Iteration 39, loss = 0.28233869\n",
      "Iteration 57, loss = 0.21396251\n",
      "Iteration 40, loss = 0.27890389\n",
      "Iteration 58, loss = 0.21065647\n",
      "Iteration 41, loss = 0.27548848\n",
      "Iteration 59, loss = 0.20813376\n",
      "Iteration 42, loss = 0.27168987\n",
      "Iteration 60, loss = 0.20559382\n",
      "Iteration 43, loss = 0.26934836\n",
      "Iteration 61, loss = 0.20356620\n",
      "Iteration 44, loss = 0.26578717\n",
      "Iteration 62, loss = 0.20104577\n",
      "Iteration 45, loss = 0.26383708\n",
      "Iteration 63, loss = 0.19903260\n",
      "Iteration 46, loss = 0.26071611\n",
      "Iteration 64, loss = 0.19693036\n",
      "Iteration 47, loss = 0.25790648\n",
      "Iteration 65, loss = 0.19502800\n",
      "Iteration 48, loss = 0.25411940\n",
      "Iteration 66, loss = 0.19222551\n",
      "Iteration 49, loss = 0.25164600\n",
      "Iteration 67, loss = 0.19042790\n",
      "Iteration 50, loss = 0.24880900\n",
      "Iteration 68, loss = 0.18919553\n",
      "Iteration 51, loss = 0.24560742\n",
      "Iteration 69, loss = 0.18934133\n",
      "Iteration 52, loss = 0.24349824\n",
      "Iteration 70, loss = 0.18475583\n",
      "Iteration 53, loss = 0.24055002\n",
      "Iteration 71, loss = 0.18472914\n",
      "Iteration 54, loss = 0.23781249\n",
      "Iteration 72, loss = 0.18173242\n",
      "Iteration 55, loss = 0.23559307\n",
      "Iteration 73, loss = 0.17990800\n",
      "Iteration 56, loss = 0.23338294\n",
      "Iteration 74, loss = 0.17739299\n",
      "Iteration 57, loss = 0.23016131\n",
      "Iteration 75, loss = 0.17727336\n",
      "Iteration 58, loss = 0.22876947\n",
      "Iteration 76, loss = 0.17384583\n",
      "Iteration 59, loss = 0.22601779\n",
      "Iteration 77, loss = 0.17402589\n",
      "Iteration 60, loss = 0.22431074\n",
      "Iteration 78, loss = 0.17087802\n",
      "Iteration 61, loss = 0.22087775\n",
      "Iteration 79, loss = 0.17091133\n",
      "Iteration 62, loss = 0.22025074\n",
      "Iteration 80, loss = 0.16866733\n",
      "Iteration 63, loss = 0.21722819\n",
      "Iteration 81, loss = 0.16560895\n",
      "Iteration 64, loss = 0.21501490\n",
      "Iteration 82, loss = 0.16563135\n",
      "Iteration 65, loss = 0.21353161\n",
      "Iteration 83, loss = 0.16273611\n",
      "Iteration 66, loss = 0.21103350\n",
      "Iteration 84, loss = 0.16223234\n",
      "Iteration 67, loss = 0.20863679\n",
      "Iteration 85, loss = 0.16013204\n",
      "Iteration 68, loss = 0.20653305\n",
      "Iteration 86, loss = 0.15939415\n",
      "Iteration 69, loss = 0.20399680\n",
      "Iteration 87, loss = 0.15737408\n",
      "Iteration 70, loss = 0.20378109\n",
      "Iteration 88, loss = 0.15593743\n",
      "Iteration 71, loss = 0.20092304\n",
      "Iteration 89, loss = 0.15476111\n",
      "Iteration 72, loss = 0.19931092\n",
      "Iteration 90, loss = 0.15277737\n",
      "Iteration 73, loss = 0.19631165\n",
      "Iteration 91, loss = 0.15154695\n",
      "Iteration 74, loss = 0.19494283\n",
      "Iteration 92, loss = 0.15047132\n",
      "Iteration 75, loss = 0.19281658\n",
      "Iteration 93, loss = 0.14891806\n",
      "Iteration 76, loss = 0.19072648\n",
      "Iteration 94, loss = 0.14768778\n",
      "Iteration 77, loss = 0.18920585\n",
      "Iteration 95, loss = 0.14731803\n",
      "Iteration 78, loss = 0.18688131\n",
      "Iteration 96, loss = 0.14564098\n",
      "Iteration 79, loss = 0.18563662\n",
      "Iteration 97, loss = 0.14416621\n",
      "Iteration 80, loss = 0.18616646\n",
      "Iteration 98, loss = 0.14319099\n",
      "Iteration 81, loss = 0.18319116\n",
      "Iteration 99, loss = 0.14182678\n",
      "Iteration 82, loss = 0.17942137\n",
      "Iteration 100, loss = 0.14128358\n",
      "Iteration 83, loss = 0.17860707\n",
      "Iteration 101, loss = 0.13946283\n",
      "Iteration 84, loss = 0.17863112\n",
      "Iteration 102, loss = 0.13990349\n",
      "Iteration 85, loss = 0.17328313\n",
      "Iteration 103, loss = 0.13741391\n",
      "Iteration 86, loss = 0.17342176\n",
      "Iteration 104, loss = 0.13618061\n",
      "Iteration 87, loss = 0.17084211\n",
      "Iteration 105, loss = 0.13598645\n",
      "Iteration 88, loss = 0.17133887\n",
      "Iteration 106, loss = 0.13417528\n",
      "Iteration 89, loss = 0.16861847\n",
      "Iteration 107, loss = 0.13423617\n",
      "Iteration 90, loss = 0.16809107\n",
      "Iteration 108, loss = 0.13284168\n",
      "Iteration 91, loss = 0.16428394\n",
      "Iteration 109, loss = 0.13247660\n",
      "Iteration 92, loss = 0.16349111\n",
      "Iteration 110, loss = 0.13018667\n",
      "Iteration 93, loss = 0.16265614\n",
      "Iteration 111, loss = 0.12980724\n",
      "Iteration 94, loss = 0.16061756\n",
      "Iteration 112, loss = 0.12786298\n",
      "Iteration 113, loss = 0.12732901\n",
      "Iteration 95, loss = 0.15804973\n",
      "Iteration 114, loss = 0.12686858\n",
      "Iteration 96, loss = 0.15615799\n",
      "Iteration 115, loss = 0.12624411\n",
      "Iteration 97, loss = 0.15553559\n",
      "Iteration 116, loss = 0.12507472\n",
      "Iteration 98, loss = 0.15377559\n",
      "Iteration 117, loss = 0.12371529\n",
      "Iteration 99, loss = 0.15154757\n",
      "Iteration 118, loss = 0.12340198\n",
      "Iteration 100, loss = 0.15084091\n",
      "Iteration 101, loss = 0.15065145\n",
      "Iteration 119, loss = 0.12205306\n",
      "Iteration 102, loss = 0.14727614\n",
      "Iteration 120, loss = 0.12201168\n",
      "Iteration 103, loss = 0.14828506\n",
      "Iteration 121, loss = 0.11980547\n",
      "Iteration 122, loss = 0.11951954\n",
      "Iteration 104, loss = 0.14567124\n",
      "Iteration 123, loss = 0.11876635\n",
      "Iteration 105, loss = 0.14390784\n",
      "Iteration 124, loss = 0.11729438\n",
      "Iteration 106, loss = 0.14616908\n",
      "Iteration 107, loss = 0.14181997\n",
      "Iteration 125, loss = 0.11730382\n",
      "Iteration 108, loss = 0.14078524\n",
      "Iteration 126, loss = 0.11680351\n",
      "Iteration 109, loss = 0.14086437\n",
      "Iteration 127, loss = 0.11540481\n",
      "Iteration 128, loss = 0.11404242\n",
      "Iteration 110, loss = 0.13976916\n",
      "Iteration 129, loss = 0.11574119\n",
      "Iteration 111, loss = 0.13744192\n",
      "Iteration 130, loss = 0.11346943\n",
      "Iteration 112, loss = 0.13779954\n",
      "Iteration 131, loss = 0.11331695\n",
      "Iteration 113, loss = 0.13686233\n",
      "Iteration 132, loss = 0.11166369\n",
      "Iteration 114, loss = 0.13233840\n",
      "Iteration 133, loss = 0.11043801\n",
      "Iteration 115, loss = 0.13308414\n",
      "Iteration 134, loss = 0.10985533\n",
      "Iteration 116, loss = 0.13101112\n",
      "Iteration 135, loss = 0.10984159\n",
      "Iteration 117, loss = 0.12985292\n",
      "Iteration 136, loss = 0.11257748\n",
      "Iteration 118, loss = 0.12853114\n",
      "Iteration 137, loss = 0.11270446\n",
      "Iteration 119, loss = 0.12831232\n",
      "Iteration 138, loss = 0.10696571\n",
      "Iteration 120, loss = 0.12660518\n",
      "Iteration 139, loss = 0.10879385\n",
      "Iteration 121, loss = 0.12900428\n",
      "Iteration 140, loss = 0.10647799\n",
      "Iteration 122, loss = 0.12568613\n",
      "Iteration 141, loss = 0.10480078\n",
      "Iteration 123, loss = 0.12367066\n",
      "Iteration 142, loss = 0.10498685\n",
      "Iteration 124, loss = 0.12320700\n",
      "Iteration 143, loss = 0.10446431\n",
      "Iteration 125, loss = 0.12291304\n",
      "Iteration 144, loss = 0.10310941\n",
      "Iteration 126, loss = 0.11941992\n",
      "Iteration 145, loss = 0.10280126\n",
      "Iteration 127, loss = 0.12022659\n",
      "Iteration 146, loss = 0.10352270\n",
      "Iteration 128, loss = 0.12198295\n",
      "Iteration 147, loss = 0.10178167\n",
      "Iteration 129, loss = 0.11617717\n",
      "Iteration 148, loss = 0.10244714\n",
      "Iteration 130, loss = 0.11870745\n",
      "Iteration 149, loss = 0.10033015\n",
      "Iteration 131, loss = 0.11643037\n",
      "Iteration 150, loss = 0.09966584\n",
      "Iteration 132, loss = 0.11534532\n",
      "Iteration 151, loss = 0.09991897\n",
      "Iteration 133, loss = 0.11369104\n",
      "Iteration 152, loss = 0.09866290\n",
      "Iteration 134, loss = 0.11411483\n",
      "Iteration 153, loss = 0.09840311\n",
      "Iteration 135, loss = 0.11168878\n",
      "Iteration 154, loss = 0.09667175\n",
      "Iteration 136, loss = 0.11304035\n",
      "Iteration 155, loss = 0.09599064\n",
      "Iteration 137, loss = 0.11296363\n",
      "Iteration 156, loss = 0.09549197\n",
      "Iteration 138, loss = 0.11549527\n",
      "Iteration 157, loss = 0.09496599\n",
      "Iteration 139, loss = 0.10879477\n",
      "Iteration 158, loss = 0.09422110\n",
      "Iteration 140, loss = 0.11021463\n",
      "Iteration 159, loss = 0.09382457\n",
      "Iteration 141, loss = 0.11027866\n",
      "Iteration 160, loss = 0.09243998\n",
      "Iteration 142, loss = 0.10784070\n",
      "Iteration 161, loss = 0.09299081\n",
      "Iteration 143, loss = 0.10721693\n",
      "Iteration 162, loss = 0.09154159\n",
      "Iteration 144, loss = 0.10620964\n",
      "Iteration 163, loss = 0.09105078\n",
      "Iteration 145, loss = 0.10499903\n",
      "Iteration 164, loss = 0.09037634\n",
      "Iteration 146, loss = 0.10703909\n",
      "Iteration 165, loss = 0.08936385\n",
      "Iteration 147, loss = 0.10272911\n",
      "Iteration 166, loss = 0.08905608\n",
      "Iteration 148, loss = 0.10320125\n",
      "Iteration 167, loss = 0.08926473\n",
      "Iteration 149, loss = 0.10171689\n",
      "Iteration 168, loss = 0.08812614\n",
      "Iteration 150, loss = 0.09959490\n",
      "Iteration 169, loss = 0.08711686\n",
      "Iteration 151, loss = 0.09972093\n",
      "Iteration 170, loss = 0.08920457\n",
      "Iteration 152, loss = 0.09904602\n",
      "Iteration 171, loss = 0.08669429\n",
      "Iteration 153, loss = 0.09836503\n",
      "Iteration 172, loss = 0.08748834\n",
      "Iteration 154, loss = 0.09851348\n",
      "Iteration 173, loss = 0.08747505\n",
      "Iteration 155, loss = 0.09640203\n",
      "Iteration 174, loss = 0.08460861\n",
      "Iteration 156, loss = 0.09707246\n",
      "Iteration 175, loss = 0.08467473\n",
      "Iteration 157, loss = 0.09833358\n",
      "Iteration 176, loss = 0.08357596\n",
      "Iteration 158, loss = 0.09383863\n",
      "Iteration 177, loss = 0.08435142\n",
      "Iteration 159, loss = 0.09519633\n",
      "Iteration 178, loss = 0.08330865\n",
      "Iteration 160, loss = 0.09275640\n",
      "Iteration 179, loss = 0.08231887\n",
      "Iteration 161, loss = 0.09421212\n",
      "Iteration 180, loss = 0.08265590\n",
      "Iteration 162, loss = 0.09442541\n",
      "Iteration 181, loss = 0.08099739\n",
      "Iteration 163, loss = 0.09231609\n",
      "Iteration 182, loss = 0.08339513\n",
      "Iteration 164, loss = 0.09168248\n",
      "Iteration 183, loss = 0.08194007\n",
      "Iteration 165, loss = 0.09025706\n",
      "Iteration 184, loss = 0.07943041\n",
      "Iteration 166, loss = 0.08965384\n",
      "Iteration 185, loss = 0.08051838\n",
      "Iteration 167, loss = 0.08836799\n",
      "Iteration 186, loss = 0.08041842\n",
      "Iteration 187, loss = 0.08014162\n",
      "Iteration 168, loss = 0.08772502\n",
      "Iteration 188, loss = 0.07888844\n",
      "Iteration 169, loss = 0.08783396\n",
      "Iteration 189, loss = 0.08100477\n",
      "Iteration 170, loss = 0.08715964\n",
      "Iteration 190, loss = 0.08213322\n",
      "Iteration 171, loss = 0.08625444\n",
      "Iteration 191, loss = 0.08012301\n",
      "Iteration 172, loss = 0.08622243\n",
      "Iteration 192, loss = 0.07645876\n",
      "Iteration 173, loss = 0.08510455\n",
      "Iteration 174, loss = 0.08770904\n",
      "Iteration 193, loss = 0.07655431\n",
      "Iteration 175, loss = 0.08380110\n",
      "Iteration 194, loss = 0.07693819\n",
      "Iteration 176, loss = 0.08380110\n",
      "Iteration 195, loss = 0.07864301\n",
      "Iteration 177, loss = 0.08421401\n",
      "Iteration 196, loss = 0.07504620\n",
      "Iteration 178, loss = 0.08452833\n",
      "Iteration 197, loss = 0.07446279\n",
      "Iteration 179, loss = 0.08065164\n",
      "Iteration 198, loss = 0.07340996\n",
      "Iteration 180, loss = 0.08261488\n",
      "Iteration 199, loss = 0.07211607\n",
      "Iteration 181, loss = 0.08054067\n",
      "Iteration 200, loss = 0.07230610\n",
      "Iteration 182, loss = 0.08022301\n",
      "Iteration 183, loss = 0.08031697\n",
      "Iteration 184, loss = 0.07852884\n",
      "Iteration 185, loss = 0.07821125\n",
      "Iteration 186, loss = 0.07832457\n",
      "Iteration 187, loss = 0.07710337\n",
      "Iteration 188, loss = 0.07714822\n",
      "Iteration 189, loss = 0.07542992\n",
      "Iteration 190, loss = 0.07511735\n",
      "Iteration 191, loss = 0.07466584\n",
      "Iteration 192, loss = 0.07450501\n",
      "Iteration 193, loss = 0.07517881\n",
      "Iteration 194, loss = 0.07405503\n",
      "Iteration 195, loss = 0.07413295\n",
      "Iteration 196, loss = 0.07269287\n",
      "Iteration 197, loss = 0.07119842\n",
      "Iteration 198, loss = 0.07148616\n",
      "Iteration 199, loss = 0.07084090\n",
      "Iteration 1, loss = 1.12165578\n",
      "Iteration 200, loss = 0.06990342\n",
      "Iteration 2, loss = 0.97465607\n",
      "Iteration 3, loss = 0.87042825\n",
      "Iteration 4, loss = 0.78582200\n",
      "Iteration 5, loss = 0.71678461\n",
      "Iteration 6, loss = 0.65734936\n",
      "Iteration 7, loss = 0.60734061\n",
      "Iteration 8, loss = 0.56726263\n",
      "Iteration 9, loss = 0.53208807\n",
      "Iteration 10, loss = 0.50437744\n",
      "Iteration 11, loss = 0.48046918\n",
      "Iteration 12, loss = 0.46031522\n",
      "Iteration 13, loss = 0.44340829\n",
      "Iteration 14, loss = 0.42835481\n",
      "Iteration 15, loss = 0.41635350\n",
      "Iteration 16, loss = 0.40364127\n",
      "Iteration 17, loss = 0.39290671\n",
      "Iteration 18, loss = 0.38319362\n",
      "Iteration 19, loss = 0.37389868\n",
      "Iteration 1, loss = 1.27598389\n",
      "Iteration 20, loss = 0.36625233\n",
      "Iteration 2, loss = 1.02622963\n",
      "Iteration 21, loss = 0.35881770\n",
      "Iteration 3, loss = 0.88194585\n",
      "Iteration 22, loss = 0.35171533\n",
      "Iteration 4, loss = 0.79226658\n",
      "Iteration 23, loss = 0.34531346\n",
      "Iteration 5, loss = 0.71512096\n",
      "Iteration 24, loss = 0.33945749\n",
      "Iteration 6, loss = 0.65248740\n",
      "Iteration 25, loss = 0.33421745\n",
      "Iteration 7, loss = 0.60434919\n",
      "Iteration 26, loss = 0.32842032\n",
      "Iteration 8, loss = 0.56668988\n",
      "Iteration 27, loss = 0.32422133\n",
      "Iteration 9, loss = 0.53346079\n",
      "Iteration 28, loss = 0.31859435\n",
      "Iteration 10, loss = 0.50613276\n",
      "Iteration 29, loss = 0.31348201\n",
      "Iteration 11, loss = 0.48338933\n",
      "Iteration 30, loss = 0.31007918\n",
      "Iteration 12, loss = 0.46315815\n",
      "Iteration 31, loss = 0.30471555\n",
      "Iteration 13, loss = 0.44629754\n",
      "Iteration 32, loss = 0.30023603\n",
      "Iteration 14, loss = 0.42957045\n",
      "Iteration 33, loss = 0.29665939\n",
      "Iteration 15, loss = 0.41587908\n",
      "Iteration 34, loss = 0.29366743\n",
      "Iteration 16, loss = 0.40362736\n",
      "Iteration 35, loss = 0.28902671\n",
      "Iteration 17, loss = 0.39251004\n",
      "Iteration 36, loss = 0.28530543\n",
      "Iteration 18, loss = 0.38204369\n",
      "Iteration 37, loss = 0.28145984\n",
      "Iteration 19, loss = 0.37310670\n",
      "Iteration 38, loss = 0.27980773\n",
      "Iteration 20, loss = 0.36515520\n",
      "Iteration 39, loss = 0.27552223\n",
      "Iteration 21, loss = 0.35598897\n",
      "Iteration 40, loss = 0.27222134\n",
      "Iteration 22, loss = 0.34903155\n",
      "Iteration 41, loss = 0.26953319\n",
      "Iteration 23, loss = 0.34319471\n",
      "Iteration 42, loss = 0.26583530\n",
      "Iteration 24, loss = 0.33588272\n",
      "Iteration 43, loss = 0.26234833\n",
      "Iteration 25, loss = 0.32957503\n",
      "Iteration 26, loss = 0.32421817\n",
      "Iteration 44, loss = 0.25994993\n",
      "Iteration 27, loss = 0.31896683\n",
      "Iteration 45, loss = 0.25756304\n",
      "Iteration 28, loss = 0.31331424\n",
      "Iteration 46, loss = 0.25580748\n",
      "Iteration 29, loss = 0.30807482\n",
      "Iteration 47, loss = 0.25157374\n",
      "Iteration 30, loss = 0.30352616\n",
      "Iteration 48, loss = 0.24834856\n",
      "Iteration 31, loss = 0.29855315\n",
      "Iteration 49, loss = 0.24561301\n",
      "Iteration 32, loss = 0.29456428\n",
      "Iteration 50, loss = 0.24337778\n",
      "Iteration 33, loss = 0.29039259\n",
      "Iteration 51, loss = 0.24130286\n",
      "Iteration 34, loss = 0.28590760\n",
      "Iteration 52, loss = 0.23885982\n",
      "Iteration 35, loss = 0.28212647\n",
      "Iteration 53, loss = 0.23601256\n",
      "Iteration 36, loss = 0.27824719\n",
      "Iteration 54, loss = 0.23339253\n",
      "Iteration 37, loss = 0.27547220\n",
      "Iteration 55, loss = 0.23094726\n",
      "Iteration 38, loss = 0.27110712\n",
      "Iteration 56, loss = 0.22975621\n",
      "Iteration 39, loss = 0.26750778\n",
      "Iteration 57, loss = 0.22659306\n",
      "Iteration 40, loss = 0.26604339\n",
      "Iteration 58, loss = 0.22422664\n",
      "Iteration 41, loss = 0.26124508\n",
      "Iteration 59, loss = 0.22184991\n",
      "Iteration 42, loss = 0.25823993\n",
      "Iteration 60, loss = 0.22041665\n",
      "Iteration 43, loss = 0.25480978\n",
      "Iteration 61, loss = 0.21996333\n",
      "Iteration 44, loss = 0.25336792\n",
      "Iteration 62, loss = 0.21560879\n",
      "Iteration 45, loss = 0.25005300\n",
      "Iteration 63, loss = 0.21590389\n",
      "Iteration 46, loss = 0.24831165\n",
      "Iteration 64, loss = 0.21270076\n",
      "Iteration 47, loss = 0.24409324\n",
      "Iteration 65, loss = 0.21061345\n",
      "Iteration 48, loss = 0.24121358\n",
      "Iteration 66, loss = 0.20836284\n",
      "Iteration 49, loss = 0.23732683\n",
      "Iteration 67, loss = 0.20703529\n",
      "Iteration 50, loss = 0.23626573\n",
      "Iteration 68, loss = 0.20508384\n",
      "Iteration 51, loss = 0.23328123\n",
      "Iteration 69, loss = 0.20161371\n",
      "Iteration 52, loss = 0.23039966\n",
      "Iteration 70, loss = 0.20083277\n",
      "Iteration 53, loss = 0.22714388\n",
      "Iteration 71, loss = 0.19929970\n",
      "Iteration 54, loss = 0.22471981\n",
      "Iteration 72, loss = 0.19797515\n",
      "Iteration 55, loss = 0.22177270\n",
      "Iteration 73, loss = 0.19481822\n",
      "Iteration 56, loss = 0.21919229\n",
      "Iteration 74, loss = 0.19373509\n",
      "Iteration 57, loss = 0.21644929\n",
      "Iteration 75, loss = 0.19183235\n",
      "Iteration 58, loss = 0.21420770\n",
      "Iteration 76, loss = 0.18920037\n",
      "Iteration 59, loss = 0.21201618\n",
      "Iteration 77, loss = 0.18959132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, loss = 0.21134816\n",
      "Iteration 78, loss = 0.18564846\n",
      "Iteration 61, loss = 0.20727903\n",
      "Iteration 79, loss = 0.18593035\n",
      "Iteration 62, loss = 0.20631871\n",
      "Iteration 80, loss = 0.18414086\n",
      "Iteration 63, loss = 0.20433699\n",
      "Iteration 81, loss = 0.18143724\n",
      "Iteration 64, loss = 0.20108145\n",
      "Iteration 82, loss = 0.18057976\n",
      "Iteration 65, loss = 0.19883541\n",
      "Iteration 83, loss = 0.17761303\n",
      "Iteration 66, loss = 0.19641398\n",
      "Iteration 84, loss = 0.17807794\n",
      "Iteration 67, loss = 0.19409578\n",
      "Iteration 85, loss = 0.17498124\n",
      "Iteration 68, loss = 0.19173400\n",
      "Iteration 86, loss = 0.17379980\n",
      "Iteration 69, loss = 0.19130476\n",
      "Iteration 87, loss = 0.17238064\n",
      "Iteration 70, loss = 0.18783276\n",
      "Iteration 88, loss = 0.17113899\n",
      "Iteration 71, loss = 0.18708873\n",
      "Iteration 89, loss = 0.16848795\n",
      "Iteration 72, loss = 0.18410833\n",
      "Iteration 90, loss = 0.16802042\n",
      "Iteration 73, loss = 0.18204562\n",
      "Iteration 91, loss = 0.16629214\n",
      "Iteration 74, loss = 0.18060988\n",
      "Iteration 92, loss = 0.16594105\n",
      "Iteration 75, loss = 0.17845837\n",
      "Iteration 93, loss = 0.16275004\n",
      "Iteration 76, loss = 0.17601549\n",
      "Iteration 94, loss = 0.16114933\n",
      "Iteration 77, loss = 0.17318811\n",
      "Iteration 95, loss = 0.16034235\n",
      "Iteration 78, loss = 0.17272978\n",
      "Iteration 96, loss = 0.15853560\n",
      "Iteration 79, loss = 0.17033128\n",
      "Iteration 97, loss = 0.15770238\n",
      "Iteration 80, loss = 0.16816887\n",
      "Iteration 98, loss = 0.15668887\n",
      "Iteration 81, loss = 0.16796344\n",
      "Iteration 99, loss = 0.15566205\n",
      "Iteration 82, loss = 0.16569152\n",
      "Iteration 100, loss = 0.15397523\n",
      "Iteration 83, loss = 0.16405908\n",
      "Iteration 101, loss = 0.15497099\n",
      "Iteration 84, loss = 0.16149019\n",
      "Iteration 102, loss = 0.14988964\n",
      "Iteration 85, loss = 0.16057497\n",
      "Iteration 103, loss = 0.15054879\n",
      "Iteration 86, loss = 0.15822926\n",
      "Iteration 104, loss = 0.14833108\n",
      "Iteration 87, loss = 0.15743013\n",
      "Iteration 105, loss = 0.14766823\n",
      "Iteration 88, loss = 0.15414857\n",
      "Iteration 106, loss = 0.14558236\n",
      "Iteration 89, loss = 0.15464875\n",
      "Iteration 107, loss = 0.14445080\n",
      "Iteration 90, loss = 0.15044484\n",
      "Iteration 108, loss = 0.14338989\n",
      "Iteration 91, loss = 0.15130154\n",
      "Iteration 109, loss = 0.14316846\n",
      "Iteration 92, loss = 0.15004438\n",
      "Iteration 93, loss = 0.14736521\n",
      "Iteration 110, loss = 0.14152108\n",
      "Iteration 94, loss = 0.14806779\n",
      "Iteration 111, loss = 0.13987837\n",
      "Iteration 95, loss = 0.14373101\n",
      "Iteration 112, loss = 0.13902222\n",
      "Iteration 96, loss = 0.14365129\n",
      "Iteration 113, loss = 0.13787415\n",
      "Iteration 97, loss = 0.14195466\n",
      "Iteration 114, loss = 0.13659435\n",
      "Iteration 98, loss = 0.13968338\n",
      "Iteration 115, loss = 0.13551798\n",
      "Iteration 99, loss = 0.13782494\n",
      "Iteration 116, loss = 0.13462937\n",
      "Iteration 100, loss = 0.13663864\n",
      "Iteration 117, loss = 0.13454769\n",
      "Iteration 101, loss = 0.13679005\n",
      "Iteration 118, loss = 0.13194391\n",
      "Iteration 102, loss = 0.13474646\n",
      "Iteration 119, loss = 0.13326464\n",
      "Iteration 103, loss = 0.13325267\n",
      "Iteration 120, loss = 0.13052662\n",
      "Iteration 104, loss = 0.13260546\n",
      "Iteration 121, loss = 0.12949703\n",
      "Iteration 105, loss = 0.12907930\n",
      "Iteration 122, loss = 0.12858857\n",
      "Iteration 106, loss = 0.13002666\n",
      "Iteration 123, loss = 0.12713494\n",
      "Iteration 107, loss = 0.12729720\n",
      "Iteration 124, loss = 0.12594981\n",
      "Iteration 108, loss = 0.12679113\n",
      "Iteration 125, loss = 0.12486918\n",
      "Iteration 109, loss = 0.12796588\n",
      "Iteration 126, loss = 0.12489905\n",
      "Iteration 110, loss = 0.12482175\n",
      "Iteration 127, loss = 0.12396972\n",
      "Iteration 111, loss = 0.12393228\n",
      "Iteration 128, loss = 0.12242292\n",
      "Iteration 112, loss = 0.12401098\n",
      "Iteration 129, loss = 0.12256935\n",
      "Iteration 113, loss = 0.11972438\n",
      "Iteration 130, loss = 0.12134143\n",
      "Iteration 114, loss = 0.12209791\n",
      "Iteration 131, loss = 0.12307005\n",
      "Iteration 115, loss = 0.11986800\n",
      "Iteration 132, loss = 0.12010466\n",
      "Iteration 116, loss = 0.11817296\n",
      "Iteration 133, loss = 0.11824931\n",
      "Iteration 117, loss = 0.11732717\n",
      "Iteration 134, loss = 0.11840695\n",
      "Iteration 118, loss = 0.11722428\n",
      "Iteration 135, loss = 0.11632132\n",
      "Iteration 119, loss = 0.11437546\n",
      "Iteration 136, loss = 0.11530689\n",
      "Iteration 120, loss = 0.11526121\n",
      "Iteration 137, loss = 0.11458615\n",
      "Iteration 121, loss = 0.11383443\n",
      "Iteration 138, loss = 0.11391764\n",
      "Iteration 122, loss = 0.11393264\n",
      "Iteration 139, loss = 0.11428068\n",
      "Iteration 123, loss = 0.11037297\n",
      "Iteration 140, loss = 0.11228291\n",
      "Iteration 124, loss = 0.11083878\n",
      "Iteration 141, loss = 0.11150591\n",
      "Iteration 125, loss = 0.10796694\n",
      "Iteration 142, loss = 0.11057646\n",
      "Iteration 126, loss = 0.10905303\n",
      "Iteration 143, loss = 0.11046021\n",
      "Iteration 127, loss = 0.10653840\n",
      "Iteration 144, loss = 0.10938122\n",
      "Iteration 128, loss = 0.10643323\n",
      "Iteration 145, loss = 0.10718423\n",
      "Iteration 129, loss = 0.10435211\n",
      "Iteration 146, loss = 0.10756122\n",
      "Iteration 130, loss = 0.10493035\n",
      "Iteration 147, loss = 0.10695607\n",
      "Iteration 148, loss = 0.10545326\n",
      "Iteration 131, loss = 0.10372255\n",
      "Iteration 149, loss = 0.10474308\n",
      "Iteration 132, loss = 0.10385909\n",
      "Iteration 150, loss = 0.10412701\n",
      "Iteration 133, loss = 0.10027794\n",
      "Iteration 134, loss = 0.10167694\n",
      "Iteration 151, loss = 0.10378837\n",
      "Iteration 135, loss = 0.09971596\n",
      "Iteration 152, loss = 0.10141370\n",
      "Iteration 153, loss = 0.10343963Iteration 136, loss = 0.09848841\n",
      "\n",
      "Iteration 154, loss = 0.10389954\n",
      "Iteration 137, loss = 0.09828609\n",
      "Iteration 155, loss = 0.10427589\n",
      "Iteration 138, loss = 0.09699929\n",
      "Iteration 156, loss = 0.09963034\n",
      "Iteration 139, loss = 0.09625688\n",
      "Iteration 157, loss = 0.10073849\n",
      "Iteration 140, loss = 0.09689968\n",
      "Iteration 141, loss = 0.09552491\n",
      "Iteration 158, loss = 0.09928386\n",
      "Iteration 142, loss = 0.09417637\n",
      "Iteration 159, loss = 0.09734880\n",
      "Iteration 143, loss = 0.09432484\n",
      "Iteration 160, loss = 0.09671906\n",
      "Iteration 144, loss = 0.09231623\n",
      "Iteration 161, loss = 0.09585268\n",
      "Iteration 145, loss = 0.09238996\n",
      "Iteration 162, loss = 0.09485320\n",
      "Iteration 146, loss = 0.09571156\n",
      "Iteration 163, loss = 0.09461894\n",
      "Iteration 147, loss = 0.09466432\n",
      "Iteration 164, loss = 0.09393213\n",
      "Iteration 148, loss = 0.09112852\n",
      "Iteration 165, loss = 0.09332543\n",
      "Iteration 166, loss = 0.09222329\n",
      "Iteration 149, loss = 0.09072784\n",
      "Iteration 167, loss = 0.09238090\n",
      "Iteration 150, loss = 0.08877079\n",
      "Iteration 168, loss = 0.09189428\n",
      "Iteration 151, loss = 0.08701274\n",
      "Iteration 169, loss = 0.09159949\n",
      "Iteration 152, loss = 0.09195200\n",
      "Iteration 170, loss = 0.09043248\n",
      "Iteration 153, loss = 0.08702050\n",
      "Iteration 171, loss = 0.08984100\n",
      "Iteration 154, loss = 0.08617024\n",
      "Iteration 172, loss = 0.08982124\n",
      "Iteration 155, loss = 0.08704197\n",
      "Iteration 156, loss = 0.08626421\n",
      "Iteration 173, loss = 0.08850038\n",
      "Iteration 157, loss = 0.08277120\n",
      "Iteration 174, loss = 0.08836633\n",
      "Iteration 158, loss = 0.08480643\n",
      "Iteration 175, loss = 0.08633496\n",
      "Iteration 159, loss = 0.08416377\n",
      "Iteration 176, loss = 0.08712204\n",
      "Iteration 160, loss = 0.08270689\n",
      "Iteration 177, loss = 0.08549798\n",
      "Iteration 161, loss = 0.08193488\n",
      "Iteration 178, loss = 0.08485454\n",
      "Iteration 162, loss = 0.08087753\n",
      "Iteration 179, loss = 0.08459359\n",
      "Iteration 163, loss = 0.08036796\n",
      "Iteration 180, loss = 0.08350076\n",
      "Iteration 164, loss = 0.08165016\n",
      "Iteration 181, loss = 0.08233981\n",
      "Iteration 165, loss = 0.07941645\n",
      "Iteration 182, loss = 0.08173714\n",
      "Iteration 166, loss = 0.07895805\n",
      "Iteration 183, loss = 0.08464413\n",
      "Iteration 167, loss = 0.07741766\n",
      "Iteration 184, loss = 0.08210083\n",
      "Iteration 168, loss = 0.07619680\n",
      "Iteration 185, loss = 0.08247092\n",
      "Iteration 169, loss = 0.07644328\n",
      "Iteration 186, loss = 0.07957503\n",
      "Iteration 170, loss = 0.07415648\n",
      "Iteration 187, loss = 0.07998901\n",
      "Iteration 171, loss = 0.07502328\n",
      "Iteration 188, loss = 0.07940524\n",
      "Iteration 172, loss = 0.07345178\n",
      "Iteration 189, loss = 0.08076648\n",
      "Iteration 173, loss = 0.07361325\n",
      "Iteration 190, loss = 0.07885165\n",
      "Iteration 174, loss = 0.07313826\n",
      "Iteration 191, loss = 0.07704254\n",
      "Iteration 175, loss = 0.07306451\n",
      "Iteration 192, loss = 0.07820485\n",
      "Iteration 176, loss = 0.07196927\n",
      "Iteration 193, loss = 0.07579461\n",
      "Iteration 177, loss = 0.07280556\n",
      "Iteration 194, loss = 0.07567457\n",
      "Iteration 178, loss = 0.07063878\n",
      "Iteration 195, loss = 0.07624371\n",
      "Iteration 179, loss = 0.06971319\n",
      "Iteration 196, loss = 0.07472608\n",
      "Iteration 180, loss = 0.07196448\n",
      "Iteration 197, loss = 0.07603966\n",
      "Iteration 181, loss = 0.06913767\n",
      "Iteration 198, loss = 0.07316407\n",
      "Iteration 182, loss = 0.06890704\n",
      "Iteration 199, loss = 0.07389501\n",
      "Iteration 183, loss = 0.06769323\n",
      "Iteration 200, loss = 0.07276350\n",
      "Iteration 184, loss = 0.06636685\n",
      "Iteration 201, loss = 0.07146713\n",
      "Iteration 185, loss = 0.06625273\n",
      "Iteration 202, loss = 0.07166952\n",
      "Iteration 186, loss = 0.06554002\n",
      "Iteration 203, loss = 0.07238359\n",
      "Iteration 187, loss = 0.06498283\n",
      "Iteration 204, loss = 0.07219503\n",
      "Iteration 188, loss = 0.06511435\n",
      "Iteration 205, loss = 0.07330975\n",
      "Iteration 189, loss = 0.06489006\n",
      "Iteration 206, loss = 0.06858839\n",
      "Iteration 207, loss = 0.06992927\n",
      "Iteration 190, loss = 0.06406872\n",
      "Iteration 191, loss = 0.06375005\n",
      "Iteration 208, loss = 0.06828519\n",
      "Iteration 192, loss = 0.06551829\n",
      "Iteration 209, loss = 0.06841971\n",
      "Iteration 193, loss = 0.06359991\n",
      "Iteration 210, loss = 0.06691860\n",
      "Iteration 194, loss = 0.06127144\n",
      "Iteration 211, loss = 0.06661221\n",
      "Iteration 195, loss = 0.06348062\n",
      "Iteration 212, loss = 0.06623759\n",
      "Iteration 196, loss = 0.06150763\n",
      "Iteration 213, loss = 0.06888356\n",
      "Iteration 197, loss = 0.05973297\n",
      "Iteration 214, loss = 0.06553263\n",
      "Iteration 198, loss = 0.06000844\n",
      "Iteration 215, loss = 0.06461939\n",
      "Iteration 199, loss = 0.05946699\n",
      "Iteration 216, loss = 0.06625590\n",
      "Iteration 200, loss = 0.05898477\n",
      "Iteration 217, loss = 0.06774779\n",
      "Iteration 201, loss = 0.05987151\n",
      "Iteration 218, loss = 0.06475095\n",
      "Iteration 202, loss = 0.05783596\n",
      "Iteration 219, loss = 0.06303100\n",
      "Iteration 203, loss = 0.05801901\n",
      "Iteration 220, loss = 0.06370052\n",
      "Iteration 204, loss = 0.05768922\n",
      "Iteration 221, loss = 0.06247801\n",
      "Iteration 205, loss = 0.05621613\n",
      "Iteration 222, loss = 0.06101011\n",
      "Iteration 206, loss = 0.05817514\n",
      "Iteration 223, loss = 0.06138217\n",
      "Iteration 207, loss = 0.05867437\n",
      "Iteration 224, loss = 0.06339479\n",
      "Iteration 208, loss = 0.05623482\n",
      "Iteration 225, loss = 0.05985824\n",
      "Iteration 209, loss = 0.05571389\n",
      "Iteration 226, loss = 0.06013509\n",
      "Iteration 210, loss = 0.05505897\n",
      "Iteration 227, loss = 0.05983333\n",
      "Iteration 211, loss = 0.05415071\n",
      "Iteration 228, loss = 0.05943720\n",
      "Iteration 212, loss = 0.05406031\n",
      "Iteration 229, loss = 0.05793060\n",
      "Iteration 213, loss = 0.05417549\n",
      "Iteration 230, loss = 0.05797263\n",
      "Iteration 214, loss = 0.05206267\n",
      "Iteration 231, loss = 0.05765208\n",
      "Iteration 215, loss = 0.05473805\n",
      "Iteration 232, loss = 0.05657088\n",
      "Iteration 216, loss = 0.05540861\n",
      "Iteration 233, loss = 0.05656894\n",
      "Iteration 217, loss = 0.05285851\n",
      "Iteration 234, loss = 0.05617685\n",
      "Iteration 218, loss = 0.05095359\n",
      "Iteration 235, loss = 0.05555092\n",
      "Iteration 219, loss = 0.05124301\n",
      "Iteration 236, loss = 0.05575070\n",
      "Iteration 220, loss = 0.05651762\n",
      "Iteration 237, loss = 0.05458867\n",
      "Iteration 221, loss = 0.05853365\n",
      "Iteration 238, loss = 0.05470060\n",
      "Iteration 222, loss = 0.05447111\n",
      "Iteration 239, loss = 0.05467495\n",
      "Iteration 223, loss = 0.05508849\n",
      "Iteration 240, loss = 0.05372437\n",
      "Iteration 224, loss = 0.05501893\n",
      "Iteration 241, loss = 0.05409812\n",
      "Iteration 225, loss = 0.04971488\n",
      "Iteration 242, loss = 0.05282682\n",
      "Iteration 226, loss = 0.05087639\n",
      "Iteration 243, loss = 0.05194521\n",
      "Iteration 227, loss = 0.04868882\n",
      "Iteration 244, loss = 0.05294958\n",
      "Iteration 228, loss = 0.04796675\n",
      "Iteration 245, loss = 0.05180900\n",
      "Iteration 229, loss = 0.04846384\n",
      "Iteration 246, loss = 0.05263938\n",
      "Iteration 230, loss = 0.04954976\n",
      "Iteration 247, loss = 0.05165646\n",
      "Iteration 231, loss = 0.05008290\n",
      "Iteration 248, loss = 0.05033475\n",
      "Iteration 232, loss = 0.04621607\n",
      "Iteration 249, loss = 0.05080337\n",
      "Iteration 233, loss = 0.04670227\n",
      "Iteration 250, loss = 0.05052907\n",
      "Iteration 234, loss = 0.05009792\n",
      "Iteration 251, loss = 0.04960873\n",
      "Iteration 235, loss = 0.04483907\n",
      "Iteration 252, loss = 0.04906524\n",
      "Iteration 236, loss = 0.04546173\n",
      "Iteration 253, loss = 0.04854389\n",
      "Iteration 237, loss = 0.04493032\n",
      "Iteration 254, loss = 0.04814294\n",
      "Iteration 238, loss = 0.04372582\n",
      "Iteration 255, loss = 0.04751453\n",
      "Iteration 239, loss = 0.04539597\n",
      "Iteration 256, loss = 0.04724372\n",
      "Iteration 240, loss = 0.04405352\n",
      "Iteration 257, loss = 0.04732288\n",
      "Iteration 241, loss = 0.04301635\n",
      "Iteration 258, loss = 0.04720977\n",
      "Iteration 242, loss = 0.04289503\n",
      "Iteration 259, loss = 0.04610560\n",
      "Iteration 243, loss = 0.04302525\n",
      "Iteration 260, loss = 0.04671456\n",
      "Iteration 244, loss = 0.04199592\n",
      "Iteration 261, loss = 0.04631738\n",
      "Iteration 245, loss = 0.04113755\n",
      "Iteration 262, loss = 0.04511219\n",
      "Iteration 246, loss = 0.04170026\n",
      "Iteration 263, loss = 0.04661758\n",
      "Iteration 247, loss = 0.04199061\n",
      "Iteration 264, loss = 0.04543457\n",
      "Iteration 248, loss = 0.04077376\n",
      "Iteration 265, loss = 0.04566581\n",
      "Iteration 249, loss = 0.04178599\n",
      "Iteration 266, loss = 0.04415432\n",
      "Iteration 250, loss = 0.04117372\n",
      "Iteration 267, loss = 0.04388459\n",
      "Iteration 251, loss = 0.03986421\n",
      "Iteration 268, loss = 0.04339038\n",
      "Iteration 252, loss = 0.04000515\n",
      "Iteration 269, loss = 0.04297666\n",
      "Iteration 253, loss = 0.04004764\n",
      "Iteration 270, loss = 0.04465348\n",
      "Iteration 254, loss = 0.03982757\n",
      "Iteration 271, loss = 0.04367194\n",
      "Iteration 255, loss = 0.04055412\n",
      "Iteration 272, loss = 0.04385367\n",
      "Iteration 256, loss = 0.04049304\n",
      "Iteration 273, loss = 0.04214460\n",
      "Iteration 257, loss = 0.03812059\n",
      "Iteration 274, loss = 0.04224885\n",
      "Iteration 258, loss = 0.03918008\n",
      "Iteration 275, loss = 0.04101459\n",
      "Iteration 259, loss = 0.03832359\n",
      "Iteration 276, loss = 0.04118085\n",
      "Iteration 260, loss = 0.03817399\n",
      "Iteration 277, loss = 0.03999365\n",
      "Iteration 261, loss = 0.03669875\n",
      "Iteration 278, loss = 0.03981905\n",
      "Iteration 262, loss = 0.03762734\n",
      "Iteration 279, loss = 0.04062444\n",
      "Iteration 263, loss = 0.03610320\n",
      "Iteration 280, loss = 0.04039906\n",
      "Iteration 264, loss = 0.03589727\n",
      "Iteration 281, loss = 0.03983960\n",
      "Iteration 265, loss = 0.03570922\n",
      "Iteration 282, loss = 0.03839554\n",
      "Iteration 266, loss = 0.03648250\n",
      "Iteration 283, loss = 0.04031776\n",
      "Iteration 267, loss = 0.03852846\n",
      "Iteration 284, loss = 0.03850348\n",
      "Iteration 268, loss = 0.03508982\n",
      "Iteration 285, loss = 0.04020927\n",
      "Iteration 269, loss = 0.03587437\n",
      "Iteration 286, loss = 0.03800630\n",
      "Iteration 270, loss = 0.03633005\n",
      "Iteration 287, loss = 0.03810383\n",
      "Iteration 271, loss = 0.03550593\n",
      "Iteration 288, loss = 0.03750021\n",
      "Iteration 272, loss = 0.03417706\n",
      "Iteration 289, loss = 0.03669903\n",
      "Iteration 273, loss = 0.03392630\n",
      "Iteration 290, loss = 0.03675698\n",
      "Iteration 274, loss = 0.03461587\n",
      "Iteration 291, loss = 0.03755491\n",
      "Iteration 275, loss = 0.03323309\n",
      "Iteration 292, loss = 0.03756129\n",
      "Iteration 276, loss = 0.03405645\n",
      "Iteration 293, loss = 0.03691086\n",
      "Iteration 277, loss = 0.03500581\n",
      "Iteration 294, loss = 0.03671523\n",
      "Iteration 278, loss = 0.03307134\n",
      "Iteration 295, loss = 0.03565626\n",
      "Iteration 279, loss = 0.03365058\n",
      "Iteration 296, loss = 0.03533568\n",
      "Iteration 280, loss = 0.03240236\n",
      "Iteration 297, loss = 0.03498402\n",
      "Iteration 281, loss = 0.03213852\n",
      "Iteration 298, loss = 0.03402390\n",
      "Iteration 282, loss = 0.03203269\n",
      "Iteration 299, loss = 0.03347273\n",
      "Iteration 283, loss = 0.03229003\n",
      "Iteration 300, loss = 0.03345223\n",
      "Iteration 284, loss = 0.03139751\n",
      "Iteration 301, loss = 0.03423845\n",
      "Iteration 285, loss = 0.03086201\n",
      "Iteration 302, loss = 0.03231150\n",
      "Iteration 286, loss = 0.03086998\n",
      "Iteration 303, loss = 0.03372486\n",
      "Iteration 287, loss = 0.03092940\n",
      "Iteration 304, loss = 0.03534534\n",
      "Iteration 288, loss = 0.02995972\n",
      "Iteration 305, loss = 0.03363548\n",
      "Iteration 289, loss = 0.03049724\n",
      "Iteration 306, loss = 0.03267780\n",
      "Iteration 290, loss = 0.02981194\n",
      "Iteration 307, loss = 0.03405400\n",
      "Iteration 291, loss = 0.02959701\n",
      "Iteration 308, loss = 0.03207812\n",
      "Iteration 292, loss = 0.03008633\n",
      "Iteration 309, loss = 0.03258364\n",
      "Iteration 293, loss = 0.02926029\n",
      "Iteration 310, loss = 0.03240850\n",
      "Iteration 294, loss = 0.02956240\n",
      "Iteration 311, loss = 0.03129246\n",
      "Iteration 295, loss = 0.02898964\n",
      "Iteration 312, loss = 0.03246272\n",
      "Iteration 296, loss = 0.02844511\n",
      "Iteration 313, loss = 0.03060181\n",
      "Iteration 297, loss = 0.02861825\n",
      "Iteration 314, loss = 0.03119113\n",
      "Iteration 298, loss = 0.02834782\n",
      "Iteration 315, loss = 0.03111541\n",
      "Iteration 299, loss = 0.02788564\n",
      "Iteration 316, loss = 0.02901928\n",
      "Iteration 300, loss = 0.02918275\n",
      "Iteration 317, loss = 0.02922713\n",
      "Iteration 301, loss = 0.02800914\n",
      "Iteration 318, loss = 0.02892295\n",
      "Iteration 302, loss = 0.02844838\n",
      "Iteration 319, loss = 0.03025940\n",
      "Iteration 303, loss = 0.02726244\n",
      "Iteration 320, loss = 0.03454646\n",
      "Iteration 304, loss = 0.02748730\n",
      "Iteration 321, loss = 0.02967236\n",
      "Iteration 305, loss = 0.02760603\n",
      "Iteration 322, loss = 0.02873070\n",
      "Iteration 306, loss = 0.02673844\n",
      "Iteration 323, loss = 0.02943646\n",
      "Iteration 307, loss = 0.02683324\n",
      "Iteration 324, loss = 0.02834528\n",
      "Iteration 308, loss = 0.02689621\n",
      "Iteration 309, loss = 0.02678103\n",
      "Iteration 325, loss = 0.02940808\n",
      "Iteration 310, loss = 0.02631762\n",
      "Iteration 326, loss = 0.02941511\n",
      "Iteration 311, loss = 0.02634572Iteration 327, loss = 0.02769927\n",
      "\n",
      "Iteration 312, loss = 0.02724273\n",
      "Iteration 328, loss = 0.02892040\n",
      "Iteration 313, loss = 0.02706904\n",
      "Iteration 329, loss = 0.02658432\n",
      "Iteration 314, loss = 0.02529242\n",
      "Iteration 330, loss = 0.02737371\n",
      "Iteration 331, loss = 0.02690173\n",
      "Iteration 315, loss = 0.02562388\n",
      "Iteration 332, loss = 0.02699319\n",
      "Iteration 316, loss = 0.02559464\n",
      "Iteration 333, loss = 0.02694374\n",
      "Iteration 317, loss = 0.02569591\n",
      "Iteration 318, loss = 0.02521706\n",
      "Iteration 334, loss = 0.02665915\n",
      "Iteration 335, loss = 0.02498131\n",
      "Iteration 319, loss = 0.02601194\n",
      "Iteration 320, loss = 0.02487275\n",
      "Iteration 336, loss = 0.02603781\n",
      "Iteration 321, loss = 0.02642552\n",
      "Iteration 337, loss = 0.02668533\n",
      "Iteration 322, loss = 0.02508214\n",
      "Iteration 338, loss = 0.02443245\n",
      "Iteration 323, loss = 0.02399435\n",
      "Iteration 339, loss = 0.02454380\n",
      "Iteration 324, loss = 0.02412676\n",
      "Iteration 340, loss = 0.02423971\n",
      "Iteration 325, loss = 0.02435775\n",
      "Iteration 341, loss = 0.02386150\n",
      "Iteration 326, loss = 0.02341193\n",
      "Iteration 342, loss = 0.02366563\n",
      "Iteration 327, loss = 0.02311607\n",
      "Iteration 343, loss = 0.02430722\n",
      "Iteration 328, loss = 0.02312820\n",
      "Iteration 344, loss = 0.02331685\n",
      "Iteration 329, loss = 0.02291237\n",
      "Iteration 345, loss = 0.02318314\n",
      "Iteration 330, loss = 0.02321756\n",
      "Iteration 346, loss = 0.02325770\n",
      "Iteration 331, loss = 0.02353151\n",
      "Iteration 347, loss = 0.02274867\n",
      "Iteration 332, loss = 0.02259899\n",
      "Iteration 348, loss = 0.02288509\n",
      "Iteration 333, loss = 0.02241106\n",
      "Iteration 349, loss = 0.02337151\n",
      "Iteration 334, loss = 0.02226385\n",
      "Iteration 350, loss = 0.02339959\n",
      "Iteration 335, loss = 0.02279216\n",
      "Iteration 351, loss = 0.02250858\n",
      "Iteration 336, loss = 0.02178222\n",
      "Iteration 352, loss = 0.02159164\n",
      "Iteration 337, loss = 0.02307946\n",
      "Iteration 353, loss = 0.02231695\n",
      "Iteration 338, loss = 0.02272645\n",
      "Iteration 354, loss = 0.02241683\n",
      "Iteration 339, loss = 0.02142594\n",
      "Iteration 355, loss = 0.02155655\n",
      "Iteration 340, loss = 0.02165645\n",
      "Iteration 356, loss = 0.02145250\n",
      "Iteration 341, loss = 0.02149639\n",
      "Iteration 357, loss = 0.02106966\n",
      "Iteration 342, loss = 0.02175034\n",
      "Iteration 358, loss = 0.02119677\n",
      "Iteration 343, loss = 0.02096781\n",
      "Iteration 359, loss = 0.02152473\n",
      "Iteration 344, loss = 0.02109619\n",
      "Iteration 360, loss = 0.02195030\n",
      "Iteration 345, loss = 0.02070601\n",
      "Iteration 361, loss = 0.02048517\n",
      "Iteration 346, loss = 0.02071551\n",
      "Iteration 362, loss = 0.02094399\n",
      "Iteration 347, loss = 0.02082293\n",
      "Iteration 363, loss = 0.02048049\n",
      "Iteration 348, loss = 0.02035064\n",
      "Iteration 364, loss = 0.02013692\n",
      "Iteration 349, loss = 0.02068681\n",
      "Iteration 365, loss = 0.02058405\n",
      "Iteration 350, loss = 0.02023743\n",
      "Iteration 366, loss = 0.02053078\n",
      "Iteration 351, loss = 0.02026798\n",
      "Iteration 367, loss = 0.02026958\n",
      "Iteration 352, loss = 0.02017224\n",
      "Iteration 368, loss = 0.01983818\n",
      "Iteration 353, loss = 0.02005948\n",
      "Iteration 369, loss = 0.01978754\n",
      "Iteration 354, loss = 0.02026063\n",
      "Iteration 370, loss = 0.01970839\n",
      "Iteration 355, loss = 0.01987170\n",
      "Iteration 371, loss = 0.01913050\n",
      "Iteration 356, loss = 0.01943879\n",
      "Iteration 372, loss = 0.01886249\n",
      "Iteration 357, loss = 0.01951190\n",
      "Iteration 373, loss = 0.01821462\n",
      "Iteration 358, loss = 0.01919688\n",
      "Iteration 374, loss = 0.01840795\n",
      "Iteration 359, loss = 0.01848107\n",
      "Iteration 375, loss = 0.01824637\n",
      "Iteration 360, loss = 0.01923568\n",
      "Iteration 376, loss = 0.01798934\n",
      "Iteration 361, loss = 0.01935625\n",
      "Iteration 377, loss = 0.01776097\n",
      "Iteration 362, loss = 0.01902750\n",
      "Iteration 378, loss = 0.01787906\n",
      "Iteration 363, loss = 0.01880919\n",
      "Iteration 379, loss = 0.01777325\n",
      "Iteration 364, loss = 0.01828216\n",
      "Iteration 380, loss = 0.01960923\n",
      "Iteration 365, loss = 0.01855431\n",
      "Iteration 381, loss = 0.01947626\n",
      "Iteration 366, loss = 0.01796512\n",
      "Iteration 382, loss = 0.01831644\n",
      "Iteration 367, loss = 0.01932018\n",
      "Iteration 383, loss = 0.01787101\n",
      "Iteration 368, loss = 0.01811529\n",
      "Iteration 384, loss = 0.01662063\n",
      "Iteration 369, loss = 0.01760357\n",
      "Iteration 385, loss = 0.01790834\n",
      "Iteration 370, loss = 0.01857363\n",
      "Iteration 386, loss = 0.01681605\n",
      "Iteration 371, loss = 0.01801747\n",
      "Iteration 387, loss = 0.01742069\n",
      "Iteration 372, loss = 0.01810223\n",
      "Iteration 388, loss = 0.01741446\n",
      "Iteration 373, loss = 0.01735826\n",
      "Iteration 389, loss = 0.01686016\n",
      "Iteration 374, loss = 0.01799907\n",
      "Iteration 390, loss = 0.01631100\n",
      "Iteration 375, loss = 0.01700033\n",
      "Iteration 391, loss = 0.01588538\n",
      "Iteration 376, loss = 0.01755533\n",
      "Iteration 392, loss = 0.01690643\n",
      "Iteration 377, loss = 0.01724044\n",
      "Iteration 393, loss = 0.01686508\n",
      "Iteration 378, loss = 0.01712937\n",
      "Iteration 394, loss = 0.01610231\n",
      "Iteration 379, loss = 0.01664421\n",
      "Iteration 395, loss = 0.01602832\n",
      "Iteration 380, loss = 0.01666489\n",
      "Iteration 396, loss = 0.01517011\n",
      "Iteration 381, loss = 0.01723920\n",
      "Iteration 397, loss = 0.01534374\n",
      "Iteration 382, loss = 0.01778902\n",
      "Iteration 398, loss = 0.01663464\n",
      "Iteration 383, loss = 0.01646154\n",
      "Iteration 399, loss = 0.01652053\n",
      "Iteration 384, loss = 0.01655510\n",
      "Iteration 400, loss = 0.01636785\n",
      "Iteration 385, loss = 0.01628318\n",
      "Iteration 401, loss = 0.01652621\n",
      "Iteration 386, loss = 0.01639386\n",
      "Iteration 402, loss = 0.01531440\n",
      "Iteration 387, loss = 0.01716215\n",
      "Iteration 403, loss = 0.01520870\n",
      "Iteration 388, loss = 0.01622056\n",
      "Iteration 404, loss = 0.01488478\n",
      "Iteration 389, loss = 0.01610978\n",
      "Iteration 405, loss = 0.01506630\n",
      "Iteration 390, loss = 0.01613233\n",
      "Iteration 406, loss = 0.01494686\n",
      "Iteration 391, loss = 0.01562567\n",
      "Iteration 407, loss = 0.01527231\n",
      "Iteration 392, loss = 0.01605205\n",
      "Iteration 408, loss = 0.01605620\n",
      "Iteration 393, loss = 0.01552138\n",
      "Iteration 409, loss = 0.01488750\n",
      "Iteration 394, loss = 0.01518043\n",
      "Iteration 410, loss = 0.01443119\n",
      "Iteration 395, loss = 0.01596762\n",
      "Iteration 411, loss = 0.01478238\n",
      "Iteration 396, loss = 0.01614649\n",
      "Iteration 412, loss = 0.01471117\n",
      "Iteration 397, loss = 0.01589490\n",
      "Iteration 413, loss = 0.01336579\n",
      "Iteration 398, loss = 0.01663537\n",
      "Iteration 414, loss = 0.01379696\n",
      "Iteration 399, loss = 0.01591747\n",
      "Iteration 415, loss = 0.01304305\n",
      "Iteration 400, loss = 0.01493178\n",
      "Iteration 416, loss = 0.01295886\n",
      "Iteration 401, loss = 0.01669994\n",
      "Iteration 417, loss = 0.01286313\n",
      "Iteration 402, loss = 0.01513358\n",
      "Iteration 418, loss = 0.01366312\n",
      "Iteration 403, loss = 0.01470821\n",
      "Iteration 419, loss = 0.01269125\n",
      "Iteration 404, loss = 0.01437199\n",
      "Iteration 420, loss = 0.01243064\n",
      "Iteration 405, loss = 0.01421507\n",
      "Iteration 421, loss = 0.01333255\n",
      "Iteration 406, loss = 0.01586576\n",
      "Iteration 422, loss = 0.01324239\n",
      "Iteration 407, loss = 0.01504593\n",
      "Iteration 423, loss = 0.01251486\n",
      "Iteration 408, loss = 0.01419609\n",
      "Iteration 424, loss = 0.01244960\n",
      "Iteration 409, loss = 0.01476309\n",
      "Iteration 425, loss = 0.01238279\n",
      "Iteration 410, loss = 0.01451408\n",
      "Iteration 426, loss = 0.01214251\n",
      "Iteration 411, loss = 0.01411114\n",
      "Iteration 427, loss = 0.01218128\n",
      "Iteration 412, loss = 0.01354320\n",
      "Iteration 428, loss = 0.01206254\n",
      "Iteration 413, loss = 0.01388462\n",
      "Iteration 429, loss = 0.01193626\n",
      "Iteration 414, loss = 0.01442400\n",
      "Iteration 430, loss = 0.01185310\n",
      "Iteration 415, loss = 0.01395807\n",
      "Iteration 431, loss = 0.01168870\n",
      "Iteration 416, loss = 0.01429810\n",
      "Iteration 432, loss = 0.01151197\n",
      "Iteration 417, loss = 0.01333832\n",
      "Iteration 433, loss = 0.01130588\n",
      "Iteration 418, loss = 0.01424525\n",
      "Iteration 434, loss = 0.01135482\n",
      "Iteration 419, loss = 0.01335659\n",
      "Iteration 435, loss = 0.01124088\n",
      "Iteration 420, loss = 0.01328773\n",
      "Iteration 436, loss = 0.01123915\n",
      "Iteration 421, loss = 0.01332423\n",
      "Iteration 437, loss = 0.01099284\n",
      "Iteration 422, loss = 0.01296931\n",
      "Iteration 438, loss = 0.01104541\n",
      "Iteration 423, loss = 0.01266235\n",
      "Iteration 439, loss = 0.01084496\n",
      "Iteration 424, loss = 0.01360149\n",
      "Iteration 440, loss = 0.01096262\n",
      "Iteration 425, loss = 0.01283016\n",
      "Iteration 441, loss = 0.01123893\n",
      "Iteration 426, loss = 0.01262154\n",
      "Iteration 442, loss = 0.01063265\n",
      "Iteration 427, loss = 0.01251465\n",
      "Iteration 443, loss = 0.01066673\n",
      "Iteration 428, loss = 0.01282917\n",
      "Iteration 444, loss = 0.01074986\n",
      "Iteration 429, loss = 0.01295930\n",
      "Iteration 445, loss = 0.01045577\n",
      "Iteration 430, loss = 0.01241813\n",
      "Iteration 446, loss = 0.01048066\n",
      "Iteration 431, loss = 0.01294702\n",
      "Iteration 447, loss = 0.01023038\n",
      "Iteration 432, loss = 0.01222172\n",
      "Iteration 448, loss = 0.01037044\n",
      "Iteration 433, loss = 0.01392163\n",
      "Iteration 449, loss = 0.01021028\n",
      "Iteration 434, loss = 0.01362935\n",
      "Iteration 450, loss = 0.01018584\n",
      "Iteration 435, loss = 0.01252830\n",
      "Iteration 451, loss = 0.01011220\n",
      "Iteration 436, loss = 0.01188637\n",
      "Iteration 452, loss = 0.00985125\n",
      "Iteration 437, loss = 0.01218280\n",
      "Iteration 453, loss = 0.01036431\n",
      "Iteration 438, loss = 0.01185947\n",
      "Iteration 454, loss = 0.01001722\n",
      "Iteration 439, loss = 0.01169025\n",
      "Iteration 455, loss = 0.00947367\n",
      "Iteration 440, loss = 0.01146160\n",
      "Iteration 456, loss = 0.00991924\n",
      "Iteration 441, loss = 0.01161596\n",
      "Iteration 457, loss = 0.00954158\n",
      "Iteration 442, loss = 0.01159719\n",
      "Iteration 458, loss = 0.00942572\n",
      "Iteration 443, loss = 0.01129125\n",
      "Iteration 459, loss = 0.00956256\n",
      "Iteration 444, loss = 0.01218565\n",
      "Iteration 460, loss = 0.00930256\n",
      "Iteration 445, loss = 0.01180573\n",
      "Iteration 461, loss = 0.00940012\n",
      "Iteration 446, loss = 0.01158107\n",
      "Iteration 462, loss = 0.00917796\n",
      "Iteration 447, loss = 0.01132503\n",
      "Iteration 463, loss = 0.00911040\n",
      "Iteration 448, loss = 0.01150935\n",
      "Iteration 464, loss = 0.00900953\n",
      "Iteration 449, loss = 0.01291477\n",
      "Iteration 450, loss = 0.01181534\n",
      "Iteration 465, loss = 0.00924396\n",
      "Iteration 451, loss = 0.01171712\n",
      "Iteration 466, loss = 0.01008568\n",
      "Iteration 452, loss = 0.01118604\n",
      "Iteration 467, loss = 0.00903081\n",
      "Iteration 468, loss = 0.00906939\n",
      "Iteration 453, loss = 0.01087248\n",
      "Iteration 454, loss = 0.01395072\n",
      "Iteration 469, loss = 0.00879101\n",
      "Iteration 455, loss = 0.01281267\n",
      "Iteration 470, loss = 0.00880249\n",
      "Iteration 456, loss = 0.01110581\n",
      "Iteration 471, loss = 0.00845947\n",
      "Iteration 457, loss = 0.01270241\n",
      "Iteration 472, loss = 0.00854293\n",
      "Iteration 458, loss = 0.01052286\n",
      "Iteration 473, loss = 0.00887427\n",
      "Iteration 459, loss = 0.01041180\n",
      "Iteration 474, loss = 0.00872841\n",
      "Iteration 460, loss = 0.01063382\n",
      "Iteration 475, loss = 0.00876345\n",
      "Iteration 461, loss = 0.01024432\n",
      "Iteration 476, loss = 0.00835703\n",
      "Iteration 462, loss = 0.01058927\n",
      "Iteration 477, loss = 0.00849929\n",
      "Iteration 463, loss = 0.01058120\n",
      "Iteration 478, loss = 0.00833494\n",
      "Iteration 464, loss = 0.01043154\n",
      "Iteration 479, loss = 0.00802423\n",
      "Iteration 465, loss = 0.01048053\n",
      "Iteration 480, loss = 0.00814837\n",
      "Iteration 466, loss = 0.00996360\n",
      "Iteration 481, loss = 0.00796404\n",
      "Iteration 467, loss = 0.00998174\n",
      "Iteration 482, loss = 0.00812845\n",
      "Iteration 468, loss = 0.01174871\n",
      "Iteration 483, loss = 0.00797573\n",
      "Iteration 469, loss = 0.01104162\n",
      "Iteration 484, loss = 0.00793049\n",
      "Iteration 470, loss = 0.01321778\n",
      "Iteration 485, loss = 0.00812152\n",
      "Iteration 471, loss = 0.01029387\n",
      "Iteration 486, loss = 0.00792759\n",
      "Iteration 472, loss = 0.01146160\n",
      "Iteration 487, loss = 0.00789784\n",
      "Iteration 473, loss = 0.00924439\n",
      "Iteration 488, loss = 0.00771346\n",
      "Iteration 474, loss = 0.01210282\n",
      "Iteration 489, loss = 0.00751656\n",
      "Iteration 475, loss = 0.01181205\n",
      "Iteration 490, loss = 0.00760587\n",
      "Iteration 476, loss = 0.01024674\n",
      "Iteration 491, loss = 0.00764235\n",
      "Iteration 477, loss = 0.01085858\n",
      "Iteration 492, loss = 0.00771727\n",
      "Iteration 478, loss = 0.01059949\n",
      "Iteration 493, loss = 0.00751194\n",
      "Iteration 479, loss = 0.01052795\n",
      "Iteration 494, loss = 0.00726174\n",
      "Iteration 480, loss = 0.00947871\n",
      "Iteration 495, loss = 0.00768026\n",
      "Iteration 481, loss = 0.00989989\n",
      "Iteration 496, loss = 0.00750625\n",
      "Iteration 482, loss = 0.01013996\n",
      "Iteration 497, loss = 0.00701564\n",
      "Iteration 483, loss = 0.00920890\n",
      "Iteration 498, loss = 0.00744527\n",
      "Iteration 484, loss = 0.00905589\n",
      "Iteration 499, loss = 0.00698412\n",
      "Iteration 485, loss = 0.00903982\n",
      "Iteration 500, loss = 0.00700557\n",
      "Iteration 486, loss = 0.00947946\n",
      "Iteration 487, loss = 0.00945793\n",
      "Iteration 488, loss = 0.00909261\n",
      "Iteration 489, loss = 0.00961609\n",
      "Iteration 490, loss = 0.00921968\n",
      "Iteration 491, loss = 0.01061552\n",
      "Iteration 492, loss = 0.01050229\n",
      "Iteration 493, loss = 0.01111933\n",
      "Iteration 494, loss = 0.00867367\n",
      "Iteration 495, loss = 0.00978331\n",
      "Iteration 496, loss = 0.00868278\n",
      "Iteration 497, loss = 0.00927731\n",
      "Iteration 498, loss = 0.01018936\n",
      "Iteration 499, loss = 0.00895592\n",
      "Iteration 500, loss = 0.00834726\n",
      "Iteration 1, loss = 1.36433432\n",
      "Iteration 2, loss = 1.13162072\n",
      "Iteration 3, loss = 0.96830029\n",
      "Iteration 4, loss = 0.84571769\n",
      "Iteration 5, loss = 0.75687136\n",
      "Iteration 6, loss = 0.68697639\n",
      "Iteration 7, loss = 0.63522742\n",
      "Iteration 8, loss = 0.59124047\n",
      "Iteration 9, loss = 0.55766359\n",
      "Iteration 10, loss = 0.52880288\n",
      "Iteration 11, loss = 0.50426166\n",
      "Iteration 12, loss = 0.48361807\n",
      "Iteration 13, loss = 0.46462697\n",
      "Iteration 14, loss = 0.44987634\n",
      "Iteration 15, loss = 0.43499501\n",
      "Iteration 1, loss = 1.53238068\n",
      "Iteration 16, loss = 0.42228141\n",
      "Iteration 2, loss = 1.22510709\n",
      "Iteration 17, loss = 0.41157023\n",
      "Iteration 3, loss = 1.08134241\n",
      "Iteration 18, loss = 0.40154205\n",
      "Iteration 4, loss = 0.95706542\n",
      "Iteration 19, loss = 0.39244143\n",
      "Iteration 5, loss = 0.86131617\n",
      "Iteration 20, loss = 0.38376363\n",
      "Iteration 6, loss = 0.79226059\n",
      "Iteration 7, loss = 0.73326272\n",
      "Iteration 21, loss = 0.37622998\n",
      "Iteration 8, loss = 0.67821938\n",
      "Iteration 22, loss = 0.36893646\n",
      "Iteration 9, loss = 0.63444716\n",
      "Iteration 23, loss = 0.36232625\n",
      "Iteration 10, loss = 0.59756050\n",
      "Iteration 24, loss = 0.35593706\n",
      "Iteration 11, loss = 0.56557921\n",
      "Iteration 25, loss = 0.35011896\n",
      "Iteration 12, loss = 0.53745945\n",
      "Iteration 26, loss = 0.34450099\n",
      "Iteration 13, loss = 0.51390100\n",
      "Iteration 27, loss = 0.33982259\n",
      "Iteration 14, loss = 0.49384907\n",
      "Iteration 28, loss = 0.33481591\n",
      "Iteration 15, loss = 0.47546773\n",
      "Iteration 29, loss = 0.33059643\n",
      "Iteration 16, loss = 0.45977213\n",
      "Iteration 30, loss = 0.32508580\n",
      "Iteration 31, loss = 0.32111386Iteration 17, loss = 0.44532774\n",
      "\n",
      "Iteration 32, loss = 0.31659903\n",
      "Iteration 18, loss = 0.43286596\n",
      "Iteration 33, loss = 0.31308580\n",
      "Iteration 19, loss = 0.42083541\n",
      "Iteration 34, loss = 0.30852576\n",
      "Iteration 20, loss = 0.41034543\n",
      "Iteration 35, loss = 0.30472203\n",
      "Iteration 21, loss = 0.40037425\n",
      "Iteration 36, loss = 0.30083777\n",
      "Iteration 22, loss = 0.39151879\n",
      "Iteration 37, loss = 0.29764323\n",
      "Iteration 23, loss = 0.38322251\n",
      "Iteration 38, loss = 0.29411694\n",
      "Iteration 24, loss = 0.37644546\n",
      "Iteration 39, loss = 0.29097258\n",
      "Iteration 25, loss = 0.36869666\n",
      "Iteration 40, loss = 0.28761474\n",
      "Iteration 26, loss = 0.36209330\n",
      "Iteration 41, loss = 0.28464493\n",
      "Iteration 27, loss = 0.35556370\n",
      "Iteration 28, loss = 0.34959062\n",
      "Iteration 42, loss = 0.28125635\n",
      "Iteration 43, loss = 0.27854941\n",
      "Iteration 29, loss = 0.34399553\n",
      "Iteration 44, loss = 0.27599090\n",
      "Iteration 30, loss = 0.33849194\n",
      "Iteration 45, loss = 0.27276713\n",
      "Iteration 31, loss = 0.33361401\n",
      "Iteration 46, loss = 0.27029052\n",
      "Iteration 32, loss = 0.32870235\n",
      "Iteration 47, loss = 0.26723628\n",
      "Iteration 33, loss = 0.32385753\n",
      "Iteration 48, loss = 0.26461144Iteration 34, loss = 0.32003263\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.31459648\n",
      "Iteration 49, loss = 0.26227032\n",
      "Iteration 36, loss = 0.31102035\n",
      "Iteration 50, loss = 0.25964968\n",
      "Iteration 37, loss = 0.30683211\n",
      "Iteration 51, loss = 0.25730144\n",
      "Iteration 38, loss = 0.30293761\n",
      "Iteration 52, loss = 0.25500815\n",
      "Iteration 53, loss = 0.25233846\n",
      "Iteration 39, loss = 0.29911170\n",
      "Iteration 54, loss = 0.24984730\n",
      "Iteration 40, loss = 0.29573299\n",
      "Iteration 55, loss = 0.24901047\n",
      "Iteration 41, loss = 0.29181998\n",
      "Iteration 56, loss = 0.24593905\n",
      "Iteration 42, loss = 0.28926251\n",
      "Iteration 57, loss = 0.24370930\n",
      "Iteration 43, loss = 0.28572239\n",
      "Iteration 58, loss = 0.24185296\n",
      "Iteration 44, loss = 0.28215005\n",
      "Iteration 59, loss = 0.23997547\n",
      "Iteration 45, loss = 0.27980391\n",
      "Iteration 60, loss = 0.23817085\n",
      "Iteration 46, loss = 0.27620501\n",
      "Iteration 61, loss = 0.23535621\n",
      "Iteration 47, loss = 0.27347847\n",
      "Iteration 48, loss = 0.27086696\n",
      "Iteration 62, loss = 0.23290813\n",
      "Iteration 49, loss = 0.26721656\n",
      "Iteration 63, loss = 0.23261789\n",
      "Iteration 50, loss = 0.26534661\n",
      "Iteration 51, loss = 0.26271705\n",
      "Iteration 64, loss = 0.22961665\n",
      "Iteration 65, loss = 0.22835008\n",
      "Iteration 52, loss = 0.25979510\n",
      "Iteration 66, loss = 0.22486269\n",
      "Iteration 53, loss = 0.25670141\n",
      "Iteration 67, loss = 0.22304995\n",
      "Iteration 68, loss = 0.22185046\n",
      "Iteration 54, loss = 0.25500940\n",
      "Iteration 69, loss = 0.21902702\n",
      "Iteration 55, loss = 0.25162791\n",
      "Iteration 70, loss = 0.21742175\n",
      "Iteration 56, loss = 0.24911431\n",
      "Iteration 57, loss = 0.24747466\n",
      "Iteration 58, loss = 0.24483910\n",
      "Iteration 71, loss = 0.21591447\n",
      "Iteration 59, loss = 0.24246271\n",
      "Iteration 72, loss = 0.21438658\n",
      "Iteration 60, loss = 0.24060206\n",
      "Iteration 73, loss = 0.21151152\n",
      "Iteration 61, loss = 0.23897708\n",
      "Iteration 74, loss = 0.20994228\n",
      "Iteration 62, loss = 0.23617956\n",
      "Iteration 75, loss = 0.20823336\n",
      "Iteration 63, loss = 0.23459746\n",
      "Iteration 76, loss = 0.20600206\n",
      "Iteration 64, loss = 0.23234180\n",
      "Iteration 77, loss = 0.20452644\n",
      "Iteration 65, loss = 0.22973293\n",
      "Iteration 78, loss = 0.20254459\n",
      "Iteration 66, loss = 0.22740583\n",
      "Iteration 79, loss = 0.20272367\n",
      "Iteration 67, loss = 0.22570578\n",
      "Iteration 80, loss = 0.19981287\n",
      "Iteration 68, loss = 0.22397734\n",
      "Iteration 81, loss = 0.19794029\n",
      "Iteration 69, loss = 0.22224517\n",
      "Iteration 82, loss = 0.20015872\n",
      "Iteration 70, loss = 0.22023216\n",
      "Iteration 71, loss = 0.21802443\n",
      "Iteration 72, loss = 0.21586284\n",
      "Iteration 83, loss = 0.19578309\n",
      "Iteration 73, loss = 0.21410890\n",
      "Iteration 84, loss = 0.19446697\n",
      "Iteration 74, loss = 0.21242695\n",
      "Iteration 85, loss = 0.19203777\n",
      "Iteration 75, loss = 0.21108343\n",
      "Iteration 86, loss = 0.18983031\n",
      "Iteration 76, loss = 0.20913080\n",
      "Iteration 87, loss = 0.18792491\n",
      "Iteration 77, loss = 0.20676142\n",
      "Iteration 88, loss = 0.18737432\n",
      "Iteration 78, loss = 0.20511325\n",
      "Iteration 89, loss = 0.18483851\n",
      "Iteration 90, loss = 0.18368617\n",
      "Iteration 79, loss = 0.20554611\n",
      "Iteration 91, loss = 0.18133234\n",
      "Iteration 80, loss = 0.20179156\n",
      "Iteration 92, loss = 0.18036463\n",
      "Iteration 81, loss = 0.20149006\n",
      "Iteration 93, loss = 0.17847024\n",
      "Iteration 82, loss = 0.19782561\n",
      "Iteration 94, loss = 0.17682805\n",
      "Iteration 83, loss = 0.20106242\n",
      "Iteration 95, loss = 0.17573694\n",
      "Iteration 84, loss = 0.19551352\n",
      "Iteration 96, loss = 0.17511547\n",
      "Iteration 85, loss = 0.19353576\n",
      "Iteration 97, loss = 0.17309838\n",
      "Iteration 98, loss = 0.17219408\n",
      "Iteration 86, loss = 0.19149491\n",
      "Iteration 99, loss = 0.17030402\n",
      "Iteration 87, loss = 0.19127508\n",
      "Iteration 100, loss = 0.16831466\n",
      "Iteration 88, loss = 0.19153005\n",
      "Iteration 101, loss = 0.16611548\n",
      "Iteration 89, loss = 0.18667052\n",
      "Iteration 102, loss = 0.16573495\n",
      "Iteration 90, loss = 0.18655443\n",
      "Iteration 103, loss = 0.16467539\n",
      "Iteration 91, loss = 0.18333727\n",
      "Iteration 104, loss = 0.16352464\n",
      "Iteration 92, loss = 0.18247801\n",
      "Iteration 93, loss = 0.17986346\n",
      "Iteration 105, loss = 0.16172270\n",
      "Iteration 106, loss = 0.15910400\n",
      "Iteration 94, loss = 0.17892472\n",
      "Iteration 107, loss = 0.15937154\n",
      "Iteration 95, loss = 0.17885887\n",
      "Iteration 108, loss = 0.15599043\n",
      "Iteration 96, loss = 0.17609350\n",
      "Iteration 109, loss = 0.15688256\n",
      "Iteration 97, loss = 0.17615168\n",
      "Iteration 110, loss = 0.15482514\n",
      "Iteration 98, loss = 0.17378323\n",
      "Iteration 111, loss = 0.15493097\n",
      "Iteration 99, loss = 0.17276044\n",
      "Iteration 112, loss = 0.15150477\n",
      "Iteration 100, loss = 0.16984183\n",
      "Iteration 101, loss = 0.17039004\n",
      "Iteration 113, loss = 0.15088683\n",
      "Iteration 102, loss = 0.16798308\n",
      "Iteration 114, loss = 0.14892321\n",
      "Iteration 103, loss = 0.16632153\n",
      "Iteration 115, loss = 0.14731561\n",
      "Iteration 116, loss = 0.14767604\n",
      "Iteration 104, loss = 0.16487149\n",
      "Iteration 117, loss = 0.14762881\n",
      "Iteration 105, loss = 0.16511285\n",
      "Iteration 118, loss = 0.14459622\n",
      "Iteration 106, loss = 0.16114927\n",
      "Iteration 119, loss = 0.14435052\n",
      "Iteration 107, loss = 0.16029162\n",
      "Iteration 120, loss = 0.14605586\n",
      "Iteration 108, loss = 0.16058958\n",
      "Iteration 121, loss = 0.14195935\n",
      "Iteration 109, loss = 0.15608529\n",
      "Iteration 122, loss = 0.14030588\n",
      "Iteration 110, loss = 0.15678215\n",
      "Iteration 123, loss = 0.13926158\n",
      "Iteration 111, loss = 0.15501008\n",
      "Iteration 124, loss = 0.13736392\n",
      "Iteration 112, loss = 0.15344765\n",
      "Iteration 125, loss = 0.13843462\n",
      "Iteration 113, loss = 0.15337170\n",
      "Iteration 126, loss = 0.13278788\n",
      "Iteration 114, loss = 0.15041275\n",
      "Iteration 127, loss = 0.13491170\n",
      "Iteration 115, loss = 0.14944514\n",
      "Iteration 128, loss = 0.13023516\n",
      "Iteration 116, loss = 0.14828256\n",
      "Iteration 129, loss = 0.13078959\n",
      "Iteration 117, loss = 0.14696690\n",
      "Iteration 130, loss = 0.12949612\n",
      "Iteration 118, loss = 0.14592288\n",
      "Iteration 131, loss = 0.12821328\n",
      "Iteration 119, loss = 0.14405666\n",
      "Iteration 132, loss = 0.12680647\n",
      "Iteration 120, loss = 0.14495977\n",
      "Iteration 133, loss = 0.12567548\n",
      "Iteration 121, loss = 0.14218449\n",
      "Iteration 134, loss = 0.12453030\n",
      "Iteration 122, loss = 0.14185762\n",
      "Iteration 135, loss = 0.12464565\n",
      "Iteration 123, loss = 0.13993600\n",
      "Iteration 136, loss = 0.12196043\n",
      "Iteration 124, loss = 0.14204948\n",
      "Iteration 137, loss = 0.12099591\n",
      "Iteration 125, loss = 0.13803413\n",
      "Iteration 138, loss = 0.12260480\n",
      "Iteration 126, loss = 0.13741841\n",
      "Iteration 139, loss = 0.11937356\n",
      "Iteration 127, loss = 0.13549263\n",
      "Iteration 128, loss = 0.13540870\n",
      "Iteration 140, loss = 0.12140871\n",
      "Iteration 129, loss = 0.13302004\n",
      "Iteration 141, loss = 0.11747593\n",
      "Iteration 130, loss = 0.13275590\n",
      "Iteration 142, loss = 0.11622027\n",
      "Iteration 131, loss = 0.13218053\n",
      "Iteration 143, loss = 0.11545461\n",
      "Iteration 132, loss = 0.13134328\n",
      "Iteration 144, loss = 0.11414194\n",
      "Iteration 133, loss = 0.12980058\n",
      "Iteration 145, loss = 0.11514573\n",
      "Iteration 146, loss = 0.11427044\n",
      "Iteration 134, loss = 0.12793144\n",
      "Iteration 147, loss = 0.11349588\n",
      "Iteration 135, loss = 0.12780081\n",
      "Iteration 148, loss = 0.11228978\n",
      "Iteration 136, loss = 0.12818732\n",
      "Iteration 149, loss = 0.11044677\n",
      "Iteration 137, loss = 0.12702789\n",
      "Iteration 150, loss = 0.11078519\n",
      "Iteration 138, loss = 0.12548534\n",
      "Iteration 151, loss = 0.10706057\n",
      "Iteration 139, loss = 0.12572169\n",
      "Iteration 152, loss = 0.10819187\n",
      "Iteration 140, loss = 0.12574069\n",
      "Iteration 153, loss = 0.10911951\n",
      "Iteration 141, loss = 0.12332055\n",
      "Iteration 154, loss = 0.10581478\n",
      "Iteration 142, loss = 0.12174394\n",
      "Iteration 155, loss = 0.10727385\n",
      "Iteration 143, loss = 0.12024050\n",
      "Iteration 156, loss = 0.10463509\n",
      "Iteration 144, loss = 0.11976934\n",
      "Iteration 157, loss = 0.10385138\n",
      "Iteration 145, loss = 0.11867976\n",
      "Iteration 158, loss = 0.10191625\n",
      "Iteration 146, loss = 0.11739024\n",
      "Iteration 159, loss = 0.10182544\n",
      "Iteration 147, loss = 0.11628061\n",
      "Iteration 160, loss = 0.10187976\n",
      "Iteration 148, loss = 0.11655892\n",
      "Iteration 161, loss = 0.09951633\n",
      "Iteration 149, loss = 0.11497911\n",
      "Iteration 162, loss = 0.10185040\n",
      "Iteration 150, loss = 0.11435331\n",
      "Iteration 163, loss = 0.09863701\n",
      "Iteration 151, loss = 0.11411622\n",
      "Iteration 164, loss = 0.09877763\n",
      "Iteration 152, loss = 0.11602639\n",
      "Iteration 165, loss = 0.09864510\n",
      "Iteration 153, loss = 0.11369529\n",
      "Iteration 166, loss = 0.09490874\n",
      "Iteration 154, loss = 0.11296083\n",
      "Iteration 167, loss = 0.09534027\n",
      "Iteration 155, loss = 0.11182419\n",
      "Iteration 168, loss = 0.09526674\n",
      "Iteration 156, loss = 0.10996497\n",
      "Iteration 169, loss = 0.09400200\n",
      "Iteration 157, loss = 0.10918717\n",
      "Iteration 170, loss = 0.09430696\n",
      "Iteration 158, loss = 0.11141590\n",
      "Iteration 171, loss = 0.09298283\n",
      "Iteration 159, loss = 0.10742060\n",
      "Iteration 172, loss = 0.09232667\n",
      "Iteration 160, loss = 0.10781637\n",
      "Iteration 173, loss = 0.09467943\n",
      "Iteration 161, loss = 0.10714725\n",
      "Iteration 174, loss = 0.09041492\n",
      "Iteration 162, loss = 0.10752387\n",
      "Iteration 175, loss = 0.09075895\n",
      "Iteration 163, loss = 0.10384707\n",
      "Iteration 176, loss = 0.08878729\n",
      "Iteration 164, loss = 0.10518466\n",
      "Iteration 177, loss = 0.08729393\n",
      "Iteration 165, loss = 0.10512416\n",
      "Iteration 178, loss = 0.08766573\n",
      "Iteration 166, loss = 0.10447924\n",
      "Iteration 179, loss = 0.08751954\n",
      "Iteration 167, loss = 0.10489162\n",
      "Iteration 180, loss = 0.08581201\n",
      "Iteration 168, loss = 0.10434084\n",
      "Iteration 181, loss = 0.08627392\n",
      "Iteration 169, loss = 0.09970372\n",
      "Iteration 182, loss = 0.08709140\n",
      "Iteration 170, loss = 0.10239171\n",
      "Iteration 183, loss = 0.08586744\n",
      "Iteration 171, loss = 0.09950375\n",
      "Iteration 184, loss = 0.08442941\n",
      "Iteration 172, loss = 0.09799998\n",
      "Iteration 185, loss = 0.08210604\n",
      "Iteration 173, loss = 0.09750875\n",
      "Iteration 186, loss = 0.08362171\n",
      "Iteration 174, loss = 0.09692369\n",
      "Iteration 187, loss = 0.08521896\n",
      "Iteration 175, loss = 0.09618604\n",
      "Iteration 188, loss = 0.08118879\n",
      "Iteration 176, loss = 0.09550181\n",
      "Iteration 189, loss = 0.08110327\n",
      "Iteration 177, loss = 0.09524788\n",
      "Iteration 190, loss = 0.07960210\n",
      "Iteration 178, loss = 0.09397427\n",
      "Iteration 191, loss = 0.07947146\n",
      "Iteration 179, loss = 0.09418204\n",
      "Iteration 192, loss = 0.07836595\n",
      "Iteration 180, loss = 0.09416857\n",
      "Iteration 193, loss = 0.07715983\n",
      "Iteration 181, loss = 0.09270215\n",
      "Iteration 194, loss = 0.07647678\n",
      "Iteration 182, loss = 0.09314365\n",
      "Iteration 195, loss = 0.07655554\n",
      "Iteration 183, loss = 0.09170830\n",
      "Iteration 196, loss = 0.07657643\n",
      "Iteration 184, loss = 0.09125053\n",
      "Iteration 197, loss = 0.07594227\n",
      "Iteration 185, loss = 0.08933876\n",
      "Iteration 198, loss = 0.07394797\n",
      "Iteration 186, loss = 0.08964513\n",
      "Iteration 199, loss = 0.07689250\n",
      "Iteration 187, loss = 0.08873081\n",
      "Iteration 200, loss = 0.07686622\n",
      "Iteration 188, loss = 0.08780990\n",
      "Iteration 201, loss = 0.07383202\n",
      "Iteration 189, loss = 0.08783438\n",
      "Iteration 202, loss = 0.07453695\n",
      "Iteration 190, loss = 0.08720515\n",
      "Iteration 203, loss = 0.07205065\n",
      "Iteration 191, loss = 0.08657040\n",
      "Iteration 204, loss = 0.07211025\n",
      "Iteration 192, loss = 0.08565298\n",
      "Iteration 205, loss = 0.07127876\n",
      "Iteration 193, loss = 0.08571462\n",
      "Iteration 206, loss = 0.07028751\n",
      "Iteration 194, loss = 0.08585415\n",
      "Iteration 207, loss = 0.07238259\n",
      "Iteration 195, loss = 0.08459482\n",
      "Iteration 208, loss = 0.06936920\n",
      "Iteration 196, loss = 0.08358633\n",
      "Iteration 209, loss = 0.06991903\n",
      "Iteration 197, loss = 0.08458915\n",
      "Iteration 210, loss = 0.06775672\n",
      "Iteration 198, loss = 0.08281659\n",
      "Iteration 211, loss = 0.06806913\n",
      "Iteration 199, loss = 0.08577203\n",
      "Iteration 212, loss = 0.06891690\n",
      "Iteration 200, loss = 0.08351386\n",
      "Iteration 213, loss = 0.06568932\n",
      "Iteration 201, loss = 0.08566832\n",
      "Iteration 214, loss = 0.06833098\n",
      "Iteration 202, loss = 0.08486022\n",
      "Iteration 215, loss = 0.06704656\n",
      "Iteration 203, loss = 0.08090046\n",
      "Iteration 216, loss = 0.06403657\n",
      "Iteration 204, loss = 0.08070903\n",
      "Iteration 217, loss = 0.06665971\n",
      "Iteration 205, loss = 0.08111735\n",
      "Iteration 218, loss = 0.06774017\n",
      "Iteration 206, loss = 0.08108907\n",
      "Iteration 219, loss = 0.06448576\n",
      "Iteration 207, loss = 0.08004159\n",
      "Iteration 220, loss = 0.06249097\n",
      "Iteration 208, loss = 0.07808479\n",
      "Iteration 221, loss = 0.06244495\n",
      "Iteration 209, loss = 0.07704783\n",
      "Iteration 222, loss = 0.06156970\n",
      "Iteration 210, loss = 0.07733103\n",
      "Iteration 223, loss = 0.06149953\n",
      "Iteration 211, loss = 0.07537076\n",
      "Iteration 224, loss = 0.06037085\n",
      "Iteration 212, loss = 0.07558690\n",
      "Iteration 225, loss = 0.05972597\n",
      "Iteration 213, loss = 0.07545120\n",
      "Iteration 226, loss = 0.05979235\n",
      "Iteration 214, loss = 0.07768202\n",
      "Iteration 227, loss = 0.05940216\n",
      "Iteration 215, loss = 0.07411143\n",
      "Iteration 228, loss = 0.05825943\n",
      "Iteration 216, loss = 0.07630287\n",
      "Iteration 229, loss = 0.05786043\n",
      "Iteration 217, loss = 0.07436190\n",
      "Iteration 230, loss = 0.05778221\n",
      "Iteration 218, loss = 0.07341571\n",
      "Iteration 231, loss = 0.05650788\n",
      "Iteration 219, loss = 0.07390842\n",
      "Iteration 232, loss = 0.05658455\n",
      "Iteration 220, loss = 0.07265650\n",
      "Iteration 233, loss = 0.05657502\n",
      "Iteration 221, loss = 0.07097328\n",
      "Iteration 234, loss = 0.05566692\n",
      "Iteration 222, loss = 0.07186298\n",
      "Iteration 235, loss = 0.05556547\n",
      "Iteration 223, loss = 0.07077881\n",
      "Iteration 236, loss = 0.05558071\n",
      "Iteration 224, loss = 0.06986461\n",
      "Iteration 237, loss = 0.05538876\n",
      "Iteration 225, loss = 0.07055100\n",
      "Iteration 238, loss = 0.05546345\n",
      "Iteration 226, loss = 0.06827871\n",
      "Iteration 239, loss = 0.05379723\n",
      "Iteration 227, loss = 0.07030705\n",
      "Iteration 240, loss = 0.05501908\n",
      "Iteration 228, loss = 0.06815371\n",
      "Iteration 241, loss = 0.05409213\n",
      "Iteration 229, loss = 0.06893238\n",
      "Iteration 242, loss = 0.05303789\n",
      "Iteration 230, loss = 0.06910630\n",
      "Iteration 243, loss = 0.05159761\n",
      "Iteration 231, loss = 0.06796586\n",
      "Iteration 244, loss = 0.05358836\n",
      "Iteration 232, loss = 0.06647205\n",
      "Iteration 245, loss = 0.05547320\n",
      "Iteration 233, loss = 0.06609639\n",
      "Iteration 246, loss = 0.05118495\n",
      "Iteration 234, loss = 0.06631836\n",
      "Iteration 247, loss = 0.05118560\n",
      "Iteration 235, loss = 0.06575145\n",
      "Iteration 248, loss = 0.05231876\n",
      "Iteration 236, loss = 0.06575644\n",
      "Iteration 249, loss = 0.05019643\n",
      "Iteration 237, loss = 0.06547534\n",
      "Iteration 250, loss = 0.05007208\n",
      "Iteration 238, loss = 0.06653022\n",
      "Iteration 251, loss = 0.05017069\n",
      "Iteration 239, loss = 0.06622713\n",
      "Iteration 252, loss = 0.04874708\n",
      "Iteration 240, loss = 0.06685081\n",
      "Iteration 253, loss = 0.04801184\n",
      "Iteration 241, loss = 0.06782231\n",
      "Iteration 254, loss = 0.04783486\n",
      "Iteration 242, loss = 0.06194308\n",
      "Iteration 255, loss = 0.04823233\n",
      "Iteration 243, loss = 0.06559545\n",
      "Iteration 256, loss = 0.04710188\n",
      "Iteration 244, loss = 0.06257301\n",
      "Iteration 257, loss = 0.04691953\n",
      "Iteration 245, loss = 0.06295749\n",
      "Iteration 258, loss = 0.04700595\n",
      "Iteration 246, loss = 0.06189594\n",
      "Iteration 259, loss = 0.04619085\n",
      "Iteration 247, loss = 0.06107472\n",
      "Iteration 260, loss = 0.04530071\n",
      "Iteration 248, loss = 0.06171241\n",
      "Iteration 261, loss = 0.04573807\n",
      "Iteration 249, loss = 0.06214188\n",
      "Iteration 262, loss = 0.04538322\n",
      "Iteration 250, loss = 0.06272878\n",
      "Iteration 263, loss = 0.04452394\n",
      "Iteration 251, loss = 0.05783090\n",
      "Iteration 264, loss = 0.04545806\n",
      "Iteration 252, loss = 0.05937368\n",
      "Iteration 265, loss = 0.04460201\n",
      "Iteration 253, loss = 0.05898613\n",
      "Iteration 266, loss = 0.04429275\n",
      "Iteration 254, loss = 0.05968996\n",
      "Iteration 267, loss = 0.04473368\n",
      "Iteration 255, loss = 0.06140127\n",
      "Iteration 268, loss = 0.04268109\n",
      "Iteration 256, loss = 0.06474958\n",
      "Iteration 269, loss = 0.04287997\n",
      "Iteration 257, loss = 0.05701828\n",
      "Iteration 270, loss = 0.04660038\n",
      "Iteration 258, loss = 0.05833181\n",
      "Iteration 271, loss = 0.04349007\n",
      "Iteration 259, loss = 0.05885567\n",
      "Iteration 272, loss = 0.04416008\n",
      "Iteration 260, loss = 0.05592593\n",
      "Iteration 273, loss = 0.04306218\n",
      "Iteration 261, loss = 0.05575937\n",
      "Iteration 274, loss = 0.04242334\n",
      "Iteration 262, loss = 0.05629626\n",
      "Iteration 275, loss = 0.04034189\n",
      "Iteration 263, loss = 0.05586121\n",
      "Iteration 276, loss = 0.04121263\n",
      "Iteration 264, loss = 0.05570957\n",
      "Iteration 277, loss = 0.04141006\n",
      "Iteration 265, loss = 0.05419773\n",
      "Iteration 278, loss = 0.03988661\n",
      "Iteration 266, loss = 0.05346603\n",
      "Iteration 279, loss = 0.03907779\n",
      "Iteration 267, loss = 0.05311100\n",
      "Iteration 280, loss = 0.03829191\n",
      "Iteration 268, loss = 0.05256836\n",
      "Iteration 281, loss = 0.03910080\n",
      "Iteration 269, loss = 0.05302449\n",
      "Iteration 282, loss = 0.03868238\n",
      "Iteration 270, loss = 0.05279453\n",
      "Iteration 283, loss = 0.03864941\n",
      "Iteration 271, loss = 0.05129189\n",
      "Iteration 284, loss = 0.03790603\n",
      "Iteration 272, loss = 0.05116747\n",
      "Iteration 285, loss = 0.03716994\n",
      "Iteration 273, loss = 0.05066849\n",
      "Iteration 286, loss = 0.03669159\n",
      "Iteration 274, loss = 0.05105354\n",
      "Iteration 287, loss = 0.03663834\n",
      "Iteration 275, loss = 0.05030479\n",
      "Iteration 288, loss = 0.03665109\n",
      "Iteration 276, loss = 0.04981971\n",
      "Iteration 289, loss = 0.03585415\n",
      "Iteration 277, loss = 0.04959353\n",
      "Iteration 290, loss = 0.03672699\n",
      "Iteration 278, loss = 0.05068374\n",
      "Iteration 291, loss = 0.03546000\n",
      "Iteration 279, loss = 0.05165284\n",
      "Iteration 292, loss = 0.03502937\n",
      "Iteration 280, loss = 0.05159205\n",
      "Iteration 293, loss = 0.03495233\n",
      "Iteration 281, loss = 0.04931063\n",
      "Iteration 294, loss = 0.03482711\n",
      "Iteration 282, loss = 0.05169595\n",
      "Iteration 295, loss = 0.03447917\n",
      "Iteration 283, loss = 0.04820651\n",
      "Iteration 296, loss = 0.03621373\n",
      "Iteration 284, loss = 0.04776035\n",
      "Iteration 297, loss = 0.03502693\n",
      "Iteration 285, loss = 0.04735455\n",
      "Iteration 298, loss = 0.03400721\n",
      "Iteration 286, loss = 0.04667683\n",
      "Iteration 299, loss = 0.03405182\n",
      "Iteration 287, loss = 0.04799659\n",
      "Iteration 300, loss = 0.03403168\n",
      "Iteration 288, loss = 0.04802404\n",
      "Iteration 301, loss = 0.03247498\n",
      "Iteration 289, loss = 0.04685550\n",
      "Iteration 302, loss = 0.03302389\n",
      "Iteration 290, loss = 0.04709946\n",
      "Iteration 303, loss = 0.03281096\n",
      "Iteration 291, loss = 0.04594512\n",
      "Iteration 304, loss = 0.03318114\n",
      "Iteration 292, loss = 0.04475755\n",
      "Iteration 305, loss = 0.03175960\n",
      "Iteration 293, loss = 0.04706384\n",
      "Iteration 306, loss = 0.03262610\n",
      "Iteration 294, loss = 0.04821258\n",
      "Iteration 307, loss = 0.03121324\n",
      "Iteration 295, loss = 0.04763777\n",
      "Iteration 308, loss = 0.03172958\n",
      "Iteration 296, loss = 0.04457014\n",
      "Iteration 309, loss = 0.03191655\n",
      "Iteration 297, loss = 0.04938761\n",
      "Iteration 310, loss = 0.03064510\n",
      "Iteration 298, loss = 0.04640905\n",
      "Iteration 311, loss = 0.03109674\n",
      "Iteration 299, loss = 0.04648834\n",
      "Iteration 312, loss = 0.03033281\n",
      "Iteration 300, loss = 0.04467233\n",
      "Iteration 313, loss = 0.03040780\n",
      "Iteration 301, loss = 0.04532672\n",
      "Iteration 314, loss = 0.03039051\n",
      "Iteration 302, loss = 0.04315010\n",
      "Iteration 315, loss = 0.02960645\n",
      "Iteration 303, loss = 0.04232592\n",
      "Iteration 316, loss = 0.02909218\n",
      "Iteration 304, loss = 0.04316811\n",
      "Iteration 317, loss = 0.02928921\n",
      "Iteration 305, loss = 0.04251332\n",
      "Iteration 318, loss = 0.02866111\n",
      "Iteration 306, loss = 0.04068881\n",
      "Iteration 319, loss = 0.02897864\n",
      "Iteration 307, loss = 0.04282170\n",
      "Iteration 320, loss = 0.02853771\n",
      "Iteration 308, loss = 0.04051917\n",
      "Iteration 321, loss = 0.02907126\n",
      "Iteration 309, loss = 0.04112012\n",
      "Iteration 322, loss = 0.03021834\n",
      "Iteration 310, loss = 0.04207463\n",
      "Iteration 323, loss = 0.02848600\n",
      "Iteration 311, loss = 0.04208152\n",
      "Iteration 324, loss = 0.03079132\n",
      "Iteration 312, loss = 0.04223436\n",
      "Iteration 325, loss = 0.02759351\n",
      "Iteration 313, loss = 0.03979718\n",
      "Iteration 326, loss = 0.02718498\n",
      "Iteration 314, loss = 0.03979443\n",
      "Iteration 327, loss = 0.02785520\n",
      "Iteration 315, loss = 0.04036560\n",
      "Iteration 328, loss = 0.02767134\n",
      "Iteration 316, loss = 0.03954274\n",
      "Iteration 329, loss = 0.02696947\n",
      "Iteration 330, loss = 0.02645970\n",
      "Iteration 317, loss = 0.04017561\n",
      "Iteration 331, loss = 0.02701659\n",
      "Iteration 318, loss = 0.03890028\n",
      "Iteration 332, loss = 0.02654747\n",
      "Iteration 319, loss = 0.03918365\n",
      "Iteration 320, loss = 0.03758159\n",
      "Iteration 333, loss = 0.02615024\n",
      "Iteration 321, loss = 0.03878489\n",
      "Iteration 334, loss = 0.02593496\n",
      "Iteration 322, loss = 0.03787148\n",
      "Iteration 335, loss = 0.02572721\n",
      "Iteration 323, loss = 0.03804803\n",
      "Iteration 336, loss = 0.02566616\n",
      "Iteration 324, loss = 0.03633774\n",
      "Iteration 337, loss = 0.02434550\n",
      "Iteration 325, loss = 0.03717211\n",
      "Iteration 338, loss = 0.02604923\n",
      "Iteration 326, loss = 0.03734101\n",
      "Iteration 339, loss = 0.02604557\n",
      "Iteration 327, loss = 0.03699335\n",
      "Iteration 340, loss = 0.02491154\n",
      "Iteration 328, loss = 0.03594288\n",
      "Iteration 341, loss = 0.02451125\n",
      "Iteration 329, loss = 0.03651520\n",
      "Iteration 342, loss = 0.02460837\n",
      "Iteration 330, loss = 0.03605132\n",
      "Iteration 343, loss = 0.02496162\n",
      "Iteration 331, loss = 0.03621377\n",
      "Iteration 344, loss = 0.02391349\n",
      "Iteration 345, loss = 0.02392301\n",
      "Iteration 332, loss = 0.03472411\n",
      "Iteration 346, loss = 0.02337200\n",
      "Iteration 333, loss = 0.03650244\n",
      "Iteration 347, loss = 0.02370073\n",
      "Iteration 334, loss = 0.03467940\n",
      "Iteration 348, loss = 0.02613352\n",
      "Iteration 335, loss = 0.03538218\n",
      "Iteration 349, loss = 0.02405475\n",
      "Iteration 336, loss = 0.03603386\n",
      "Iteration 350, loss = 0.02458780\n",
      "Iteration 337, loss = 0.03815964\n",
      "Iteration 351, loss = 0.02288126\n",
      "Iteration 338, loss = 0.04044452\n",
      "Iteration 352, loss = 0.02338857\n",
      "Iteration 339, loss = 0.03762360\n",
      "Iteration 353, loss = 0.02276950\n",
      "Iteration 340, loss = 0.03746239\n",
      "Iteration 354, loss = 0.02217461\n",
      "Iteration 341, loss = 0.03545330\n",
      "Iteration 355, loss = 0.02180333\n",
      "Iteration 342, loss = 0.03384563\n",
      "Iteration 356, loss = 0.02165254\n",
      "Iteration 343, loss = 0.03529916\n",
      "Iteration 357, loss = 0.02191650\n",
      "Iteration 344, loss = 0.03788879\n",
      "Iteration 358, loss = 0.02189068\n",
      "Iteration 345, loss = 0.03285087\n",
      "Iteration 359, loss = 0.02245607\n",
      "Iteration 346, loss = 0.03590479\n",
      "Iteration 360, loss = 0.02091846\n",
      "Iteration 347, loss = 0.03280041\n",
      "Iteration 361, loss = 0.02153418\n",
      "Iteration 348, loss = 0.03118119\n",
      "Iteration 362, loss = 0.02180563\n",
      "Iteration 349, loss = 0.03195725\n",
      "Iteration 363, loss = 0.02312650\n",
      "Iteration 350, loss = 0.03191037\n",
      "Iteration 364, loss = 0.02036349\n",
      "Iteration 351, loss = 0.03166122\n",
      "Iteration 365, loss = 0.02112040\n",
      "Iteration 352, loss = 0.03234298\n",
      "Iteration 366, loss = 0.02066771\n",
      "Iteration 353, loss = 0.03080432\n",
      "Iteration 367, loss = 0.02021081\n",
      "Iteration 354, loss = 0.03102669\n",
      "Iteration 368, loss = 0.02043315\n",
      "Iteration 355, loss = 0.03102048\n",
      "Iteration 369, loss = 0.02128860\n",
      "Iteration 356, loss = 0.03024225\n",
      "Iteration 370, loss = 0.02015307\n",
      "Iteration 357, loss = 0.02988169\n",
      "Iteration 371, loss = 0.02060154\n",
      "Iteration 358, loss = 0.02987597\n",
      "Iteration 372, loss = 0.02220365\n",
      "Iteration 359, loss = 0.02925314\n",
      "Iteration 373, loss = 0.02113562\n",
      "Iteration 360, loss = 0.02946484\n",
      "Iteration 374, loss = 0.02212840\n",
      "Iteration 361, loss = 0.02942633\n",
      "Iteration 375, loss = 0.02188026\n",
      "Iteration 362, loss = 0.02925958\n",
      "Iteration 376, loss = 0.01965684\n",
      "Iteration 363, loss = 0.02914613\n",
      "Iteration 377, loss = 0.01891225\n",
      "Iteration 364, loss = 0.03011414\n",
      "Iteration 378, loss = 0.01882546\n",
      "Iteration 365, loss = 0.02910547\n",
      "Iteration 379, loss = 0.01882264\n",
      "Iteration 366, loss = 0.03077608\n",
      "Iteration 380, loss = 0.01815015\n",
      "Iteration 367, loss = 0.02977470\n",
      "Iteration 381, loss = 0.01822330\n",
      "Iteration 368, loss = 0.02877197\n",
      "Iteration 382, loss = 0.01817721\n",
      "Iteration 369, loss = 0.02808342\n",
      "Iteration 383, loss = 0.01885572\n",
      "Iteration 370, loss = 0.02838980\n",
      "Iteration 384, loss = 0.02000735\n",
      "Iteration 371, loss = 0.02946506\n",
      "Iteration 385, loss = 0.02107844\n",
      "Iteration 372, loss = 0.02844246\n",
      "Iteration 386, loss = 0.01827496\n",
      "Iteration 373, loss = 0.02746502\n",
      "Iteration 387, loss = 0.01863835\n",
      "Iteration 374, loss = 0.02748321\n",
      "Iteration 388, loss = 0.01789194\n",
      "Iteration 375, loss = 0.02721480\n",
      "Iteration 389, loss = 0.01981337\n",
      "Iteration 376, loss = 0.02671971\n",
      "Iteration 390, loss = 0.01935778\n",
      "Iteration 377, loss = 0.02715563\n",
      "Iteration 391, loss = 0.01897673\n",
      "Iteration 378, loss = 0.02744332\n",
      "Iteration 392, loss = 0.01970721\n",
      "Iteration 379, loss = 0.02650596\n",
      "Iteration 393, loss = 0.01717371\n",
      "Iteration 380, loss = 0.02711879\n",
      "Iteration 394, loss = 0.01774214\n",
      "Iteration 381, loss = 0.02723551\n",
      "Iteration 395, loss = 0.01850732\n",
      "Iteration 382, loss = 0.02653064\n",
      "Iteration 396, loss = 0.02000733\n",
      "Iteration 383, loss = 0.02663029\n",
      "Iteration 397, loss = 0.01645128\n",
      "Iteration 384, loss = 0.02701454\n",
      "Iteration 398, loss = 0.01634584\n",
      "Iteration 385, loss = 0.02533131\n",
      "Iteration 399, loss = 0.01696314\n",
      "Iteration 386, loss = 0.02582662\n",
      "Iteration 400, loss = 0.01627428\n",
      "Iteration 387, loss = 0.02701520\n",
      "Iteration 401, loss = 0.01678509\n",
      "Iteration 388, loss = 0.02962082\n",
      "Iteration 402, loss = 0.01563261\n",
      "Iteration 389, loss = 0.02837368\n",
      "Iteration 403, loss = 0.01564274\n",
      "Iteration 390, loss = 0.02665266\n",
      "Iteration 404, loss = 0.01608846\n",
      "Iteration 391, loss = 0.02779202\n",
      "Iteration 405, loss = 0.01559776\n",
      "Iteration 392, loss = 0.02524799\n",
      "Iteration 406, loss = 0.01498460\n",
      "Iteration 393, loss = 0.02677458\n",
      "Iteration 407, loss = 0.01638985\n",
      "Iteration 394, loss = 0.02388931\n",
      "Iteration 408, loss = 0.01491090\n",
      "Iteration 395, loss = 0.02546140\n",
      "Iteration 409, loss = 0.01519954\n",
      "Iteration 396, loss = 0.02573124\n",
      "Iteration 410, loss = 0.01516805\n",
      "Iteration 397, loss = 0.02469306\n",
      "Iteration 411, loss = 0.01494736\n",
      "Iteration 398, loss = 0.02620609\n",
      "Iteration 412, loss = 0.01459390\n",
      "Iteration 399, loss = 0.02731413\n",
      "Iteration 413, loss = 0.01455096\n",
      "Iteration 400, loss = 0.02482941\n",
      "Iteration 414, loss = 0.01439698\n",
      "Iteration 401, loss = 0.02387187\n",
      "Iteration 415, loss = 0.01435176\n",
      "Iteration 402, loss = 0.02437176\n",
      "Iteration 416, loss = 0.01449652\n",
      "Iteration 403, loss = 0.02459693\n",
      "Iteration 417, loss = 0.01428255\n",
      "Iteration 404, loss = 0.02324460\n",
      "Iteration 418, loss = 0.01454277\n",
      "Iteration 405, loss = 0.02296536\n",
      "Iteration 419, loss = 0.01557888\n",
      "Iteration 406, loss = 0.02194131\n",
      "Iteration 420, loss = 0.01404440\n",
      "Iteration 407, loss = 0.02323141\n",
      "Iteration 421, loss = 0.01378813\n",
      "Iteration 408, loss = 0.02234118\n",
      "Iteration 422, loss = 0.01432009\n",
      "Iteration 409, loss = 0.02198087\n",
      "Iteration 423, loss = 0.01414361\n",
      "Iteration 410, loss = 0.02179123\n",
      "Iteration 424, loss = 0.01387290\n",
      "Iteration 411, loss = 0.02193748\n",
      "Iteration 425, loss = 0.01344668\n",
      "Iteration 412, loss = 0.02234471\n",
      "Iteration 426, loss = 0.01324109\n",
      "Iteration 413, loss = 0.02304228\n",
      "Iteration 427, loss = 0.01330117\n",
      "Iteration 414, loss = 0.02290930\n",
      "Iteration 428, loss = 0.01368979\n",
      "Iteration 415, loss = 0.02208466\n",
      "Iteration 429, loss = 0.01290607\n",
      "Iteration 416, loss = 0.02124834\n",
      "Iteration 430, loss = 0.01354075\n",
      "Iteration 417, loss = 0.02112080\n",
      "Iteration 431, loss = 0.01319755\n",
      "Iteration 418, loss = 0.02107755\n",
      "Iteration 432, loss = 0.01257915\n",
      "Iteration 419, loss = 0.02168318\n",
      "Iteration 433, loss = 0.01280571\n",
      "Iteration 420, loss = 0.02073942\n",
      "Iteration 434, loss = 0.01254180\n",
      "Iteration 421, loss = 0.02313616\n",
      "Iteration 435, loss = 0.01318583\n",
      "Iteration 422, loss = 0.02045960\n",
      "Iteration 436, loss = 0.01242748\n",
      "Iteration 423, loss = 0.02096522\n",
      "Iteration 437, loss = 0.01268235\n",
      "Iteration 424, loss = 0.02128310\n",
      "Iteration 438, loss = 0.01224874\n",
      "Iteration 425, loss = 0.02095421\n",
      "Iteration 439, loss = 0.01270548\n",
      "Iteration 440, loss = 0.01259045\n",
      "Iteration 426, loss = 0.02158373\n",
      "Iteration 441, loss = 0.01184056\n",
      "Iteration 427, loss = 0.02138731\n",
      "Iteration 442, loss = 0.01234250\n",
      "Iteration 428, loss = 0.02134403\n",
      "Iteration 443, loss = 0.01195253\n",
      "Iteration 429, loss = 0.02017223\n",
      "Iteration 444, loss = 0.01197898\n",
      "Iteration 430, loss = 0.02082936\n",
      "Iteration 445, loss = 0.01195664\n",
      "Iteration 431, loss = 0.02045913\n",
      "Iteration 446, loss = 0.01152752\n",
      "Iteration 432, loss = 0.01962987\n",
      "Iteration 447, loss = 0.01242095\n",
      "Iteration 433, loss = 0.01967212\n",
      "Iteration 448, loss = 0.01120505\n",
      "Iteration 434, loss = 0.01913563\n",
      "Iteration 449, loss = 0.01180884\n",
      "Iteration 435, loss = 0.01946624\n",
      "Iteration 450, loss = 0.01186482\n",
      "Iteration 436, loss = 0.01941064\n",
      "Iteration 451, loss = 0.01174205\n",
      "Iteration 437, loss = 0.01933658\n",
      "Iteration 452, loss = 0.01173049\n",
      "Iteration 438, loss = 0.01933268\n",
      "Iteration 453, loss = 0.01180516\n",
      "Iteration 439, loss = 0.01932986\n",
      "Iteration 454, loss = 0.01138929\n",
      "Iteration 440, loss = 0.01964968\n",
      "Iteration 455, loss = 0.01194168\n",
      "Iteration 441, loss = 0.02049627\n",
      "Iteration 456, loss = 0.01262506\n",
      "Iteration 442, loss = 0.02161765\n",
      "Iteration 457, loss = 0.01151395\n",
      "Iteration 443, loss = 0.02027550\n",
      "Iteration 458, loss = 0.01081540\n",
      "Iteration 444, loss = 0.01887206\n",
      "Iteration 459, loss = 0.01148050\n",
      "Iteration 445, loss = 0.01844249\n",
      "Iteration 460, loss = 0.01073692\n",
      "Iteration 446, loss = 0.01772839\n",
      "Iteration 461, loss = 0.01083828\n",
      "Iteration 447, loss = 0.01772289\n",
      "Iteration 462, loss = 0.01049334\n",
      "Iteration 448, loss = 0.01769319\n",
      "Iteration 463, loss = 0.01087082\n",
      "Iteration 449, loss = 0.01732726\n",
      "Iteration 450, loss = 0.01744524\n",
      "Iteration 464, loss = 0.01032733\n",
      "Iteration 451, loss = 0.01755840\n",
      "Iteration 465, loss = 0.01047142\n",
      "Iteration 466, loss = 0.01023467\n",
      "Iteration 452, loss = 0.01708962\n",
      "Iteration 467, loss = 0.01015120\n",
      "Iteration 453, loss = 0.01734935\n",
      "Iteration 468, loss = 0.01037901\n",
      "Iteration 454, loss = 0.01764414\n",
      "Iteration 469, loss = 0.01043863\n",
      "Iteration 455, loss = 0.01733765\n",
      "Iteration 470, loss = 0.01129773\n",
      "Iteration 456, loss = 0.01677743\n",
      "Iteration 471, loss = 0.01055118\n",
      "Iteration 457, loss = 0.01727345\n",
      "Iteration 458, loss = 0.01810726\n",
      "Iteration 472, loss = 0.01042494\n",
      "Iteration 473, loss = 0.01176626\n",
      "Iteration 459, loss = 0.01666704\n",
      "Iteration 474, loss = 0.01100935\n",
      "Iteration 460, loss = 0.01662677\n",
      "Iteration 461, loss = 0.01861840\n",
      "Iteration 475, loss = 0.00997276\n",
      "Iteration 462, loss = 0.01800937\n",
      "Iteration 476, loss = 0.01011649\n",
      "Iteration 463, loss = 0.01655436\n",
      "Iteration 477, loss = 0.00983854\n",
      "Iteration 464, loss = 0.01874113\n",
      "Iteration 478, loss = 0.01005787\n",
      "Iteration 465, loss = 0.01830144\n",
      "Iteration 479, loss = 0.01024297\n",
      "Iteration 466, loss = 0.01648430\n",
      "Iteration 480, loss = 0.00945144\n",
      "Iteration 467, loss = 0.01805296\n",
      "Iteration 481, loss = 0.00964493\n",
      "Iteration 468, loss = 0.01631883\n",
      "Iteration 482, loss = 0.00987612\n",
      "Iteration 469, loss = 0.01618214\n",
      "Iteration 483, loss = 0.01003614\n",
      "Iteration 470, loss = 0.01606134\n",
      "Iteration 484, loss = 0.00982189\n",
      "Iteration 471, loss = 0.01593873\n",
      "Iteration 485, loss = 0.00987458\n",
      "Iteration 472, loss = 0.01596327\n",
      "Iteration 486, loss = 0.00959807\n",
      "Iteration 473, loss = 0.01542429\n",
      "Iteration 487, loss = 0.00998373\n",
      "Iteration 474, loss = 0.01614681\n",
      "Iteration 488, loss = 0.00905061\n",
      "Iteration 475, loss = 0.01624453\n",
      "Iteration 489, loss = 0.00933652\n",
      "Iteration 476, loss = 0.01546112\n",
      "Iteration 490, loss = 0.00907934\n",
      "Iteration 477, loss = 0.01604866\n",
      "Iteration 491, loss = 0.00928030\n",
      "Iteration 478, loss = 0.01612118\n",
      "Iteration 492, loss = 0.00935156\n",
      "Iteration 479, loss = 0.01578505\n",
      "Iteration 493, loss = 0.00905993\n",
      "Iteration 480, loss = 0.01554927\n",
      "Iteration 494, loss = 0.00841242\n",
      "Iteration 481, loss = 0.01617817\n",
      "Iteration 495, loss = 0.00994069\n",
      "Iteration 482, loss = 0.01467649\n",
      "Iteration 496, loss = 0.00925341\n",
      "Iteration 483, loss = 0.01562658\n",
      "Iteration 497, loss = 0.00964994\n",
      "Iteration 484, loss = 0.01499223\n",
      "Iteration 498, loss = 0.00946904\n",
      "Iteration 485, loss = 0.01477218\n",
      "Iteration 499, loss = 0.00965069\n",
      "Iteration 486, loss = 0.01446300\n",
      "Iteration 500, loss = 0.01242619\n",
      "Iteration 487, loss = 0.01464030\n",
      "Iteration 488, loss = 0.01438818\n",
      "Iteration 489, loss = 0.01478325\n",
      "Iteration 490, loss = 0.01441482\n",
      "Iteration 491, loss = 0.01491402\n",
      "Iteration 492, loss = 0.01509555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 493, loss = 0.01421570\n",
      "Iteration 494, loss = 0.01630287\n",
      "Iteration 495, loss = 0.01722367\n",
      "Iteration 496, loss = 0.01378722\n",
      "Iteration 497, loss = 0.01539350\n",
      "Iteration 498, loss = 0.01547877\n",
      "Iteration 499, loss = 0.01595147\n",
      "Iteration 500, loss = 0.01408147\n",
      "Iteration 1, loss = 1.22466050\n",
      "Iteration 2, loss = 1.05075283\n",
      "Iteration 3, loss = 0.92780116\n",
      "Iteration 4, loss = 0.84919485\n",
      "Iteration 5, loss = 0.77800678\n",
      "Iteration 6, loss = 0.71087477\n",
      "Iteration 7, loss = 0.65631065\n",
      "Iteration 8, loss = 0.61189470\n",
      "Iteration 9, loss = 0.57747851\n",
      "Iteration 10, loss = 0.54690757\n",
      "Iteration 11, loss = 0.52132956\n",
      "Iteration 12, loss = 0.49891225\n",
      "Iteration 13, loss = 0.48023889\n",
      "Iteration 14, loss = 0.46492046\n",
      "Iteration 1, loss = 1.33374087\n",
      "Iteration 2, loss = 1.04608958\n",
      "Iteration 15, loss = 0.45024560\n",
      "Iteration 3, loss = 0.90465092\n",
      "Iteration 16, loss = 0.43791080\n",
      "Iteration 4, loss = 0.80779034\n",
      "Iteration 17, loss = 0.42715652\n",
      "Iteration 5, loss = 0.73259498\n",
      "Iteration 18, loss = 0.41638069\n",
      "Iteration 6, loss = 0.67593613\n",
      "Iteration 19, loss = 0.40719712\n",
      "Iteration 7, loss = 0.63249385\n",
      "Iteration 20, loss = 0.39942877\n",
      "Iteration 8, loss = 0.59283728\n",
      "Iteration 21, loss = 0.39239956\n",
      "Iteration 9, loss = 0.56083189\n",
      "Iteration 22, loss = 0.38381740\n",
      "Iteration 10, loss = 0.53708340\n",
      "Iteration 23, loss = 0.37744159\n",
      "Iteration 11, loss = 0.51490311\n",
      "Iteration 24, loss = 0.37215866\n",
      "Iteration 12, loss = 0.49546572\n",
      "Iteration 25, loss = 0.36560596\n",
      "Iteration 13, loss = 0.47883535\n",
      "Iteration 26, loss = 0.36051430\n",
      "Iteration 14, loss = 0.46402002\n",
      "Iteration 27, loss = 0.35562346\n",
      "Iteration 15, loss = 0.45066817\n",
      "Iteration 28, loss = 0.35081527\n",
      "Iteration 16, loss = 0.43921170\n",
      "Iteration 29, loss = 0.34656992\n",
      "Iteration 17, loss = 0.42753050\n",
      "Iteration 30, loss = 0.34153818\n",
      "Iteration 18, loss = 0.41753798\n",
      "Iteration 31, loss = 0.33706610\n",
      "Iteration 19, loss = 0.40816278\n",
      "Iteration 32, loss = 0.33290704\n",
      "Iteration 20, loss = 0.40023861\n",
      "Iteration 33, loss = 0.32910994\n",
      "Iteration 21, loss = 0.39182669\n",
      "Iteration 34, loss = 0.32499540\n",
      "Iteration 22, loss = 0.38466428\n",
      "Iteration 35, loss = 0.32146204\n",
      "Iteration 23, loss = 0.37669325\n",
      "Iteration 36, loss = 0.31784754\n",
      "Iteration 24, loss = 0.37055364\n",
      "Iteration 37, loss = 0.31418309\n",
      "Iteration 25, loss = 0.36400313\n",
      "Iteration 38, loss = 0.31080682\n",
      "Iteration 26, loss = 0.35726980\n",
      "Iteration 39, loss = 0.30741843\n",
      "Iteration 27, loss = 0.35281257\n",
      "Iteration 40, loss = 0.30455696\n",
      "Iteration 28, loss = 0.34655926\n",
      "Iteration 41, loss = 0.30213842\n",
      "Iteration 29, loss = 0.34009049\n",
      "Iteration 42, loss = 0.29921296\n",
      "Iteration 30, loss = 0.33548014\n",
      "Iteration 43, loss = 0.29576543\n",
      "Iteration 31, loss = 0.33060550\n",
      "Iteration 44, loss = 0.29228915\n",
      "Iteration 32, loss = 0.32713187\n",
      "Iteration 45, loss = 0.28955783\n",
      "Iteration 33, loss = 0.32149282\n",
      "Iteration 46, loss = 0.28631809\n",
      "Iteration 34, loss = 0.31665516\n",
      "Iteration 47, loss = 0.28341830\n",
      "Iteration 35, loss = 0.31263512\n",
      "Iteration 48, loss = 0.28070921\n",
      "Iteration 36, loss = 0.30822056\n",
      "Iteration 49, loss = 0.27805662\n",
      "Iteration 37, loss = 0.30455316\n",
      "Iteration 50, loss = 0.27534622\n",
      "Iteration 38, loss = 0.30114932\n",
      "Iteration 51, loss = 0.27265203\n",
      "Iteration 39, loss = 0.29831460\n",
      "Iteration 52, loss = 0.27044688\n",
      "Iteration 40, loss = 0.29275012\n",
      "Iteration 53, loss = 0.26804194\n",
      "Iteration 41, loss = 0.29043229\n",
      "Iteration 54, loss = 0.26605144\n",
      "Iteration 42, loss = 0.28709778\n",
      "Iteration 55, loss = 0.26337882\n",
      "Iteration 43, loss = 0.28333243\n",
      "Iteration 56, loss = 0.26115406\n",
      "Iteration 44, loss = 0.27998461\n",
      "Iteration 57, loss = 0.25812788\n",
      "Iteration 45, loss = 0.27651236\n",
      "Iteration 58, loss = 0.25732883\n",
      "Iteration 46, loss = 0.27361344\n",
      "Iteration 59, loss = 0.25423298\n",
      "Iteration 47, loss = 0.27036312\n",
      "Iteration 60, loss = 0.25134750\n",
      "Iteration 48, loss = 0.26740135\n",
      "Iteration 61, loss = 0.24917720\n",
      "Iteration 49, loss = 0.26442866\n",
      "Iteration 62, loss = 0.24709793\n",
      "Iteration 50, loss = 0.26197316\n",
      "Iteration 63, loss = 0.24484141\n",
      "Iteration 51, loss = 0.25915741\n",
      "Iteration 64, loss = 0.24326351\n",
      "Iteration 52, loss = 0.25576522\n",
      "Iteration 65, loss = 0.24124329\n",
      "Iteration 53, loss = 0.25486332\n",
      "Iteration 66, loss = 0.23911439\n",
      "Iteration 54, loss = 0.25075849\n",
      "Iteration 67, loss = 0.23732879\n",
      "Iteration 55, loss = 0.24833843\n",
      "Iteration 68, loss = 0.23592908\n",
      "Iteration 56, loss = 0.24617882\n",
      "Iteration 69, loss = 0.23388050\n",
      "Iteration 57, loss = 0.24332587\n",
      "Iteration 70, loss = 0.23093408\n",
      "Iteration 58, loss = 0.24066432\n",
      "Iteration 71, loss = 0.23150571\n",
      "Iteration 59, loss = 0.23832712\n",
      "Iteration 72, loss = 0.22733134\n",
      "Iteration 60, loss = 0.23650697\n",
      "Iteration 73, loss = 0.22555434\n",
      "Iteration 61, loss = 0.23382416\n",
      "Iteration 74, loss = 0.22418564\n",
      "Iteration 62, loss = 0.23198970\n",
      "Iteration 75, loss = 0.22200819\n",
      "Iteration 63, loss = 0.22921669\n",
      "Iteration 76, loss = 0.22021293\n",
      "Iteration 64, loss = 0.22719083\n",
      "Iteration 77, loss = 0.21933500\n",
      "Iteration 65, loss = 0.22510076\n",
      "Iteration 78, loss = 0.21656109\n",
      "Iteration 66, loss = 0.22290962\n",
      "Iteration 79, loss = 0.21527963\n",
      "Iteration 67, loss = 0.22019025\n",
      "Iteration 80, loss = 0.21254995\n",
      "Iteration 68, loss = 0.21827542\n",
      "Iteration 81, loss = 0.21156025\n",
      "Iteration 69, loss = 0.21614308\n",
      "Iteration 82, loss = 0.20962237\n",
      "Iteration 70, loss = 0.21453773\n",
      "Iteration 83, loss = 0.20886852\n",
      "Iteration 71, loss = 0.21302002\n",
      "Iteration 84, loss = 0.20615787\n",
      "Iteration 72, loss = 0.21012468\n",
      "Iteration 85, loss = 0.20462118\n",
      "Iteration 73, loss = 0.20802111\n",
      "Iteration 86, loss = 0.20306164\n",
      "Iteration 74, loss = 0.20781015\n",
      "Iteration 87, loss = 0.20109922\n",
      "Iteration 75, loss = 0.20410343\n",
      "Iteration 88, loss = 0.19951141\n",
      "Iteration 76, loss = 0.20275569\n",
      "Iteration 89, loss = 0.19850138\n",
      "Iteration 77, loss = 0.20012691\n",
      "Iteration 90, loss = 0.19637599\n",
      "Iteration 78, loss = 0.19873760\n",
      "Iteration 91, loss = 0.19518903\n",
      "Iteration 79, loss = 0.19668819\n",
      "Iteration 92, loss = 0.19336278\n",
      "Iteration 80, loss = 0.19472740\n",
      "Iteration 93, loss = 0.19394066\n",
      "Iteration 81, loss = 0.19311756\n",
      "Iteration 94, loss = 0.19087277\n",
      "Iteration 82, loss = 0.19149182\n",
      "Iteration 95, loss = 0.18927717\n",
      "Iteration 83, loss = 0.18951985\n",
      "Iteration 96, loss = 0.18680782\n",
      "Iteration 84, loss = 0.18711851\n",
      "Iteration 97, loss = 0.18649049\n",
      "Iteration 85, loss = 0.18648825\n",
      "Iteration 98, loss = 0.18602746\n",
      "Iteration 86, loss = 0.18548785\n",
      "Iteration 99, loss = 0.18247340\n",
      "Iteration 87, loss = 0.18375926\n",
      "Iteration 100, loss = 0.18228549\n",
      "Iteration 88, loss = 0.18368195\n",
      "Iteration 101, loss = 0.18002269\n",
      "Iteration 89, loss = 0.17924773\n",
      "Iteration 102, loss = 0.17846103\n",
      "Iteration 90, loss = 0.17850077\n",
      "Iteration 103, loss = 0.17732070\n",
      "Iteration 91, loss = 0.17652096\n",
      "Iteration 104, loss = 0.17484859\n",
      "Iteration 92, loss = 0.17499193\n",
      "Iteration 105, loss = 0.17467817\n",
      "Iteration 93, loss = 0.17218707\n",
      "Iteration 106, loss = 0.17234165\n",
      "Iteration 94, loss = 0.17023895\n",
      "Iteration 107, loss = 0.17079101\n",
      "Iteration 95, loss = 0.17002102\n",
      "Iteration 108, loss = 0.16965516\n",
      "Iteration 96, loss = 0.16757628\n",
      "Iteration 109, loss = 0.16726296\n",
      "Iteration 97, loss = 0.16598688\n",
      "Iteration 110, loss = 0.16895524\n",
      "Iteration 98, loss = 0.16549813\n",
      "Iteration 111, loss = 0.16467853\n",
      "Iteration 99, loss = 0.16351731\n",
      "Iteration 112, loss = 0.16456317\n",
      "Iteration 100, loss = 0.16161528\n",
      "Iteration 113, loss = 0.16145777\n",
      "Iteration 101, loss = 0.16106326\n",
      "Iteration 114, loss = 0.16134114\n",
      "Iteration 102, loss = 0.15835804\n",
      "Iteration 115, loss = 0.15966179\n",
      "Iteration 103, loss = 0.15734311\n",
      "Iteration 116, loss = 0.15688013\n",
      "Iteration 104, loss = 0.15640198\n",
      "Iteration 117, loss = 0.15901516\n",
      "Iteration 105, loss = 0.15476576\n",
      "Iteration 118, loss = 0.15514563\n",
      "Iteration 106, loss = 0.15278517\n",
      "Iteration 119, loss = 0.15596453\n",
      "Iteration 120, loss = 0.15526138\n",
      "Iteration 107, loss = 0.15182777\n",
      "Iteration 108, loss = 0.15011921\n",
      "Iteration 121, loss = 0.15290407\n",
      "Iteration 109, loss = 0.14980663\n",
      "Iteration 122, loss = 0.15217727\n",
      "Iteration 110, loss = 0.14642919\n",
      "Iteration 123, loss = 0.14988982\n",
      "Iteration 111, loss = 0.14744459\n",
      "Iteration 124, loss = 0.14835185\n",
      "Iteration 112, loss = 0.14506472\n",
      "Iteration 125, loss = 0.14669850\n",
      "Iteration 113, loss = 0.14453637\n",
      "Iteration 126, loss = 0.14692440\n",
      "Iteration 114, loss = 0.14216391\n",
      "Iteration 127, loss = 0.14263437\n",
      "Iteration 115, loss = 0.14225072\n",
      "Iteration 128, loss = 0.14409759\n",
      "Iteration 116, loss = 0.14010697\n",
      "Iteration 129, loss = 0.14023294\n",
      "Iteration 117, loss = 0.14061120\n",
      "Iteration 130, loss = 0.14191884\n",
      "Iteration 118, loss = 0.13854035\n",
      "Iteration 131, loss = 0.13937102\n",
      "Iteration 119, loss = 0.13936022\n",
      "Iteration 132, loss = 0.13835291\n",
      "Iteration 120, loss = 0.13547731\n",
      "Iteration 133, loss = 0.13620192\n",
      "Iteration 121, loss = 0.13516705\n",
      "Iteration 134, loss = 0.13641439\n",
      "Iteration 135, loss = 0.13613039\n",
      "Iteration 136, loss = 0.13261323\n",
      "Iteration 122, loss = 0.13470958\n",
      "Iteration 137, loss = 0.13250772\n",
      "Iteration 123, loss = 0.13210940\n",
      "Iteration 124, loss = 0.13133718\n",
      "Iteration 125, loss = 0.12954747\n",
      "Iteration 138, loss = 0.13087038\n",
      "Iteration 139, loss = 0.12998648\n",
      "Iteration 126, loss = 0.12907893\n",
      "Iteration 127, loss = 0.12933434\n",
      "Iteration 140, loss = 0.12837169\n",
      "Iteration 128, loss = 0.12730902\n",
      "Iteration 129, loss = 0.12738554\n",
      "Iteration 141, loss = 0.12901468\n",
      "Iteration 130, loss = 0.12561907\n",
      "Iteration 142, loss = 0.12677409\n",
      "Iteration 131, loss = 0.12426055\n",
      "Iteration 143, loss = 0.12662664\n",
      "Iteration 132, loss = 0.12510338\n",
      "Iteration 144, loss = 0.12560772\n",
      "Iteration 133, loss = 0.12192006\n",
      "Iteration 145, loss = 0.12355058\n",
      "Iteration 146, loss = 0.12572132\n",
      "Iteration 147, loss = 0.12200463\n",
      "Iteration 134, loss = 0.12301461\n",
      "Iteration 148, loss = 0.12256508\n",
      "Iteration 149, loss = 0.12013170\n",
      "Iteration 150, loss = 0.12072315\n",
      "Iteration 151, loss = 0.11842377\n",
      "Iteration 135, loss = 0.12224423\n",
      "Iteration 136, loss = 0.11990545\n",
      "Iteration 152, loss = 0.11910871\n",
      "Iteration 137, loss = 0.11973115\n",
      "Iteration 153, loss = 0.11946706\n",
      "Iteration 138, loss = 0.11688776\n",
      "Iteration 154, loss = 0.11822014\n",
      "Iteration 139, loss = 0.11881026\n",
      "Iteration 155, loss = 0.11517792\n",
      "Iteration 140, loss = 0.11658199\n",
      "Iteration 156, loss = 0.11481388\n",
      "Iteration 141, loss = 0.11521137\n",
      "Iteration 157, loss = 0.11354021\n",
      "Iteration 142, loss = 0.11520746\n",
      "Iteration 158, loss = 0.11281677\n",
      "Iteration 143, loss = 0.11402524\n",
      "Iteration 159, loss = 0.11151032\n",
      "Iteration 144, loss = 0.11381422\n",
      "Iteration 160, loss = 0.11381030\n",
      "Iteration 161, loss = 0.11088354\n",
      "Iteration 162, loss = 0.10970534\n",
      "Iteration 145, loss = 0.11175722\n",
      "Iteration 146, loss = 0.11216516\n",
      "Iteration 163, loss = 0.10775327\n",
      "Iteration 147, loss = 0.10967415\n",
      "Iteration 164, loss = 0.10838061\n",
      "Iteration 148, loss = 0.11023902\n",
      "Iteration 165, loss = 0.10734045\n",
      "Iteration 149, loss = 0.10779763\n",
      "Iteration 166, loss = 0.10592303\n",
      "Iteration 150, loss = 0.10770915\n",
      "Iteration 167, loss = 0.10547041\n",
      "Iteration 151, loss = 0.10744775\n",
      "Iteration 168, loss = 0.10418419\n",
      "Iteration 152, loss = 0.10617792\n",
      "Iteration 169, loss = 0.10518474\n",
      "Iteration 153, loss = 0.10544371\n",
      "Iteration 170, loss = 0.10219872\n",
      "Iteration 154, loss = 0.10520159\n",
      "Iteration 171, loss = 0.10187033\n",
      "Iteration 155, loss = 0.10459105\n",
      "Iteration 172, loss = 0.10194144\n",
      "Iteration 156, loss = 0.10297745\n",
      "Iteration 173, loss = 0.10010669\n",
      "Iteration 157, loss = 0.10237361\n",
      "Iteration 174, loss = 0.10144596\n",
      "Iteration 158, loss = 0.10140816\n",
      "Iteration 175, loss = 0.10115891\n",
      "Iteration 159, loss = 0.10237019\n",
      "Iteration 176, loss = 0.09983874\n",
      "Iteration 160, loss = 0.10361931\n",
      "Iteration 177, loss = 0.09750962\n",
      "Iteration 161, loss = 0.09887629\n",
      "Iteration 178, loss = 0.09860942\n",
      "Iteration 162, loss = 0.10116612\n",
      "Iteration 179, loss = 0.09660923\n",
      "Iteration 163, loss = 0.09807817\n",
      "Iteration 180, loss = 0.09575404\n",
      "Iteration 164, loss = 0.09839739\n",
      "Iteration 181, loss = 0.09601172\n",
      "Iteration 165, loss = 0.09745761\n",
      "Iteration 182, loss = 0.09638864\n",
      "Iteration 166, loss = 0.09529411\n",
      "Iteration 167, loss = 0.09685539Iteration 183, loss = 0.09363054\n",
      "\n",
      "Iteration 184, loss = 0.09419279\n",
      "Iteration 168, loss = 0.09580978\n",
      "Iteration 185, loss = 0.09273107\n",
      "Iteration 169, loss = 0.09557137\n",
      "Iteration 170, loss = 0.09338458\n",
      "Iteration 186, loss = 0.09208923\n",
      "Iteration 171, loss = 0.09440047\n",
      "Iteration 187, loss = 0.09383399\n",
      "Iteration 172, loss = 0.09149340\n",
      "Iteration 188, loss = 0.09278549\n",
      "Iteration 173, loss = 0.09244599\n",
      "Iteration 189, loss = 0.09478892\n",
      "Iteration 174, loss = 0.09046300\n",
      "Iteration 190, loss = 0.09077289\n",
      "Iteration 175, loss = 0.09012160\n",
      "Iteration 191, loss = 0.09012517\n",
      "Iteration 176, loss = 0.08960659\n",
      "Iteration 192, loss = 0.08802630\n",
      "Iteration 177, loss = 0.08901355\n",
      "Iteration 193, loss = 0.08722824\n",
      "Iteration 178, loss = 0.08739461\n",
      "Iteration 194, loss = 0.08864034\n",
      "Iteration 179, loss = 0.08763997\n",
      "Iteration 195, loss = 0.08503499\n",
      "Iteration 180, loss = 0.08657690\n",
      "Iteration 196, loss = 0.08714356\n",
      "Iteration 181, loss = 0.08647740\n",
      "Iteration 197, loss = 0.08877488\n",
      "Iteration 182, loss = 0.08549807\n",
      "Iteration 198, loss = 0.08362265\n",
      "Iteration 183, loss = 0.08473181\n",
      "Iteration 199, loss = 0.08319341\n",
      "Iteration 184, loss = 0.08463054\n",
      "Iteration 200, loss = 0.08281977\n",
      "Iteration 185, loss = 0.08354842\n",
      "Iteration 201, loss = 0.08206045\n",
      "Iteration 186, loss = 0.08433658\n",
      "Iteration 202, loss = 0.08191402\n",
      "Iteration 187, loss = 0.08654097\n",
      "Iteration 203, loss = 0.08153736\n",
      "Iteration 188, loss = 0.08448107\n",
      "Iteration 204, loss = 0.08117114\n",
      "Iteration 189, loss = 0.08148994\n",
      "Iteration 205, loss = 0.07999365\n",
      "Iteration 190, loss = 0.08365687\n",
      "Iteration 206, loss = 0.08026762\n",
      "Iteration 191, loss = 0.08295184\n",
      "Iteration 207, loss = 0.07850802\n",
      "Iteration 192, loss = 0.07924047\n",
      "Iteration 208, loss = 0.07870945\n",
      "Iteration 193, loss = 0.08054883\n",
      "Iteration 209, loss = 0.08111644\n",
      "Iteration 194, loss = 0.07846087\n",
      "Iteration 210, loss = 0.08033823\n",
      "Iteration 195, loss = 0.07970244\n",
      "Iteration 211, loss = 0.07874339\n",
      "Iteration 196, loss = 0.07769610\n",
      "Iteration 212, loss = 0.07898269\n",
      "Iteration 197, loss = 0.07806442\n",
      "Iteration 213, loss = 0.07549771\n",
      "Iteration 198, loss = 0.07671417\n",
      "Iteration 214, loss = 0.07452153\n",
      "Iteration 199, loss = 0.07508787\n",
      "Iteration 215, loss = 0.07583575\n",
      "Iteration 200, loss = 0.07583148\n",
      "Iteration 216, loss = 0.07300803\n",
      "Iteration 201, loss = 0.07524425\n",
      "Iteration 217, loss = 0.07357505\n",
      "Iteration 202, loss = 0.07372779\n",
      "Iteration 218, loss = 0.07256806\n",
      "Iteration 203, loss = 0.07448219\n",
      "Iteration 219, loss = 0.07243076\n",
      "Iteration 204, loss = 0.07286382\n",
      "Iteration 220, loss = 0.07156554\n",
      "Iteration 205, loss = 0.07291480\n",
      "Iteration 221, loss = 0.07168076\n",
      "Iteration 206, loss = 0.07406792\n",
      "Iteration 222, loss = 0.07046007\n",
      "Iteration 207, loss = 0.07231586\n",
      "Iteration 223, loss = 0.06987071\n",
      "Iteration 208, loss = 0.07102471\n",
      "Iteration 224, loss = 0.07000503\n",
      "Iteration 209, loss = 0.07122872\n",
      "Iteration 225, loss = 0.06874088\n",
      "Iteration 210, loss = 0.07225291\n",
      "Iteration 226, loss = 0.06815738\n",
      "Iteration 211, loss = 0.06944012\n",
      "Iteration 227, loss = 0.06789368\n",
      "Iteration 212, loss = 0.07121534\n",
      "Iteration 228, loss = 0.06769551\n",
      "Iteration 213, loss = 0.07303440\n",
      "Iteration 229, loss = 0.06764240\n",
      "Iteration 214, loss = 0.06934311\n",
      "Iteration 230, loss = 0.06709131\n",
      "Iteration 215, loss = 0.06769746\n",
      "Iteration 231, loss = 0.06657784\n",
      "Iteration 216, loss = 0.07241554\n",
      "Iteration 232, loss = 0.06576666\n",
      "Iteration 217, loss = 0.06823240\n",
      "Iteration 233, loss = 0.06557368\n",
      "Iteration 218, loss = 0.06845266\n",
      "Iteration 234, loss = 0.06480938\n",
      "Iteration 219, loss = 0.06725180\n",
      "Iteration 235, loss = 0.06385552\n",
      "Iteration 220, loss = 0.06835694\n",
      "Iteration 236, loss = 0.06332166\n",
      "Iteration 221, loss = 0.06474406\n",
      "Iteration 237, loss = 0.06264040\n",
      "Iteration 222, loss = 0.06773413\n",
      "Iteration 238, loss = 0.06280390\n",
      "Iteration 223, loss = 0.06613375\n",
      "Iteration 239, loss = 0.06198997\n",
      "Iteration 224, loss = 0.06368815\n",
      "Iteration 240, loss = 0.06222138\n",
      "Iteration 225, loss = 0.06409781\n",
      "Iteration 241, loss = 0.06115911\n",
      "Iteration 226, loss = 0.06420389\n",
      "Iteration 242, loss = 0.06034182\n",
      "Iteration 227, loss = 0.06258579\n",
      "Iteration 243, loss = 0.06201863\n",
      "Iteration 228, loss = 0.06332978\n",
      "Iteration 244, loss = 0.06160733\n",
      "Iteration 229, loss = 0.06171437\n",
      "Iteration 245, loss = 0.05967548\n",
      "Iteration 230, loss = 0.06118051\n",
      "Iteration 246, loss = 0.05960419\n",
      "Iteration 231, loss = 0.06282390\n",
      "Iteration 247, loss = 0.05883824\n",
      "Iteration 232, loss = 0.06345956\n",
      "Iteration 248, loss = 0.05945487\n",
      "Iteration 233, loss = 0.06226933\n",
      "Iteration 249, loss = 0.05825572\n",
      "Iteration 234, loss = 0.06015351\n",
      "Iteration 250, loss = 0.05712345\n",
      "Iteration 235, loss = 0.05875081\n",
      "Iteration 251, loss = 0.05873263\n",
      "Iteration 236, loss = 0.06074623\n",
      "Iteration 252, loss = 0.05592266\n",
      "Iteration 237, loss = 0.05906342\n",
      "Iteration 253, loss = 0.05606475\n",
      "Iteration 238, loss = 0.06043720\n",
      "Iteration 254, loss = 0.05615656\n",
      "Iteration 239, loss = 0.05849302\n",
      "Iteration 255, loss = 0.05392453\n",
      "Iteration 240, loss = 0.05828432\n",
      "Iteration 256, loss = 0.05556084\n",
      "Iteration 241, loss = 0.05686196\n",
      "Iteration 257, loss = 0.05704650\n",
      "Iteration 242, loss = 0.05982646\n",
      "Iteration 258, loss = 0.05319187\n",
      "Iteration 243, loss = 0.05662241\n",
      "Iteration 259, loss = 0.05432824\n",
      "Iteration 244, loss = 0.05724093\n",
      "Iteration 260, loss = 0.05408805\n",
      "Iteration 245, loss = 0.05625126\n",
      "Iteration 261, loss = 0.05266207\n",
      "Iteration 246, loss = 0.05555017\n",
      "Iteration 262, loss = 0.05251477\n",
      "Iteration 247, loss = 0.05394532\n",
      "Iteration 263, loss = 0.05282630\n",
      "Iteration 248, loss = 0.05454824\n",
      "Iteration 264, loss = 0.05229232\n",
      "Iteration 249, loss = 0.05283270\n",
      "Iteration 265, loss = 0.05207161\n",
      "Iteration 250, loss = 0.05331062\n",
      "Iteration 266, loss = 0.05083714\n",
      "Iteration 251, loss = 0.05319119\n",
      "Iteration 267, loss = 0.05114568\n",
      "Iteration 252, loss = 0.05322483\n",
      "Iteration 268, loss = 0.04952437\n",
      "Iteration 253, loss = 0.05275621\n",
      "Iteration 269, loss = 0.05149567\n",
      "Iteration 254, loss = 0.05144219\n",
      "Iteration 270, loss = 0.04895058\n",
      "Iteration 255, loss = 0.05124194\n",
      "Iteration 271, loss = 0.04928230\n",
      "Iteration 256, loss = 0.05198724\n",
      "Iteration 272, loss = 0.04897328\n",
      "Iteration 257, loss = 0.05041630\n",
      "Iteration 273, loss = 0.04884086\n",
      "Iteration 258, loss = 0.05084206\n",
      "Iteration 274, loss = 0.04825569\n",
      "Iteration 259, loss = 0.04955645\n",
      "Iteration 275, loss = 0.04717440\n",
      "Iteration 260, loss = 0.05061625\n",
      "Iteration 276, loss = 0.04754315\n",
      "Iteration 261, loss = 0.04958824\n",
      "Iteration 277, loss = 0.04726567\n",
      "Iteration 262, loss = 0.05001396\n",
      "Iteration 278, loss = 0.04621378\n",
      "Iteration 263, loss = 0.04967884\n",
      "Iteration 279, loss = 0.04611736\n",
      "Iteration 264, loss = 0.04940146\n",
      "Iteration 280, loss = 0.04559570\n",
      "Iteration 265, loss = 0.04961593\n",
      "Iteration 281, loss = 0.04512104\n",
      "Iteration 266, loss = 0.05088355\n",
      "Iteration 282, loss = 0.04516260\n",
      "Iteration 267, loss = 0.04866696\n",
      "Iteration 283, loss = 0.04465244\n",
      "Iteration 268, loss = 0.04653232\n",
      "Iteration 284, loss = 0.04722119\n",
      "Iteration 269, loss = 0.04954169\n",
      "Iteration 285, loss = 0.04800032\n",
      "Iteration 270, loss = 0.04760554\n",
      "Iteration 286, loss = 0.04522860\n",
      "Iteration 271, loss = 0.04756257\n",
      "Iteration 287, loss = 0.04314321\n",
      "Iteration 272, loss = 0.04477940\n",
      "Iteration 288, loss = 0.04598409\n",
      "Iteration 273, loss = 0.04615722\n",
      "Iteration 289, loss = 0.04395232\n",
      "Iteration 274, loss = 0.04574328\n",
      "Iteration 290, loss = 0.04286620\n",
      "Iteration 275, loss = 0.04674694\n",
      "Iteration 291, loss = 0.04262227\n",
      "Iteration 276, loss = 0.04334022\n",
      "Iteration 292, loss = 0.04174512\n",
      "Iteration 277, loss = 0.04667142\n",
      "Iteration 293, loss = 0.04108370\n",
      "Iteration 278, loss = 0.04542780\n",
      "Iteration 294, loss = 0.04146556\n",
      "Iteration 279, loss = 0.04378857\n",
      "Iteration 295, loss = 0.04060035\n",
      "Iteration 280, loss = 0.04416458\n",
      "Iteration 296, loss = 0.04009952\n",
      "Iteration 281, loss = 0.04387131\n",
      "Iteration 297, loss = 0.04071740\n",
      "Iteration 282, loss = 0.04245650\n",
      "Iteration 298, loss = 0.03934484\n",
      "Iteration 283, loss = 0.04247371\n",
      "Iteration 299, loss = 0.03918744\n",
      "Iteration 284, loss = 0.04248112\n",
      "Iteration 300, loss = 0.03899723\n",
      "Iteration 285, loss = 0.04266437\n",
      "Iteration 301, loss = 0.04025719\n",
      "Iteration 286, loss = 0.04106004\n",
      "Iteration 302, loss = 0.03961486\n",
      "Iteration 287, loss = 0.04108845\n",
      "Iteration 303, loss = 0.03850307\n",
      "Iteration 288, loss = 0.04056840\n",
      "Iteration 304, loss = 0.03939647\n",
      "Iteration 289, loss = 0.04109825\n",
      "Iteration 305, loss = 0.03718506\n",
      "Iteration 290, loss = 0.04083894\n",
      "Iteration 306, loss = 0.03789938\n",
      "Iteration 291, loss = 0.03993007\n",
      "Iteration 307, loss = 0.03720789\n",
      "Iteration 292, loss = 0.03946629\n",
      "Iteration 308, loss = 0.03690194\n",
      "Iteration 293, loss = 0.03907180\n",
      "Iteration 309, loss = 0.03686871\n",
      "Iteration 294, loss = 0.03930412\n",
      "Iteration 310, loss = 0.03703032\n",
      "Iteration 295, loss = 0.03836158\n",
      "Iteration 311, loss = 0.03600368\n",
      "Iteration 296, loss = 0.03870462\n",
      "Iteration 312, loss = 0.03653426\n",
      "Iteration 297, loss = 0.03913711\n",
      "Iteration 313, loss = 0.03844154\n",
      "Iteration 298, loss = 0.03819931\n",
      "Iteration 314, loss = 0.03776870\n",
      "Iteration 299, loss = 0.03789872\n",
      "Iteration 315, loss = 0.03466810\n",
      "Iteration 300, loss = 0.03724120\n",
      "Iteration 316, loss = 0.03503343\n",
      "Iteration 301, loss = 0.03678723\n",
      "Iteration 317, loss = 0.03541512\n",
      "Iteration 302, loss = 0.03777006\n",
      "Iteration 318, loss = 0.03490723\n",
      "Iteration 303, loss = 0.03907642\n",
      "Iteration 319, loss = 0.03558816\n",
      "Iteration 304, loss = 0.03840751\n",
      "Iteration 320, loss = 0.03377752\n",
      "Iteration 305, loss = 0.03735950\n",
      "Iteration 321, loss = 0.03414658\n",
      "Iteration 306, loss = 0.03636367\n",
      "Iteration 322, loss = 0.03325006\n",
      "Iteration 307, loss = 0.03636832\n",
      "Iteration 323, loss = 0.03685929\n",
      "Iteration 308, loss = 0.03643270\n",
      "Iteration 324, loss = 0.03674105\n",
      "Iteration 309, loss = 0.03677057\n",
      "Iteration 325, loss = 0.03725416\n",
      "Iteration 310, loss = 0.03543848\n",
      "Iteration 326, loss = 0.03404350\n",
      "Iteration 311, loss = 0.03610333\n",
      "Iteration 327, loss = 0.03414541\n",
      "Iteration 312, loss = 0.03687942\n",
      "Iteration 328, loss = 0.03184583\n",
      "Iteration 313, loss = 0.03342576\n",
      "Iteration 329, loss = 0.03202738\n",
      "Iteration 314, loss = 0.03476261\n",
      "Iteration 330, loss = 0.03134471\n",
      "Iteration 315, loss = 0.03354017\n",
      "Iteration 331, loss = 0.03093171\n",
      "Iteration 316, loss = 0.03384145\n",
      "Iteration 332, loss = 0.03073467\n",
      "Iteration 317, loss = 0.03345068\n",
      "Iteration 333, loss = 0.03036337\n",
      "Iteration 318, loss = 0.03304127\n",
      "Iteration 334, loss = 0.03171385\n",
      "Iteration 319, loss = 0.03293042\n",
      "Iteration 335, loss = 0.03083476\n",
      "Iteration 320, loss = 0.03319998\n",
      "Iteration 336, loss = 0.03113562\n",
      "Iteration 321, loss = 0.03200745\n",
      "Iteration 337, loss = 0.03029777\n",
      "Iteration 322, loss = 0.03252900\n",
      "Iteration 338, loss = 0.02920088\n",
      "Iteration 323, loss = 0.03194291\n",
      "Iteration 339, loss = 0.03075571\n",
      "Iteration 324, loss = 0.03287986\n",
      "Iteration 340, loss = 0.02974628\n",
      "Iteration 325, loss = 0.03178096\n",
      "Iteration 341, loss = 0.02992674\n",
      "Iteration 326, loss = 0.03315761\n",
      "Iteration 342, loss = 0.02824846\n",
      "Iteration 327, loss = 0.03256660\n",
      "Iteration 343, loss = 0.03160006\n",
      "Iteration 328, loss = 0.03136628\n",
      "Iteration 344, loss = 0.02922046\n",
      "Iteration 329, loss = 0.03174819\n",
      "Iteration 345, loss = 0.02956606\n",
      "Iteration 330, loss = 0.03082985\n",
      "Iteration 346, loss = 0.02816342\n",
      "Iteration 331, loss = 0.03046463\n",
      "Iteration 347, loss = 0.02958492\n",
      "Iteration 332, loss = 0.03079451\n",
      "Iteration 348, loss = 0.02917651\n",
      "Iteration 333, loss = 0.02939359\n",
      "Iteration 349, loss = 0.02803108\n",
      "Iteration 334, loss = 0.02971369\n",
      "Iteration 350, loss = 0.02779592\n",
      "Iteration 335, loss = 0.03147117\n",
      "Iteration 351, loss = 0.02685310\n",
      "Iteration 336, loss = 0.03052502\n",
      "Iteration 352, loss = 0.02742486\n",
      "Iteration 337, loss = 0.02915767\n",
      "Iteration 353, loss = 0.02664094\n",
      "Iteration 338, loss = 0.03035546\n",
      "Iteration 354, loss = 0.02665885\n",
      "Iteration 339, loss = 0.03030292\n",
      "Iteration 355, loss = 0.02617116\n",
      "Iteration 340, loss = 0.02992849\n",
      "Iteration 356, loss = 0.02600824\n",
      "Iteration 341, loss = 0.03002040\n",
      "Iteration 357, loss = 0.02594664\n",
      "Iteration 342, loss = 0.02856518\n",
      "Iteration 358, loss = 0.02658376\n",
      "Iteration 343, loss = 0.02832149\n",
      "Iteration 359, loss = 0.02545949\n",
      "Iteration 344, loss = 0.02799301\n",
      "Iteration 360, loss = 0.02504777\n",
      "Iteration 345, loss = 0.03093733\n",
      "Iteration 361, loss = 0.02559712\n",
      "Iteration 346, loss = 0.02947173\n",
      "Iteration 362, loss = 0.02516941\n",
      "Iteration 347, loss = 0.02693392\n",
      "Iteration 363, loss = 0.02532228\n",
      "Iteration 348, loss = 0.02781723\n",
      "Iteration 364, loss = 0.02536309\n",
      "Iteration 349, loss = 0.02673393\n",
      "Iteration 365, loss = 0.02473992\n",
      "Iteration 350, loss = 0.02731539\n",
      "Iteration 366, loss = 0.02390285\n",
      "Iteration 351, loss = 0.02710923\n",
      "Iteration 367, loss = 0.02388100\n",
      "Iteration 352, loss = 0.02620060\n",
      "Iteration 368, loss = 0.02421542\n",
      "Iteration 353, loss = 0.02676450\n",
      "Iteration 369, loss = 0.02451235\n",
      "Iteration 354, loss = 0.02633999\n",
      "Iteration 370, loss = 0.02370301\n",
      "Iteration 355, loss = 0.02631693\n",
      "Iteration 371, loss = 0.02561524\n",
      "Iteration 356, loss = 0.02638875\n",
      "Iteration 372, loss = 0.02547120\n",
      "Iteration 357, loss = 0.02656439\n",
      "Iteration 373, loss = 0.02603246\n",
      "Iteration 358, loss = 0.02736938\n",
      "Iteration 374, loss = 0.02431637\n",
      "Iteration 359, loss = 0.02680162\n",
      "Iteration 375, loss = 0.02388873\n",
      "Iteration 360, loss = 0.02514795\n",
      "Iteration 376, loss = 0.02354021\n",
      "Iteration 361, loss = 0.02679521\n",
      "Iteration 377, loss = 0.02276322\n",
      "Iteration 362, loss = 0.02526551\n",
      "Iteration 378, loss = 0.02255573\n",
      "Iteration 363, loss = 0.02469522\n",
      "Iteration 379, loss = 0.02275103\n",
      "Iteration 364, loss = 0.02510202\n",
      "Iteration 380, loss = 0.02294935\n",
      "Iteration 365, loss = 0.02632651\n",
      "Iteration 381, loss = 0.02118346\n",
      "Iteration 366, loss = 0.02464227\n",
      "Iteration 382, loss = 0.02250421\n",
      "Iteration 367, loss = 0.02678411\n",
      "Iteration 383, loss = 0.02479130\n",
      "Iteration 368, loss = 0.02427008\n",
      "Iteration 384, loss = 0.02188767\n",
      "Iteration 369, loss = 0.02403355\n",
      "Iteration 385, loss = 0.02223228\n",
      "Iteration 370, loss = 0.02398699\n",
      "Iteration 386, loss = 0.02171556\n",
      "Iteration 371, loss = 0.02343139\n",
      "Iteration 387, loss = 0.02165100\n",
      "Iteration 372, loss = 0.02318904\n",
      "Iteration 388, loss = 0.02191224\n",
      "Iteration 373, loss = 0.02273098\n",
      "Iteration 389, loss = 0.02108708\n",
      "Iteration 374, loss = 0.02257810\n",
      "Iteration 375, loss = 0.02257320\n",
      "Iteration 390, loss = 0.02187832\n",
      "Iteration 391, loss = 0.02119134\n",
      "Iteration 376, loss = 0.02214001\n",
      "Iteration 392, loss = 0.02038766\n",
      "Iteration 377, loss = 0.02230874\n",
      "Iteration 393, loss = 0.02026200\n",
      "Iteration 378, loss = 0.02219750\n",
      "Iteration 394, loss = 0.01998419\n",
      "Iteration 379, loss = 0.02320379\n",
      "Iteration 395, loss = 0.02012769\n",
      "Iteration 380, loss = 0.02168864\n",
      "Iteration 396, loss = 0.02002041\n",
      "Iteration 381, loss = 0.02248420\n",
      "Iteration 397, loss = 0.02057364\n",
      "Iteration 382, loss = 0.02204841\n",
      "Iteration 398, loss = 0.02121758\n",
      "Iteration 383, loss = 0.02142709\n",
      "Iteration 399, loss = 0.02038232\n",
      "Iteration 384, loss = 0.02146584\n",
      "Iteration 400, loss = 0.02008075\n",
      "Iteration 385, loss = 0.02124056\n",
      "Iteration 401, loss = 0.02005544\n",
      "Iteration 386, loss = 0.02189763\n",
      "Iteration 402, loss = 0.02000891\n",
      "Iteration 387, loss = 0.02282472\n",
      "Iteration 403, loss = 0.01960017\n",
      "Iteration 388, loss = 0.02209855\n",
      "Iteration 404, loss = 0.01976574\n",
      "Iteration 389, loss = 0.02187606\n",
      "Iteration 405, loss = 0.01918668\n",
      "Iteration 390, loss = 0.02137684\n",
      "Iteration 406, loss = 0.01965639\n",
      "Iteration 391, loss = 0.02131573\n",
      "Iteration 407, loss = 0.01923528\n",
      "Iteration 392, loss = 0.02096584\n",
      "Iteration 408, loss = 0.01825637\n",
      "Iteration 393, loss = 0.02083122\n",
      "Iteration 409, loss = 0.01914526\n",
      "Iteration 394, loss = 0.02182683\n",
      "Iteration 410, loss = 0.02247011\n",
      "Iteration 395, loss = 0.02042173\n",
      "Iteration 411, loss = 0.02028252\n",
      "Iteration 396, loss = 0.01981846\n",
      "Iteration 412, loss = 0.02171308\n",
      "Iteration 397, loss = 0.02038180\n",
      "Iteration 413, loss = 0.01874797\n",
      "Iteration 398, loss = 0.02016898\n",
      "Iteration 414, loss = 0.01789185\n",
      "Iteration 399, loss = 0.01978850\n",
      "Iteration 415, loss = 0.02141380\n",
      "Iteration 400, loss = 0.02016419\n",
      "Iteration 416, loss = 0.02418918\n",
      "Iteration 401, loss = 0.02002853\n",
      "Iteration 417, loss = 0.02111231\n",
      "Iteration 402, loss = 0.01946630\n",
      "Iteration 418, loss = 0.02175104\n",
      "Iteration 403, loss = 0.01988450\n",
      "Iteration 419, loss = 0.01920032\n",
      "Iteration 404, loss = 0.01923518\n",
      "Iteration 420, loss = 0.01927282\n",
      "Iteration 405, loss = 0.01889192\n",
      "Iteration 421, loss = 0.01847780\n",
      "Iteration 406, loss = 0.01850925\n",
      "Iteration 422, loss = 0.01754327\n",
      "Iteration 407, loss = 0.01885463\n",
      "Iteration 423, loss = 0.01833733\n",
      "Iteration 408, loss = 0.01840734\n",
      "Iteration 424, loss = 0.01789727\n",
      "Iteration 409, loss = 0.01811602\n",
      "Iteration 425, loss = 0.01655021\n",
      "Iteration 410, loss = 0.01811849\n",
      "Iteration 426, loss = 0.01685914\n",
      "Iteration 411, loss = 0.01766403\n",
      "Iteration 427, loss = 0.01680549\n",
      "Iteration 412, loss = 0.01773348\n",
      "Iteration 428, loss = 0.01770266\n",
      "Iteration 413, loss = 0.01758832\n",
      "Iteration 429, loss = 0.01648093\n",
      "Iteration 414, loss = 0.01750513\n",
      "Iteration 430, loss = 0.01838284\n",
      "Iteration 415, loss = 0.01787584\n",
      "Iteration 431, loss = 0.01713095\n",
      "Iteration 416, loss = 0.01696156\n",
      "Iteration 432, loss = 0.01582028\n",
      "Iteration 417, loss = 0.01755422\n",
      "Iteration 433, loss = 0.01758910\n",
      "Iteration 418, loss = 0.01687479\n",
      "Iteration 434, loss = 0.01814091\n",
      "Iteration 419, loss = 0.01706388\n",
      "Iteration 435, loss = 0.01744494\n",
      "Iteration 420, loss = 0.01659220\n",
      "Iteration 436, loss = 0.01619463\n",
      "Iteration 421, loss = 0.01713885\n",
      "Iteration 437, loss = 0.01658435\n",
      "Iteration 422, loss = 0.01781964\n",
      "Iteration 438, loss = 0.01534160\n",
      "Iteration 423, loss = 0.01787948\n",
      "Iteration 439, loss = 0.01548727\n",
      "Iteration 424, loss = 0.01687751\n",
      "Iteration 440, loss = 0.01565057\n",
      "Iteration 425, loss = 0.01692629\n",
      "Iteration 441, loss = 0.01512914\n",
      "Iteration 426, loss = 0.01719926\n",
      "Iteration 442, loss = 0.01531199\n",
      "Iteration 427, loss = 0.01670361\n",
      "Iteration 443, loss = 0.01502387\n",
      "Iteration 428, loss = 0.01607425\n",
      "Iteration 444, loss = 0.01477134\n",
      "Iteration 429, loss = 0.01635183\n",
      "Iteration 445, loss = 0.01465812\n",
      "Iteration 430, loss = 0.01715621\n",
      "Iteration 446, loss = 0.01465661\n",
      "Iteration 431, loss = 0.01690929\n",
      "Iteration 447, loss = 0.01458676\n",
      "Iteration 432, loss = 0.01755217\n",
      "Iteration 448, loss = 0.01420313\n",
      "Iteration 433, loss = 0.01557439\n",
      "Iteration 449, loss = 0.01444878\n",
      "Iteration 434, loss = 0.01651136\n",
      "Iteration 450, loss = 0.01410494\n",
      "Iteration 435, loss = 0.01576122\n",
      "Iteration 451, loss = 0.01530444\n",
      "Iteration 436, loss = 0.01570259\n",
      "Iteration 452, loss = 0.01500829\n",
      "Iteration 437, loss = 0.01567165\n",
      "Iteration 453, loss = 0.01448877\n",
      "Iteration 438, loss = 0.01617863\n",
      "Iteration 454, loss = 0.01369658\n",
      "Iteration 439, loss = 0.01516634\n",
      "Iteration 455, loss = 0.01520066\n",
      "Iteration 440, loss = 0.01507094\n",
      "Iteration 456, loss = 0.01426027\n",
      "Iteration 441, loss = 0.01609023\n",
      "Iteration 457, loss = 0.01341701\n",
      "Iteration 442, loss = 0.01728655\n",
      "Iteration 458, loss = 0.01433790\n",
      "Iteration 443, loss = 0.01501942\n",
      "Iteration 459, loss = 0.01407328\n",
      "Iteration 444, loss = 0.01565345\n",
      "Iteration 460, loss = 0.01362403\n",
      "Iteration 445, loss = 0.01576563\n",
      "Iteration 461, loss = 0.01472271\n",
      "Iteration 446, loss = 0.01450926\n",
      "Iteration 462, loss = 0.01566779\n",
      "Iteration 447, loss = 0.01487675\n",
      "Iteration 463, loss = 0.01327988\n",
      "Iteration 448, loss = 0.01465753\n",
      "Iteration 464, loss = 0.01415276\n",
      "Iteration 449, loss = 0.01446662\n",
      "Iteration 465, loss = 0.01499557\n",
      "Iteration 450, loss = 0.01395843\n",
      "Iteration 466, loss = 0.01341124\n",
      "Iteration 451, loss = 0.01498716\n",
      "Iteration 467, loss = 0.01592761\n",
      "Iteration 452, loss = 0.01573792\n",
      "Iteration 468, loss = 0.01439922\n",
      "Iteration 453, loss = 0.01472091\n",
      "Iteration 469, loss = 0.01340532\n",
      "Iteration 454, loss = 0.01418969\n",
      "Iteration 470, loss = 0.01391138\n",
      "Iteration 455, loss = 0.01789787\n",
      "Iteration 471, loss = 0.01272989\n",
      "Iteration 456, loss = 0.01604768\n",
      "Iteration 472, loss = 0.01259395\n",
      "Iteration 457, loss = 0.01568005\n",
      "Iteration 473, loss = 0.01269135\n",
      "Iteration 458, loss = 0.01454956\n",
      "Iteration 474, loss = 0.01253857\n",
      "Iteration 459, loss = 0.01596158\n",
      "Iteration 475, loss = 0.01260019\n",
      "Iteration 460, loss = 0.01746411\n",
      "Iteration 476, loss = 0.01259939\n",
      "Iteration 461, loss = 0.01393106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 477, loss = 0.01249983\n",
      "Iteration 478, loss = 0.01201752\n",
      "Iteration 479, loss = 0.01217849\n",
      "Iteration 480, loss = 0.01189783\n",
      "Iteration 481, loss = 0.01203748\n",
      "Iteration 482, loss = 0.01174886\n",
      "Iteration 483, loss = 0.01167570\n",
      "Iteration 484, loss = 0.01158731\n",
      "Iteration 485, loss = 0.01200510\n",
      "Iteration 486, loss = 0.01193164\n",
      "Iteration 487, loss = 0.01417496\n",
      "Iteration 488, loss = 0.01232398\n",
      "Iteration 489, loss = 0.01269061\n",
      "Iteration 490, loss = 0.01307507\n",
      "Iteration 491, loss = 0.01317200\n",
      "Iteration 492, loss = 0.01364968\n",
      "Iteration 493, loss = 0.01197296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06681696\n",
      "Iteration 2, loss = 0.94058191\n",
      "Iteration 3, loss = 0.84825197\n",
      "Iteration 4, loss = 0.76860754\n",
      "Iteration 5, loss = 0.70431000\n",
      "Iteration 6, loss = 0.65058742\n",
      "Iteration 7, loss = 0.60660425\n",
      "Iteration 8, loss = 0.57009627\n",
      "Iteration 9, loss = 0.53920180\n",
      "Iteration 10, loss = 0.51399124\n",
      "Iteration 11, loss = 0.49165919\n",
      "Iteration 12, loss = 0.47376168\n",
      "Iteration 13, loss = 0.45640367\n",
      "Iteration 14, loss = 0.44111715\n",
      "Iteration 15, loss = 0.42824018\n",
      "Iteration 16, loss = 0.41633911\n",
      "Iteration 17, loss = 0.40566127\n",
      "Iteration 18, loss = 0.39466676\n",
      "Iteration 1, loss = 1.14668364\n",
      "Iteration 19, loss = 0.38671377\n",
      "Iteration 2, loss = 1.02020947\n",
      "Iteration 20, loss = 0.37881177\n",
      "Iteration 3, loss = 0.91731963\n",
      "Iteration 21, loss = 0.37077015\n",
      "Iteration 4, loss = 0.83570527\n",
      "Iteration 22, loss = 0.36341508\n",
      "Iteration 5, loss = 0.77000976\n",
      "Iteration 23, loss = 0.35626364\n",
      "Iteration 6, loss = 0.71255483\n",
      "Iteration 24, loss = 0.35031484\n",
      "Iteration 7, loss = 0.65922973\n",
      "Iteration 25, loss = 0.34391347\n",
      "Iteration 8, loss = 0.61523811\n",
      "Iteration 26, loss = 0.33860570\n",
      "Iteration 9, loss = 0.57695324\n",
      "Iteration 27, loss = 0.33402526\n",
      "Iteration 10, loss = 0.54303804\n",
      "Iteration 28, loss = 0.32773648\n",
      "Iteration 11, loss = 0.51476359\n",
      "Iteration 29, loss = 0.32329195\n",
      "Iteration 12, loss = 0.49046120\n",
      "Iteration 30, loss = 0.31818744\n",
      "Iteration 13, loss = 0.47046240\n",
      "Iteration 31, loss = 0.31340291\n",
      "Iteration 14, loss = 0.45122709\n",
      "Iteration 32, loss = 0.30990920\n",
      "Iteration 15, loss = 0.43522154\n",
      "Iteration 33, loss = 0.30527267\n",
      "Iteration 16, loss = 0.42006115\n",
      "Iteration 34, loss = 0.30217872\n",
      "Iteration 17, loss = 0.40694672\n",
      "Iteration 35, loss = 0.29750421\n",
      "Iteration 18, loss = 0.39562771\n",
      "Iteration 36, loss = 0.29390490\n",
      "Iteration 19, loss = 0.38464528\n",
      "Iteration 37, loss = 0.29050763\n",
      "Iteration 20, loss = 0.37505212\n",
      "Iteration 38, loss = 0.28767439\n",
      "Iteration 21, loss = 0.36575085\n",
      "Iteration 39, loss = 0.28322252\n",
      "Iteration 22, loss = 0.35789356\n",
      "Iteration 40, loss = 0.27947695\n",
      "Iteration 23, loss = 0.35040956\n",
      "Iteration 41, loss = 0.27614343\n",
      "Iteration 24, loss = 0.34257123\n",
      "Iteration 42, loss = 0.27337826\n",
      "Iteration 25, loss = 0.33564538\n",
      "Iteration 43, loss = 0.27108105\n",
      "Iteration 26, loss = 0.32940446\n",
      "Iteration 44, loss = 0.26746225\n",
      "Iteration 27, loss = 0.32363222\n",
      "Iteration 45, loss = 0.26377937\n",
      "Iteration 28, loss = 0.31824602\n",
      "Iteration 46, loss = 0.26112792\n",
      "Iteration 29, loss = 0.31253587\n",
      "Iteration 47, loss = 0.25840669\n",
      "Iteration 30, loss = 0.30774145\n",
      "Iteration 48, loss = 0.25540106\n",
      "Iteration 31, loss = 0.30193227\n",
      "Iteration 49, loss = 0.25263857\n",
      "Iteration 32, loss = 0.29694161\n",
      "Iteration 50, loss = 0.24978992\n",
      "Iteration 33, loss = 0.29198652\n",
      "Iteration 51, loss = 0.24753847\n",
      "Iteration 34, loss = 0.28816906\n",
      "Iteration 52, loss = 0.24532024\n",
      "Iteration 35, loss = 0.28283147\n",
      "Iteration 53, loss = 0.24225607\n",
      "Iteration 36, loss = 0.27927876\n",
      "Iteration 54, loss = 0.23937286\n",
      "Iteration 37, loss = 0.27484405\n",
      "Iteration 55, loss = 0.23743554\n",
      "Iteration 38, loss = 0.27094012\n",
      "Iteration 56, loss = 0.23545954\n",
      "Iteration 39, loss = 0.26725959\n",
      "Iteration 57, loss = 0.23215016\n",
      "Iteration 40, loss = 0.26281104\n",
      "Iteration 58, loss = 0.23098211\n",
      "Iteration 41, loss = 0.25911933\n",
      "Iteration 59, loss = 0.22833025\n",
      "Iteration 42, loss = 0.25557796\n",
      "Iteration 60, loss = 0.22712963\n",
      "Iteration 43, loss = 0.25152014\n",
      "Iteration 61, loss = 0.22371791\n",
      "Iteration 44, loss = 0.24841679\n",
      "Iteration 62, loss = 0.22252311\n",
      "Iteration 45, loss = 0.24453964\n",
      "Iteration 63, loss = 0.21977455\n",
      "Iteration 46, loss = 0.24216925\n",
      "Iteration 64, loss = 0.21809658\n",
      "Iteration 47, loss = 0.23874758\n",
      "Iteration 65, loss = 0.21681818\n",
      "Iteration 48, loss = 0.23555603\n",
      "Iteration 66, loss = 0.21345909\n",
      "Iteration 67, loss = 0.21214554\n",
      "Iteration 49, loss = 0.23211193\n",
      "Iteration 68, loss = 0.21036552\n",
      "Iteration 50, loss = 0.22909599\n",
      "Iteration 69, loss = 0.20800650\n",
      "Iteration 51, loss = 0.22719711\n",
      "Iteration 70, loss = 0.20528016\n",
      "Iteration 52, loss = 0.22257931\n",
      "Iteration 71, loss = 0.20419713\n",
      "Iteration 53, loss = 0.22000587\n",
      "Iteration 72, loss = 0.20217475\n",
      "Iteration 54, loss = 0.21716011\n",
      "Iteration 73, loss = 0.20023724\n",
      "Iteration 55, loss = 0.21434735\n",
      "Iteration 74, loss = 0.19934196\n",
      "Iteration 56, loss = 0.21263148\n",
      "Iteration 75, loss = 0.19805838\n",
      "Iteration 57, loss = 0.20836137\n",
      "Iteration 76, loss = 0.19658850\n",
      "Iteration 58, loss = 0.20581143\n",
      "Iteration 77, loss = 0.19519083\n",
      "Iteration 59, loss = 0.20473754\n",
      "Iteration 78, loss = 0.19249854\n",
      "Iteration 60, loss = 0.20151915\n",
      "Iteration 79, loss = 0.19062896\n",
      "Iteration 61, loss = 0.19838390\n",
      "Iteration 80, loss = 0.18950543\n",
      "Iteration 62, loss = 0.19715915\n",
      "Iteration 81, loss = 0.18906669\n",
      "Iteration 63, loss = 0.19317573\n",
      "Iteration 82, loss = 0.18411914\n",
      "Iteration 64, loss = 0.19184188\n",
      "Iteration 83, loss = 0.18520539\n",
      "Iteration 65, loss = 0.19021999\n",
      "Iteration 84, loss = 0.18312715\n",
      "Iteration 66, loss = 0.18708119\n",
      "Iteration 85, loss = 0.18054099\n",
      "Iteration 67, loss = 0.18360183\n",
      "Iteration 86, loss = 0.17826617\n",
      "Iteration 68, loss = 0.18194525\n",
      "Iteration 87, loss = 0.17833671\n",
      "Iteration 69, loss = 0.17975721\n",
      "Iteration 88, loss = 0.17662603\n",
      "Iteration 70, loss = 0.17608140\n",
      "Iteration 89, loss = 0.17650400\n",
      "Iteration 71, loss = 0.17597658\n",
      "Iteration 72, loss = 0.17237084Iteration 90, loss = 0.17336256\n",
      "\n",
      "Iteration 91, loss = 0.17099910Iteration 73, loss = 0.17052048\n",
      "\n",
      "Iteration 74, loss = 0.16789857\n",
      "Iteration 92, loss = 0.17000499\n",
      "Iteration 75, loss = 0.16448059\n",
      "Iteration 93, loss = 0.16835419\n",
      "Iteration 76, loss = 0.16365766\n",
      "Iteration 94, loss = 0.16760502\n",
      "Iteration 77, loss = 0.16022169\n",
      "Iteration 95, loss = 0.16683375\n",
      "Iteration 96, loss = 0.16442143\n",
      "Iteration 78, loss = 0.15879394\n",
      "Iteration 79, loss = 0.15577032\n",
      "Iteration 97, loss = 0.16306023\n",
      "Iteration 98, loss = 0.16208781\n",
      "Iteration 80, loss = 0.15477905\n",
      "Iteration 99, loss = 0.15947679\n",
      "Iteration 81, loss = 0.15229965\n",
      "Iteration 100, loss = 0.15928297\n",
      "Iteration 82, loss = 0.15136120\n",
      "Iteration 101, loss = 0.15719064\n",
      "Iteration 83, loss = 0.14852870\n",
      "Iteration 102, loss = 0.15625752\n",
      "Iteration 84, loss = 0.14799549\n",
      "Iteration 103, loss = 0.15472573\n",
      "Iteration 85, loss = 0.14526880\n",
      "Iteration 104, loss = 0.15392537\n",
      "Iteration 86, loss = 0.14351133\n",
      "Iteration 105, loss = 0.15124129\n",
      "Iteration 87, loss = 0.14301433\n",
      "Iteration 106, loss = 0.15016419\n",
      "Iteration 88, loss = 0.13978518\n",
      "Iteration 107, loss = 0.15087684\n",
      "Iteration 89, loss = 0.13823019\n",
      "Iteration 108, loss = 0.14623398\n",
      "Iteration 90, loss = 0.13869496\n",
      "Iteration 109, loss = 0.14677064\n",
      "Iteration 91, loss = 0.13489890\n",
      "Iteration 110, loss = 0.14422652\n",
      "Iteration 92, loss = 0.13501635\n",
      "Iteration 111, loss = 0.14379470\n",
      "Iteration 93, loss = 0.13250376\n",
      "Iteration 112, loss = 0.14445386\n",
      "Iteration 94, loss = 0.13047738\n",
      "Iteration 113, loss = 0.14143339\n",
      "Iteration 95, loss = 0.12993904\n",
      "Iteration 114, loss = 0.14316121\n",
      "Iteration 96, loss = 0.12839434\n",
      "Iteration 115, loss = 0.13958016\n",
      "Iteration 97, loss = 0.12577851\n",
      "Iteration 116, loss = 0.13772355\n",
      "Iteration 98, loss = 0.12550903\n",
      "Iteration 117, loss = 0.13411571\n",
      "Iteration 99, loss = 0.12556791\n",
      "Iteration 118, loss = 0.13660739\n",
      "Iteration 100, loss = 0.12175487\n",
      "Iteration 119, loss = 0.13247699\n",
      "Iteration 101, loss = 0.12042465\n",
      "Iteration 120, loss = 0.13504877\n",
      "Iteration 102, loss = 0.11897768\n",
      "Iteration 121, loss = 0.13522241\n",
      "Iteration 103, loss = 0.11881143\n",
      "Iteration 122, loss = 0.13276082\n",
      "Iteration 104, loss = 0.11705283\n",
      "Iteration 123, loss = 0.12947632\n",
      "Iteration 105, loss = 0.11662044\n",
      "Iteration 124, loss = 0.12731191\n",
      "Iteration 106, loss = 0.11502762\n",
      "Iteration 125, loss = 0.12711126\n",
      "Iteration 107, loss = 0.11305360\n",
      "Iteration 126, loss = 0.12539939\n",
      "Iteration 108, loss = 0.11157528\n",
      "Iteration 127, loss = 0.12556371\n",
      "Iteration 109, loss = 0.11128977\n",
      "Iteration 128, loss = 0.12511701\n",
      "Iteration 110, loss = 0.11170140\n",
      "Iteration 129, loss = 0.12436893\n",
      "Iteration 111, loss = 0.11097793\n",
      "Iteration 130, loss = 0.12200670\n",
      "Iteration 112, loss = 0.10690536\n",
      "Iteration 131, loss = 0.12465339\n",
      "Iteration 113, loss = 0.10685275\n",
      "Iteration 132, loss = 0.12007342\n",
      "Iteration 114, loss = 0.10487422\n",
      "Iteration 133, loss = 0.12138922\n",
      "Iteration 115, loss = 0.10605684\n",
      "Iteration 134, loss = 0.11891006\n",
      "Iteration 116, loss = 0.10317666\n",
      "Iteration 135, loss = 0.11689450\n",
      "Iteration 117, loss = 0.10336843\n",
      "Iteration 136, loss = 0.11562126\n",
      "Iteration 118, loss = 0.10600714\n",
      "Iteration 137, loss = 0.11508366\n",
      "Iteration 119, loss = 0.10052355\n",
      "Iteration 138, loss = 0.11423778\n",
      "Iteration 120, loss = 0.09944090\n",
      "Iteration 139, loss = 0.11396538\n",
      "Iteration 121, loss = 0.10085251\n",
      "Iteration 140, loss = 0.11484646\n",
      "Iteration 122, loss = 0.09797060\n",
      "Iteration 141, loss = 0.11247686\n",
      "Iteration 123, loss = 0.09706440\n",
      "Iteration 142, loss = 0.11393247\n",
      "Iteration 124, loss = 0.09773043\n",
      "Iteration 143, loss = 0.10966838\n",
      "Iteration 125, loss = 0.09733870\n",
      "Iteration 144, loss = 0.11036102\n",
      "Iteration 126, loss = 0.09290723\n",
      "Iteration 145, loss = 0.11328406\n",
      "Iteration 127, loss = 0.09445208\n",
      "Iteration 146, loss = 0.11004264\n",
      "Iteration 128, loss = 0.09160147\n",
      "Iteration 147, loss = 0.10961085\n",
      "Iteration 129, loss = 0.09121375\n",
      "Iteration 148, loss = 0.10898352\n",
      "Iteration 130, loss = 0.09118275\n",
      "Iteration 149, loss = 0.11077464\n",
      "Iteration 131, loss = 0.09017612\n",
      "Iteration 150, loss = 0.10556472\n",
      "Iteration 132, loss = 0.09172794\n",
      "Iteration 151, loss = 0.10420233\n",
      "Iteration 133, loss = 0.08982850\n",
      "Iteration 152, loss = 0.10427023\n",
      "Iteration 134, loss = 0.08861786\n",
      "Iteration 153, loss = 0.10378456\n",
      "Iteration 135, loss = 0.08739125\n",
      "Iteration 154, loss = 0.10158248\n",
      "Iteration 136, loss = 0.08449097\n",
      "Iteration 155, loss = 0.10106067\n",
      "Iteration 137, loss = 0.08571489\n",
      "Iteration 156, loss = 0.09976499\n",
      "Iteration 138, loss = 0.08373514\n",
      "Iteration 157, loss = 0.09962816\n",
      "Iteration 139, loss = 0.08226145\n",
      "Iteration 158, loss = 0.09972267\n",
      "Iteration 140, loss = 0.08193947\n",
      "Iteration 159, loss = 0.09822074\n",
      "Iteration 141, loss = 0.08180586\n",
      "Iteration 160, loss = 0.09778567\n",
      "Iteration 142, loss = 0.08288545\n",
      "Iteration 161, loss = 0.09890245\n",
      "Iteration 143, loss = 0.07929293\n",
      "Iteration 162, loss = 0.09745881\n",
      "Iteration 144, loss = 0.07820527\n",
      "Iteration 163, loss = 0.10381927\n",
      "Iteration 145, loss = 0.07873750\n",
      "Iteration 164, loss = 0.09766270\n",
      "Iteration 146, loss = 0.07759153\n",
      "Iteration 165, loss = 0.09748146\n",
      "Iteration 147, loss = 0.07793856\n",
      "Iteration 166, loss = 0.09527386\n",
      "Iteration 148, loss = 0.07569462\n",
      "Iteration 167, loss = 0.09333279\n",
      "Iteration 149, loss = 0.07640899\n",
      "Iteration 168, loss = 0.09326681\n",
      "Iteration 150, loss = 0.07675183\n",
      "Iteration 169, loss = 0.09279381\n",
      "Iteration 151, loss = 0.07495054\n",
      "Iteration 170, loss = 0.09493900\n",
      "Iteration 152, loss = 0.07291867\n",
      "Iteration 171, loss = 0.08968231\n",
      "Iteration 153, loss = 0.07407838\n",
      "Iteration 172, loss = 0.09175640\n",
      "Iteration 154, loss = 0.07205477\n",
      "Iteration 173, loss = 0.08897224\n",
      "Iteration 155, loss = 0.07180118\n",
      "Iteration 174, loss = 0.08880153\n",
      "Iteration 156, loss = 0.07032703\n",
      "Iteration 175, loss = 0.08880264\n",
      "Iteration 157, loss = 0.06899219\n",
      "Iteration 176, loss = 0.08998065\n",
      "Iteration 158, loss = 0.06886825\n",
      "Iteration 177, loss = 0.09057303\n",
      "Iteration 159, loss = 0.06782227\n",
      "Iteration 178, loss = 0.08558571\n",
      "Iteration 160, loss = 0.06782309\n",
      "Iteration 179, loss = 0.08700226\n",
      "Iteration 161, loss = 0.06685040\n",
      "Iteration 180, loss = 0.08626663\n",
      "Iteration 162, loss = 0.06664124\n",
      "Iteration 181, loss = 0.08554080\n",
      "Iteration 163, loss = 0.06589383\n",
      "Iteration 182, loss = 0.08340094\n",
      "Iteration 164, loss = 0.06620099\n",
      "Iteration 183, loss = 0.08655076\n",
      "Iteration 165, loss = 0.06483930\n",
      "Iteration 184, loss = 0.08469008\n",
      "Iteration 166, loss = 0.06469868\n",
      "Iteration 185, loss = 0.08168064\n",
      "Iteration 167, loss = 0.06372633\n",
      "Iteration 186, loss = 0.08601501\n",
      "Iteration 168, loss = 0.06379669\n",
      "Iteration 187, loss = 0.08573901\n",
      "Iteration 169, loss = 0.06222992\n",
      "Iteration 188, loss = 0.08030471\n",
      "Iteration 170, loss = 0.06130076\n",
      "Iteration 189, loss = 0.08332952\n",
      "Iteration 171, loss = 0.06075984\n",
      "Iteration 190, loss = 0.08146205\n",
      "Iteration 172, loss = 0.06086243\n",
      "Iteration 191, loss = 0.07754388\n",
      "Iteration 173, loss = 0.06045172\n",
      "Iteration 192, loss = 0.08060184\n",
      "Iteration 174, loss = 0.06127799\n",
      "Iteration 193, loss = 0.07850936\n",
      "Iteration 175, loss = 0.05938611\n",
      "Iteration 194, loss = 0.07711088\n",
      "Iteration 176, loss = 0.05867551\n",
      "Iteration 195, loss = 0.07759674\n",
      "Iteration 177, loss = 0.06094995\n",
      "Iteration 196, loss = 0.07627759\n",
      "Iteration 178, loss = 0.06066151\n",
      "Iteration 197, loss = 0.07581455\n",
      "Iteration 179, loss = 0.05843950\n",
      "Iteration 198, loss = 0.07466474\n",
      "Iteration 180, loss = 0.05785855\n",
      "Iteration 199, loss = 0.07487528\n",
      "Iteration 181, loss = 0.05827193\n",
      "Iteration 200, loss = 0.07439989\n",
      "Iteration 182, loss = 0.05812085\n",
      "Iteration 201, loss = 0.07269342\n",
      "Iteration 183, loss = 0.05762645\n",
      "Iteration 202, loss = 0.07299014\n",
      "Iteration 184, loss = 0.05476804\n",
      "Iteration 203, loss = 0.07332427\n",
      "Iteration 185, loss = 0.05458737\n",
      "Iteration 204, loss = 0.07285211\n",
      "Iteration 186, loss = 0.05479581\n",
      "Iteration 205, loss = 0.07176656\n",
      "Iteration 187, loss = 0.05276620\n",
      "Iteration 206, loss = 0.07422164\n",
      "Iteration 188, loss = 0.05343113\n",
      "Iteration 207, loss = 0.07307055\n",
      "Iteration 189, loss = 0.05251290\n",
      "Iteration 208, loss = 0.06869843\n",
      "Iteration 190, loss = 0.05180338\n",
      "Iteration 209, loss = 0.07101088\n",
      "Iteration 191, loss = 0.05290047\n",
      "Iteration 210, loss = 0.06911043\n",
      "Iteration 192, loss = 0.05125397\n",
      "Iteration 211, loss = 0.06982677\n",
      "Iteration 193, loss = 0.05150669\n",
      "Iteration 212, loss = 0.06896243\n",
      "Iteration 194, loss = 0.05175343\n",
      "Iteration 213, loss = 0.07222880\n",
      "Iteration 195, loss = 0.05071905\n",
      "Iteration 214, loss = 0.06904594\n",
      "Iteration 196, loss = 0.05070965\n",
      "Iteration 215, loss = 0.06876977\n",
      "Iteration 197, loss = 0.04903719\n",
      "Iteration 216, loss = 0.07045986\n",
      "Iteration 198, loss = 0.04923242\n",
      "Iteration 217, loss = 0.06764391\n",
      "Iteration 199, loss = 0.04798806\n",
      "Iteration 218, loss = 0.06700532\n",
      "Iteration 200, loss = 0.04789044\n",
      "Iteration 219, loss = 0.06621873\n",
      "Iteration 201, loss = 0.04695283\n",
      "Iteration 220, loss = 0.06460443\n",
      "Iteration 202, loss = 0.04672878\n",
      "Iteration 221, loss = 0.06510213\n",
      "Iteration 203, loss = 0.04651269\n",
      "Iteration 222, loss = 0.06325137\n",
      "Iteration 204, loss = 0.04594784\n",
      "Iteration 223, loss = 0.06470313\n",
      "Iteration 205, loss = 0.04512463\n",
      "Iteration 224, loss = 0.06286303\n",
      "Iteration 206, loss = 0.04598854\n",
      "Iteration 225, loss = 0.06395108\n",
      "Iteration 207, loss = 0.04526144\n",
      "Iteration 226, loss = 0.06297383\n",
      "Iteration 208, loss = 0.04414305\n",
      "Iteration 227, loss = 0.06183705\n",
      "Iteration 209, loss = 0.04413251\n",
      "Iteration 228, loss = 0.06154127\n",
      "Iteration 210, loss = 0.04362964\n",
      "Iteration 229, loss = 0.06059734\n",
      "Iteration 211, loss = 0.04398625\n",
      "Iteration 230, loss = 0.06003014\n",
      "Iteration 212, loss = 0.04355147\n",
      "Iteration 231, loss = 0.05960989\n",
      "Iteration 213, loss = 0.04267605\n",
      "Iteration 232, loss = 0.05929151\n",
      "Iteration 214, loss = 0.04225044\n",
      "Iteration 233, loss = 0.05988998\n",
      "Iteration 215, loss = 0.04142456\n",
      "Iteration 234, loss = 0.05859694\n",
      "Iteration 216, loss = 0.04118113\n",
      "Iteration 235, loss = 0.05895163\n",
      "Iteration 217, loss = 0.04073882\n",
      "Iteration 236, loss = 0.05978162\n",
      "Iteration 218, loss = 0.04079704\n",
      "Iteration 237, loss = 0.05752874\n",
      "Iteration 219, loss = 0.04127980\n",
      "Iteration 238, loss = 0.06038830\n",
      "Iteration 220, loss = 0.04202326\n",
      "Iteration 221, loss = 0.04082195\n",
      "Iteration 239, loss = 0.05838761\n",
      "Iteration 240, loss = 0.05721143\n",
      "Iteration 222, loss = 0.04115216\n",
      "Iteration 241, loss = 0.05845130\n",
      "Iteration 223, loss = 0.03991772\n",
      "Iteration 242, loss = 0.05646483\n",
      "Iteration 224, loss = 0.04121441\n",
      "Iteration 243, loss = 0.05601614\n",
      "Iteration 225, loss = 0.03801658\n",
      "Iteration 244, loss = 0.05546241\n",
      "Iteration 226, loss = 0.03872464\n",
      "Iteration 245, loss = 0.05527548\n",
      "Iteration 246, loss = 0.05481126\n",
      "Iteration 227, loss = 0.03773672\n",
      "Iteration 247, loss = 0.05514627\n",
      "Iteration 228, loss = 0.03738688\n",
      "Iteration 248, loss = 0.05468429\n",
      "Iteration 229, loss = 0.03703585\n",
      "Iteration 249, loss = 0.05313175\n",
      "Iteration 230, loss = 0.03672134\n",
      "Iteration 250, loss = 0.05332044\n",
      "Iteration 231, loss = 0.03623520\n",
      "Iteration 251, loss = 0.05314608\n",
      "Iteration 232, loss = 0.03620354\n",
      "Iteration 252, loss = 0.05189264\n",
      "Iteration 233, loss = 0.03533210\n",
      "Iteration 253, loss = 0.05284696\n",
      "Iteration 234, loss = 0.03540159\n",
      "Iteration 254, loss = 0.05178086\n",
      "Iteration 235, loss = 0.03464702\n",
      "Iteration 255, loss = 0.05151632\n",
      "Iteration 236, loss = 0.03533496\n",
      "Iteration 256, loss = 0.05081993\n",
      "Iteration 237, loss = 0.03450840\n",
      "Iteration 257, loss = 0.05067751\n",
      "Iteration 238, loss = 0.03409498\n",
      "Iteration 258, loss = 0.05325963\n",
      "Iteration 239, loss = 0.03357666\n",
      "Iteration 259, loss = 0.05078495\n",
      "Iteration 240, loss = 0.03404894\n",
      "Iteration 260, loss = 0.05103698\n",
      "Iteration 241, loss = 0.03498194\n",
      "Iteration 261, loss = 0.04979592\n",
      "Iteration 242, loss = 0.03394761\n",
      "Iteration 262, loss = 0.05105430\n",
      "Iteration 243, loss = 0.03268673\n",
      "Iteration 263, loss = 0.04817546\n",
      "Iteration 244, loss = 0.03330046\n",
      "Iteration 264, loss = 0.04807891\n",
      "Iteration 245, loss = 0.03227522\n",
      "Iteration 265, loss = 0.04930577\n",
      "Iteration 246, loss = 0.03213351\n",
      "Iteration 266, loss = 0.04822248\n",
      "Iteration 247, loss = 0.03222545\n",
      "Iteration 267, loss = 0.04729095\n",
      "Iteration 248, loss = 0.03139584\n",
      "Iteration 268, loss = 0.04642122\n",
      "Iteration 249, loss = 0.03140717\n",
      "Iteration 269, loss = 0.04675982\n",
      "Iteration 250, loss = 0.03114533\n",
      "Iteration 270, loss = 0.04669199\n",
      "Iteration 251, loss = 0.03024979\n",
      "Iteration 271, loss = 0.04582878\n",
      "Iteration 252, loss = 0.02997915\n",
      "Iteration 272, loss = 0.04537456\n",
      "Iteration 253, loss = 0.03033670\n",
      "Iteration 273, loss = 0.04631328\n",
      "Iteration 254, loss = 0.03071697\n",
      "Iteration 274, loss = 0.04457073\n",
      "Iteration 255, loss = 0.03011893\n",
      "Iteration 275, loss = 0.04557534\n",
      "Iteration 256, loss = 0.03057331\n",
      "Iteration 276, loss = 0.04560782\n",
      "Iteration 257, loss = 0.03096517\n",
      "Iteration 277, loss = 0.04384848\n",
      "Iteration 258, loss = 0.02909375\n",
      "Iteration 278, loss = 0.04391897\n",
      "Iteration 259, loss = 0.02974622\n",
      "Iteration 279, loss = 0.04389134\n",
      "Iteration 260, loss = 0.02866342\n",
      "Iteration 280, loss = 0.04421460\n",
      "Iteration 261, loss = 0.02853360\n",
      "Iteration 281, loss = 0.04363106\n",
      "Iteration 262, loss = 0.02831028\n",
      "Iteration 282, loss = 0.04245591\n",
      "Iteration 263, loss = 0.02749864\n",
      "Iteration 283, loss = 0.04215714\n",
      "Iteration 264, loss = 0.02751225\n",
      "Iteration 284, loss = 0.04398242\n",
      "Iteration 265, loss = 0.02722426\n",
      "Iteration 285, loss = 0.04313344\n",
      "Iteration 266, loss = 0.02657032\n",
      "Iteration 286, loss = 0.04250186\n",
      "Iteration 267, loss = 0.02678155\n",
      "Iteration 287, loss = 0.04150424\n",
      "Iteration 268, loss = 0.02613530\n",
      "Iteration 288, loss = 0.04149240\n",
      "Iteration 269, loss = 0.02606598\n",
      "Iteration 289, loss = 0.04175442\n",
      "Iteration 270, loss = 0.02630547\n",
      "Iteration 290, loss = 0.04005608\n",
      "Iteration 271, loss = 0.02529902\n",
      "Iteration 291, loss = 0.04121526\n",
      "Iteration 272, loss = 0.02547291\n",
      "Iteration 292, loss = 0.04088831\n",
      "Iteration 273, loss = 0.02535266\n",
      "Iteration 293, loss = 0.04108172\n",
      "Iteration 274, loss = 0.02483750\n",
      "Iteration 294, loss = 0.04042304\n",
      "Iteration 275, loss = 0.02449942\n",
      "Iteration 295, loss = 0.03961005\n",
      "Iteration 276, loss = 0.02454475\n",
      "Iteration 296, loss = 0.03922059\n",
      "Iteration 277, loss = 0.02452759\n",
      "Iteration 297, loss = 0.03987914\n",
      "Iteration 278, loss = 0.02378614\n",
      "Iteration 298, loss = 0.03850176\n",
      "Iteration 279, loss = 0.02477125\n",
      "Iteration 299, loss = 0.03967763\n",
      "Iteration 280, loss = 0.02397293\n",
      "Iteration 300, loss = 0.03832324\n",
      "Iteration 281, loss = 0.02339670\n",
      "Iteration 301, loss = 0.03781866\n",
      "Iteration 282, loss = 0.02406424\n",
      "Iteration 302, loss = 0.03832734\n",
      "Iteration 283, loss = 0.02345323\n",
      "Iteration 303, loss = 0.03834915\n",
      "Iteration 284, loss = 0.02231423\n",
      "Iteration 304, loss = 0.03710117\n",
      "Iteration 285, loss = 0.02351292\n",
      "Iteration 305, loss = 0.03782484\n",
      "Iteration 286, loss = 0.02248508\n",
      "Iteration 306, loss = 0.03868174\n",
      "Iteration 287, loss = 0.02205749\n",
      "Iteration 307, loss = 0.03972882\n",
      "Iteration 288, loss = 0.02204618\n",
      "Iteration 308, loss = 0.03731689\n",
      "Iteration 289, loss = 0.02227611\n",
      "Iteration 309, loss = 0.04060092\n",
      "Iteration 290, loss = 0.02327701\n",
      "Iteration 310, loss = 0.03525863\n",
      "Iteration 291, loss = 0.02210187\n",
      "Iteration 311, loss = 0.03903749\n",
      "Iteration 292, loss = 0.02119540\n",
      "Iteration 312, loss = 0.03718524\n",
      "Iteration 293, loss = 0.02159431\n",
      "Iteration 313, loss = 0.03667468\n",
      "Iteration 294, loss = 0.02136708\n",
      "Iteration 314, loss = 0.03486005\n",
      "Iteration 295, loss = 0.02146727\n",
      "Iteration 315, loss = 0.03604195\n",
      "Iteration 296, loss = 0.02095675\n",
      "Iteration 316, loss = 0.03447110\n",
      "Iteration 297, loss = 0.02053857\n",
      "Iteration 317, loss = 0.03403497\n",
      "Iteration 298, loss = 0.02050988\n",
      "Iteration 318, loss = 0.03391340\n",
      "Iteration 299, loss = 0.02023067\n",
      "Iteration 319, loss = 0.03375537\n",
      "Iteration 300, loss = 0.01975339\n",
      "Iteration 320, loss = 0.03301662\n",
      "Iteration 301, loss = 0.01985584\n",
      "Iteration 321, loss = 0.03375890\n",
      "Iteration 302, loss = 0.02016058\n",
      "Iteration 322, loss = 0.03395391\n",
      "Iteration 303, loss = 0.01955772\n",
      "Iteration 323, loss = 0.03241454\n",
      "Iteration 304, loss = 0.01996749\n",
      "Iteration 324, loss = 0.03307242\n",
      "Iteration 305, loss = 0.01927488\n",
      "Iteration 325, loss = 0.03245767\n",
      "Iteration 306, loss = 0.01884721\n",
      "Iteration 326, loss = 0.03195917\n",
      "Iteration 307, loss = 0.01849884\n",
      "Iteration 327, loss = 0.03162454\n",
      "Iteration 308, loss = 0.01843714\n",
      "Iteration 328, loss = 0.03197203\n",
      "Iteration 309, loss = 0.01836633\n",
      "Iteration 329, loss = 0.03152422\n",
      "Iteration 310, loss = 0.01832319\n",
      "Iteration 330, loss = 0.03179686\n",
      "Iteration 311, loss = 0.01818482\n",
      "Iteration 331, loss = 0.03130890\n",
      "Iteration 312, loss = 0.01838190\n",
      "Iteration 332, loss = 0.03119783\n",
      "Iteration 313, loss = 0.01859619\n",
      "Iteration 333, loss = 0.03190870\n",
      "Iteration 314, loss = 0.01783088\n",
      "Iteration 334, loss = 0.03135043\n",
      "Iteration 315, loss = 0.01821194\n",
      "Iteration 335, loss = 0.03246240\n",
      "Iteration 316, loss = 0.01821261\n",
      "Iteration 336, loss = 0.03067586\n",
      "Iteration 317, loss = 0.01722802\n",
      "Iteration 337, loss = 0.02967713\n",
      "Iteration 338, loss = 0.02968138\n",
      "Iteration 318, loss = 0.01840780\n",
      "Iteration 339, loss = 0.03051105\n",
      "Iteration 319, loss = 0.01909015\n",
      "Iteration 340, loss = 0.02980361\n",
      "Iteration 320, loss = 0.01780806\n",
      "Iteration 341, loss = 0.02939523\n",
      "Iteration 321, loss = 0.01800269\n",
      "Iteration 342, loss = 0.02902625\n",
      "Iteration 322, loss = 0.01692387\n",
      "Iteration 343, loss = 0.02873062\n",
      "Iteration 323, loss = 0.01678934\n",
      "Iteration 344, loss = 0.02894320\n",
      "Iteration 324, loss = 0.01699992\n",
      "Iteration 345, loss = 0.03104786\n",
      "Iteration 325, loss = 0.01613647\n",
      "Iteration 346, loss = 0.03517631\n",
      "Iteration 326, loss = 0.01584575\n",
      "Iteration 347, loss = 0.03633153\n",
      "Iteration 327, loss = 0.01587457\n",
      "Iteration 348, loss = 0.02926925\n",
      "Iteration 328, loss = 0.01598127\n",
      "Iteration 349, loss = 0.02959246\n",
      "Iteration 329, loss = 0.01608067\n",
      "Iteration 350, loss = 0.03062628\n",
      "Iteration 330, loss = 0.01539911\n",
      "Iteration 351, loss = 0.02837340\n",
      "Iteration 331, loss = 0.01654898\n",
      "Iteration 352, loss = 0.02808847\n",
      "Iteration 332, loss = 0.01563978\n",
      "Iteration 353, loss = 0.02717527\n",
      "Iteration 333, loss = 0.01553224\n",
      "Iteration 354, loss = 0.02865858\n",
      "Iteration 334, loss = 0.01602451\n",
      "Iteration 355, loss = 0.02833389\n",
      "Iteration 335, loss = 0.01624068\n",
      "Iteration 356, loss = 0.02822947\n",
      "Iteration 336, loss = 0.01566119\n",
      "Iteration 357, loss = 0.02649650\n",
      "Iteration 337, loss = 0.01463332\n",
      "Iteration 358, loss = 0.02670646\n",
      "Iteration 338, loss = 0.01595926\n",
      "Iteration 359, loss = 0.02568248\n",
      "Iteration 339, loss = 0.01472606\n",
      "Iteration 360, loss = 0.02680135\n",
      "Iteration 340, loss = 0.01522062\n",
      "Iteration 361, loss = 0.02721961\n",
      "Iteration 341, loss = 0.01538157\n",
      "Iteration 362, loss = 0.02896280\n",
      "Iteration 342, loss = 0.01408973\n",
      "Iteration 363, loss = 0.02853218\n",
      "Iteration 343, loss = 0.01543775\n",
      "Iteration 364, loss = 0.02715449\n",
      "Iteration 344, loss = 0.01524016\n",
      "Iteration 365, loss = 0.02619605\n",
      "Iteration 345, loss = 0.01507898\n",
      "Iteration 366, loss = 0.02533346\n",
      "Iteration 346, loss = 0.01352701\n",
      "Iteration 367, loss = 0.02513322\n",
      "Iteration 347, loss = 0.01538395\n",
      "Iteration 368, loss = 0.02480246\n",
      "Iteration 348, loss = 0.01534676\n",
      "Iteration 369, loss = 0.02570287\n",
      "Iteration 349, loss = 0.01453973\n",
      "Iteration 370, loss = 0.02512558\n",
      "Iteration 350, loss = 0.01360223\n",
      "Iteration 371, loss = 0.02546895\n",
      "Iteration 351, loss = 0.01349740\n",
      "Iteration 372, loss = 0.02342190\n",
      "Iteration 352, loss = 0.01302637\n",
      "Iteration 373, loss = 0.02614010\n",
      "Iteration 353, loss = 0.01281122\n",
      "Iteration 374, loss = 0.02448153\n",
      "Iteration 354, loss = 0.01287420\n",
      "Iteration 375, loss = 0.02442019\n",
      "Iteration 355, loss = 0.01291255\n",
      "Iteration 376, loss = 0.02605551\n",
      "Iteration 356, loss = 0.01270573\n",
      "Iteration 377, loss = 0.02483040\n",
      "Iteration 357, loss = 0.01265377\n",
      "Iteration 378, loss = 0.02574326\n",
      "Iteration 358, loss = 0.01220220\n",
      "Iteration 379, loss = 0.02778160\n",
      "Iteration 359, loss = 0.01221788\n",
      "Iteration 380, loss = 0.02465522\n",
      "Iteration 360, loss = 0.01202888\n",
      "Iteration 381, loss = 0.02277380\n",
      "Iteration 361, loss = 0.01185037\n",
      "Iteration 382, loss = 0.02431517\n",
      "Iteration 362, loss = 0.01203336\n",
      "Iteration 383, loss = 0.02241344\n",
      "Iteration 363, loss = 0.01203743\n",
      "Iteration 384, loss = 0.02259026\n",
      "Iteration 364, loss = 0.01177938\n",
      "Iteration 385, loss = 0.02279725\n",
      "Iteration 365, loss = 0.01183409\n",
      "Iteration 386, loss = 0.02210800\n",
      "Iteration 366, loss = 0.01143067\n",
      "Iteration 387, loss = 0.02287002\n",
      "Iteration 367, loss = 0.01136213\n",
      "Iteration 388, loss = 0.02225283\n",
      "Iteration 368, loss = 0.01138869\n",
      "Iteration 389, loss = 0.02224249\n",
      "Iteration 369, loss = 0.01119921\n",
      "Iteration 390, loss = 0.02154011\n",
      "Iteration 370, loss = 0.01181806\n",
      "Iteration 391, loss = 0.02179137\n",
      "Iteration 371, loss = 0.01095298\n",
      "Iteration 392, loss = 0.02168268\n",
      "Iteration 372, loss = 0.01131820\n",
      "Iteration 393, loss = 0.02136214\n",
      "Iteration 373, loss = 0.01090452\n",
      "Iteration 394, loss = 0.02202475\n",
      "Iteration 374, loss = 0.01081306\n",
      "Iteration 395, loss = 0.02161092\n",
      "Iteration 375, loss = 0.01080199\n",
      "Iteration 396, loss = 0.02344741\n",
      "Iteration 376, loss = 0.01087592\n",
      "Iteration 397, loss = 0.02079757\n",
      "Iteration 377, loss = 0.01132051\n",
      "Iteration 398, loss = 0.02356844\n",
      "Iteration 378, loss = 0.01045146\n",
      "Iteration 399, loss = 0.02111162\n",
      "Iteration 379, loss = 0.01083381\n",
      "Iteration 400, loss = 0.02171950\n",
      "Iteration 380, loss = 0.01021164\n",
      "Iteration 401, loss = 0.02189280\n",
      "Iteration 381, loss = 0.01029353\n",
      "Iteration 402, loss = 0.02022840\n",
      "Iteration 382, loss = 0.00993693\n",
      "Iteration 403, loss = 0.02062882\n",
      "Iteration 383, loss = 0.01018320\n",
      "Iteration 404, loss = 0.02039174\n",
      "Iteration 384, loss = 0.01003800\n",
      "Iteration 405, loss = 0.01990228\n",
      "Iteration 385, loss = 0.00982342\n",
      "Iteration 406, loss = 0.01940105\n",
      "Iteration 386, loss = 0.00966610\n",
      "Iteration 407, loss = 0.02018243\n",
      "Iteration 387, loss = 0.00975069\n",
      "Iteration 408, loss = 0.02089649\n",
      "Iteration 388, loss = 0.00986528\n",
      "Iteration 409, loss = 0.01962970\n",
      "Iteration 389, loss = 0.00986085\n",
      "Iteration 410, loss = 0.01967945\n",
      "Iteration 390, loss = 0.01014779\n",
      "Iteration 411, loss = 0.01976148\n",
      "Iteration 391, loss = 0.00994989\n",
      "Iteration 412, loss = 0.01901800\n",
      "Iteration 392, loss = 0.01028853\n",
      "Iteration 413, loss = 0.01917659\n",
      "Iteration 393, loss = 0.01001291\n",
      "Iteration 414, loss = 0.01976093\n",
      "Iteration 394, loss = 0.01035737\n",
      "Iteration 415, loss = 0.02127443\n",
      "Iteration 395, loss = 0.01055419\n",
      "Iteration 416, loss = 0.02046414\n",
      "Iteration 396, loss = 0.00953520\n",
      "Iteration 417, loss = 0.02024740\n",
      "Iteration 397, loss = 0.00895956\n",
      "Iteration 418, loss = 0.01925104\n",
      "Iteration 398, loss = 0.00914629\n",
      "Iteration 419, loss = 0.02090485\n",
      "Iteration 399, loss = 0.00886098\n",
      "Iteration 420, loss = 0.02127971\n",
      "Iteration 400, loss = 0.00898722\n",
      "Iteration 421, loss = 0.01894683\n",
      "Iteration 401, loss = 0.00875496\n",
      "Iteration 422, loss = 0.01805542\n",
      "Iteration 402, loss = 0.00879814\n",
      "Iteration 423, loss = 0.01842108\n",
      "Iteration 403, loss = 0.00875721\n",
      "Iteration 424, loss = 0.01819925\n",
      "Iteration 404, loss = 0.00841151\n",
      "Iteration 425, loss = 0.01943397\n",
      "Iteration 405, loss = 0.00860468\n",
      "Iteration 426, loss = 0.02327162\n",
      "Iteration 406, loss = 0.00844955\n",
      "Iteration 427, loss = 0.02153581\n",
      "Iteration 407, loss = 0.00862930\n",
      "Iteration 428, loss = 0.01869274\n",
      "Iteration 408, loss = 0.00822871\n",
      "Iteration 429, loss = 0.01805355\n",
      "Iteration 409, loss = 0.00834308\n",
      "Iteration 430, loss = 0.01788978\n",
      "Iteration 410, loss = 0.00814405\n",
      "Iteration 431, loss = 0.02143628\n",
      "Iteration 411, loss = 0.00818570\n",
      "Iteration 432, loss = 0.02042307\n",
      "Iteration 412, loss = 0.00792452\n",
      "Iteration 433, loss = 0.01854526\n",
      "Iteration 413, loss = 0.00810415\n",
      "Iteration 434, loss = 0.01699625\n",
      "Iteration 414, loss = 0.00780863\n",
      "Iteration 435, loss = 0.01752373\n",
      "Iteration 415, loss = 0.00781343\n",
      "Iteration 436, loss = 0.01875325\n",
      "Iteration 416, loss = 0.00777345\n",
      "Iteration 437, loss = 0.02016927\n",
      "Iteration 417, loss = 0.00768491\n",
      "Iteration 438, loss = 0.01721265\n",
      "Iteration 418, loss = 0.00761630\n",
      "Iteration 439, loss = 0.01767817\n",
      "Iteration 419, loss = 0.00763071\n",
      "Iteration 440, loss = 0.01656490\n",
      "Iteration 420, loss = 0.00754099\n",
      "Iteration 441, loss = 0.01671228\n",
      "Iteration 421, loss = 0.00750731\n",
      "Iteration 442, loss = 0.01613012\n",
      "Iteration 422, loss = 0.00745187\n",
      "Iteration 443, loss = 0.01625062\n",
      "Iteration 423, loss = 0.00735404\n",
      "Iteration 444, loss = 0.01664824\n",
      "Iteration 424, loss = 0.00737031\n",
      "Iteration 445, loss = 0.01645103\n",
      "Iteration 425, loss = 0.00725074\n",
      "Iteration 446, loss = 0.01564386\n",
      "Iteration 426, loss = 0.00739939\n",
      "Iteration 447, loss = 0.01654967\n",
      "Iteration 427, loss = 0.00736226\n",
      "Iteration 448, loss = 0.01567722\n",
      "Iteration 428, loss = 0.00702303\n",
      "Iteration 449, loss = 0.01527302\n",
      "Iteration 429, loss = 0.00722703\n",
      "Iteration 450, loss = 0.01575496\n",
      "Iteration 430, loss = 0.00710148\n",
      "Iteration 451, loss = 0.01624606\n",
      "Iteration 431, loss = 0.00701676\n",
      "Iteration 452, loss = 0.01649939\n",
      "Iteration 432, loss = 0.00707180\n",
      "Iteration 453, loss = 0.01709592\n",
      "Iteration 433, loss = 0.00694161\n",
      "Iteration 454, loss = 0.01527422\n",
      "Iteration 434, loss = 0.00699258\n",
      "Iteration 455, loss = 0.01581333\n",
      "Iteration 435, loss = 0.00676054\n",
      "Iteration 456, loss = 0.01646803\n",
      "Iteration 436, loss = 0.00693742\n",
      "Iteration 457, loss = 0.01519483\n",
      "Iteration 437, loss = 0.00689076\n",
      "Iteration 438, loss = 0.00679657\n",
      "Iteration 458, loss = 0.01497145\n",
      "Iteration 439, loss = 0.00661391\n",
      "Iteration 459, loss = 0.01699373\n",
      "Iteration 460, loss = 0.01683483\n",
      "Iteration 440, loss = 0.00691823\n",
      "Iteration 461, loss = 0.01625952\n",
      "Iteration 441, loss = 0.00690039\n",
      "Iteration 442, loss = 0.00675744\n",
      "Iteration 462, loss = 0.01938266\n",
      "Iteration 463, loss = 0.01651319Iteration 443, loss = 0.00635635\n",
      "\n",
      "Iteration 444, loss = 0.00661795\n",
      "Iteration 464, loss = 0.01444585\n",
      "Iteration 445, loss = 0.00660927\n",
      "Iteration 465, loss = 0.01537392\n",
      "Iteration 446, loss = 0.00671045\n",
      "Iteration 466, loss = 0.01494884\n",
      "Iteration 447, loss = 0.00664251\n",
      "Iteration 467, loss = 0.01473763\n",
      "Iteration 448, loss = 0.00633283\n",
      "Iteration 468, loss = 0.01478981\n",
      "Iteration 449, loss = 0.00676393\n",
      "Iteration 469, loss = 0.01619118\n",
      "Iteration 450, loss = 0.00628553\n",
      "Iteration 470, loss = 0.01894018\n",
      "Iteration 451, loss = 0.00684795\n",
      "Iteration 471, loss = 0.01622967\n",
      "Iteration 452, loss = 0.00640525\n",
      "Iteration 472, loss = 0.01360039\n",
      "Iteration 453, loss = 0.00594331\n",
      "Iteration 473, loss = 0.01411681\n",
      "Iteration 454, loss = 0.00622953\n",
      "Iteration 474, loss = 0.01407161\n",
      "Iteration 455, loss = 0.00588576\n",
      "Iteration 475, loss = 0.01467109\n",
      "Iteration 456, loss = 0.00602025\n",
      "Iteration 476, loss = 0.01274499\n",
      "Iteration 457, loss = 0.00619475\n",
      "Iteration 477, loss = 0.01380534\n",
      "Iteration 458, loss = 0.00604428\n",
      "Iteration 478, loss = 0.01474611\n",
      "Iteration 459, loss = 0.00594886\n",
      "Iteration 479, loss = 0.01312947\n",
      "Iteration 460, loss = 0.00599484\n",
      "Iteration 480, loss = 0.01327862\n",
      "Iteration 461, loss = 0.00565636\n",
      "Iteration 481, loss = 0.01354306\n",
      "Iteration 462, loss = 0.00585142\n",
      "Iteration 482, loss = 0.01317359\n",
      "Iteration 463, loss = 0.00569503\n",
      "Iteration 483, loss = 0.01367483\n",
      "Iteration 464, loss = 0.00548550\n",
      "Iteration 484, loss = 0.01269453\n",
      "Iteration 465, loss = 0.00561192\n",
      "Iteration 485, loss = 0.01298626\n",
      "Iteration 466, loss = 0.00561564\n",
      "Iteration 486, loss = 0.01237256\n",
      "Iteration 467, loss = 0.00532650\n",
      "Iteration 487, loss = 0.01251232\n",
      "Iteration 468, loss = 0.00565306\n",
      "Iteration 488, loss = 0.01355618\n",
      "Iteration 469, loss = 0.00528203\n",
      "Iteration 489, loss = 0.01279626\n",
      "Iteration 470, loss = 0.00539803\n",
      "Iteration 490, loss = 0.01195319\n",
      "Iteration 471, loss = 0.00521250\n",
      "Iteration 491, loss = 0.01220008\n",
      "Iteration 472, loss = 0.00533548\n",
      "Iteration 492, loss = 0.01294538\n",
      "Iteration 473, loss = 0.00528336\n",
      "Iteration 493, loss = 0.01234955\n",
      "Iteration 474, loss = 0.00519112\n",
      "Iteration 494, loss = 0.01196970\n",
      "Iteration 475, loss = 0.00511080\n",
      "Iteration 495, loss = 0.01174765\n",
      "Iteration 476, loss = 0.00511044\n",
      "Iteration 496, loss = 0.01211945\n",
      "Iteration 477, loss = 0.00516108\n",
      "Iteration 497, loss = 0.01172927\n",
      "Iteration 478, loss = 0.00513600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 498, loss = 0.01168735\n",
      "Iteration 499, loss = 0.01173437\n",
      "Iteration 500, loss = 0.01140686\n",
      "Iteration 1, loss = 1.22940881\n",
      "Iteration 2, loss = 1.02949694\n",
      "Iteration 3, loss = 0.91552035\n",
      "Iteration 1, loss = 1.13801360\n",
      "Iteration 4, loss = 0.81583505\n",
      "Iteration 2, loss = 0.90933033\n",
      "Iteration 5, loss = 0.73613387\n",
      "Iteration 3, loss = 0.80355322\n",
      "Iteration 6, loss = 0.67601204\n",
      "Iteration 4, loss = 0.72129342\n",
      "Iteration 7, loss = 0.62491204\n",
      "Iteration 5, loss = 0.66620778\n",
      "Iteration 8, loss = 0.58104705\n",
      "Iteration 6, loss = 0.62573895\n",
      "Iteration 9, loss = 0.54757813\n",
      "Iteration 7, loss = 0.59097512\n",
      "Iteration 10, loss = 0.51647012\n",
      "Iteration 8, loss = 0.55989990\n",
      "Iteration 11, loss = 0.49101362\n",
      "Iteration 9, loss = 0.53460588\n",
      "Iteration 12, loss = 0.46892224\n",
      "Iteration 10, loss = 0.51065002\n",
      "Iteration 13, loss = 0.45026646\n",
      "Iteration 11, loss = 0.49116319\n",
      "Iteration 14, loss = 0.43309995\n",
      "Iteration 12, loss = 0.47257717\n",
      "Iteration 15, loss = 0.41818485\n",
      "Iteration 13, loss = 0.45668047\n",
      "Iteration 16, loss = 0.40471953\n",
      "Iteration 14, loss = 0.44264199\n",
      "Iteration 17, loss = 0.39276105\n",
      "Iteration 15, loss = 0.42967107\n",
      "Iteration 18, loss = 0.38189963\n",
      "Iteration 16, loss = 0.41732059\n",
      "Iteration 19, loss = 0.37156780\n",
      "Iteration 17, loss = 0.40666093\n",
      "Iteration 20, loss = 0.36294151\n",
      "Iteration 18, loss = 0.39669115\n",
      "Iteration 21, loss = 0.35441235\n",
      "Iteration 19, loss = 0.38695595\n",
      "Iteration 22, loss = 0.34653383\n",
      "Iteration 20, loss = 0.37950516\n",
      "Iteration 23, loss = 0.33941016\n",
      "Iteration 21, loss = 0.37170176\n",
      "Iteration 24, loss = 0.33240738\n",
      "Iteration 22, loss = 0.36329481\n",
      "Iteration 25, loss = 0.32606038\n",
      "Iteration 23, loss = 0.35734867\n",
      "Iteration 26, loss = 0.32052600\n",
      "Iteration 24, loss = 0.35009810\n",
      "Iteration 27, loss = 0.31476753\n",
      "Iteration 25, loss = 0.34372494\n",
      "Iteration 28, loss = 0.30944700\n",
      "Iteration 26, loss = 0.33851131\n",
      "Iteration 29, loss = 0.30384565\n",
      "Iteration 27, loss = 0.33430137\n",
      "Iteration 30, loss = 0.29959523\n",
      "Iteration 28, loss = 0.32706285\n",
      "Iteration 31, loss = 0.29485619\n",
      "Iteration 29, loss = 0.32239551\n",
      "Iteration 32, loss = 0.29026722\n",
      "Iteration 30, loss = 0.31801537\n",
      "Iteration 33, loss = 0.28613779\n",
      "Iteration 31, loss = 0.31303922\n",
      "Iteration 34, loss = 0.28203346\n",
      "Iteration 32, loss = 0.30867794\n",
      "Iteration 35, loss = 0.27821062\n",
      "Iteration 33, loss = 0.30422272\n",
      "Iteration 36, loss = 0.27436383\n",
      "Iteration 34, loss = 0.30017513\n",
      "Iteration 37, loss = 0.27046555\n",
      "Iteration 35, loss = 0.29688017\n",
      "Iteration 38, loss = 0.26674177\n",
      "Iteration 36, loss = 0.29326816\n",
      "Iteration 39, loss = 0.26274210\n",
      "Iteration 37, loss = 0.28870455\n",
      "Iteration 40, loss = 0.26008442\n",
      "Iteration 38, loss = 0.28524035\n",
      "Iteration 41, loss = 0.25675930\n",
      "Iteration 39, loss = 0.28278870\n",
      "Iteration 42, loss = 0.25399809\n",
      "Iteration 40, loss = 0.27839825\n",
      "Iteration 43, loss = 0.25032576\n",
      "Iteration 41, loss = 0.27529936\n",
      "Iteration 44, loss = 0.24690642\n",
      "Iteration 42, loss = 0.27341014\n",
      "Iteration 45, loss = 0.24389120\n",
      "Iteration 43, loss = 0.26946248\n",
      "Iteration 46, loss = 0.24133559\n",
      "Iteration 44, loss = 0.26696428\n",
      "Iteration 47, loss = 0.23830557\n",
      "Iteration 45, loss = 0.26298518\n",
      "Iteration 48, loss = 0.23544356\n",
      "Iteration 46, loss = 0.26174824\n",
      "Iteration 49, loss = 0.23335591\n",
      "Iteration 47, loss = 0.25713834\n",
      "Iteration 50, loss = 0.23000191\n",
      "Iteration 48, loss = 0.25524288\n",
      "Iteration 51, loss = 0.22751490\n",
      "Iteration 49, loss = 0.25269379\n",
      "Iteration 52, loss = 0.22592038\n",
      "Iteration 50, loss = 0.24980750\n",
      "Iteration 53, loss = 0.22239821\n",
      "Iteration 51, loss = 0.24717948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, loss = 0.22175430\n",
      "Iteration 52, loss = 0.24477159\n",
      "Iteration 55, loss = 0.21807441\n",
      "Iteration 53, loss = 0.24229313\n",
      "Iteration 56, loss = 0.21496459\n",
      "Iteration 54, loss = 0.24035006\n",
      "Iteration 57, loss = 0.21191811\n",
      "Iteration 55, loss = 0.23818503\n",
      "Iteration 58, loss = 0.21099925\n",
      "Iteration 56, loss = 0.23556194\n",
      "Iteration 59, loss = 0.20848470\n",
      "Iteration 57, loss = 0.23433942\n",
      "Iteration 60, loss = 0.20632733\n",
      "Iteration 58, loss = 0.23052422\n",
      "Iteration 61, loss = 0.20322106\n",
      "Iteration 59, loss = 0.22829955\n",
      "Iteration 62, loss = 0.20067197\n",
      "Iteration 60, loss = 0.22647326\n",
      "Iteration 63, loss = 0.19871367\n",
      "Iteration 61, loss = 0.22450304\n",
      "Iteration 64, loss = 0.19753874\n",
      "Iteration 62, loss = 0.22190286\n",
      "Iteration 65, loss = 0.19479069\n",
      "Iteration 63, loss = 0.21984627\n",
      "Iteration 66, loss = 0.19270429\n",
      "Iteration 64, loss = 0.21766162\n",
      "Iteration 67, loss = 0.19108107\n",
      "Iteration 65, loss = 0.21483059\n",
      "Iteration 68, loss = 0.18906415\n",
      "Iteration 66, loss = 0.21352948\n",
      "Iteration 69, loss = 0.18684653\n",
      "Iteration 67, loss = 0.21123366\n",
      "Iteration 70, loss = 0.18498208\n",
      "Iteration 68, loss = 0.20929223\n",
      "Iteration 71, loss = 0.18305537\n",
      "Iteration 69, loss = 0.20730061\n",
      "Iteration 72, loss = 0.18130287\n",
      "Iteration 70, loss = 0.20522359\n",
      "Iteration 73, loss = 0.17936185\n",
      "Iteration 71, loss = 0.20398306\n",
      "Iteration 74, loss = 0.17795294\n",
      "Iteration 72, loss = 0.20305558\n",
      "Iteration 75, loss = 0.17645288\n",
      "Iteration 73, loss = 0.20076053\n",
      "Iteration 76, loss = 0.17429788\n",
      "Iteration 74, loss = 0.19827615\n",
      "Iteration 77, loss = 0.17230386\n",
      "Iteration 75, loss = 0.19732064\n",
      "Iteration 78, loss = 0.17104905\n",
      "Iteration 76, loss = 0.19636152\n",
      "Iteration 79, loss = 0.16990154\n",
      "Iteration 77, loss = 0.19318595\n",
      "Iteration 80, loss = 0.16761683\n",
      "Iteration 78, loss = 0.19040429\n",
      "Iteration 81, loss = 0.16648305\n",
      "Iteration 79, loss = 0.18795805\n",
      "Iteration 82, loss = 0.16512307\n",
      "Iteration 80, loss = 0.18668665\n",
      "Iteration 83, loss = 0.16395868\n",
      "Iteration 81, loss = 0.18476158\n",
      "Iteration 84, loss = 0.16174152\n",
      "Iteration 82, loss = 0.18324840\n",
      "Iteration 85, loss = 0.16143614\n",
      "Iteration 83, loss = 0.18254602\n",
      "Iteration 86, loss = 0.16052789\n",
      "Iteration 84, loss = 0.17932240\n",
      "Iteration 87, loss = 0.15848787\n",
      "Iteration 85, loss = 0.17769029\n",
      "Iteration 88, loss = 0.15614856\n",
      "Iteration 86, loss = 0.17658642\n",
      "Iteration 89, loss = 0.15449782\n",
      "Iteration 87, loss = 0.17623778\n",
      "Iteration 90, loss = 0.15368172\n",
      "Iteration 88, loss = 0.17263432\n",
      "Iteration 91, loss = 0.15342635\n",
      "Iteration 89, loss = 0.17186318\n",
      "Iteration 92, loss = 0.15016840\n",
      "Iteration 90, loss = 0.16945117\n",
      "Iteration 93, loss = 0.14966586\n",
      "Iteration 91, loss = 0.16847917\n",
      "Iteration 94, loss = 0.14927962\n",
      "Iteration 92, loss = 0.16771422\n",
      "Iteration 95, loss = 0.14760431\n",
      "Iteration 93, loss = 0.16577086\n",
      "Iteration 96, loss = 0.14562743\n",
      "Iteration 94, loss = 0.16335842\n",
      "Iteration 97, loss = 0.14471075\n",
      "Iteration 95, loss = 0.16221498\n",
      "Iteration 98, loss = 0.14329557\n",
      "Iteration 96, loss = 0.16012102\n",
      "Iteration 99, loss = 0.14261886\n",
      "Iteration 97, loss = 0.15810884\n",
      "Iteration 100, loss = 0.14164932\n",
      "Iteration 98, loss = 0.15778547\n",
      "Iteration 101, loss = 0.14023960\n",
      "Iteration 99, loss = 0.15533980\n",
      "Iteration 102, loss = 0.13856790\n",
      "Iteration 100, loss = 0.15636264\n",
      "Iteration 103, loss = 0.13799014\n",
      "Iteration 101, loss = 0.15216629\n",
      "Iteration 104, loss = 0.14002950\n",
      "Iteration 102, loss = 0.15024075\n",
      "Iteration 105, loss = 0.13736135\n",
      "Iteration 103, loss = 0.14949617\n",
      "Iteration 106, loss = 0.13436463\n",
      "Iteration 104, loss = 0.14782241\n",
      "Iteration 107, loss = 0.13399921\n",
      "Iteration 105, loss = 0.14559311\n",
      "Iteration 108, loss = 0.13263097\n",
      "Iteration 106, loss = 0.14495729\n",
      "Iteration 109, loss = 0.13122688\n",
      "Iteration 107, loss = 0.14497471\n",
      "Iteration 110, loss = 0.13043394\n",
      "Iteration 108, loss = 0.14027745\n",
      "Iteration 111, loss = 0.13066084\n",
      "Iteration 109, loss = 0.14052926\n",
      "Iteration 112, loss = 0.12804492\n",
      "Iteration 110, loss = 0.13856330\n",
      "Iteration 113, loss = 0.12793101\n",
      "Iteration 111, loss = 0.13807706\n",
      "Iteration 114, loss = 0.12549982\n",
      "Iteration 112, loss = 0.13618468\n",
      "Iteration 115, loss = 0.12613968\n",
      "Iteration 113, loss = 0.13454941\n",
      "Iteration 116, loss = 0.12434234\n",
      "Iteration 114, loss = 0.13292936\n",
      "Iteration 117, loss = 0.12372900\n",
      "Iteration 115, loss = 0.13217622\n",
      "Iteration 118, loss = 0.12250273\n",
      "Iteration 116, loss = 0.13130493\n",
      "Iteration 119, loss = 0.12208250\n",
      "Iteration 117, loss = 0.12968535\n",
      "Iteration 120, loss = 0.12053906\n",
      "Iteration 118, loss = 0.13030851\n",
      "Iteration 121, loss = 0.11984465\n",
      "Iteration 119, loss = 0.12712257\n",
      "Iteration 122, loss = 0.11915737\n",
      "Iteration 120, loss = 0.12685221\n",
      "Iteration 123, loss = 0.11823367\n",
      "Iteration 121, loss = 0.12716620\n",
      "Iteration 124, loss = 0.11866751\n",
      "Iteration 122, loss = 0.12331757\n",
      "Iteration 125, loss = 0.11564135\n",
      "Iteration 123, loss = 0.12504846\n",
      "Iteration 126, loss = 0.11605736\n",
      "Iteration 124, loss = 0.12271762\n",
      "Iteration 127, loss = 0.11525800\n",
      "Iteration 125, loss = 0.12244981\n",
      "Iteration 128, loss = 0.11619617\n",
      "Iteration 126, loss = 0.12073323\n",
      "Iteration 129, loss = 0.11263508\n",
      "Iteration 127, loss = 0.11977961\n",
      "Iteration 130, loss = 0.11333978\n",
      "Iteration 128, loss = 0.11781686\n",
      "Iteration 131, loss = 0.11372891\n",
      "Iteration 129, loss = 0.11917179\n",
      "Iteration 132, loss = 0.11076222\n",
      "Iteration 130, loss = 0.11667191\n",
      "Iteration 133, loss = 0.11128702\n",
      "Iteration 131, loss = 0.11649731\n",
      "Iteration 134, loss = 0.11048604\n",
      "Iteration 135, loss = 0.10833462\n",
      "Iteration 132, loss = 0.11542941\n",
      "Iteration 133, loss = 0.11409604\n",
      "Iteration 136, loss = 0.10978824\n",
      "Iteration 134, loss = 0.11281856\n",
      "Iteration 137, loss = 0.10611628\n",
      "Iteration 135, loss = 0.11262946\n",
      "Iteration 138, loss = 0.10833885\n",
      "Iteration 136, loss = 0.11232587\n",
      "Iteration 139, loss = 0.10699506\n",
      "Iteration 137, loss = 0.11114042\n",
      "Iteration 140, loss = 0.10586232\n",
      "Iteration 138, loss = 0.10918338\n",
      "Iteration 141, loss = 0.10435586\n",
      "Iteration 139, loss = 0.10918618\n",
      "Iteration 142, loss = 0.10479744\n",
      "Iteration 140, loss = 0.10786608\n",
      "Iteration 143, loss = 0.10275600\n",
      "Iteration 141, loss = 0.10883932\n",
      "Iteration 144, loss = 0.10313790\n",
      "Iteration 142, loss = 0.10913842\n",
      "Iteration 145, loss = 0.10098849\n",
      "Iteration 143, loss = 0.10528865\n",
      "Iteration 146, loss = 0.10053518\n",
      "Iteration 144, loss = 0.10910388\n",
      "Iteration 147, loss = 0.09965310\n",
      "Iteration 145, loss = 0.10443859\n",
      "Iteration 148, loss = 0.09945831\n",
      "Iteration 146, loss = 0.10447522\n",
      "Iteration 149, loss = 0.09914462\n",
      "Iteration 147, loss = 0.10329817\n",
      "Iteration 150, loss = 0.09745848\n",
      "Iteration 148, loss = 0.10129820\n",
      "Iteration 151, loss = 0.09785045\n",
      "Iteration 149, loss = 0.09986424\n",
      "Iteration 152, loss = 0.09702748\n",
      "Iteration 150, loss = 0.10091547\n",
      "Iteration 153, loss = 0.09572572\n",
      "Iteration 151, loss = 0.09867040\n",
      "Iteration 154, loss = 0.09552967\n",
      "Iteration 152, loss = 0.09842720\n",
      "Iteration 155, loss = 0.09541329\n",
      "Iteration 153, loss = 0.09762429\n",
      "Iteration 156, loss = 0.09370917\n",
      "Iteration 154, loss = 0.09621065\n",
      "Iteration 157, loss = 0.09428587\n",
      "Iteration 155, loss = 0.09655903\n",
      "Iteration 158, loss = 0.09518008\n",
      "Iteration 156, loss = 0.09723069\n",
      "Iteration 159, loss = 0.09386914\n",
      "Iteration 157, loss = 0.09910727\n",
      "Iteration 160, loss = 0.09244988\n",
      "Iteration 158, loss = 0.09340862\n",
      "Iteration 161, loss = 0.09204529\n",
      "Iteration 159, loss = 0.09456450\n",
      "Iteration 162, loss = 0.09068520\n",
      "Iteration 160, loss = 0.09625713\n",
      "Iteration 163, loss = 0.09150283\n",
      "Iteration 161, loss = 0.09416969\n",
      "Iteration 164, loss = 0.08973226\n",
      "Iteration 162, loss = 0.09162609\n",
      "Iteration 165, loss = 0.08884244\n",
      "Iteration 163, loss = 0.09209827\n",
      "Iteration 166, loss = 0.08912490\n",
      "Iteration 164, loss = 0.09368423\n",
      "Iteration 167, loss = 0.08866284\n",
      "Iteration 165, loss = 0.09299344\n",
      "Iteration 168, loss = 0.08831455\n",
      "Iteration 166, loss = 0.09170838\n",
      "Iteration 169, loss = 0.08693594\n",
      "Iteration 167, loss = 0.08772048\n",
      "Iteration 170, loss = 0.08610615\n",
      "Iteration 168, loss = 0.08771239\n",
      "Iteration 171, loss = 0.08540847\n",
      "Iteration 169, loss = 0.08898479\n",
      "Iteration 172, loss = 0.08500780\n",
      "Iteration 170, loss = 0.08506037\n",
      "Iteration 173, loss = 0.08491905\n",
      "Iteration 171, loss = 0.08619879\n",
      "Iteration 174, loss = 0.08381478\n",
      "Iteration 172, loss = 0.08541676\n",
      "Iteration 175, loss = 0.08325905\n",
      "Iteration 173, loss = 0.08405089\n",
      "Iteration 176, loss = 0.08280068\n",
      "Iteration 174, loss = 0.08297993\n",
      "Iteration 177, loss = 0.08558638\n",
      "Iteration 175, loss = 0.08509097\n",
      "Iteration 178, loss = 0.08187378\n",
      "Iteration 176, loss = 0.08296783\n",
      "Iteration 179, loss = 0.08321730\n",
      "Iteration 177, loss = 0.08313281\n",
      "Iteration 180, loss = 0.08431346\n",
      "Iteration 178, loss = 0.08170822\n",
      "Iteration 181, loss = 0.08116398\n",
      "Iteration 179, loss = 0.08117081\n",
      "Iteration 182, loss = 0.08032750\n",
      "Iteration 180, loss = 0.08002049\n",
      "Iteration 183, loss = 0.07992168\n",
      "Iteration 181, loss = 0.07904313\n",
      "Iteration 184, loss = 0.07835469\n",
      "Iteration 182, loss = 0.07949174\n",
      "Iteration 185, loss = 0.07959023\n",
      "Iteration 183, loss = 0.07771928\n",
      "Iteration 186, loss = 0.07738439\n",
      "Iteration 187, loss = 0.07745470\n",
      "Iteration 184, loss = 0.07709599\n",
      "Iteration 185, loss = 0.07747472\n",
      "Iteration 188, loss = 0.07929678\n",
      "Iteration 186, loss = 0.07584395\n",
      "Iteration 189, loss = 0.07609725\n",
      "Iteration 187, loss = 0.07724286\n",
      "Iteration 190, loss = 0.07657394\n",
      "Iteration 191, loss = 0.07599823\n",
      "Iteration 188, loss = 0.07479429\n",
      "Iteration 189, loss = 0.07525793\n",
      "Iteration 192, loss = 0.07600827\n",
      "Iteration 193, loss = 0.07422394\n",
      "Iteration 190, loss = 0.07557791\n",
      "Iteration 191, loss = 0.07246149\n",
      "Iteration 194, loss = 0.07421767\n",
      "Iteration 195, loss = 0.07599924\n",
      "Iteration 192, loss = 0.07505396\n",
      "Iteration 196, loss = 0.07478543\n",
      "Iteration 193, loss = 0.07309042\n",
      "Iteration 197, loss = 0.07368189\n",
      "Iteration 194, loss = 0.07269457\n",
      "Iteration 195, loss = 0.07250724\n",
      "Iteration 198, loss = 0.07266824\n",
      "Iteration 196, loss = 0.07025319\n",
      "Iteration 199, loss = 0.07294048\n",
      "Iteration 200, loss = 0.07155814\n",
      "Iteration 197, loss = 0.07156011\n",
      "Iteration 201, loss = 0.07133429\n",
      "Iteration 198, loss = 0.06969232\n",
      "Iteration 202, loss = 0.07109148\n",
      "Iteration 199, loss = 0.07070647\n",
      "Iteration 203, loss = 0.06958676\n",
      "Iteration 200, loss = 0.06884409\n",
      "Iteration 204, loss = 0.07051473\n",
      "Iteration 201, loss = 0.07222468\n",
      "Iteration 205, loss = 0.06822557\n",
      "Iteration 202, loss = 0.06989384\n",
      "Iteration 206, loss = 0.06878030\n",
      "Iteration 203, loss = 0.06789288\n",
      "Iteration 207, loss = 0.06971073\n",
      "Iteration 204, loss = 0.06878036\n",
      "Iteration 208, loss = 0.06781869\n",
      "Iteration 205, loss = 0.06790654\n",
      "Iteration 209, loss = 0.06690647\n",
      "Iteration 206, loss = 0.06765499\n",
      "Iteration 210, loss = 0.06768149\n",
      "Iteration 207, loss = 0.06561726\n",
      "Iteration 211, loss = 0.06817396\n",
      "Iteration 208, loss = 0.06500444\n",
      "Iteration 212, loss = 0.06698362\n",
      "Iteration 209, loss = 0.06525923\n",
      "Iteration 213, loss = 0.06731056\n",
      "Iteration 210, loss = 0.06393581\n",
      "Iteration 214, loss = 0.06522252\n",
      "Iteration 211, loss = 0.06447155\n",
      "Iteration 215, loss = 0.06530285\n",
      "Iteration 212, loss = 0.06362375\n",
      "Iteration 216, loss = 0.06592122\n",
      "Iteration 213, loss = 0.06256178\n",
      "Iteration 217, loss = 0.06422925\n",
      "Iteration 214, loss = 0.06228172\n",
      "Iteration 218, loss = 0.06413905\n",
      "Iteration 215, loss = 0.06143099\n",
      "Iteration 219, loss = 0.06335319\n",
      "Iteration 216, loss = 0.06225714\n",
      "Iteration 220, loss = 0.06328296\n",
      "Iteration 217, loss = 0.05995771\n",
      "Iteration 221, loss = 0.06324423\n",
      "Iteration 218, loss = 0.06013570\n",
      "Iteration 222, loss = 0.06429257\n",
      "Iteration 219, loss = 0.05948289\n",
      "Iteration 223, loss = 0.06389395\n",
      "Iteration 220, loss = 0.05917487\n",
      "Iteration 224, loss = 0.06408052\n",
      "Iteration 221, loss = 0.05970925\n",
      "Iteration 225, loss = 0.06005728\n",
      "Iteration 222, loss = 0.05860948\n",
      "Iteration 226, loss = 0.06134477\n",
      "Iteration 223, loss = 0.05750186\n",
      "Iteration 227, loss = 0.05950890\n",
      "Iteration 224, loss = 0.05711034\n",
      "Iteration 228, loss = 0.05933666\n",
      "Iteration 225, loss = 0.05876219\n",
      "Iteration 229, loss = 0.05892482\n",
      "Iteration 226, loss = 0.06126798\n",
      "Iteration 230, loss = 0.05950719\n",
      "Iteration 227, loss = 0.05472265\n",
      "Iteration 231, loss = 0.05887981\n",
      "Iteration 228, loss = 0.05824716\n",
      "Iteration 232, loss = 0.05849213\n",
      "Iteration 229, loss = 0.05764868\n",
      "Iteration 233, loss = 0.05797760\n",
      "Iteration 230, loss = 0.05519795\n",
      "Iteration 234, loss = 0.05803351\n",
      "Iteration 231, loss = 0.05621790\n",
      "Iteration 235, loss = 0.05770503\n",
      "Iteration 232, loss = 0.05597480\n",
      "Iteration 236, loss = 0.05633085\n",
      "Iteration 233, loss = 0.05582876\n",
      "Iteration 237, loss = 0.05650931\n",
      "Iteration 234, loss = 0.05375119\n",
      "Iteration 238, loss = 0.05781087\n",
      "Iteration 235, loss = 0.05547102\n",
      "Iteration 239, loss = 0.05778634\n",
      "Iteration 236, loss = 0.05266006\n",
      "Iteration 240, loss = 0.05767218\n",
      "Iteration 237, loss = 0.05247137\n",
      "Iteration 241, loss = 0.05569521\n",
      "Iteration 238, loss = 0.05265877\n",
      "Iteration 242, loss = 0.05533364\n",
      "Iteration 239, loss = 0.05123394\n",
      "Iteration 243, loss = 0.05548098\n",
      "Iteration 240, loss = 0.05128988\n",
      "Iteration 244, loss = 0.05378854\n",
      "Iteration 241, loss = 0.05121451\n",
      "Iteration 245, loss = 0.05673311\n",
      "Iteration 242, loss = 0.05085380\n",
      "Iteration 246, loss = 0.05591212\n",
      "Iteration 243, loss = 0.05309169\n",
      "Iteration 247, loss = 0.05400525\n",
      "Iteration 244, loss = 0.05319001\n",
      "Iteration 248, loss = 0.05452254\n",
      "Iteration 245, loss = 0.05042431\n",
      "Iteration 249, loss = 0.05189296\n",
      "Iteration 246, loss = 0.04965031\n",
      "Iteration 250, loss = 0.05421828\n",
      "Iteration 247, loss = 0.04840809\n",
      "Iteration 251, loss = 0.05053901\n",
      "Iteration 248, loss = 0.04910443\n",
      "Iteration 252, loss = 0.05238704\n",
      "Iteration 249, loss = 0.04903748\n",
      "Iteration 253, loss = 0.05147447\n",
      "Iteration 250, loss = 0.04698306\n",
      "Iteration 254, loss = 0.05285946\n",
      "Iteration 251, loss = 0.04633914\n",
      "Iteration 255, loss = 0.05280603\n",
      "Iteration 252, loss = 0.04642261\n",
      "Iteration 256, loss = 0.05005467\n",
      "Iteration 253, loss = 0.04730019\n",
      "Iteration 257, loss = 0.04918812\n",
      "Iteration 254, loss = 0.04677015\n",
      "Iteration 258, loss = 0.04981794\n",
      "Iteration 255, loss = 0.04619996\n",
      "Iteration 259, loss = 0.04845768\n",
      "Iteration 256, loss = 0.04549932\n",
      "Iteration 260, loss = 0.04807641\n",
      "Iteration 257, loss = 0.04444644\n",
      "Iteration 261, loss = 0.04861594\n",
      "Iteration 258, loss = 0.04440063\n",
      "Iteration 262, loss = 0.04815904\n",
      "Iteration 259, loss = 0.04482450\n",
      "Iteration 263, loss = 0.04688813\n",
      "Iteration 260, loss = 0.04459213\n",
      "Iteration 264, loss = 0.04706375\n",
      "Iteration 261, loss = 0.04348513\n",
      "Iteration 265, loss = 0.04627874\n",
      "Iteration 262, loss = 0.04350668\n",
      "Iteration 266, loss = 0.04575018\n",
      "Iteration 263, loss = 0.04215808\n",
      "Iteration 267, loss = 0.04523309\n",
      "Iteration 264, loss = 0.04297058\n",
      "Iteration 268, loss = 0.04518866\n",
      "Iteration 265, loss = 0.04263299\n",
      "Iteration 269, loss = 0.04483552\n",
      "Iteration 266, loss = 0.04210295\n",
      "Iteration 270, loss = 0.04451974\n",
      "Iteration 267, loss = 0.04167492\n",
      "Iteration 271, loss = 0.04417924\n",
      "Iteration 268, loss = 0.04001009\n",
      "Iteration 272, loss = 0.04488637\n",
      "Iteration 269, loss = 0.04091386\n",
      "Iteration 273, loss = 0.04363198\n",
      "Iteration 270, loss = 0.04042173\n",
      "Iteration 274, loss = 0.04320403\n",
      "Iteration 271, loss = 0.04035252\n",
      "Iteration 275, loss = 0.04361251\n",
      "Iteration 272, loss = 0.03971920\n",
      "Iteration 276, loss = 0.04534073\n",
      "Iteration 273, loss = 0.04190635\n",
      "Iteration 277, loss = 0.04289025\n",
      "Iteration 274, loss = 0.04237075\n",
      "Iteration 278, loss = 0.04324042\n",
      "Iteration 275, loss = 0.03787335\n",
      "Iteration 279, loss = 0.04231113\n",
      "Iteration 276, loss = 0.04111518\n",
      "Iteration 280, loss = 0.04264719\n",
      "Iteration 277, loss = 0.04032850\n",
      "Iteration 281, loss = 0.04155118\n",
      "Iteration 278, loss = 0.03807190\n",
      "Iteration 282, loss = 0.04185921\n",
      "Iteration 279, loss = 0.03851773\n",
      "Iteration 283, loss = 0.04265057\n",
      "Iteration 280, loss = 0.03677514\n",
      "Iteration 284, loss = 0.04083540\n",
      "Iteration 281, loss = 0.03721921\n",
      "Iteration 285, loss = 0.04002289\n",
      "Iteration 282, loss = 0.03628738\n",
      "Iteration 286, loss = 0.04199921\n",
      "Iteration 283, loss = 0.03642890\n",
      "Iteration 287, loss = 0.04245998\n",
      "Iteration 284, loss = 0.03623242\n",
      "Iteration 288, loss = 0.04182780\n",
      "Iteration 285, loss = 0.03488416\n",
      "Iteration 289, loss = 0.03947857\n",
      "Iteration 286, loss = 0.03562003\n",
      "Iteration 290, loss = 0.04058400\n",
      "Iteration 287, loss = 0.03489085\n",
      "Iteration 291, loss = 0.03948788\n",
      "Iteration 288, loss = 0.03463708\n",
      "Iteration 292, loss = 0.03924081\n",
      "Iteration 289, loss = 0.03386727\n",
      "Iteration 293, loss = 0.03889576\n",
      "Iteration 290, loss = 0.03405710\n",
      "Iteration 294, loss = 0.04039648\n",
      "Iteration 291, loss = 0.03510281\n",
      "Iteration 295, loss = 0.03943030\n",
      "Iteration 292, loss = 0.03529101\n",
      "Iteration 296, loss = 0.03775613\n",
      "Iteration 293, loss = 0.03590160\n",
      "Iteration 297, loss = 0.03790458\n",
      "Iteration 294, loss = 0.03437776\n",
      "Iteration 298, loss = 0.03990922\n",
      "Iteration 295, loss = 0.03338521\n",
      "Iteration 299, loss = 0.03710379\n",
      "Iteration 296, loss = 0.03247639Iteration 300, loss = 0.03669655\n",
      "\n",
      "Iteration 301, loss = 0.03733129\n",
      "Iteration 297, loss = 0.03325570\n",
      "Iteration 298, loss = 0.03393702Iteration 302, loss = 0.03682786\n",
      "\n",
      "Iteration 303, loss = 0.03608924\n",
      "Iteration 299, loss = 0.03279158\n",
      "Iteration 304, loss = 0.03643803\n",
      "Iteration 300, loss = 0.03178835\n",
      "Iteration 305, loss = 0.03531022\n",
      "Iteration 301, loss = 0.03175496\n",
      "Iteration 302, loss = 0.03062748\n",
      "Iteration 306, loss = 0.03529226\n",
      "Iteration 303, loss = 0.03116596\n",
      "Iteration 307, loss = 0.03629261\n",
      "Iteration 304, loss = 0.03105045\n",
      "Iteration 308, loss = 0.03763077\n",
      "Iteration 305, loss = 0.03104560\n",
      "Iteration 309, loss = 0.03593601\n",
      "Iteration 306, loss = 0.03165537\n",
      "Iteration 310, loss = 0.03684456\n",
      "Iteration 307, loss = 0.03041538\n",
      "Iteration 311, loss = 0.03528448\n",
      "Iteration 308, loss = 0.03024978\n",
      "Iteration 312, loss = 0.03359738\n",
      "Iteration 309, loss = 0.03007806\n",
      "Iteration 313, loss = 0.03420022\n",
      "Iteration 310, loss = 0.02973120\n",
      "Iteration 314, loss = 0.03430045\n",
      "Iteration 311, loss = 0.02947354\n",
      "Iteration 315, loss = 0.03312349\n",
      "Iteration 312, loss = 0.02954477\n",
      "Iteration 316, loss = 0.03309895\n",
      "Iteration 313, loss = 0.02899493\n",
      "Iteration 317, loss = 0.03212574\n",
      "Iteration 314, loss = 0.02919842\n",
      "Iteration 318, loss = 0.03315688\n",
      "Iteration 315, loss = 0.02873886\n",
      "Iteration 319, loss = 0.03302027\n",
      "Iteration 316, loss = 0.02833046\n",
      "Iteration 320, loss = 0.03241838\n",
      "Iteration 317, loss = 0.02830783\n",
      "Iteration 321, loss = 0.03155357\n",
      "Iteration 318, loss = 0.02745883\n",
      "Iteration 322, loss = 0.03136607\n",
      "Iteration 319, loss = 0.02712868\n",
      "Iteration 323, loss = 0.03093035\n",
      "Iteration 320, loss = 0.02800761\n",
      "Iteration 324, loss = 0.03197577\n",
      "Iteration 321, loss = 0.02723044\n",
      "Iteration 325, loss = 0.03025745\n",
      "Iteration 322, loss = 0.02768353\n",
      "Iteration 326, loss = 0.03082530\n",
      "Iteration 323, loss = 0.02613985\n",
      "Iteration 327, loss = 0.03111884\n",
      "Iteration 324, loss = 0.02699285\n",
      "Iteration 328, loss = 0.02994128\n",
      "Iteration 325, loss = 0.02595147\n",
      "Iteration 329, loss = 0.03052316\n",
      "Iteration 326, loss = 0.02646108\n",
      "Iteration 330, loss = 0.03077477\n",
      "Iteration 327, loss = 0.02570583\n",
      "Iteration 331, loss = 0.03086354\n",
      "Iteration 328, loss = 0.02711264\n",
      "Iteration 332, loss = 0.03295552\n",
      "Iteration 329, loss = 0.02700265\n",
      "Iteration 333, loss = 0.03083155\n",
      "Iteration 330, loss = 0.02463120\n",
      "Iteration 334, loss = 0.03000106\n",
      "Iteration 331, loss = 0.02538767\n",
      "Iteration 335, loss = 0.02890445\n",
      "Iteration 332, loss = 0.02549115\n",
      "Iteration 336, loss = 0.02945941\n",
      "Iteration 333, loss = 0.02686534\n",
      "Iteration 337, loss = 0.03242541\n",
      "Iteration 334, loss = 0.02549374\n",
      "Iteration 338, loss = 0.03418190\n",
      "Iteration 335, loss = 0.02534840\n",
      "Iteration 339, loss = 0.02949411\n",
      "Iteration 336, loss = 0.02404009\n",
      "Iteration 340, loss = 0.03015674\n",
      "Iteration 337, loss = 0.02304598\n",
      "Iteration 341, loss = 0.02876641\n",
      "Iteration 338, loss = 0.02361314\n",
      "Iteration 342, loss = 0.03126551\n",
      "Iteration 339, loss = 0.02409599\n",
      "Iteration 343, loss = 0.02952488\n",
      "Iteration 340, loss = 0.02357829\n",
      "Iteration 344, loss = 0.02788758\n",
      "Iteration 341, loss = 0.02268087\n",
      "Iteration 345, loss = 0.02788790\n",
      "Iteration 342, loss = 0.02379829\n",
      "Iteration 346, loss = 0.02792512\n",
      "Iteration 343, loss = 0.02231207\n",
      "Iteration 347, loss = 0.02810944\n",
      "Iteration 344, loss = 0.02328350\n",
      "Iteration 348, loss = 0.02626693\n",
      "Iteration 345, loss = 0.02249113\n",
      "Iteration 349, loss = 0.02912835\n",
      "Iteration 346, loss = 0.02336822\n",
      "Iteration 350, loss = 0.02742966\n",
      "Iteration 347, loss = 0.02244075\n",
      "Iteration 351, loss = 0.02567705\n",
      "Iteration 348, loss = 0.02234040\n",
      "Iteration 352, loss = 0.02731995\n",
      "Iteration 349, loss = 0.02126576\n",
      "Iteration 353, loss = 0.02646559\n",
      "Iteration 350, loss = 0.02191684\n",
      "Iteration 354, loss = 0.02804751\n",
      "Iteration 351, loss = 0.02155979\n",
      "Iteration 355, loss = 0.02633112\n",
      "Iteration 352, loss = 0.02226956\n",
      "Iteration 356, loss = 0.02660886\n",
      "Iteration 353, loss = 0.02197148\n",
      "Iteration 357, loss = 0.02504627\n",
      "Iteration 354, loss = 0.02082535\n",
      "Iteration 358, loss = 0.02655845\n",
      "Iteration 355, loss = 0.02290548\n",
      "Iteration 359, loss = 0.02559102\n",
      "Iteration 356, loss = 0.02147642\n",
      "Iteration 360, loss = 0.02423811\n",
      "Iteration 357, loss = 0.02150626\n",
      "Iteration 361, loss = 0.02558669\n",
      "Iteration 358, loss = 0.02036352\n",
      "Iteration 362, loss = 0.02447038\n",
      "Iteration 359, loss = 0.02426384\n",
      "Iteration 363, loss = 0.02457874\n",
      "Iteration 360, loss = 0.02266985\n",
      "Iteration 364, loss = 0.02382868\n",
      "Iteration 361, loss = 0.02145620\n",
      "Iteration 365, loss = 0.02376846\n",
      "Iteration 362, loss = 0.01964810\n",
      "Iteration 366, loss = 0.02324482\n",
      "Iteration 363, loss = 0.02203516\n",
      "Iteration 367, loss = 0.02310054\n",
      "Iteration 364, loss = 0.02108821\n",
      "Iteration 368, loss = 0.02362245\n",
      "Iteration 365, loss = 0.01911828\n",
      "Iteration 369, loss = 0.02409766\n",
      "Iteration 366, loss = 0.01944600\n",
      "Iteration 370, loss = 0.02284649\n",
      "Iteration 367, loss = 0.01891241\n",
      "Iteration 371, loss = 0.02363082\n",
      "Iteration 372, loss = 0.02304089\n",
      "Iteration 368, loss = 0.01849521\n",
      "Iteration 373, loss = 0.02266274\n",
      "Iteration 369, loss = 0.01874337\n",
      "Iteration 374, loss = 0.02513774\n",
      "Iteration 370, loss = 0.01862575\n",
      "Iteration 371, loss = 0.01850972\n",
      "Iteration 375, loss = 0.02509416\n",
      "Iteration 372, loss = 0.01823852\n",
      "Iteration 376, loss = 0.02608345\n",
      "Iteration 377, loss = 0.02568473\n",
      "Iteration 373, loss = 0.01806692\n",
      "Iteration 378, loss = 0.02716250\n",
      "Iteration 374, loss = 0.01791273\n",
      "Iteration 379, loss = 0.02478747\n",
      "Iteration 375, loss = 0.01774248\n",
      "Iteration 380, loss = 0.02332820\n",
      "Iteration 376, loss = 0.01742841\n",
      "Iteration 381, loss = 0.02367117\n",
      "Iteration 377, loss = 0.01749449\n",
      "Iteration 382, loss = 0.02353369\n",
      "Iteration 378, loss = 0.01785000\n",
      "Iteration 383, loss = 0.02343248\n",
      "Iteration 379, loss = 0.01736382\n",
      "Iteration 380, loss = 0.01730374\n",
      "Iteration 384, loss = 0.02261599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 381, loss = 0.01809082\n",
      "Iteration 382, loss = 0.01747324\n",
      "Iteration 383, loss = 0.01669900\n",
      "Iteration 384, loss = 0.01748687\n",
      "Iteration 385, loss = 0.01726807\n",
      "Iteration 386, loss = 0.01762431\n",
      "Iteration 387, loss = 0.01717272\n",
      "Iteration 388, loss = 0.01653432\n",
      "Iteration 389, loss = 0.01705719\n",
      "Iteration 390, loss = 0.01664930\n",
      "Iteration 391, loss = 0.01623735\n",
      "Iteration 392, loss = 0.01654779\n",
      "Iteration 393, loss = 0.01576789\n",
      "Iteration 394, loss = 0.01602641\n",
      "Iteration 395, loss = 0.01526515\n",
      "Iteration 396, loss = 0.01547362\n",
      "Iteration 397, loss = 0.01508527\n",
      "Iteration 398, loss = 0.01497462\n",
      "Iteration 1, loss = 1.62776320\n",
      "Iteration 399, loss = 0.01522186\n",
      "Iteration 2, loss = 1.24888970\n",
      "Iteration 400, loss = 0.01527879\n",
      "Iteration 3, loss = 1.06479759\n",
      "Iteration 401, loss = 0.01474233\n",
      "Iteration 4, loss = 0.96915735\n",
      "Iteration 402, loss = 0.01532664\n",
      "Iteration 5, loss = 0.89351048\n",
      "Iteration 403, loss = 0.01473411\n",
      "Iteration 6, loss = 0.81175774\n",
      "Iteration 404, loss = 0.01476848\n",
      "Iteration 7, loss = 0.73415474\n",
      "Iteration 405, loss = 0.01397962\n",
      "Iteration 8, loss = 0.67279838\n",
      "Iteration 406, loss = 0.01463958\n",
      "Iteration 9, loss = 0.62220321\n",
      "Iteration 407, loss = 0.01453222\n",
      "Iteration 10, loss = 0.57973787\n",
      "Iteration 408, loss = 0.01432641\n",
      "Iteration 11, loss = 0.54367356\n",
      "Iteration 409, loss = 0.01376975\n",
      "Iteration 12, loss = 0.51535945\n",
      "Iteration 410, loss = 0.01404962\n",
      "Iteration 13, loss = 0.49182422\n",
      "Iteration 411, loss = 0.01407776\n",
      "Iteration 14, loss = 0.47195540\n",
      "Iteration 412, loss = 0.01389831\n",
      "Iteration 15, loss = 0.45514682\n",
      "Iteration 413, loss = 0.01363069\n",
      "Iteration 16, loss = 0.43947410\n",
      "Iteration 414, loss = 0.01333436\n",
      "Iteration 17, loss = 0.42584731\n",
      "Iteration 415, loss = 0.01308032\n",
      "Iteration 18, loss = 0.41477107\n",
      "Iteration 416, loss = 0.01438284\n",
      "Iteration 19, loss = 0.40374551\n",
      "Iteration 417, loss = 0.01417892\n",
      "Iteration 20, loss = 0.39419208\n",
      "Iteration 418, loss = 0.01415325\n",
      "Iteration 21, loss = 0.38456683\n",
      "Iteration 419, loss = 0.01258249\n",
      "Iteration 22, loss = 0.37643845\n",
      "Iteration 420, loss = 0.01452688\n",
      "Iteration 23, loss = 0.36907323\n",
      "Iteration 421, loss = 0.01281789\n",
      "Iteration 24, loss = 0.36168961\n",
      "Iteration 422, loss = 0.01302397\n",
      "Iteration 25, loss = 0.35487253\n",
      "Iteration 423, loss = 0.01222534\n",
      "Iteration 26, loss = 0.34899542\n",
      "Iteration 424, loss = 0.01270925\n",
      "Iteration 27, loss = 0.34307255\n",
      "Iteration 425, loss = 0.01333824\n",
      "Iteration 28, loss = 0.33737248\n",
      "Iteration 426, loss = 0.01327483\n",
      "Iteration 29, loss = 0.33221992\n",
      "Iteration 427, loss = 0.01218792\n",
      "Iteration 30, loss = 0.32725425\n",
      "Iteration 428, loss = 0.01276785\n",
      "Iteration 31, loss = 0.32223286\n",
      "Iteration 32, loss = 0.31841952\n",
      "Iteration 429, loss = 0.01172178\n",
      "Iteration 33, loss = 0.31325710\n",
      "Iteration 430, loss = 0.01250133\n",
      "Iteration 34, loss = 0.30887776\n",
      "Iteration 431, loss = 0.01213922\n",
      "Iteration 35, loss = 0.30509958Iteration 432, loss = 0.01179052\n",
      "\n",
      "Iteration 36, loss = 0.30112738\n",
      "Iteration 433, loss = 0.01159571\n",
      "Iteration 37, loss = 0.29725846\n",
      "Iteration 434, loss = 0.01157875\n",
      "Iteration 38, loss = 0.29407186\n",
      "Iteration 435, loss = 0.01154659\n",
      "Iteration 436, loss = 0.01195362\n",
      "Iteration 39, loss = 0.28996389\n",
      "Iteration 437, loss = 0.01154358\n",
      "Iteration 40, loss = 0.28688069\n",
      "Iteration 41, loss = 0.28383773\n",
      "Iteration 438, loss = 0.01226760\n",
      "Iteration 42, loss = 0.27999680\n",
      "Iteration 439, loss = 0.01211561\n",
      "Iteration 43, loss = 0.27666989\n",
      "Iteration 440, loss = 0.01198800\n",
      "Iteration 441, loss = 0.01229579\n",
      "Iteration 44, loss = 0.27413573\n",
      "Iteration 442, loss = 0.01284400\n",
      "Iteration 45, loss = 0.27067332\n",
      "Iteration 46, loss = 0.26778189\n",
      "Iteration 443, loss = 0.01139090\n",
      "Iteration 47, loss = 0.26514443\n",
      "Iteration 444, loss = 0.01061035\n",
      "Iteration 48, loss = 0.26212123\n",
      "Iteration 445, loss = 0.01205554\n",
      "Iteration 49, loss = 0.25879059\n",
      "Iteration 446, loss = 0.01259700\n",
      "Iteration 50, loss = 0.25651608\n",
      "Iteration 447, loss = 0.01382430\n",
      "Iteration 51, loss = 0.25388546\n",
      "Iteration 448, loss = 0.01159909\n",
      "Iteration 52, loss = 0.25145612\n",
      "Iteration 449, loss = 0.01054876\n",
      "Iteration 53, loss = 0.24843906\n",
      "Iteration 450, loss = 0.01108145\n",
      "Iteration 54, loss = 0.24614428\n",
      "Iteration 451, loss = 0.01127111\n",
      "Iteration 55, loss = 0.24313547\n",
      "Iteration 452, loss = 0.01051271\n",
      "Iteration 56, loss = 0.24203269\n",
      "Iteration 453, loss = 0.01042448\n",
      "Iteration 57, loss = 0.23985961\n",
      "Iteration 454, loss = 0.00995959\n",
      "Iteration 58, loss = 0.23710200\n",
      "Iteration 455, loss = 0.01032202\n",
      "Iteration 59, loss = 0.23369333\n",
      "Iteration 456, loss = 0.00975252\n",
      "Iteration 60, loss = 0.23217324\n",
      "Iteration 457, loss = 0.01001916\n",
      "Iteration 61, loss = 0.23016837\n",
      "Iteration 458, loss = 0.00971912\n",
      "Iteration 62, loss = 0.22747926\n",
      "Iteration 459, loss = 0.00962032\n",
      "Iteration 63, loss = 0.22594073\n",
      "Iteration 460, loss = 0.00947045\n",
      "Iteration 64, loss = 0.22354960\n",
      "Iteration 461, loss = 0.00960256\n",
      "Iteration 65, loss = 0.22119232\n",
      "Iteration 462, loss = 0.00991391\n",
      "Iteration 66, loss = 0.21913372\n",
      "Iteration 463, loss = 0.00953140\n",
      "Iteration 67, loss = 0.21823561\n",
      "Iteration 464, loss = 0.01028509\n",
      "Iteration 68, loss = 0.21601594\n",
      "Iteration 465, loss = 0.00955400\n",
      "Iteration 69, loss = 0.21290482\n",
      "Iteration 466, loss = 0.00959561\n",
      "Iteration 70, loss = 0.21122882\n",
      "Iteration 467, loss = 0.00935883\n",
      "Iteration 71, loss = 0.20925162\n",
      "Iteration 468, loss = 0.00918793\n",
      "Iteration 72, loss = 0.20737774\n",
      "Iteration 469, loss = 0.00910129\n",
      "Iteration 73, loss = 0.20627230\n",
      "Iteration 470, loss = 0.00899301\n",
      "Iteration 74, loss = 0.20378489\n",
      "Iteration 471, loss = 0.00915495\n",
      "Iteration 75, loss = 0.20210992\n",
      "Iteration 472, loss = 0.00897489\n",
      "Iteration 76, loss = 0.20173566\n",
      "Iteration 473, loss = 0.00892965\n",
      "Iteration 77, loss = 0.19854744\n",
      "Iteration 474, loss = 0.00874432\n",
      "Iteration 78, loss = 0.19696175\n",
      "Iteration 475, loss = 0.00901122\n",
      "Iteration 79, loss = 0.19596979\n",
      "Iteration 476, loss = 0.00849566\n",
      "Iteration 80, loss = 0.19426755\n",
      "Iteration 477, loss = 0.00870607\n",
      "Iteration 81, loss = 0.19131242\n",
      "Iteration 478, loss = 0.00867202\n",
      "Iteration 82, loss = 0.19039465\n",
      "Iteration 479, loss = 0.00836285\n",
      "Iteration 83, loss = 0.18882939\n",
      "Iteration 480, loss = 0.00870274\n",
      "Iteration 84, loss = 0.18686092\n",
      "Iteration 481, loss = 0.00889570\n",
      "Iteration 85, loss = 0.18632413\n",
      "Iteration 482, loss = 0.00864145\n",
      "Iteration 86, loss = 0.18412460\n",
      "Iteration 483, loss = 0.00860562\n",
      "Iteration 87, loss = 0.18222066\n",
      "Iteration 484, loss = 0.00817484\n",
      "Iteration 88, loss = 0.18037635\n",
      "Iteration 485, loss = 0.00852593\n",
      "Iteration 89, loss = 0.17979230\n",
      "Iteration 486, loss = 0.00825079\n",
      "Iteration 90, loss = 0.17817908\n",
      "Iteration 487, loss = 0.00808203\n",
      "Iteration 91, loss = 0.17603526\n",
      "Iteration 488, loss = 0.00789677\n",
      "Iteration 92, loss = 0.17523819\n",
      "Iteration 489, loss = 0.00795375\n",
      "Iteration 93, loss = 0.17347013\n",
      "Iteration 490, loss = 0.00774825\n",
      "Iteration 94, loss = 0.17093536\n",
      "Iteration 491, loss = 0.00797733\n",
      "Iteration 95, loss = 0.17114175\n",
      "Iteration 492, loss = 0.00773028\n",
      "Iteration 96, loss = 0.16884140\n",
      "Iteration 493, loss = 0.00777611\n",
      "Iteration 97, loss = 0.16709174\n",
      "Iteration 494, loss = 0.00789043\n",
      "Iteration 98, loss = 0.16622458\n",
      "Iteration 495, loss = 0.00765391\n",
      "Iteration 99, loss = 0.16356837\n",
      "Iteration 496, loss = 0.00811329\n",
      "Iteration 100, loss = 0.16336894\n",
      "Iteration 497, loss = 0.00962837\n",
      "Iteration 101, loss = 0.16074706\n",
      "Iteration 498, loss = 0.00784931\n",
      "Iteration 102, loss = 0.16105554\n",
      "Iteration 499, loss = 0.00786051\n",
      "Iteration 103, loss = 0.15879091\n",
      "Iteration 500, loss = 0.00794118\n",
      "Iteration 104, loss = 0.15777852\n",
      "Iteration 105, loss = 0.15563744\n",
      "Iteration 106, loss = 0.15665957\n",
      "Iteration 107, loss = 0.15442593\n",
      "Iteration 108, loss = 0.15261128\n",
      "Iteration 109, loss = 0.15142780\n",
      "Iteration 110, loss = 0.14975124\n",
      "Iteration 111, loss = 0.14882468\n",
      "Iteration 112, loss = 0.14918558\n",
      "Iteration 113, loss = 0.14563318\n",
      "Iteration 114, loss = 0.14545041\n",
      "Iteration 115, loss = 0.14399771\n",
      "Iteration 116, loss = 0.14344172\n",
      "Iteration 117, loss = 0.14196475\n",
      "Iteration 118, loss = 0.14102091\n",
      "Iteration 119, loss = 0.13949240\n",
      "Iteration 120, loss = 0.13744449\n",
      "Iteration 121, loss = 0.13715051\n",
      "Iteration 1, loss = 0.98755719\n",
      "Iteration 122, loss = 0.13627858\n",
      "Iteration 2, loss = 0.86764661\n",
      "Iteration 123, loss = 0.13636878\n",
      "Iteration 3, loss = 0.78163006\n",
      "Iteration 124, loss = 0.13408383\n",
      "Iteration 4, loss = 0.70916605\n",
      "Iteration 125, loss = 0.13339882\n",
      "Iteration 5, loss = 0.65287072\n",
      "Iteration 126, loss = 0.13466865\n",
      "Iteration 6, loss = 0.60403610\n",
      "Iteration 127, loss = 0.13113457\n",
      "Iteration 7, loss = 0.56332982\n",
      "Iteration 128, loss = 0.13026402\n",
      "Iteration 8, loss = 0.52988676\n",
      "Iteration 129, loss = 0.12966915\n",
      "Iteration 9, loss = 0.50156517\n",
      "Iteration 130, loss = 0.12702962\n",
      "Iteration 10, loss = 0.47830528\n",
      "Iteration 131, loss = 0.12727600\n",
      "Iteration 11, loss = 0.46044598\n",
      "Iteration 132, loss = 0.12606275\n",
      "Iteration 12, loss = 0.44246307\n",
      "Iteration 133, loss = 0.12558550\n",
      "Iteration 13, loss = 0.42861815\n",
      "Iteration 134, loss = 0.12356283\n",
      "Iteration 14, loss = 0.41566059\n",
      "Iteration 135, loss = 0.12378812\n",
      "Iteration 15, loss = 0.40491673\n",
      "Iteration 136, loss = 0.12319898\n",
      "Iteration 16, loss = 0.39408953\n",
      "Iteration 137, loss = 0.12140823\n",
      "Iteration 17, loss = 0.38506377\n",
      "Iteration 138, loss = 0.12003033\n",
      "Iteration 18, loss = 0.37638870\n",
      "Iteration 139, loss = 0.11995968\n",
      "Iteration 19, loss = 0.36830595\n",
      "Iteration 140, loss = 0.11893926\n",
      "Iteration 20, loss = 0.36086359\n",
      "Iteration 141, loss = 0.11781830\n",
      "Iteration 21, loss = 0.35359870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 142, loss = 0.11637048\n",
      "Iteration 22, loss = 0.34690557\n",
      "Iteration 143, loss = 0.11579055\n",
      "Iteration 23, loss = 0.34147491\n",
      "Iteration 144, loss = 0.11594433\n",
      "Iteration 24, loss = 0.33489899\n",
      "Iteration 145, loss = 0.11380704\n",
      "Iteration 25, loss = 0.32929117\n",
      "Iteration 146, loss = 0.11333008\n",
      "Iteration 26, loss = 0.32374513\n",
      "Iteration 147, loss = 0.11293845\n",
      "Iteration 27, loss = 0.31861511\n",
      "Iteration 148, loss = 0.11156319\n",
      "Iteration 28, loss = 0.31328434\n",
      "Iteration 149, loss = 0.11174555\n",
      "Iteration 29, loss = 0.30964426\n",
      "Iteration 150, loss = 0.11062900\n",
      "Iteration 30, loss = 0.30426745\n",
      "Iteration 151, loss = 0.10888536\n",
      "Iteration 31, loss = 0.30015622\n",
      "Iteration 152, loss = 0.10836203\n",
      "Iteration 32, loss = 0.29535105\n",
      "Iteration 153, loss = 0.10795202\n",
      "Iteration 33, loss = 0.29193065\n",
      "Iteration 154, loss = 0.10670174\n",
      "Iteration 34, loss = 0.28781515\n",
      "Iteration 155, loss = 0.10715238\n",
      "Iteration 35, loss = 0.28403329\n",
      "Iteration 156, loss = 0.10581904\n",
      "Iteration 36, loss = 0.27946751\n",
      "Iteration 157, loss = 0.10552908\n",
      "Iteration 37, loss = 0.27640370\n",
      "Iteration 158, loss = 0.10462098\n",
      "Iteration 38, loss = 0.27322899\n",
      "Iteration 159, loss = 0.10348018\n",
      "Iteration 39, loss = 0.26845773\n",
      "Iteration 160, loss = 0.10312253\n",
      "Iteration 40, loss = 0.26553795\n",
      "Iteration 161, loss = 0.10116587\n",
      "Iteration 41, loss = 0.26185586\n",
      "Iteration 162, loss = 0.10187940\n",
      "Iteration 42, loss = 0.25882257\n",
      "Iteration 163, loss = 0.10088327\n",
      "Iteration 43, loss = 0.25640150\n",
      "Iteration 164, loss = 0.09972577\n",
      "Iteration 44, loss = 0.25189597\n",
      "Iteration 165, loss = 0.09898756\n",
      "Iteration 45, loss = 0.24946276\n",
      "Iteration 166, loss = 0.09794789\n",
      "Iteration 46, loss = 0.24635537\n",
      "Iteration 167, loss = 0.09750552\n",
      "Iteration 47, loss = 0.24362442\n",
      "Iteration 168, loss = 0.09658193\n",
      "Iteration 48, loss = 0.24136179\n",
      "Iteration 169, loss = 0.09707961\n",
      "Iteration 49, loss = 0.23762886\n",
      "Iteration 170, loss = 0.09511592\n",
      "Iteration 50, loss = 0.23533311\n",
      "Iteration 171, loss = 0.09635855\n",
      "Iteration 51, loss = 0.23189875\n",
      "Iteration 172, loss = 0.09486796\n",
      "Iteration 52, loss = 0.22946763\n",
      "Iteration 173, loss = 0.09357055\n",
      "Iteration 53, loss = 0.22631294\n",
      "Iteration 174, loss = 0.09430778\n",
      "Iteration 54, loss = 0.22407570\n",
      "Iteration 175, loss = 0.09199130\n",
      "Iteration 55, loss = 0.22121368\n",
      "Iteration 176, loss = 0.09249435\n",
      "Iteration 56, loss = 0.22035623\n",
      "Iteration 177, loss = 0.09071605\n",
      "Iteration 57, loss = 0.21643599\n",
      "Iteration 178, loss = 0.08988650\n",
      "Iteration 58, loss = 0.21450845\n",
      "Iteration 179, loss = 0.08993581\n",
      "Iteration 59, loss = 0.21148964\n",
      "Iteration 180, loss = 0.09000373\n",
      "Iteration 60, loss = 0.21012058\n",
      "Iteration 181, loss = 0.09025405\n",
      "Iteration 61, loss = 0.20720878\n",
      "Iteration 182, loss = 0.09147830\n",
      "Iteration 62, loss = 0.20388222\n",
      "Iteration 183, loss = 0.08926402\n",
      "Iteration 63, loss = 0.20299888\n",
      "Iteration 184, loss = 0.08766590\n",
      "Iteration 64, loss = 0.20143759\n",
      "Iteration 185, loss = 0.08732617\n",
      "Iteration 65, loss = 0.19763982\n",
      "Iteration 186, loss = 0.08677994\n",
      "Iteration 66, loss = 0.19664688\n",
      "Iteration 187, loss = 0.08488074\n",
      "Iteration 67, loss = 0.19390090\n",
      "Iteration 188, loss = 0.08479808\n",
      "Iteration 68, loss = 0.19170180\n",
      "Iteration 189, loss = 0.08367424\n",
      "Iteration 69, loss = 0.18911182\n",
      "Iteration 190, loss = 0.08194318\n",
      "Iteration 70, loss = 0.18728687\n",
      "Iteration 191, loss = 0.08319446\n",
      "Iteration 71, loss = 0.18465615\n",
      "Iteration 192, loss = 0.08351352\n",
      "Iteration 72, loss = 0.18371686\n",
      "Iteration 193, loss = 0.08050979\n",
      "Iteration 73, loss = 0.18056165\n",
      "Iteration 194, loss = 0.08096349\n",
      "Iteration 195, loss = 0.08152891\n",
      "Iteration 74, loss = 0.17851499\n",
      "Iteration 75, loss = 0.17762053\n",
      "Iteration 196, loss = 0.07952614\n",
      "Iteration 76, loss = 0.17527190\n",
      "Iteration 197, loss = 0.08022552\n",
      "Iteration 77, loss = 0.17309669\n",
      "Iteration 198, loss = 0.07842724\n",
      "Iteration 78, loss = 0.17092456\n",
      "Iteration 199, loss = 0.07893536\n",
      "Iteration 200, loss = 0.07742961\n",
      "Iteration 79, loss = 0.16996760\n",
      "Iteration 201, loss = 0.07615244\n",
      "Iteration 80, loss = 0.16677497\n",
      "Iteration 202, loss = 0.07940767\n",
      "Iteration 81, loss = 0.16625481\n",
      "Iteration 203, loss = 0.07838372\n",
      "Iteration 82, loss = 0.16435862\n",
      "Iteration 204, loss = 0.07614902\n",
      "Iteration 83, loss = 0.16276856\n",
      "Iteration 205, loss = 0.07702979\n",
      "Iteration 84, loss = 0.16195571\n",
      "Iteration 206, loss = 0.07345368\n",
      "Iteration 85, loss = 0.15965932\n",
      "Iteration 207, loss = 0.07278124\n",
      "Iteration 86, loss = 0.15694372\n",
      "Iteration 208, loss = 0.07293428\n",
      "Iteration 87, loss = 0.15587029\n",
      "Iteration 209, loss = 0.07215978\n",
      "Iteration 88, loss = 0.15310381\n",
      "Iteration 210, loss = 0.07493846\n",
      "Iteration 89, loss = 0.15355216\n",
      "Iteration 211, loss = 0.07536521\n",
      "Iteration 90, loss = 0.15212774\n",
      "Iteration 212, loss = 0.07313777\n",
      "Iteration 91, loss = 0.15094063\n",
      "Iteration 213, loss = 0.07177492\n",
      "Iteration 92, loss = 0.14872679\n",
      "Iteration 214, loss = 0.07222179\n",
      "Iteration 93, loss = 0.14623402\n",
      "Iteration 215, loss = 0.07240363\n",
      "Iteration 94, loss = 0.14589355\n",
      "Iteration 216, loss = 0.06850759\n",
      "Iteration 95, loss = 0.14407211\n",
      "Iteration 217, loss = 0.06890714\n",
      "Iteration 96, loss = 0.14146095\n",
      "Iteration 218, loss = 0.06844653\n",
      "Iteration 97, loss = 0.14138787\n",
      "Iteration 219, loss = 0.06707901\n",
      "Iteration 98, loss = 0.13916531\n",
      "Iteration 220, loss = 0.06587080\n",
      "Iteration 99, loss = 0.13761977\n",
      "Iteration 221, loss = 0.06829846\n",
      "Iteration 100, loss = 0.13630942\n",
      "Iteration 222, loss = 0.06568448\n",
      "Iteration 101, loss = 0.13899556\n",
      "Iteration 223, loss = 0.06664293\n",
      "Iteration 102, loss = 0.13634200\n",
      "Iteration 224, loss = 0.06552079\n",
      "Iteration 103, loss = 0.13322450\n",
      "Iteration 225, loss = 0.06492958\n",
      "Iteration 104, loss = 0.13577916\n",
      "Iteration 226, loss = 0.06484748\n",
      "Iteration 105, loss = 0.13163862\n",
      "Iteration 227, loss = 0.06388650\n",
      "Iteration 106, loss = 0.13128416\n",
      "Iteration 228, loss = 0.06311745\n",
      "Iteration 107, loss = 0.13047815\n",
      "Iteration 229, loss = 0.06281754\n",
      "Iteration 108, loss = 0.12731709\n",
      "Iteration 230, loss = 0.06240590\n",
      "Iteration 109, loss = 0.12639159\n",
      "Iteration 231, loss = 0.06342065\n",
      "Iteration 110, loss = 0.12583122\n",
      "Iteration 232, loss = 0.06143387\n",
      "Iteration 111, loss = 0.12505626\n",
      "Iteration 233, loss = 0.06101595\n",
      "Iteration 112, loss = 0.12435925\n",
      "Iteration 234, loss = 0.06069462\n",
      "Iteration 113, loss = 0.12146532\n",
      "Iteration 235, loss = 0.06042611\n",
      "Iteration 114, loss = 0.12298389\n",
      "Iteration 236, loss = 0.05975999\n",
      "Iteration 115, loss = 0.11989202\n",
      "Iteration 237, loss = 0.05956601\n",
      "Iteration 116, loss = 0.11933228\n",
      "Iteration 238, loss = 0.05946649\n",
      "Iteration 117, loss = 0.12070193\n",
      "Iteration 239, loss = 0.05863932\n",
      "Iteration 118, loss = 0.11650915\n",
      "Iteration 240, loss = 0.06000476\n",
      "Iteration 241, loss = 0.05808964\n",
      "Iteration 119, loss = 0.11865755\n",
      "Iteration 242, loss = 0.05886787\n",
      "Iteration 120, loss = 0.11392004\n",
      "Iteration 243, loss = 0.05619863\n",
      "Iteration 244, loss = 0.05684925\n",
      "Iteration 121, loss = 0.11613887\n",
      "Iteration 245, loss = 0.05604324\n",
      "Iteration 122, loss = 0.11318444\n",
      "Iteration 246, loss = 0.05516136\n",
      "Iteration 123, loss = 0.11239130\n",
      "Iteration 247, loss = 0.05582703\n",
      "Iteration 124, loss = 0.11064806\n",
      "Iteration 248, loss = 0.05484916\n",
      "Iteration 125, loss = 0.10947649\n",
      "Iteration 249, loss = 0.05585077\n",
      "Iteration 126, loss = 0.10846249\n",
      "Iteration 250, loss = 0.05426313\n",
      "Iteration 251, loss = 0.05486807\n",
      "Iteration 252, loss = 0.05352009\n",
      "Iteration 127, loss = 0.10939114\n",
      "Iteration 253, loss = 0.05334589\n",
      "Iteration 128, loss = 0.10711131\n",
      "Iteration 254, loss = 0.05244779\n",
      "Iteration 129, loss = 0.10647912\n",
      "Iteration 255, loss = 0.05125269\n",
      "Iteration 130, loss = 0.10530569\n",
      "Iteration 131, loss = 0.10508345\n",
      "Iteration 256, loss = 0.05096033\n",
      "Iteration 132, loss = 0.10298965\n",
      "Iteration 133, loss = 0.10382276\n",
      "Iteration 257, loss = 0.05245253\n",
      "Iteration 134, loss = 0.10302080\n",
      "Iteration 258, loss = 0.05335890\n",
      "Iteration 135, loss = 0.10042573\n",
      "Iteration 136, loss = 0.10142831\n",
      "Iteration 259, loss = 0.05009253\n",
      "Iteration 137, loss = 0.09878557\n",
      "Iteration 260, loss = 0.05045521\n",
      "Iteration 261, loss = 0.05042713\n",
      "Iteration 262, loss = 0.05140321\n",
      "Iteration 138, loss = 0.09826629\n",
      "Iteration 263, loss = 0.04806351\n",
      "Iteration 139, loss = 0.09801031\n",
      "Iteration 264, loss = 0.04967523\n",
      "Iteration 265, loss = 0.04798365\n",
      "Iteration 140, loss = 0.09657966\n",
      "Iteration 266, loss = 0.04742777\n",
      "Iteration 267, loss = 0.04969083\n",
      "Iteration 141, loss = 0.09662313\n",
      "Iteration 268, loss = 0.04787850\n",
      "Iteration 142, loss = 0.09656449\n",
      "Iteration 143, loss = 0.09897241\n",
      "Iteration 144, loss = 0.09424120\n",
      "Iteration 269, loss = 0.04832251\n",
      "Iteration 270, loss = 0.04641201\n",
      "Iteration 145, loss = 0.09566459\n",
      "Iteration 271, loss = 0.04592479\n",
      "Iteration 146, loss = 0.09293294\n",
      "Iteration 272, loss = 0.04607292\n",
      "Iteration 147, loss = 0.09171674\n",
      "Iteration 273, loss = 0.04618037\n",
      "Iteration 148, loss = 0.09146532\n",
      "Iteration 274, loss = 0.04526544\n",
      "Iteration 149, loss = 0.08988553\n",
      "Iteration 150, loss = 0.09085762\n",
      "Iteration 275, loss = 0.04387540\n",
      "Iteration 276, loss = 0.04432177\n",
      "Iteration 151, loss = 0.09159187\n",
      "Iteration 277, loss = 0.04461521\n",
      "Iteration 152, loss = 0.08836905\n",
      "Iteration 278, loss = 0.04422112\n",
      "Iteration 153, loss = 0.08750663\n",
      "Iteration 279, loss = 0.04328227\n",
      "Iteration 154, loss = 0.08725822\n",
      "Iteration 280, loss = 0.04368059\n",
      "Iteration 155, loss = 0.08555579\n",
      "Iteration 281, loss = 0.04214287\n",
      "Iteration 156, loss = 0.08475229\n",
      "Iteration 282, loss = 0.04311806\n",
      "Iteration 157, loss = 0.08464253\n",
      "Iteration 283, loss = 0.04306338\n",
      "Iteration 158, loss = 0.08322194\n",
      "Iteration 284, loss = 0.04331585\n",
      "Iteration 159, loss = 0.08407546\n",
      "Iteration 285, loss = 0.04438408\n",
      "Iteration 160, loss = 0.08474370\n",
      "Iteration 286, loss = 0.04119972\n",
      "Iteration 161, loss = 0.08184253\n",
      "Iteration 287, loss = 0.04091664\n",
      "Iteration 162, loss = 0.08334581\n",
      "Iteration 288, loss = 0.04182851\n",
      "Iteration 163, loss = 0.08128897\n",
      "Iteration 289, loss = 0.04083904\n",
      "Iteration 164, loss = 0.07945845\n",
      "Iteration 290, loss = 0.03978052\n",
      "Iteration 165, loss = 0.08047037\n",
      "Iteration 291, loss = 0.03968927\n",
      "Iteration 166, loss = 0.07890695\n",
      "Iteration 292, loss = 0.03889404\n",
      "Iteration 167, loss = 0.07949993\n",
      "Iteration 293, loss = 0.04040978\n",
      "Iteration 168, loss = 0.07897140\n",
      "Iteration 294, loss = 0.03853950\n",
      "Iteration 169, loss = 0.07790965\n",
      "Iteration 295, loss = 0.03896676\n",
      "Iteration 170, loss = 0.07774147\n",
      "Iteration 296, loss = 0.03759685\n",
      "Iteration 171, loss = 0.07587362\n",
      "Iteration 297, loss = 0.03815534\n",
      "Iteration 172, loss = 0.07735087\n",
      "Iteration 298, loss = 0.03666699\n",
      "Iteration 173, loss = 0.07412006\n",
      "Iteration 299, loss = 0.03663244\n",
      "Iteration 174, loss = 0.07611657\n",
      "Iteration 300, loss = 0.03688587\n",
      "Iteration 175, loss = 0.07626220\n",
      "Iteration 301, loss = 0.03687612\n",
      "Iteration 176, loss = 0.07224911\n",
      "Iteration 302, loss = 0.03526052\n",
      "Iteration 177, loss = 0.07408214\n",
      "Iteration 303, loss = 0.03607116\n",
      "Iteration 178, loss = 0.07580200\n",
      "Iteration 304, loss = 0.03734754\n",
      "Iteration 179, loss = 0.07249233\n",
      "Iteration 305, loss = 0.03536527\n",
      "Iteration 180, loss = 0.07195050\n",
      "Iteration 306, loss = 0.03488053\n",
      "Iteration 181, loss = 0.07083352\n",
      "Iteration 307, loss = 0.03494829\n",
      "Iteration 182, loss = 0.07189733\n",
      "Iteration 308, loss = 0.03463284\n",
      "Iteration 183, loss = 0.06961340\n",
      "Iteration 309, loss = 0.03518502\n",
      "Iteration 184, loss = 0.06905340\n",
      "Iteration 310, loss = 0.03471883\n",
      "Iteration 185, loss = 0.06873517\n",
      "Iteration 311, loss = 0.03388593\n",
      "Iteration 186, loss = 0.06750838\n",
      "Iteration 312, loss = 0.03387661\n",
      "Iteration 187, loss = 0.06802849\n",
      "Iteration 313, loss = 0.03357763\n",
      "Iteration 188, loss = 0.06942248\n",
      "Iteration 314, loss = 0.03253619\n",
      "Iteration 189, loss = 0.07011109\n",
      "Iteration 315, loss = 0.03249653\n",
      "Iteration 190, loss = 0.06784000\n",
      "Iteration 316, loss = 0.03278592\n",
      "Iteration 191, loss = 0.06801088\n",
      "Iteration 317, loss = 0.03214891\n",
      "Iteration 192, loss = 0.07029151\n",
      "Iteration 318, loss = 0.03197978\n",
      "Iteration 193, loss = 0.06567119\n",
      "Iteration 319, loss = 0.03157985\n",
      "Iteration 194, loss = 0.06459196\n",
      "Iteration 320, loss = 0.03233451\n",
      "Iteration 195, loss = 0.06540501\n",
      "Iteration 321, loss = 0.03135837\n",
      "Iteration 196, loss = 0.06389264\n",
      "Iteration 322, loss = 0.03209113\n",
      "Iteration 197, loss = 0.06264160\n",
      "Iteration 323, loss = 0.03093685\n",
      "Iteration 198, loss = 0.06508949\n",
      "Iteration 324, loss = 0.03099879\n",
      "Iteration 199, loss = 0.06131202\n",
      "Iteration 325, loss = 0.03189754\n",
      "Iteration 200, loss = 0.06212604\n",
      "Iteration 326, loss = 0.02899074\n",
      "Iteration 201, loss = 0.06446729\n",
      "Iteration 327, loss = 0.03071635\n",
      "Iteration 202, loss = 0.05955499\n",
      "Iteration 328, loss = 0.03126437\n",
      "Iteration 203, loss = 0.05925789\n",
      "Iteration 329, loss = 0.02848122\n",
      "Iteration 204, loss = 0.06060927\n",
      "Iteration 330, loss = 0.03034645\n",
      "Iteration 205, loss = 0.05806016\n",
      "Iteration 331, loss = 0.03100610\n",
      "Iteration 206, loss = 0.05768157\n",
      "Iteration 332, loss = 0.02916541\n",
      "Iteration 207, loss = 0.05687006\n",
      "Iteration 333, loss = 0.02859673\n",
      "Iteration 208, loss = 0.05734898\n",
      "Iteration 334, loss = 0.02795116\n",
      "Iteration 209, loss = 0.05569259\n",
      "Iteration 335, loss = 0.02821980\n",
      "Iteration 210, loss = 0.05525393\n",
      "Iteration 336, loss = 0.02858662\n",
      "Iteration 211, loss = 0.05661182\n",
      "Iteration 337, loss = 0.02890716\n",
      "Iteration 212, loss = 0.05356225\n",
      "Iteration 338, loss = 0.02851888\n",
      "Iteration 213, loss = 0.05572494\n",
      "Iteration 339, loss = 0.02828580\n",
      "Iteration 214, loss = 0.05355384\n",
      "Iteration 340, loss = 0.02747767\n",
      "Iteration 215, loss = 0.05347895\n",
      "Iteration 341, loss = 0.02819064\n",
      "Iteration 216, loss = 0.05266908\n",
      "Iteration 342, loss = 0.02682542\n",
      "Iteration 217, loss = 0.05304599\n",
      "Iteration 343, loss = 0.02772197\n",
      "Iteration 218, loss = 0.05316202\n",
      "Iteration 344, loss = 0.02632101\n",
      "Iteration 219, loss = 0.05363452\n",
      "Iteration 345, loss = 0.02606007\n",
      "Iteration 220, loss = 0.05400234\n",
      "Iteration 346, loss = 0.02649602\n",
      "Iteration 221, loss = 0.05227044\n",
      "Iteration 347, loss = 0.02571432\n",
      "Iteration 222, loss = 0.05392621\n",
      "Iteration 348, loss = 0.02472788\n",
      "Iteration 223, loss = 0.05205385\n",
      "Iteration 349, loss = 0.02614942\n",
      "Iteration 224, loss = 0.04980979\n",
      "Iteration 350, loss = 0.02522691\n",
      "Iteration 225, loss = 0.05150727\n",
      "Iteration 351, loss = 0.02590152\n",
      "Iteration 226, loss = 0.04853058\n",
      "Iteration 352, loss = 0.02537212\n",
      "Iteration 227, loss = 0.04895750\n",
      "Iteration 353, loss = 0.02495449\n",
      "Iteration 228, loss = 0.04894116\n",
      "Iteration 354, loss = 0.02446486\n",
      "Iteration 229, loss = 0.04789245\n",
      "Iteration 355, loss = 0.02379755\n",
      "Iteration 230, loss = 0.04695089\n",
      "Iteration 356, loss = 0.02336438\n",
      "Iteration 231, loss = 0.04679967\n",
      "Iteration 357, loss = 0.02387810\n",
      "Iteration 232, loss = 0.04645605\n",
      "Iteration 358, loss = 0.02367878\n",
      "Iteration 233, loss = 0.04655719\n",
      "Iteration 359, loss = 0.02355027\n",
      "Iteration 234, loss = 0.04550989\n",
      "Iteration 360, loss = 0.02262864\n",
      "Iteration 235, loss = 0.04663403\n",
      "Iteration 361, loss = 0.02251859\n",
      "Iteration 236, loss = 0.04604920\n",
      "Iteration 362, loss = 0.02324856\n",
      "Iteration 237, loss = 0.04441959\n",
      "Iteration 363, loss = 0.02681452\n",
      "Iteration 238, loss = 0.04719236\n",
      "Iteration 364, loss = 0.02865845\n",
      "Iteration 239, loss = 0.04427760\n",
      "Iteration 365, loss = 0.02362697\n",
      "Iteration 240, loss = 0.04334485\n",
      "Iteration 366, loss = 0.02575805\n",
      "Iteration 241, loss = 0.04441605\n",
      "Iteration 367, loss = 0.02425641\n",
      "Iteration 242, loss = 0.04605514\n",
      "Iteration 368, loss = 0.02235496\n",
      "Iteration 243, loss = 0.04565474\n",
      "Iteration 369, loss = 0.02208460\n",
      "Iteration 244, loss = 0.04434752\n",
      "Iteration 370, loss = 0.02147041\n",
      "Iteration 371, loss = 0.02183673\n",
      "Iteration 245, loss = 0.04206405\n",
      "Iteration 246, loss = 0.04293170\n",
      "Iteration 372, loss = 0.02068414\n",
      "Iteration 247, loss = 0.04285851\n",
      "Iteration 373, loss = 0.02183031\n",
      "Iteration 248, loss = 0.04264787\n",
      "Iteration 374, loss = 0.02067316\n",
      "Iteration 375, loss = 0.02048394\n",
      "Iteration 249, loss = 0.04097261\n",
      "Iteration 376, loss = 0.02031571\n",
      "Iteration 250, loss = 0.04125138\n",
      "Iteration 251, loss = 0.04035152\n",
      "Iteration 377, loss = 0.02029093\n",
      "Iteration 252, loss = 0.04046630\n",
      "Iteration 378, loss = 0.01998337\n",
      "Iteration 253, loss = 0.03900029\n",
      "Iteration 379, loss = 0.02031836\n",
      "Iteration 254, loss = 0.04022049\n",
      "Iteration 380, loss = 0.02048830\n",
      "Iteration 255, loss = 0.03873690\n",
      "Iteration 381, loss = 0.01941257\n",
      "Iteration 256, loss = 0.03834666\n",
      "Iteration 382, loss = 0.01899785\n",
      "Iteration 257, loss = 0.03893013\n",
      "Iteration 383, loss = 0.01887784\n",
      "Iteration 258, loss = 0.03820822\n",
      "Iteration 384, loss = 0.01896627\n",
      "Iteration 259, loss = 0.03732323\n",
      "Iteration 385, loss = 0.01876075\n",
      "Iteration 260, loss = 0.03882341\n",
      "Iteration 386, loss = 0.01869946\n",
      "Iteration 261, loss = 0.03693102\n",
      "Iteration 387, loss = 0.01928058\n",
      "Iteration 262, loss = 0.03935205\n",
      "Iteration 388, loss = 0.01817106\n",
      "Iteration 263, loss = 0.04011362\n",
      "Iteration 389, loss = 0.01874169\n",
      "Iteration 264, loss = 0.03904354\n",
      "Iteration 390, loss = 0.01891065\n",
      "Iteration 265, loss = 0.03516152\n",
      "Iteration 391, loss = 0.01842458\n",
      "Iteration 266, loss = 0.03768505\n",
      "Iteration 392, loss = 0.01798429\n",
      "Iteration 267, loss = 0.03753369\n",
      "Iteration 393, loss = 0.01821114\n",
      "Iteration 268, loss = 0.03742597\n",
      "Iteration 394, loss = 0.01711683\n",
      "Iteration 269, loss = 0.03638845\n",
      "Iteration 395, loss = 0.01759506\n",
      "Iteration 270, loss = 0.03576479\n",
      "Iteration 396, loss = 0.01709489\n",
      "Iteration 271, loss = 0.03698944\n",
      "Iteration 397, loss = 0.01718217\n",
      "Iteration 272, loss = 0.03548064\n",
      "Iteration 398, loss = 0.01714179\n",
      "Iteration 273, loss = 0.03493566\n",
      "Iteration 399, loss = 0.01694151\n",
      "Iteration 274, loss = 0.03340641\n",
      "Iteration 400, loss = 0.01671512\n",
      "Iteration 275, loss = 0.03385470\n",
      "Iteration 401, loss = 0.01639937\n",
      "Iteration 276, loss = 0.03291795\n",
      "Iteration 402, loss = 0.01652037\n",
      "Iteration 277, loss = 0.03304294\n",
      "Iteration 403, loss = 0.01597732\n",
      "Iteration 278, loss = 0.03280787\n",
      "Iteration 404, loss = 0.01585608\n",
      "Iteration 279, loss = 0.03268500\n",
      "Iteration 405, loss = 0.01584778\n",
      "Iteration 280, loss = 0.03244189\n",
      "Iteration 406, loss = 0.01600648\n",
      "Iteration 281, loss = 0.03241934\n",
      "Iteration 407, loss = 0.01535032\n",
      "Iteration 282, loss = 0.03168582\n",
      "Iteration 408, loss = 0.01579136\n",
      "Iteration 283, loss = 0.03154334\n",
      "Iteration 409, loss = 0.01523547\n",
      "Iteration 284, loss = 0.03187235\n",
      "Iteration 410, loss = 0.01528164\n",
      "Iteration 285, loss = 0.03157625\n",
      "Iteration 411, loss = 0.01529546\n",
      "Iteration 286, loss = 0.03089700\n",
      "Iteration 412, loss = 0.01506486\n",
      "Iteration 287, loss = 0.03081950\n",
      "Iteration 413, loss = 0.01487699\n",
      "Iteration 288, loss = 0.03029568\n",
      "Iteration 414, loss = 0.01475326\n",
      "Iteration 289, loss = 0.02993423\n",
      "Iteration 415, loss = 0.01513583\n",
      "Iteration 290, loss = 0.02996602\n",
      "Iteration 416, loss = 0.01454777\n",
      "Iteration 291, loss = 0.03040622\n",
      "Iteration 417, loss = 0.01444442\n",
      "Iteration 292, loss = 0.02960334\n",
      "Iteration 418, loss = 0.01438354\n",
      "Iteration 293, loss = 0.02925412\n",
      "Iteration 419, loss = 0.01444817\n",
      "Iteration 294, loss = 0.02880746\n",
      "Iteration 420, loss = 0.01465809\n",
      "Iteration 295, loss = 0.02950439\n",
      "Iteration 421, loss = 0.01416362\n",
      "Iteration 296, loss = 0.03202257\n",
      "Iteration 422, loss = 0.01397553\n",
      "Iteration 297, loss = 0.03170114\n",
      "Iteration 423, loss = 0.01390690\n",
      "Iteration 298, loss = 0.02698291\n",
      "Iteration 424, loss = 0.01379588\n",
      "Iteration 299, loss = 0.02978482\n",
      "Iteration 425, loss = 0.01392660\n",
      "Iteration 300, loss = 0.03162014\n",
      "Iteration 426, loss = 0.01402313\n",
      "Iteration 301, loss = 0.03099349\n",
      "Iteration 427, loss = 0.01399192\n",
      "Iteration 302, loss = 0.02681396\n",
      "Iteration 428, loss = 0.01463834\n",
      "Iteration 303, loss = 0.02937241\n",
      "Iteration 429, loss = 0.01487180\n",
      "Iteration 304, loss = 0.02964703\n",
      "Iteration 430, loss = 0.01340921\n",
      "Iteration 305, loss = 0.02753813\n",
      "Iteration 431, loss = 0.01354569\n",
      "Iteration 306, loss = 0.02649546\n",
      "Iteration 432, loss = 0.01397843\n",
      "Iteration 307, loss = 0.02762813\n",
      "Iteration 433, loss = 0.01339364\n",
      "Iteration 308, loss = 0.02858100\n",
      "Iteration 434, loss = 0.01277191\n",
      "Iteration 309, loss = 0.02636561\n",
      "Iteration 435, loss = 0.01280557\n",
      "Iteration 310, loss = 0.02709941\n",
      "Iteration 436, loss = 0.01286049\n",
      "Iteration 311, loss = 0.02634570\n",
      "Iteration 437, loss = 0.01238473\n",
      "Iteration 312, loss = 0.02623095\n",
      "Iteration 438, loss = 0.01234832\n",
      "Iteration 313, loss = 0.02524797\n",
      "Iteration 439, loss = 0.01204772\n",
      "Iteration 314, loss = 0.02598159\n",
      "Iteration 440, loss = 0.01222439\n",
      "Iteration 315, loss = 0.02559744\n",
      "Iteration 441, loss = 0.01203766\n",
      "Iteration 316, loss = 0.02676193\n",
      "Iteration 442, loss = 0.01187347\n",
      "Iteration 317, loss = 0.02568511\n",
      "Iteration 443, loss = 0.01170138\n",
      "Iteration 318, loss = 0.02465829\n",
      "Iteration 444, loss = 0.01174955\n",
      "Iteration 319, loss = 0.02794389\n",
      "Iteration 445, loss = 0.01200110\n",
      "Iteration 320, loss = 0.02583912\n",
      "Iteration 446, loss = 0.01180374\n",
      "Iteration 321, loss = 0.02654017\n",
      "Iteration 447, loss = 0.01216938\n",
      "Iteration 322, loss = 0.02564886\n",
      "Iteration 448, loss = 0.01178140\n",
      "Iteration 323, loss = 0.02510795\n",
      "Iteration 449, loss = 0.01166882\n",
      "Iteration 324, loss = 0.02476477\n",
      "Iteration 450, loss = 0.01144403\n",
      "Iteration 325, loss = 0.02477435\n",
      "Iteration 451, loss = 0.01128059\n",
      "Iteration 326, loss = 0.02312659\n",
      "Iteration 452, loss = 0.01146388\n",
      "Iteration 327, loss = 0.02430108\n",
      "Iteration 328, loss = 0.02328079\n",
      "Iteration 453, loss = 0.01134288\n",
      "Iteration 329, loss = 0.02292135\n",
      "Iteration 454, loss = 0.01106868\n",
      "Iteration 330, loss = 0.02332009\n",
      "Iteration 455, loss = 0.01074341\n",
      "Iteration 331, loss = 0.02299276\n",
      "Iteration 456, loss = 0.01116609\n",
      "Iteration 332, loss = 0.02245413\n",
      "Iteration 457, loss = 0.01065842\n",
      "Iteration 333, loss = 0.02220811Iteration 458, loss = 0.01057882\n",
      "\n",
      "Iteration 459, loss = 0.01058834\n",
      "Iteration 334, loss = 0.02226498\n",
      "Iteration 460, loss = 0.01068182\n",
      "Iteration 335, loss = 0.02217232\n",
      "Iteration 461, loss = 0.01028495\n",
      "Iteration 336, loss = 0.02179964\n",
      "Iteration 462, loss = 0.01072313\n",
      "Iteration 337, loss = 0.02206784\n",
      "Iteration 463, loss = 0.01022937\n",
      "Iteration 338, loss = 0.02144092\n",
      "Iteration 464, loss = 0.01023222\n",
      "Iteration 339, loss = 0.02145794\n",
      "Iteration 465, loss = 0.00997614\n",
      "Iteration 340, loss = 0.02284495\n",
      "Iteration 466, loss = 0.01001886\n",
      "Iteration 341, loss = 0.02535615\n",
      "Iteration 342, loss = 0.02760491\n",
      "Iteration 467, loss = 0.00986019\n",
      "Iteration 343, loss = 0.02082379\n",
      "Iteration 468, loss = 0.01019424\n",
      "Iteration 469, loss = 0.01071650\n",
      "Iteration 344, loss = 0.02028128\n",
      "Iteration 470, loss = 0.01029506Iteration 345, loss = 0.02463056\n",
      "\n",
      "Iteration 471, loss = 0.01018631\n",
      "Iteration 346, loss = 0.02326744\n",
      "Iteration 472, loss = 0.00983791\n",
      "Iteration 347, loss = 0.02294932\n",
      "Iteration 473, loss = 0.01015584\n",
      "Iteration 348, loss = 0.02025414\n",
      "Iteration 474, loss = 0.00982671\n",
      "Iteration 349, loss = 0.02033504\n",
      "Iteration 475, loss = 0.00962723\n",
      "Iteration 350, loss = 0.01977611\n",
      "Iteration 476, loss = 0.00938765\n",
      "Iteration 351, loss = 0.01989773\n",
      "Iteration 477, loss = 0.00970514\n",
      "Iteration 352, loss = 0.01940421\n",
      "Iteration 478, loss = 0.00919322\n",
      "Iteration 353, loss = 0.01920030\n",
      "Iteration 479, loss = 0.00909250\n",
      "Iteration 354, loss = 0.02034352\n",
      "Iteration 480, loss = 0.00916665\n",
      "Iteration 355, loss = 0.01951095\n",
      "Iteration 481, loss = 0.00919440\n",
      "Iteration 356, loss = 0.01876844\n",
      "Iteration 482, loss = 0.00882551\n",
      "Iteration 357, loss = 0.01889099\n",
      "Iteration 483, loss = 0.00880951\n",
      "Iteration 358, loss = 0.01856744\n",
      "Iteration 359, loss = 0.01840310\n",
      "Iteration 484, loss = 0.00882215\n",
      "Iteration 360, loss = 0.01838685\n",
      "Iteration 485, loss = 0.00862190\n",
      "Iteration 361, loss = 0.01825774\n",
      "Iteration 486, loss = 0.00880530\n",
      "Iteration 362, loss = 0.01895988\n",
      "Iteration 487, loss = 0.00848669\n",
      "Iteration 363, loss = 0.01809408\n",
      "Iteration 488, loss = 0.00848381\n",
      "Iteration 364, loss = 0.01759129\n",
      "Iteration 489, loss = 0.00849840\n",
      "Iteration 365, loss = 0.01872998\n",
      "Iteration 490, loss = 0.00831625\n",
      "Iteration 366, loss = 0.01746214\n",
      "Iteration 491, loss = 0.00831390\n",
      "Iteration 367, loss = 0.01745185\n",
      "Iteration 492, loss = 0.00842685\n",
      "Iteration 368, loss = 0.01731606\n",
      "Iteration 493, loss = 0.00812870\n",
      "Iteration 369, loss = 0.01705123\n",
      "Iteration 494, loss = 0.00838712\n",
      "Iteration 370, loss = 0.01731847\n",
      "Iteration 495, loss = 0.00823052\n",
      "Iteration 371, loss = 0.01726208\n",
      "Iteration 496, loss = 0.00805192\n",
      "Iteration 372, loss = 0.01708124\n",
      "Iteration 497, loss = 0.00802759\n",
      "Iteration 373, loss = 0.01686218\n",
      "Iteration 498, loss = 0.00816743\n",
      "Iteration 374, loss = 0.01651844\n",
      "Iteration 499, loss = 0.00776866\n",
      "Iteration 375, loss = 0.01647760\n",
      "Iteration 500, loss = 0.00789952\n",
      "Iteration 376, loss = 0.01671765\n",
      "Iteration 501, loss = 0.00770379\n",
      "Iteration 377, loss = 0.01660942\n",
      "Iteration 502, loss = 0.00779340\n",
      "Iteration 378, loss = 0.01870921\n",
      "Iteration 503, loss = 0.00760587\n",
      "Iteration 379, loss = 0.01630901\n",
      "Iteration 504, loss = 0.00767323\n",
      "Iteration 380, loss = 0.01735119\n",
      "Iteration 505, loss = 0.00760407\n",
      "Iteration 381, loss = 0.01692463\n",
      "Iteration 506, loss = 0.00778172\n",
      "Iteration 382, loss = 0.01663177\n",
      "Iteration 507, loss = 0.00780827\n",
      "Iteration 383, loss = 0.01668220\n",
      "Iteration 508, loss = 0.00748714\n",
      "Iteration 384, loss = 0.01663471\n",
      "Iteration 509, loss = 0.00739155\n",
      "Iteration 385, loss = 0.01637562\n",
      "Iteration 510, loss = 0.00725942\n",
      "Iteration 386, loss = 0.01731677\n",
      "Iteration 511, loss = 0.00755738\n",
      "Iteration 387, loss = 0.01534106\n",
      "Iteration 512, loss = 0.00768899\n",
      "Iteration 388, loss = 0.01575038\n",
      "Iteration 513, loss = 0.00735773\n",
      "Iteration 389, loss = 0.01562296\n",
      "Iteration 514, loss = 0.00712666\n",
      "Iteration 390, loss = 0.01655722\n",
      "Iteration 515, loss = 0.00760007\n",
      "Iteration 391, loss = 0.01624969\n",
      "Iteration 516, loss = 0.00709009\n",
      "Iteration 392, loss = 0.01580870\n",
      "Iteration 517, loss = 0.00708582\n",
      "Iteration 393, loss = 0.01470469\n",
      "Iteration 518, loss = 0.00699103\n",
      "Iteration 394, loss = 0.01605677\n",
      "Iteration 519, loss = 0.00681165\n",
      "Iteration 395, loss = 0.01494742\n",
      "Iteration 520, loss = 0.00682702\n",
      "Iteration 396, loss = 0.01458176\n",
      "Iteration 521, loss = 0.00670874\n",
      "Iteration 397, loss = 0.01487274\n",
      "Iteration 522, loss = 0.00675132\n",
      "Iteration 398, loss = 0.01446807\n",
      "Iteration 523, loss = 0.00660318\n",
      "Iteration 399, loss = 0.01466388\n",
      "Iteration 524, loss = 0.00659120\n",
      "Iteration 400, loss = 0.01541525\n",
      "Iteration 525, loss = 0.00675726\n",
      "Iteration 401, loss = 0.01495109\n",
      "Iteration 526, loss = 0.00678992\n",
      "Iteration 402, loss = 0.01513346\n",
      "Iteration 527, loss = 0.00665777\n",
      "Iteration 403, loss = 0.01357400\n",
      "Iteration 528, loss = 0.00668074\n",
      "Iteration 404, loss = 0.01526154\n",
      "Iteration 529, loss = 0.00638104\n",
      "Iteration 405, loss = 0.01487685\n",
      "Iteration 530, loss = 0.00655164\n",
      "Iteration 406, loss = 0.01487468\n",
      "Iteration 531, loss = 0.00652563\n",
      "Iteration 407, loss = 0.01361414\n",
      "Iteration 532, loss = 0.00626716\n",
      "Iteration 533, loss = 0.00627348\n",
      "Iteration 408, loss = 0.01492768\n",
      "Iteration 534, loss = 0.00640942\n",
      "Iteration 409, loss = 0.01435076\n",
      "Iteration 535, loss = 0.00618746\n",
      "Iteration 410, loss = 0.01338279\n",
      "Iteration 536, loss = 0.00610256\n",
      "Iteration 411, loss = 0.01376622\n",
      "Iteration 537, loss = 0.00613073\n",
      "Iteration 412, loss = 0.01415438\n",
      "Iteration 413, loss = 0.01525811\n",
      "Iteration 538, loss = 0.00600378\n",
      "Iteration 414, loss = 0.01414755\n",
      "Iteration 539, loss = 0.00593635\n",
      "Iteration 415, loss = 0.01494986\n",
      "Iteration 540, loss = 0.00598990\n",
      "Iteration 416, loss = 0.01290912\n",
      "Iteration 541, loss = 0.00620590\n",
      "Iteration 417, loss = 0.01368208\n",
      "Iteration 542, loss = 0.00582174\n",
      "Iteration 418, loss = 0.01351562\n",
      "Iteration 543, loss = 0.00610126\n",
      "Iteration 419, loss = 0.01301578\n",
      "Iteration 544, loss = 0.00586137\n",
      "Iteration 420, loss = 0.01286490\n",
      "Iteration 545, loss = 0.00579986\n",
      "Iteration 421, loss = 0.01297237\n",
      "Iteration 546, loss = 0.00581287\n",
      "Iteration 422, loss = 0.01503416\n",
      "Iteration 547, loss = 0.00573871\n",
      "Iteration 423, loss = 0.01248419\n",
      "Iteration 548, loss = 0.00562341\n",
      "Iteration 424, loss = 0.01275011\n",
      "Iteration 549, loss = 0.00607256\n",
      "Iteration 425, loss = 0.01294937\n",
      "Iteration 550, loss = 0.00571878\n",
      "Iteration 426, loss = 0.01235587\n",
      "Iteration 551, loss = 0.00563551\n",
      "Iteration 552, loss = 0.00577928\n",
      "Iteration 427, loss = 0.01301086\n",
      "Iteration 553, loss = 0.00593239\n",
      "Iteration 428, loss = 0.01409797\n",
      "Iteration 554, loss = 0.00575268\n",
      "Iteration 429, loss = 0.01277321\n",
      "Iteration 555, loss = 0.00548194\n",
      "Iteration 430, loss = 0.01204970\n",
      "Iteration 556, loss = 0.00547882\n",
      "Iteration 431, loss = 0.01265046\n",
      "Iteration 557, loss = 0.00596149\n",
      "Iteration 432, loss = 0.01165468\n",
      "Iteration 558, loss = 0.00520970\n",
      "Iteration 433, loss = 0.01169696\n",
      "Iteration 559, loss = 0.00526770\n",
      "Iteration 434, loss = 0.01140383\n",
      "Iteration 560, loss = 0.00519700Iteration 435, loss = 0.01146162\n",
      "\n",
      "Iteration 436, loss = 0.01132887\n",
      "Iteration 561, loss = 0.00516196\n",
      "Iteration 562, loss = 0.00527238\n",
      "Iteration 437, loss = 0.01122625\n",
      "Iteration 563, loss = 0.00523991\n",
      "Iteration 438, loss = 0.01115100\n",
      "Iteration 564, loss = 0.00525437\n",
      "Iteration 439, loss = 0.01119853\n",
      "Iteration 565, loss = 0.00522355\n",
      "Iteration 440, loss = 0.01142098\n",
      "Iteration 566, loss = 0.00521694\n",
      "Iteration 441, loss = 0.01093769\n",
      "Iteration 567, loss = 0.00512857\n",
      "Iteration 442, loss = 0.01111245\n",
      "Iteration 568, loss = 0.00496399\n",
      "Iteration 443, loss = 0.01085231\n",
      "Iteration 569, loss = 0.00518112\n",
      "Iteration 444, loss = 0.01169732\n",
      "Iteration 570, loss = 0.00488914Iteration 445, loss = 0.01071155\n",
      "\n",
      "Iteration 446, loss = 0.01067417\n",
      "Iteration 571, loss = 0.00492689\n",
      "Iteration 572, loss = 0.00479840Iteration 447, loss = 0.01237821\n",
      "\n",
      "Iteration 448, loss = 0.01109907\n",
      "Iteration 573, loss = 0.00487350\n",
      "Iteration 449, loss = 0.01010159\n",
      "Iteration 574, loss = 0.00469141\n",
      "Iteration 450, loss = 0.01155722\n",
      "Iteration 575, loss = 0.00480941\n",
      "Iteration 451, loss = 0.01068555\n",
      "Iteration 576, loss = 0.00478813\n",
      "Iteration 452, loss = 0.01019433\n",
      "Iteration 577, loss = 0.00467219\n",
      "Iteration 453, loss = 0.01035079\n",
      "Iteration 578, loss = 0.00464456\n",
      "Iteration 454, loss = 0.01042616\n",
      "Iteration 579, loss = 0.00466941\n",
      "Iteration 455, loss = 0.01017915\n",
      "Iteration 580, loss = 0.00460870\n",
      "Iteration 581, loss = 0.00456581\n",
      "Iteration 456, loss = 0.01039525\n",
      "Iteration 582, loss = 0.00456957\n",
      "Iteration 457, loss = 0.01113735\n",
      "Iteration 583, loss = 0.00448506\n",
      "Iteration 458, loss = 0.01227174\n",
      "Iteration 584, loss = 0.00464336\n",
      "Iteration 459, loss = 0.01370742\n",
      "Iteration 585, loss = 0.00465823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 460, loss = 0.01094332\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35134470\n",
      "Iteration 2, loss = 1.16167575\n",
      "Iteration 3, loss = 1.07074276\n",
      "Iteration 1, loss = 1.24281468\n",
      "Iteration 4, loss = 0.98493356\n",
      "Iteration 2, loss = 1.05351870\n",
      "Iteration 5, loss = 0.91235550\n",
      "Iteration 3, loss = 0.91705968\n",
      "Iteration 6, loss = 0.85098306\n",
      "Iteration 4, loss = 0.81439954\n",
      "Iteration 7, loss = 0.78955366\n",
      "Iteration 5, loss = 0.73577975\n",
      "Iteration 8, loss = 0.72855203\n",
      "Iteration 6, loss = 0.67037496\n",
      "Iteration 9, loss = 0.67660452\n",
      "Iteration 7, loss = 0.62268334\n",
      "Iteration 10, loss = 0.63001852\n",
      "Iteration 8, loss = 0.58273490\n",
      "Iteration 11, loss = 0.59115263\n",
      "Iteration 9, loss = 0.55085354\n",
      "Iteration 12, loss = 0.55897422\n",
      "Iteration 10, loss = 0.52464223\n",
      "Iteration 13, loss = 0.53085347\n",
      "Iteration 11, loss = 0.50151444\n",
      "Iteration 14, loss = 0.50643263\n",
      "Iteration 12, loss = 0.48205315\n",
      "Iteration 15, loss = 0.48621865\n",
      "Iteration 13, loss = 0.46551392\n",
      "Iteration 16, loss = 0.46838261\n",
      "Iteration 14, loss = 0.44949366\n",
      "Iteration 17, loss = 0.45240123\n",
      "Iteration 15, loss = 0.43608316\n",
      "Iteration 18, loss = 0.43804193\n",
      "Iteration 16, loss = 0.42362825\n",
      "Iteration 19, loss = 0.42643684\n",
      "Iteration 17, loss = 0.41282128\n",
      "Iteration 20, loss = 0.41419633\n",
      "Iteration 18, loss = 0.40296670\n",
      "Iteration 21, loss = 0.40418939\n",
      "Iteration 19, loss = 0.39343453\n",
      "Iteration 22, loss = 0.39481503\n",
      "Iteration 20, loss = 0.38516953\n",
      "Iteration 23, loss = 0.38676814\n",
      "Iteration 21, loss = 0.37686759\n",
      "Iteration 24, loss = 0.37825744\n",
      "Iteration 22, loss = 0.37010031\n",
      "Iteration 25, loss = 0.37125155\n",
      "Iteration 23, loss = 0.36356897\n",
      "Iteration 26, loss = 0.36427299\n",
      "Iteration 24, loss = 0.35661128\n",
      "Iteration 27, loss = 0.35786640\n",
      "Iteration 25, loss = 0.35069319\n",
      "Iteration 28, loss = 0.35163152\n",
      "Iteration 26, loss = 0.34558146\n",
      "Iteration 29, loss = 0.34568914\n",
      "Iteration 27, loss = 0.33952920\n",
      "Iteration 30, loss = 0.34034246\n",
      "Iteration 28, loss = 0.33422002\n",
      "Iteration 31, loss = 0.33549873\n",
      "Iteration 29, loss = 0.32948555\n",
      "Iteration 32, loss = 0.33053911\n",
      "Iteration 30, loss = 0.32454234\n",
      "Iteration 33, loss = 0.32541027\n",
      "Iteration 31, loss = 0.32133446\n",
      "Iteration 34, loss = 0.32079881\n",
      "Iteration 32, loss = 0.31627273\n",
      "Iteration 35, loss = 0.31640914\n",
      "Iteration 33, loss = 0.31334269\n",
      "Iteration 36, loss = 0.31183708\n",
      "Iteration 34, loss = 0.30768062\n",
      "Iteration 37, loss = 0.30770465\n",
      "Iteration 35, loss = 0.30554779\n",
      "Iteration 38, loss = 0.30407228\n",
      "Iteration 36, loss = 0.30028501\n",
      "Iteration 39, loss = 0.29958602\n",
      "Iteration 37, loss = 0.29760911\n",
      "Iteration 40, loss = 0.29602720\n",
      "Iteration 38, loss = 0.29352378\n",
      "Iteration 41, loss = 0.29210810\n",
      "Iteration 39, loss = 0.29010706\n",
      "Iteration 42, loss = 0.28987519\n",
      "Iteration 40, loss = 0.28687571\n",
      "Iteration 43, loss = 0.28548860\n",
      "Iteration 41, loss = 0.28384723\n",
      "Iteration 44, loss = 0.28261225\n",
      "Iteration 42, loss = 0.28048671\n",
      "Iteration 45, loss = 0.27840146\n",
      "Iteration 43, loss = 0.27959181\n",
      "Iteration 46, loss = 0.27613119\n",
      "Iteration 44, loss = 0.27596918\n",
      "Iteration 47, loss = 0.27311336\n",
      "Iteration 45, loss = 0.27155748\n",
      "Iteration 48, loss = 0.26924080\n",
      "Iteration 46, loss = 0.26843639\n",
      "Iteration 49, loss = 0.26667063\n",
      "Iteration 47, loss = 0.26687100\n",
      "Iteration 50, loss = 0.26411283\n",
      "Iteration 48, loss = 0.26291601\n",
      "Iteration 51, loss = 0.26206207\n",
      "Iteration 49, loss = 0.26055174\n",
      "Iteration 52, loss = 0.25774082\n",
      "Iteration 50, loss = 0.25951342\n",
      "Iteration 53, loss = 0.25554810\n",
      "Iteration 51, loss = 0.25523426\n",
      "Iteration 54, loss = 0.25266785\n",
      "Iteration 52, loss = 0.25338417\n",
      "Iteration 55, loss = 0.25002564\n",
      "Iteration 53, loss = 0.25026560\n",
      "Iteration 56, loss = 0.24797571\n",
      "Iteration 54, loss = 0.24746663\n",
      "Iteration 57, loss = 0.24512479\n",
      "Iteration 55, loss = 0.24549892\n",
      "Iteration 58, loss = 0.24202405\n",
      "Iteration 56, loss = 0.24267355\n",
      "Iteration 59, loss = 0.24122183\n",
      "Iteration 57, loss = 0.24065702\n",
      "Iteration 60, loss = 0.23745146\n",
      "Iteration 58, loss = 0.23847964\n",
      "Iteration 61, loss = 0.23500429\n",
      "Iteration 59, loss = 0.23695905\n",
      "Iteration 62, loss = 0.23260462\n",
      "Iteration 60, loss = 0.23441412\n",
      "Iteration 63, loss = 0.23118957\n",
      "Iteration 61, loss = 0.23205496\n",
      "Iteration 64, loss = 0.22824990\n",
      "Iteration 62, loss = 0.23018126\n",
      "Iteration 65, loss = 0.22610005\n",
      "Iteration 63, loss = 0.22735121\n",
      "Iteration 66, loss = 0.22373427\n",
      "Iteration 64, loss = 0.22536839\n",
      "Iteration 67, loss = 0.22173742\n",
      "Iteration 68, loss = 0.21945083\n",
      "Iteration 65, loss = 0.22370726\n",
      "Iteration 66, loss = 0.22266741\n",
      "Iteration 69, loss = 0.21687825\n",
      "Iteration 67, loss = 0.22042036\n",
      "Iteration 70, loss = 0.21522160\n",
      "Iteration 68, loss = 0.21740702\n",
      "Iteration 71, loss = 0.21316384\n",
      "Iteration 72, loss = 0.21123302\n",
      "Iteration 69, loss = 0.21506860\n",
      "Iteration 73, loss = 0.20920158\n",
      "Iteration 70, loss = 0.21490413\n",
      "Iteration 74, loss = 0.20896335\n",
      "Iteration 71, loss = 0.21249108\n",
      "Iteration 75, loss = 0.20514646\n",
      "Iteration 72, loss = 0.21056821\n",
      "Iteration 76, loss = 0.20371595\n",
      "Iteration 73, loss = 0.20857972\n",
      "Iteration 77, loss = 0.20172861\n",
      "Iteration 74, loss = 0.20683110\n",
      "Iteration 78, loss = 0.19963583\n",
      "Iteration 75, loss = 0.20427896\n",
      "Iteration 79, loss = 0.19812449\n",
      "Iteration 76, loss = 0.20310376\n",
      "Iteration 80, loss = 0.19563022\n",
      "Iteration 77, loss = 0.20121051\n",
      "Iteration 78, loss = 0.19927312\n",
      "Iteration 81, loss = 0.19534298\n",
      "Iteration 79, loss = 0.19768025\n",
      "Iteration 82, loss = 0.19193363\n",
      "Iteration 80, loss = 0.19598279\n",
      "Iteration 83, loss = 0.19024402\n",
      "Iteration 81, loss = 0.19566666\n",
      "Iteration 84, loss = 0.18852783\n",
      "Iteration 82, loss = 0.19445876\n",
      "Iteration 85, loss = 0.18614290\n",
      "Iteration 83, loss = 0.19270227\n",
      "Iteration 86, loss = 0.18485829\n",
      "Iteration 84, loss = 0.19212929\n",
      "Iteration 87, loss = 0.18123073\n",
      "Iteration 85, loss = 0.18880711\n",
      "Iteration 88, loss = 0.18152458\n",
      "Iteration 86, loss = 0.18740521\n",
      "Iteration 89, loss = 0.17883895\n",
      "Iteration 87, loss = 0.18651037\n",
      "Iteration 90, loss = 0.17762588\n",
      "Iteration 88, loss = 0.18334703\n",
      "Iteration 91, loss = 0.17538080\n",
      "Iteration 89, loss = 0.18287134\n",
      "Iteration 92, loss = 0.17311675\n",
      "Iteration 90, loss = 0.18046103\n",
      "Iteration 93, loss = 0.17270374\n",
      "Iteration 91, loss = 0.18005914\n",
      "Iteration 94, loss = 0.17196970\n",
      "Iteration 95, loss = 0.16961091\n",
      "Iteration 92, loss = 0.17689840\n",
      "Iteration 96, loss = 0.16767785\n",
      "Iteration 93, loss = 0.17742438\n",
      "Iteration 97, loss = 0.16727665\n",
      "Iteration 94, loss = 0.17930568\n",
      "Iteration 98, loss = 0.16460432\n",
      "Iteration 95, loss = 0.17280384\n",
      "Iteration 99, loss = 0.16264856\n",
      "Iteration 96, loss = 0.17533423\n",
      "Iteration 100, loss = 0.16071180\n",
      "Iteration 97, loss = 0.17239120\n",
      "Iteration 101, loss = 0.16121878\n",
      "Iteration 98, loss = 0.16936224\n",
      "Iteration 102, loss = 0.15828467\n",
      "Iteration 99, loss = 0.16861442\n",
      "Iteration 103, loss = 0.15615902\n",
      "Iteration 100, loss = 0.16707267\n",
      "Iteration 104, loss = 0.15618208\n",
      "Iteration 101, loss = 0.16785846\n",
      "Iteration 105, loss = 0.15439688\n",
      "Iteration 102, loss = 0.16444560\n",
      "Iteration 103, loss = 0.16288109\n",
      "Iteration 106, loss = 0.15154431\n",
      "Iteration 107, loss = 0.15054472\n",
      "Iteration 104, loss = 0.16251286\n",
      "Iteration 108, loss = 0.14898292\n",
      "Iteration 105, loss = 0.16274712\n",
      "Iteration 106, loss = 0.16010440Iteration 109, loss = 0.14752535\n",
      "\n",
      "Iteration 110, loss = 0.14582957\n",
      "Iteration 107, loss = 0.15819210\n",
      "Iteration 111, loss = 0.14493079\n",
      "Iteration 108, loss = 0.15659633\n",
      "Iteration 112, loss = 0.14327840\n",
      "Iteration 109, loss = 0.15513232\n",
      "Iteration 113, loss = 0.14178786\n",
      "Iteration 110, loss = 0.15693457\n",
      "Iteration 114, loss = 0.14113952\n",
      "Iteration 111, loss = 0.15447025\n",
      "Iteration 115, loss = 0.14043683\n",
      "Iteration 112, loss = 0.15303948\n",
      "Iteration 116, loss = 0.13922888\n",
      "Iteration 113, loss = 0.14980163\n",
      "Iteration 117, loss = 0.13790555\n",
      "Iteration 114, loss = 0.14727893\n",
      "Iteration 118, loss = 0.13738865\n",
      "Iteration 115, loss = 0.14722987\n",
      "Iteration 119, loss = 0.13480249\n",
      "Iteration 116, loss = 0.14580386\n",
      "Iteration 120, loss = 0.13659793\n",
      "Iteration 117, loss = 0.14554749\n",
      "Iteration 121, loss = 0.13235133\n",
      "Iteration 118, loss = 0.14269931\n",
      "Iteration 119, loss = 0.14394293\n",
      "Iteration 122, loss = 0.13379856\n",
      "Iteration 120, loss = 0.14024141\n",
      "Iteration 123, loss = 0.13015071\n",
      "Iteration 121, loss = 0.14110416\n",
      "Iteration 124, loss = 0.12930382\n",
      "Iteration 122, loss = 0.13893723\n",
      "Iteration 125, loss = 0.12769365\n",
      "Iteration 123, loss = 0.13652669\n",
      "Iteration 126, loss = 0.12716431\n",
      "Iteration 124, loss = 0.13542997\n",
      "Iteration 127, loss = 0.12585299\n",
      "Iteration 128, loss = 0.12564920\n",
      "Iteration 125, loss = 0.13483997\n",
      "Iteration 129, loss = 0.12377972\n",
      "Iteration 126, loss = 0.13326758\n",
      "Iteration 130, loss = 0.12431312\n",
      "Iteration 127, loss = 0.13252428\n",
      "Iteration 131, loss = 0.12268055\n",
      "Iteration 128, loss = 0.13104348\n",
      "Iteration 132, loss = 0.12218274\n",
      "Iteration 129, loss = 0.13075847\n",
      "Iteration 133, loss = 0.12048603\n",
      "Iteration 130, loss = 0.12986404\n",
      "Iteration 134, loss = 0.11873734\n",
      "Iteration 131, loss = 0.12776346\n",
      "Iteration 135, loss = 0.11949711\n",
      "Iteration 132, loss = 0.12929581\n",
      "Iteration 136, loss = 0.11765681\n",
      "Iteration 133, loss = 0.12674385\n",
      "Iteration 137, loss = 0.11608268\n",
      "Iteration 134, loss = 0.12583460\n",
      "Iteration 138, loss = 0.11631346\n",
      "Iteration 135, loss = 0.12572603\n",
      "Iteration 139, loss = 0.11869454\n",
      "Iteration 136, loss = 0.12505460\n",
      "Iteration 140, loss = 0.11728337\n",
      "Iteration 137, loss = 0.12324363\n",
      "Iteration 141, loss = 0.11305389\n",
      "Iteration 138, loss = 0.12156598\n",
      "Iteration 142, loss = 0.11361349\n",
      "Iteration 139, loss = 0.12360532\n",
      "Iteration 143, loss = 0.11153908\n",
      "Iteration 140, loss = 0.12329155\n",
      "Iteration 144, loss = 0.11090521\n",
      "Iteration 141, loss = 0.11938708\n",
      "Iteration 145, loss = 0.10945813\n",
      "Iteration 142, loss = 0.11942436\n",
      "Iteration 146, loss = 0.10985398\n",
      "Iteration 143, loss = 0.11797744\n",
      "Iteration 147, loss = 0.10733517\n",
      "Iteration 144, loss = 0.11760458\n",
      "Iteration 148, loss = 0.10722294\n",
      "Iteration 145, loss = 0.11674353\n",
      "Iteration 149, loss = 0.10623363\n",
      "Iteration 146, loss = 0.11532686\n",
      "Iteration 150, loss = 0.10909165\n",
      "Iteration 147, loss = 0.11517718\n",
      "Iteration 151, loss = 0.10501426\n",
      "Iteration 148, loss = 0.11566483\n",
      "Iteration 152, loss = 0.10392142\n",
      "Iteration 149, loss = 0.11252546\n",
      "Iteration 153, loss = 0.10452127\n",
      "Iteration 150, loss = 0.11171092\n",
      "Iteration 154, loss = 0.10218329\n",
      "Iteration 151, loss = 0.11054095\n",
      "Iteration 155, loss = 0.10284053\n",
      "Iteration 152, loss = 0.11054328\n",
      "Iteration 156, loss = 0.10131909\n",
      "Iteration 153, loss = 0.10916684\n",
      "Iteration 157, loss = 0.10157642\n",
      "Iteration 154, loss = 0.10787519\n",
      "Iteration 158, loss = 0.09848829\n",
      "Iteration 155, loss = 0.10778348\n",
      "Iteration 159, loss = 0.09796696\n",
      "Iteration 156, loss = 0.10667510\n",
      "Iteration 160, loss = 0.09795342\n",
      "Iteration 157, loss = 0.10531395\n",
      "Iteration 161, loss = 0.09715745\n",
      "Iteration 158, loss = 0.10551664\n",
      "Iteration 162, loss = 0.09601269\n",
      "Iteration 159, loss = 0.10401284\n",
      "Iteration 163, loss = 0.09771585\n",
      "Iteration 160, loss = 0.10403991\n",
      "Iteration 164, loss = 0.09594430\n",
      "Iteration 161, loss = 0.10601523\n",
      "Iteration 165, loss = 0.09396290\n",
      "Iteration 162, loss = 0.10266470\n",
      "Iteration 166, loss = 0.09366779\n",
      "Iteration 163, loss = 0.10208086\n",
      "Iteration 167, loss = 0.09827723\n",
      "Iteration 164, loss = 0.10259119\n",
      "Iteration 168, loss = 0.09278954\n",
      "Iteration 165, loss = 0.10185578\n",
      "Iteration 169, loss = 0.09257997\n",
      "Iteration 166, loss = 0.10069938\n",
      "Iteration 170, loss = 0.09305463\n",
      "Iteration 167, loss = 0.09878289\n",
      "Iteration 171, loss = 0.08977838\n",
      "Iteration 168, loss = 0.09867213\n",
      "Iteration 172, loss = 0.09028182\n",
      "Iteration 169, loss = 0.09898486\n",
      "Iteration 173, loss = 0.08870305\n",
      "Iteration 170, loss = 0.09586616\n",
      "Iteration 174, loss = 0.08813531\n",
      "Iteration 171, loss = 0.09761985\n",
      "Iteration 175, loss = 0.08785944\n",
      "Iteration 172, loss = 0.09492702\n",
      "Iteration 176, loss = 0.08779267\n",
      "Iteration 173, loss = 0.09565315\n",
      "Iteration 177, loss = 0.08634794\n",
      "Iteration 174, loss = 0.09366011\n",
      "Iteration 178, loss = 0.08899787\n",
      "Iteration 175, loss = 0.09349508\n",
      "Iteration 179, loss = 0.08554363\n",
      "Iteration 176, loss = 0.09513260\n",
      "Iteration 180, loss = 0.08472008\n",
      "Iteration 177, loss = 0.09106475\n",
      "Iteration 181, loss = 0.08529356\n",
      "Iteration 178, loss = 0.09274236\n",
      "Iteration 182, loss = 0.08226903\n",
      "Iteration 179, loss = 0.09150676\n",
      "Iteration 183, loss = 0.08214715\n",
      "Iteration 180, loss = 0.08993983\n",
      "Iteration 184, loss = 0.08204002\n",
      "Iteration 181, loss = 0.09056455\n",
      "Iteration 185, loss = 0.08101939\n",
      "Iteration 182, loss = 0.08958864\n",
      "Iteration 186, loss = 0.08062844\n",
      "Iteration 183, loss = 0.08973806\n",
      "Iteration 187, loss = 0.07977089\n",
      "Iteration 184, loss = 0.08889567\n",
      "Iteration 188, loss = 0.07997530\n",
      "Iteration 185, loss = 0.08916659\n",
      "Iteration 189, loss = 0.07911489\n",
      "Iteration 186, loss = 0.08889539\n",
      "Iteration 190, loss = 0.07822039\n",
      "Iteration 187, loss = 0.08382975\n",
      "Iteration 191, loss = 0.07966328\n",
      "Iteration 188, loss = 0.08933467\n",
      "Iteration 192, loss = 0.07720887\n",
      "Iteration 189, loss = 0.09000690\n",
      "Iteration 193, loss = 0.07630195\n",
      "Iteration 190, loss = 0.08582003\n",
      "Iteration 194, loss = 0.07679664\n",
      "Iteration 191, loss = 0.08681412\n",
      "Iteration 195, loss = 0.07481196\n",
      "Iteration 192, loss = 0.08551684\n",
      "Iteration 196, loss = 0.07478505\n",
      "Iteration 193, loss = 0.08316640\n",
      "Iteration 197, loss = 0.07360002\n",
      "Iteration 194, loss = 0.08152494\n",
      "Iteration 198, loss = 0.07378541\n",
      "Iteration 195, loss = 0.08092908\n",
      "Iteration 199, loss = 0.07406338\n",
      "Iteration 196, loss = 0.08096983\n",
      "Iteration 197, loss = 0.08131965\n",
      "Iteration 200, loss = 0.07339645\n",
      "Iteration 198, loss = 0.08001974\n",
      "Iteration 201, loss = 0.07335409\n",
      "Iteration 199, loss = 0.07980019\n",
      "Iteration 202, loss = 0.07287943\n",
      "Iteration 200, loss = 0.07948098\n",
      "Iteration 203, loss = 0.07308469\n",
      "Iteration 204, loss = 0.07447820\n",
      "Iteration 201, loss = 0.07813782\n",
      "Iteration 205, loss = 0.07436846\n",
      "Iteration 202, loss = 0.07878488\n",
      "Iteration 206, loss = 0.07138780\n",
      "Iteration 203, loss = 0.07945778\n",
      "Iteration 207, loss = 0.07118065\n",
      "Iteration 204, loss = 0.07687566\n",
      "Iteration 208, loss = 0.07024111\n",
      "Iteration 205, loss = 0.07783075\n",
      "Iteration 209, loss = 0.06984576\n",
      "Iteration 206, loss = 0.07689406\n",
      "Iteration 210, loss = 0.07084463\n",
      "Iteration 207, loss = 0.07678792\n",
      "Iteration 211, loss = 0.07057100\n",
      "Iteration 208, loss = 0.07413535\n",
      "Iteration 212, loss = 0.06675844\n",
      "Iteration 209, loss = 0.07461141\n",
      "Iteration 213, loss = 0.06795358\n",
      "Iteration 210, loss = 0.07501542\n",
      "Iteration 214, loss = 0.06749306\n",
      "Iteration 211, loss = 0.07798831\n",
      "Iteration 215, loss = 0.06554965\n",
      "Iteration 212, loss = 0.07306645\n",
      "Iteration 216, loss = 0.06490160\n",
      "Iteration 213, loss = 0.07478089\n",
      "Iteration 217, loss = 0.06484015\n",
      "Iteration 214, loss = 0.07428254\n",
      "Iteration 218, loss = 0.06368592\n",
      "Iteration 215, loss = 0.07335931\n",
      "Iteration 219, loss = 0.06366818\n",
      "Iteration 216, loss = 0.07051139\n",
      "Iteration 220, loss = 0.06228360\n",
      "Iteration 217, loss = 0.07306682\n",
      "Iteration 221, loss = 0.06432053\n",
      "Iteration 218, loss = 0.06993761\n",
      "Iteration 222, loss = 0.06271526\n",
      "Iteration 219, loss = 0.07241549\n",
      "Iteration 223, loss = 0.06181850\n",
      "Iteration 220, loss = 0.07021165\n",
      "Iteration 224, loss = 0.06306436\n",
      "Iteration 221, loss = 0.06866611\n",
      "Iteration 225, loss = 0.06056602\n",
      "Iteration 222, loss = 0.06937993\n",
      "Iteration 226, loss = 0.06026791\n",
      "Iteration 223, loss = 0.06726918\n",
      "Iteration 227, loss = 0.05941980\n",
      "Iteration 224, loss = 0.06741383\n",
      "Iteration 225, loss = 0.06879554\n",
      "Iteration 228, loss = 0.05920303\n",
      "Iteration 226, loss = 0.06724218\n",
      "Iteration 229, loss = 0.05915767\n",
      "Iteration 227, loss = 0.06611460\n",
      "Iteration 230, loss = 0.05902872\n",
      "Iteration 228, loss = 0.06522450\n",
      "Iteration 231, loss = 0.06008973\n",
      "Iteration 232, loss = 0.06058044\n",
      "Iteration 229, loss = 0.06605685\n",
      "Iteration 230, loss = 0.06557109\n",
      "Iteration 233, loss = 0.05894619\n",
      "Iteration 231, loss = 0.06377720\n",
      "Iteration 234, loss = 0.05831010\n",
      "Iteration 235, loss = 0.05572353Iteration 232, loss = 0.06461898\n",
      "\n",
      "Iteration 236, loss = 0.05817244\n",
      "Iteration 233, loss = 0.07123919\n",
      "Iteration 237, loss = 0.05643155\n",
      "Iteration 234, loss = 0.06714315\n",
      "Iteration 238, loss = 0.05453297\n",
      "Iteration 235, loss = 0.06530857\n",
      "Iteration 239, loss = 0.05644026\n",
      "Iteration 236, loss = 0.06450633\n",
      "Iteration 240, loss = 0.05482380\n",
      "Iteration 237, loss = 0.06237700\n",
      "Iteration 241, loss = 0.05460085\n",
      "Iteration 238, loss = 0.06434674\n",
      "Iteration 242, loss = 0.05406903\n",
      "Iteration 239, loss = 0.06149237\n",
      "Iteration 243, loss = 0.05296942\n",
      "Iteration 240, loss = 0.06063885\n",
      "Iteration 244, loss = 0.05263683\n",
      "Iteration 241, loss = 0.06017642\n",
      "Iteration 245, loss = 0.05228482\n",
      "Iteration 242, loss = 0.06181172\n",
      "Iteration 246, loss = 0.05141147\n",
      "Iteration 243, loss = 0.06128567\n",
      "Iteration 247, loss = 0.05110399\n",
      "Iteration 244, loss = 0.05937692\n",
      "Iteration 248, loss = 0.05111958\n",
      "Iteration 245, loss = 0.05923837\n",
      "Iteration 249, loss = 0.05062134\n",
      "Iteration 246, loss = 0.05938288\n",
      "Iteration 250, loss = 0.04987983\n",
      "Iteration 247, loss = 0.05979487\n",
      "Iteration 251, loss = 0.05151208\n",
      "Iteration 248, loss = 0.05848089\n",
      "Iteration 252, loss = 0.05080721\n",
      "Iteration 249, loss = 0.05877108\n",
      "Iteration 253, loss = 0.04903126\n",
      "Iteration 250, loss = 0.05647583\n",
      "Iteration 254, loss = 0.04882044\n",
      "Iteration 251, loss = 0.05745220\n",
      "Iteration 255, loss = 0.04876862\n",
      "Iteration 252, loss = 0.05579465\n",
      "Iteration 256, loss = 0.04735977\n",
      "Iteration 253, loss = 0.05533682\n",
      "Iteration 257, loss = 0.04798223\n",
      "Iteration 254, loss = 0.05489693\n",
      "Iteration 258, loss = 0.04776803\n",
      "Iteration 255, loss = 0.05622200\n",
      "Iteration 259, loss = 0.04820808\n",
      "Iteration 256, loss = 0.05682625\n",
      "Iteration 260, loss = 0.04659831\n",
      "Iteration 257, loss = 0.05527725\n",
      "Iteration 261, loss = 0.04647917\n",
      "Iteration 258, loss = 0.05478435\n",
      "Iteration 262, loss = 0.04658176\n",
      "Iteration 259, loss = 0.05651325\n",
      "Iteration 263, loss = 0.04604988\n",
      "Iteration 260, loss = 0.05559757\n",
      "Iteration 264, loss = 0.04552755\n",
      "Iteration 261, loss = 0.05375504\n",
      "Iteration 265, loss = 0.04597905\n",
      "Iteration 262, loss = 0.05262416\n",
      "Iteration 266, loss = 0.04426446\n",
      "Iteration 263, loss = 0.05251664\n",
      "Iteration 267, loss = 0.04511060\n",
      "Iteration 264, loss = 0.05183571\n",
      "Iteration 268, loss = 0.04705820\n",
      "Iteration 265, loss = 0.05264988\n",
      "Iteration 269, loss = 0.04835813\n",
      "Iteration 266, loss = 0.05081248\n",
      "Iteration 270, loss = 0.04625727\n",
      "Iteration 267, loss = 0.05143950\n",
      "Iteration 271, loss = 0.04441430\n",
      "Iteration 268, loss = 0.05060105\n",
      "Iteration 272, loss = 0.04361225\n",
      "Iteration 269, loss = 0.05015041\n",
      "Iteration 273, loss = 0.04337406\n",
      "Iteration 270, loss = 0.05084655\n",
      "Iteration 274, loss = 0.04285601\n",
      "Iteration 271, loss = 0.04971117\n",
      "Iteration 275, loss = 0.04181484\n",
      "Iteration 272, loss = 0.04888878\n",
      "Iteration 276, loss = 0.04159821\n",
      "Iteration 273, loss = 0.05101855\n",
      "Iteration 277, loss = 0.04119784\n",
      "Iteration 274, loss = 0.04980235\n",
      "Iteration 278, loss = 0.04152277\n",
      "Iteration 275, loss = 0.04985176\n",
      "Iteration 279, loss = 0.04047686\n",
      "Iteration 276, loss = 0.05100858\n",
      "Iteration 280, loss = 0.04097468\n",
      "Iteration 277, loss = 0.04830081\n",
      "Iteration 281, loss = 0.04091379\n",
      "Iteration 278, loss = 0.04759672\n",
      "Iteration 282, loss = 0.04152942\n",
      "Iteration 279, loss = 0.04833796\n",
      "Iteration 283, loss = 0.04136221\n",
      "Iteration 280, loss = 0.04738476\n",
      "Iteration 284, loss = 0.03983791\n",
      "Iteration 281, loss = 0.04675835\n",
      "Iteration 285, loss = 0.03897303\n",
      "Iteration 282, loss = 0.04528318\n",
      "Iteration 286, loss = 0.03894667\n",
      "Iteration 283, loss = 0.04831585\n",
      "Iteration 287, loss = 0.03796630\n",
      "Iteration 284, loss = 0.04549703\n",
      "Iteration 288, loss = 0.03872346\n",
      "Iteration 285, loss = 0.04859038\n",
      "Iteration 289, loss = 0.03790704\n",
      "Iteration 286, loss = 0.04346233\n",
      "Iteration 290, loss = 0.03719825\n",
      "Iteration 287, loss = 0.04662684\n",
      "Iteration 291, loss = 0.03703651\n",
      "Iteration 288, loss = 0.04566760\n",
      "Iteration 292, loss = 0.03961109\n",
      "Iteration 289, loss = 0.04407749\n",
      "Iteration 293, loss = 0.04260501\n",
      "Iteration 290, loss = 0.04358114\n",
      "Iteration 294, loss = 0.03729390\n",
      "Iteration 291, loss = 0.04287440\n",
      "Iteration 295, loss = 0.03773047\n",
      "Iteration 292, loss = 0.04281154\n",
      "Iteration 296, loss = 0.03687738\n",
      "Iteration 293, loss = 0.04330043\n",
      "Iteration 297, loss = 0.03638895\n",
      "Iteration 294, loss = 0.04256596\n",
      "Iteration 298, loss = 0.03635282\n",
      "Iteration 295, loss = 0.04385701\n",
      "Iteration 299, loss = 0.03467219\n",
      "Iteration 296, loss = 0.04198749\n",
      "Iteration 300, loss = 0.03583215\n",
      "Iteration 297, loss = 0.04259356\n",
      "Iteration 301, loss = 0.03459368\n",
      "Iteration 298, loss = 0.04112764\n",
      "Iteration 302, loss = 0.03440351\n",
      "Iteration 299, loss = 0.04208083\n",
      "Iteration 303, loss = 0.03661134\n",
      "Iteration 300, loss = 0.04092553\n",
      "Iteration 304, loss = 0.03589572\n",
      "Iteration 301, loss = 0.04204856\n",
      "Iteration 305, loss = 0.03277436\n",
      "Iteration 302, loss = 0.04073526\n",
      "Iteration 306, loss = 0.03466315\n",
      "Iteration 303, loss = 0.04047781\n",
      "Iteration 307, loss = 0.03394830\n",
      "Iteration 304, loss = 0.03999342\n",
      "Iteration 308, loss = 0.03330984\n",
      "Iteration 305, loss = 0.04018742\n",
      "Iteration 309, loss = 0.03253379\n",
      "Iteration 306, loss = 0.03872428\n",
      "Iteration 310, loss = 0.03299000\n",
      "Iteration 307, loss = 0.03904256\n",
      "Iteration 311, loss = 0.03221540\n",
      "Iteration 308, loss = 0.03855173\n",
      "Iteration 312, loss = 0.03240647\n",
      "Iteration 309, loss = 0.03849201\n",
      "Iteration 313, loss = 0.03184403\n",
      "Iteration 310, loss = 0.03857763\n",
      "Iteration 314, loss = 0.03135337\n",
      "Iteration 311, loss = 0.03815474\n",
      "Iteration 315, loss = 0.03163681\n",
      "Iteration 312, loss = 0.03800073\n",
      "Iteration 316, loss = 0.03196251\n",
      "Iteration 313, loss = 0.03765243\n",
      "Iteration 317, loss = 0.03171617\n",
      "Iteration 314, loss = 0.03890658\n",
      "Iteration 318, loss = 0.03024924\n",
      "Iteration 315, loss = 0.03732095\n",
      "Iteration 319, loss = 0.03112436\n",
      "Iteration 316, loss = 0.03678034\n",
      "Iteration 320, loss = 0.03024410\n",
      "Iteration 317, loss = 0.03628832\n",
      "Iteration 321, loss = 0.03070125\n",
      "Iteration 318, loss = 0.03789743\n",
      "Iteration 322, loss = 0.02953757\n",
      "Iteration 319, loss = 0.03692143\n",
      "Iteration 323, loss = 0.03045851\n",
      "Iteration 320, loss = 0.03483484\n",
      "Iteration 324, loss = 0.02862445\n",
      "Iteration 321, loss = 0.03730770\n",
      "Iteration 325, loss = 0.02999070\n",
      "Iteration 322, loss = 0.03548893\n",
      "Iteration 326, loss = 0.03027218\n",
      "Iteration 323, loss = 0.03537614\n",
      "Iteration 327, loss = 0.02900860\n",
      "Iteration 324, loss = 0.03561061\n",
      "Iteration 328, loss = 0.02822083\n",
      "Iteration 325, loss = 0.03613360\n",
      "Iteration 329, loss = 0.02830610\n",
      "Iteration 326, loss = 0.03526853\n",
      "Iteration 330, loss = 0.02790308\n",
      "Iteration 327, loss = 0.03414720\n",
      "Iteration 331, loss = 0.02814119\n",
      "Iteration 328, loss = 0.03421117\n",
      "Iteration 332, loss = 0.03036307\n",
      "Iteration 329, loss = 0.03383140\n",
      "Iteration 333, loss = 0.02944718\n",
      "Iteration 330, loss = 0.03499448\n",
      "Iteration 334, loss = 0.02826037\n",
      "Iteration 331, loss = 0.03290854\n",
      "Iteration 335, loss = 0.02780427\n",
      "Iteration 332, loss = 0.03369730\n",
      "Iteration 336, loss = 0.02702430\n",
      "Iteration 333, loss = 0.03291044\n",
      "Iteration 337, loss = 0.02822299\n",
      "Iteration 334, loss = 0.03268638\n",
      "Iteration 338, loss = 0.02716669\n",
      "Iteration 335, loss = 0.03262059\n",
      "Iteration 339, loss = 0.02611971\n",
      "Iteration 336, loss = 0.03516527\n",
      "Iteration 340, loss = 0.02615673\n",
      "Iteration 337, loss = 0.03318199\n",
      "Iteration 341, loss = 0.02615537\n",
      "Iteration 338, loss = 0.03095368\n",
      "Iteration 342, loss = 0.02639626\n",
      "Iteration 339, loss = 0.03330774\n",
      "Iteration 343, loss = 0.02591945\n",
      "Iteration 340, loss = 0.03385858\n",
      "Iteration 344, loss = 0.02593791\n",
      "Iteration 341, loss = 0.03092300\n",
      "Iteration 345, loss = 0.02575465\n",
      "Iteration 346, loss = 0.02686234\n",
      "Iteration 342, loss = 0.03191222\n",
      "Iteration 347, loss = 0.02443888\n",
      "Iteration 343, loss = 0.03101997\n",
      "Iteration 348, loss = 0.02581925\n",
      "Iteration 344, loss = 0.03076486\n",
      "Iteration 345, loss = 0.03030969\n",
      "Iteration 349, loss = 0.02571791\n",
      "Iteration 350, loss = 0.02514984\n",
      "Iteration 346, loss = 0.03098418\n",
      "Iteration 351, loss = 0.02481757\n",
      "Iteration 347, loss = 0.03130414\n",
      "Iteration 352, loss = 0.02555101\n",
      "Iteration 348, loss = 0.03136927\n",
      "Iteration 353, loss = 0.02526790\n",
      "Iteration 349, loss = 0.03144172\n",
      "Iteration 354, loss = 0.02562490\n",
      "Iteration 350, loss = 0.03057521\n",
      "Iteration 355, loss = 0.02312837\n",
      "Iteration 351, loss = 0.03029140\n",
      "Iteration 356, loss = 0.02419382\n",
      "Iteration 352, loss = 0.03189769\n",
      "Iteration 357, loss = 0.02335821\n",
      "Iteration 353, loss = 0.02889803\n",
      "Iteration 358, loss = 0.02327029\n",
      "Iteration 354, loss = 0.02938581\n",
      "Iteration 359, loss = 0.02318845\n",
      "Iteration 355, loss = 0.02982836\n",
      "Iteration 360, loss = 0.02348649\n",
      "Iteration 356, loss = 0.03017445\n",
      "Iteration 361, loss = 0.02430147\n",
      "Iteration 357, loss = 0.03011247\n",
      "Iteration 362, loss = 0.02398503\n",
      "Iteration 358, loss = 0.02898555\n",
      "Iteration 363, loss = 0.02359000\n",
      "Iteration 359, loss = 0.02794385\n",
      "Iteration 364, loss = 0.02368731\n",
      "Iteration 360, loss = 0.02889429\n",
      "Iteration 365, loss = 0.02408527\n",
      "Iteration 361, loss = 0.02860817\n",
      "Iteration 366, loss = 0.02180740\n",
      "Iteration 362, loss = 0.02902749\n",
      "Iteration 367, loss = 0.02241367\n",
      "Iteration 363, loss = 0.02763441\n",
      "Iteration 368, loss = 0.02205350\n",
      "Iteration 364, loss = 0.02860227\n",
      "Iteration 369, loss = 0.02261085\n",
      "Iteration 365, loss = 0.02693368\n",
      "Iteration 370, loss = 0.02156620\n",
      "Iteration 366, loss = 0.02883264\n",
      "Iteration 371, loss = 0.02148396\n",
      "Iteration 367, loss = 0.02709880\n",
      "Iteration 372, loss = 0.02127158\n",
      "Iteration 368, loss = 0.02729399\n",
      "Iteration 373, loss = 0.02082725\n",
      "Iteration 369, loss = 0.02665912\n",
      "Iteration 374, loss = 0.02164646\n",
      "Iteration 370, loss = 0.02752526\n",
      "Iteration 371, loss = 0.02609739\n",
      "Iteration 375, loss = 0.02083749\n",
      "Iteration 372, loss = 0.02576159\n",
      "Iteration 376, loss = 0.02002026\n",
      "Iteration 373, loss = 0.02602674\n",
      "Iteration 377, loss = 0.02053700\n",
      "Iteration 374, loss = 0.02576095\n",
      "Iteration 378, loss = 0.02062122\n",
      "Iteration 375, loss = 0.02510525\n",
      "Iteration 379, loss = 0.02165897\n",
      "Iteration 376, loss = 0.02687926\n",
      "Iteration 380, loss = 0.02024201\n",
      "Iteration 377, loss = 0.02477562\n",
      "Iteration 381, loss = 0.01966506\n",
      "Iteration 382, loss = 0.01987866\n",
      "Iteration 378, loss = 0.02579997\n",
      "Iteration 379, loss = 0.02444915\n",
      "Iteration 383, loss = 0.01947052\n",
      "Iteration 380, loss = 0.02595449\n",
      "Iteration 384, loss = 0.01897663\n",
      "Iteration 381, loss = 0.02483519\n",
      "Iteration 385, loss = 0.01903744\n",
      "Iteration 382, loss = 0.02468344\n",
      "Iteration 386, loss = 0.02016134\n",
      "Iteration 387, loss = 0.02141561\n",
      "Iteration 383, loss = 0.02508357\n",
      "Iteration 388, loss = 0.02164532\n",
      "Iteration 384, loss = 0.02672063\n",
      "Iteration 385, loss = 0.02369430\n",
      "Iteration 389, loss = 0.01881235\n",
      "Iteration 386, loss = 0.02382839\n",
      "Iteration 390, loss = 0.02008953\n",
      "Iteration 387, loss = 0.02556181\n",
      "Iteration 391, loss = 0.01953838\n",
      "Iteration 392, loss = 0.01941588\n",
      "Iteration 388, loss = 0.02401403\n",
      "Iteration 393, loss = 0.01979284\n",
      "Iteration 389, loss = 0.02435469\n",
      "Iteration 394, loss = 0.01939930\n",
      "Iteration 390, loss = 0.02321459\n",
      "Iteration 395, loss = 0.01956955\n",
      "Iteration 391, loss = 0.02393922\n",
      "Iteration 396, loss = 0.01921256\n",
      "Iteration 392, loss = 0.02357975\n",
      "Iteration 397, loss = 0.02042868\n",
      "Iteration 393, loss = 0.02309589\n",
      "Iteration 398, loss = 0.02000363\n",
      "Iteration 394, loss = 0.02260511\n",
      "Iteration 399, loss = 0.01941507\n",
      "Iteration 395, loss = 0.02240405\n",
      "Iteration 396, loss = 0.02329996\n",
      "Iteration 400, loss = 0.01831080\n",
      "Iteration 397, loss = 0.02283413\n",
      "Iteration 401, loss = 0.01788183\n",
      "Iteration 402, loss = 0.01726889\n",
      "Iteration 398, loss = 0.02184431\n",
      "Iteration 403, loss = 0.01741501\n",
      "Iteration 399, loss = 0.02314475\n",
      "Iteration 400, loss = 0.02152897\n",
      "Iteration 404, loss = 0.01767664\n",
      "Iteration 405, loss = 0.01730973\n",
      "Iteration 401, loss = 0.02215421\n",
      "Iteration 402, loss = 0.02272088\n",
      "Iteration 406, loss = 0.01758460\n",
      "Iteration 403, loss = 0.02197551\n",
      "Iteration 407, loss = 0.01664207\n",
      "Iteration 404, loss = 0.02195726\n",
      "Iteration 408, loss = 0.01674227\n",
      "Iteration 405, loss = 0.02166704\n",
      "Iteration 409, loss = 0.01656660\n",
      "Iteration 406, loss = 0.02166983\n",
      "Iteration 410, loss = 0.01639310\n",
      "Iteration 411, loss = 0.01606967\n",
      "Iteration 407, loss = 0.02135072\n",
      "Iteration 412, loss = 0.01660147\n",
      "Iteration 408, loss = 0.02212630\n",
      "Iteration 409, loss = 0.02079126\n",
      "Iteration 413, loss = 0.01645184\n",
      "Iteration 410, loss = 0.02051532\n",
      "Iteration 414, loss = 0.01633146\n",
      "Iteration 411, loss = 0.02268661\n",
      "Iteration 415, loss = 0.01605971\n",
      "Iteration 416, loss = 0.01567474\n",
      "Iteration 412, loss = 0.01955200\n",
      "Iteration 417, loss = 0.01705639\n",
      "Iteration 413, loss = 0.02162476\n",
      "Iteration 418, loss = 0.01606723\n",
      "Iteration 414, loss = 0.02061973\n",
      "Iteration 419, loss = 0.01709533\n",
      "Iteration 415, loss = 0.02034309\n",
      "Iteration 420, loss = 0.01810608\n",
      "Iteration 416, loss = 0.01958660\n",
      "Iteration 421, loss = 0.01701807\n",
      "Iteration 417, loss = 0.02016635\n",
      "Iteration 422, loss = 0.01585303\n",
      "Iteration 418, loss = 0.01989523\n",
      "Iteration 423, loss = 0.01556676\n",
      "Iteration 419, loss = 0.01986718\n",
      "Iteration 424, loss = 0.01507930\n",
      "Iteration 420, loss = 0.01930072\n",
      "Iteration 425, loss = 0.01507233\n",
      "Iteration 421, loss = 0.01946773\n",
      "Iteration 426, loss = 0.01466199\n",
      "Iteration 422, loss = 0.01999078\n",
      "Iteration 427, loss = 0.01495700\n",
      "Iteration 423, loss = 0.01890712\n",
      "Iteration 424, loss = 0.01898024\n",
      "Iteration 428, loss = 0.01468491\n",
      "Iteration 425, loss = 0.01887931\n",
      "Iteration 429, loss = 0.01406648\n",
      "Iteration 426, loss = 0.01865638\n",
      "Iteration 430, loss = 0.01425892\n",
      "Iteration 427, loss = 0.01835421\n",
      "Iteration 431, loss = 0.01475147\n",
      "Iteration 428, loss = 0.01856756\n",
      "Iteration 432, loss = 0.01384012\n",
      "Iteration 429, loss = 0.01839035\n",
      "Iteration 433, loss = 0.01417269\n",
      "Iteration 430, loss = 0.02029186\n",
      "Iteration 434, loss = 0.01388713\n",
      "Iteration 431, loss = 0.01966604\n",
      "Iteration 435, loss = 0.01410929\n",
      "Iteration 432, loss = 0.01922895\n",
      "Iteration 436, loss = 0.01355192\n",
      "Iteration 433, loss = 0.01877266\n",
      "Iteration 437, loss = 0.01363947\n",
      "Iteration 434, loss = 0.01841119\n",
      "Iteration 438, loss = 0.01351510\n",
      "Iteration 435, loss = 0.01863840\n",
      "Iteration 439, loss = 0.01440070\n",
      "Iteration 436, loss = 0.01768780\n",
      "Iteration 440, loss = 0.01530543\n",
      "Iteration 437, loss = 0.01816652\n",
      "Iteration 441, loss = 0.01556055\n",
      "Iteration 438, loss = 0.01818200\n",
      "Iteration 442, loss = 0.01441224\n",
      "Iteration 439, loss = 0.01857821\n",
      "Iteration 443, loss = 0.01362733\n",
      "Iteration 440, loss = 0.01806808\n",
      "Iteration 444, loss = 0.01344479\n",
      "Iteration 441, loss = 0.01761433\n",
      "Iteration 445, loss = 0.01391477\n",
      "Iteration 442, loss = 0.01673157\n",
      "Iteration 446, loss = 0.01365999\n",
      "Iteration 443, loss = 0.01720886\n",
      "Iteration 447, loss = 0.01284224\n",
      "Iteration 444, loss = 0.01741376\n",
      "Iteration 448, loss = 0.01346052\n",
      "Iteration 445, loss = 0.01778878\n",
      "Iteration 449, loss = 0.01238386\n",
      "Iteration 446, loss = 0.01642113\n",
      "Iteration 450, loss = 0.01290897\n",
      "Iteration 447, loss = 0.01675437\n",
      "Iteration 451, loss = 0.01249414\n",
      "Iteration 448, loss = 0.01775567\n",
      "Iteration 452, loss = 0.01264311\n",
      "Iteration 453, loss = 0.01249068Iteration 449, loss = 0.01661930\n",
      "\n",
      "Iteration 454, loss = 0.01237688\n",
      "Iteration 450, loss = 0.01658603\n",
      "Iteration 455, loss = 0.01273925\n",
      "Iteration 451, loss = 0.01730955\n",
      "Iteration 456, loss = 0.01228180\n",
      "Iteration 452, loss = 0.01712571\n",
      "Iteration 457, loss = 0.01288387\n",
      "Iteration 453, loss = 0.01657860\n",
      "Iteration 458, loss = 0.01340028\n",
      "Iteration 454, loss = 0.01585639\n",
      "Iteration 459, loss = 0.01404586\n",
      "Iteration 455, loss = 0.01679608\n",
      "Iteration 460, loss = 0.01402099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 456, loss = 0.01610465\n",
      "Iteration 457, loss = 0.01653443\n",
      "Iteration 458, loss = 0.01573081\n",
      "Iteration 459, loss = 0.01645667\n",
      "Iteration 460, loss = 0.01595159\n",
      "Iteration 461, loss = 0.01730315\n",
      "Iteration 462, loss = 0.01576660\n",
      "Iteration 463, loss = 0.01583241\n",
      "Iteration 464, loss = 0.01574147\n",
      "Iteration 465, loss = 0.01515058\n",
      "Iteration 466, loss = 0.01590647\n",
      "Iteration 467, loss = 0.01694555\n",
      "Iteration 468, loss = 0.01546249\n",
      "Iteration 469, loss = 0.01606940\n",
      "Iteration 470, loss = 0.01553429\n",
      "Iteration 471, loss = 0.01464721\n",
      "Iteration 472, loss = 0.01559388\n",
      "Iteration 473, loss = 0.01447066\n",
      "Iteration 1, loss = 1.18003019\n",
      "Iteration 474, loss = 0.01466113\n",
      "Iteration 2, loss = 0.99160787\n",
      "Iteration 475, loss = 0.01470670\n",
      "Iteration 476, loss = 0.01412286\n",
      "Iteration 3, loss = 0.86694255\n",
      "Iteration 477, loss = 0.01409064\n",
      "Iteration 4, loss = 0.77328267\n",
      "Iteration 478, loss = 0.01429568\n",
      "Iteration 5, loss = 0.70311998\n",
      "Iteration 479, loss = 0.01514823\n",
      "Iteration 6, loss = 0.64598087\n",
      "Iteration 7, loss = 0.60065326\n",
      "Iteration 480, loss = 0.01450304\n",
      "Iteration 8, loss = 0.56312144\n",
      "Iteration 481, loss = 0.01403770\n",
      "Iteration 9, loss = 0.53165653\n",
      "Iteration 482, loss = 0.01379609\n",
      "Iteration 10, loss = 0.50518611\n",
      "Iteration 483, loss = 0.01375706\n",
      "Iteration 11, loss = 0.48305157\n",
      "Iteration 484, loss = 0.01482194\n",
      "Iteration 485, loss = 0.01333293\n",
      "Iteration 12, loss = 0.46429091\n",
      "Iteration 486, loss = 0.01331518\n",
      "Iteration 13, loss = 0.44778976\n",
      "Iteration 487, loss = 0.01353123\n",
      "Iteration 14, loss = 0.43337980\n",
      "Iteration 488, loss = 0.01349511\n",
      "Iteration 15, loss = 0.42080199\n",
      "Iteration 16, loss = 0.40928701\n",
      "Iteration 489, loss = 0.01336184\n",
      "Iteration 17, loss = 0.39873347\n",
      "Iteration 490, loss = 0.01339292\n",
      "Iteration 18, loss = 0.38957719\n",
      "Iteration 491, loss = 0.01297292\n",
      "Iteration 19, loss = 0.38170481\n",
      "Iteration 492, loss = 0.01291419\n",
      "Iteration 20, loss = 0.37370428\n",
      "Iteration 493, loss = 0.01324837\n",
      "Iteration 21, loss = 0.36672511\n",
      "Iteration 494, loss = 0.01377601\n",
      "Iteration 22, loss = 0.36086967\n",
      "Iteration 495, loss = 0.01441534\n",
      "Iteration 496, loss = 0.01519378\n",
      "Iteration 23, loss = 0.35439292\n",
      "Iteration 497, loss = 0.01434634\n",
      "Iteration 24, loss = 0.34804752\n",
      "Iteration 25, loss = 0.34358921\n",
      "Iteration 498, loss = 0.01230797\n",
      "Iteration 26, loss = 0.33796081\n",
      "Iteration 499, loss = 0.01434958\n",
      "Iteration 27, loss = 0.33300881\n",
      "Iteration 500, loss = 0.01325211\n",
      "Iteration 28, loss = 0.32897577\n",
      "Iteration 501, loss = 0.01382988\n",
      "Iteration 29, loss = 0.32350876\n",
      "Iteration 502, loss = 0.01497450\n",
      "Iteration 30, loss = 0.32028124\n",
      "Iteration 503, loss = 0.01655225\n",
      "Iteration 31, loss = 0.31625545\n",
      "Iteration 504, loss = 0.01240569\n",
      "Iteration 32, loss = 0.31205728\n",
      "Iteration 505, loss = 0.01443899\n",
      "Iteration 33, loss = 0.30849008\n",
      "Iteration 506, loss = 0.01576686\n",
      "Iteration 34, loss = 0.30513526\n",
      "Iteration 507, loss = 0.01293083\n",
      "Iteration 35, loss = 0.30144738\n",
      "Iteration 508, loss = 0.01445233\n",
      "Iteration 36, loss = 0.29765397\n",
      "Iteration 509, loss = 0.01333257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.29491199\n",
      "Iteration 38, loss = 0.29160875\n",
      "Iteration 39, loss = 0.28900235\n",
      "Iteration 40, loss = 0.28489082\n",
      "Iteration 41, loss = 0.28230384\n",
      "Iteration 42, loss = 0.27924371\n",
      "Iteration 43, loss = 0.27630917\n",
      "Iteration 44, loss = 0.27314958\n",
      "Iteration 45, loss = 0.26984983\n",
      "Iteration 46, loss = 0.26804596\n",
      "Iteration 47, loss = 0.26588914\n",
      "Iteration 48, loss = 0.26306626\n",
      "Iteration 49, loss = 0.25986441\n",
      "Iteration 50, loss = 0.25647721\n",
      "Iteration 51, loss = 0.25643736\n",
      "Iteration 52, loss = 0.25328161\n",
      "Iteration 53, loss = 0.24968090\n",
      "Iteration 54, loss = 0.24743444\n",
      "Iteration 55, loss = 0.24531583\n",
      "Iteration 1, loss = 1.25131020\n",
      "Iteration 56, loss = 0.24250340\n",
      "Iteration 2, loss = 1.01668488\n",
      "Iteration 3, loss = 0.89049116\n",
      "Iteration 57, loss = 0.24014847\n",
      "Iteration 4, loss = 0.81034341\n",
      "Iteration 58, loss = 0.23757990\n",
      "Iteration 59, loss = 0.23547640\n",
      "Iteration 5, loss = 0.73622249\n",
      "Iteration 6, loss = 0.67616217\n",
      "Iteration 60, loss = 0.23338799\n",
      "Iteration 61, loss = 0.23131136\n",
      "Iteration 7, loss = 0.63100171\n",
      "Iteration 62, loss = 0.22972705\n",
      "Iteration 8, loss = 0.59350997\n",
      "Iteration 9, loss = 0.56091388\n",
      "Iteration 63, loss = 0.22734484\n",
      "Iteration 10, loss = 0.53329126\n",
      "Iteration 64, loss = 0.22559784\n",
      "Iteration 11, loss = 0.51151141\n",
      "Iteration 65, loss = 0.22268586\n",
      "Iteration 12, loss = 0.49135224\n",
      "Iteration 66, loss = 0.22297883\n",
      "Iteration 13, loss = 0.47451166\n",
      "Iteration 67, loss = 0.21922821\n",
      "Iteration 14, loss = 0.45940659\n",
      "Iteration 68, loss = 0.21697905\n",
      "Iteration 15, loss = 0.44570085\n",
      "Iteration 69, loss = 0.21455080\n",
      "Iteration 70, loss = 0.21336464\n",
      "Iteration 16, loss = 0.43303848\n",
      "Iteration 71, loss = 0.21091196\n",
      "Iteration 17, loss = 0.42199661\n",
      "Iteration 72, loss = 0.20913239\n",
      "Iteration 18, loss = 0.41180986\n",
      "Iteration 73, loss = 0.20665735\n",
      "Iteration 19, loss = 0.40242947\n",
      "Iteration 74, loss = 0.20633152\n",
      "Iteration 20, loss = 0.39369522\n",
      "Iteration 75, loss = 0.20228662\n",
      "Iteration 21, loss = 0.38517424\n",
      "Iteration 76, loss = 0.20239774\n",
      "Iteration 22, loss = 0.37716822\n",
      "Iteration 77, loss = 0.19995540\n",
      "Iteration 23, loss = 0.36987590\n",
      "Iteration 78, loss = 0.19797738\n",
      "Iteration 24, loss = 0.36360630\n",
      "Iteration 79, loss = 0.19577714\n",
      "Iteration 25, loss = 0.35699164\n",
      "Iteration 80, loss = 0.19378488\n",
      "Iteration 26, loss = 0.35062000\n",
      "Iteration 81, loss = 0.19227998\n",
      "Iteration 27, loss = 0.34422298\n",
      "Iteration 82, loss = 0.19102991\n",
      "Iteration 28, loss = 0.33898784\n",
      "Iteration 83, loss = 0.18994976\n",
      "Iteration 29, loss = 0.33311488\n",
      "Iteration 84, loss = 0.18707931\n",
      "Iteration 30, loss = 0.32827960\n",
      "Iteration 85, loss = 0.18463887\n",
      "Iteration 31, loss = 0.32249002\n",
      "Iteration 86, loss = 0.18366042\n",
      "Iteration 32, loss = 0.31808816\n",
      "Iteration 33, loss = 0.31300768\n",
      "Iteration 87, loss = 0.18113325\n",
      "Iteration 34, loss = 0.30890295\n",
      "Iteration 88, loss = 0.17969184\n",
      "Iteration 35, loss = 0.30464791\n",
      "Iteration 89, loss = 0.17731202\n",
      "Iteration 90, loss = 0.17760354\n",
      "Iteration 36, loss = 0.30089236\n",
      "Iteration 91, loss = 0.17492837\n",
      "Iteration 37, loss = 0.29627238\n",
      "Iteration 92, loss = 0.17305651\n",
      "Iteration 38, loss = 0.29293348\n",
      "Iteration 93, loss = 0.17135661\n",
      "Iteration 39, loss = 0.28867196\n",
      "Iteration 94, loss = 0.17029735\n",
      "Iteration 40, loss = 0.28464579\n",
      "Iteration 95, loss = 0.16833837\n",
      "Iteration 41, loss = 0.28110942\n",
      "Iteration 96, loss = 0.16644872\n",
      "Iteration 42, loss = 0.27862457\n",
      "Iteration 97, loss = 0.16742129\n",
      "Iteration 43, loss = 0.27520157\n",
      "Iteration 98, loss = 0.16268960\n",
      "Iteration 44, loss = 0.27098471\n",
      "Iteration 99, loss = 0.16230063\n",
      "Iteration 45, loss = 0.26768217\n",
      "Iteration 100, loss = 0.15919859\n",
      "Iteration 46, loss = 0.26476600\n",
      "Iteration 101, loss = 0.15920917\n",
      "Iteration 47, loss = 0.26165185\n",
      "Iteration 102, loss = 0.15776839\n",
      "Iteration 48, loss = 0.25889951\n",
      "Iteration 103, loss = 0.15830055\n",
      "Iteration 49, loss = 0.25653192\n",
      "Iteration 104, loss = 0.15575128\n",
      "Iteration 50, loss = 0.25280424\n",
      "Iteration 105, loss = 0.15284296\n",
      "Iteration 51, loss = 0.25049944\n",
      "Iteration 106, loss = 0.15259891\n",
      "Iteration 52, loss = 0.24730170\n",
      "Iteration 107, loss = 0.14981724\n",
      "Iteration 53, loss = 0.24495808\n",
      "Iteration 108, loss = 0.14873960\n",
      "Iteration 54, loss = 0.24317971\n",
      "Iteration 109, loss = 0.14702501\n",
      "Iteration 55, loss = 0.24076273\n",
      "Iteration 110, loss = 0.14611337\n",
      "Iteration 56, loss = 0.23686250\n",
      "Iteration 111, loss = 0.14509771\n",
      "Iteration 57, loss = 0.23421516\n",
      "Iteration 112, loss = 0.14522480\n",
      "Iteration 58, loss = 0.23250263\n",
      "Iteration 113, loss = 0.14234317\n",
      "Iteration 59, loss = 0.23124453\n",
      "Iteration 114, loss = 0.14184149\n",
      "Iteration 60, loss = 0.22764489\n",
      "Iteration 115, loss = 0.13950090\n",
      "Iteration 61, loss = 0.22578783\n",
      "Iteration 116, loss = 0.13898122\n",
      "Iteration 62, loss = 0.22331218\n",
      "Iteration 117, loss = 0.13676349\n",
      "Iteration 63, loss = 0.22062169\n",
      "Iteration 118, loss = 0.13649546\n",
      "Iteration 64, loss = 0.21963145\n",
      "Iteration 119, loss = 0.13692590\n",
      "Iteration 65, loss = 0.21621671\n",
      "Iteration 120, loss = 0.13466874\n",
      "Iteration 66, loss = 0.21457186\n",
      "Iteration 121, loss = 0.13481614\n",
      "Iteration 67, loss = 0.21166457\n",
      "Iteration 122, loss = 0.13281017\n",
      "Iteration 68, loss = 0.20951696\n",
      "Iteration 123, loss = 0.13091019\n",
      "Iteration 69, loss = 0.20795721\n",
      "Iteration 124, loss = 0.13068966\n",
      "Iteration 70, loss = 0.20586420\n",
      "Iteration 125, loss = 0.12781965\n",
      "Iteration 71, loss = 0.20412157\n",
      "Iteration 126, loss = 0.12775073\n",
      "Iteration 72, loss = 0.20175952\n",
      "Iteration 127, loss = 0.12644113\n",
      "Iteration 73, loss = 0.19903079\n",
      "Iteration 128, loss = 0.12684287\n",
      "Iteration 74, loss = 0.19696225\n",
      "Iteration 129, loss = 0.12585631\n",
      "Iteration 75, loss = 0.19601961\n",
      "Iteration 130, loss = 0.12335081\n",
      "Iteration 76, loss = 0.19304354\n",
      "Iteration 131, loss = 0.12198962\n",
      "Iteration 77, loss = 0.19239142\n",
      "Iteration 132, loss = 0.12212715\n",
      "Iteration 78, loss = 0.18981814\n",
      "Iteration 133, loss = 0.12058061\n",
      "Iteration 79, loss = 0.18864727\n",
      "Iteration 134, loss = 0.11956665\n",
      "Iteration 80, loss = 0.18632818\n",
      "Iteration 135, loss = 0.11967874\n",
      "Iteration 81, loss = 0.18401905\n",
      "Iteration 136, loss = 0.12401433\n",
      "Iteration 82, loss = 0.18279990\n",
      "Iteration 137, loss = 0.11529657\n",
      "Iteration 83, loss = 0.18109577\n",
      "Iteration 138, loss = 0.11854741\n",
      "Iteration 84, loss = 0.18009325\n",
      "Iteration 139, loss = 0.11381866\n",
      "Iteration 140, loss = 0.11467819\n",
      "Iteration 85, loss = 0.17724659\n",
      "Iteration 141, loss = 0.11479453\n",
      "Iteration 86, loss = 0.17528789\n",
      "Iteration 142, loss = 0.11261068\n",
      "Iteration 87, loss = 0.17491108\n",
      "Iteration 143, loss = 0.11174379\n",
      "Iteration 88, loss = 0.17137055\n",
      "Iteration 144, loss = 0.11168415\n",
      "Iteration 89, loss = 0.17167639\n",
      "Iteration 145, loss = 0.10945430\n",
      "Iteration 90, loss = 0.16810938\n",
      "Iteration 146, loss = 0.10965012\n",
      "Iteration 91, loss = 0.16779285\n",
      "Iteration 147, loss = 0.10730369\n",
      "Iteration 92, loss = 0.16409546\n",
      "Iteration 148, loss = 0.10666135\n",
      "Iteration 93, loss = 0.16564941\n",
      "Iteration 149, loss = 0.10609225\n",
      "Iteration 94, loss = 0.16406725\n",
      "Iteration 150, loss = 0.10539594\n",
      "Iteration 95, loss = 0.16450047\n",
      "Iteration 151, loss = 0.10577676\n",
      "Iteration 96, loss = 0.16019470\n",
      "Iteration 152, loss = 0.10242982\n",
      "Iteration 97, loss = 0.15947680\n",
      "Iteration 153, loss = 0.10595917\n",
      "Iteration 98, loss = 0.15842232\n",
      "Iteration 154, loss = 0.10316180\n",
      "Iteration 99, loss = 0.15758394\n",
      "Iteration 155, loss = 0.10028320\n",
      "Iteration 100, loss = 0.15217218\n",
      "Iteration 156, loss = 0.10248650\n",
      "Iteration 101, loss = 0.15260804\n",
      "Iteration 157, loss = 0.10270491\n",
      "Iteration 102, loss = 0.15123415\n",
      "Iteration 158, loss = 0.09887164\n",
      "Iteration 103, loss = 0.15102369\n",
      "Iteration 159, loss = 0.10083375\n",
      "Iteration 104, loss = 0.14957244\n",
      "Iteration 160, loss = 0.10058824\n",
      "Iteration 105, loss = 0.14760012\n",
      "Iteration 161, loss = 0.09895174\n",
      "Iteration 106, loss = 0.14435270\n",
      "Iteration 162, loss = 0.09660631\n",
      "Iteration 107, loss = 0.14317335\n",
      "Iteration 163, loss = 0.09545773\n",
      "Iteration 108, loss = 0.14284059\n",
      "Iteration 164, loss = 0.09468132\n",
      "Iteration 109, loss = 0.14109241\n",
      "Iteration 165, loss = 0.09401188\n",
      "Iteration 110, loss = 0.14044177\n",
      "Iteration 166, loss = 0.09299652\n",
      "Iteration 111, loss = 0.13911536\n",
      "Iteration 167, loss = 0.09207621\n",
      "Iteration 112, loss = 0.13774294\n",
      "Iteration 168, loss = 0.09287501\n",
      "Iteration 113, loss = 0.13503685\n",
      "Iteration 169, loss = 0.09026263\n",
      "Iteration 114, loss = 0.13410332\n",
      "Iteration 115, loss = 0.13393567\n",
      "Iteration 170, loss = 0.09094904\n",
      "Iteration 116, loss = 0.13192290\n",
      "Iteration 171, loss = 0.08832207\n",
      "Iteration 117, loss = 0.13168567\n",
      "Iteration 172, loss = 0.08924843\n",
      "Iteration 118, loss = 0.13071553\n",
      "Iteration 173, loss = 0.08818693\n",
      "Iteration 174, loss = 0.08757055\n",
      "Iteration 119, loss = 0.13055455\n",
      "Iteration 175, loss = 0.08983342\n",
      "Iteration 120, loss = 0.12793632\n",
      "Iteration 121, loss = 0.12650437\n",
      "Iteration 176, loss = 0.08850898\n",
      "Iteration 122, loss = 0.12560864\n",
      "Iteration 177, loss = 0.08761976\n",
      "Iteration 123, loss = 0.12423494\n",
      "Iteration 178, loss = 0.08509500\n",
      "Iteration 124, loss = 0.12332252\n",
      "Iteration 179, loss = 0.08418137\n",
      "Iteration 125, loss = 0.12126108\n",
      "Iteration 180, loss = 0.08386298\n",
      "Iteration 126, loss = 0.12086124\n",
      "Iteration 181, loss = 0.08557197\n",
      "Iteration 127, loss = 0.12017768\n",
      "Iteration 182, loss = 0.08358572\n",
      "Iteration 128, loss = 0.11955520\n",
      "Iteration 183, loss = 0.08284414\n",
      "Iteration 129, loss = 0.11802836\n",
      "Iteration 184, loss = 0.08027035\n",
      "Iteration 130, loss = 0.11790125\n",
      "Iteration 185, loss = 0.08114230\n",
      "Iteration 131, loss = 0.11535644\n",
      "Iteration 186, loss = 0.07882923\n",
      "Iteration 132, loss = 0.11507020\n",
      "Iteration 187, loss = 0.07893252\n",
      "Iteration 133, loss = 0.11486113\n",
      "Iteration 188, loss = 0.07914682\n",
      "Iteration 134, loss = 0.11368404\n",
      "Iteration 189, loss = 0.07725608\n",
      "Iteration 135, loss = 0.11247816\n",
      "Iteration 190, loss = 0.07717079\n",
      "Iteration 136, loss = 0.11190112\n",
      "Iteration 191, loss = 0.07637735\n",
      "Iteration 137, loss = 0.11103661\n",
      "Iteration 192, loss = 0.07611915\n",
      "Iteration 138, loss = 0.10859524\n",
      "Iteration 193, loss = 0.07591687\n",
      "Iteration 139, loss = 0.11033789\n",
      "Iteration 194, loss = 0.07470432\n",
      "Iteration 140, loss = 0.10788105\n",
      "Iteration 195, loss = 0.07476606\n",
      "Iteration 141, loss = 0.10586649\n",
      "Iteration 196, loss = 0.07438357\n",
      "Iteration 142, loss = 0.10697705\n",
      "Iteration 197, loss = 0.07303219\n",
      "Iteration 143, loss = 0.10458354\n",
      "Iteration 198, loss = 0.07327853\n",
      "Iteration 144, loss = 0.10508188\n",
      "Iteration 199, loss = 0.07442699\n",
      "Iteration 145, loss = 0.10395051\n",
      "Iteration 200, loss = 0.07186987\n",
      "Iteration 146, loss = 0.10361225\n",
      "Iteration 201, loss = 0.07050259\n",
      "Iteration 147, loss = 0.10234789\n",
      "Iteration 202, loss = 0.07280695\n",
      "Iteration 148, loss = 0.10163655\n",
      "Iteration 203, loss = 0.07106296\n",
      "Iteration 149, loss = 0.09943664\n",
      "Iteration 204, loss = 0.07007692\n",
      "Iteration 150, loss = 0.09962300\n",
      "Iteration 205, loss = 0.06897529\n",
      "Iteration 151, loss = 0.09904004\n",
      "Iteration 206, loss = 0.06878007\n",
      "Iteration 152, loss = 0.09910738\n",
      "Iteration 207, loss = 0.06710380\n",
      "Iteration 153, loss = 0.09810918\n",
      "Iteration 208, loss = 0.06870978\n",
      "Iteration 154, loss = 0.09606395\n",
      "Iteration 209, loss = 0.06860920\n",
      "Iteration 155, loss = 0.09623702\n",
      "Iteration 210, loss = 0.06707677\n",
      "Iteration 156, loss = 0.09679141\n",
      "Iteration 211, loss = 0.06567663\n",
      "Iteration 157, loss = 0.09421506\n",
      "Iteration 212, loss = 0.06495581\n",
      "Iteration 158, loss = 0.09481621\n",
      "Iteration 213, loss = 0.06564620\n",
      "Iteration 159, loss = 0.09361146\n",
      "Iteration 214, loss = 0.06559335\n",
      "Iteration 160, loss = 0.09205861\n",
      "Iteration 215, loss = 0.06407034\n",
      "Iteration 161, loss = 0.09065346\n",
      "Iteration 216, loss = 0.06271884\n",
      "Iteration 162, loss = 0.09095165\n",
      "Iteration 217, loss = 0.06261902\n",
      "Iteration 163, loss = 0.08940707\n",
      "Iteration 218, loss = 0.06303789\n",
      "Iteration 164, loss = 0.08894084\n",
      "Iteration 219, loss = 0.06059992\n",
      "Iteration 165, loss = 0.08921271\n",
      "Iteration 220, loss = 0.06220908\n",
      "Iteration 166, loss = 0.08851918\n",
      "Iteration 221, loss = 0.06382909\n",
      "Iteration 167, loss = 0.08848177\n",
      "Iteration 222, loss = 0.05941215\n",
      "Iteration 223, loss = 0.06086948\n",
      "Iteration 168, loss = 0.08585991\n",
      "Iteration 224, loss = 0.06038691\n",
      "Iteration 169, loss = 0.08708193\n",
      "Iteration 170, loss = 0.08746411\n",
      "Iteration 225, loss = 0.05830946\n",
      "Iteration 171, loss = 0.08815631\n",
      "Iteration 226, loss = 0.05833922\n",
      "Iteration 172, loss = 0.08293890\n",
      "Iteration 227, loss = 0.05754946\n",
      "Iteration 173, loss = 0.08480811\n",
      "Iteration 228, loss = 0.05749769\n",
      "Iteration 174, loss = 0.08331505\n",
      "Iteration 229, loss = 0.05649940\n",
      "Iteration 230, loss = 0.05742374Iteration 175, loss = 0.08138898\n",
      "\n",
      "Iteration 231, loss = 0.05646501\n",
      "Iteration 176, loss = 0.08056851\n",
      "Iteration 232, loss = 0.05527677Iteration 177, loss = 0.08179565\n",
      "\n",
      "Iteration 233, loss = 0.05497863\n",
      "Iteration 178, loss = 0.07846020\n",
      "Iteration 234, loss = 0.05480874\n",
      "Iteration 179, loss = 0.07955008\n",
      "Iteration 235, loss = 0.05473079\n",
      "Iteration 180, loss = 0.08182458\n",
      "Iteration 236, loss = 0.05278018\n",
      "Iteration 181, loss = 0.07936588\n",
      "Iteration 237, loss = 0.05462425\n",
      "Iteration 182, loss = 0.07836782\n",
      "Iteration 238, loss = 0.05329236\n",
      "Iteration 183, loss = 0.07679913\n",
      "Iteration 239, loss = 0.05242578\n",
      "Iteration 184, loss = 0.07844601\n",
      "Iteration 240, loss = 0.05501636\n",
      "Iteration 185, loss = 0.07495929\n",
      "Iteration 241, loss = 0.05270268\n",
      "Iteration 186, loss = 0.07489977\n",
      "Iteration 242, loss = 0.05098696\n",
      "Iteration 187, loss = 0.07523605\n",
      "Iteration 243, loss = 0.05332632\n",
      "Iteration 188, loss = 0.07270755\n",
      "Iteration 189, loss = 0.07373569\n",
      "Iteration 244, loss = 0.05281923\n",
      "Iteration 190, loss = 0.07167048\n",
      "Iteration 245, loss = 0.05069681\n",
      "Iteration 246, loss = 0.05001986\n",
      "Iteration 191, loss = 0.07163870\n",
      "Iteration 247, loss = 0.04949233\n",
      "Iteration 192, loss = 0.07272171\n",
      "Iteration 248, loss = 0.04910024\n",
      "Iteration 193, loss = 0.07192185\n",
      "Iteration 249, loss = 0.04871090\n",
      "Iteration 194, loss = 0.07040911\n",
      "Iteration 250, loss = 0.05122070\n",
      "Iteration 195, loss = 0.07004614\n",
      "Iteration 251, loss = 0.05278930\n",
      "Iteration 196, loss = 0.06907620\n",
      "Iteration 252, loss = 0.05017682\n",
      "Iteration 197, loss = 0.07051427\n",
      "Iteration 253, loss = 0.04827923\n",
      "Iteration 198, loss = 0.06905834\n",
      "Iteration 199, loss = 0.06831719\n",
      "Iteration 254, loss = 0.04616838\n",
      "Iteration 200, loss = 0.06797779\n",
      "Iteration 255, loss = 0.04727839\n",
      "Iteration 201, loss = 0.06678532\n",
      "Iteration 256, loss = 0.04717011\n",
      "Iteration 202, loss = 0.06582596\n",
      "Iteration 257, loss = 0.04611840\n",
      "Iteration 203, loss = 0.06740574\n",
      "Iteration 258, loss = 0.04525641\n",
      "Iteration 204, loss = 0.06614419\n",
      "Iteration 259, loss = 0.04477876\n",
      "Iteration 205, loss = 0.06505474\n",
      "Iteration 260, loss = 0.04421852\n",
      "Iteration 206, loss = 0.06508574\n",
      "Iteration 261, loss = 0.04467198\n",
      "Iteration 262, loss = 0.04816417\n",
      "Iteration 207, loss = 0.06342352\n",
      "Iteration 263, loss = 0.04711983\n",
      "Iteration 208, loss = 0.06327303\n",
      "Iteration 264, loss = 0.04404217\n",
      "Iteration 209, loss = 0.06279592\n",
      "Iteration 265, loss = 0.04674228\n",
      "Iteration 210, loss = 0.06213156\n",
      "Iteration 266, loss = 0.05076711\n",
      "Iteration 211, loss = 0.06161082\n",
      "Iteration 267, loss = 0.04484956\n",
      "Iteration 212, loss = 0.06084198\n",
      "Iteration 268, loss = 0.04442648\n",
      "Iteration 213, loss = 0.06169165\n",
      "Iteration 269, loss = 0.04503376\n",
      "Iteration 214, loss = 0.05999606\n",
      "Iteration 270, loss = 0.04135200\n",
      "Iteration 215, loss = 0.05951478\n",
      "Iteration 271, loss = 0.04131083\n",
      "Iteration 216, loss = 0.05951397\n",
      "Iteration 272, loss = 0.04060297\n",
      "Iteration 217, loss = 0.06077089\n",
      "Iteration 218, loss = 0.06018668\n",
      "Iteration 273, loss = 0.04115503\n",
      "Iteration 219, loss = 0.06188321\n",
      "Iteration 274, loss = 0.04112728\n",
      "Iteration 220, loss = 0.05864312\n",
      "Iteration 275, loss = 0.03986944\n",
      "Iteration 221, loss = 0.05847643\n",
      "Iteration 276, loss = 0.03993856\n",
      "Iteration 222, loss = 0.05800379\n",
      "Iteration 277, loss = 0.03862748\n",
      "Iteration 223, loss = 0.05620372\n",
      "Iteration 278, loss = 0.03919956\n",
      "Iteration 224, loss = 0.05813915\n",
      "Iteration 279, loss = 0.04145485\n",
      "Iteration 225, loss = 0.05762377\n",
      "Iteration 280, loss = 0.03938845\n",
      "Iteration 281, loss = 0.03891963\n",
      "Iteration 226, loss = 0.05568342\n",
      "Iteration 282, loss = 0.03833686\n",
      "Iteration 227, loss = 0.05611980\n",
      "Iteration 283, loss = 0.03800816\n",
      "Iteration 228, loss = 0.05422032\n",
      "Iteration 284, loss = 0.03706921\n",
      "Iteration 229, loss = 0.05397798\n",
      "Iteration 285, loss = 0.03641357\n",
      "Iteration 230, loss = 0.05373682\n",
      "Iteration 286, loss = 0.03644169\n",
      "Iteration 231, loss = 0.05735991\n",
      "Iteration 287, loss = 0.03605832\n",
      "Iteration 232, loss = 0.05348236\n",
      "Iteration 288, loss = 0.03680366\n",
      "Iteration 233, loss = 0.05588978\n",
      "Iteration 289, loss = 0.03725017\n",
      "Iteration 234, loss = 0.05278298\n",
      "Iteration 290, loss = 0.03657767\n",
      "Iteration 235, loss = 0.05324604\n",
      "Iteration 291, loss = 0.03459183\n",
      "Iteration 236, loss = 0.05020312\n",
      "Iteration 292, loss = 0.03600480\n",
      "Iteration 237, loss = 0.05004393\n",
      "Iteration 293, loss = 0.03449374\n",
      "Iteration 238, loss = 0.04957024\n",
      "Iteration 294, loss = 0.03564416\n",
      "Iteration 239, loss = 0.04920696\n",
      "Iteration 295, loss = 0.03427435\n",
      "Iteration 240, loss = 0.04930948\n",
      "Iteration 296, loss = 0.03360834\n",
      "Iteration 241, loss = 0.04828874\n",
      "Iteration 297, loss = 0.03357327\n",
      "Iteration 242, loss = 0.04993015\n",
      "Iteration 298, loss = 0.03295835\n",
      "Iteration 243, loss = 0.05094942\n",
      "Iteration 299, loss = 0.03357956\n",
      "Iteration 244, loss = 0.04680845\n",
      "Iteration 300, loss = 0.03332228\n",
      "Iteration 245, loss = 0.04828954\n",
      "Iteration 301, loss = 0.03213414\n",
      "Iteration 246, loss = 0.05054552\n",
      "Iteration 302, loss = 0.03242133\n",
      "Iteration 247, loss = 0.04827928\n",
      "Iteration 303, loss = 0.03204534\n",
      "Iteration 248, loss = 0.04698495\n",
      "Iteration 304, loss = 0.03178967\n",
      "Iteration 249, loss = 0.04587753\n",
      "Iteration 305, loss = 0.03207607\n",
      "Iteration 250, loss = 0.04613941\n",
      "Iteration 306, loss = 0.03087791\n",
      "Iteration 251, loss = 0.04591091\n",
      "Iteration 307, loss = 0.03145206\n",
      "Iteration 252, loss = 0.04538742\n",
      "Iteration 308, loss = 0.03160040\n",
      "Iteration 253, loss = 0.04446819\n",
      "Iteration 309, loss = 0.03186511\n",
      "Iteration 254, loss = 0.04474251\n",
      "Iteration 310, loss = 0.03230645\n",
      "Iteration 255, loss = 0.04497863\n",
      "Iteration 311, loss = 0.03026524\n",
      "Iteration 256, loss = 0.04495929\n",
      "Iteration 312, loss = 0.03124889\n",
      "Iteration 257, loss = 0.04320860\n",
      "Iteration 313, loss = 0.03188016\n",
      "Iteration 258, loss = 0.04594842\n",
      "Iteration 259, loss = 0.04784756\n",
      "Iteration 314, loss = 0.03067112\n",
      "Iteration 315, loss = 0.02958540\n",
      "Iteration 260, loss = 0.04321344\n",
      "Iteration 316, loss = 0.02948441\n",
      "Iteration 261, loss = 0.04505920\n",
      "Iteration 262, loss = 0.04207028\n",
      "Iteration 317, loss = 0.02994814\n",
      "Iteration 318, loss = 0.03077307\n",
      "Iteration 263, loss = 0.04317190\n",
      "Iteration 319, loss = 0.02885908\n",
      "Iteration 264, loss = 0.04243707\n",
      "Iteration 320, loss = 0.02822529\n",
      "Iteration 265, loss = 0.04290183\n",
      "Iteration 321, loss = 0.03043966\n",
      "Iteration 266, loss = 0.04218250\n",
      "Iteration 322, loss = 0.02892453\n",
      "Iteration 267, loss = 0.04005884\n",
      "Iteration 323, loss = 0.02767071\n",
      "Iteration 268, loss = 0.04011289\n",
      "Iteration 324, loss = 0.02930692\n",
      "Iteration 269, loss = 0.04052124\n",
      "Iteration 325, loss = 0.02799242\n",
      "Iteration 270, loss = 0.03904626\n",
      "Iteration 326, loss = 0.02687490\n",
      "Iteration 271, loss = 0.03899051\n",
      "Iteration 327, loss = 0.02763375\n",
      "Iteration 272, loss = 0.03899025\n",
      "Iteration 328, loss = 0.02747199\n",
      "Iteration 273, loss = 0.03884876\n",
      "Iteration 329, loss = 0.02653806\n",
      "Iteration 274, loss = 0.03883558\n",
      "Iteration 330, loss = 0.02772816\n",
      "Iteration 275, loss = 0.03883653\n",
      "Iteration 331, loss = 0.02695338\n",
      "Iteration 276, loss = 0.03778712\n",
      "Iteration 332, loss = 0.02688263\n",
      "Iteration 277, loss = 0.03891305\n",
      "Iteration 333, loss = 0.02600632\n",
      "Iteration 278, loss = 0.03890017\n",
      "Iteration 334, loss = 0.02600341\n",
      "Iteration 279, loss = 0.03699025\n",
      "Iteration 335, loss = 0.02567554\n",
      "Iteration 280, loss = 0.03685144\n",
      "Iteration 336, loss = 0.02514147\n",
      "Iteration 281, loss = 0.03796874\n",
      "Iteration 337, loss = 0.02551031\n",
      "Iteration 282, loss = 0.03652137\n",
      "Iteration 338, loss = 0.02521108\n",
      "Iteration 283, loss = 0.03668881\n",
      "Iteration 339, loss = 0.02554416\n",
      "Iteration 284, loss = 0.03586738\n",
      "Iteration 340, loss = 0.02541736\n",
      "Iteration 285, loss = 0.03550271\n",
      "Iteration 341, loss = 0.02470819\n",
      "Iteration 286, loss = 0.03644339\n",
      "Iteration 342, loss = 0.02586672\n",
      "Iteration 287, loss = 0.03615701\n",
      "Iteration 343, loss = 0.02537587\n",
      "Iteration 288, loss = 0.03424441\n",
      "Iteration 344, loss = 0.02797862\n",
      "Iteration 289, loss = 0.03536582\n",
      "Iteration 345, loss = 0.02637154\n",
      "Iteration 290, loss = 0.03415853\n",
      "Iteration 346, loss = 0.02652026\n",
      "Iteration 291, loss = 0.03351369\n",
      "Iteration 347, loss = 0.02556696\n",
      "Iteration 292, loss = 0.03421148\n",
      "Iteration 348, loss = 0.02529477\n",
      "Iteration 293, loss = 0.03342407\n",
      "Iteration 349, loss = 0.02560707\n",
      "Iteration 294, loss = 0.03403439\n",
      "Iteration 350, loss = 0.02441523\n",
      "Iteration 295, loss = 0.03306976\n",
      "Iteration 351, loss = 0.02414518\n",
      "Iteration 296, loss = 0.03265724\n",
      "Iteration 352, loss = 0.02509987\n",
      "Iteration 297, loss = 0.03344736\n",
      "Iteration 353, loss = 0.02422543\n",
      "Iteration 298, loss = 0.03197846\n",
      "Iteration 354, loss = 0.02453676\n",
      "Iteration 299, loss = 0.03164817\n",
      "Iteration 355, loss = 0.02266204\n",
      "Iteration 300, loss = 0.03194667\n",
      "Iteration 356, loss = 0.02412536\n",
      "Iteration 301, loss = 0.03149133\n",
      "Iteration 357, loss = 0.02365252\n",
      "Iteration 302, loss = 0.03207660\n",
      "Iteration 358, loss = 0.02345704\n",
      "Iteration 303, loss = 0.03089591\n",
      "Iteration 359, loss = 0.02264375\n",
      "Iteration 304, loss = 0.03094370\n",
      "Iteration 360, loss = 0.02226996\n",
      "Iteration 305, loss = 0.03168636\n",
      "Iteration 361, loss = 0.02203088\n",
      "Iteration 306, loss = 0.03077274\n",
      "Iteration 362, loss = 0.02215706\n",
      "Iteration 307, loss = 0.03009206\n",
      "Iteration 363, loss = 0.02127311\n",
      "Iteration 308, loss = 0.02935926\n",
      "Iteration 364, loss = 0.02160378\n",
      "Iteration 309, loss = 0.02976683\n",
      "Iteration 365, loss = 0.02187541\n",
      "Iteration 310, loss = 0.03059400\n",
      "Iteration 366, loss = 0.02063180\n",
      "Iteration 311, loss = 0.02943869\n",
      "Iteration 367, loss = 0.02097739\n",
      "Iteration 312, loss = 0.02966150\n",
      "Iteration 368, loss = 0.02305047\n",
      "Iteration 313, loss = 0.02941636\n",
      "Iteration 369, loss = 0.02134985\n",
      "Iteration 314, loss = 0.02879346\n",
      "Iteration 370, loss = 0.02205866\n",
      "Iteration 315, loss = 0.02825227\n",
      "Iteration 371, loss = 0.02081209\n",
      "Iteration 316, loss = 0.02902278\n",
      "Iteration 372, loss = 0.02053058\n",
      "Iteration 317, loss = 0.03009417\n",
      "Iteration 318, loss = 0.03085461\n",
      "Iteration 373, loss = 0.02052060\n",
      "Iteration 319, loss = 0.02815532\n",
      "Iteration 374, loss = 0.02016380\n",
      "Iteration 375, loss = 0.02003603\n",
      "Iteration 320, loss = 0.02791983\n",
      "Iteration 321, loss = 0.02718681\n",
      "Iteration 376, loss = 0.02091082\n",
      "Iteration 322, loss = 0.02726193\n",
      "Iteration 377, loss = 0.02051425\n",
      "Iteration 323, loss = 0.02708596\n",
      "Iteration 378, loss = 0.01985870\n",
      "Iteration 324, loss = 0.02718965\n",
      "Iteration 379, loss = 0.02012318\n",
      "Iteration 325, loss = 0.02685873\n",
      "Iteration 380, loss = 0.01938375\n",
      "Iteration 326, loss = 0.02732621\n",
      "Iteration 381, loss = 0.01873548\n",
      "Iteration 382, loss = 0.01884186\n",
      "Iteration 327, loss = 0.02719064\n",
      "Iteration 328, loss = 0.02834833\n",
      "Iteration 383, loss = 0.01997066\n",
      "Iteration 384, loss = 0.01885268\n",
      "Iteration 329, loss = 0.02723087\n",
      "Iteration 385, loss = 0.01848916\n",
      "Iteration 330, loss = 0.02689061\n",
      "Iteration 386, loss = 0.01899769\n",
      "Iteration 331, loss = 0.02601285\n",
      "Iteration 387, loss = 0.02063141\n",
      "Iteration 332, loss = 0.02608972\n",
      "Iteration 388, loss = 0.02126261\n",
      "Iteration 333, loss = 0.02476336\n",
      "Iteration 389, loss = 0.01839129\n",
      "Iteration 334, loss = 0.02465488\n",
      "Iteration 390, loss = 0.02108515\n",
      "Iteration 335, loss = 0.02502986\n",
      "Iteration 336, loss = 0.02445307\n",
      "Iteration 391, loss = 0.01812632\n",
      "Iteration 337, loss = 0.02472865\n",
      "Iteration 392, loss = 0.01820679\n",
      "Iteration 338, loss = 0.02443887\n",
      "Iteration 393, loss = 0.01847772\n",
      "Iteration 394, loss = 0.01766017\n",
      "Iteration 339, loss = 0.02431512\n",
      "Iteration 395, loss = 0.01737068\n",
      "Iteration 340, loss = 0.02359898\n",
      "Iteration 396, loss = 0.01751508\n",
      "Iteration 341, loss = 0.02395730\n",
      "Iteration 397, loss = 0.01697097\n",
      "Iteration 342, loss = 0.02342892\n",
      "Iteration 398, loss = 0.01723168\n",
      "Iteration 343, loss = 0.02312337\n",
      "Iteration 399, loss = 0.01713802\n",
      "Iteration 344, loss = 0.02376880\n",
      "Iteration 400, loss = 0.01793381\n",
      "Iteration 345, loss = 0.02254496\n",
      "Iteration 401, loss = 0.02000659\n",
      "Iteration 346, loss = 0.02346608\n",
      "Iteration 402, loss = 0.02296575\n",
      "Iteration 347, loss = 0.02226630\n",
      "Iteration 403, loss = 0.02155949\n",
      "Iteration 348, loss = 0.02288455\n",
      "Iteration 404, loss = 0.02001995\n",
      "Iteration 349, loss = 0.02391535\n",
      "Iteration 405, loss = 0.02021868\n",
      "Iteration 350, loss = 0.02630884\n",
      "Iteration 406, loss = 0.01744565\n",
      "Iteration 351, loss = 0.02536258\n",
      "Iteration 407, loss = 0.02065874\n",
      "Iteration 352, loss = 0.02275265\n",
      "Iteration 408, loss = 0.01904543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 353, loss = 0.02266048\n",
      "Iteration 354, loss = 0.02347561\n",
      "Iteration 355, loss = 0.02307384\n",
      "Iteration 356, loss = 0.02204223\n",
      "Iteration 357, loss = 0.02236179\n",
      "Iteration 358, loss = 0.02165919\n",
      "Iteration 359, loss = 0.02136079\n",
      "Iteration 360, loss = 0.02133070\n",
      "Iteration 361, loss = 0.02068694\n",
      "Iteration 362, loss = 0.02056280\n",
      "Iteration 363, loss = 0.02017056\n",
      "Iteration 364, loss = 0.02027016\n",
      "Iteration 365, loss = 0.02056951\n",
      "Iteration 366, loss = 0.02048532\n",
      "Iteration 367, loss = 0.02011467\n",
      "Iteration 368, loss = 0.02039898\n",
      "Iteration 369, loss = 0.01968072\n",
      "Iteration 370, loss = 0.01975780\n",
      "Iteration 371, loss = 0.01906643\n",
      "Iteration 1, loss = 1.08522758\n",
      "Iteration 372, loss = 0.01877707\n",
      "Iteration 2, loss = 0.94644185\n",
      "Iteration 373, loss = 0.01959642\n",
      "Iteration 3, loss = 0.86330189\n",
      "Iteration 374, loss = 0.01992714\n",
      "Iteration 4, loss = 0.79122722\n",
      "Iteration 375, loss = 0.01846058\n",
      "Iteration 5, loss = 0.72484783\n",
      "Iteration 376, loss = 0.01948922\n",
      "Iteration 6, loss = 0.66861811\n",
      "Iteration 377, loss = 0.01920986\n",
      "Iteration 7, loss = 0.62086322\n",
      "Iteration 378, loss = 0.01860081\n",
      "Iteration 8, loss = 0.57856343\n",
      "Iteration 379, loss = 0.01883078\n",
      "Iteration 9, loss = 0.54185556\n",
      "Iteration 380, loss = 0.01953052\n",
      "Iteration 10, loss = 0.51143531\n",
      "Iteration 381, loss = 0.01877184\n",
      "Iteration 11, loss = 0.48664021\n",
      "Iteration 382, loss = 0.02127727\n",
      "Iteration 12, loss = 0.46426461\n",
      "Iteration 383, loss = 0.01789075\n",
      "Iteration 13, loss = 0.44617584\n",
      "Iteration 384, loss = 0.01830630\n",
      "Iteration 14, loss = 0.42999933\n",
      "Iteration 385, loss = 0.01864645\n",
      "Iteration 15, loss = 0.41630141\n",
      "Iteration 386, loss = 0.01852638\n",
      "Iteration 16, loss = 0.40424710\n",
      "Iteration 387, loss = 0.01757246\n",
      "Iteration 17, loss = 0.39327294\n",
      "Iteration 388, loss = 0.01717022\n",
      "Iteration 18, loss = 0.38451567\n",
      "Iteration 389, loss = 0.01773556\n",
      "Iteration 19, loss = 0.37582844\n",
      "Iteration 390, loss = 0.01751110\n",
      "Iteration 20, loss = 0.36746218\n",
      "Iteration 391, loss = 0.01655936\n",
      "Iteration 21, loss = 0.36122909\n",
      "Iteration 392, loss = 0.01705927\n",
      "Iteration 22, loss = 0.35486584\n",
      "Iteration 393, loss = 0.01691864\n",
      "Iteration 23, loss = 0.34794932\n",
      "Iteration 394, loss = 0.01649772\n",
      "Iteration 395, loss = 0.01658305\n",
      "Iteration 24, loss = 0.34224330\n",
      "Iteration 396, loss = 0.01721695\n",
      "Iteration 25, loss = 0.33690357\n",
      "Iteration 397, loss = 0.01623791\n",
      "Iteration 26, loss = 0.33125054\n",
      "Iteration 398, loss = 0.01594482\n",
      "Iteration 27, loss = 0.32648357\n",
      "Iteration 399, loss = 0.01651403\n",
      "Iteration 28, loss = 0.32188860\n",
      "Iteration 400, loss = 0.01606088\n",
      "Iteration 29, loss = 0.31736393\n",
      "Iteration 401, loss = 0.01588893\n",
      "Iteration 30, loss = 0.31263593\n",
      "Iteration 402, loss = 0.01592163\n",
      "Iteration 31, loss = 0.30804462\n",
      "Iteration 403, loss = 0.01575690\n",
      "Iteration 32, loss = 0.30388166\n",
      "Iteration 404, loss = 0.01672518\n",
      "Iteration 33, loss = 0.30018034\n",
      "Iteration 405, loss = 0.01671201\n",
      "Iteration 34, loss = 0.29655504\n",
      "Iteration 406, loss = 0.01617047\n",
      "Iteration 35, loss = 0.29323759\n",
      "Iteration 407, loss = 0.01689369\n",
      "Iteration 36, loss = 0.28935495\n",
      "Iteration 408, loss = 0.01626256\n",
      "Iteration 37, loss = 0.28627505\n",
      "Iteration 409, loss = 0.01628307\n",
      "Iteration 38, loss = 0.28227985\n",
      "Iteration 410, loss = 0.01519250\n",
      "Iteration 39, loss = 0.27980684\n",
      "Iteration 411, loss = 0.01610486\n",
      "Iteration 40, loss = 0.27558047\n",
      "Iteration 412, loss = 0.01553682\n",
      "Iteration 41, loss = 0.27308411\n",
      "Iteration 413, loss = 0.01509101Iteration 42, loss = 0.26944848\n",
      "\n",
      "Iteration 414, loss = 0.01456596\n",
      "Iteration 43, loss = 0.26664854\n",
      "Iteration 44, loss = 0.26418759\n",
      "Iteration 415, loss = 0.01473466\n",
      "Iteration 45, loss = 0.26059829\n",
      "Iteration 416, loss = 0.01480046\n",
      "Iteration 46, loss = 0.25796141\n",
      "Iteration 417, loss = 0.01511162\n",
      "Iteration 47, loss = 0.25512040\n",
      "Iteration 418, loss = 0.01384914\n",
      "Iteration 48, loss = 0.25285987\n",
      "Iteration 419, loss = 0.01516841\n",
      "Iteration 49, loss = 0.25130901\n",
      "Iteration 420, loss = 0.01434667\n",
      "Iteration 50, loss = 0.24745654\n",
      "Iteration 421, loss = 0.01434451\n",
      "Iteration 422, loss = 0.01403390\n",
      "Iteration 51, loss = 0.24472714\n",
      "Iteration 423, loss = 0.01451974\n",
      "Iteration 52, loss = 0.24273584\n",
      "Iteration 424, loss = 0.01380050\n",
      "Iteration 53, loss = 0.24116268\n",
      "Iteration 425, loss = 0.01356710\n",
      "Iteration 54, loss = 0.23765800\n",
      "Iteration 426, loss = 0.01367171\n",
      "Iteration 55, loss = 0.23492052\n",
      "Iteration 427, loss = 0.01373260\n",
      "Iteration 56, loss = 0.23244879\n",
      "Iteration 428, loss = 0.01321957\n",
      "Iteration 57, loss = 0.23037500\n",
      "Iteration 429, loss = 0.01348038\n",
      "Iteration 58, loss = 0.22816239\n",
      "Iteration 430, loss = 0.01349794\n",
      "Iteration 59, loss = 0.22554069\n",
      "Iteration 431, loss = 0.01378181\n",
      "Iteration 60, loss = 0.22383208\n",
      "Iteration 432, loss = 0.01348451\n",
      "Iteration 61, loss = 0.22097810\n",
      "Iteration 433, loss = 0.01303933\n",
      "Iteration 62, loss = 0.21887851\n",
      "Iteration 434, loss = 0.01322055\n",
      "Iteration 63, loss = 0.21683491\n",
      "Iteration 435, loss = 0.01283186\n",
      "Iteration 64, loss = 0.21522104\n",
      "Iteration 436, loss = 0.01324952\n",
      "Iteration 65, loss = 0.21310754\n",
      "Iteration 437, loss = 0.01312920\n",
      "Iteration 66, loss = 0.21033296\n",
      "Iteration 438, loss = 0.01265639\n",
      "Iteration 67, loss = 0.21085033\n",
      "Iteration 439, loss = 0.01393727\n",
      "Iteration 68, loss = 0.20830957\n",
      "Iteration 440, loss = 0.01324592\n",
      "Iteration 69, loss = 0.20503778\n",
      "Iteration 441, loss = 0.01257068\n",
      "Iteration 70, loss = 0.20417955\n",
      "Iteration 442, loss = 0.01224703\n",
      "Iteration 71, loss = 0.20077558\n",
      "Iteration 443, loss = 0.01229631\n",
      "Iteration 72, loss = 0.19971188\n",
      "Iteration 444, loss = 0.01229905\n",
      "Iteration 73, loss = 0.19804657\n",
      "Iteration 445, loss = 0.01301474\n",
      "Iteration 74, loss = 0.19553512\n",
      "Iteration 446, loss = 0.01318114\n",
      "Iteration 75, loss = 0.19361405\n",
      "Iteration 447, loss = 0.01338437\n",
      "Iteration 76, loss = 0.19177343\n",
      "Iteration 448, loss = 0.01214498\n",
      "Iteration 77, loss = 0.19139792\n",
      "Iteration 449, loss = 0.01210262\n",
      "Iteration 78, loss = 0.18783542\n",
      "Iteration 450, loss = 0.01221432\n",
      "Iteration 79, loss = 0.18720555\n",
      "Iteration 451, loss = 0.01239935\n",
      "Iteration 80, loss = 0.18479511\n",
      "Iteration 452, loss = 0.01214060\n",
      "Iteration 81, loss = 0.18299733\n",
      "Iteration 453, loss = 0.01196895\n",
      "Iteration 82, loss = 0.18288680\n",
      "Iteration 454, loss = 0.01223685\n",
      "Iteration 83, loss = 0.18028220\n",
      "Iteration 455, loss = 0.01145871\n",
      "Iteration 84, loss = 0.17858080\n",
      "Iteration 456, loss = 0.01191281\n",
      "Iteration 85, loss = 0.17859231\n",
      "Iteration 457, loss = 0.01151785\n",
      "Iteration 86, loss = 0.17521021\n",
      "Iteration 458, loss = 0.01258717\n",
      "Iteration 87, loss = 0.17699078\n",
      "Iteration 459, loss = 0.01206498\n",
      "Iteration 88, loss = 0.17157543\n",
      "Iteration 460, loss = 0.01128600\n",
      "Iteration 89, loss = 0.17071562\n",
      "Iteration 461, loss = 0.01105862\n",
      "Iteration 90, loss = 0.16905858\n",
      "Iteration 462, loss = 0.01126865\n",
      "Iteration 91, loss = 0.16803594\n",
      "Iteration 463, loss = 0.01149513\n",
      "Iteration 92, loss = 0.16638599\n",
      "Iteration 464, loss = 0.01067421\n",
      "Iteration 93, loss = 0.16577710\n",
      "Iteration 465, loss = 0.01055710\n",
      "Iteration 94, loss = 0.16459595\n",
      "Iteration 466, loss = 0.01125911\n",
      "Iteration 95, loss = 0.16123995\n",
      "Iteration 467, loss = 0.01103755\n",
      "Iteration 96, loss = 0.16042158\n",
      "Iteration 468, loss = 0.01028300\n",
      "Iteration 97, loss = 0.15933257\n",
      "Iteration 469, loss = 0.01052708\n",
      "Iteration 98, loss = 0.15688463\n",
      "Iteration 470, loss = 0.01050859\n",
      "Iteration 99, loss = 0.15605676\n",
      "Iteration 471, loss = 0.01069591\n",
      "Iteration 100, loss = 0.15475353\n",
      "Iteration 472, loss = 0.01042138\n",
      "Iteration 101, loss = 0.15439447\n",
      "Iteration 473, loss = 0.01010312\n",
      "Iteration 102, loss = 0.15206209\n",
      "Iteration 474, loss = 0.01092287\n",
      "Iteration 103, loss = 0.15066145\n",
      "Iteration 475, loss = 0.01147281\n",
      "Iteration 104, loss = 0.14869914\n",
      "Iteration 476, loss = 0.01064515\n",
      "Iteration 105, loss = 0.14804982\n",
      "Iteration 477, loss = 0.01022161\n",
      "Iteration 106, loss = 0.14561769\n",
      "Iteration 478, loss = 0.01058080\n",
      "Iteration 107, loss = 0.14455263\n",
      "Iteration 479, loss = 0.00978159\n",
      "Iteration 108, loss = 0.14364186\n",
      "Iteration 480, loss = 0.01070938\n",
      "Iteration 109, loss = 0.14349812\n",
      "Iteration 481, loss = 0.01140125\n",
      "Iteration 110, loss = 0.14181318\n",
      "Iteration 482, loss = 0.01135289\n",
      "Iteration 111, loss = 0.13976338\n",
      "Iteration 483, loss = 0.01023065\n",
      "Iteration 112, loss = 0.13902871\n",
      "Iteration 484, loss = 0.00999787\n",
      "Iteration 113, loss = 0.13704203\n",
      "Iteration 485, loss = 0.01091509\n",
      "Iteration 114, loss = 0.13581230\n",
      "Iteration 486, loss = 0.01038670\n",
      "Iteration 115, loss = 0.13474639\n",
      "Iteration 487, loss = 0.00895572\n",
      "Iteration 116, loss = 0.13416226\n",
      "Iteration 488, loss = 0.01062026\n",
      "Iteration 117, loss = 0.13229050\n",
      "Iteration 489, loss = 0.00948855\n",
      "Iteration 118, loss = 0.13148237\n",
      "Iteration 490, loss = 0.00934706\n",
      "Iteration 119, loss = 0.13048743\n",
      "Iteration 491, loss = 0.00881549\n",
      "Iteration 120, loss = 0.13002624\n",
      "Iteration 492, loss = 0.00991677\n",
      "Iteration 121, loss = 0.12784461\n",
      "Iteration 493, loss = 0.00966925\n",
      "Iteration 122, loss = 0.12713911\n",
      "Iteration 494, loss = 0.01122799\n",
      "Iteration 123, loss = 0.12773490\n",
      "Iteration 495, loss = 0.00874924\n",
      "Iteration 124, loss = 0.12514300\n",
      "Iteration 496, loss = 0.01009710\n",
      "Iteration 125, loss = 0.12397062\n",
      "Iteration 497, loss = 0.01042623\n",
      "Iteration 126, loss = 0.12572403\n",
      "Iteration 498, loss = 0.01018144\n",
      "Iteration 127, loss = 0.12298534\n",
      "Iteration 499, loss = 0.00887751\n",
      "Iteration 128, loss = 0.12379315\n",
      "Iteration 500, loss = 0.00898485\n",
      "Iteration 129, loss = 0.12724090\n",
      "Iteration 501, loss = 0.00871887\n",
      "Iteration 130, loss = 0.11939496\n",
      "Iteration 502, loss = 0.00873445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 131, loss = 0.12182882\n",
      "Iteration 132, loss = 0.11891345\n",
      "Iteration 133, loss = 0.11688239\n",
      "Iteration 134, loss = 0.11880935\n",
      "Iteration 135, loss = 0.11530892\n",
      "Iteration 136, loss = 0.11662701\n",
      "Iteration 137, loss = 0.11297641\n",
      "Iteration 138, loss = 0.11345559\n",
      "Iteration 139, loss = 0.11139979\n",
      "Iteration 140, loss = 0.11092294\n",
      "Iteration 141, loss = 0.11078097\n",
      "Iteration 142, loss = 0.11046542\n",
      "Iteration 143, loss = 0.10846968\n",
      "Iteration 144, loss = 0.10844487\n",
      "Iteration 145, loss = 0.10634834\n",
      "Iteration 146, loss = 0.10666641\n",
      "Iteration 147, loss = 0.10653424\n",
      "Iteration 148, loss = 0.10451002\n",
      "Iteration 149, loss = 0.10434228\n",
      "Iteration 1, loss = 1.33817488\n",
      "Iteration 150, loss = 0.10368379\n",
      "Iteration 2, loss = 1.09187292\n",
      "Iteration 151, loss = 0.10495247\n",
      "Iteration 3, loss = 0.92576043\n",
      "Iteration 152, loss = 0.10283155\n",
      "Iteration 4, loss = 0.82546716\n",
      "Iteration 153, loss = 0.10155939\n",
      "Iteration 5, loss = 0.75473215\n",
      "Iteration 154, loss = 0.10126972\n",
      "Iteration 6, loss = 0.69590591\n",
      "Iteration 155, loss = 0.09861676\n",
      "Iteration 7, loss = 0.64476182\n",
      "Iteration 156, loss = 0.10001255\n",
      "Iteration 8, loss = 0.60130055\n",
      "Iteration 157, loss = 0.09872927\n",
      "Iteration 9, loss = 0.56800947\n",
      "Iteration 158, loss = 0.09688755\n",
      "Iteration 10, loss = 0.53988321\n",
      "Iteration 159, loss = 0.09718948\n",
      "Iteration 11, loss = 0.51529849\n",
      "Iteration 160, loss = 0.09929523\n",
      "Iteration 12, loss = 0.49434805\n",
      "Iteration 161, loss = 0.09473252\n",
      "Iteration 13, loss = 0.47554345\n",
      "Iteration 162, loss = 0.09614728\n",
      "Iteration 14, loss = 0.45891495\n",
      "Iteration 163, loss = 0.09533204\n",
      "Iteration 15, loss = 0.44444871\n",
      "Iteration 164, loss = 0.09374538\n",
      "Iteration 16, loss = 0.43144621\n",
      "Iteration 165, loss = 0.09448789\n",
      "Iteration 17, loss = 0.41885226\n",
      "Iteration 166, loss = 0.09364872\n",
      "Iteration 18, loss = 0.40837949\n",
      "Iteration 19, loss = 0.39849853\n",
      "Iteration 167, loss = 0.09064258\n",
      "Iteration 20, loss = 0.38859779\n",
      "Iteration 168, loss = 0.09284676\n",
      "Iteration 21, loss = 0.37982299\n",
      "Iteration 169, loss = 0.09063218\n",
      "Iteration 22, loss = 0.37179231\n",
      "Iteration 170, loss = 0.08923498\n",
      "Iteration 23, loss = 0.36480078\n",
      "Iteration 171, loss = 0.08958863\n",
      "Iteration 24, loss = 0.35716669\n",
      "Iteration 172, loss = 0.08789388\n",
      "Iteration 25, loss = 0.35070853\n",
      "Iteration 173, loss = 0.08782925\n",
      "Iteration 26, loss = 0.34459037\n",
      "Iteration 174, loss = 0.08814588\n",
      "Iteration 175, loss = 0.08650482\n",
      "Iteration 27, loss = 0.33837369\n",
      "Iteration 176, loss = 0.08550056\n",
      "Iteration 28, loss = 0.33227879\n",
      "Iteration 177, loss = 0.08671854\n",
      "Iteration 29, loss = 0.32729562\n",
      "Iteration 178, loss = 0.08584368\n",
      "Iteration 30, loss = 0.32218189\n",
      "Iteration 31, loss = 0.31741760Iteration 179, loss = 0.08698275\n",
      "\n",
      "Iteration 32, loss = 0.31244249\n",
      "Iteration 180, loss = 0.08324677\n",
      "Iteration 33, loss = 0.30728712\n",
      "Iteration 181, loss = 0.08484407\n",
      "Iteration 34, loss = 0.30311889\n",
      "Iteration 182, loss = 0.08169228\n",
      "Iteration 35, loss = 0.29842007\n",
      "Iteration 183, loss = 0.08146601\n",
      "Iteration 36, loss = 0.29515290\n",
      "Iteration 184, loss = 0.08241178\n",
      "Iteration 37, loss = 0.29061809\n",
      "Iteration 185, loss = 0.08019520\n",
      "Iteration 38, loss = 0.28637842\n",
      "Iteration 186, loss = 0.08032807\n",
      "Iteration 39, loss = 0.28248245\n",
      "Iteration 187, loss = 0.08034949\n",
      "Iteration 40, loss = 0.27833445\n",
      "Iteration 188, loss = 0.07881259\n",
      "Iteration 41, loss = 0.27519533\n",
      "Iteration 189, loss = 0.07796496\n",
      "Iteration 42, loss = 0.27146989\n",
      "Iteration 190, loss = 0.07761678\n",
      "Iteration 43, loss = 0.26834797\n",
      "Iteration 191, loss = 0.07877835\n",
      "Iteration 44, loss = 0.26430395\n",
      "Iteration 192, loss = 0.07719074\n",
      "Iteration 45, loss = 0.26093719\n",
      "Iteration 193, loss = 0.07720460\n",
      "Iteration 46, loss = 0.25785098\n",
      "Iteration 194, loss = 0.07612400\n",
      "Iteration 47, loss = 0.25524117\n",
      "Iteration 195, loss = 0.07528848\n",
      "Iteration 48, loss = 0.25204367\n",
      "Iteration 196, loss = 0.07655086\n",
      "Iteration 49, loss = 0.24826971\n",
      "Iteration 197, loss = 0.07640119\n",
      "Iteration 50, loss = 0.24551377\n",
      "Iteration 198, loss = 0.07512924\n",
      "Iteration 51, loss = 0.24260207\n",
      "Iteration 199, loss = 0.07334333\n",
      "Iteration 52, loss = 0.23961317\n",
      "Iteration 200, loss = 0.07234718\n",
      "Iteration 53, loss = 0.23887403\n",
      "Iteration 201, loss = 0.07174306\n",
      "Iteration 54, loss = 0.23409126\n",
      "Iteration 202, loss = 0.07261471\n",
      "Iteration 55, loss = 0.23101358\n",
      "Iteration 203, loss = 0.07028890\n",
      "Iteration 56, loss = 0.22971619\n",
      "Iteration 204, loss = 0.06968983\n",
      "Iteration 57, loss = 0.22513118\n",
      "Iteration 205, loss = 0.07182299\n",
      "Iteration 58, loss = 0.22361243\n",
      "Iteration 206, loss = 0.07248687\n",
      "Iteration 59, loss = 0.22024218\n",
      "Iteration 207, loss = 0.07221700\n",
      "Iteration 60, loss = 0.22096426\n",
      "Iteration 208, loss = 0.06901190\n",
      "Iteration 61, loss = 0.21647671\n",
      "Iteration 209, loss = 0.06910136\n",
      "Iteration 62, loss = 0.21395310\n",
      "Iteration 210, loss = 0.06834421\n",
      "Iteration 63, loss = 0.21038819\n",
      "Iteration 211, loss = 0.06831330\n",
      "Iteration 64, loss = 0.20982762\n",
      "Iteration 212, loss = 0.06636228\n",
      "Iteration 65, loss = 0.20682942\n",
      "Iteration 213, loss = 0.06835471\n",
      "Iteration 66, loss = 0.20427048\n",
      "Iteration 214, loss = 0.06750010\n",
      "Iteration 67, loss = 0.20092122\n",
      "Iteration 215, loss = 0.06673427\n",
      "Iteration 68, loss = 0.19858456\n",
      "Iteration 216, loss = 0.06465623\n",
      "Iteration 69, loss = 0.19744363\n",
      "Iteration 217, loss = 0.06677123\n",
      "Iteration 70, loss = 0.19458339\n",
      "Iteration 218, loss = 0.06472042\n",
      "Iteration 71, loss = 0.19226045\n",
      "Iteration 219, loss = 0.06509167\n",
      "Iteration 72, loss = 0.18954992\n",
      "Iteration 220, loss = 0.06239868\n",
      "Iteration 73, loss = 0.18838150\n",
      "Iteration 221, loss = 0.06423134\n",
      "Iteration 74, loss = 0.18555650\n",
      "Iteration 222, loss = 0.06347036\n",
      "Iteration 75, loss = 0.18334933\n",
      "Iteration 223, loss = 0.06236749\n",
      "Iteration 76, loss = 0.18113241\n",
      "Iteration 224, loss = 0.06365487\n",
      "Iteration 77, loss = 0.17915426\n",
      "Iteration 225, loss = 0.06626690\n",
      "Iteration 78, loss = 0.17684074\n",
      "Iteration 226, loss = 0.06233279\n",
      "Iteration 79, loss = 0.17494707\n",
      "Iteration 227, loss = 0.06342194\n",
      "Iteration 80, loss = 0.17338614\n",
      "Iteration 228, loss = 0.06054272\n",
      "Iteration 81, loss = 0.17120121\n",
      "Iteration 229, loss = 0.05927424\n",
      "Iteration 82, loss = 0.16894742\n",
      "Iteration 230, loss = 0.06030493\n",
      "Iteration 83, loss = 0.16596450\n",
      "Iteration 231, loss = 0.05941533\n",
      "Iteration 84, loss = 0.16509077\n",
      "Iteration 232, loss = 0.05794057\n",
      "Iteration 85, loss = 0.16242941\n",
      "Iteration 233, loss = 0.05752608\n",
      "Iteration 86, loss = 0.16035441\n",
      "Iteration 234, loss = 0.05820092\n",
      "Iteration 87, loss = 0.15872630\n",
      "Iteration 235, loss = 0.05683353\n",
      "Iteration 88, loss = 0.15631234\n",
      "Iteration 236, loss = 0.05659854\n",
      "Iteration 89, loss = 0.15488739\n",
      "Iteration 237, loss = 0.05730146\n",
      "Iteration 90, loss = 0.15272016\n",
      "Iteration 238, loss = 0.05585999\n",
      "Iteration 91, loss = 0.15234471\n",
      "Iteration 239, loss = 0.05490861\n",
      "Iteration 92, loss = 0.14938139\n",
      "Iteration 240, loss = 0.05522983\n",
      "Iteration 93, loss = 0.14772713\n",
      "Iteration 241, loss = 0.05619298\n",
      "Iteration 94, loss = 0.14563707\n",
      "Iteration 242, loss = 0.05352484\n",
      "Iteration 95, loss = 0.14453073\n",
      "Iteration 243, loss = 0.05626824\n",
      "Iteration 96, loss = 0.14269846\n",
      "Iteration 244, loss = 0.05678606\n",
      "Iteration 97, loss = 0.14140085\n",
      "Iteration 245, loss = 0.05539937\n",
      "Iteration 98, loss = 0.14042912\n",
      "Iteration 246, loss = 0.05406419\n",
      "Iteration 99, loss = 0.13844721\n",
      "Iteration 247, loss = 0.05357807\n",
      "Iteration 100, loss = 0.13850722\n",
      "Iteration 248, loss = 0.05329960\n",
      "Iteration 101, loss = 0.13489838\n",
      "Iteration 249, loss = 0.05253580\n",
      "Iteration 102, loss = 0.13376388\n",
      "Iteration 250, loss = 0.05235171\n",
      "Iteration 103, loss = 0.13280016\n",
      "Iteration 251, loss = 0.05202718\n",
      "Iteration 104, loss = 0.13140822\n",
      "Iteration 252, loss = 0.05138408\n",
      "Iteration 105, loss = 0.12919598\n",
      "Iteration 253, loss = 0.05361236\n",
      "Iteration 106, loss = 0.12931457\n",
      "Iteration 254, loss = 0.05195743\n",
      "Iteration 107, loss = 0.12603354\n",
      "Iteration 255, loss = 0.05184493\n",
      "Iteration 108, loss = 0.12649607\n",
      "Iteration 256, loss = 0.05266587\n",
      "Iteration 109, loss = 0.12344904\n",
      "Iteration 257, loss = 0.05111832\n",
      "Iteration 110, loss = 0.12220647\n",
      "Iteration 258, loss = 0.04909629\n",
      "Iteration 111, loss = 0.12163392\n",
      "Iteration 259, loss = 0.04884288\n",
      "Iteration 112, loss = 0.11884644\n",
      "Iteration 260, loss = 0.04862824\n",
      "Iteration 113, loss = 0.11893953\n",
      "Iteration 261, loss = 0.04891448\n",
      "Iteration 114, loss = 0.11704643\n",
      "Iteration 262, loss = 0.05096679\n",
      "Iteration 115, loss = 0.11585604\n",
      "Iteration 263, loss = 0.04866178\n",
      "Iteration 116, loss = 0.11451407\n",
      "Iteration 264, loss = 0.04779127\n",
      "Iteration 117, loss = 0.11301426\n",
      "Iteration 265, loss = 0.04813847\n",
      "Iteration 118, loss = 0.11209317\n",
      "Iteration 266, loss = 0.05037953\n",
      "Iteration 119, loss = 0.11201869\n",
      "Iteration 267, loss = 0.05179856\n",
      "Iteration 120, loss = 0.10867049\n",
      "Iteration 268, loss = 0.04824283\n",
      "Iteration 121, loss = 0.10977985\n",
      "Iteration 269, loss = 0.04622895\n",
      "Iteration 122, loss = 0.11166240\n",
      "Iteration 270, loss = 0.04713608\n",
      "Iteration 123, loss = 0.10666661\n",
      "Iteration 271, loss = 0.04580430\n",
      "Iteration 124, loss = 0.10721324\n",
      "Iteration 272, loss = 0.04960644\n",
      "Iteration 125, loss = 0.10417032\n",
      "Iteration 273, loss = 0.04846049\n",
      "Iteration 126, loss = 0.10373679\n",
      "Iteration 274, loss = 0.04509900\n",
      "Iteration 127, loss = 0.10547025\n",
      "Iteration 275, loss = 0.04828295\n",
      "Iteration 128, loss = 0.10154040\n",
      "Iteration 276, loss = 0.04435632\n",
      "Iteration 129, loss = 0.10234230\n",
      "Iteration 277, loss = 0.04522946\n",
      "Iteration 130, loss = 0.10114830\n",
      "Iteration 278, loss = 0.04355544\n",
      "Iteration 131, loss = 0.09794440\n",
      "Iteration 279, loss = 0.04239349\n",
      "Iteration 132, loss = 0.09863888\n",
      "Iteration 280, loss = 0.04281402\n",
      "Iteration 133, loss = 0.09545483\n",
      "Iteration 281, loss = 0.04209327\n",
      "Iteration 134, loss = 0.09448256\n",
      "Iteration 282, loss = 0.04115151\n",
      "Iteration 135, loss = 0.09407878\n",
      "Iteration 283, loss = 0.04405963\n",
      "Iteration 136, loss = 0.09352548\n",
      "Iteration 284, loss = 0.04171166\n",
      "Iteration 137, loss = 0.09163188\n",
      "Iteration 285, loss = 0.04143172\n",
      "Iteration 138, loss = 0.09189208\n",
      "Iteration 286, loss = 0.04186058\n",
      "Iteration 139, loss = 0.08839388\n",
      "Iteration 287, loss = 0.04221990\n",
      "Iteration 140, loss = 0.08940255\n",
      "Iteration 288, loss = 0.04311111\n",
      "Iteration 141, loss = 0.08815631\n",
      "Iteration 289, loss = 0.03994327\n",
      "Iteration 142, loss = 0.08769450\n",
      "Iteration 290, loss = 0.04163719\n",
      "Iteration 143, loss = 0.08665810\n",
      "Iteration 291, loss = 0.04063755\n",
      "Iteration 144, loss = 0.08587040\n",
      "Iteration 292, loss = 0.04068516\n",
      "Iteration 145, loss = 0.08589738\n",
      "Iteration 293, loss = 0.03888394\n",
      "Iteration 146, loss = 0.08338215\n",
      "Iteration 294, loss = 0.03952176\n",
      "Iteration 147, loss = 0.08355483\n",
      "Iteration 295, loss = 0.03986517\n",
      "Iteration 148, loss = 0.08278506\n",
      "Iteration 296, loss = 0.03814806\n",
      "Iteration 149, loss = 0.08113101\n",
      "Iteration 297, loss = 0.03763036\n",
      "Iteration 150, loss = 0.08028785\n",
      "Iteration 298, loss = 0.03966801\n",
      "Iteration 151, loss = 0.07917708\n",
      "Iteration 299, loss = 0.04047168\n",
      "Iteration 152, loss = 0.07912317\n",
      "Iteration 300, loss = 0.03876400\n",
      "Iteration 153, loss = 0.07890994\n",
      "Iteration 301, loss = 0.03709646\n",
      "Iteration 154, loss = 0.07724734\n",
      "Iteration 302, loss = 0.03815495\n",
      "Iteration 155, loss = 0.07644173\n",
      "Iteration 303, loss = 0.03718954\n",
      "Iteration 156, loss = 0.07510310\n",
      "Iteration 304, loss = 0.03622618\n",
      "Iteration 157, loss = 0.07493694\n",
      "Iteration 305, loss = 0.03875604\n",
      "Iteration 158, loss = 0.07433594\n",
      "Iteration 306, loss = 0.03675536\n",
      "Iteration 159, loss = 0.07355092\n",
      "Iteration 307, loss = 0.03806230\n",
      "Iteration 160, loss = 0.07385141\n",
      "Iteration 308, loss = 0.04140511\n",
      "Iteration 161, loss = 0.07204778\n",
      "Iteration 309, loss = 0.03566287\n",
      "Iteration 162, loss = 0.07218110\n",
      "Iteration 310, loss = 0.03747190\n",
      "Iteration 163, loss = 0.07103738\n",
      "Iteration 311, loss = 0.03585776\n",
      "Iteration 164, loss = 0.07041512\n",
      "Iteration 312, loss = 0.03754463\n",
      "Iteration 165, loss = 0.06978876\n",
      "Iteration 313, loss = 0.03747499\n",
      "Iteration 166, loss = 0.06946352\n",
      "Iteration 314, loss = 0.03537656\n",
      "Iteration 167, loss = 0.06876898\n",
      "Iteration 315, loss = 0.03412028\n",
      "Iteration 168, loss = 0.06820761\n",
      "Iteration 316, loss = 0.03469201\n",
      "Iteration 169, loss = 0.06687175\n",
      "Iteration 317, loss = 0.03357355\n",
      "Iteration 170, loss = 0.06778826\n",
      "Iteration 318, loss = 0.03392940\n",
      "Iteration 171, loss = 0.06676238\n",
      "Iteration 319, loss = 0.03416904\n",
      "Iteration 172, loss = 0.06608127\n",
      "Iteration 320, loss = 0.03329681\n",
      "Iteration 173, loss = 0.06490546\n",
      "Iteration 321, loss = 0.03317330\n",
      "Iteration 174, loss = 0.06505829\n",
      "Iteration 322, loss = 0.03304650\n",
      "Iteration 175, loss = 0.06301366\n",
      "Iteration 323, loss = 0.03202313\n",
      "Iteration 176, loss = 0.06410937\n",
      "Iteration 324, loss = 0.03211853\n",
      "Iteration 177, loss = 0.06237309\n",
      "Iteration 325, loss = 0.03153069\n",
      "Iteration 178, loss = 0.06205040\n",
      "Iteration 326, loss = 0.03192537\n",
      "Iteration 179, loss = 0.06144067\n",
      "Iteration 327, loss = 0.03155930\n",
      "Iteration 180, loss = 0.06011627\n",
      "Iteration 328, loss = 0.03137006\n",
      "Iteration 181, loss = 0.06021431\n",
      "Iteration 329, loss = 0.03097038\n",
      "Iteration 182, loss = 0.06008434\n",
      "Iteration 330, loss = 0.03132618\n",
      "Iteration 183, loss = 0.05739944\n",
      "Iteration 331, loss = 0.03060719\n",
      "Iteration 184, loss = 0.06079383\n",
      "Iteration 332, loss = 0.03051807\n",
      "Iteration 185, loss = 0.05885445\n",
      "Iteration 333, loss = 0.03095230\n",
      "Iteration 186, loss = 0.05893373\n",
      "Iteration 334, loss = 0.02996073\n",
      "Iteration 187, loss = 0.05630131\n",
      "Iteration 335, loss = 0.03026170\n",
      "Iteration 188, loss = 0.05678629\n",
      "Iteration 336, loss = 0.03272819\n",
      "Iteration 189, loss = 0.05547513\n",
      "Iteration 337, loss = 0.02947593\n",
      "Iteration 190, loss = 0.05509083\n",
      "Iteration 338, loss = 0.02925634\n",
      "Iteration 191, loss = 0.05520764\n",
      "Iteration 339, loss = 0.02942156\n",
      "Iteration 192, loss = 0.05423594\n",
      "Iteration 340, loss = 0.02935246\n",
      "Iteration 193, loss = 0.05332676\n",
      "Iteration 341, loss = 0.02894551\n",
      "Iteration 194, loss = 0.05419236\n",
      "Iteration 342, loss = 0.03017652\n",
      "Iteration 195, loss = 0.05285200\n",
      "Iteration 343, loss = 0.02867257\n",
      "Iteration 196, loss = 0.05321275\n",
      "Iteration 344, loss = 0.02897394\n",
      "Iteration 197, loss = 0.05177906\n",
      "Iteration 345, loss = 0.02790437\n",
      "Iteration 198, loss = 0.05256395\n",
      "Iteration 346, loss = 0.02809942\n",
      "Iteration 199, loss = 0.05276972\n",
      "Iteration 347, loss = 0.03074910\n",
      "Iteration 200, loss = 0.05270659\n",
      "Iteration 348, loss = 0.02814760\n",
      "Iteration 201, loss = 0.05216357\n",
      "Iteration 349, loss = 0.02726273\n",
      "Iteration 202, loss = 0.05035443\n",
      "Iteration 350, loss = 0.02733599\n",
      "Iteration 203, loss = 0.04954485\n",
      "Iteration 351, loss = 0.02742754\n",
      "Iteration 204, loss = 0.04805885\n",
      "Iteration 352, loss = 0.02649470\n",
      "Iteration 205, loss = 0.04921160\n",
      "Iteration 353, loss = 0.02686382\n",
      "Iteration 206, loss = 0.04762539\n",
      "Iteration 354, loss = 0.02741321\n",
      "Iteration 207, loss = 0.04738191\n",
      "Iteration 355, loss = 0.02863809\n",
      "Iteration 208, loss = 0.04673915\n",
      "Iteration 356, loss = 0.02638782\n",
      "Iteration 209, loss = 0.04657148\n",
      "Iteration 357, loss = 0.02764674\n",
      "Iteration 210, loss = 0.04718930\n",
      "Iteration 358, loss = 0.02883667\n",
      "Iteration 211, loss = 0.04542004\n",
      "Iteration 359, loss = 0.03056996\n",
      "Iteration 212, loss = 0.04579749\n",
      "Iteration 360, loss = 0.02581849\n",
      "Iteration 213, loss = 0.04520724\n",
      "Iteration 361, loss = 0.02665629\n",
      "Iteration 214, loss = 0.04520659\n",
      "Iteration 362, loss = 0.03222772\n",
      "Iteration 215, loss = 0.04427438\n",
      "Iteration 363, loss = 0.02823801\n",
      "Iteration 216, loss = 0.04445931\n",
      "Iteration 364, loss = 0.02614897\n",
      "Iteration 217, loss = 0.04560955\n",
      "Iteration 365, loss = 0.02978151\n",
      "Iteration 218, loss = 0.04332626\n",
      "Iteration 366, loss = 0.02863661\n",
      "Iteration 219, loss = 0.04280245\n",
      "Iteration 367, loss = 0.02970221\n",
      "Iteration 220, loss = 0.04403882\n",
      "Iteration 368, loss = 0.02605955\n",
      "Iteration 221, loss = 0.04181074\n",
      "Iteration 369, loss = 0.02821822\n",
      "Iteration 222, loss = 0.04214583\n",
      "Iteration 370, loss = 0.02631878\n",
      "Iteration 223, loss = 0.04190053\n",
      "Iteration 371, loss = 0.02410698\n",
      "Iteration 224, loss = 0.04078954\n",
      "Iteration 372, loss = 0.02579319\n",
      "Iteration 225, loss = 0.04241523\n",
      "Iteration 373, loss = 0.02497686\n",
      "Iteration 226, loss = 0.04014785\n",
      "Iteration 374, loss = 0.02376313\n",
      "Iteration 227, loss = 0.04035362\n",
      "Iteration 375, loss = 0.02501322\n",
      "Iteration 228, loss = 0.04032741\n",
      "Iteration 376, loss = 0.02370626\n",
      "Iteration 229, loss = 0.04124573\n",
      "Iteration 377, loss = 0.02394099\n",
      "Iteration 230, loss = 0.03843257\n",
      "Iteration 378, loss = 0.02380332\n",
      "Iteration 231, loss = 0.03923712\n",
      "Iteration 379, loss = 0.02286851\n",
      "Iteration 232, loss = 0.03865468\n",
      "Iteration 380, loss = 0.02301583\n",
      "Iteration 233, loss = 0.03817053\n",
      "Iteration 381, loss = 0.02284209\n",
      "Iteration 234, loss = 0.03831293\n",
      "Iteration 382, loss = 0.02315724\n",
      "Iteration 235, loss = 0.03696198\n",
      "Iteration 383, loss = 0.02317331\n",
      "Iteration 236, loss = 0.03654146\n",
      "Iteration 384, loss = 0.02223503\n",
      "Iteration 237, loss = 0.03614288\n",
      "Iteration 385, loss = 0.02286508\n",
      "Iteration 238, loss = 0.03547002\n",
      "Iteration 386, loss = 0.02191046\n",
      "Iteration 239, loss = 0.03688397\n",
      "Iteration 387, loss = 0.02212301\n",
      "Iteration 240, loss = 0.03594501\n",
      "Iteration 388, loss = 0.02155637\n",
      "Iteration 241, loss = 0.03539383\n",
      "Iteration 389, loss = 0.02186818\n",
      "Iteration 242, loss = 0.03493144\n",
      "Iteration 390, loss = 0.02193262\n",
      "Iteration 243, loss = 0.03422649\n",
      "Iteration 391, loss = 0.02185201\n",
      "Iteration 244, loss = 0.03440635\n",
      "Iteration 392, loss = 0.02216421\n",
      "Iteration 245, loss = 0.03376351\n",
      "Iteration 393, loss = 0.02320690\n",
      "Iteration 246, loss = 0.03348208\n",
      "Iteration 394, loss = 0.02355254\n",
      "Iteration 247, loss = 0.03294242\n",
      "Iteration 395, loss = 0.02019469\n",
      "Iteration 248, loss = 0.03308922\n",
      "Iteration 396, loss = 0.02313894\n",
      "Iteration 249, loss = 0.03237221\n",
      "Iteration 397, loss = 0.02678676\n",
      "Iteration 250, loss = 0.03351302\n",
      "Iteration 398, loss = 0.02268853\n",
      "Iteration 251, loss = 0.03238896\n",
      "Iteration 399, loss = 0.02178563\n",
      "Iteration 252, loss = 0.03354013\n",
      "Iteration 400, loss = 0.02204174\n",
      "Iteration 253, loss = 0.03194969\n",
      "Iteration 401, loss = 0.02324337\n",
      "Iteration 254, loss = 0.03117294\n",
      "Iteration 402, loss = 0.02128137\n",
      "Iteration 255, loss = 0.03319128\n",
      "Iteration 403, loss = 0.02009783\n",
      "Iteration 256, loss = 0.03174714\n",
      "Iteration 404, loss = 0.02006797\n",
      "Iteration 257, loss = 0.03354079\n",
      "Iteration 405, loss = 0.02236965\n",
      "Iteration 258, loss = 0.03204976\n",
      "Iteration 406, loss = 0.02424175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 259, loss = 0.03049142\n",
      "Iteration 260, loss = 0.03145881\n",
      "Iteration 261, loss = 0.02929752\n",
      "Iteration 262, loss = 0.02962647\n",
      "Iteration 263, loss = 0.03003272\n",
      "Iteration 264, loss = 0.02932046\n",
      "Iteration 265, loss = 0.02960639\n",
      "Iteration 266, loss = 0.02855587\n",
      "Iteration 267, loss = 0.02826806\n",
      "Iteration 268, loss = 0.02776970\n",
      "Iteration 269, loss = 0.02825832\n",
      "Iteration 270, loss = 0.02810034\n",
      "Iteration 271, loss = 0.02713427\n",
      "Iteration 272, loss = 0.02749644\n",
      "Iteration 273, loss = 0.02643864\n",
      "Iteration 274, loss = 0.02651065\n",
      "Iteration 275, loss = 0.02651597\n",
      "Iteration 276, loss = 0.02681868\n",
      "Iteration 1, loss = 1.31304959\n",
      "Iteration 277, loss = 0.02517824\n",
      "Iteration 2, loss = 1.14179487\n",
      "Iteration 278, loss = 0.02609688\n",
      "Iteration 3, loss = 1.03302451\n",
      "Iteration 279, loss = 0.02525330\n",
      "Iteration 4, loss = 0.93607946\n",
      "Iteration 280, loss = 0.02516012\n",
      "Iteration 5, loss = 0.84178430\n",
      "Iteration 281, loss = 0.02548081\n",
      "Iteration 6, loss = 0.75715398\n",
      "Iteration 282, loss = 0.02549864\n",
      "Iteration 7, loss = 0.68948487\n",
      "Iteration 283, loss = 0.02467182\n",
      "Iteration 8, loss = 0.63164432\n",
      "Iteration 284, loss = 0.02515979\n",
      "Iteration 9, loss = 0.58306991\n",
      "Iteration 285, loss = 0.02388431\n",
      "Iteration 10, loss = 0.54252272\n",
      "Iteration 286, loss = 0.02467304\n",
      "Iteration 11, loss = 0.51057820\n",
      "Iteration 287, loss = 0.02457456\n",
      "Iteration 12, loss = 0.48344209\n",
      "Iteration 288, loss = 0.02375549\n",
      "Iteration 13, loss = 0.46060183\n",
      "Iteration 289, loss = 0.02351656\n",
      "Iteration 14, loss = 0.44082172\n",
      "Iteration 290, loss = 0.02412643\n",
      "Iteration 15, loss = 0.42418712\n",
      "Iteration 291, loss = 0.02398825\n",
      "Iteration 16, loss = 0.40912711\n",
      "Iteration 292, loss = 0.02324350\n",
      "Iteration 17, loss = 0.39568996\n",
      "Iteration 293, loss = 0.02290207\n",
      "Iteration 18, loss = 0.38427027\n",
      "Iteration 294, loss = 0.02271590\n",
      "Iteration 19, loss = 0.37312941\n",
      "Iteration 295, loss = 0.02197847\n",
      "Iteration 20, loss = 0.36304425\n",
      "Iteration 296, loss = 0.02256501\n",
      "Iteration 21, loss = 0.35558038\n",
      "Iteration 297, loss = 0.02146226\n",
      "Iteration 22, loss = 0.34641678\n",
      "Iteration 298, loss = 0.02198770\n",
      "Iteration 23, loss = 0.33928964\n",
      "Iteration 299, loss = 0.02137737\n",
      "Iteration 24, loss = 0.33188440\n",
      "Iteration 300, loss = 0.02117660\n",
      "Iteration 25, loss = 0.32603349\n",
      "Iteration 301, loss = 0.02094780\n",
      "Iteration 26, loss = 0.31934258\n",
      "Iteration 302, loss = 0.02146680\n",
      "Iteration 27, loss = 0.31318459\n",
      "Iteration 303, loss = 0.02045379\n",
      "Iteration 28, loss = 0.30760563\n",
      "Iteration 304, loss = 0.02077272\n",
      "Iteration 29, loss = 0.30208309\n",
      "Iteration 305, loss = 0.02063159\n",
      "Iteration 30, loss = 0.29735486\n",
      "Iteration 306, loss = 0.02038814\n",
      "Iteration 31, loss = 0.29280680\n",
      "Iteration 307, loss = 0.01990148\n",
      "Iteration 32, loss = 0.28845134\n",
      "Iteration 308, loss = 0.01976998\n",
      "Iteration 33, loss = 0.28410807\n",
      "Iteration 309, loss = 0.01948218\n",
      "Iteration 34, loss = 0.27942040\n",
      "Iteration 310, loss = 0.01949584\n",
      "Iteration 35, loss = 0.27588923\n",
      "Iteration 311, loss = 0.02063660\n",
      "Iteration 36, loss = 0.27116766\n",
      "Iteration 312, loss = 0.02034822\n",
      "Iteration 37, loss = 0.26784641\n",
      "Iteration 313, loss = 0.01990444\n",
      "Iteration 38, loss = 0.26360448\n",
      "Iteration 314, loss = 0.01884947\n",
      "Iteration 315, loss = 0.01865197\n",
      "Iteration 39, loss = 0.25978491\n",
      "Iteration 316, loss = 0.01857549\n",
      "Iteration 40, loss = 0.25658837\n",
      "Iteration 317, loss = 0.01860614\n",
      "Iteration 41, loss = 0.25382246\n",
      "Iteration 42, loss = 0.24952659\n",
      "Iteration 318, loss = 0.01837116\n",
      "Iteration 43, loss = 0.24647002\n",
      "Iteration 319, loss = 0.01827150\n",
      "Iteration 320, loss = 0.01860164\n",
      "Iteration 44, loss = 0.24319564\n",
      "Iteration 321, loss = 0.01814470\n",
      "Iteration 45, loss = 0.24050291\n",
      "Iteration 322, loss = 0.01781931\n",
      "Iteration 46, loss = 0.23733053\n",
      "Iteration 323, loss = 0.01832552\n",
      "Iteration 47, loss = 0.23473612\n",
      "Iteration 324, loss = 0.01761524\n",
      "Iteration 48, loss = 0.23300129\n",
      "Iteration 325, loss = 0.01707786\n",
      "Iteration 49, loss = 0.22999424\n",
      "Iteration 326, loss = 0.01731487\n",
      "Iteration 50, loss = 0.22607951\n",
      "Iteration 327, loss = 0.01721465\n",
      "Iteration 51, loss = 0.22348745\n",
      "Iteration 328, loss = 0.01699032\n",
      "Iteration 52, loss = 0.22072898\n",
      "Iteration 329, loss = 0.01675992\n",
      "Iteration 53, loss = 0.21871059\n",
      "Iteration 330, loss = 0.01721487\n",
      "Iteration 54, loss = 0.21534627\n",
      "Iteration 331, loss = 0.01678870\n",
      "Iteration 55, loss = 0.21323529\n",
      "Iteration 332, loss = 0.01657138\n",
      "Iteration 56, loss = 0.21150483\n",
      "Iteration 333, loss = 0.01619401\n",
      "Iteration 57, loss = 0.20880992\n",
      "Iteration 334, loss = 0.01636401\n",
      "Iteration 58, loss = 0.20724239\n",
      "Iteration 335, loss = 0.01625062\n",
      "Iteration 59, loss = 0.20345760\n",
      "Iteration 336, loss = 0.01595346\n",
      "Iteration 60, loss = 0.20151673\n",
      "Iteration 337, loss = 0.01583094\n",
      "Iteration 61, loss = 0.19868664\n",
      "Iteration 338, loss = 0.01547056\n",
      "Iteration 62, loss = 0.19681942\n",
      "Iteration 339, loss = 0.01534958\n",
      "Iteration 63, loss = 0.19403396\n",
      "Iteration 340, loss = 0.01593636\n",
      "Iteration 64, loss = 0.19275469\n",
      "Iteration 65, loss = 0.19064779\n",
      "Iteration 341, loss = 0.01578875\n",
      "Iteration 66, loss = 0.18905494\n",
      "Iteration 342, loss = 0.01553388\n",
      "Iteration 67, loss = 0.18593277\n",
      "Iteration 343, loss = 0.01517609\n",
      "Iteration 68, loss = 0.18575976\n",
      "Iteration 344, loss = 0.01510491\n",
      "Iteration 69, loss = 0.18317803\n",
      "Iteration 345, loss = 0.01462385\n",
      "Iteration 70, loss = 0.18120865\n",
      "Iteration 346, loss = 0.01476623\n",
      "Iteration 347, loss = 0.01456039\n",
      "Iteration 71, loss = 0.17880675\n",
      "Iteration 72, loss = 0.17660925\n",
      "Iteration 348, loss = 0.01439015\n",
      "Iteration 73, loss = 0.17470444\n",
      "Iteration 349, loss = 0.01418726\n",
      "Iteration 74, loss = 0.17336569\n",
      "Iteration 350, loss = 0.01412194\n",
      "Iteration 75, loss = 0.17236838\n",
      "Iteration 351, loss = 0.01387649\n",
      "Iteration 76, loss = 0.17028713\n",
      "Iteration 352, loss = 0.01423300\n",
      "Iteration 77, loss = 0.16941222\n",
      "Iteration 353, loss = 0.01523195\n",
      "Iteration 78, loss = 0.16603466\n",
      "Iteration 354, loss = 0.01558581\n",
      "Iteration 79, loss = 0.16575562\n",
      "Iteration 355, loss = 0.01438023\n",
      "Iteration 80, loss = 0.16344858\n",
      "Iteration 356, loss = 0.01509118\n",
      "Iteration 81, loss = 0.16147394\n",
      "Iteration 357, loss = 0.01404300\n",
      "Iteration 82, loss = 0.16039660\n",
      "Iteration 83, loss = 0.15786384\n",
      "Iteration 358, loss = 0.01345087\n",
      "Iteration 84, loss = 0.15633422\n",
      "Iteration 359, loss = 0.01366694\n",
      "Iteration 85, loss = 0.15789167\n",
      "Iteration 86, loss = 0.15346975\n",
      "Iteration 87, loss = 0.15370498\n",
      "Iteration 360, loss = 0.01358495\n",
      "Iteration 88, loss = 0.15197597\n",
      "Iteration 361, loss = 0.01388746\n",
      "Iteration 89, loss = 0.15014669\n",
      "Iteration 362, loss = 0.01351180\n",
      "Iteration 90, loss = 0.14843894\n",
      "Iteration 363, loss = 0.01306201\n",
      "Iteration 91, loss = 0.14758803\n",
      "Iteration 364, loss = 0.01362211\n",
      "Iteration 92, loss = 0.14649114\n",
      "Iteration 365, loss = 0.01350319\n",
      "Iteration 93, loss = 0.14449375\n",
      "Iteration 366, loss = 0.01236180\n",
      "Iteration 94, loss = 0.14338945\n",
      "Iteration 367, loss = 0.01423110\n",
      "Iteration 95, loss = 0.14209803\n",
      "Iteration 368, loss = 0.01263694\n",
      "Iteration 96, loss = 0.14046053\n",
      "Iteration 369, loss = 0.01221172\n",
      "Iteration 97, loss = 0.13928834\n",
      "Iteration 370, loss = 0.01297451\n",
      "Iteration 98, loss = 0.13797985\n",
      "Iteration 371, loss = 0.01273540\n",
      "Iteration 99, loss = 0.13725542\n",
      "Iteration 372, loss = 0.01264139\n",
      "Iteration 100, loss = 0.13551981\n",
      "Iteration 373, loss = 0.01158899\n",
      "Iteration 101, loss = 0.13458284\n",
      "Iteration 374, loss = 0.01183527\n",
      "Iteration 102, loss = 0.13311906\n",
      "Iteration 375, loss = 0.01157375\n",
      "Iteration 103, loss = 0.13418824\n",
      "Iteration 376, loss = 0.01154922\n",
      "Iteration 104, loss = 0.13307809\n",
      "Iteration 377, loss = 0.01176061\n",
      "Iteration 105, loss = 0.13254298\n",
      "Iteration 378, loss = 0.01162229\n",
      "Iteration 106, loss = 0.12935945\n",
      "Iteration 379, loss = 0.01127812\n",
      "Iteration 107, loss = 0.13067684\n",
      "Iteration 380, loss = 0.01155737\n",
      "Iteration 108, loss = 0.12622357\n",
      "Iteration 381, loss = 0.01136495\n",
      "Iteration 109, loss = 0.12862883\n",
      "Iteration 110, loss = 0.12445344\n",
      "Iteration 382, loss = 0.01138683\n",
      "Iteration 111, loss = 0.12607829\n",
      "Iteration 383, loss = 0.01111995\n",
      "Iteration 112, loss = 0.12382357\n",
      "Iteration 384, loss = 0.01092146\n",
      "Iteration 113, loss = 0.12273881\n",
      "Iteration 385, loss = 0.01069461\n",
      "Iteration 386, loss = 0.01072650\n",
      "Iteration 114, loss = 0.12248329\n",
      "Iteration 387, loss = 0.01079054\n",
      "Iteration 388, loss = 0.01068405\n",
      "Iteration 115, loss = 0.12051634\n",
      "Iteration 389, loss = 0.01057914\n",
      "Iteration 116, loss = 0.12004309\n",
      "Iteration 390, loss = 0.01055100\n",
      "Iteration 117, loss = 0.12066368\n",
      "Iteration 391, loss = 0.01086522\n",
      "Iteration 118, loss = 0.12020799\n",
      "Iteration 392, loss = 0.01017896\n",
      "Iteration 119, loss = 0.11875477\n",
      "Iteration 393, loss = 0.01054469\n",
      "Iteration 120, loss = 0.11658065\n",
      "Iteration 394, loss = 0.01029217\n",
      "Iteration 121, loss = 0.11623280\n",
      "Iteration 395, loss = 0.00974105\n",
      "Iteration 122, loss = 0.11509194\n",
      "Iteration 396, loss = 0.01026580\n",
      "Iteration 123, loss = 0.11395504\n",
      "Iteration 397, loss = 0.00972412\n",
      "Iteration 124, loss = 0.11365008\n",
      "Iteration 398, loss = 0.00971157\n",
      "Iteration 125, loss = 0.11246330\n",
      "Iteration 399, loss = 0.00985305\n",
      "Iteration 126, loss = 0.11203636\n",
      "Iteration 400, loss = 0.00950916\n",
      "Iteration 127, loss = 0.11131401\n",
      "Iteration 401, loss = 0.01000986\n",
      "Iteration 128, loss = 0.10979284\n",
      "Iteration 402, loss = 0.00956002\n",
      "Iteration 129, loss = 0.11043545\n",
      "Iteration 403, loss = 0.00935036\n",
      "Iteration 130, loss = 0.10842062\n",
      "Iteration 404, loss = 0.00954704\n",
      "Iteration 131, loss = 0.10880658\n",
      "Iteration 405, loss = 0.00925075\n",
      "Iteration 132, loss = 0.10643662\n",
      "Iteration 406, loss = 0.00916626\n",
      "Iteration 133, loss = 0.10581294\n",
      "Iteration 407, loss = 0.00934412\n",
      "Iteration 134, loss = 0.10509822\n",
      "Iteration 408, loss = 0.00912080\n",
      "Iteration 135, loss = 0.10508990\n",
      "Iteration 409, loss = 0.00978793\n",
      "Iteration 136, loss = 0.10408378\n",
      "Iteration 410, loss = 0.00907152\n",
      "Iteration 137, loss = 0.10303982\n",
      "Iteration 411, loss = 0.00911718\n",
      "Iteration 138, loss = 0.10376732\n",
      "Iteration 412, loss = 0.00958567\n",
      "Iteration 139, loss = 0.10146987\n",
      "Iteration 413, loss = 0.00883022\n",
      "Iteration 140, loss = 0.10124660\n",
      "Iteration 414, loss = 0.00876510\n",
      "Iteration 141, loss = 0.10095669\n",
      "Iteration 415, loss = 0.00919337\n",
      "Iteration 142, loss = 0.10097230\n",
      "Iteration 416, loss = 0.00878987\n",
      "Iteration 143, loss = 0.09833027\n",
      "Iteration 417, loss = 0.00880917\n",
      "Iteration 144, loss = 0.09934270\n",
      "Iteration 418, loss = 0.00883181\n",
      "Iteration 145, loss = 0.09816153\n",
      "Iteration 419, loss = 0.00841360\n",
      "Iteration 146, loss = 0.09748861\n",
      "Iteration 420, loss = 0.00848429\n",
      "Iteration 147, loss = 0.09623901\n",
      "Iteration 421, loss = 0.00830248\n",
      "Iteration 148, loss = 0.09870718\n",
      "Iteration 422, loss = 0.00821369\n",
      "Iteration 149, loss = 0.09496773\n",
      "Iteration 423, loss = 0.00825797\n",
      "Iteration 150, loss = 0.09627102\n",
      "Iteration 424, loss = 0.00835637\n",
      "Iteration 151, loss = 0.09434161\n",
      "Iteration 425, loss = 0.00809657\n",
      "Iteration 152, loss = 0.09378879\n",
      "Iteration 426, loss = 0.00796482\n",
      "Iteration 153, loss = 0.09285226\n",
      "Iteration 427, loss = 0.00815654\n",
      "Iteration 154, loss = 0.09405500\n",
      "Iteration 428, loss = 0.00785525\n",
      "Iteration 155, loss = 0.09203842\n",
      "Iteration 429, loss = 0.00784825\n",
      "Iteration 156, loss = 0.09174553\n",
      "Iteration 430, loss = 0.00773777\n",
      "Iteration 157, loss = 0.09200721\n",
      "Iteration 431, loss = 0.00816784\n",
      "Iteration 158, loss = 0.09074957\n",
      "Iteration 432, loss = 0.00807440\n",
      "Iteration 159, loss = 0.08900278\n",
      "Iteration 433, loss = 0.00770040\n",
      "Iteration 160, loss = 0.08904441\n",
      "Iteration 434, loss = 0.00839030\n",
      "Iteration 161, loss = 0.08857674\n",
      "Iteration 435, loss = 0.00744326\n",
      "Iteration 162, loss = 0.08767073\n",
      "Iteration 436, loss = 0.00739605\n",
      "Iteration 163, loss = 0.08806721\n",
      "Iteration 437, loss = 0.00736038\n",
      "Iteration 164, loss = 0.08592624\n",
      "Iteration 438, loss = 0.00745115\n",
      "Iteration 165, loss = 0.08636941\n",
      "Iteration 439, loss = 0.00731151\n",
      "Iteration 166, loss = 0.08977603\n",
      "Iteration 440, loss = 0.00738272\n",
      "Iteration 167, loss = 0.08626306\n",
      "Iteration 441, loss = 0.00723228\n",
      "Iteration 168, loss = 0.08703255\n",
      "Iteration 442, loss = 0.00716747\n",
      "Iteration 169, loss = 0.08398039\n",
      "Iteration 443, loss = 0.00714157\n",
      "Iteration 170, loss = 0.08548712\n",
      "Iteration 444, loss = 0.00703553\n",
      "Iteration 171, loss = 0.08603790\n",
      "Iteration 445, loss = 0.00702518\n",
      "Iteration 172, loss = 0.08183256\n",
      "Iteration 446, loss = 0.00679914\n",
      "Iteration 173, loss = 0.08408861\n",
      "Iteration 447, loss = 0.00704579\n",
      "Iteration 174, loss = 0.08098102\n",
      "Iteration 448, loss = 0.00688602\n",
      "Iteration 175, loss = 0.08049034\n",
      "Iteration 449, loss = 0.00665112\n",
      "Iteration 176, loss = 0.07992976\n",
      "Iteration 450, loss = 0.00689597\n",
      "Iteration 177, loss = 0.07904400\n",
      "Iteration 451, loss = 0.00695013\n",
      "Iteration 178, loss = 0.07903799\n",
      "Iteration 452, loss = 0.00662545\n",
      "Iteration 179, loss = 0.07987622\n",
      "Iteration 453, loss = 0.00678068\n",
      "Iteration 180, loss = 0.07723626\n",
      "Iteration 454, loss = 0.00685153\n",
      "Iteration 181, loss = 0.07856182\n",
      "Iteration 455, loss = 0.00663226\n",
      "Iteration 182, loss = 0.07950622\n",
      "Iteration 456, loss = 0.00683653\n",
      "Iteration 183, loss = 0.07861633\n",
      "Iteration 457, loss = 0.00665720\n",
      "Iteration 184, loss = 0.07658407\n",
      "Iteration 458, loss = 0.00658601\n",
      "Iteration 185, loss = 0.07683466\n",
      "Iteration 459, loss = 0.00645911\n",
      "Iteration 186, loss = 0.07679190\n",
      "Iteration 460, loss = 0.00621709\n",
      "Iteration 187, loss = 0.07529920\n",
      "Iteration 461, loss = 0.00620667\n",
      "Iteration 188, loss = 0.07545843\n",
      "Iteration 462, loss = 0.00616510\n",
      "Iteration 189, loss = 0.07383729\n",
      "Iteration 463, loss = 0.00623962\n",
      "Iteration 190, loss = 0.07360466\n",
      "Iteration 464, loss = 0.00610763\n",
      "Iteration 191, loss = 0.07359330\n",
      "Iteration 465, loss = 0.00628037\n",
      "Iteration 192, loss = 0.07432280\n",
      "Iteration 466, loss = 0.00652095\n",
      "Iteration 193, loss = 0.07136709\n",
      "Iteration 467, loss = 0.00660733\n",
      "Iteration 194, loss = 0.07346962\n",
      "Iteration 468, loss = 0.00605876\n",
      "Iteration 195, loss = 0.07352608\n",
      "Iteration 469, loss = 0.00647139\n",
      "Iteration 196, loss = 0.07287634\n",
      "Iteration 470, loss = 0.00595344\n",
      "Iteration 197, loss = 0.06983579\n",
      "Iteration 471, loss = 0.00589449\n",
      "Iteration 198, loss = 0.07307947\n",
      "Iteration 472, loss = 0.00592864\n",
      "Iteration 199, loss = 0.07199390\n",
      "Iteration 473, loss = 0.00575568\n",
      "Iteration 200, loss = 0.06932055\n",
      "Iteration 474, loss = 0.00578813\n",
      "Iteration 201, loss = 0.06856766\n",
      "Iteration 475, loss = 0.00575249\n",
      "Iteration 202, loss = 0.06850965\n",
      "Iteration 476, loss = 0.00567345\n",
      "Iteration 203, loss = 0.06797330\n",
      "Iteration 477, loss = 0.00563385\n",
      "Iteration 204, loss = 0.06782855\n",
      "Iteration 478, loss = 0.00592564\n",
      "Iteration 205, loss = 0.06702510\n",
      "Iteration 479, loss = 0.00566547\n",
      "Iteration 206, loss = 0.06702558\n",
      "Iteration 480, loss = 0.00560199\n",
      "Iteration 207, loss = 0.06566014\n",
      "Iteration 481, loss = 0.00545740\n",
      "Iteration 208, loss = 0.06604974\n",
      "Iteration 482, loss = 0.00545691\n",
      "Iteration 209, loss = 0.06637429\n",
      "Iteration 483, loss = 0.00528987\n",
      "Iteration 210, loss = 0.06405106\n",
      "Iteration 484, loss = 0.00541942\n",
      "Iteration 211, loss = 0.06450513\n",
      "Iteration 485, loss = 0.00553285\n",
      "Iteration 212, loss = 0.06379179\n",
      "Iteration 486, loss = 0.00520397\n",
      "Iteration 213, loss = 0.06421325\n",
      "Iteration 487, loss = 0.00531683\n",
      "Iteration 214, loss = 0.06298913\n",
      "Iteration 488, loss = 0.00517107\n",
      "Iteration 215, loss = 0.06243949\n",
      "Iteration 489, loss = 0.00519088\n",
      "Iteration 216, loss = 0.06309657\n",
      "Iteration 490, loss = 0.00515993\n",
      "Iteration 217, loss = 0.06151076\n",
      "Iteration 491, loss = 0.00513574\n",
      "Iteration 218, loss = 0.06175896\n",
      "Iteration 492, loss = 0.00517404\n",
      "Iteration 219, loss = 0.06057324\n",
      "Iteration 493, loss = 0.00509282\n",
      "Iteration 220, loss = 0.06114809\n",
      "Iteration 494, loss = 0.00505269\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 221, loss = 0.06304345\n",
      "Iteration 222, loss = 0.06005387\n",
      "Iteration 223, loss = 0.05900215\n",
      "Iteration 224, loss = 0.05978604\n",
      "Iteration 225, loss = 0.06000160\n",
      "Iteration 226, loss = 0.05847835\n",
      "Iteration 227, loss = 0.05762229\n",
      "Iteration 228, loss = 0.05847441\n",
      "Iteration 229, loss = 0.06263649\n",
      "Iteration 230, loss = 0.06102975\n",
      "Iteration 231, loss = 0.05829707\n",
      "Iteration 232, loss = 0.05745357\n",
      "Iteration 233, loss = 0.05759186\n",
      "Iteration 234, loss = 0.05713511\n",
      "Iteration 235, loss = 0.05570971\n",
      "Iteration 236, loss = 0.05437100\n",
      "Iteration 237, loss = 0.05413306\n",
      "Iteration 238, loss = 0.05494978\n",
      "Iteration 1, loss = 1.38607800\n",
      "Iteration 239, loss = 0.05475677\n",
      "Iteration 2, loss = 1.11996746\n",
      "Iteration 240, loss = 0.05362749\n",
      "Iteration 3, loss = 0.96056616\n",
      "Iteration 241, loss = 0.05360555\n",
      "Iteration 4, loss = 0.86087779\n",
      "Iteration 242, loss = 0.05288815\n",
      "Iteration 5, loss = 0.77921176\n",
      "Iteration 243, loss = 0.05258633\n",
      "Iteration 6, loss = 0.70700545\n",
      "Iteration 244, loss = 0.05266759\n",
      "Iteration 7, loss = 0.64853321\n",
      "Iteration 245, loss = 0.05438888\n",
      "Iteration 8, loss = 0.60269182\n",
      "Iteration 246, loss = 0.05306465\n",
      "Iteration 9, loss = 0.56418071\n",
      "Iteration 247, loss = 0.05269363\n",
      "Iteration 10, loss = 0.53266375\n",
      "Iteration 248, loss = 0.05124554\n",
      "Iteration 11, loss = 0.50587833\n",
      "Iteration 249, loss = 0.05168247\n",
      "Iteration 12, loss = 0.48447302\n",
      "Iteration 250, loss = 0.04970731\n",
      "Iteration 13, loss = 0.46441065\n",
      "Iteration 251, loss = 0.05074872\n",
      "Iteration 14, loss = 0.44893974\n",
      "Iteration 252, loss = 0.04980534\n",
      "Iteration 15, loss = 0.43526974\n",
      "Iteration 253, loss = 0.05014567\n",
      "Iteration 16, loss = 0.42295204\n",
      "Iteration 254, loss = 0.04862653\n",
      "Iteration 17, loss = 0.41154200\n",
      "Iteration 255, loss = 0.04870782\n",
      "Iteration 18, loss = 0.40180627\n",
      "Iteration 256, loss = 0.04917685\n",
      "Iteration 19, loss = 0.39268828\n",
      "Iteration 257, loss = 0.04746059\n",
      "Iteration 20, loss = 0.38449258\n",
      "Iteration 258, loss = 0.04716925\n",
      "Iteration 21, loss = 0.37682781\n",
      "Iteration 259, loss = 0.04812558\n",
      "Iteration 22, loss = 0.36932875\n",
      "Iteration 260, loss = 0.04785824\n",
      "Iteration 23, loss = 0.36308593\n",
      "Iteration 261, loss = 0.04672428\n",
      "Iteration 24, loss = 0.35726714\n",
      "Iteration 262, loss = 0.04637615\n",
      "Iteration 25, loss = 0.35176340\n",
      "Iteration 263, loss = 0.04661178\n",
      "Iteration 26, loss = 0.34537255\n",
      "Iteration 264, loss = 0.04516601\n",
      "Iteration 27, loss = 0.33999507\n",
      "Iteration 265, loss = 0.04638158\n",
      "Iteration 28, loss = 0.33471857\n",
      "Iteration 266, loss = 0.04586785\n",
      "Iteration 29, loss = 0.32923789\n",
      "Iteration 267, loss = 0.04485780\n",
      "Iteration 30, loss = 0.32474162\n",
      "Iteration 268, loss = 0.04396606\n",
      "Iteration 31, loss = 0.32000504\n",
      "Iteration 269, loss = 0.04446239\n",
      "Iteration 32, loss = 0.31664991\n",
      "Iteration 270, loss = 0.04453140\n",
      "Iteration 33, loss = 0.31184755\n",
      "Iteration 271, loss = 0.04427848\n",
      "Iteration 34, loss = 0.30707314\n",
      "Iteration 272, loss = 0.04332677\n",
      "Iteration 35, loss = 0.30365481\n",
      "Iteration 273, loss = 0.04375939\n",
      "Iteration 36, loss = 0.30079891\n",
      "Iteration 274, loss = 0.04358565\n",
      "Iteration 37, loss = 0.29589450\n",
      "Iteration 275, loss = 0.04351353\n",
      "Iteration 38, loss = 0.29301881\n",
      "Iteration 276, loss = 0.04331454\n",
      "Iteration 39, loss = 0.29023392\n",
      "Iteration 277, loss = 0.04141885\n",
      "Iteration 40, loss = 0.28530576\n",
      "Iteration 278, loss = 0.04198365\n",
      "Iteration 41, loss = 0.28316666\n",
      "Iteration 279, loss = 0.04257265\n",
      "Iteration 42, loss = 0.27878559\n",
      "Iteration 280, loss = 0.04153280\n",
      "Iteration 43, loss = 0.27677167\n",
      "Iteration 281, loss = 0.04297505\n",
      "Iteration 44, loss = 0.27224913\n",
      "Iteration 282, loss = 0.04003837\n",
      "Iteration 45, loss = 0.27027389\n",
      "Iteration 283, loss = 0.04386756\n",
      "Iteration 46, loss = 0.26704760\n",
      "Iteration 284, loss = 0.04463247\n",
      "Iteration 47, loss = 0.26395859\n",
      "Iteration 285, loss = 0.04243783\n",
      "Iteration 48, loss = 0.26093863\n",
      "Iteration 286, loss = 0.04448024\n",
      "Iteration 49, loss = 0.25794658\n",
      "Iteration 287, loss = 0.03934742\n",
      "Iteration 50, loss = 0.25645509\n",
      "Iteration 288, loss = 0.04154204\n",
      "Iteration 51, loss = 0.25263996\n",
      "Iteration 289, loss = 0.04152012\n",
      "Iteration 52, loss = 0.25020512\n",
      "Iteration 290, loss = 0.03997720\n",
      "Iteration 53, loss = 0.24756507\n",
      "Iteration 291, loss = 0.04266227\n",
      "Iteration 54, loss = 0.24520415\n",
      "Iteration 292, loss = 0.04038655\n",
      "Iteration 55, loss = 0.24294088\n",
      "Iteration 293, loss = 0.03980469\n",
      "Iteration 56, loss = 0.24038658\n",
      "Iteration 294, loss = 0.03924648\n",
      "Iteration 57, loss = 0.23821358\n",
      "Iteration 295, loss = 0.03957569\n",
      "Iteration 58, loss = 0.23634154\n",
      "Iteration 296, loss = 0.03750664\n",
      "Iteration 59, loss = 0.23589631\n",
      "Iteration 297, loss = 0.03858700\n",
      "Iteration 60, loss = 0.23312001\n",
      "Iteration 298, loss = 0.03750778\n",
      "Iteration 61, loss = 0.22951992\n",
      "Iteration 299, loss = 0.03687844\n",
      "Iteration 62, loss = 0.22733453\n",
      "Iteration 300, loss = 0.03562635\n",
      "Iteration 63, loss = 0.22534373\n",
      "Iteration 301, loss = 0.03579803\n",
      "Iteration 64, loss = 0.22377484\n",
      "Iteration 302, loss = 0.03601186\n",
      "Iteration 65, loss = 0.22095899\n",
      "Iteration 303, loss = 0.03721773\n",
      "Iteration 66, loss = 0.21889182\n",
      "Iteration 304, loss = 0.03659338\n",
      "Iteration 67, loss = 0.21661121\n",
      "Iteration 305, loss = 0.03572715\n",
      "Iteration 68, loss = 0.21470234\n",
      "Iteration 306, loss = 0.03600853\n",
      "Iteration 69, loss = 0.21248683\n",
      "Iteration 307, loss = 0.03508460\n",
      "Iteration 70, loss = 0.21070355\n",
      "Iteration 308, loss = 0.03792433\n",
      "Iteration 71, loss = 0.20973572\n",
      "Iteration 309, loss = 0.03811448\n",
      "Iteration 72, loss = 0.20722290\n",
      "Iteration 310, loss = 0.03801911\n",
      "Iteration 73, loss = 0.20534504\n",
      "Iteration 311, loss = 0.03973418\n",
      "Iteration 74, loss = 0.20291436\n",
      "Iteration 312, loss = 0.03558859\n",
      "Iteration 75, loss = 0.20148237\n",
      "Iteration 313, loss = 0.03526317\n",
      "Iteration 76, loss = 0.19951651\n",
      "Iteration 314, loss = 0.03471584\n",
      "Iteration 77, loss = 0.19725800\n",
      "Iteration 315, loss = 0.03350328\n",
      "Iteration 78, loss = 0.19692104\n",
      "Iteration 316, loss = 0.03555131\n",
      "Iteration 79, loss = 0.19359241\n",
      "Iteration 317, loss = 0.03308555\n",
      "Iteration 80, loss = 0.19484166\n",
      "Iteration 318, loss = 0.03157393\n",
      "Iteration 81, loss = 0.19038554\n",
      "Iteration 319, loss = 0.03193522\n",
      "Iteration 82, loss = 0.19022745\n",
      "Iteration 320, loss = 0.03289048\n",
      "Iteration 83, loss = 0.18742754\n",
      "Iteration 321, loss = 0.03196312\n",
      "Iteration 84, loss = 0.18687106\n",
      "Iteration 322, loss = 0.03111864\n",
      "Iteration 85, loss = 0.18393401\n",
      "Iteration 323, loss = 0.03113808\n",
      "Iteration 86, loss = 0.18262461\n",
      "Iteration 324, loss = 0.03073109\n",
      "Iteration 87, loss = 0.18088594\n",
      "Iteration 325, loss = 0.03011551\n",
      "Iteration 88, loss = 0.17909278\n",
      "Iteration 326, loss = 0.02989959\n",
      "Iteration 89, loss = 0.17953048\n",
      "Iteration 327, loss = 0.02979594\n",
      "Iteration 90, loss = 0.17557475\n",
      "Iteration 328, loss = 0.02968330\n",
      "Iteration 91, loss = 0.17458314\n",
      "Iteration 329, loss = 0.02984779\n",
      "Iteration 92, loss = 0.17289796\n",
      "Iteration 330, loss = 0.02952863\n",
      "Iteration 93, loss = 0.17009173\n",
      "Iteration 331, loss = 0.02959847\n",
      "Iteration 94, loss = 0.16974428\n",
      "Iteration 332, loss = 0.02916400\n",
      "Iteration 95, loss = 0.16933018\n",
      "Iteration 333, loss = 0.02899259\n",
      "Iteration 96, loss = 0.16706752\n",
      "Iteration 334, loss = 0.02852022\n",
      "Iteration 97, loss = 0.16506520\n",
      "Iteration 335, loss = 0.02869715\n",
      "Iteration 98, loss = 0.16381402\n",
      "Iteration 336, loss = 0.02815022\n",
      "Iteration 99, loss = 0.16369538\n",
      "Iteration 337, loss = 0.02764330\n",
      "Iteration 100, loss = 0.16045175\n",
      "Iteration 338, loss = 0.02963319\n",
      "Iteration 101, loss = 0.16159673\n",
      "Iteration 339, loss = 0.02969420\n",
      "Iteration 102, loss = 0.15911693\n",
      "Iteration 340, loss = 0.02743201\n",
      "Iteration 103, loss = 0.15713694\n",
      "Iteration 341, loss = 0.02780905\n",
      "Iteration 104, loss = 0.15637974\n",
      "Iteration 342, loss = 0.02773119\n",
      "Iteration 105, loss = 0.15389087\n",
      "Iteration 343, loss = 0.02688690\n",
      "Iteration 106, loss = 0.15417511\n",
      "Iteration 344, loss = 0.02695288\n",
      "Iteration 107, loss = 0.15117938\n",
      "Iteration 345, loss = 0.02726073\n",
      "Iteration 108, loss = 0.15031698\n",
      "Iteration 346, loss = 0.02669689\n",
      "Iteration 109, loss = 0.14781599\n",
      "Iteration 347, loss = 0.02724633\n",
      "Iteration 110, loss = 0.14832574\n",
      "Iteration 111, loss = 0.14684609\n",
      "Iteration 348, loss = 0.02654135\n",
      "Iteration 349, loss = 0.02626065\n",
      "Iteration 112, loss = 0.14477233\n",
      "Iteration 350, loss = 0.02648614\n",
      "Iteration 113, loss = 0.14528503\n",
      "Iteration 351, loss = 0.02577683\n",
      "Iteration 114, loss = 0.14304668\n",
      "Iteration 352, loss = 0.02526057\n",
      "Iteration 115, loss = 0.14200537\n",
      "Iteration 353, loss = 0.02628630\n",
      "Iteration 116, loss = 0.14047477\n",
      "Iteration 354, loss = 0.02522323\n",
      "Iteration 117, loss = 0.14027295\n",
      "Iteration 355, loss = 0.02610049\n",
      "Iteration 118, loss = 0.13796076\n",
      "Iteration 356, loss = 0.02463441\n",
      "Iteration 119, loss = 0.13719723\n",
      "Iteration 357, loss = 0.02492519\n",
      "Iteration 120, loss = 0.13597341\n",
      "Iteration 358, loss = 0.02557578\n",
      "Iteration 121, loss = 0.13583335\n",
      "Iteration 359, loss = 0.02500833\n",
      "Iteration 122, loss = 0.13390589\n",
      "Iteration 360, loss = 0.02385075\n",
      "Iteration 123, loss = 0.13401592\n",
      "Iteration 361, loss = 0.02379988\n",
      "Iteration 124, loss = 0.13494239\n",
      "Iteration 362, loss = 0.02511368\n",
      "Iteration 125, loss = 0.13062117\n",
      "Iteration 363, loss = 0.02393012\n",
      "Iteration 126, loss = 0.13169638\n",
      "Iteration 364, loss = 0.02559065\n",
      "Iteration 127, loss = 0.13054861\n",
      "Iteration 365, loss = 0.02489498\n",
      "Iteration 128, loss = 0.13132966\n",
      "Iteration 366, loss = 0.02386473\n",
      "Iteration 129, loss = 0.12900722\n",
      "Iteration 367, loss = 0.02341306\n",
      "Iteration 130, loss = 0.12788558\n",
      "Iteration 368, loss = 0.02315438\n",
      "Iteration 131, loss = 0.12473406\n",
      "Iteration 369, loss = 0.02324251\n",
      "Iteration 132, loss = 0.12530871\n",
      "Iteration 370, loss = 0.02347381\n",
      "Iteration 133, loss = 0.12368283\n",
      "Iteration 371, loss = 0.02389277\n",
      "Iteration 134, loss = 0.12218030\n",
      "Iteration 372, loss = 0.02289059\n",
      "Iteration 135, loss = 0.12183686\n",
      "Iteration 373, loss = 0.02227519\n",
      "Iteration 136, loss = 0.12180845\n",
      "Iteration 374, loss = 0.02204463\n",
      "Iteration 137, loss = 0.12045443\n",
      "Iteration 375, loss = 0.02246148\n",
      "Iteration 138, loss = 0.12017082\n",
      "Iteration 376, loss = 0.02215087\n",
      "Iteration 139, loss = 0.11920393\n",
      "Iteration 377, loss = 0.02272258\n",
      "Iteration 140, loss = 0.11757371\n",
      "Iteration 378, loss = 0.02300983\n",
      "Iteration 141, loss = 0.11663416\n",
      "Iteration 379, loss = 0.02187787\n",
      "Iteration 142, loss = 0.11596544\n",
      "Iteration 380, loss = 0.02162613\n",
      "Iteration 143, loss = 0.11445723\n",
      "Iteration 381, loss = 0.02142039\n",
      "Iteration 144, loss = 0.11407149\n",
      "Iteration 382, loss = 0.02134774\n",
      "Iteration 145, loss = 0.11529718\n",
      "Iteration 383, loss = 0.02105603\n",
      "Iteration 146, loss = 0.11180526\n",
      "Iteration 384, loss = 0.02127790\n",
      "Iteration 147, loss = 0.11336741\n",
      "Iteration 385, loss = 0.02090848\n",
      "Iteration 148, loss = 0.11108709\n",
      "Iteration 386, loss = 0.02103124\n",
      "Iteration 149, loss = 0.11067263\n",
      "Iteration 387, loss = 0.02124092\n",
      "Iteration 150, loss = 0.11078026\n",
      "Iteration 388, loss = 0.02167675\n",
      "Iteration 151, loss = 0.10811084\n",
      "Iteration 389, loss = 0.02054928\n",
      "Iteration 152, loss = 0.10884286\n",
      "Iteration 390, loss = 0.02104533\n",
      "Iteration 153, loss = 0.10869385\n",
      "Iteration 391, loss = 0.02001340\n",
      "Iteration 154, loss = 0.10623528\n",
      "Iteration 392, loss = 0.02042955\n",
      "Iteration 155, loss = 0.10567895\n",
      "Iteration 393, loss = 0.02017521\n",
      "Iteration 156, loss = 0.10549876\n",
      "Iteration 394, loss = 0.01922870\n",
      "Iteration 157, loss = 0.10374647\n",
      "Iteration 395, loss = 0.02069208\n",
      "Iteration 158, loss = 0.10355686\n",
      "Iteration 396, loss = 0.02022282\n",
      "Iteration 159, loss = 0.10260755\n",
      "Iteration 397, loss = 0.01952569\n",
      "Iteration 160, loss = 0.10157813\n",
      "Iteration 398, loss = 0.02033418\n",
      "Iteration 161, loss = 0.10116920\n",
      "Iteration 399, loss = 0.01905884\n",
      "Iteration 162, loss = 0.10093420\n",
      "Iteration 400, loss = 0.01909914\n",
      "Iteration 163, loss = 0.10034519\n",
      "Iteration 401, loss = 0.01900902\n",
      "Iteration 164, loss = 0.09961264\n",
      "Iteration 402, loss = 0.01828147\n",
      "Iteration 165, loss = 0.09945841\n",
      "Iteration 403, loss = 0.01904158\n",
      "Iteration 166, loss = 0.09903675\n",
      "Iteration 404, loss = 0.01907479\n",
      "Iteration 167, loss = 0.09703123\n",
      "Iteration 405, loss = 0.02014760\n",
      "Iteration 168, loss = 0.09908932\n",
      "Iteration 406, loss = 0.01931184\n",
      "Iteration 169, loss = 0.09557735\n",
      "Iteration 170, loss = 0.09455242\n",
      "Iteration 407, loss = 0.01822918\n",
      "Iteration 171, loss = 0.09481280\n",
      "Iteration 408, loss = 0.01918804\n",
      "Iteration 172, loss = 0.09611348\n",
      "Iteration 409, loss = 0.01873869\n",
      "Iteration 173, loss = 0.09376391\n",
      "Iteration 410, loss = 0.01891331\n",
      "Iteration 174, loss = 0.09222579\n",
      "Iteration 411, loss = 0.01885803\n",
      "Iteration 175, loss = 0.09276905\n",
      "Iteration 412, loss = 0.01882688\n",
      "Iteration 176, loss = 0.09210941\n",
      "Iteration 413, loss = 0.01896697\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 177, loss = 0.09011270\n",
      "Iteration 178, loss = 0.09373635\n",
      "Iteration 179, loss = 0.09044041\n",
      "Iteration 180, loss = 0.08930833\n",
      "Iteration 181, loss = 0.08858139\n",
      "Iteration 182, loss = 0.08831255\n",
      "Iteration 183, loss = 0.08701560\n",
      "Iteration 184, loss = 0.08575467\n",
      "Iteration 185, loss = 0.08772070\n",
      "Iteration 186, loss = 0.08588983\n",
      "Iteration 187, loss = 0.08607661\n",
      "Iteration 188, loss = 0.08559895\n",
      "Iteration 189, loss = 0.08554002\n",
      "Iteration 190, loss = 0.08319095\n",
      "Iteration 191, loss = 0.08503596\n",
      "Iteration 192, loss = 0.08532435\n",
      "Iteration 193, loss = 0.08217054\n",
      "Iteration 194, loss = 0.08229771\n",
      "Iteration 1, loss = 17.90506076\n",
      "Iteration 195, loss = 0.08096113\n",
      "Iteration 2, loss = 15.26308910\n",
      "Iteration 196, loss = 0.08055382\n",
      "Iteration 3, loss = 9.27573394\n",
      "Iteration 197, loss = 0.08027274\n",
      "Iteration 4, loss = 9.56615116\n",
      "Iteration 198, loss = 0.07877125\n",
      "Iteration 5, loss = 7.69409178\n",
      "Iteration 199, loss = 0.08171114\n",
      "Iteration 6, loss = 7.91422233\n",
      "Iteration 200, loss = 0.07928788\n",
      "Iteration 7, loss = 8.60628132\n",
      "Iteration 201, loss = 0.07876903\n",
      "Iteration 8, loss = 7.10830333\n",
      "Iteration 202, loss = 0.07862497\n",
      "Iteration 9, loss = 6.12839765\n",
      "Iteration 203, loss = 0.07612601\n",
      "Iteration 10, loss = 6.27489900\n",
      "Iteration 204, loss = 0.07661602\n",
      "Iteration 11, loss = 7.55602143\n",
      "Iteration 205, loss = 0.07646583\n",
      "Iteration 12, loss = 4.62243287\n",
      "Iteration 206, loss = 0.07697916\n",
      "Iteration 13, loss = 3.92123674\n",
      "Iteration 207, loss = 0.07535093\n",
      "Iteration 14, loss = 3.81042848\n",
      "Iteration 208, loss = 0.07423781\n",
      "Iteration 15, loss = 3.87827739\n",
      "Iteration 209, loss = 0.07483736\n",
      "Iteration 16, loss = 5.45271632\n",
      "Iteration 210, loss = 0.07312990\n",
      "Iteration 17, loss = 4.39218505\n",
      "Iteration 211, loss = 0.07296457\n",
      "Iteration 18, loss = 4.06566146\n",
      "Iteration 212, loss = 0.07451160\n",
      "Iteration 19, loss = 5.86653246\n",
      "Iteration 213, loss = 0.07311787\n",
      "Iteration 20, loss = 5.39188258\n",
      "Iteration 214, loss = 0.07420230\n",
      "Iteration 21, loss = 4.66864067\n",
      "Iteration 215, loss = 0.07178727\n",
      "Iteration 22, loss = 3.72803830\n",
      "Iteration 216, loss = 0.07161113\n",
      "Iteration 23, loss = 2.61008062\n",
      "Iteration 217, loss = 0.06992759\n",
      "Iteration 24, loss = 2.86034382\n",
      "Iteration 218, loss = 0.06956417\n",
      "Iteration 25, loss = 2.99077009\n",
      "Iteration 219, loss = 0.06995112\n",
      "Iteration 26, loss = 2.79475846\n",
      "Iteration 220, loss = 0.06874131\n",
      "Iteration 27, loss = 2.39648051\n",
      "Iteration 221, loss = 0.06909589\n",
      "Iteration 28, loss = 1.83327905\n",
      "Iteration 222, loss = 0.06802443\n",
      "Iteration 29, loss = 1.63118757\n",
      "Iteration 223, loss = 0.06748781\n",
      "Iteration 30, loss = 1.63582592\n",
      "Iteration 224, loss = 0.06926249\n",
      "Iteration 31, loss = 1.98848376\n",
      "Iteration 225, loss = 0.06595325\n",
      "Iteration 32, loss = 1.53847350\n",
      "Iteration 226, loss = 0.06700070\n",
      "Iteration 33, loss = 1.17851917\n",
      "Iteration 227, loss = 0.06403003\n",
      "Iteration 34, loss = 1.39535601\n",
      "Iteration 228, loss = 0.06849346\n",
      "Iteration 35, loss = 0.95878171\n",
      "Iteration 229, loss = 0.06565579\n",
      "Iteration 36, loss = 0.68229079\n",
      "Iteration 230, loss = 0.06718342\n",
      "Iteration 37, loss = 0.77636032\n",
      "Iteration 231, loss = 0.06378052\n",
      "Iteration 38, loss = 1.43251061\n",
      "Iteration 232, loss = 0.06363211\n",
      "Iteration 39, loss = 1.50791787\n",
      "Iteration 233, loss = 0.06173881\n",
      "Iteration 40, loss = 1.37520751\n",
      "Iteration 234, loss = 0.06310124Iteration 41, loss = 1.98004768\n",
      "\n",
      "Iteration 42, loss = 1.87948627\n",
      "Iteration 235, loss = 0.06164479\n",
      "Iteration 43, loss = 1.31869236\n",
      "Iteration 236, loss = 0.06310430\n",
      "Iteration 44, loss = 1.51460673\n",
      "Iteration 237, loss = 0.06192745\n",
      "Iteration 45, loss = 0.81386686\n",
      "Iteration 238, loss = 0.06036399\n",
      "Iteration 46, loss = 0.90681772\n",
      "Iteration 239, loss = 0.06298193\n",
      "Iteration 47, loss = 1.01108546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 240, loss = 0.06043914\n",
      "Iteration 241, loss = 0.06026854\n",
      "Iteration 242, loss = 0.05974058\n",
      "Iteration 243, loss = 0.05931122\n",
      "Iteration 244, loss = 0.05834876\n",
      "Iteration 245, loss = 0.06106955\n",
      "Iteration 246, loss = 0.05774892\n",
      "Iteration 247, loss = 0.05707452\n",
      "Iteration 248, loss = 0.06053687\n",
      "Iteration 249, loss = 0.05772812\n",
      "Iteration 250, loss = 0.05643801\n",
      "Iteration 251, loss = 0.05698835\n",
      "Iteration 252, loss = 0.05522929\n",
      "Iteration 253, loss = 0.05515517\n",
      "Iteration 254, loss = 0.05676135\n",
      "Iteration 255, loss = 0.05347200\n",
      "Iteration 256, loss = 0.05355028\n",
      "Iteration 257, loss = 0.05382175\n",
      "Iteration 258, loss = 0.05228059\n",
      "Iteration 1, loss = 17.36259889\n",
      "Iteration 2, loss = 18.93816115\n",
      "Iteration 259, loss = 0.05220669\n",
      "Iteration 3, loss = 13.35292438\n",
      "Iteration 260, loss = 0.05334756\n",
      "Iteration 4, loss = 10.61674765\n",
      "Iteration 261, loss = 0.05250343\n",
      "Iteration 262, loss = 0.05082470\n",
      "Iteration 5, loss = 9.87864942\n",
      "Iteration 6, loss = 12.06601650\n",
      "Iteration 263, loss = 0.05134509\n",
      "Iteration 264, loss = 0.05021246\n",
      "Iteration 7, loss = 8.27906889\n",
      "Iteration 265, loss = 0.05023985\n",
      "Iteration 8, loss = 7.46909730\n",
      "Iteration 266, loss = 0.05046969\n",
      "Iteration 9, loss = 7.64980152\n",
      "Iteration 10, loss = 7.83879938\n",
      "Iteration 267, loss = 0.05102767\n",
      "Iteration 268, loss = 0.05017280\n",
      "Iteration 11, loss = 7.81733022\n",
      "Iteration 12, loss = 6.64378846\n",
      "Iteration 269, loss = 0.04877425\n",
      "Iteration 13, loss = 4.94269431\n",
      "Iteration 270, loss = 0.04877384\n",
      "Iteration 14, loss = 5.99650912\n",
      "Iteration 271, loss = 0.04798704\n",
      "Iteration 15, loss = 6.29756193\n",
      "Iteration 272, loss = 0.04746311\n",
      "Iteration 16, loss = 4.26089284\n",
      "Iteration 273, loss = 0.04844050\n",
      "Iteration 17, loss = 3.70141110\n",
      "Iteration 274, loss = 0.04846961\n",
      "Iteration 18, loss = 3.80448125\n",
      "Iteration 275, loss = 0.05011037\n",
      "Iteration 19, loss = 3.60290707\n",
      "Iteration 276, loss = 0.04945557\n",
      "Iteration 20, loss = 2.82028201\n",
      "Iteration 277, loss = 0.05400262\n",
      "Iteration 21, loss = 2.95826690\n",
      "Iteration 278, loss = 0.04753113\n",
      "Iteration 22, loss = 2.30380526\n",
      "Iteration 279, loss = 0.04784899\n",
      "Iteration 23, loss = 2.31555004\n",
      "Iteration 280, loss = 0.04630251\n",
      "Iteration 24, loss = 1.94615714\n",
      "Iteration 281, loss = 0.04522405\n",
      "Iteration 25, loss = 1.65290637\n",
      "Iteration 282, loss = 0.04541833\n",
      "Iteration 26, loss = 1.72963130\n",
      "Iteration 283, loss = 0.04458965\n",
      "Iteration 27, loss = 1.28075064\n",
      "Iteration 284, loss = 0.04425464\n",
      "Iteration 28, loss = 1.51077211\n",
      "Iteration 285, loss = 0.04299898\n",
      "Iteration 29, loss = 1.54804316\n",
      "Iteration 286, loss = 0.04475820\n",
      "Iteration 30, loss = 1.67329741\n",
      "Iteration 287, loss = 0.04321088\n",
      "Iteration 31, loss = 1.85267845\n",
      "Iteration 288, loss = 0.04454424\n",
      "Iteration 32, loss = 1.57892320\n",
      "Iteration 289, loss = 0.04218618\n",
      "Iteration 33, loss = 1.13301358\n",
      "Iteration 290, loss = 0.04294759\n",
      "Iteration 34, loss = 1.28401865\n",
      "Iteration 291, loss = 0.04167612\n",
      "Iteration 35, loss = 1.18725494\n",
      "Iteration 292, loss = 0.04135775\n",
      "Iteration 36, loss = 0.99413734\n",
      "Iteration 293, loss = 0.04163862\n",
      "Iteration 37, loss = 1.01753897\n",
      "Iteration 294, loss = 0.04137694\n",
      "Iteration 38, loss = 1.08397013\n",
      "Iteration 295, loss = 0.04064997\n",
      "Iteration 39, loss = 1.32263839\n",
      "Iteration 296, loss = 0.04053747\n",
      "Iteration 40, loss = 1.19358141\n",
      "Iteration 297, loss = 0.03972539\n",
      "Iteration 41, loss = 0.87354308\n",
      "Iteration 298, loss = 0.04178898\n",
      "Iteration 42, loss = 1.02657337\n",
      "Iteration 299, loss = 0.04076989\n",
      "Iteration 43, loss = 0.83363272\n",
      "Iteration 300, loss = 0.03970575\n",
      "Iteration 44, loss = 0.61441679\n",
      "Iteration 301, loss = 0.04088738\n",
      "Iteration 45, loss = 0.71542503\n",
      "Iteration 302, loss = 0.03900013\n",
      "Iteration 46, loss = 0.73170652\n",
      "Iteration 303, loss = 0.03983472\n",
      "Iteration 47, loss = 0.64672040\n",
      "Iteration 304, loss = 0.03812531\n",
      "Iteration 48, loss = 0.59005395\n",
      "Iteration 305, loss = 0.03715018\n",
      "Iteration 49, loss = 0.45929817\n",
      "Iteration 306, loss = 0.03869043\n",
      "Iteration 50, loss = 0.48512892\n",
      "Iteration 307, loss = 0.03842034\n",
      "Iteration 51, loss = 0.54782207\n",
      "Iteration 308, loss = 0.03793205\n",
      "Iteration 52, loss = 0.47774360\n",
      "Iteration 309, loss = 0.03706400\n",
      "Iteration 53, loss = 0.44160550\n",
      "Iteration 310, loss = 0.03651617\n",
      "Iteration 54, loss = 0.43031113\n",
      "Iteration 311, loss = 0.03658243\n",
      "Iteration 55, loss = 0.39208594\n",
      "Iteration 312, loss = 0.03613937\n",
      "Iteration 56, loss = 0.38092564\n",
      "Iteration 313, loss = 0.03572274\n",
      "Iteration 57, loss = 0.43635200\n",
      "Iteration 314, loss = 0.03518366\n",
      "Iteration 58, loss = 0.31704716\n",
      "Iteration 315, loss = 0.03541598\n",
      "Iteration 59, loss = 0.63439516\n",
      "Iteration 316, loss = 0.03482881\n",
      "Iteration 60, loss = 0.69042513\n",
      "Iteration 317, loss = 0.03660392\n",
      "Iteration 61, loss = 0.62017473\n",
      "Iteration 318, loss = 0.03501922\n",
      "Iteration 62, loss = 0.66989760\n",
      "Iteration 319, loss = 0.03486685\n",
      "Iteration 63, loss = 0.51993385\n",
      "Iteration 320, loss = 0.03330124\n",
      "Iteration 64, loss = 0.45808140\n",
      "Iteration 321, loss = 0.03557320\n",
      "Iteration 65, loss = 0.47841537\n",
      "Iteration 322, loss = 0.03476390\n",
      "Iteration 66, loss = 0.44090452\n",
      "Iteration 323, loss = 0.03461179\n",
      "Iteration 67, loss = 0.40706117\n",
      "Iteration 324, loss = 0.03332915\n",
      "Iteration 68, loss = 0.50244906\n",
      "Iteration 325, loss = 0.03481862\n",
      "Iteration 69, loss = 0.47676784\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 326, loss = 0.03198810\n",
      "Iteration 327, loss = 0.03322130\n",
      "Iteration 328, loss = 0.03252851\n",
      "Iteration 329, loss = 0.03148105\n",
      "Iteration 330, loss = 0.03134101\n",
      "Iteration 331, loss = 0.03127522\n",
      "Iteration 332, loss = 0.03137492\n",
      "Iteration 333, loss = 0.03083876\n",
      "Iteration 334, loss = 0.03074462\n",
      "Iteration 335, loss = 0.03045485\n",
      "Iteration 336, loss = 0.03109446\n",
      "Iteration 337, loss = 0.03042974\n",
      "Iteration 338, loss = 0.03012082\n",
      "Iteration 339, loss = 0.03010010\n",
      "Iteration 340, loss = 0.02950184\n",
      "Iteration 341, loss = 0.02929522\n",
      "Iteration 342, loss = 0.02952847\n",
      "Iteration 343, loss = 0.02971347\n",
      "Iteration 1, loss = 16.97965031\n",
      "Iteration 344, loss = 0.02999557\n",
      "Iteration 2, loss = 14.67845663\n",
      "Iteration 345, loss = 0.02966092\n",
      "Iteration 3, loss = 13.88684698\n",
      "Iteration 346, loss = 0.03010976\n",
      "Iteration 4, loss = 10.17532853\n",
      "Iteration 347, loss = 0.02905415\n",
      "Iteration 5, loss = 6.49062070\n",
      "Iteration 348, loss = 0.02763173\n",
      "Iteration 6, loss = 6.40484252\n",
      "Iteration 349, loss = 0.02865197\n",
      "Iteration 7, loss = 6.98423432\n",
      "Iteration 350, loss = 0.02761831\n",
      "Iteration 8, loss = 6.86041757\n",
      "Iteration 351, loss = 0.02733734\n",
      "Iteration 9, loss = 5.80654536\n",
      "Iteration 352, loss = 0.02703109\n",
      "Iteration 10, loss = 6.07202367\n",
      "Iteration 353, loss = 0.02725123\n",
      "Iteration 11, loss = 6.57293672\n",
      "Iteration 354, loss = 0.02651838\n",
      "Iteration 12, loss = 4.65617996\n",
      "Iteration 355, loss = 0.02633567\n",
      "Iteration 13, loss = 4.95350647\n",
      "Iteration 356, loss = 0.02643287\n",
      "Iteration 14, loss = 6.05092736\n",
      "Iteration 357, loss = 0.02674793\n",
      "Iteration 15, loss = 4.58474851\n",
      "Iteration 358, loss = 0.02675856\n",
      "Iteration 16, loss = 4.16901022\n",
      "Iteration 359, loss = 0.02582431\n",
      "Iteration 17, loss = 4.46196390\n",
      "Iteration 360, loss = 0.02657216\n",
      "Iteration 18, loss = 5.08467625\n",
      "Iteration 361, loss = 0.02624479\n",
      "Iteration 19, loss = 3.22268102\n",
      "Iteration 362, loss = 0.02605410\n",
      "Iteration 20, loss = 4.37279705\n",
      "Iteration 363, loss = 0.02581751\n",
      "Iteration 21, loss = 4.31326318\n",
      "Iteration 364, loss = 0.02473564\n",
      "Iteration 22, loss = 3.52489797\n",
      "Iteration 365, loss = 0.02538415\n",
      "Iteration 23, loss = 4.08102457\n",
      "Iteration 366, loss = 0.02476876\n",
      "Iteration 24, loss = 3.06515457\n",
      "Iteration 367, loss = 0.02475215\n",
      "Iteration 25, loss = 3.31536923\n",
      "Iteration 368, loss = 0.02599949\n",
      "Iteration 26, loss = 2.41797174\n",
      "Iteration 369, loss = 0.02516487\n",
      "Iteration 27, loss = 2.18669283\n",
      "Iteration 370, loss = 0.02486156\n",
      "Iteration 28, loss = 2.51161045\n",
      "Iteration 371, loss = 0.02393976\n",
      "Iteration 29, loss = 2.64750833\n",
      "Iteration 372, loss = 0.02429122\n",
      "Iteration 30, loss = 2.04717961\n",
      "Iteration 373, loss = 0.02371336\n",
      "Iteration 31, loss = 1.97436873\n",
      "Iteration 374, loss = 0.02304541\n",
      "Iteration 32, loss = 2.06624178\n",
      "Iteration 375, loss = 0.02370908\n",
      "Iteration 33, loss = 2.27766681\n",
      "Iteration 34, loss = 2.69301730\n",
      "Iteration 376, loss = 0.02313949\n",
      "Iteration 35, loss = 2.51668083\n",
      "Iteration 377, loss = 0.02263710\n",
      "Iteration 36, loss = 2.14770821\n",
      "Iteration 378, loss = 0.02458919\n",
      "Iteration 37, loss = 2.30749448\n",
      "Iteration 379, loss = 0.02262967\n",
      "Iteration 38, loss = 2.74291914\n",
      "Iteration 380, loss = 0.02302707\n",
      "Iteration 39, loss = 2.51484959\n",
      "Iteration 381, loss = 0.02245180\n",
      "Iteration 40, loss = 2.51572718\n",
      "Iteration 382, loss = 0.02287373\n",
      "Iteration 41, loss = 2.98589974\n",
      "Iteration 383, loss = 0.02220231\n",
      "Iteration 42, loss = 2.63879707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 384, loss = 0.02156879\n",
      "Iteration 385, loss = 0.02154552\n",
      "Iteration 386, loss = 0.02136896\n",
      "Iteration 387, loss = 0.02176207\n",
      "Iteration 388, loss = 0.02092676\n",
      "Iteration 389, loss = 0.02128068\n",
      "Iteration 390, loss = 0.02094506\n",
      "Iteration 391, loss = 0.02109188\n",
      "Iteration 392, loss = 0.02081564\n",
      "Iteration 393, loss = 0.02073081\n",
      "Iteration 394, loss = 0.02159392\n",
      "Iteration 395, loss = 0.02137542\n",
      "Iteration 396, loss = 0.02006923\n",
      "Iteration 397, loss = 0.02061455\n",
      "Iteration 398, loss = 0.01986528\n",
      "Iteration 399, loss = 0.01954603\n",
      "Iteration 400, loss = 0.01959344\n",
      "Iteration 401, loss = 0.01910553\n",
      "Iteration 402, loss = 0.01927557\n",
      "Iteration 1, loss = 19.41599223\n",
      "Iteration 403, loss = 0.02001319\n",
      "Iteration 2, loss = 15.43636418\n",
      "Iteration 3, loss = 12.38584538\n",
      "Iteration 404, loss = 0.01868687\n",
      "Iteration 4, loss = 9.20737987\n",
      "Iteration 405, loss = 0.01902270\n",
      "Iteration 5, loss = 7.40196696\n",
      "Iteration 406, loss = 0.01862217\n",
      "Iteration 6, loss = 6.09782271\n",
      "Iteration 407, loss = 0.01862054\n",
      "Iteration 7, loss = 11.51009546\n",
      "Iteration 408, loss = 0.01852071\n",
      "Iteration 8, loss = 11.64749932\n",
      "Iteration 409, loss = 0.01846220\n",
      "Iteration 9, loss = 9.73329249\n",
      "Iteration 410, loss = 0.01851455\n",
      "Iteration 10, loss = 10.43461499\n",
      "Iteration 411, loss = 0.01828430\n",
      "Iteration 11, loss = 7.97665161\n",
      "Iteration 412, loss = 0.01771177\n",
      "Iteration 12, loss = 6.47751459\n",
      "Iteration 413, loss = 0.01826064\n",
      "Iteration 13, loss = 8.49430178\n",
      "Iteration 414, loss = 0.01803206\n",
      "Iteration 14, loss = 6.08783256\n",
      "Iteration 415, loss = 0.01746855\n",
      "Iteration 15, loss = 6.06221871\n",
      "Iteration 416, loss = 0.01722012\n",
      "Iteration 16, loss = 6.28874273\n",
      "Iteration 417, loss = 0.01727052\n",
      "Iteration 17, loss = 4.38869965\n",
      "Iteration 418, loss = 0.01720506\n",
      "Iteration 18, loss = 5.26162973\n",
      "Iteration 419, loss = 0.01752405\n",
      "Iteration 19, loss = 3.65743357\n",
      "Iteration 420, loss = 0.01871570\n",
      "Iteration 20, loss = 3.69800744\n",
      "Iteration 421, loss = 0.01751275\n",
      "Iteration 21, loss = 3.18634769\n",
      "Iteration 422, loss = 0.01648281\n",
      "Iteration 22, loss = 3.27091087\n",
      "Iteration 423, loss = 0.01683452\n",
      "Iteration 23, loss = 2.91135250\n",
      "Iteration 424, loss = 0.01634856\n",
      "Iteration 24, loss = 2.88687644\n",
      "Iteration 425, loss = 0.01651260\n",
      "Iteration 25, loss = 3.06881313\n",
      "Iteration 426, loss = 0.01639228\n",
      "Iteration 26, loss = 2.32784236\n",
      "Iteration 427, loss = 0.01595580\n",
      "Iteration 27, loss = 3.38209990\n",
      "Iteration 428, loss = 0.01662138\n",
      "Iteration 28, loss = 2.82033909\n",
      "Iteration 429, loss = 0.01672209\n",
      "Iteration 29, loss = 2.75613979\n",
      "Iteration 430, loss = 0.01593810\n",
      "Iteration 30, loss = 2.15427206\n",
      "Iteration 431, loss = 0.01604208\n",
      "Iteration 31, loss = 2.95786303\n",
      "Iteration 432, loss = 0.01627986\n",
      "Iteration 32, loss = 3.61500840\n",
      "Iteration 433, loss = 0.01542011\n",
      "Iteration 33, loss = 2.40063821\n",
      "Iteration 434, loss = 0.01649741\n",
      "Iteration 34, loss = 3.13398800\n",
      "Iteration 435, loss = 0.01707609\n",
      "Iteration 35, loss = 2.60084089\n",
      "Iteration 436, loss = 0.01587686\n",
      "Iteration 36, loss = 2.25491278\n",
      "Iteration 437, loss = 0.01566626\n",
      "Iteration 37, loss = 1.44723212\n",
      "Iteration 438, loss = 0.01511498\n",
      "Iteration 38, loss = 1.69158089\n",
      "Iteration 439, loss = 0.01508200\n",
      "Iteration 39, loss = 1.45719568\n",
      "Iteration 440, loss = 0.01524611\n",
      "Iteration 40, loss = 1.38990709\n",
      "Iteration 441, loss = 0.01537882\n",
      "Iteration 41, loss = 1.08113116\n",
      "Iteration 442, loss = 0.01466413\n",
      "Iteration 42, loss = 1.04369127\n",
      "Iteration 443, loss = 0.01502707\n",
      "Iteration 43, loss = 1.16866370\n",
      "Iteration 444, loss = 0.01449047\n",
      "Iteration 44, loss = 1.19556395\n",
      "Iteration 445, loss = 0.01418436\n",
      "Iteration 45, loss = 1.19515729\n",
      "Iteration 446, loss = 0.01440885\n",
      "Iteration 46, loss = 1.14764737\n",
      "Iteration 447, loss = 0.01455735\n",
      "Iteration 47, loss = 1.23426326\n",
      "Iteration 448, loss = 0.01437212\n",
      "Iteration 48, loss = 0.79114246\n",
      "Iteration 449, loss = 0.01405899\n",
      "Iteration 49, loss = 1.13326851\n",
      "Iteration 450, loss = 0.01437286\n",
      "Iteration 50, loss = 1.51027101\n",
      "Iteration 451, loss = 0.01423016\n",
      "Iteration 51, loss = 0.90765768\n",
      "Iteration 452, loss = 0.01375205\n",
      "Iteration 52, loss = 0.97035667\n",
      "Iteration 453, loss = 0.01403582\n",
      "Iteration 53, loss = 0.78550983\n",
      "Iteration 454, loss = 0.01340055\n",
      "Iteration 54, loss = 0.61163769\n",
      "Iteration 455, loss = 0.01379575\n",
      "Iteration 55, loss = 0.66780623\n",
      "Iteration 456, loss = 0.01354881\n",
      "Iteration 56, loss = 0.62558808\n",
      "Iteration 457, loss = 0.01393718\n",
      "Iteration 57, loss = 1.11915313\n",
      "Iteration 458, loss = 0.01306794\n",
      "Iteration 58, loss = 1.17745377\n",
      "Iteration 459, loss = 0.01349432\n",
      "Iteration 59, loss = 0.93533528\n",
      "Iteration 460, loss = 0.01330194\n",
      "Iteration 60, loss = 1.08098182\n",
      "Iteration 461, loss = 0.01314669\n",
      "Iteration 61, loss = 1.75417571\n",
      "Iteration 462, loss = 0.01316180\n",
      "Iteration 62, loss = 2.02494957\n",
      "Iteration 463, loss = 0.01292418\n",
      "Iteration 63, loss = 1.59955664\n",
      "Iteration 464, loss = 0.01335505\n",
      "Iteration 64, loss = 1.65968857\n",
      "Iteration 465, loss = 0.01248797\n",
      "Iteration 65, loss = 1.97029303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 466, loss = 0.01274942\n",
      "Iteration 467, loss = 0.01313884\n",
      "Iteration 468, loss = 0.01344774\n",
      "Iteration 469, loss = 0.01315660\n",
      "Iteration 470, loss = 0.01244794\n",
      "Iteration 471, loss = 0.01298675\n",
      "Iteration 472, loss = 0.01205170\n",
      "Iteration 473, loss = 0.01205556\n",
      "Iteration 474, loss = 0.01202485\n",
      "Iteration 475, loss = 0.01183795\n",
      "Iteration 476, loss = 0.01243246\n",
      "Iteration 477, loss = 0.01278006\n",
      "Iteration 478, loss = 0.01127787\n",
      "Iteration 479, loss = 0.01262573\n",
      "Iteration 480, loss = 0.01153283\n",
      "Iteration 481, loss = 0.01167471\n",
      "Iteration 482, loss = 0.01151268\n",
      "Iteration 483, loss = 0.01109882\n",
      "Iteration 484, loss = 0.01142471\n",
      "Iteration 1, loss = 16.63071867\n",
      "Iteration 485, loss = 0.01156570\n",
      "Iteration 2, loss = 15.16360135\n",
      "Iteration 3, loss = 11.22317206\n",
      "Iteration 486, loss = 0.01209789\n",
      "Iteration 4, loss = 10.26100294\n",
      "Iteration 487, loss = 0.01128111\n",
      "Iteration 5, loss = 11.09981507\n",
      "Iteration 488, loss = 0.01170601\n",
      "Iteration 6, loss = 10.34878975\n",
      "Iteration 489, loss = 0.01069349\n",
      "Iteration 7, loss = 11.36540948\n",
      "Iteration 490, loss = 0.01135512\n",
      "Iteration 8, loss = 8.74426603\n",
      "Iteration 491, loss = 0.01059296\n",
      "Iteration 9, loss = 5.96856390\n",
      "Iteration 492, loss = 0.01066990\n",
      "Iteration 10, loss = 5.87902260\n",
      "Iteration 493, loss = 0.01051096\n",
      "Iteration 11, loss = 4.70148736\n",
      "Iteration 494, loss = 0.01038984\n",
      "Iteration 12, loss = 4.31869236\n",
      "Iteration 495, loss = 0.01052841\n",
      "Iteration 13, loss = 4.52903164\n",
      "Iteration 496, loss = 0.01027170\n",
      "Iteration 14, loss = 4.52109791\n",
      "Iteration 497, loss = 0.01024380\n",
      "Iteration 15, loss = 4.64721239\n",
      "Iteration 498, loss = 0.01012822\n",
      "Iteration 16, loss = 8.26405037\n",
      "Iteration 499, loss = 0.01007258\n",
      "Iteration 17, loss = 5.49348322\n",
      "Iteration 500, loss = 0.01028097\n",
      "Iteration 18, loss = 5.22303072\n",
      "Iteration 501, loss = 0.00990993\n",
      "Iteration 19, loss = 3.93834020\n",
      "Iteration 502, loss = 0.01012740\n",
      "Iteration 20, loss = 3.89579526\n",
      "Iteration 503, loss = 0.01029989\n",
      "Iteration 21, loss = 4.14365969\n",
      "Iteration 504, loss = 0.01012745\n",
      "Iteration 22, loss = 3.71024900\n",
      "Iteration 505, loss = 0.01026534\n",
      "Iteration 23, loss = 3.15844247\n",
      "Iteration 506, loss = 0.00949828\n",
      "Iteration 24, loss = 3.11403066\n",
      "Iteration 507, loss = 0.00980346\n",
      "Iteration 25, loss = 2.53706584\n",
      "Iteration 508, loss = 0.01029087\n",
      "Iteration 26, loss = 1.98270344\n",
      "Iteration 509, loss = 0.01021155\n",
      "Iteration 27, loss = 1.88110433\n",
      "Iteration 510, loss = 0.00936387\n",
      "Iteration 28, loss = 1.70420410\n",
      "Iteration 511, loss = 0.00985072\n",
      "Iteration 29, loss = 1.97411566\n",
      "Iteration 512, loss = 0.00940073\n",
      "Iteration 30, loss = 2.19624182\n",
      "Iteration 513, loss = 0.00941010\n",
      "Iteration 31, loss = 2.05477541\n",
      "Iteration 514, loss = 0.00951425\n",
      "Iteration 32, loss = 1.39480454\n",
      "Iteration 515, loss = 0.00927250\n",
      "Iteration 33, loss = 1.22577154\n",
      "Iteration 516, loss = 0.00934761\n",
      "Iteration 34, loss = 1.03686406\n",
      "Iteration 517, loss = 0.00909452\n",
      "Iteration 35, loss = 1.09022018\n",
      "Iteration 518, loss = 0.00972342\n",
      "Iteration 36, loss = 0.98607568\n",
      "Iteration 519, loss = 0.00964296\n",
      "Iteration 37, loss = 0.98594889\n",
      "Iteration 520, loss = 0.00936360\n",
      "Iteration 38, loss = 0.72026572\n",
      "Iteration 521, loss = 0.00893813\n",
      "Iteration 39, loss = 0.75870964\n",
      "Iteration 522, loss = 0.00991012\n",
      "Iteration 40, loss = 0.66297006\n",
      "Iteration 523, loss = 0.00930419\n",
      "Iteration 41, loss = 0.50925440\n",
      "Iteration 524, loss = 0.00890982\n",
      "Iteration 42, loss = 0.43903547\n",
      "Iteration 525, loss = 0.00927415\n",
      "Iteration 43, loss = 0.47626303\n",
      "Iteration 526, loss = 0.00902897\n",
      "Iteration 44, loss = 0.44953669\n",
      "Iteration 527, loss = 0.00859658\n",
      "Iteration 45, loss = 0.55099028\n",
      "Iteration 528, loss = 0.00880749\n",
      "Iteration 46, loss = 0.51718035\n",
      "Iteration 529, loss = 0.00833569\n",
      "Iteration 47, loss = 0.41691145\n",
      "Iteration 530, loss = 0.00836810\n",
      "Iteration 48, loss = 0.52802294\n",
      "Iteration 531, loss = 0.00842236\n",
      "Iteration 49, loss = 0.41797416\n",
      "Iteration 532, loss = 0.00861173\n",
      "Iteration 50, loss = 0.40009050\n",
      "Iteration 533, loss = 0.00839461\n",
      "Iteration 51, loss = 0.32780924\n",
      "Iteration 534, loss = 0.00937543\n",
      "Iteration 52, loss = 0.39784652\n",
      "Iteration 535, loss = 0.00855142\n",
      "Iteration 53, loss = 0.36623255\n",
      "Iteration 536, loss = 0.00821854\n",
      "Iteration 54, loss = 0.48188376\n",
      "Iteration 537, loss = 0.00841922\n",
      "Iteration 55, loss = 0.37102197\n",
      "Iteration 538, loss = 0.00806136\n",
      "Iteration 56, loss = 0.43235971\n",
      "Iteration 539, loss = 0.00798226\n",
      "Iteration 57, loss = 0.34476427\n",
      "Iteration 540, loss = 0.00812155\n",
      "Iteration 58, loss = 0.33098852\n",
      "Iteration 541, loss = 0.00789763\n",
      "Iteration 59, loss = 0.23554160\n",
      "Iteration 542, loss = 0.00780428\n",
      "Iteration 60, loss = 0.18474111\n",
      "Iteration 543, loss = 0.00767118\n",
      "Iteration 61, loss = 0.22023995\n",
      "Iteration 544, loss = 0.00773672\n",
      "Iteration 62, loss = 0.27747848\n",
      "Iteration 545, loss = 0.00769332\n",
      "Iteration 63, loss = 0.25851996\n",
      "Iteration 546, loss = 0.00761710\n",
      "Iteration 64, loss = 0.19938057\n",
      "Iteration 547, loss = 0.00763068\n",
      "Iteration 65, loss = 0.19495280\n",
      "Iteration 548, loss = 0.00781666\n",
      "Iteration 66, loss = 0.17944795\n",
      "Iteration 549, loss = 0.00737601\n",
      "Iteration 67, loss = 0.20029301\n",
      "Iteration 550, loss = 0.00747473\n",
      "Iteration 68, loss = 0.19739696\n",
      "Iteration 551, loss = 0.00739333\n",
      "Iteration 69, loss = 0.22302092\n",
      "Iteration 552, loss = 0.00749372\n",
      "Iteration 70, loss = 0.24656023\n",
      "Iteration 553, loss = 0.00741288\n",
      "Iteration 71, loss = 0.28811851\n",
      "Iteration 554, loss = 0.00717873\n",
      "Iteration 72, loss = 0.35581384\n",
      "Iteration 555, loss = 0.00726926\n",
      "Iteration 73, loss = 0.31744096\n",
      "Iteration 556, loss = 0.00714472\n",
      "Iteration 74, loss = 0.38322312\n",
      "Iteration 557, loss = 0.00720518\n",
      "Iteration 75, loss = 0.30228351\n",
      "Iteration 558, loss = 0.00711566\n",
      "Iteration 76, loss = 0.23252272\n",
      "Iteration 559, loss = 0.00714520\n",
      "Iteration 77, loss = 0.20533271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 560, loss = 0.00701459\n",
      "Iteration 561, loss = 0.00699052\n",
      "Iteration 562, loss = 0.00714641\n",
      "Iteration 563, loss = 0.00672546\n",
      "Iteration 564, loss = 0.00684752\n",
      "Iteration 565, loss = 0.00679372\n",
      "Iteration 566, loss = 0.00702542\n",
      "Iteration 567, loss = 0.00696370\n",
      "Iteration 568, loss = 0.00676613\n",
      "Iteration 569, loss = 0.00697779\n",
      "Iteration 570, loss = 0.00694228\n",
      "Iteration 571, loss = 0.00694824\n",
      "Iteration 572, loss = 0.00705283\n",
      "Iteration 573, loss = 0.00631634\n",
      "Iteration 574, loss = 0.00686489\n",
      "Iteration 575, loss = 0.00631284\n",
      "Iteration 576, loss = 0.00654650\n",
      "Iteration 577, loss = 0.00625269\n",
      "Iteration 1, loss = 16.16769847\n",
      "Iteration 578, loss = 0.00639187\n",
      "Iteration 2, loss = 15.69268991\n",
      "Iteration 579, loss = 0.00615127\n",
      "Iteration 3, loss = 12.12838952\n",
      "Iteration 580, loss = 0.00621645\n",
      "Iteration 4, loss = 7.48981176\n",
      "Iteration 581, loss = 0.00618265\n",
      "Iteration 5, loss = 5.92953347\n",
      "Iteration 582, loss = 0.00613937\n",
      "Iteration 6, loss = 5.70039082\n",
      "Iteration 583, loss = 0.00620824\n",
      "Iteration 7, loss = 10.95564659\n",
      "Iteration 584, loss = 0.00647851\n",
      "Iteration 8, loss = 5.63284112\n",
      "Iteration 585, loss = 0.00632180\n",
      "Iteration 9, loss = 5.45073487\n",
      "Iteration 586, loss = 0.00642819\n",
      "Iteration 10, loss = 6.42451958\n",
      "Iteration 587, loss = 0.00620364\n",
      "Iteration 11, loss = 6.49914102\n",
      "Iteration 588, loss = 0.00598338\n",
      "Iteration 12, loss = 4.67251497\n",
      "Iteration 589, loss = 0.00593268\n",
      "Iteration 13, loss = 4.71157546\n",
      "Iteration 590, loss = 0.00589725\n",
      "Iteration 14, loss = 3.72804673\n",
      "Iteration 591, loss = 0.00592533\n",
      "Iteration 15, loss = 4.55577153\n",
      "Iteration 16, loss = 4.64949198\n",
      "Iteration 592, loss = 0.00574583\n",
      "Iteration 17, loss = 4.41824397\n",
      "Iteration 593, loss = 0.00582091\n",
      "Iteration 18, loss = 5.56410027\n",
      "Iteration 594, loss = 0.00590078\n",
      "Iteration 19, loss = 3.97152821\n",
      "Iteration 595, loss = 0.00563929\n",
      "Iteration 596, loss = 0.00575085\n",
      "Iteration 20, loss = 3.40459294\n",
      "Iteration 597, loss = 0.00554046\n",
      "Iteration 21, loss = 3.21949117\n",
      "Iteration 22, loss = 2.64379224\n",
      "Iteration 598, loss = 0.00578384\n",
      "Iteration 23, loss = 2.44447337\n",
      "Iteration 599, loss = 0.00547269\n",
      "Iteration 24, loss = 3.98842549\n",
      "Iteration 600, loss = 0.00554158\n",
      "Iteration 25, loss = 4.65731301\n",
      "Iteration 601, loss = 0.00566333\n",
      "Iteration 26, loss = 2.78915915\n",
      "Iteration 602, loss = 0.00573593\n",
      "Iteration 27, loss = 3.23263307\n",
      "Iteration 603, loss = 0.00637846\n",
      "Iteration 28, loss = 2.83840838\n",
      "Iteration 604, loss = 0.00532298\n",
      "Iteration 29, loss = 2.84812263\n",
      "Iteration 605, loss = 0.00598965\n",
      "Iteration 30, loss = 2.24503291\n",
      "Iteration 606, loss = 0.00530831\n",
      "Iteration 31, loss = 2.15029922\n",
      "Iteration 607, loss = 0.00534727\n",
      "Iteration 32, loss = 1.89963770\n",
      "Iteration 608, loss = 0.00527468\n",
      "Iteration 33, loss = 1.58219136\n",
      "Iteration 609, loss = 0.00533908\n",
      "Iteration 34, loss = 2.36811837\n",
      "Iteration 610, loss = 0.00545954\n",
      "Iteration 35, loss = 2.04847665\n",
      "Iteration 611, loss = 0.00531591\n",
      "Iteration 36, loss = 1.91861343\n",
      "Iteration 612, loss = 0.00529720\n",
      "Iteration 37, loss = 1.43486593\n",
      "Iteration 613, loss = 0.00525989\n",
      "Iteration 38, loss = 2.01907932\n",
      "Iteration 614, loss = 0.00514505\n",
      "Iteration 39, loss = 1.95739413\n",
      "Iteration 615, loss = 0.00508930\n",
      "Iteration 40, loss = 2.15533233\n",
      "Iteration 616, loss = 0.00509644\n",
      "Iteration 41, loss = 1.88338867\n",
      "Iteration 617, loss = 0.00490295\n",
      "Iteration 42, loss = 2.16122698\n",
      "Iteration 618, loss = 0.00515531\n",
      "Iteration 43, loss = 1.58284969\n",
      "Iteration 619, loss = 0.00492357\n",
      "Iteration 44, loss = 1.36219138\n",
      "Iteration 620, loss = 0.00489689\n",
      "Iteration 45, loss = 2.21785705\n",
      "Iteration 621, loss = 0.00484066\n",
      "Iteration 46, loss = 2.61632364\n",
      "Iteration 622, loss = 0.00493004\n",
      "Iteration 47, loss = 1.72610745\n",
      "Iteration 623, loss = 0.00535661\n",
      "Iteration 48, loss = 1.81939437\n",
      "Iteration 624, loss = 0.00481622\n",
      "Iteration 49, loss = 1.16259839\n",
      "Iteration 625, loss = 0.00493859\n",
      "Iteration 50, loss = 1.71574081\n",
      "Iteration 626, loss = 0.00474313\n",
      "Iteration 51, loss = 1.12418521\n",
      "Iteration 627, loss = 0.00470607\n",
      "Iteration 52, loss = 1.10790171\n",
      "Iteration 628, loss = 0.00476871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 1.44355395\n",
      "Iteration 54, loss = 1.56032740\n",
      "Iteration 55, loss = 1.68069428\n",
      "Iteration 56, loss = 1.41744118\n",
      "Iteration 57, loss = 1.44772076\n",
      "Iteration 58, loss = 1.20712698\n",
      "Iteration 59, loss = 1.03501602\n",
      "Iteration 60, loss = 1.27709534\n",
      "Iteration 61, loss = 1.14073393\n",
      "Iteration 62, loss = 0.84012650\n",
      "Iteration 63, loss = 0.92350318\n",
      "Iteration 64, loss = 0.53792452\n",
      "Iteration 65, loss = 0.69652527\n",
      "Iteration 66, loss = 0.67218305\n",
      "Iteration 67, loss = 0.67216680\n",
      "Iteration 68, loss = 0.51351677\n",
      "Iteration 69, loss = 0.80543757\n",
      "Iteration 70, loss = 1.48322469\n",
      "Iteration 1, loss = 16.44516546\n",
      "Iteration 71, loss = 1.01918709\n",
      "Iteration 2, loss = 14.56394157\n",
      "Iteration 72, loss = 0.95005174\n",
      "Iteration 3, loss = 13.88017266\n",
      "Iteration 73, loss = 0.89497516\n",
      "Iteration 4, loss = 11.01008537\n",
      "Iteration 74, loss = 0.70499584\n",
      "Iteration 5, loss = 10.32064728\n",
      "Iteration 75, loss = 0.59030592\n",
      "Iteration 6, loss = 9.38231973\n",
      "Iteration 76, loss = 0.70712608\n",
      "Iteration 7, loss = 5.78321319\n",
      "Iteration 77, loss = 0.76358645\n",
      "Iteration 8, loss = 6.09106998\n",
      "Iteration 78, loss = 0.84827205\n",
      "Iteration 9, loss = 5.21585639\n",
      "Iteration 79, loss = 0.42785450\n",
      "Iteration 10, loss = 7.24080042\n",
      "Iteration 80, loss = 0.52835280\n",
      "Iteration 11, loss = 6.89831557\n",
      "Iteration 81, loss = 0.54161606\n",
      "Iteration 12, loss = 5.89446246\n",
      "Iteration 82, loss = 0.67154174\n",
      "Iteration 13, loss = 5.07957292\n",
      "Iteration 83, loss = 0.64882492\n",
      "Iteration 14, loss = 4.17195675\n",
      "Iteration 84, loss = 0.83407541\n",
      "Iteration 15, loss = 4.57134936\n",
      "Iteration 85, loss = 0.88264125\n",
      "Iteration 16, loss = 10.70809739\n",
      "Iteration 86, loss = 0.83708198\n",
      "Iteration 17, loss = 7.90770157\n",
      "Iteration 87, loss = 0.87966560\n",
      "Iteration 18, loss = 6.89287095\n",
      "Iteration 88, loss = 1.08406449\n",
      "Iteration 19, loss = 7.55771995\n",
      "Iteration 89, loss = 1.02661989\n",
      "Iteration 20, loss = 11.22650436\n",
      "Iteration 90, loss = 1.16417992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 4.72616065\n",
      "Iteration 22, loss = 4.19394378\n",
      "Iteration 23, loss = 4.21229201\n",
      "Iteration 24, loss = 3.33933548\n",
      "Iteration 25, loss = 2.75654915\n",
      "Iteration 26, loss = 5.54434659\n",
      "Iteration 27, loss = 4.95038691\n",
      "Iteration 28, loss = 6.41864793\n",
      "Iteration 29, loss = 4.67117511\n",
      "Iteration 30, loss = 3.47841512\n",
      "Iteration 31, loss = 3.76529323\n",
      "Iteration 32, loss = 2.98155570\n",
      "Iteration 33, loss = 2.17391194\n",
      "Iteration 34, loss = 1.74431334\n",
      "Iteration 35, loss = 1.83677468\n",
      "Iteration 36, loss = 1.39996940\n",
      "Iteration 37, loss = 1.27253783\n",
      "Iteration 38, loss = 1.11163974\n",
      "Iteration 1, loss = 17.43210266\n",
      "Iteration 39, loss = 1.77281508\n",
      "Iteration 2, loss = 9.36713972\n",
      "Iteration 40, loss = 1.33005571\n",
      "Iteration 3, loss = 12.48511942\n",
      "Iteration 41, loss = 1.13211000\n",
      "Iteration 4, loss = 9.11778835\n",
      "Iteration 42, loss = 1.04652076\n",
      "Iteration 5, loss = 8.34080228\n",
      "Iteration 43, loss = 0.66222755\n",
      "Iteration 6, loss = 6.72877365\n",
      "Iteration 44, loss = 0.63893297\n",
      "Iteration 7, loss = 5.99740431\n",
      "Iteration 45, loss = 0.54849880\n",
      "Iteration 8, loss = 5.85153722\n",
      "Iteration 46, loss = 0.62857271\n",
      "Iteration 9, loss = 7.30691521\n",
      "Iteration 47, loss = 0.58794178\n",
      "Iteration 10, loss = 5.62047658\n",
      "Iteration 48, loss = 0.48590624\n",
      "Iteration 11, loss = 5.72086692\n",
      "Iteration 49, loss = 0.58535518\n",
      "Iteration 12, loss = 7.48367959\n",
      "Iteration 50, loss = 0.65435777\n",
      "Iteration 13, loss = 5.82524622\n",
      "Iteration 51, loss = 0.53242676\n",
      "Iteration 14, loss = 3.96993687\n",
      "Iteration 52, loss = 0.53233536\n",
      "Iteration 15, loss = 3.87597916\n",
      "Iteration 53, loss = 0.43808085\n",
      "Iteration 16, loss = 4.09174453\n",
      "Iteration 54, loss = 0.59999414\n",
      "Iteration 17, loss = 4.18950832\n",
      "Iteration 55, loss = 0.69382158\n",
      "Iteration 18, loss = 3.66445166\n",
      "Iteration 56, loss = 0.60926101\n",
      "Iteration 19, loss = 3.17533111\n",
      "Iteration 57, loss = 0.61188414\n",
      "Iteration 20, loss = 3.18121490\n",
      "Iteration 58, loss = 0.37870420\n",
      "Iteration 21, loss = 2.24328365\n",
      "Iteration 59, loss = 0.29255087\n",
      "Iteration 22, loss = 2.49332389\n",
      "Iteration 60, loss = 0.40353737\n",
      "Iteration 23, loss = 2.09034873\n",
      "Iteration 61, loss = 0.42243956\n",
      "Iteration 24, loss = 1.67831948\n",
      "Iteration 62, loss = 0.33966759\n",
      "Iteration 25, loss = 1.59143229\n",
      "Iteration 63, loss = 0.33235376\n",
      "Iteration 26, loss = 1.39360598\n",
      "Iteration 64, loss = 0.36355840\n",
      "Iteration 27, loss = 1.27133037\n",
      "Iteration 65, loss = 0.26245417\n",
      "Iteration 28, loss = 1.10028843\n",
      "Iteration 66, loss = 0.33146660\n",
      "Iteration 29, loss = 1.25355889\n",
      "Iteration 67, loss = 0.33990930\n",
      "Iteration 30, loss = 1.30130623\n",
      "Iteration 68, loss = 0.22696318\n",
      "Iteration 31, loss = 1.43809237\n",
      "Iteration 69, loss = 0.24669736\n",
      "Iteration 32, loss = 1.80248302\n",
      "Iteration 70, loss = 0.18730317\n",
      "Iteration 33, loss = 1.72610777\n",
      "Iteration 34, loss = 1.65645826\n",
      "Iteration 71, loss = 0.26639908\n",
      "Iteration 72, loss = 0.28542952\n",
      "Iteration 35, loss = 1.60631571\n",
      "Iteration 73, loss = 0.24425031\n",
      "Iteration 36, loss = 2.42300188\n",
      "Iteration 37, loss = 2.73771479\n",
      "Iteration 74, loss = 0.23894589\n",
      "Iteration 38, loss = 2.52454077\n",
      "Iteration 75, loss = 0.23041983\n",
      "Iteration 39, loss = 2.12390688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.27908841\n",
      "Iteration 77, loss = 0.18925536\n",
      "Iteration 78, loss = 0.23533155\n",
      "Iteration 79, loss = 0.22537862\n",
      "Iteration 80, loss = 0.18266446\n",
      "Iteration 81, loss = 0.25528397\n",
      "Iteration 82, loss = 0.22341137\n",
      "Iteration 83, loss = 0.17876254\n",
      "Iteration 84, loss = 0.23793069\n",
      "Iteration 85, loss = 0.21900969\n",
      "Iteration 86, loss = 0.21501721\n",
      "Iteration 87, loss = 0.25801408\n",
      "Iteration 88, loss = 0.22842020\n",
      "Iteration 89, loss = 0.20778502\n",
      "Iteration 90, loss = 0.19277560\n",
      "Iteration 91, loss = 0.20873072\n",
      "Iteration 92, loss = 0.17844750\n",
      "Iteration 93, loss = 0.15677239\n",
      "Iteration 94, loss = 0.17127865\n",
      "Iteration 1, loss = 17.27103769\n",
      "Iteration 95, loss = 0.14536341\n",
      "Iteration 2, loss = 16.24084632\n",
      "Iteration 96, loss = 0.20459827\n",
      "Iteration 3, loss = 14.88176875\n",
      "Iteration 97, loss = 0.18715358\n",
      "Iteration 4, loss = 11.82068049\n",
      "Iteration 98, loss = 0.20599931\n",
      "Iteration 5, loss = 6.75189829\n",
      "Iteration 99, loss = 0.17355783\n",
      "Iteration 6, loss = 6.06900024\n",
      "Iteration 7, loss = 6.37366696\n",
      "Iteration 100, loss = 0.16175452\n",
      "Iteration 8, loss = 7.85692011\n",
      "Iteration 9, loss = 7.16744350\n",
      "Iteration 10, loss = 6.29588052\n",
      "Iteration 11, loss = 5.19170877\n",
      "Iteration 12, loss = 4.69067335\n",
      "Iteration 13, loss = 4.64054898\n",
      "Iteration 14, loss = 4.40104382\n",
      "Iteration 15, loss = 4.00341627\n",
      "Iteration 16, loss = 5.72679787\n",
      "Iteration 17, loss = 4.73279733\n",
      "Iteration 18, loss = 4.46959818\n",
      "Iteration 19, loss = 3.59420612\n",
      "Iteration 20, loss = 3.33146170\n",
      "Iteration 21, loss = 4.17817737\n",
      "Iteration 22, loss = 3.63420985\n",
      "Iteration 23, loss = 2.93349643\n",
      "Iteration 24, loss = 2.34205489\n",
      "Iteration 25, loss = 4.65883194\n",
      "Iteration 1, loss = 15.12114551\n",
      "Iteration 26, loss = 4.93064681\n",
      "Iteration 2, loss = 12.32568596\n",
      "Iteration 27, loss = 3.50720760\n",
      "Iteration 3, loss = 10.34059672\n",
      "Iteration 28, loss = 3.93074486\n",
      "Iteration 4, loss = 7.42245039\n",
      "Iteration 29, loss = 2.79936949\n",
      "Iteration 5, loss = 8.94097761\n",
      "Iteration 30, loss = 2.90526591\n",
      "Iteration 6, loss = 7.45546358\n",
      "Iteration 31, loss = 2.90900786\n",
      "Iteration 7, loss = 9.67885777\n",
      "Iteration 32, loss = 2.74707566\n",
      "Iteration 8, loss = 8.78625147\n",
      "Iteration 33, loss = 2.10904756\n",
      "Iteration 9, loss = 5.36850038\n",
      "Iteration 34, loss = 2.14290263\n",
      "Iteration 10, loss = 5.54120126\n",
      "Iteration 35, loss = 2.03109955\n",
      "Iteration 11, loss = 5.47885581\n",
      "Iteration 36, loss = 2.14762334\n",
      "Iteration 12, loss = 4.80140059\n",
      "Iteration 37, loss = 2.46404598\n",
      "Iteration 13, loss = 4.67257166\n",
      "Iteration 38, loss = 1.88043009\n",
      "Iteration 14, loss = 5.37424716\n",
      "Iteration 39, loss = 2.50869770\n",
      "Iteration 15, loss = 4.94327141\n",
      "Iteration 40, loss = 2.32923522\n",
      "Iteration 16, loss = 5.23235559\n",
      "Iteration 41, loss = 1.86738267\n",
      "Iteration 17, loss = 3.66612331\n",
      "Iteration 42, loss = 1.57107735\n",
      "Iteration 18, loss = 4.28052241\n",
      "Iteration 43, loss = 1.70309388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 3.61703940\n",
      "Iteration 44, loss = 1.31752448\n",
      "Iteration 20, loss = 6.38763083\n",
      "Iteration 45, loss = 0.97075395\n",
      "Iteration 21, loss = 5.67005166\n",
      "Iteration 46, loss = 0.88883268\n",
      "Iteration 22, loss = 5.71347602\n",
      "Iteration 47, loss = 0.99973810\n",
      "Iteration 23, loss = 5.11034660\n",
      "Iteration 48, loss = 1.02322902\n",
      "Iteration 24, loss = 4.79373970\n",
      "Iteration 49, loss = 0.98870677\n",
      "Iteration 25, loss = 4.34939418\n",
      "Iteration 50, loss = 1.08409084\n",
      "Iteration 26, loss = 3.80861835\n",
      "Iteration 51, loss = 1.07699170\n",
      "Iteration 27, loss = 2.82617359\n",
      "Iteration 52, loss = 1.00240758\n",
      "Iteration 28, loss = 3.12249589\n",
      "Iteration 53, loss = 1.40312367\n",
      "Iteration 29, loss = 3.51481418\n",
      "Iteration 54, loss = 1.14682020\n",
      "Iteration 30, loss = 1.84418275\n",
      "Iteration 55, loss = 0.76641569\n",
      "Iteration 31, loss = 2.15992025\n",
      "Iteration 56, loss = 0.95126293\n",
      "Iteration 32, loss = 1.89621569\n",
      "Iteration 57, loss = 0.82576957\n",
      "Iteration 33, loss = 1.88695177\n",
      "Iteration 58, loss = 1.26687968\n",
      "Iteration 34, loss = 1.22601418\n",
      "Iteration 59, loss = 1.10401690\n",
      "Iteration 35, loss = 0.85805936\n",
      "Iteration 60, loss = 0.71153254\n",
      "Iteration 36, loss = 0.84132115\n",
      "Iteration 61, loss = 0.82822238\n",
      "Iteration 37, loss = 0.81953961\n",
      "Iteration 62, loss = 0.57566061\n",
      "Iteration 38, loss = 0.75402210\n",
      "Iteration 63, loss = 0.38831076\n",
      "Iteration 39, loss = 0.84543131\n",
      "Iteration 64, loss = 0.48869851\n",
      "Iteration 40, loss = 0.62506054\n",
      "Iteration 65, loss = 0.40388935\n",
      "Iteration 41, loss = 0.83339905\n",
      "Iteration 66, loss = 0.42913304\n",
      "Iteration 42, loss = 1.08474673\n",
      "Iteration 67, loss = 0.66411542\n",
      "Iteration 43, loss = 0.70996293\n",
      "Iteration 68, loss = 0.86919438\n",
      "Iteration 44, loss = 1.08351391\n",
      "Iteration 69, loss = 0.90962964\n",
      "Iteration 45, loss = 0.87353393\n",
      "Iteration 70, loss = 0.83346126\n",
      "Iteration 46, loss = 1.07665721\n",
      "Iteration 71, loss = 0.59490914\n",
      "Iteration 47, loss = 0.88913503\n",
      "Iteration 72, loss = 0.45328209\n",
      "Iteration 48, loss = 0.50410340\n",
      "Iteration 73, loss = 0.56353623\n",
      "Iteration 49, loss = 0.67068191\n",
      "Iteration 74, loss = 0.40070322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.55569267\n",
      "Iteration 51, loss = 0.63340189\n",
      "Iteration 52, loss = 0.60202810\n",
      "Iteration 53, loss = 0.73139181\n",
      "Iteration 54, loss = 0.43753593\n",
      "Iteration 55, loss = 0.36034262\n",
      "Iteration 56, loss = 0.63108325\n",
      "Iteration 57, loss = 0.88638688\n",
      "Iteration 58, loss = 1.02041504\n",
      "Iteration 59, loss = 0.49269543\n",
      "Iteration 60, loss = 0.38231702\n",
      "Iteration 61, loss = 0.59720529\n",
      "Iteration 62, loss = 0.64912267\n",
      "Iteration 63, loss = 0.49966966\n",
      "Iteration 64, loss = 0.54968030\n",
      "Iteration 65, loss = 0.42860583\n",
      "Iteration 66, loss = 0.69366909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.86309081\n",
      "Iteration 2, loss = 18.03809741\n",
      "Iteration 3, loss = 13.90421732\n",
      "Iteration 4, loss = 9.57743905\n",
      "Iteration 5, loss = 9.19006848\n",
      "Iteration 6, loss = 6.09949611\n",
      "Iteration 7, loss = 6.58451441\n",
      "Iteration 8, loss = 5.99610211\n",
      "Iteration 9, loss = 8.77426320\n",
      "Iteration 10, loss = 7.33967331\n",
      "Iteration 11, loss = 5.06710560\n",
      "Iteration 12, loss = 4.57904194\n",
      "Iteration 13, loss = 5.13294658\n",
      "Iteration 14, loss = 3.43193583\n",
      "Iteration 15, loss = 4.60800038\n",
      "Iteration 16, loss = 4.10687718\n",
      "Iteration 17, loss = 3.90299219\n",
      "Iteration 1, loss = 17.86133208\n",
      "Iteration 18, loss = 4.14657288\n",
      "Iteration 2, loss = 11.99231489\n",
      "Iteration 19, loss = 3.38508516\n",
      "Iteration 3, loss = 8.05763645\n",
      "Iteration 20, loss = 4.50952921\n",
      "Iteration 4, loss = 7.81304990\n",
      "Iteration 21, loss = 3.61132129\n",
      "Iteration 5, loss = 6.08313468\n",
      "Iteration 22, loss = 2.72643584\n",
      "Iteration 6, loss = 7.53629464\n",
      "Iteration 23, loss = 2.97757152\n",
      "Iteration 7, loss = 5.74957391\n",
      "Iteration 24, loss = 3.82805440\n",
      "Iteration 8, loss = 9.94799796\n",
      "Iteration 25, loss = 2.98851320\n",
      "Iteration 9, loss = 5.74768281\n",
      "Iteration 26, loss = 3.48020441\n",
      "Iteration 10, loss = 5.89275635\n",
      "Iteration 27, loss = 3.03475377\n",
      "Iteration 11, loss = 4.83650226\n",
      "Iteration 28, loss = 2.58541706\n",
      "Iteration 12, loss = 5.71848876\n",
      "Iteration 29, loss = 2.31144731\n",
      "Iteration 13, loss = 4.69849254\n",
      "Iteration 30, loss = 2.87004055\n",
      "Iteration 14, loss = 4.10752619\n",
      "Iteration 31, loss = 2.55715200\n",
      "Iteration 15, loss = 4.98619114\n",
      "Iteration 32, loss = 2.87159664\n",
      "Iteration 16, loss = 4.41442159\n",
      "Iteration 33, loss = 2.72469614\n",
      "Iteration 17, loss = 3.48097832\n",
      "Iteration 34, loss = 1.86411770\n",
      "Iteration 18, loss = 4.65971943\n",
      "Iteration 35, loss = 1.39881471\n",
      "Iteration 19, loss = 3.73615499\n",
      "Iteration 36, loss = 1.57595750\n",
      "Iteration 20, loss = 2.88501919\n",
      "Iteration 37, loss = 1.22843377\n",
      "Iteration 21, loss = 3.16904173\n",
      "Iteration 38, loss = 1.48272098\n",
      "Iteration 22, loss = 3.64936960\n",
      "Iteration 39, loss = 1.56188056\n",
      "Iteration 23, loss = 2.99409632\n",
      "Iteration 40, loss = 1.68686280\n",
      "Iteration 24, loss = 3.07785887\n",
      "Iteration 41, loss = 1.67965438\n",
      "Iteration 25, loss = 2.39399171\n",
      "Iteration 42, loss = 1.85993866\n",
      "Iteration 26, loss = 2.05617029\n",
      "Iteration 43, loss = 1.27447618\n",
      "Iteration 27, loss = 2.51742314\n",
      "Iteration 44, loss = 1.82077631\n",
      "Iteration 28, loss = 2.31471642\n",
      "Iteration 45, loss = 1.51843229\n",
      "Iteration 29, loss = 1.92667272\n",
      "Iteration 46, loss = 1.82192476\n",
      "Iteration 30, loss = 1.65885514\n",
      "Iteration 47, loss = 1.32203005\n",
      "Iteration 31, loss = 2.12239393\n",
      "Iteration 48, loss = 1.20681303\n",
      "Iteration 32, loss = 1.77026565\n",
      "Iteration 49, loss = 1.07137880\n",
      "Iteration 33, loss = 2.02628092\n",
      "Iteration 50, loss = 0.84478266\n",
      "Iteration 34, loss = 1.74225791\n",
      "Iteration 51, loss = 0.95905758\n",
      "Iteration 35, loss = 2.76943272\n",
      "Iteration 52, loss = 0.80949874\n",
      "Iteration 36, loss = 2.28842785\n",
      "Iteration 53, loss = 0.73025305\n",
      "Iteration 37, loss = 2.57907668\n",
      "Iteration 54, loss = 0.73948835\n",
      "Iteration 38, loss = 1.50817597\n",
      "Iteration 55, loss = 0.68085017\n",
      "Iteration 39, loss = 1.33528819\n",
      "Iteration 56, loss = 0.61966982\n",
      "Iteration 40, loss = 1.33422167\n",
      "Iteration 57, loss = 0.48814666\n",
      "Iteration 41, loss = 1.12464976\n",
      "Iteration 58, loss = 0.62855090\n",
      "Iteration 42, loss = 0.97164547\n",
      "Iteration 59, loss = 0.90953653\n",
      "Iteration 43, loss = 1.78900103\n",
      "Iteration 60, loss = 1.24596580\n",
      "Iteration 44, loss = 1.19751529\n",
      "Iteration 61, loss = 0.94102405\n",
      "Iteration 45, loss = 0.84269440\n",
      "Iteration 62, loss = 0.57685889\n",
      "Iteration 46, loss = 1.06940591\n",
      "Iteration 63, loss = 1.16916157\n",
      "Iteration 47, loss = 0.72297168\n",
      "Iteration 64, loss = 0.63785670\n",
      "Iteration 48, loss = 0.67450813\n",
      "Iteration 65, loss = 0.90655792\n",
      "Iteration 49, loss = 0.89293726\n",
      "Iteration 66, loss = 1.67680030\n",
      "Iteration 50, loss = 0.68549894\n",
      "Iteration 67, loss = 1.51815923\n",
      "Iteration 51, loss = 0.52664429\n",
      "Iteration 68, loss = 0.82191945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.71780718\n",
      "Iteration 53, loss = 0.63967860\n",
      "Iteration 54, loss = 0.90925488\n",
      "Iteration 55, loss = 0.78919097\n",
      "Iteration 56, loss = 1.11170920\n",
      "Iteration 57, loss = 0.70884704\n",
      "Iteration 58, loss = 0.97550766\n",
      "Iteration 59, loss = 0.77139638\n",
      "Iteration 60, loss = 0.46513372\n",
      "Iteration 61, loss = 0.51494571\n",
      "Iteration 62, loss = 0.49332867\n",
      "Iteration 63, loss = 0.40977873\n",
      "Iteration 64, loss = 0.41839197\n",
      "Iteration 65, loss = 0.43566683\n",
      "Iteration 66, loss = 0.46933564\n",
      "Iteration 67, loss = 0.69690859\n",
      "Iteration 68, loss = 0.64804787\n",
      "Iteration 69, loss = 0.84989589\n",
      "Iteration 1, loss = 18.45669317\n",
      "Iteration 70, loss = 0.66348091\n",
      "Iteration 2, loss = 15.55437576\n",
      "Iteration 71, loss = 0.59266550\n",
      "Iteration 3, loss = 13.64881834\n",
      "Iteration 72, loss = 0.51538732\n",
      "Iteration 4, loss = 9.41591623\n",
      "Iteration 73, loss = 0.61846148\n",
      "Iteration 5, loss = 9.25832276\n",
      "Iteration 74, loss = 0.35536461\n",
      "Iteration 6, loss = 8.78924351\n",
      "Iteration 75, loss = 0.36603051\n",
      "Iteration 7, loss = 10.86436403\n",
      "Iteration 76, loss = 0.36847853\n",
      "Iteration 8, loss = 7.83938282\n",
      "Iteration 77, loss = 0.40512234\n",
      "Iteration 9, loss = 7.70424091\n",
      "Iteration 78, loss = 0.39080308\n",
      "Iteration 10, loss = 6.38385423\n",
      "Iteration 79, loss = 0.44640687\n",
      "Iteration 11, loss = 7.69571653\n",
      "Iteration 80, loss = 1.31953117\n",
      "Iteration 12, loss = 7.99191191\n",
      "Iteration 81, loss = 0.67099510\n",
      "Iteration 13, loss = 8.18386360\n",
      "Iteration 82, loss = 0.95176178\n",
      "Iteration 14, loss = 6.26611905\n",
      "Iteration 83, loss = 1.36659867\n",
      "Iteration 15, loss = 4.66128892\n",
      "Iteration 84, loss = 0.87977188\n",
      "Iteration 16, loss = 4.42311201\n",
      "Iteration 85, loss = 0.89508217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 5.36629522\n",
      "Iteration 18, loss = 5.46544347\n",
      "Iteration 19, loss = 4.72739972\n",
      "Iteration 20, loss = 4.34277905\n",
      "Iteration 21, loss = 3.49875736\n",
      "Iteration 22, loss = 3.94876442\n",
      "Iteration 23, loss = 4.30011536\n",
      "Iteration 24, loss = 4.10697490\n",
      "Iteration 25, loss = 2.69232531\n",
      "Iteration 26, loss = 2.96367233\n",
      "Iteration 27, loss = 2.69327409\n",
      "Iteration 28, loss = 2.42631043\n",
      "Iteration 29, loss = 2.94687931\n",
      "Iteration 30, loss = 2.85188733\n",
      "Iteration 31, loss = 2.68028293\n",
      "Iteration 32, loss = 2.24106187\n",
      "Iteration 33, loss = 2.15735779\n",
      "Iteration 34, loss = 1.59458223\n",
      "Iteration 35, loss = 1.59435963\n",
      "Iteration 1, loss = 22.99611444\n",
      "Iteration 36, loss = 1.63993694\n",
      "Iteration 2, loss = 18.75836439\n",
      "Iteration 37, loss = 1.50629663\n",
      "Iteration 3, loss = 14.17103268\n",
      "Iteration 38, loss = 1.14400691\n",
      "Iteration 4, loss = 10.03711662\n",
      "Iteration 39, loss = 1.24696089\n",
      "Iteration 5, loss = 8.19174227\n",
      "Iteration 40, loss = 0.78658738\n",
      "Iteration 6, loss = 9.16638206\n",
      "Iteration 41, loss = 1.19888751\n",
      "Iteration 7, loss = 12.01402702\n",
      "Iteration 42, loss = 0.76579390\n",
      "Iteration 8, loss = 16.70809067\n",
      "Iteration 43, loss = 0.78339508\n",
      "Iteration 9, loss = 12.97341097\n",
      "Iteration 44, loss = 1.03422444\n",
      "Iteration 10, loss = 11.32044919\n",
      "Iteration 45, loss = 1.31813400\n",
      "Iteration 11, loss = 10.31970233\n",
      "Iteration 46, loss = 1.27709305\n",
      "Iteration 12, loss = 8.42161748\n",
      "Iteration 47, loss = 1.08694622\n",
      "Iteration 13, loss = 7.58611874\n",
      "Iteration 48, loss = 1.32643080\n",
      "Iteration 14, loss = 8.99965426\n",
      "Iteration 49, loss = 1.41914759\n",
      "Iteration 15, loss = 6.35380515\n",
      "Iteration 50, loss = 1.07666582\n",
      "Iteration 16, loss = 6.00854362\n",
      "Iteration 51, loss = 1.16483537\n",
      "Iteration 17, loss = 5.66729549\n",
      "Iteration 52, loss = 1.36112387\n",
      "Iteration 18, loss = 4.62215533\n",
      "Iteration 53, loss = 1.45418177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 3.61817906\n",
      "Iteration 20, loss = 3.78599566\n",
      "Iteration 21, loss = 3.25230405\n",
      "Iteration 22, loss = 2.64629323\n",
      "Iteration 23, loss = 2.90694383\n",
      "Iteration 24, loss = 2.42139949\n",
      "Iteration 25, loss = 3.51608317\n",
      "Iteration 26, loss = 2.68753803\n",
      "Iteration 27, loss = 2.35560491\n",
      "Iteration 28, loss = 2.72639274\n",
      "Iteration 29, loss = 1.60287053\n",
      "Iteration 30, loss = 2.05102021\n",
      "Iteration 31, loss = 1.47530782\n",
      "Iteration 32, loss = 1.36334518\n",
      "Iteration 33, loss = 1.40529351\n",
      "Iteration 34, loss = 1.44694349\n",
      "Iteration 35, loss = 1.30486781\n",
      "Iteration 36, loss = 0.89263307\n",
      "Iteration 1, loss = 16.18401342\n",
      "Iteration 37, loss = 0.89004229\n",
      "Iteration 2, loss = 12.86801303\n",
      "Iteration 38, loss = 0.81875762\n",
      "Iteration 3, loss = 9.63970291\n",
      "Iteration 39, loss = 0.72496912\n",
      "Iteration 4, loss = 10.28166505\n",
      "Iteration 40, loss = 1.53822498\n",
      "Iteration 5, loss = 10.82767862\n",
      "Iteration 41, loss = 1.15526815\n",
      "Iteration 6, loss = 8.17537610\n",
      "Iteration 42, loss = 1.32668330\n",
      "Iteration 7, loss = 6.36281129\n",
      "Iteration 43, loss = 1.07052814\n",
      "Iteration 8, loss = 6.64086921\n",
      "Iteration 44, loss = 1.16784736\n",
      "Iteration 9, loss = 5.46222653\n",
      "Iteration 45, loss = 1.34428596\n",
      "Iteration 10, loss = 6.20339492\n",
      "Iteration 46, loss = 1.31412029\n",
      "Iteration 11, loss = 7.26991986\n",
      "Iteration 47, loss = 0.96477147\n",
      "Iteration 12, loss = 5.79419329\n",
      "Iteration 48, loss = 1.18400154\n",
      "Iteration 13, loss = 4.35044464\n",
      "Iteration 49, loss = 1.51594250\n",
      "Iteration 14, loss = 4.99856914\n",
      "Iteration 50, loss = 1.08682798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 5.24638649\n",
      "Iteration 16, loss = 4.12907417\n",
      "Iteration 17, loss = 4.27851772\n",
      "Iteration 18, loss = 4.24002256\n",
      "Iteration 19, loss = 4.10217149\n",
      "Iteration 20, loss = 4.29715602\n",
      "Iteration 21, loss = 3.36547523\n",
      "Iteration 22, loss = 3.56039897\n",
      "Iteration 23, loss = 3.95913458\n",
      "Iteration 24, loss = 2.78814314\n",
      "Iteration 25, loss = 2.57175163\n",
      "Iteration 26, loss = 2.16844725\n",
      "Iteration 27, loss = 1.91774032\n",
      "Iteration 28, loss = 1.71108582\n",
      "Iteration 29, loss = 1.64566702\n",
      "Iteration 30, loss = 1.18439616\n",
      "Iteration 31, loss = 1.31878514\n",
      "Iteration 32, loss = 1.04442481\n",
      "Iteration 1, loss = 17.65749316\n",
      "Iteration 33, loss = 1.20648471\n",
      "Iteration 2, loss = 17.17378968\n",
      "Iteration 34, loss = 0.99728477\n",
      "Iteration 3, loss = 9.98521316\n",
      "Iteration 35, loss = 1.13928652\n",
      "Iteration 4, loss = 7.93461779\n",
      "Iteration 36, loss = 1.49469640\n",
      "Iteration 5, loss = 9.21261679\n",
      "Iteration 37, loss = 1.49451759\n",
      "Iteration 6, loss = 8.61810805\n",
      "Iteration 38, loss = 1.35168382\n",
      "Iteration 7, loss = 7.15926122\n",
      "Iteration 39, loss = 1.46110629\n",
      "Iteration 8, loss = 8.83197297\n",
      "Iteration 40, loss = 1.30743724\n",
      "Iteration 9, loss = 5.74519515\n",
      "Iteration 41, loss = 0.98617818\n",
      "Iteration 10, loss = 6.94739129\n",
      "Iteration 42, loss = 1.29550267\n",
      "Iteration 11, loss = 7.58211804\n",
      "Iteration 43, loss = 0.88655678\n",
      "Iteration 12, loss = 5.90761662\n",
      "Iteration 44, loss = 0.75686004\n",
      "Iteration 13, loss = 6.26794854\n",
      "Iteration 45, loss = 0.89829456\n",
      "Iteration 14, loss = 5.32086979\n",
      "Iteration 46, loss = 0.80971059\n",
      "Iteration 15, loss = 5.46958293\n",
      "Iteration 47, loss = 0.90786665\n",
      "Iteration 16, loss = 5.68658040\n",
      "Iteration 48, loss = 0.87285650\n",
      "Iteration 17, loss = 6.00242419\n",
      "Iteration 49, loss = 0.93878590\n",
      "Iteration 18, loss = 4.12402909\n",
      "Iteration 50, loss = 1.04920636\n",
      "Iteration 19, loss = 4.89794056\n",
      "Iteration 51, loss = 0.92142251\n",
      "Iteration 20, loss = 4.00519761\n",
      "Iteration 52, loss = 0.67748756\n",
      "Iteration 21, loss = 4.37624455\n",
      "Iteration 53, loss = 0.50652275\n",
      "Iteration 22, loss = 3.33029569\n",
      "Iteration 54, loss = 0.55371908\n",
      "Iteration 23, loss = 3.07909930\n",
      "Iteration 55, loss = 0.53838878\n",
      "Iteration 24, loss = 2.26226076\n",
      "Iteration 56, loss = 2.03119832\n",
      "Iteration 25, loss = 2.29971312\n",
      "Iteration 57, loss = 1.84036557\n",
      "Iteration 26, loss = 2.15844697\n",
      "Iteration 58, loss = 1.61277858\n",
      "Iteration 27, loss = 1.69498682\n",
      "Iteration 59, loss = 1.94238767\n",
      "Iteration 28, loss = 1.47336892\n",
      "Iteration 60, loss = 1.50911388\n",
      "Iteration 29, loss = 1.62672573\n",
      "Iteration 61, loss = 1.72000561\n",
      "Iteration 30, loss = 1.15560522\n",
      "Iteration 62, loss = 1.71975154\n",
      "Iteration 31, loss = 1.06094259\n",
      "Iteration 63, loss = 1.88184376\n",
      "Iteration 32, loss = 0.93405957\n",
      "Iteration 64, loss = 1.62174934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.64730837\n",
      "Iteration 34, loss = 1.35133783\n",
      "Iteration 35, loss = 0.89546817\n",
      "Iteration 36, loss = 1.01829558\n",
      "Iteration 37, loss = 0.95105307\n",
      "Iteration 38, loss = 0.63772821\n",
      "Iteration 39, loss = 0.64357132\n",
      "Iteration 40, loss = 0.61662344\n",
      "Iteration 41, loss = 0.45754449\n",
      "Iteration 42, loss = 0.37776816\n",
      "Iteration 43, loss = 0.50216122\n",
      "Iteration 44, loss = 0.50700323\n",
      "Iteration 45, loss = 0.38799099\n",
      "Iteration 46, loss = 0.49730387\n",
      "Iteration 47, loss = 0.56519441\n",
      "Iteration 48, loss = 0.46940233\n",
      "Iteration 49, loss = 0.45113712\n",
      "Iteration 50, loss = 0.52926414\n",
      "Iteration 1, loss = 17.19818167\n",
      "Iteration 51, loss = 0.59335397\n",
      "Iteration 2, loss = 14.19310185\n",
      "Iteration 52, loss = 0.48224490\n",
      "Iteration 3, loss = 12.87202730\n",
      "Iteration 53, loss = 0.46812583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 8.61362291\n",
      "Iteration 5, loss = 6.44974396\n",
      "Iteration 6, loss = 7.15113767\n",
      "Iteration 7, loss = 6.56222011\n",
      "Iteration 8, loss = 5.74264719\n",
      "Iteration 9, loss = 5.75974254\n",
      "Iteration 10, loss = 4.16836948\n",
      "Iteration 11, loss = 3.91559683\n",
      "Iteration 12, loss = 4.68543738\n",
      "Iteration 13, loss = 6.58116500\n",
      "Iteration 14, loss = 5.13496229\n",
      "Iteration 15, loss = 3.88422354\n",
      "Iteration 16, loss = 3.81890256\n",
      "Iteration 17, loss = 3.24059339\n",
      "Iteration 18, loss = 3.87639476\n",
      "Iteration 19, loss = 5.89911489\n",
      "Iteration 20, loss = 3.86934689\n",
      "Iteration 21, loss = 4.03084470\n",
      "Iteration 1, loss = 14.45390201\n",
      "Iteration 22, loss = 5.25788011\n",
      "Iteration 2, loss = 10.42131859\n",
      "Iteration 23, loss = 4.07462314\n",
      "Iteration 3, loss = 11.73721889\n",
      "Iteration 24, loss = 4.10743361\n",
      "Iteration 4, loss = 8.72445840\n",
      "Iteration 25, loss = 4.35702632\n",
      "Iteration 5, loss = 9.68389533\n",
      "Iteration 26, loss = 2.89876572\n",
      "Iteration 6, loss = 7.93379681\n",
      "Iteration 27, loss = 2.35276315\n",
      "Iteration 7, loss = 7.94124637\n",
      "Iteration 28, loss = 2.93438698\n",
      "Iteration 8, loss = 5.57886646\n",
      "Iteration 29, loss = 2.30044186\n",
      "Iteration 9, loss = 5.19654767\n",
      "Iteration 30, loss = 2.39697673\n",
      "Iteration 10, loss = 4.65151972\n",
      "Iteration 31, loss = 3.12535341\n",
      "Iteration 11, loss = 4.84400221\n",
      "Iteration 32, loss = 2.28826025\n",
      "Iteration 12, loss = 3.96124384\n",
      "Iteration 33, loss = 2.93586534\n",
      "Iteration 13, loss = 2.88900031\n",
      "Iteration 34, loss = 2.39632400\n",
      "Iteration 14, loss = 3.08430635\n",
      "Iteration 35, loss = 2.39752018\n",
      "Iteration 15, loss = 2.59209853\n",
      "Iteration 36, loss = 1.83251081\n",
      "Iteration 16, loss = 2.62290208\n",
      "Iteration 37, loss = 1.62458047\n",
      "Iteration 17, loss = 2.28841390\n",
      "Iteration 38, loss = 2.36462337\n",
      "Iteration 18, loss = 2.62243801\n",
      "Iteration 39, loss = 1.74843253\n",
      "Iteration 19, loss = 2.46086179\n",
      "Iteration 40, loss = 1.95425646\n",
      "Iteration 20, loss = 2.50045387\n",
      "Iteration 41, loss = 1.85380590\n",
      "Iteration 21, loss = 2.49188750\n",
      "Iteration 42, loss = 1.73990334\n",
      "Iteration 22, loss = 1.65022643\n",
      "Iteration 43, loss = 1.92252981\n",
      "Iteration 23, loss = 1.67504873\n",
      "Iteration 44, loss = 1.41112260\n",
      "Iteration 24, loss = 1.17494043\n",
      "Iteration 45, loss = 1.10573103\n",
      "Iteration 25, loss = 1.21658785\n",
      "Iteration 46, loss = 1.16093067\n",
      "Iteration 26, loss = 1.23817276\n",
      "Iteration 47, loss = 1.60920914\n",
      "Iteration 27, loss = 1.36524205\n",
      "Iteration 48, loss = 1.67580225\n",
      "Iteration 28, loss = 1.98088886\n",
      "Iteration 49, loss = 2.06448169\n",
      "Iteration 29, loss = 1.84829163\n",
      "Iteration 50, loss = 1.71047782\n",
      "Iteration 30, loss = 1.68698710\n",
      "Iteration 51, loss = 1.78943354\n",
      "Iteration 31, loss = 1.88311006\n",
      "Iteration 52, loss = 0.96792269\n",
      "Iteration 32, loss = 1.44744984\n",
      "Iteration 53, loss = 2.22762681\n",
      "Iteration 33, loss = 1.86467884\n",
      "Iteration 54, loss = 1.19646430\n",
      "Iteration 34, loss = 1.42249002\n",
      "Iteration 55, loss = 1.73661778\n",
      "Iteration 35, loss = 1.83374107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 1.05498789\n",
      "Iteration 57, loss = 2.15114512\n",
      "Iteration 58, loss = 2.25655801\n",
      "Iteration 59, loss = 1.98834889\n",
      "Iteration 60, loss = 3.06356449\n",
      "Iteration 61, loss = 2.34942199\n",
      "Iteration 62, loss = 2.03480626\n",
      "Iteration 63, loss = 2.50235489\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 13.35506612\n",
      "Iteration 2, loss = 14.45169831\n",
      "Iteration 3, loss = 8.55549692\n",
      "Iteration 4, loss = 8.43393509\n",
      "Iteration 5, loss = 5.65124448\n",
      "Iteration 6, loss = 5.98156372\n",
      "Iteration 7, loss = 5.02275067\n",
      "Iteration 8, loss = 6.08091610\n",
      "Iteration 1, loss = 15.69580663\n",
      "Iteration 9, loss = 5.50447463\n",
      "Iteration 2, loss = 16.89163275\n",
      "Iteration 10, loss = 5.21841869\n",
      "Iteration 3, loss = 11.84015441\n",
      "Iteration 11, loss = 7.75937340\n",
      "Iteration 4, loss = 8.90729993\n",
      "Iteration 12, loss = 6.01414700\n",
      "Iteration 5, loss = 7.71315011\n",
      "Iteration 13, loss = 5.10447103\n",
      "Iteration 6, loss = 7.25881418\n",
      "Iteration 14, loss = 5.58234278\n",
      "Iteration 7, loss = 9.81793430\n",
      "Iteration 15, loss = 5.08526937\n",
      "Iteration 8, loss = 6.84413966\n",
      "Iteration 16, loss = 4.07874239\n",
      "Iteration 9, loss = 6.37112912\n",
      "Iteration 17, loss = 3.59841338\n",
      "Iteration 10, loss = 8.01792429\n",
      "Iteration 18, loss = 3.33420540\n",
      "Iteration 11, loss = 6.41841563\n",
      "Iteration 19, loss = 2.99968865\n",
      "Iteration 12, loss = 6.27780241\n",
      "Iteration 20, loss = 3.62281291\n",
      "Iteration 13, loss = 4.56318226\n",
      "Iteration 21, loss = 2.95773235\n",
      "Iteration 14, loss = 3.87915646\n",
      "Iteration 22, loss = 2.91086724\n",
      "Iteration 15, loss = 4.61181112\n",
      "Iteration 23, loss = 2.26089935\n",
      "Iteration 16, loss = 4.72565908\n",
      "Iteration 24, loss = 1.91504849\n",
      "Iteration 17, loss = 3.51455678\n",
      "Iteration 25, loss = 1.80670032\n",
      "Iteration 18, loss = 3.10203361\n",
      "Iteration 26, loss = 1.70896734\n",
      "Iteration 19, loss = 2.75573492\n",
      "Iteration 27, loss = 1.51596001\n",
      "Iteration 20, loss = 2.55121601\n",
      "Iteration 28, loss = 1.66913165\n",
      "Iteration 21, loss = 2.03178610\n",
      "Iteration 29, loss = 2.16458573\n",
      "Iteration 22, loss = 2.96425333\n",
      "Iteration 30, loss = 2.40892593\n",
      "Iteration 23, loss = 2.36535607\n",
      "Iteration 31, loss = 1.42094080\n",
      "Iteration 24, loss = 2.50306404\n",
      "Iteration 32, loss = 1.64956338\n",
      "Iteration 25, loss = 3.14662932\n",
      "Iteration 33, loss = 1.21512750\n",
      "Iteration 26, loss = 1.82831413\n",
      "Iteration 34, loss = 0.96935763\n",
      "Iteration 27, loss = 1.74006524\n",
      "Iteration 35, loss = 0.90130001\n",
      "Iteration 28, loss = 2.76014491\n",
      "Iteration 36, loss = 0.88852984\n",
      "Iteration 29, loss = 1.70793459\n",
      "Iteration 37, loss = 0.71249992\n",
      "Iteration 30, loss = 2.47101187\n",
      "Iteration 38, loss = 1.25364219\n",
      "Iteration 31, loss = 1.88473733\n",
      "Iteration 39, loss = 1.35669154\n",
      "Iteration 32, loss = 1.91774825\n",
      "Iteration 40, loss = 1.42779728\n",
      "Iteration 33, loss = 1.68029626\n",
      "Iteration 41, loss = 0.88794680\n",
      "Iteration 34, loss = 2.36502596\n",
      "Iteration 42, loss = 0.88459818\n",
      "Iteration 35, loss = 1.89138651\n",
      "Iteration 43, loss = 1.04498764\n",
      "Iteration 36, loss = 1.33000049\n",
      "Iteration 44, loss = 1.38648205\n",
      "Iteration 37, loss = 1.89440650\n",
      "Iteration 45, loss = 0.79095290\n",
      "Iteration 38, loss = 1.60133758\n",
      "Iteration 46, loss = 0.52401301\n",
      "Iteration 39, loss = 1.26501465\n",
      "Iteration 47, loss = 0.87393904\n",
      "Iteration 40, loss = 1.36808762\n",
      "Iteration 48, loss = 1.22473634\n",
      "Iteration 41, loss = 1.73158529\n",
      "Iteration 49, loss = 1.07729447\n",
      "Iteration 42, loss = 1.92574950\n",
      "Iteration 50, loss = 1.09243200\n",
      "Iteration 43, loss = 3.23316346\n",
      "Iteration 51, loss = 0.85345873\n",
      "Iteration 44, loss = 2.69441085\n",
      "Iteration 52, loss = 0.58217316\n",
      "Iteration 45, loss = 2.83169020\n",
      "Iteration 53, loss = 0.65514923\n",
      "Iteration 46, loss = 2.83461935\n",
      "Iteration 54, loss = 0.52288979\n",
      "Iteration 47, loss = 2.74900542\n",
      "Iteration 55, loss = 0.49000395\n",
      "Iteration 48, loss = 1.94151604\n",
      "Iteration 56, loss = 0.39294499\n",
      "Iteration 49, loss = 1.64580949\n",
      "Iteration 57, loss = 0.35482780\n",
      "Iteration 50, loss = 2.29776058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.27351370\n",
      "Iteration 59, loss = 0.36509127\n",
      "Iteration 60, loss = 0.33660016\n",
      "Iteration 61, loss = 0.30176454\n",
      "Iteration 62, loss = 0.39916374\n",
      "Iteration 63, loss = 0.34048860\n",
      "Iteration 64, loss = 0.65675586\n",
      "Iteration 65, loss = 0.57375492\n",
      "Iteration 66, loss = 0.46539792\n",
      "Iteration 67, loss = 0.46071296\n",
      "Iteration 68, loss = 0.96573981\n",
      "Iteration 69, loss = 0.98258568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.66209376\n",
      "Iteration 2, loss = 14.60938395\n",
      "Iteration 3, loss = 12.49191017\n",
      "Iteration 4, loss = 8.68671350\n",
      "Iteration 5, loss = 9.56155101\n",
      "Iteration 6, loss = 7.75739868\n",
      "Iteration 7, loss = 7.31535933\n",
      "Iteration 8, loss = 5.88834478\n",
      "Iteration 9, loss = 6.90894005\n",
      "Iteration 10, loss = 5.91819767\n",
      "Iteration 11, loss = 5.34639562\n",
      "Iteration 12, loss = 4.66431250\n",
      "Iteration 1, loss = 15.73831538\n",
      "Iteration 13, loss = 4.77690810\n",
      "Iteration 14, loss = 4.97075323\n",
      "Iteration 2, loss = 10.59545655\n",
      "Iteration 15, loss = 4.92574706Iteration 3, loss = 11.63410803\n",
      "\n",
      "Iteration 4, loss = 9.65600901Iteration 16, loss = 5.96539663\n",
      "\n",
      "Iteration 17, loss = 5.37318508\n",
      "Iteration 5, loss = 11.46684751\n",
      "Iteration 18, loss = 4.55439623\n",
      "Iteration 6, loss = 8.16052625\n",
      "Iteration 19, loss = 4.61992783\n",
      "Iteration 7, loss = 7.38468510\n",
      "Iteration 20, loss = 4.50197175\n",
      "Iteration 8, loss = 7.50891758\n",
      "Iteration 21, loss = 4.21034341\n",
      "Iteration 9, loss = 7.10842727\n",
      "Iteration 22, loss = 2.93771223\n",
      "Iteration 10, loss = 7.21306149\n",
      "Iteration 23, loss = 3.31586275\n",
      "Iteration 11, loss = 4.52574595\n",
      "Iteration 24, loss = 2.72415461\n",
      "Iteration 12, loss = 5.98692166\n",
      "Iteration 25, loss = 2.94531896\n",
      "Iteration 13, loss = 4.09485451\n",
      "Iteration 26, loss = 2.99978424\n",
      "Iteration 14, loss = 3.76656994\n",
      "Iteration 27, loss = 3.26775195\n",
      "Iteration 15, loss = 3.89833903\n",
      "Iteration 28, loss = 3.52034709\n",
      "Iteration 16, loss = 2.67824078\n",
      "Iteration 29, loss = 2.43809021\n",
      "Iteration 17, loss = 4.21888457\n",
      "Iteration 30, loss = 2.08987438\n",
      "Iteration 18, loss = 2.82078331\n",
      "Iteration 31, loss = 2.51354545\n",
      "Iteration 19, loss = 3.61213019\n",
      "Iteration 32, loss = 2.07190651\n",
      "Iteration 20, loss = 3.37781398\n",
      "Iteration 33, loss = 1.74082914\n",
      "Iteration 21, loss = 2.61907456\n",
      "Iteration 34, loss = 1.42830719\n",
      "Iteration 22, loss = 2.24742635\n",
      "Iteration 35, loss = 1.63826567\n",
      "Iteration 23, loss = 1.89611163\n",
      "Iteration 36, loss = 1.52631237\n",
      "Iteration 24, loss = 1.75757619\n",
      "Iteration 37, loss = 1.92169557\n",
      "Iteration 25, loss = 1.95175646\n",
      "Iteration 38, loss = 1.97994390\n",
      "Iteration 26, loss = 2.58445325\n",
      "Iteration 39, loss = 1.61037872\n",
      "Iteration 27, loss = 2.26573305\n",
      "Iteration 40, loss = 2.17520122\n",
      "Iteration 28, loss = 2.24144260\n",
      "Iteration 41, loss = 2.10356836\n",
      "Iteration 29, loss = 2.39404328\n",
      "Iteration 42, loss = 1.46178011\n",
      "Iteration 30, loss = 1.94770365\n",
      "Iteration 43, loss = 1.64199506\n",
      "Iteration 31, loss = 1.57185205\n",
      "Iteration 44, loss = 1.21750551\n",
      "Iteration 32, loss = 1.35142941\n",
      "Iteration 45, loss = 1.26610814\n",
      "Iteration 33, loss = 1.16534704\n",
      "Iteration 46, loss = 0.91851347\n",
      "Iteration 34, loss = 0.99687159\n",
      "Iteration 47, loss = 0.97320840\n",
      "Iteration 35, loss = 1.19616699\n",
      "Iteration 48, loss = 0.88376646\n",
      "Iteration 36, loss = 1.36105429\n",
      "Iteration 49, loss = 1.17621102\n",
      "Iteration 37, loss = 1.51702295\n",
      "Iteration 50, loss = 2.90242789\n",
      "Iteration 38, loss = 1.17697727\n",
      "Iteration 51, loss = 2.84217891\n",
      "Iteration 39, loss = 1.16326645\n",
      "Iteration 52, loss = 2.47408992\n",
      "Iteration 40, loss = 1.07444986\n",
      "Iteration 53, loss = 2.55470890\n",
      "Iteration 41, loss = 0.87418289\n",
      "Iteration 54, loss = 2.83006545\n",
      "Iteration 42, loss = 0.99058396\n",
      "Iteration 55, loss = 2.31145984\n",
      "Iteration 43, loss = 0.91427380\n",
      "Iteration 56, loss = 1.82848579\n",
      "Iteration 44, loss = 0.73775616\n",
      "Iteration 57, loss = 2.50762585\n",
      "Iteration 45, loss = 0.87799074\n",
      "Iteration 58, loss = 1.67111163\n",
      "Iteration 46, loss = 0.98186643\n",
      "Iteration 59, loss = 1.81324691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.86223923\n",
      "Iteration 48, loss = 0.56592612\n",
      "Iteration 49, loss = 0.62360017\n",
      "Iteration 50, loss = 0.50744376\n",
      "Iteration 51, loss = 0.43484629\n",
      "Iteration 52, loss = 0.45012743\n",
      "Iteration 53, loss = 0.74443275\n",
      "Iteration 54, loss = 0.51891822\n",
      "Iteration 55, loss = 0.50109146\n",
      "Iteration 56, loss = 0.64319630\n",
      "Iteration 57, loss = 0.66031900\n",
      "Iteration 58, loss = 0.34608066\n",
      "Iteration 59, loss = 0.42662909\n",
      "Iteration 60, loss = 0.35888712\n",
      "Iteration 61, loss = 0.58591784\n",
      "Iteration 62, loss = 0.66168924\n",
      "Iteration 63, loss = 0.55398864\n",
      "Iteration 64, loss = 0.78352618\n",
      "Iteration 1, loss = 16.99264530\n",
      "Iteration 65, loss = 0.67354772\n",
      "Iteration 2, loss = 13.86296914\n",
      "Iteration 66, loss = 0.82493124\n",
      "Iteration 3, loss = 11.42845977\n",
      "Iteration 67, loss = 1.13829032\n",
      "Iteration 4, loss = 8.56755118\n",
      "Iteration 68, loss = 1.44601689\n",
      "Iteration 5, loss = 7.18632920\n",
      "Iteration 69, loss = 1.40428432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 8.77684389\n",
      "Iteration 7, loss = 8.56933517\n",
      "Iteration 8, loss = 12.26160077\n",
      "Iteration 9, loss = 11.57542378\n",
      "Iteration 10, loss = 8.73354323\n",
      "Iteration 11, loss = 6.59178564\n",
      "Iteration 12, loss = 5.63019732\n",
      "Iteration 13, loss = 6.32874506\n",
      "Iteration 14, loss = 8.26738757\n",
      "Iteration 15, loss = 7.49286492\n",
      "Iteration 16, loss = 5.47101269\n",
      "Iteration 17, loss = 4.67919726\n",
      "Iteration 18, loss = 4.81469366\n",
      "Iteration 19, loss = 4.27790954\n",
      "Iteration 20, loss = 3.94164449\n",
      "Iteration 21, loss = 3.03746461\n",
      "Iteration 22, loss = 3.36627467\n",
      "Iteration 23, loss = 3.23463087\n",
      "Iteration 1, loss = 14.77153073\n",
      "Iteration 24, loss = 2.31598755\n",
      "Iteration 2, loss = 13.64185680\n",
      "Iteration 25, loss = 2.65080406\n",
      "Iteration 3, loss = 13.43097029\n",
      "Iteration 26, loss = 2.25079565\n",
      "Iteration 4, loss = 14.14655295\n",
      "Iteration 27, loss = 2.24938833\n",
      "Iteration 5, loss = 11.37406842\n",
      "Iteration 28, loss = 1.94601600\n",
      "Iteration 6, loss = 10.85235575\n",
      "Iteration 29, loss = 2.16359820\n",
      "Iteration 7, loss = 7.63450107\n",
      "Iteration 30, loss = 1.91933754\n",
      "Iteration 8, loss = 5.29293425\n",
      "Iteration 31, loss = 1.93165699\n",
      "Iteration 9, loss = 5.19782485\n",
      "Iteration 32, loss = 1.42891460\n",
      "Iteration 10, loss = 6.26868420\n",
      "Iteration 33, loss = 1.67109019\n",
      "Iteration 11, loss = 6.43784434\n",
      "Iteration 34, loss = 1.86391489\n",
      "Iteration 12, loss = 7.38656407\n",
      "Iteration 35, loss = 2.05611121\n",
      "Iteration 13, loss = 6.07194098\n",
      "Iteration 36, loss = 1.71322490\n",
      "Iteration 14, loss = 4.48128363\n",
      "Iteration 37, loss = 1.74038671\n",
      "Iteration 15, loss = 4.61564111\n",
      "Iteration 38, loss = 1.63938464\n",
      "Iteration 16, loss = 3.65985188\n",
      "Iteration 39, loss = 1.21357987\n",
      "Iteration 17, loss = 4.70158977\n",
      "Iteration 40, loss = 1.25325108\n",
      "Iteration 18, loss = 5.31204438\n",
      "Iteration 41, loss = 1.58632949\n",
      "Iteration 19, loss = 4.62420445\n",
      "Iteration 42, loss = 1.89693510\n",
      "Iteration 20, loss = 3.88256845\n",
      "Iteration 43, loss = 1.52103646\n",
      "Iteration 21, loss = 3.50439774\n",
      "Iteration 44, loss = 1.09750881\n",
      "Iteration 22, loss = 2.67407808\n",
      "Iteration 45, loss = 1.15078677\n",
      "Iteration 23, loss = 2.69209017\n",
      "Iteration 46, loss = 1.07910558\n",
      "Iteration 24, loss = 2.67958729\n",
      "Iteration 47, loss = 1.01804839\n",
      "Iteration 25, loss = 2.72365571\n",
      "Iteration 48, loss = 1.10916797\n",
      "Iteration 26, loss = 2.43314746\n",
      "Iteration 49, loss = 1.00631980\n",
      "Iteration 27, loss = 2.31715325\n",
      "Iteration 50, loss = 1.12585574\n",
      "Iteration 28, loss = 2.23614254\n",
      "Iteration 51, loss = 1.12525373\n",
      "Iteration 29, loss = 2.23606065\n",
      "Iteration 52, loss = 0.88191311\n",
      "Iteration 30, loss = 1.93975461\n",
      "Iteration 53, loss = 1.48495812\n",
      "Iteration 31, loss = 1.80728847\n",
      "Iteration 54, loss = 1.01315906\n",
      "Iteration 32, loss = 2.06640397\n",
      "Iteration 55, loss = 1.31682944\n",
      "Iteration 33, loss = 1.85687482\n",
      "Iteration 56, loss = 1.68978531\n",
      "Iteration 34, loss = 1.60898393\n",
      "Iteration 57, loss = 1.95063294\n",
      "Iteration 35, loss = 1.16543662\n",
      "Iteration 58, loss = 1.90227732\n",
      "Iteration 36, loss = 1.25441941\n",
      "Iteration 59, loss = 1.52629774\n",
      "Iteration 37, loss = 1.29403901\n",
      "Iteration 60, loss = 1.09871443\n",
      "Iteration 38, loss = 1.21633495\n",
      "Iteration 61, loss = 1.20423266\n",
      "Iteration 39, loss = 1.22119584\n",
      "Iteration 62, loss = 0.97901867\n",
      "Iteration 40, loss = 1.30864837\n",
      "Iteration 63, loss = 1.01596899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 1.35613480\n",
      "Iteration 42, loss = 1.19883425\n",
      "Iteration 43, loss = 0.89230470\n",
      "Iteration 44, loss = 0.71699742\n",
      "Iteration 45, loss = 0.74301090\n",
      "Iteration 46, loss = 1.11723930\n",
      "Iteration 47, loss = 0.72830738\n",
      "Iteration 48, loss = 0.64517665\n",
      "Iteration 49, loss = 0.72517106\n",
      "Iteration 50, loss = 0.63711334\n",
      "Iteration 51, loss = 0.86860803\n",
      "Iteration 52, loss = 1.48123166\n",
      "Iteration 53, loss = 1.16958561\n",
      "Iteration 54, loss = 1.27007079\n",
      "Iteration 55, loss = 0.84659209\n",
      "Iteration 56, loss = 0.96849914\n",
      "Iteration 57, loss = 0.84924151\n",
      "Iteration 58, loss = 0.90722093\n",
      "Iteration 1, loss = 15.02189663\n",
      "Iteration 59, loss = 1.24401935\n",
      "Iteration 2, loss = 13.62561614\n",
      "Iteration 60, loss = 0.74664473\n",
      "Iteration 3, loss = 15.12339401\n",
      "Iteration 61, loss = 0.66526213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 12.20985401\n",
      "Iteration 5, loss = 9.04826645\n",
      "Iteration 6, loss = 8.26622951\n",
      "Iteration 7, loss = 7.22926103\n",
      "Iteration 8, loss = 8.58210103\n",
      "Iteration 9, loss = 11.50377729\n",
      "Iteration 10, loss = 8.44151935\n",
      "Iteration 11, loss = 6.71219945\n",
      "Iteration 12, loss = 5.47856007\n",
      "Iteration 13, loss = 7.24856209\n",
      "Iteration 14, loss = 5.64078544\n",
      "Iteration 15, loss = 4.88846727\n",
      "Iteration 16, loss = 4.57177922\n",
      "Iteration 17, loss = 4.64823943\n",
      "Iteration 18, loss = 3.58561875\n",
      "Iteration 19, loss = 3.35579645\n",
      "Iteration 20, loss = 3.01810006\n",
      "Iteration 21, loss = 2.12875090\n",
      "Iteration 1, loss = 18.45378028\n",
      "Iteration 22, loss = 2.30121370\n",
      "Iteration 2, loss = 16.07271277\n",
      "Iteration 23, loss = 1.89828748\n",
      "Iteration 3, loss = 11.01742142\n",
      "Iteration 24, loss = 2.93140955\n",
      "Iteration 4, loss = 7.58053892\n",
      "Iteration 25, loss = 2.88054662\n",
      "Iteration 5, loss = 9.36055651\n",
      "Iteration 26, loss = 2.45201474\n",
      "Iteration 6, loss = 8.70029928\n",
      "Iteration 27, loss = 1.65171639\n",
      "Iteration 7, loss = 6.50934956\n",
      "Iteration 28, loss = 1.47753972\n",
      "Iteration 8, loss = 7.78425041\n",
      "Iteration 29, loss = 1.83089903\n",
      "Iteration 9, loss = 5.59313988\n",
      "Iteration 30, loss = 1.50684022\n",
      "Iteration 10, loss = 5.28107260\n",
      "Iteration 31, loss = 1.73156848\n",
      "Iteration 11, loss = 5.00707787\n",
      "Iteration 32, loss = 1.44346223\n",
      "Iteration 12, loss = 4.99704929\n",
      "Iteration 33, loss = 1.14566703\n",
      "Iteration 13, loss = 4.50799968\n",
      "Iteration 34, loss = 1.14182363\n",
      "Iteration 14, loss = 4.87675002\n",
      "Iteration 35, loss = 1.05149035\n",
      "Iteration 15, loss = 4.31405160\n",
      "Iteration 36, loss = 0.79859737\n",
      "Iteration 16, loss = 6.25758547\n",
      "Iteration 37, loss = 0.83372601\n",
      "Iteration 17, loss = 4.74357524\n",
      "Iteration 38, loss = 0.72388065\n",
      "Iteration 18, loss = 3.47117175\n",
      "Iteration 39, loss = 0.76707308\n",
      "Iteration 19, loss = 2.74373179\n",
      "Iteration 40, loss = 1.05244930\n",
      "Iteration 20, loss = 2.54528078\n",
      "Iteration 41, loss = 0.89438267\n",
      "Iteration 21, loss = 2.55411235\n",
      "Iteration 42, loss = 0.95669538\n",
      "Iteration 22, loss = 2.03574257\n",
      "Iteration 43, loss = 0.86199548\n",
      "Iteration 23, loss = 1.65870686\n",
      "Iteration 44, loss = 0.52407009\n",
      "Iteration 24, loss = 1.43546812\n",
      "Iteration 45, loss = 0.46273813\n",
      "Iteration 25, loss = 1.16320711\n",
      "Iteration 46, loss = 0.51561248\n",
      "Iteration 26, loss = 1.73276769\n",
      "Iteration 47, loss = 0.42765711\n",
      "Iteration 27, loss = 1.18934350\n",
      "Iteration 48, loss = 0.39442974\n",
      "Iteration 28, loss = 1.08180026\n",
      "Iteration 49, loss = 0.33039266\n",
      "Iteration 29, loss = 1.66046928\n",
      "Iteration 50, loss = 0.37731026\n",
      "Iteration 30, loss = 1.69305465\n",
      "Iteration 51, loss = 0.51164708\n",
      "Iteration 31, loss = 1.82211336\n",
      "Iteration 52, loss = 0.37460534\n",
      "Iteration 32, loss = 1.47490207\n",
      "Iteration 53, loss = 0.36848997\n",
      "Iteration 33, loss = 1.95105793\n",
      "Iteration 54, loss = 0.29028194\n",
      "Iteration 34, loss = 1.43879059\n",
      "Iteration 55, loss = 0.31855468\n",
      "Iteration 35, loss = 1.66309226\n",
      "Iteration 56, loss = 0.35867347\n",
      "Iteration 36, loss = 2.21240574\n",
      "Iteration 57, loss = 0.30526185\n",
      "Iteration 37, loss = 2.23733828\n",
      "Iteration 58, loss = 0.26742357\n",
      "Iteration 38, loss = 1.73567988\n",
      "Iteration 59, loss = 0.30814790\n",
      "Iteration 39, loss = 1.61265325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 0.35952337\n",
      "Iteration 61, loss = 0.20854571\n",
      "Iteration 62, loss = 0.20676146\n",
      "Iteration 63, loss = 0.16618220\n",
      "Iteration 64, loss = 0.18197784\n",
      "Iteration 65, loss = 0.15871836\n",
      "Iteration 66, loss = 0.20437471\n",
      "Iteration 67, loss = 0.16919069\n",
      "Iteration 68, loss = 0.19048173\n",
      "Iteration 69, loss = 0.14953131\n",
      "Iteration 70, loss = 0.12849165\n",
      "Iteration 71, loss = 0.11172205\n",
      "Iteration 72, loss = 0.12070247\n",
      "Iteration 73, loss = 0.17526589\n",
      "Iteration 74, loss = 0.14765792\n",
      "Iteration 75, loss = 0.13966598\n",
      "Iteration 76, loss = 0.16932663\n",
      "Iteration 77, loss = 0.16067916\n",
      "Iteration 1, loss = 17.78608555\n",
      "Iteration 78, loss = 0.16913483\n",
      "Iteration 2, loss = 20.15827711\n",
      "Iteration 79, loss = 0.13291018\n",
      "Iteration 3, loss = 15.11340942\n",
      "Iteration 80, loss = 0.11671940\n",
      "Iteration 4, loss = 8.32102794\n",
      "Iteration 81, loss = 0.12970046\n",
      "Iteration 5, loss = 8.77776529\n",
      "Iteration 82, loss = 0.10963534\n",
      "Iteration 6, loss = 10.19781862\n",
      "Iteration 83, loss = 0.13894092\n",
      "Iteration 7, loss = 11.06713566\n",
      "Iteration 84, loss = 0.21013434\n",
      "Iteration 8, loss = 12.05033184\n",
      "Iteration 85, loss = 0.19560969\n",
      "Iteration 9, loss = 10.41415538\n",
      "Iteration 86, loss = 0.16811743\n",
      "Iteration 10, loss = 11.48561833\n",
      "Iteration 87, loss = 0.13782322\n",
      "Iteration 11, loss = 7.39406735\n",
      "Iteration 88, loss = 0.14876562\n",
      "Iteration 12, loss = 6.68251189\n",
      "Iteration 89, loss = 0.14410195\n",
      "Iteration 13, loss = 7.62421010\n",
      "Iteration 90, loss = 0.16009970\n",
      "Iteration 14, loss = 5.93368511\n",
      "Iteration 91, loss = 0.15537836\n",
      "Iteration 15, loss = 4.88619458\n",
      "Iteration 92, loss = 0.16209368\n",
      "Iteration 16, loss = 4.52589787\n",
      "Iteration 93, loss = 0.28461528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 3.71009596\n",
      "Iteration 18, loss = 5.65169271\n",
      "Iteration 19, loss = 5.01260483\n",
      "Iteration 20, loss = 3.77333668\n",
      "Iteration 21, loss = 3.90111321\n",
      "Iteration 22, loss = 3.24157187\n",
      "Iteration 23, loss = 4.49059888\n",
      "Iteration 24, loss = 4.15393884\n",
      "Iteration 25, loss = 3.37221924\n",
      "Iteration 26, loss = 3.99738654\n",
      "Iteration 27, loss = 3.25922898\n",
      "Iteration 28, loss = 3.68764930\n",
      "Iteration 29, loss = 3.72288885\n",
      "Iteration 30, loss = 3.23310885\n",
      "Iteration 31, loss = 2.86726103\n",
      "Iteration 32, loss = 2.29773583\n",
      "Iteration 33, loss = 3.05722320\n",
      "Iteration 34, loss = 1.95517382\n",
      "Iteration 1, loss = 18.04237083\n",
      "Iteration 35, loss = 2.40698915\n",
      "Iteration 2, loss = 14.97760684\n",
      "Iteration 36, loss = 2.41618781\n",
      "Iteration 3, loss = 11.65462001\n",
      "Iteration 37, loss = 2.81579229\n",
      "Iteration 4, loss = 7.80229074\n",
      "Iteration 38, loss = 2.28762238\n",
      "Iteration 5, loss = 6.44615217\n",
      "Iteration 39, loss = 2.99350062\n",
      "Iteration 6, loss = 5.72743289\n",
      "Iteration 40, loss = 2.73350864\n",
      "Iteration 7, loss = 9.77541992\n",
      "Iteration 41, loss = 2.96549756\n",
      "Iteration 8, loss = 7.82154923\n",
      "Iteration 42, loss = 2.54784184\n",
      "Iteration 9, loss = 6.61826757\n",
      "Iteration 43, loss = 2.61258408\n",
      "Iteration 10, loss = 5.18359003\n",
      "Iteration 44, loss = 2.21017702\n",
      "Iteration 11, loss = 5.48388681\n",
      "Iteration 45, loss = 1.43253852\n",
      "Iteration 12, loss = 5.21443847\n",
      "Iteration 46, loss = 1.83166394\n",
      "Iteration 13, loss = 6.04461106\n",
      "Iteration 47, loss = 1.41080054\n",
      "Iteration 14, loss = 4.39285377\n",
      "Iteration 48, loss = 1.51905844\n",
      "Iteration 15, loss = 4.12173273\n",
      "Iteration 49, loss = 1.35667044\n",
      "Iteration 16, loss = 4.68487391\n",
      "Iteration 50, loss = 1.19287310\n",
      "Iteration 17, loss = 3.59274669\n",
      "Iteration 51, loss = 1.32533146\n",
      "Iteration 18, loss = 3.50634255\n",
      "Iteration 52, loss = 1.41253878\n",
      "Iteration 19, loss = 2.72851134\n",
      "Iteration 53, loss = 1.49963183\n",
      "Iteration 20, loss = 2.59232134\n",
      "Iteration 54, loss = 1.56665567\n",
      "Iteration 21, loss = 2.03674717\n",
      "Iteration 55, loss = 1.41485714\n",
      "Iteration 22, loss = 1.67705879\n",
      "Iteration 56, loss = 1.25599873\n",
      "Iteration 23, loss = 1.90509663\n",
      "Iteration 57, loss = 1.20644772\n",
      "Iteration 24, loss = 1.77740586\n",
      "Iteration 58, loss = 1.17604489\n",
      "Iteration 25, loss = 3.23185533\n",
      "Iteration 59, loss = 1.36159221\n",
      "Iteration 26, loss = 2.48786449\n",
      "Iteration 60, loss = 1.00173843\n",
      "Iteration 27, loss = 2.41984476\n",
      "Iteration 61, loss = 1.14688126\n",
      "Iteration 28, loss = 2.21584207\n",
      "Iteration 62, loss = 1.50525478\n",
      "Iteration 29, loss = 1.56984516\n",
      "Iteration 63, loss = 1.40004868\n",
      "Iteration 30, loss = 1.68002789\n",
      "Iteration 64, loss = 1.48601481\n",
      "Iteration 31, loss = 1.04994208\n",
      "Iteration 65, loss = 1.20299680\n",
      "Iteration 32, loss = 1.58295915\n",
      "Iteration 66, loss = 1.12434250\n",
      "Iteration 33, loss = 1.85025033\n",
      "Iteration 67, loss = 0.89775241\n",
      "Iteration 34, loss = 1.25959797\n",
      "Iteration 68, loss = 0.93984344\n",
      "Iteration 35, loss = 1.07468110\n",
      "Iteration 69, loss = 0.78814772\n",
      "Iteration 36, loss = 1.49101922\n",
      "Iteration 70, loss = 0.56487739\n",
      "Iteration 37, loss = 0.88559199\n",
      "Iteration 71, loss = 0.57876688\n",
      "Iteration 38, loss = 0.64669173\n",
      "Iteration 72, loss = 0.53760728\n",
      "Iteration 39, loss = 0.76074896\n",
      "Iteration 73, loss = 0.58909891\n",
      "Iteration 40, loss = 1.00574847\n",
      "Iteration 74, loss = 0.77201436\n",
      "Iteration 41, loss = 0.90473020\n",
      "Iteration 75, loss = 0.44368116\n",
      "Iteration 42, loss = 0.77988199\n",
      "Iteration 76, loss = 0.57215472\n",
      "Iteration 43, loss = 0.73584534\n",
      "Iteration 77, loss = 1.12445704\n",
      "Iteration 44, loss = 0.53806292\n",
      "Iteration 78, loss = 1.39029845\n",
      "Iteration 45, loss = 0.54820644\n",
      "Iteration 79, loss = 0.90581498\n",
      "Iteration 46, loss = 0.32115743\n",
      "Iteration 80, loss = 0.77428245\n",
      "Iteration 47, loss = 0.39973425\n",
      "Iteration 81, loss = 0.70765759\n",
      "Iteration 48, loss = 0.46462054\n",
      "Iteration 82, loss = 0.93393999\n",
      "Iteration 49, loss = 0.50838825\n",
      "Iteration 83, loss = 1.05703833\n",
      "Iteration 50, loss = 0.47386737\n",
      "Iteration 84, loss = 0.92298501\n",
      "Iteration 51, loss = 0.41172502\n",
      "Iteration 85, loss = 0.92256996\n",
      "Iteration 52, loss = 0.39520831\n",
      "Iteration 86, loss = 0.79616789\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.39023304\n",
      "Iteration 54, loss = 0.51139959\n",
      "Iteration 55, loss = 0.49192625\n",
      "Iteration 56, loss = 0.29484944\n",
      "Iteration 57, loss = 0.32157095\n",
      "Iteration 58, loss = 0.34373033\n",
      "Iteration 59, loss = 0.34308891\n",
      "Iteration 60, loss = 0.34606790\n",
      "Iteration 61, loss = 0.31165142\n",
      "Iteration 62, loss = 0.23461415\n",
      "Iteration 63, loss = 0.20530182\n",
      "Iteration 64, loss = 0.13444799\n",
      "Iteration 65, loss = 0.25228329\n",
      "Iteration 66, loss = 0.22563290\n",
      "Iteration 67, loss = 0.22342913\n",
      "Iteration 68, loss = 0.38184519\n",
      "Iteration 69, loss = 0.25405139\n",
      "Iteration 70, loss = 0.33864098\n",
      "Iteration 1, loss = 18.24660735\n",
      "Iteration 71, loss = 0.28953125\n",
      "Iteration 2, loss = 13.64194205\n",
      "Iteration 72, loss = 0.50705851\n",
      "Iteration 3, loss = 9.30936650\n",
      "Iteration 73, loss = 0.47065147\n",
      "Iteration 4, loss = 9.67643414\n",
      "Iteration 74, loss = 0.33987928\n",
      "Iteration 5, loss = 10.24901691\n",
      "Iteration 75, loss = 0.37685478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 6.86789024\n",
      "Iteration 7, loss = 5.94952548\n",
      "Iteration 8, loss = 5.37713582\n",
      "Iteration 9, loss = 4.99352169\n",
      "Iteration 10, loss = 8.07418399\n",
      "Iteration 11, loss = 7.09918310\n",
      "Iteration 12, loss = 5.53930399\n",
      "Iteration 13, loss = 4.56745015\n",
      "Iteration 14, loss = 3.92692932\n",
      "Iteration 15, loss = 3.68048929\n",
      "Iteration 16, loss = 3.92558985\n",
      "Iteration 17, loss = 3.80079853\n",
      "Iteration 18, loss = 4.15399724\n",
      "Iteration 19, loss = 3.08248053\n",
      "Iteration 20, loss = 2.78009884\n",
      "Iteration 21, loss = 2.38497052\n",
      "Iteration 22, loss = 2.70185822\n",
      "Iteration 23, loss = 2.27122141\n",
      "Iteration 1, loss = 18.03234864\n",
      "Iteration 24, loss = 2.30396886\n",
      "Iteration 2, loss = 21.07900715\n",
      "Iteration 25, loss = 1.89358518\n",
      "Iteration 3, loss = 16.70718339\n",
      "Iteration 26, loss = 1.45971082\n",
      "Iteration 4, loss = 9.06154225\n",
      "Iteration 27, loss = 1.81569662\n",
      "Iteration 5, loss = 7.04092745\n",
      "Iteration 28, loss = 1.36044536\n",
      "Iteration 6, loss = 7.19696898\n",
      "Iteration 29, loss = 1.69702368\n",
      "Iteration 7, loss = 7.20530304\n",
      "Iteration 30, loss = 2.09829893\n",
      "Iteration 8, loss = 7.63957801\n",
      "Iteration 31, loss = 1.41338222\n",
      "Iteration 9, loss = 8.16514196\n",
      "Iteration 32, loss = 1.40706448\n",
      "Iteration 10, loss = 5.25331978\n",
      "Iteration 33, loss = 2.15724426\n",
      "Iteration 11, loss = 6.86741636\n",
      "Iteration 34, loss = 1.92309931\n",
      "Iteration 12, loss = 4.68920766\n",
      "Iteration 35, loss = 1.24838698\n",
      "Iteration 13, loss = 4.24538265\n",
      "Iteration 36, loss = 1.15584261\n",
      "Iteration 14, loss = 4.73684520\n",
      "Iteration 37, loss = 1.50523327\n",
      "Iteration 15, loss = 3.31602831\n",
      "Iteration 38, loss = 1.17306840\n",
      "Iteration 16, loss = 3.13447279\n",
      "Iteration 39, loss = 1.02507025\n",
      "Iteration 17, loss = 6.21225529\n",
      "Iteration 40, loss = 0.88217958\n",
      "Iteration 18, loss = 4.22602818\n",
      "Iteration 41, loss = 1.27246193\n",
      "Iteration 19, loss = 6.19406626\n",
      "Iteration 42, loss = 1.35402942\n",
      "Iteration 20, loss = 5.40264491\n",
      "Iteration 43, loss = 0.79346752\n",
      "Iteration 21, loss = 4.16412335\n",
      "Iteration 44, loss = 0.76325914\n",
      "Iteration 22, loss = 3.51894531\n",
      "Iteration 45, loss = 0.66734518\n",
      "Iteration 23, loss = 3.07198448\n",
      "Iteration 46, loss = 0.55907457\n",
      "Iteration 24, loss = 2.93077993\n",
      "Iteration 47, loss = 1.33434944\n",
      "Iteration 25, loss = 1.93582966\n",
      "Iteration 48, loss = 0.66694975\n",
      "Iteration 26, loss = 2.10013329\n",
      "Iteration 49, loss = 0.63777078\n",
      "Iteration 27, loss = 1.67213097\n",
      "Iteration 50, loss = 0.43902412\n",
      "Iteration 28, loss = 1.53522486\n",
      "Iteration 51, loss = 0.46647195\n",
      "Iteration 29, loss = 1.31407387\n",
      "Iteration 52, loss = 0.41028797\n",
      "Iteration 30, loss = 1.30603274\n",
      "Iteration 53, loss = 0.43133819\n",
      "Iteration 31, loss = 1.10603777\n",
      "Iteration 54, loss = 0.45624946\n",
      "Iteration 32, loss = 1.19140089\n",
      "Iteration 55, loss = 0.45635105\n",
      "Iteration 33, loss = 1.47333750\n",
      "Iteration 56, loss = 0.52268275\n",
      "Iteration 34, loss = 1.15729565\n",
      "Iteration 57, loss = 0.58614057\n",
      "Iteration 35, loss = 1.11258954\n",
      "Iteration 58, loss = 0.62613708\n",
      "Iteration 36, loss = 0.89525346\n",
      "Iteration 59, loss = 1.13102501\n",
      "Iteration 37, loss = 0.99948724\n",
      "Iteration 60, loss = 0.98177691\n",
      "Iteration 38, loss = 1.08829625\n",
      "Iteration 61, loss = 0.80567689\n",
      "Iteration 39, loss = 1.41485216\n",
      "Iteration 62, loss = 1.02398866\n",
      "Iteration 40, loss = 2.05435777\n",
      "Iteration 63, loss = 0.88248791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 1.20552530\n",
      "Iteration 42, loss = 1.14222464\n",
      "Iteration 43, loss = 1.03856475\n",
      "Iteration 44, loss = 0.97681528\n",
      "Iteration 45, loss = 1.05836835\n",
      "Iteration 46, loss = 1.02787040\n",
      "Iteration 47, loss = 1.20923108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.80977109\n",
      "Iteration 2, loss = 16.36630074\n",
      "Iteration 3, loss = 9.84793118\n",
      "Iteration 4, loss = 12.46293102\n",
      "Iteration 5, loss = 10.89817875\n",
      "Iteration 6, loss = 7.46322841\n",
      "Iteration 7, loss = 7.55569771\n",
      "Iteration 1, loss = 16.38087098\n",
      "Iteration 8, loss = 6.94903808\n",
      "Iteration 2, loss = 13.86372248\n",
      "Iteration 9, loss = 6.09946554\n",
      "Iteration 3, loss = 13.21535831\n",
      "Iteration 10, loss = 5.71373173\n",
      "Iteration 4, loss = 9.10828256\n",
      "Iteration 11, loss = 4.76730792\n",
      "Iteration 5, loss = 8.11854378\n",
      "Iteration 12, loss = 4.20584186\n",
      "Iteration 6, loss = 9.82547007\n",
      "Iteration 13, loss = 4.26775986\n",
      "Iteration 7, loss = 7.18194299\n",
      "Iteration 14, loss = 4.18247078\n",
      "Iteration 8, loss = 9.87557533\n",
      "Iteration 15, loss = 5.01483599\n",
      "Iteration 9, loss = 6.87737366\n",
      "Iteration 16, loss = 4.23298179\n",
      "Iteration 10, loss = 11.26817255\n",
      "Iteration 17, loss = 3.43704433\n",
      "Iteration 11, loss = 7.90101281\n",
      "Iteration 18, loss = 3.42626538\n",
      "Iteration 12, loss = 11.38823946\n",
      "Iteration 19, loss = 3.23773156\n",
      "Iteration 13, loss = 9.28547233\n",
      "Iteration 20, loss = 2.89128070\n",
      "Iteration 14, loss = 5.96028042\n",
      "Iteration 21, loss = 3.88366312\n",
      "Iteration 15, loss = 5.44275191\n",
      "Iteration 22, loss = 3.22255287\n",
      "Iteration 16, loss = 5.04508904\n",
      "Iteration 23, loss = 3.01852532\n",
      "Iteration 17, loss = 4.49257447\n",
      "Iteration 24, loss = 2.79200433\n",
      "Iteration 18, loss = 4.46372433\n",
      "Iteration 25, loss = 2.50230755\n",
      "Iteration 19, loss = 4.40778458\n",
      "Iteration 26, loss = 2.60711540\n",
      "Iteration 20, loss = 4.62559523\n",
      "Iteration 27, loss = 2.84363140\n",
      "Iteration 21, loss = 3.32892721\n",
      "Iteration 22, loss = 2.73191176\n",
      "Iteration 28, loss = 3.47173075\n",
      "Iteration 23, loss = 2.55247346\n",
      "Iteration 29, loss = 2.71731888\n",
      "Iteration 24, loss = 2.16009054\n",
      "Iteration 30, loss = 2.47901465\n",
      "Iteration 25, loss = 1.58619974\n",
      "Iteration 31, loss = 1.70061246\n",
      "Iteration 26, loss = 1.56730702\n",
      "Iteration 32, loss = 2.19932174\n",
      "Iteration 27, loss = 1.74199155\n",
      "Iteration 33, loss = 2.10863913\n",
      "Iteration 28, loss = 1.10001284\n",
      "Iteration 34, loss = 1.77447416\n",
      "Iteration 29, loss = 0.95720347\n",
      "Iteration 35, loss = 1.40871102\n",
      "Iteration 30, loss = 1.04202300\n",
      "Iteration 36, loss = 1.91125114\n",
      "Iteration 31, loss = 1.00620379\n",
      "Iteration 37, loss = 1.30067128\n",
      "Iteration 32, loss = 0.85581358\n",
      "Iteration 38, loss = 1.87113415\n",
      "Iteration 33, loss = 0.85525596\n",
      "Iteration 39, loss = 1.76464362\n",
      "Iteration 34, loss = 1.34354656\n",
      "Iteration 40, loss = 1.81100173\n",
      "Iteration 35, loss = 1.30700292\n",
      "Iteration 41, loss = 1.45654903\n",
      "Iteration 36, loss = 1.56619959\n",
      "Iteration 42, loss = 1.72776213\n",
      "Iteration 37, loss = 1.50382627\n",
      "Iteration 43, loss = 2.02708919\n",
      "Iteration 38, loss = 1.23820567\n",
      "Iteration 44, loss = 2.49255404\n",
      "Iteration 39, loss = 0.99444268\n",
      "Iteration 45, loss = 2.58634601\n",
      "Iteration 46, loss = 2.37266275\n",
      "Iteration 40, loss = 0.86259978\n",
      "Iteration 47, loss = 2.34247580\n",
      "Iteration 41, loss = 0.81128593\n",
      "Iteration 48, loss = 2.17039664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.86253835\n",
      "Iteration 43, loss = 0.74256492\n",
      "Iteration 44, loss = 0.68000869\n",
      "Iteration 45, loss = 0.56573694\n",
      "Iteration 46, loss = 0.49739823\n",
      "Iteration 47, loss = 0.59601446\n",
      "Iteration 48, loss = 0.64983239\n",
      "Iteration 49, loss = 0.63224681\n",
      "Iteration 50, loss = 0.52364795\n",
      "Iteration 51, loss = 0.52733468\n",
      "Iteration 52, loss = 0.51633600\n",
      "Iteration 53, loss = 0.51602484\n",
      "Iteration 54, loss = 0.59211573\n",
      "Iteration 55, loss = 0.69218773\n",
      "Iteration 56, loss = 0.70343658\n",
      "Iteration 57, loss = 0.67431841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.73197049\n",
      "Iteration 2, loss = 14.01198629\n",
      "Iteration 3, loss = 14.78921865\n",
      "Iteration 4, loss = 10.77857625\n",
      "Iteration 5, loss = 9.82434852\n",
      "Iteration 6, loss = 7.64741851\n",
      "Iteration 7, loss = 6.33615001\n",
      "Iteration 8, loss = 6.31034227\n",
      "Iteration 9, loss = 6.54410549\n",
      "Iteration 10, loss = 7.54991638\n",
      "Iteration 11, loss = 6.65187843\n",
      "Iteration 12, loss = 5.04306227\n",
      "Iteration 13, loss = 4.00081375\n",
      "Iteration 14, loss = 3.35817369\n",
      "Iteration 15, loss = 3.18489147\n",
      "Iteration 16, loss = 3.20770490\n",
      "Iteration 1, loss = 18.39372948\n",
      "Iteration 17, loss = 2.51506138\n",
      "Iteration 2, loss = 15.82698667\n",
      "Iteration 18, loss = 2.75135077\n",
      "Iteration 3, loss = 9.68126783\n",
      "Iteration 19, loss = 2.47847957\n",
      "Iteration 4, loss = 12.64169644\n",
      "Iteration 20, loss = 3.05013446\n",
      "Iteration 5, loss = 8.74278089\n",
      "Iteration 21, loss = 3.12858847\n",
      "Iteration 6, loss = 5.59959583\n",
      "Iteration 22, loss = 2.82273368\n",
      "Iteration 7, loss = 6.65414934\n",
      "Iteration 23, loss = 2.15620093\n",
      "Iteration 8, loss = 7.58403984\n",
      "Iteration 24, loss = 1.67983229\n",
      "Iteration 9, loss = 5.40416639\n",
      "Iteration 25, loss = 1.40591620\n",
      "Iteration 10, loss = 6.20427779\n",
      "Iteration 26, loss = 1.51354878\n",
      "Iteration 11, loss = 5.57335363\n",
      "Iteration 27, loss = 1.58562572\n",
      "Iteration 12, loss = 5.17646603\n",
      "Iteration 28, loss = 1.10663382\n",
      "Iteration 13, loss = 7.59443197\n",
      "Iteration 29, loss = 0.84819907\n",
      "Iteration 14, loss = 7.22825735\n",
      "Iteration 30, loss = 1.13373011\n",
      "Iteration 15, loss = 6.53406982\n",
      "Iteration 31, loss = 1.37305987\n",
      "Iteration 16, loss = 5.35469605\n",
      "Iteration 32, loss = 1.08853062\n",
      "Iteration 17, loss = 3.97590821\n",
      "Iteration 33, loss = 0.88384653\n",
      "Iteration 18, loss = 3.94932588\n",
      "Iteration 34, loss = 0.82578176\n",
      "Iteration 19, loss = 3.65793769\n",
      "Iteration 35, loss = 0.79682558\n",
      "Iteration 20, loss = 3.21686967\n",
      "Iteration 21, loss = 2.87241912\n",
      "Iteration 36, loss = 0.59664607\n",
      "Iteration 22, loss = 3.09323499\n",
      "Iteration 37, loss = 0.62460440\n",
      "Iteration 23, loss = 2.47184806\n",
      "Iteration 38, loss = 0.49552062\n",
      "Iteration 24, loss = 2.91847188\n",
      "Iteration 39, loss = 0.61692873\n",
      "Iteration 25, loss = 2.75214069\n",
      "Iteration 40, loss = 0.46665844\n",
      "Iteration 41, loss = 0.66403867\n",
      "Iteration 26, loss = 2.40571105\n",
      "Iteration 27, loss = 1.96751659\n",
      "Iteration 42, loss = 0.71403444\n",
      "Iteration 43, loss = 0.76523592\n",
      "Iteration 28, loss = 1.86397811\n",
      "Iteration 29, loss = 1.75177701\n",
      "Iteration 44, loss = 0.59634764\n",
      "Iteration 30, loss = 1.83470960\n",
      "Iteration 45, loss = 0.49717445\n",
      "Iteration 31, loss = 2.15438568\n",
      "Iteration 46, loss = 0.50070886\n",
      "Iteration 32, loss = 2.49621133\n",
      "Iteration 47, loss = 0.58708640\n",
      "Iteration 33, loss = 2.17289780\n",
      "Iteration 48, loss = 0.55722099\n",
      "Iteration 34, loss = 1.71190109\n",
      "Iteration 49, loss = 0.66917509\n",
      "Iteration 35, loss = 1.60804384\n",
      "Iteration 50, loss = 0.40736253\n",
      "Iteration 36, loss = 1.46027317\n",
      "Iteration 51, loss = 0.42595919\n",
      "Iteration 37, loss = 1.57422088\n",
      "Iteration 52, loss = 0.45232524\n",
      "Iteration 38, loss = 1.64977311\n",
      "Iteration 53, loss = 0.42724643\n",
      "Iteration 39, loss = 1.54921903\n",
      "Iteration 54, loss = 0.45571789\n",
      "Iteration 40, loss = 1.24470583\n",
      "Iteration 55, loss = 0.41635468\n",
      "Iteration 41, loss = 1.84442838\n",
      "Iteration 56, loss = 0.33115397\n",
      "Iteration 42, loss = 1.83125272\n",
      "Iteration 57, loss = 0.43733958\n",
      "Iteration 43, loss = 2.46740320\n",
      "Iteration 58, loss = 0.57982016\n",
      "Iteration 44, loss = 1.40423705\n",
      "Iteration 59, loss = 0.46978114\n",
      "Iteration 45, loss = 1.56082419\n",
      "Iteration 60, loss = 0.54558700\n",
      "Iteration 46, loss = 1.29062117\n",
      "Iteration 61, loss = 0.49242832\n",
      "Iteration 47, loss = 1.03242383\n",
      "Iteration 62, loss = 0.32465885\n",
      "Iteration 48, loss = 0.99473599\n",
      "Iteration 63, loss = 0.45041656\n",
      "Iteration 49, loss = 1.62692554\n",
      "Iteration 64, loss = 0.88516701\n",
      "Iteration 50, loss = 1.04937490\n",
      "Iteration 65, loss = 0.98393336\n",
      "Iteration 51, loss = 1.35583435\n",
      "Iteration 66, loss = 1.09684081\n",
      "Iteration 52, loss = 1.34650191\n",
      "Iteration 67, loss = 0.93080199\n",
      "Iteration 53, loss = 1.55111570\n",
      "Iteration 68, loss = 1.30876413\n",
      "Iteration 54, loss = 1.30053062\n",
      "Iteration 69, loss = 1.24435763\n",
      "Iteration 55, loss = 0.90094483\n",
      "Iteration 70, loss = 1.04794000\n",
      "Iteration 56, loss = 0.86780085\n",
      "Iteration 71, loss = 0.81683875\n",
      "Iteration 57, loss = 1.06019655\n",
      "Iteration 72, loss = 0.76272858\n",
      "Iteration 58, loss = 1.15294352\n",
      "Iteration 73, loss = 0.85030735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 1.02024354\n",
      "Iteration 60, loss = 1.29444096\n",
      "Iteration 61, loss = 0.79577941\n",
      "Iteration 62, loss = 0.66957642\n",
      "Iteration 63, loss = 1.42093594\n",
      "Iteration 64, loss = 1.34935347\n",
      "Iteration 65, loss = 1.31845757\n",
      "Iteration 66, loss = 1.04927386\n",
      "Iteration 67, loss = 0.75680359\n",
      "Iteration 68, loss = 0.62744886\n",
      "Iteration 69, loss = 0.81836717\n",
      "Iteration 70, loss = 0.63205013\n",
      "Iteration 71, loss = 1.01084407\n",
      "Iteration 72, loss = 1.26609295\n",
      "Iteration 73, loss = 1.11376362\n",
      "Iteration 74, loss = 0.72799116\n",
      "Iteration 75, loss = 0.60830278\n",
      "Iteration 76, loss = 0.73338602\n",
      "Iteration 1, loss = 20.76140943\n",
      "Iteration 77, loss = 0.51549959\n",
      "Iteration 2, loss = 14.63349347\n",
      "Iteration 78, loss = 0.54367669\n",
      "Iteration 3, loss = 8.74875849\n",
      "Iteration 79, loss = 0.51388582\n",
      "Iteration 4, loss = 11.29942625\n",
      "Iteration 80, loss = 0.36131685\n",
      "Iteration 5, loss = 8.15753333\n",
      "Iteration 81, loss = 0.29555864\n",
      "Iteration 6, loss = 6.03796746\n",
      "Iteration 82, loss = 0.38206638\n",
      "Iteration 7, loss = 6.14481779\n",
      "Iteration 83, loss = 0.37456944\n",
      "Iteration 8, loss = 10.23374191\n",
      "Iteration 84, loss = 0.30668449\n",
      "Iteration 9, loss = 8.42812633\n",
      "Iteration 85, loss = 0.54127496\n",
      "Iteration 10, loss = 7.13122565\n",
      "Iteration 86, loss = 0.29272656\n",
      "Iteration 11, loss = 6.84494455\n",
      "Iteration 87, loss = 0.31431813\n",
      "Iteration 12, loss = 6.16144599\n",
      "Iteration 88, loss = 0.69714139\n",
      "Iteration 13, loss = 6.52507868\n",
      "Iteration 89, loss = 0.46807195\n",
      "Iteration 14, loss = 4.80743804\n",
      "Iteration 90, loss = 0.37764256\n",
      "Iteration 15, loss = 4.97919884\n",
      "Iteration 91, loss = 0.71496507\n",
      "Iteration 16, loss = 5.32934218\n",
      "Iteration 92, loss = 0.66034762\n",
      "Iteration 17, loss = 5.03098759\n",
      "Iteration 93, loss = 0.84661827\n",
      "Iteration 18, loss = 3.75655311\n",
      "Iteration 94, loss = 1.06648482\n",
      "Iteration 19, loss = 3.28555191\n",
      "Iteration 95, loss = 0.77589545\n",
      "Iteration 20, loss = 2.61890694\n",
      "Iteration 96, loss = 0.66934661\n",
      "Iteration 21, loss = 2.54016381\n",
      "Iteration 97, loss = 1.02664922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 2.24989511\n",
      "Iteration 23, loss = 2.24628163\n",
      "Iteration 24, loss = 2.35464902\n",
      "Iteration 25, loss = 1.59827406\n",
      "Iteration 26, loss = 5.47379144\n",
      "Iteration 27, loss = 3.61868772\n",
      "Iteration 28, loss = 3.16361441\n",
      "Iteration 29, loss = 2.97175038\n",
      "Iteration 30, loss = 3.05044153\n",
      "Iteration 31, loss = 3.26570664\n",
      "Iteration 32, loss = 3.21023527\n",
      "Iteration 33, loss = 2.39926339\n",
      "Iteration 34, loss = 2.27843206\n",
      "Iteration 35, loss = 2.65376046\n",
      "Iteration 36, loss = 1.87033926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.87551837\n",
      "Iteration 2, loss = 17.89506615\n",
      "Iteration 3, loss = 10.03250269\n",
      "Iteration 4, loss = 9.46216671\n",
      "Iteration 5, loss = 10.71437803\n",
      "Iteration 6, loss = 9.41518116\n",
      "Iteration 7, loss = 5.81487997\n",
      "Iteration 8, loss = 5.40269522\n",
      "Iteration 9, loss = 7.49350395\n",
      "Iteration 10, loss = 8.50121363\n",
      "Iteration 11, loss = 7.71406343\n",
      "Iteration 12, loss = 6.01336952\n",
      "Iteration 13, loss = 4.47275914\n",
      "Iteration 14, loss = 4.04811025\n",
      "Iteration 15, loss = 3.97050737\n",
      "Iteration 1, loss = 21.27429667\n",
      "Iteration 16, loss = 4.01215186\n",
      "Iteration 2, loss = 23.42353238\n",
      "Iteration 17, loss = 3.29160142\n",
      "Iteration 3, loss = 13.03171385\n",
      "Iteration 18, loss = 3.69427898\n",
      "Iteration 4, loss = 14.54284362\n",
      "Iteration 19, loss = 3.11342066\n",
      "Iteration 5, loss = 8.15179682\n",
      "Iteration 20, loss = 3.11775029\n",
      "Iteration 6, loss = 6.41683984\n",
      "Iteration 21, loss = 2.73415582\n",
      "Iteration 7, loss = 6.08291444\n",
      "Iteration 22, loss = 3.44558888\n",
      "Iteration 8, loss = 8.31756393\n",
      "Iteration 23, loss = 3.80853307\n",
      "Iteration 9, loss = 6.33536446\n",
      "Iteration 24, loss = 3.10038767\n",
      "Iteration 10, loss = 4.47351389\n",
      "Iteration 25, loss = 2.55797696\n",
      "Iteration 11, loss = 4.73917818\n",
      "Iteration 26, loss = 2.13398411\n",
      "Iteration 12, loss = 4.82704782\n",
      "Iteration 27, loss = 2.13437196\n",
      "Iteration 13, loss = 7.01945127\n",
      "Iteration 28, loss = 2.27705629\n",
      "Iteration 14, loss = 6.06031553\n",
      "Iteration 29, loss = 2.10239457\n",
      "Iteration 15, loss = 4.47019407\n",
      "Iteration 30, loss = 1.85068992\n",
      "Iteration 16, loss = 4.69981690\n",
      "Iteration 31, loss = 2.15266580\n",
      "Iteration 17, loss = 5.27293136\n",
      "Iteration 32, loss = 2.20340543\n",
      "Iteration 18, loss = 4.81519697\n",
      "Iteration 33, loss = 2.34785867\n",
      "Iteration 19, loss = 4.04050833\n",
      "Iteration 34, loss = 2.54918704\n",
      "Iteration 20, loss = 4.04921682\n",
      "Iteration 35, loss = 2.88472118\n",
      "Iteration 21, loss = 3.73728964\n",
      "Iteration 36, loss = 2.23390925\n",
      "Iteration 22, loss = 3.58198129\n",
      "Iteration 37, loss = 2.24500621\n",
      "Iteration 23, loss = 3.11128281\n",
      "Iteration 38, loss = 1.99516569\n",
      "Iteration 24, loss = 3.62172413\n",
      "Iteration 39, loss = 2.08440416\n",
      "Iteration 25, loss = 3.58165308\n",
      "Iteration 40, loss = 1.55520502\n",
      "Iteration 26, loss = 2.21052963\n",
      "Iteration 41, loss = 1.45415167\n",
      "Iteration 27, loss = 1.98209463\n",
      "Iteration 42, loss = 1.34365707\n",
      "Iteration 28, loss = 2.01076451\n",
      "Iteration 43, loss = 1.05428643\n",
      "Iteration 29, loss = 1.88768545\n",
      "Iteration 44, loss = 0.89487843\n",
      "Iteration 30, loss = 1.34067434\n",
      "Iteration 45, loss = 0.95119883\n",
      "Iteration 31, loss = 1.61766772\n",
      "Iteration 46, loss = 0.87472841\n",
      "Iteration 32, loss = 1.91711445\n",
      "Iteration 47, loss = 1.11857085\n",
      "Iteration 33, loss = 1.55271425\n",
      "Iteration 48, loss = 1.96436482\n",
      "Iteration 34, loss = 1.28179509\n",
      "Iteration 49, loss = 1.64731271\n",
      "Iteration 35, loss = 1.49729613\n",
      "Iteration 50, loss = 1.63818645\n",
      "Iteration 36, loss = 1.75749276\n",
      "Iteration 51, loss = 1.69323175\n",
      "Iteration 37, loss = 1.36805017\n",
      "Iteration 52, loss = 1.37681742\n",
      "Iteration 38, loss = 1.21805398\n",
      "Iteration 53, loss = 1.41833975\n",
      "Iteration 39, loss = 1.04714682\n",
      "Iteration 54, loss = 1.88904939\n",
      "Iteration 40, loss = 1.66332597\n",
      "Iteration 55, loss = 1.25534332\n",
      "Iteration 41, loss = 1.13930876\n",
      "Iteration 56, loss = 1.37530558\n",
      "Iteration 42, loss = 0.98613351\n",
      "Iteration 57, loss = 1.51816184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.94108072\n",
      "Iteration 44, loss = 1.01295024\n",
      "Iteration 45, loss = 1.25042116\n",
      "Iteration 46, loss = 0.86454351\n",
      "Iteration 47, loss = 0.64157717\n",
      "Iteration 48, loss = 0.67251952\n",
      "Iteration 49, loss = 0.98316663\n",
      "Iteration 50, loss = 0.91462686\n",
      "Iteration 51, loss = 0.87311883\n",
      "Iteration 52, loss = 1.54142808\n",
      "Iteration 53, loss = 1.54452484\n",
      "Iteration 54, loss = 1.14034551\n",
      "Iteration 55, loss = 1.22856716\n",
      "Iteration 56, loss = 0.74561580\n",
      "Iteration 57, loss = 0.87446903\n",
      "Iteration 58, loss = 0.92394376\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.92354685\n",
      "Iteration 2, loss = 13.69851468\n",
      "Iteration 3, loss = 9.84727564\n",
      "Iteration 4, loss = 8.62953221\n",
      "Iteration 5, loss = 8.22727511\n",
      "Iteration 6, loss = 8.93715715\n",
      "Iteration 7, loss = 8.90024488\n",
      "Iteration 8, loss = 6.47926263\n",
      "Iteration 9, loss = 5.56961390\n",
      "Iteration 10, loss = 7.74654193\n",
      "Iteration 11, loss = 7.31841320\n",
      "Iteration 12, loss = 8.10544793\n",
      "Iteration 13, loss = 6.27833251\n",
      "Iteration 14, loss = 5.85200567\n",
      "Iteration 15, loss = 4.89188327\n",
      "Iteration 16, loss = 3.67943231\n",
      "Iteration 17, loss = 3.58210192\n",
      "Iteration 1, loss = 17.51694876\n",
      "Iteration 18, loss = 3.83261378\n",
      "Iteration 2, loss = 12.44583409\n",
      "Iteration 19, loss = 4.11476076\n",
      "Iteration 3, loss = 8.12471276\n",
      "Iteration 20, loss = 3.26508765\n",
      "Iteration 4, loss = 6.66357345\n",
      "Iteration 21, loss = 2.76910110\n",
      "Iteration 5, loss = 9.01929749\n",
      "Iteration 22, loss = 2.34435499\n",
      "Iteration 6, loss = 7.68640032\n",
      "Iteration 23, loss = 2.65386100\n",
      "Iteration 7, loss = 5.25793799\n",
      "Iteration 24, loss = 3.21313797\n",
      "Iteration 8, loss = 5.40848249\n",
      "Iteration 25, loss = 2.43160535\n",
      "Iteration 9, loss = 5.79842173\n",
      "Iteration 26, loss = 3.25883605\n",
      "Iteration 10, loss = 5.36069622\n",
      "Iteration 27, loss = 2.52542503\n",
      "Iteration 11, loss = 4.93178384\n",
      "Iteration 28, loss = 2.30389426\n",
      "Iteration 12, loss = 3.64264490\n",
      "Iteration 29, loss = 2.30361189\n",
      "Iteration 13, loss = 3.83117621\n",
      "Iteration 30, loss = 1.82054208\n",
      "Iteration 14, loss = 3.72235145\n",
      "Iteration 31, loss = 1.53168046\n",
      "Iteration 15, loss = 3.56858069\n",
      "Iteration 32, loss = 1.76652155\n",
      "Iteration 16, loss = 3.07624709\n",
      "Iteration 33, loss = 1.59302604\n",
      "Iteration 17, loss = 2.84079031\n",
      "Iteration 34, loss = 1.02385958\n",
      "Iteration 18, loss = 2.80640512\n",
      "Iteration 35, loss = 0.78855495\n",
      "Iteration 19, loss = 3.66491833\n",
      "Iteration 36, loss = 0.60300232\n",
      "Iteration 20, loss = 3.86731917\n",
      "Iteration 37, loss = 0.71483653\n",
      "Iteration 21, loss = 2.94302219\n",
      "Iteration 38, loss = 0.65564604\n",
      "Iteration 22, loss = 2.43386811\n",
      "Iteration 39, loss = 0.64125110\n",
      "Iteration 23, loss = 2.66603968\n",
      "Iteration 40, loss = 0.65795017\n",
      "Iteration 24, loss = 2.97286885\n",
      "Iteration 41, loss = 0.56820911\n",
      "Iteration 25, loss = 1.97975893\n",
      "Iteration 42, loss = 0.62812381\n",
      "Iteration 26, loss = 2.14565387\n",
      "Iteration 43, loss = 0.46691731\n",
      "Iteration 27, loss = 2.24608941\n",
      "Iteration 44, loss = 0.85362658\n",
      "Iteration 28, loss = 2.53588360\n",
      "Iteration 45, loss = 0.86920682\n",
      "Iteration 29, loss = 1.71900371\n",
      "Iteration 46, loss = 0.58446780\n",
      "Iteration 30, loss = 1.78931345\n",
      "Iteration 47, loss = 0.77771703\n",
      "Iteration 31, loss = 1.82822795\n",
      "Iteration 48, loss = 1.05004924\n",
      "Iteration 32, loss = 1.71565795\n",
      "Iteration 49, loss = 0.77213725\n",
      "Iteration 33, loss = 2.00978689\n",
      "Iteration 50, loss = 1.19255412\n",
      "Iteration 34, loss = 3.20757997\n",
      "Iteration 51, loss = 0.78793875\n",
      "Iteration 35, loss = 2.14964185\n",
      "Iteration 52, loss = 0.58572013\n",
      "Iteration 36, loss = 2.08522294\n",
      "Iteration 53, loss = 0.41386894\n",
      "Iteration 54, loss = 0.62901142\n",
      "Iteration 55, loss = 0.40055341\n",
      "Iteration 37, loss = 1.54328019\n",
      "Iteration 56, loss = 0.32452181\n",
      "Iteration 57, loss = 0.28879955\n",
      "Iteration 58, loss = 0.46193089\n",
      "Iteration 38, loss = 1.75123470\n",
      "Iteration 59, loss = 0.57306723\n",
      "Iteration 39, loss = 2.14218755\n",
      "Iteration 60, loss = 0.69624719\n",
      "Iteration 40, loss = 1.79180195\n",
      "Iteration 61, loss = 0.30632322\n",
      "Iteration 41, loss = 1.29426107\n",
      "Iteration 62, loss = 0.25173415\n",
      "Iteration 42, loss = 1.21850826\n",
      "Iteration 63, loss = 0.30185128\n",
      "Iteration 43, loss = 1.50112134\n",
      "Iteration 64, loss = 0.20697281\n",
      "Iteration 44, loss = 1.80592787\n",
      "Iteration 65, loss = 0.16136721\n",
      "Iteration 45, loss = 1.68386239\n",
      "Iteration 66, loss = 0.20059591\n",
      "Iteration 46, loss = 1.87573474\n",
      "Iteration 67, loss = 0.20072537\n",
      "Iteration 47, loss = 1.73079439\n",
      "Iteration 68, loss = 0.12002588\n",
      "Iteration 48, loss = 2.07539083\n",
      "Iteration 49, loss = 1.68730944\n",
      "Iteration 50, loss = 1.69062510\n",
      "Iteration 51, loss = 1.89219980\n",
      "Iteration 69, loss = 0.17505882\n",
      "Iteration 52, loss = 2.16553889\n",
      "Iteration 53, loss = 2.34627096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 70, loss = 0.23420689\n",
      "Iteration 71, loss = 0.35621013\n",
      "Iteration 72, loss = 0.23311399\n",
      "Iteration 73, loss = 0.34502201\n",
      "Iteration 74, loss = 0.22052921\n",
      "Iteration 75, loss = 0.11328252\n",
      "Iteration 76, loss = 0.12613935\n",
      "Iteration 77, loss = 0.09754252\n",
      "Iteration 78, loss = 0.09678839\n",
      "Iteration 79, loss = 0.24670941\n",
      "Iteration 80, loss = 0.34516936\n",
      "Iteration 81, loss = 0.44006751\n",
      "Iteration 82, loss = 0.44571386\n",
      "Iteration 83, loss = 0.60780174\n",
      "Iteration 84, loss = 0.73867441\n",
      "Iteration 85, loss = 0.83034027\n",
      "Iteration 86, loss = 0.96234218\n",
      "Iteration 87, loss = 1.05229293\n",
      "Iteration 88, loss = 0.86723012\n",
      "Iteration 89, loss = 0.83403097\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.31853785\n",
      "Iteration 2, loss = 16.65585371\n",
      "Iteration 3, loss = 8.82895032\n",
      "Iteration 4, loss = 7.50787974\n",
      "Iteration 5, loss = 11.87610796\n",
      "Iteration 6, loss = 8.09643202\n",
      "Iteration 7, loss = 8.96215957\n",
      "Iteration 8, loss = 8.90719432\n",
      "Iteration 9, loss = 7.81762113\n",
      "Iteration 10, loss = 7.17884583\n",
      "Iteration 11, loss = 4.51870627\n",
      "Iteration 12, loss = 4.24539893\n",
      "Iteration 13, loss = 3.88910935\n",
      "Iteration 1, loss = 17.13574693\n",
      "Iteration 14, loss = 4.88241991\n",
      "Iteration 2, loss = 13.68315616\n",
      "Iteration 15, loss = 3.75005102\n",
      "Iteration 3, loss = 14.01506699\n",
      "Iteration 16, loss = 3.90782418\n",
      "Iteration 4, loss = 12.97918517\n",
      "Iteration 17, loss = 3.65712497\n",
      "Iteration 5, loss = 9.06654442\n",
      "Iteration 18, loss = 3.48949008\n",
      "Iteration 6, loss = 10.13410598\n",
      "Iteration 19, loss = 3.17868718\n",
      "Iteration 7, loss = 7.33266974\n",
      "Iteration 20, loss = 2.25478944\n",
      "Iteration 8, loss = 7.29228209\n",
      "Iteration 21, loss = 1.63770087\n",
      "Iteration 9, loss = 8.04550911\n",
      "Iteration 22, loss = 2.37706589\n",
      "Iteration 10, loss = 7.32946482\n",
      "Iteration 23, loss = 1.96739433\n",
      "Iteration 11, loss = 8.15574630\n",
      "Iteration 24, loss = 1.64371894\n",
      "Iteration 12, loss = 9.19896504\n",
      "Iteration 25, loss = 2.37669880\n",
      "Iteration 13, loss = 6.57913611\n",
      "Iteration 26, loss = 2.77431371\n",
      "Iteration 14, loss = 6.61764668\n",
      "Iteration 27, loss = 2.33344643\n",
      "Iteration 15, loss = 6.01510564\n",
      "Iteration 28, loss = 2.70708525\n",
      "Iteration 16, loss = 5.83937758\n",
      "Iteration 29, loss = 2.03732605\n",
      "Iteration 17, loss = 5.26583473\n",
      "Iteration 30, loss = 3.22041162\n",
      "Iteration 18, loss = 5.90467457\n",
      "Iteration 31, loss = 2.09338877\n",
      "Iteration 19, loss = 5.83347977\n",
      "Iteration 32, loss = 2.36198958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 6.11196184\n",
      "Iteration 21, loss = 7.92104795\n",
      "Iteration 22, loss = 6.66611412\n",
      "Iteration 23, loss = 7.37806462\n",
      "Iteration 24, loss = 5.06813707\n",
      "Iteration 25, loss = 5.82178060\n",
      "Iteration 26, loss = 6.14033598\n",
      "Iteration 27, loss = 5.55590263\n",
      "Iteration 28, loss = 5.71795916\n",
      "Iteration 29, loss = 6.03505218\n",
      "Iteration 30, loss = 5.30358077\n",
      "Iteration 31, loss = 5.05655203\n",
      "Iteration 32, loss = 5.92132753\n",
      "Iteration 33, loss = 5.99987866\n",
      "Iteration 34, loss = 5.25328478\n",
      "Iteration 35, loss = 4.74208840\n",
      "Iteration 36, loss = 4.85952400\n",
      "Iteration 37, loss = 5.21307196\n",
      "Iteration 38, loss = 5.25219514\n",
      "Iteration 1, loss = 18.01203533\n",
      "Iteration 39, loss = 5.60375797\n",
      "Iteration 2, loss = 15.89275868\n",
      "Iteration 40, loss = 5.60056576\n",
      "Iteration 3, loss = 13.44987719\n",
      "Iteration 4, loss = 13.07160295\n",
      "Iteration 41, loss = 4.77309235\n",
      "Iteration 5, loss = 10.54964416\n",
      "Iteration 42, loss = 4.77017282\n",
      "Iteration 6, loss = 9.19068563\n",
      "Iteration 43, loss = 4.33721040\n",
      "Iteration 44, loss = 4.58412262\n",
      "Iteration 7, loss = 9.58060476\n",
      "Iteration 45, loss = 4.68951311\n",
      "Iteration 8, loss = 10.71502375\n",
      "Iteration 46, loss = 5.08128081\n",
      "Iteration 9, loss = 8.24004497\n",
      "Iteration 47, loss = 5.18581880\n",
      "Iteration 10, loss = 9.21892005\n",
      "Iteration 48, loss = 5.50409884\n",
      "Iteration 11, loss = 7.89246296\n",
      "Iteration 49, loss = 5.15713202\n",
      "Iteration 12, loss = 6.03506110\n",
      "Iteration 50, loss = 4.60567759\n",
      "Iteration 13, loss = 6.51664223\n",
      "Iteration 51, loss = 4.48680446\n",
      "Iteration 14, loss = 7.49778195\n",
      "Iteration 52, loss = 4.73623146\n",
      "Iteration 15, loss = 10.78676251\n",
      "Iteration 53, loss = 4.49271330\n",
      "Iteration 16, loss = 8.65715416\n",
      "Iteration 54, loss = 4.75736762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 7.26612972\n",
      "Iteration 18, loss = 5.95035207\n",
      "Iteration 19, loss = 6.47569328\n",
      "Iteration 20, loss = 6.60507596\n",
      "Iteration 21, loss = 6.73100632\n",
      "Iteration 22, loss = 5.87510130\n",
      "Iteration 23, loss = 5.95963488\n",
      "Iteration 24, loss = 5.41611347\n",
      "Iteration 25, loss = 5.69582508\n",
      "Iteration 26, loss = 6.17038557\n",
      "Iteration 27, loss = 6.56484982\n",
      "Iteration 28, loss = 5.08965353\n",
      "Iteration 29, loss = 6.02083698\n",
      "Iteration 30, loss = 5.74685448\n",
      "Iteration 31, loss = 6.45357418\n",
      "Iteration 32, loss = 5.98414131\n",
      "Iteration 33, loss = 5.27925889\n",
      "Iteration 34, loss = 5.55502608\n",
      "Iteration 1, loss = 15.74743703\n",
      "Iteration 35, loss = 6.02657709\n",
      "Iteration 2, loss = 13.17975268\n",
      "Iteration 36, loss = 5.90972039\n",
      "Iteration 3, loss = 13.57060197\n",
      "Iteration 37, loss = 5.36171887\n",
      "Iteration 4, loss = 14.17745333\n",
      "Iteration 38, loss = 4.97039689\n",
      "Iteration 5, loss = 12.00933850\n",
      "Iteration 39, loss = 5.40287228\n",
      "Iteration 6, loss = 12.68754545\n",
      "Iteration 40, loss = 5.63835396\n",
      "Iteration 7, loss = 10.32870471\n",
      "Iteration 41, loss = 6.00688932\n",
      "Iteration 8, loss = 7.59143291\n",
      "Iteration 42, loss = 5.83593177\n",
      "Iteration 9, loss = 8.01925986\n",
      "Iteration 43, loss = 5.83682682\n",
      "Iteration 44, loss = 4.94153585\n",
      "Iteration 10, loss = 8.16694901\n",
      "Iteration 45, loss = 4.46568357\n",
      "Iteration 11, loss = 10.01878492\n",
      "Iteration 46, loss = 4.76279225\n",
      "Iteration 12, loss = 9.96711894\n",
      "Iteration 47, loss = 4.58433497\n",
      "Iteration 13, loss = 8.29596234\n",
      "Iteration 14, loss = 7.31674193\n",
      "Iteration 48, loss = 5.94623455\n",
      "Iteration 15, loss = 6.59842689\n",
      "Iteration 49, loss = 6.66241941\n",
      "Iteration 16, loss = 6.46240859\n",
      "Iteration 50, loss = 5.44607511\n",
      "Iteration 17, loss = 6.87074643\n",
      "Iteration 51, loss = 5.09319911\n",
      "Iteration 18, loss = 8.84297494\n",
      "Iteration 52, loss = 4.89746995\n",
      "Iteration 19, loss = 9.79220188\n",
      "Iteration 53, loss = 4.78000969\n",
      "Iteration 20, loss = 6.97504407\n",
      "Iteration 54, loss = 4.66279061\n",
      "Iteration 21, loss = 6.35141742\n",
      "Iteration 55, loss = 4.66290019\n",
      "Iteration 22, loss = 6.35536794\n",
      "Iteration 56, loss = 5.05497733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 6.67378688\n",
      "Iteration 24, loss = 6.16771818\n",
      "Iteration 25, loss = 6.60239624\n",
      "Iteration 26, loss = 6.95994106\n",
      "Iteration 27, loss = 7.20015726\n",
      "Iteration 28, loss = 6.41915583\n",
      "Iteration 29, loss = 6.54029253\n",
      "Iteration 30, loss = 6.15371143\n",
      "Iteration 31, loss = 4.98102295\n",
      "Iteration 32, loss = 5.53295076\n",
      "Iteration 33, loss = 5.53603447\n",
      "Iteration 34, loss = 5.26375594\n",
      "Iteration 35, loss = 5.07010043\n",
      "Iteration 36, loss = 4.91588666\n",
      "Iteration 37, loss = 5.24537797\n",
      "Iteration 38, loss = 5.07755031\n",
      "Iteration 39, loss = 4.49030015\n",
      "Iteration 40, loss = 4.95832366\n",
      "Iteration 1, loss = 19.17259961\n",
      "Iteration 41, loss = 5.23731065\n",
      "Iteration 2, loss = 12.96437828\n",
      "Iteration 42, loss = 4.80737990\n",
      "Iteration 3, loss = 10.58097068\n",
      "Iteration 43, loss = 4.84788966\n",
      "Iteration 4, loss = 7.97038997\n",
      "Iteration 44, loss = 4.80927230\n",
      "Iteration 5, loss = 12.94788932\n",
      "Iteration 45, loss = 4.81643471\n",
      "Iteration 6, loss = 10.99835103\n",
      "Iteration 46, loss = 5.24870769\n",
      "Iteration 7, loss = 9.82519778\n",
      "Iteration 47, loss = 5.50693079\n",
      "Iteration 8, loss = 9.23764661\n",
      "Iteration 48, loss = 5.48463696\n",
      "Iteration 9, loss = 7.55163150\n",
      "Iteration 49, loss = 5.51894581\n",
      "Iteration 10, loss = 6.23638796\n",
      "Iteration 50, loss = 4.85267636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 6.71293483\n",
      "Iteration 12, loss = 6.58777259\n",
      "Iteration 13, loss = 6.45319898\n",
      "Iteration 14, loss = 7.05653411\n",
      "Iteration 15, loss = 6.75238567\n",
      "Iteration 16, loss = 6.71915613\n",
      "Iteration 17, loss = 6.17386077\n",
      "Iteration 18, loss = 6.05852835\n",
      "Iteration 19, loss = 6.49169063\n",
      "Iteration 20, loss = 6.76741160\n",
      "Iteration 21, loss = 6.25885271\n",
      "Iteration 22, loss = 7.24071831\n",
      "Iteration 23, loss = 6.92765239\n",
      "Iteration 24, loss = 5.79052555\n",
      "Iteration 25, loss = 6.41815602\n",
      "Iteration 26, loss = 5.55489479\n",
      "Iteration 27, loss = 6.61381966\n",
      "Iteration 28, loss = 6.37853950\n",
      "Iteration 1, loss = 16.51755120\n",
      "Iteration 29, loss = 6.96772622\n",
      "Iteration 2, loss = 14.61924868\n",
      "Iteration 30, loss = 6.85150401\n",
      "Iteration 3, loss = 12.61551207\n",
      "Iteration 31, loss = 6.49745359\n",
      "Iteration 4, loss = 11.81445335\n",
      "Iteration 32, loss = 5.83601974\n",
      "Iteration 5, loss = 11.31173518\n",
      "Iteration 33, loss = 5.21045509\n",
      "Iteration 6, loss = 11.56733211\n",
      "Iteration 34, loss = 4.86380603\n",
      "Iteration 7, loss = 10.86778475\n",
      "Iteration 35, loss = 4.97943608\n",
      "Iteration 8, loss = 9.42382892\n",
      "Iteration 36, loss = 4.62395516\n",
      "Iteration 9, loss = 10.38945521\n",
      "Iteration 37, loss = 4.89928093\n",
      "Iteration 10, loss = 8.50633830\n",
      "Iteration 38, loss = 5.44858909\n",
      "Iteration 11, loss = 11.07697034\n",
      "Iteration 39, loss = 5.37104484\n",
      "Iteration 12, loss = 8.33198482\n",
      "Iteration 40, loss = 4.94118986\n",
      "Iteration 13, loss = 7.54122414\n",
      "Iteration 41, loss = 4.62817250\n",
      "Iteration 14, loss = 7.72686730\n",
      "Iteration 42, loss = 5.84493143\n",
      "Iteration 15, loss = 7.63000096\n",
      "Iteration 43, loss = 5.29686136\n",
      "Iteration 16, loss = 6.50692868\n",
      "Iteration 44, loss = 4.74869976\n",
      "Iteration 17, loss = 6.32058568\n",
      "Iteration 45, loss = 4.31761854\n",
      "Iteration 18, loss = 6.13270389\n",
      "Iteration 46, loss = 4.74897204\n",
      "Iteration 19, loss = 6.02252126\n",
      "Iteration 47, loss = 5.18093092\n",
      "Iteration 20, loss = 6.34294888\n",
      "Iteration 48, loss = 4.35739356\n",
      "Iteration 21, loss = 7.87386121\n",
      "Iteration 49, loss = 4.25068057\n",
      "Iteration 22, loss = 6.54222669\n",
      "Iteration 50, loss = 4.54717390\n",
      "Iteration 23, loss = 6.77962926\n",
      "Iteration 51, loss = 4.39656333\n",
      "Iteration 24, loss = 6.19428410\n",
      "Iteration 52, loss = 4.94358051\n",
      "Iteration 25, loss = 5.92339266\n",
      "Iteration 53, loss = 5.10359410\n",
      "Iteration 26, loss = 6.39769225\n",
      "Iteration 54, loss = 4.28057943\n",
      "Iteration 27, loss = 6.98852420\n",
      "Iteration 55, loss = 5.49642636\n",
      "Iteration 28, loss = 6.83472036\n",
      "Iteration 56, loss = 4.35847336\n",
      "Iteration 29, loss = 6.09148364\n",
      "Iteration 57, loss = 5.88787545\n",
      "Iteration 30, loss = 5.62289565\n",
      "Iteration 58, loss = 5.73119175\n",
      "Iteration 31, loss = 6.04879727\n",
      "Iteration 59, loss = 5.74930005\n",
      "Iteration 32, loss = 5.66382709\n",
      "Iteration 60, loss = 4.28121300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 5.58620698\n",
      "Iteration 34, loss = 6.05732349\n",
      "Iteration 35, loss = 5.39133346\n",
      "Iteration 36, loss = 6.13685244\n",
      "Iteration 37, loss = 5.82366187\n",
      "Iteration 38, loss = 5.23754417\n",
      "Iteration 39, loss = 5.31803896\n",
      "Iteration 40, loss = 4.88833154\n",
      "Iteration 41, loss = 6.28457567\n",
      "Iteration 42, loss = 5.12626203\n",
      "Iteration 43, loss = 4.97019319\n",
      "Iteration 44, loss = 5.51967538\n",
      "Iteration 45, loss = 5.63806732\n",
      "Iteration 46, loss = 5.59877505\n",
      "Iteration 47, loss = 6.34409125\n",
      "Iteration 48, loss = 5.82273689\n",
      "Iteration 49, loss = 5.99302110\n",
      "Iteration 50, loss = 5.68069910\n",
      "Iteration 51, loss = 6.11423639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.02361807\n",
      "Iteration 2, loss = 17.79841404\n",
      "Iteration 3, loss = 11.84681527\n",
      "Iteration 4, loss = 12.22825255\n",
      "Iteration 5, loss = 14.14101687\n",
      "Iteration 6, loss = 11.34903157\n",
      "Iteration 7, loss = 10.13624037\n",
      "Iteration 8, loss = 7.79798214\n",
      "Iteration 9, loss = 8.36611110\n",
      "Iteration 10, loss = 8.96847345\n",
      "Iteration 11, loss = 8.62638123\n",
      "Iteration 12, loss = 8.14029911\n",
      "Iteration 13, loss = 9.16089315\n",
      "Iteration 14, loss = 9.22516078\n",
      "Iteration 15, loss = 7.67127062\n",
      "Iteration 16, loss = 7.24823708\n",
      "Iteration 17, loss = 6.07541421\n",
      "Iteration 18, loss = 6.47009285\n",
      "Iteration 19, loss = 7.20959125\n",
      "Iteration 1, loss = 18.64444080\n",
      "Iteration 20, loss = 5.73230433\n",
      "Iteration 2, loss = 12.03932116\n",
      "Iteration 21, loss = 6.75551087\n",
      "Iteration 3, loss = 14.95532121\n",
      "Iteration 22, loss = 6.44447438\n",
      "Iteration 4, loss = 8.58776579\n",
      "Iteration 23, loss = 6.64444459\n",
      "Iteration 5, loss = 8.84225741\n",
      "Iteration 6, loss = 8.89432019Iteration 24, loss = 6.17686582\n",
      "\n",
      "Iteration 7, loss = 9.89572673\n",
      "Iteration 25, loss = 6.88536070\n",
      "Iteration 8, loss = 10.26285300\n",
      "Iteration 26, loss = 7.08311383\n",
      "Iteration 9, loss = 8.85932902\n",
      "Iteration 27, loss = 5.94879089\n",
      "Iteration 10, loss = 6.56405169\n",
      "Iteration 28, loss = 5.20767393\n",
      "Iteration 11, loss = 7.60971383\n",
      "Iteration 29, loss = 5.79867009\n",
      "Iteration 12, loss = 7.01708683\n",
      "Iteration 30, loss = 5.40628823\n",
      "Iteration 13, loss = 7.38056759\n",
      "Iteration 31, loss = 5.68058916\n",
      "Iteration 14, loss = 8.19402006\n",
      "Iteration 32, loss = 6.50391570\n",
      "Iteration 15, loss = 6.63586918\n",
      "Iteration 33, loss = 6.26748499\n",
      "Iteration 16, loss = 7.06083303\n",
      "Iteration 34, loss = 6.22721803\n",
      "Iteration 17, loss = 6.77097925\n",
      "Iteration 35, loss = 6.14892176\n",
      "Iteration 18, loss = 6.51806363\n",
      "Iteration 36, loss = 6.07019976\n",
      "Iteration 19, loss = 5.83033473\n",
      "Iteration 37, loss = 5.67577255\n",
      "Iteration 20, loss = 6.27614536\n",
      "Iteration 38, loss = 6.10541595\n",
      "Iteration 21, loss = 5.65798010\n",
      "Iteration 39, loss = 6.02495260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 6.64556694\n",
      "Iteration 23, loss = 7.00365333\n",
      "Iteration 24, loss = 5.86917393\n",
      "Iteration 25, loss = 5.32154925\n",
      "Iteration 26, loss = 5.67489137\n",
      "Iteration 27, loss = 5.12723372\n",
      "Iteration 28, loss = 5.75673466\n",
      "Iteration 29, loss = 5.64255240\n",
      "Iteration 30, loss = 6.58850881\n",
      "Iteration 31, loss = 6.67103783\n",
      "Iteration 32, loss = 6.00708770\n",
      "Iteration 33, loss = 6.20572503\n",
      "Iteration 34, loss = 6.60012037\n",
      "Iteration 35, loss = 5.30953134\n",
      "Iteration 36, loss = 5.66443539\n",
      "Iteration 37, loss = 4.52774291\n",
      "Iteration 38, loss = 4.76484873\n",
      "Iteration 39, loss = 5.11901760\n",
      "Iteration 1, loss = 17.84292760\n",
      "Iteration 40, loss = 5.47291935\n",
      "Iteration 2, loss = 21.50695098\n",
      "Iteration 41, loss = 4.73012056\n",
      "Iteration 3, loss = 11.97631067\n",
      "Iteration 42, loss = 4.57573569\n",
      "Iteration 4, loss = 11.68005273\n",
      "Iteration 43, loss = 4.85324909\n",
      "Iteration 5, loss = 9.88842544\n",
      "Iteration 44, loss = 5.09164553\n",
      "Iteration 6, loss = 10.74981460\n",
      "Iteration 45, loss = 4.46587443\n",
      "Iteration 7, loss = 9.36128410\n",
      "Iteration 46, loss = 4.62534780\n",
      "Iteration 8, loss = 8.96133923\n",
      "Iteration 47, loss = 4.82287277\n",
      "Iteration 9, loss = 7.71964453\n",
      "Iteration 48, loss = 4.74511514\n",
      "Iteration 10, loss = 6.85731591\n",
      "Iteration 49, loss = 4.43355033\n",
      "Iteration 11, loss = 8.26123212\n",
      "Iteration 50, loss = 5.37940361\n",
      "Iteration 12, loss = 8.04411749\n",
      "Iteration 51, loss = 4.91165579\n",
      "Iteration 13, loss = 6.94696337\n",
      "Iteration 52, loss = 5.07232923\n",
      "Iteration 14, loss = 6.81861771\n",
      "Iteration 53, loss = 5.43008703\n",
      "Iteration 15, loss = 6.44753614\n",
      "Iteration 54, loss = 6.65096902\n",
      "Iteration 16, loss = 6.26915570\n",
      "Iteration 55, loss = 5.75153300\n",
      "Iteration 17, loss = 6.71369768\n",
      "Iteration 56, loss = 5.20589684\n",
      "Iteration 18, loss = 6.41197132\n",
      "Iteration 57, loss = 5.36435622\n",
      "Iteration 19, loss = 6.02818363\n",
      "Iteration 58, loss = 4.65917430\n",
      "Iteration 20, loss = 5.25008113\n",
      "Iteration 59, loss = 4.42415070\n",
      "Iteration 21, loss = 5.09870665\n",
      "Iteration 60, loss = 5.05475351\n",
      "Iteration 22, loss = 6.63216065\n",
      "Iteration 61, loss = 4.58835044\n",
      "Iteration 23, loss = 7.02723656\n",
      "Iteration 62, loss = 4.47569704\n",
      "Iteration 24, loss = 5.81423959\n",
      "Iteration 63, loss = 4.63606669\n",
      "Iteration 25, loss = 5.03220441\n",
      "Iteration 64, loss = 4.64176739\n",
      "Iteration 26, loss = 5.11240173\n",
      "Iteration 65, loss = 4.68698099\n",
      "Iteration 27, loss = 5.19141414\n",
      "Iteration 66, loss = 3.86717178\n",
      "Iteration 67, loss = 4.92847509\n",
      "Iteration 28, loss = 4.24994074\n",
      "Iteration 68, loss = 4.89061473\n",
      "Iteration 29, loss = 5.19151741\n",
      "Iteration 69, loss = 4.38190588\n",
      "Iteration 30, loss = 5.89721136\n",
      "Iteration 70, loss = 4.30503076\n",
      "Iteration 31, loss = 5.34788851\n",
      "Iteration 71, loss = 4.18736493\n",
      "Iteration 32, loss = 5.03428903\n",
      "Iteration 33, loss = 4.87809076\n",
      "Iteration 72, loss = 4.42133927\n",
      "Iteration 34, loss = 5.54569246\n",
      "Iteration 73, loss = 4.85089113\n",
      "Iteration 35, loss = 4.99790414\n",
      "Iteration 74, loss = 5.04620606\n",
      "Iteration 36, loss = 5.50841456\n",
      "Iteration 75, loss = 4.69588673\n",
      "Iteration 37, loss = 4.99901125\n",
      "Iteration 76, loss = 4.58244622\n",
      "Iteration 38, loss = 3.94164155\n",
      "Iteration 77, loss = 4.93830542\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 4.49304999\n",
      "Iteration 40, loss = 4.76897853\n",
      "Iteration 41, loss = 5.51557807\n",
      "Iteration 42, loss = 4.77260845\n",
      "Iteration 43, loss = 5.04771576\n",
      "Iteration 44, loss = 5.00874704\n",
      "Iteration 45, loss = 4.61701198\n",
      "Iteration 46, loss = 4.57973390\n",
      "Iteration 47, loss = 4.18954118\n",
      "Iteration 48, loss = 3.72018149\n",
      "Iteration 49, loss = 4.46697563\n",
      "Iteration 50, loss = 4.27230159\n",
      "Iteration 51, loss = 4.35281806\n",
      "Iteration 52, loss = 4.72793708\n",
      "Iteration 53, loss = 4.16552707\n",
      "Iteration 54, loss = 4.09046747\n",
      "Iteration 55, loss = 4.44569390\n",
      "Iteration 56, loss = 3.74086158\n",
      "Iteration 1, loss = 18.51047558\n",
      "Iteration 57, loss = 3.62563205\n",
      "Iteration 2, loss = 20.02148285\n",
      "Iteration 58, loss = 3.55849281\n",
      "Iteration 3, loss = 9.57646312\n",
      "Iteration 59, loss = 4.33793048\n",
      "Iteration 4, loss = 11.29436791\n",
      "Iteration 5, loss = 11.16096667\n",
      "Iteration 60, loss = 3.83092671\n",
      "Iteration 6, loss = 8.66347034\n",
      "Iteration 61, loss = 3.67629112\n",
      "Iteration 7, loss = 8.79993197\n",
      "Iteration 62, loss = 4.19697182\n",
      "Iteration 8, loss = 7.55578668\n",
      "Iteration 63, loss = 4.18923375\n",
      "Iteration 9, loss = 7.21454657\n",
      "Iteration 64, loss = 3.44548296\n",
      "Iteration 10, loss = 6.87054858\n",
      "Iteration 65, loss = 3.72158044\n",
      "Iteration 11, loss = 7.23336946\n",
      "Iteration 66, loss = 3.76181918\n",
      "Iteration 12, loss = 6.31299393\n",
      "Iteration 67, loss = 3.60582395\n",
      "Iteration 13, loss = 6.74393723\n",
      "Iteration 68, loss = 3.64787037\n",
      "Iteration 14, loss = 6.29798479\n",
      "Iteration 69, loss = 3.64363822\n",
      "Iteration 15, loss = 6.39243907\n",
      "Iteration 70, loss = 3.13756719\n",
      "Iteration 16, loss = 6.67845219\n",
      "Iteration 71, loss = 3.48817084\n",
      "Iteration 17, loss = 5.58740953\n",
      "Iteration 72, loss = 3.49101448\n",
      "Iteration 18, loss = 5.82571600\n",
      "Iteration 73, loss = 2.98146390\n",
      "Iteration 19, loss = 5.59106826\n",
      "Iteration 74, loss = 3.49141016\n",
      "Iteration 20, loss = 5.98405512\n",
      "Iteration 75, loss = 3.06007365\n",
      "Iteration 21, loss = 6.17994168\n",
      "Iteration 76, loss = 3.13844165\n",
      "Iteration 77, loss = 3.53083417\n",
      "Iteration 22, loss = 5.00290914\n",
      "Iteration 23, loss = 5.62968810\n",
      "Iteration 78, loss = 3.42082796\n",
      "Iteration 24, loss = 4.72645452\n",
      "Iteration 79, loss = 2.78703111\n",
      "Iteration 25, loss = 4.68637345\n",
      "Iteration 80, loss = 2.78722526\n",
      "Iteration 26, loss = 6.68586400\n",
      "Iteration 81, loss = 3.25800630\n",
      "Iteration 27, loss = 6.88132573\n",
      "Iteration 82, loss = 3.25815102\n",
      "Iteration 28, loss = 6.02062871\n",
      "Iteration 83, loss = 3.53264480\n",
      "Iteration 29, loss = 5.86693777\n",
      "Iteration 84, loss = 3.33677389\n",
      "Iteration 30, loss = 5.12372322\n",
      "Iteration 85, loss = 3.33710283\n",
      "Iteration 31, loss = 4.65588200\n",
      "Iteration 86, loss = 3.18039078\n",
      "Iteration 32, loss = 4.61976952\n",
      "Iteration 87, loss = 3.61814975\n",
      "Iteration 88, loss = 3.96467611\n",
      "Iteration 33, loss = 4.70143100\n",
      "Iteration 34, loss = 5.13540225\n",
      "Iteration 89, loss = 3.60111975\n",
      "Iteration 35, loss = 4.82300827\n",
      "Iteration 90, loss = 4.64620153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 4.98043198\n",
      "Iteration 37, loss = 5.21637277\n",
      "Iteration 38, loss = 5.45220081\n",
      "Iteration 39, loss = 4.74597565\n",
      "Iteration 40, loss = 4.31447718\n",
      "Iteration 41, loss = 4.86460141\n",
      "Iteration 42, loss = 5.33513745\n",
      "Iteration 43, loss = 4.86387002\n",
      "Iteration 44, loss = 4.70670201\n",
      "Iteration 45, loss = 4.47161046\n",
      "Iteration 46, loss = 5.49137153\n",
      "Iteration 47, loss = 4.94235659\n",
      "Iteration 48, loss = 5.21768256\n",
      "Iteration 49, loss = 5.41531330\n",
      "Iteration 50, loss = 4.55263879\n",
      "Iteration 51, loss = 4.31740434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.13947433\n",
      "Iteration 2, loss = 13.13311277\n",
      "Iteration 3, loss = 10.70036799\n",
      "Iteration 4, loss = 8.62718006\n",
      "Iteration 5, loss = 8.43548111\n",
      "Iteration 6, loss = 12.25336452\n",
      "Iteration 7, loss = 10.00201030\n",
      "Iteration 8, loss = 11.42610663\n",
      "Iteration 9, loss = 10.33028788\n",
      "Iteration 10, loss = 8.25144711\n",
      "Iteration 11, loss = 7.80052964\n",
      "Iteration 12, loss = 7.32328161\n",
      "Iteration 13, loss = 7.69603011\n",
      "Iteration 14, loss = 6.87358353\n",
      "Iteration 15, loss = 6.76498065\n",
      "Iteration 16, loss = 6.56641366\n",
      "Iteration 1, loss = 19.03601759\n",
      "Iteration 17, loss = 6.92982709\n",
      "Iteration 2, loss = 15.04656036\n",
      "Iteration 18, loss = 7.17311493\n",
      "Iteration 3, loss = 12.35954265\n",
      "Iteration 19, loss = 6.12258665\n",
      "Iteration 4, loss = 12.57871166\n",
      "Iteration 20, loss = 6.09053635\n",
      "Iteration 5, loss = 8.57386754\n",
      "Iteration 21, loss = 5.99633847\n",
      "Iteration 6, loss = 8.07892340\n",
      "Iteration 22, loss = 6.04261177\n",
      "Iteration 7, loss = 8.78138645\n",
      "Iteration 23, loss = 5.94372409\n",
      "Iteration 8, loss = 9.61953356\n",
      "Iteration 24, loss = 7.27831965\n",
      "Iteration 9, loss = 7.16868234\n",
      "Iteration 25, loss = 8.02389987\n",
      "Iteration 10, loss = 7.83313669\n",
      "Iteration 26, loss = 6.60239488\n",
      "Iteration 11, loss = 8.20325697\n",
      "Iteration 27, loss = 5.78603655\n",
      "Iteration 12, loss = 7.60775398\n",
      "Iteration 28, loss = 5.94863975\n",
      "Iteration 13, loss = 7.94635822\n",
      "Iteration 29, loss = 6.41171428\n",
      "Iteration 14, loss = 6.90324455\n",
      "Iteration 30, loss = 6.13129946\n",
      "Iteration 15, loss = 6.40410597\n",
      "Iteration 31, loss = 6.41110452\n",
      "Iteration 32, loss = 5.90068428\n",
      "Iteration 16, loss = 7.23769841\n",
      "Iteration 33, loss = 6.05704586\n",
      "Iteration 17, loss = 6.22308104\n",
      "Iteration 34, loss = 6.73876409\n",
      "Iteration 18, loss = 7.28707066\n",
      "Iteration 35, loss = 5.42095910\n",
      "Iteration 19, loss = 6.18968982\n",
      "Iteration 36, loss = 5.61941862\n",
      "Iteration 20, loss = 6.46502688\n",
      "Iteration 37, loss = 5.19013583\n",
      "Iteration 21, loss = 6.35694962\n",
      "Iteration 38, loss = 6.13023128\n",
      "Iteration 22, loss = 5.67927904\n",
      "Iteration 39, loss = 5.77745655\n",
      "Iteration 23, loss = 6.14952698\n",
      "Iteration 40, loss = 5.69875519\n",
      "Iteration 24, loss = 4.69637790\n",
      "Iteration 41, loss = 5.85157808\n",
      "Iteration 25, loss = 5.20727567\n",
      "Iteration 42, loss = 5.81580323\n",
      "Iteration 26, loss = 4.69677047\n",
      "Iteration 43, loss = 5.26604306\n",
      "Iteration 27, loss = 5.13451002\n",
      "Iteration 44, loss = 5.69659618\n",
      "Iteration 28, loss = 7.13199878\n",
      "Iteration 45, loss = 5.47870274\n",
      "Iteration 29, loss = 5.72024730\n",
      "Iteration 46, loss = 6.04890131\n",
      "Iteration 30, loss = 5.79915871\n",
      "Iteration 47, loss = 5.42133249\n",
      "Iteration 31, loss = 5.68112776\n",
      "Iteration 48, loss = 5.26427007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 5.56371591\n",
      "Iteration 33, loss = 5.36740072\n",
      "Iteration 34, loss = 5.60303383\n",
      "Iteration 35, loss = 5.52443772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.53161713\n",
      "Iteration 2, loss = 18.53964342\n",
      "Iteration 3, loss = 13.82393451\n",
      "Iteration 1, loss = 21.61869484\n",
      "Iteration 4, loss = 16.74029342\n",
      "Iteration 2, loss = 17.19895967\n",
      "Iteration 5, loss = 9.87510192\n",
      "Iteration 3, loss = 12.53993287\n",
      "Iteration 6, loss = 9.14004843\n",
      "Iteration 4, loss = 12.89695391\n",
      "Iteration 7, loss = 9.43698977\n",
      "Iteration 5, loss = 13.63269057\n",
      "Iteration 8, loss = 7.53234452\n",
      "Iteration 6, loss = 13.87278973\n",
      "Iteration 9, loss = 8.28004247\n",
      "Iteration 7, loss = 12.78055982\n",
      "Iteration 10, loss = 8.25642955\n",
      "Iteration 8, loss = 10.73583546\n",
      "Iteration 11, loss = 7.62196168\n",
      "Iteration 9, loss = 9.64291412\n",
      "Iteration 12, loss = 7.43658065\n",
      "Iteration 10, loss = 8.99743458\n",
      "Iteration 13, loss = 8.88089707\n",
      "Iteration 11, loss = 10.36698403\n",
      "Iteration 14, loss = 7.60781452\n",
      "Iteration 12, loss = 9.16719370\n",
      "Iteration 15, loss = 6.60038320\n",
      "Iteration 13, loss = 10.14871409\n",
      "Iteration 16, loss = 6.09899796\n",
      "Iteration 14, loss = 9.64449766\n",
      "Iteration 17, loss = 6.18378010\n",
      "Iteration 15, loss = 7.80562015\n",
      "Iteration 18, loss = 5.01166548\n",
      "Iteration 16, loss = 7.19291955\n",
      "Iteration 19, loss = 4.78046641\n",
      "Iteration 17, loss = 7.72347036\n",
      "Iteration 20, loss = 4.66587539\n",
      "Iteration 18, loss = 6.58270949\n",
      "Iteration 21, loss = 4.78613698\n",
      "Iteration 19, loss = 8.15632286\n",
      "Iteration 22, loss = 4.27850329\n",
      "Iteration 20, loss = 7.96360126\n",
      "Iteration 23, loss = 4.20195890\n",
      "Iteration 21, loss = 7.65072031\n",
      "Iteration 24, loss = 4.91279501\n",
      "Iteration 22, loss = 7.10921354\n",
      "Iteration 25, loss = 5.49740827\n",
      "Iteration 23, loss = 6.51394957\n",
      "Iteration 26, loss = 5.06512806\n",
      "Iteration 24, loss = 6.39866609\n",
      "Iteration 27, loss = 4.35848313\n",
      "Iteration 25, loss = 6.28021249\n",
      "Iteration 28, loss = 4.86756242\n",
      "Iteration 26, loss = 6.81433430\n",
      "Iteration 29, loss = 4.90595374\n",
      "Iteration 27, loss = 7.62572511\n",
      "Iteration 30, loss = 4.55220099\n",
      "Iteration 28, loss = 6.94924027\n",
      "Iteration 31, loss = 3.92323430\n",
      "Iteration 29, loss = 7.81381546\n",
      "Iteration 32, loss = 4.35471163\n",
      "Iteration 30, loss = 6.75378026\n",
      "Iteration 33, loss = 4.69494805\n",
      "Iteration 31, loss = 6.83283477\n",
      "Iteration 34, loss = 4.23841496\n",
      "Iteration 32, loss = 7.34329739\n",
      "Iteration 35, loss = 4.86690138\n",
      "Iteration 33, loss = 7.96803677\n",
      "Iteration 36, loss = 5.06182812\n",
      "Iteration 34, loss = 7.18771858\n",
      "Iteration 37, loss = 4.19707384\n",
      "Iteration 35, loss = 7.81627076\n",
      "Iteration 38, loss = 5.03242887\n",
      "Iteration 36, loss = 6.25230490\n",
      "Iteration 39, loss = 8.39268332\n",
      "Iteration 37, loss = 6.48441495\n",
      "Iteration 40, loss = 5.84420030\n",
      "Iteration 38, loss = 6.68121573\n",
      "Iteration 41, loss = 5.96345530\n",
      "Iteration 39, loss = 6.49658871\n",
      "Iteration 42, loss = 4.90661850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 6.64325159\n",
      "Iteration 41, loss = 7.32200187\n",
      "Iteration 42, loss = 6.84685580\n",
      "Iteration 43, loss = 5.91972199\n",
      "Iteration 44, loss = 7.11502346\n",
      "Iteration 45, loss = 7.50749605\n",
      "Iteration 46, loss = 6.49171687\n",
      "Iteration 47, loss = 6.81329842\n",
      "Iteration 48, loss = 6.84202897\n",
      "Iteration 49, loss = 6.82274279\n",
      "Iteration 50, loss = 7.30623427\n",
      "Iteration 51, loss = 6.01891293\n",
      "Iteration 52, loss = 6.52842629\n",
      "Iteration 53, loss = 6.91926214\n",
      "Iteration 54, loss = 7.62885592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.71372365\n",
      "Iteration 2, loss = 14.43617268\n",
      "Iteration 3, loss = 8.17658503\n",
      "Iteration 4, loss = 9.80206121\n",
      "Iteration 5, loss = 8.42014457\n",
      "Iteration 6, loss = 7.06888620\n",
      "Iteration 7, loss = 7.84283134\n",
      "Iteration 8, loss = 7.42542562\n",
      "Iteration 9, loss = 7.72033386\n",
      "Iteration 10, loss = 9.23708538\n",
      "Iteration 11, loss = 8.29246092\n",
      "Iteration 12, loss = 7.25154916\n",
      "Iteration 13, loss = 7.60169594\n",
      "Iteration 14, loss = 6.99851310\n",
      "Iteration 15, loss = 6.18960350\n",
      "Iteration 1, loss = 17.41359581\n",
      "Iteration 16, loss = 5.84276014\n",
      "Iteration 2, loss = 14.32098533\n",
      "Iteration 17, loss = 6.24048354\n",
      "Iteration 3, loss = 11.36193757\n",
      "Iteration 18, loss = 7.06894613\n",
      "Iteration 4, loss = 12.97272183\n",
      "Iteration 19, loss = 6.36878979\n",
      "Iteration 5, loss = 11.13888925\n",
      "Iteration 20, loss = 6.05922589\n",
      "Iteration 6, loss = 8.44749478\n",
      "Iteration 21, loss = 5.27864583\n",
      "Iteration 7, loss = 7.68648974\n",
      "Iteration 22, loss = 5.51789980\n",
      "Iteration 8, loss = 9.01864728\n",
      "Iteration 23, loss = 5.24052235\n",
      "Iteration 9, loss = 8.05919915\n",
      "Iteration 24, loss = 5.08724539\n",
      "Iteration 10, loss = 6.61943143\n",
      "Iteration 25, loss = 5.20600900\n",
      "Iteration 11, loss = 6.66201950\n",
      "Iteration 26, loss = 5.79853578\n",
      "Iteration 12, loss = 6.10564901\n",
      "Iteration 27, loss = 5.66676257\n",
      "Iteration 13, loss = 8.05648919\n",
      "Iteration 28, loss = 5.85490593\n",
      "Iteration 14, loss = 6.83230676\n",
      "Iteration 29, loss = 5.22858688\n",
      "Iteration 15, loss = 6.15115766\n",
      "Iteration 30, loss = 5.54715781\n",
      "Iteration 16, loss = 6.24822194\n",
      "Iteration 31, loss = 5.35428762\n",
      "Iteration 17, loss = 6.57631358\n",
      "Iteration 32, loss = 5.20021968\n",
      "Iteration 18, loss = 6.63644687\n",
      "Iteration 33, loss = 5.16182602\n",
      "Iteration 19, loss = 5.48089985\n",
      "Iteration 34, loss = 6.18323936\n",
      "Iteration 20, loss = 5.38022002\n",
      "Iteration 35, loss = 4.73519246\n",
      "Iteration 21, loss = 6.14321243\n",
      "Iteration 36, loss = 5.05806398\n",
      "Iteration 22, loss = 5.66439304\n",
      "Iteration 37, loss = 5.36635523\n",
      "Iteration 23, loss = 5.69318166\n",
      "Iteration 38, loss = 4.54173694\n",
      "Iteration 24, loss = 5.85631805\n",
      "Iteration 39, loss = 4.89340410\n",
      "Iteration 25, loss = 6.01691770\n",
      "Iteration 40, loss = 4.14768550\n",
      "Iteration 26, loss = 5.82384078\n",
      "Iteration 41, loss = 5.12821859\n",
      "Iteration 27, loss = 5.55367413\n",
      "Iteration 42, loss = 4.46232305\n",
      "Iteration 28, loss = 5.48231919\n",
      "Iteration 43, loss = 4.04835428\n",
      "Iteration 29, loss = 5.72497578\n",
      "Iteration 44, loss = 4.58023931\n",
      "Iteration 30, loss = 6.00672469\n",
      "Iteration 45, loss = 4.21346762\n",
      "Iteration 31, loss = 5.69488939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 4.89602762\n",
      "Iteration 47, loss = 5.32805679\n",
      "Iteration 48, loss = 6.03461402\n",
      "Iteration 49, loss = 5.52474164\n",
      "Iteration 50, loss = 5.24872324\n",
      "Iteration 51, loss = 4.46243418\n",
      "Iteration 52, loss = 5.24676201\n",
      "Iteration 53, loss = 4.58115910\n",
      "Iteration 54, loss = 5.05336817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.68663980\n",
      "Iteration 2, loss = 15.74003997\n",
      "Iteration 3, loss = 15.57479290\n",
      "Iteration 4, loss = 10.59652022\n",
      "Iteration 5, loss = 10.56077995\n",
      "Iteration 6, loss = 7.11278854\n",
      "Iteration 7, loss = 10.48105927\n",
      "Iteration 8, loss = 10.11146356\n",
      "Iteration 9, loss = 13.41804869\n",
      "Iteration 1, loss = 18.91929373\n",
      "Iteration 10, loss = 9.15231032\n",
      "Iteration 2, loss = 14.63384660\n",
      "Iteration 3, loss = 11.54529972\n",
      "Iteration 11, loss = 7.54442285\n",
      "Iteration 4, loss = 8.84123874\n",
      "Iteration 12, loss = 7.02294433\n",
      "Iteration 5, loss = 11.25951969\n",
      "Iteration 13, loss = 6.76211055\n",
      "Iteration 6, loss = 7.50547442\n",
      "Iteration 14, loss = 6.72350946\n",
      "Iteration 15, loss = 6.59801122\n",
      "Iteration 7, loss = 6.98182072\n",
      "Iteration 16, loss = 5.68219336\n",
      "Iteration 8, loss = 8.13269541\n",
      "Iteration 9, loss = 10.26162822\n",
      "Iteration 17, loss = 5.38796555\n",
      "Iteration 10, loss = 7.25088301\n",
      "Iteration 18, loss = 5.20584177\n",
      "Iteration 19, loss = 5.02134209\n",
      "Iteration 11, loss = 6.72707070\n",
      "Iteration 20, loss = 4.95306546\n",
      "Iteration 12, loss = 6.70243203\n",
      "Iteration 21, loss = 5.94008307\n",
      "Iteration 13, loss = 7.01742515\n",
      "Iteration 22, loss = 5.39464979\n",
      "Iteration 14, loss = 7.51111971\n",
      "Iteration 23, loss = 5.71164014\n",
      "Iteration 15, loss = 7.87192144\n",
      "Iteration 24, loss = 6.73394455\n",
      "Iteration 16, loss = 6.57203342\n",
      "Iteration 25, loss = 5.40636187\n",
      "Iteration 17, loss = 6.51977189\n",
      "Iteration 26, loss = 4.58936881\n",
      "Iteration 18, loss = 6.10951445\n",
      "Iteration 27, loss = 5.10651290\n",
      "Iteration 19, loss = 6.28175151\n",
      "Iteration 28, loss = 4.99367991\n",
      "Iteration 20, loss = 6.72614250\n",
      "Iteration 29, loss = 5.38798562\n",
      "Iteration 21, loss = 6.38446246\n",
      "Iteration 30, loss = 4.95739810\n",
      "Iteration 22, loss = 6.03442116\n",
      "Iteration 31, loss = 4.25107941\n",
      "Iteration 23, loss = 5.69369787\n",
      "Iteration 32, loss = 4.40822659\n",
      "Iteration 24, loss = 5.61774434\n",
      "Iteration 33, loss = 4.09519217\n",
      "Iteration 25, loss = 5.38180119\n",
      "Iteration 34, loss = 4.25356952\n",
      "Iteration 26, loss = 5.50181505\n",
      "Iteration 35, loss = 3.94100807\n",
      "Iteration 27, loss = 4.95580960\n",
      "Iteration 36, loss = 4.41257444\n",
      "Iteration 28, loss = 4.48133683\n",
      "Iteration 37, loss = 4.45390242\n",
      "Iteration 29, loss = 5.81592221\n",
      "Iteration 38, loss = 4.84768481\n",
      "Iteration 30, loss = 5.18925326\n",
      "Iteration 39, loss = 5.27981938\n",
      "Iteration 31, loss = 5.93548667\n",
      "Iteration 40, loss = 4.76977959\n",
      "Iteration 32, loss = 5.66101237\n",
      "Iteration 41, loss = 4.49529146\n",
      "Iteration 33, loss = 6.01041903\n",
      "Iteration 42, loss = 5.23968822\n",
      "Iteration 34, loss = 5.18309008\n",
      "Iteration 43, loss = 4.69190003\n",
      "Iteration 35, loss = 4.82855813\n",
      "Iteration 44, loss = 4.14576639\n",
      "Iteration 36, loss = 4.98733419\n",
      "Iteration 45, loss = 3.87383850\n",
      "Iteration 37, loss = 5.02967042\n",
      "Iteration 46, loss = 4.18865103\n",
      "Iteration 38, loss = 5.77819185\n",
      "Iteration 47, loss = 4.11067290\n",
      "Iteration 39, loss = 5.93905922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 4.97342064\n",
      "Iteration 49, loss = 4.03191869\n",
      "Iteration 50, loss = 4.73904418\n",
      "Iteration 51, loss = 4.73917443\n",
      "Iteration 52, loss = 4.34656326\n",
      "Iteration 53, loss = 4.11107123\n",
      "Iteration 54, loss = 4.26795277\n",
      "Iteration 55, loss = 4.18845327\n",
      "Iteration 56, loss = 4.46238554\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.69699050\n",
      "Iteration 2, loss = 13.13635365\n",
      "Iteration 3, loss = 15.15099841\n",
      "Iteration 4, loss = 9.33593499\n",
      "Iteration 5, loss = 10.19976678\n",
      "Iteration 6, loss = 12.96275696\n",
      "Iteration 7, loss = 8.26057501\n",
      "Iteration 8, loss = 9.31108555\n",
      "Iteration 9, loss = 7.66963424\n",
      "Iteration 1, loss = 20.38231338\n",
      "Iteration 10, loss = 7.32252783\n",
      "Iteration 2, loss = 24.76580150\n",
      "Iteration 11, loss = 6.84309425\n",
      "Iteration 3, loss = 12.83479649\n",
      "Iteration 12, loss = 8.69267713\n",
      "Iteration 4, loss = 9.66033256\n",
      "Iteration 13, loss = 7.53384217\n",
      "Iteration 5, loss = 9.30491927\n",
      "Iteration 14, loss = 6.58727707\n",
      "Iteration 6, loss = 7.71348290\n",
      "Iteration 15, loss = 6.52624661\n",
      "Iteration 7, loss = 6.10399212\n",
      "Iteration 16, loss = 6.60811523\n",
      "Iteration 8, loss = 6.03904943\n",
      "Iteration 17, loss = 7.46076899\n",
      "Iteration 9, loss = 10.96021459\n",
      "Iteration 18, loss = 6.85461862\n",
      "Iteration 10, loss = 10.24027916\n",
      "Iteration 19, loss = 8.79846231\n",
      "Iteration 11, loss = 7.67366085\n",
      "Iteration 20, loss = 7.25049411\n",
      "Iteration 12, loss = 7.03410274\n",
      "Iteration 21, loss = 7.98176624\n",
      "Iteration 13, loss = 5.76740475\n",
      "Iteration 22, loss = 6.90921800\n",
      "Iteration 14, loss = 5.79362657\n",
      "Iteration 23, loss = 7.56501172\n",
      "Iteration 15, loss = 5.44403319\n",
      "Iteration 24, loss = 7.71094779\n",
      "Iteration 16, loss = 5.86140879\n",
      "Iteration 25, loss = 5.79936959\n",
      "Iteration 17, loss = 6.14438381\n",
      "Iteration 26, loss = 5.84758099\n",
      "Iteration 18, loss = 6.06017771\n",
      "Iteration 27, loss = 6.28604144\n",
      "Iteration 19, loss = 6.63808736\n",
      "Iteration 28, loss = 6.36919585\n",
      "Iteration 20, loss = 6.50055619\n",
      "Iteration 29, loss = 6.37920234\n",
      "Iteration 21, loss = 5.92643782\n",
      "Iteration 30, loss = 6.29499102\n",
      "Iteration 22, loss = 6.28606830\n",
      "Iteration 31, loss = 5.43409345\n",
      "Iteration 23, loss = 6.91789417\n",
      "Iteration 32, loss = 5.16129798\n",
      "Iteration 24, loss = 5.90729682\n",
      "Iteration 33, loss = 4.49575474\n",
      "Iteration 25, loss = 5.95390853\n",
      "Iteration 34, loss = 5.83044768\n",
      "Iteration 26, loss = 5.28882895\n",
      "Iteration 35, loss = 5.67488094\n",
      "Iteration 27, loss = 5.21127573\n",
      "Iteration 36, loss = 5.20564092\n",
      "Iteration 28, loss = 5.48636171\n",
      "Iteration 37, loss = 5.01063889\n",
      "Iteration 29, loss = 4.97706657\n",
      "Iteration 38, loss = 4.46158384\n",
      "Iteration 30, loss = 4.74307782\n",
      "Iteration 39, loss = 5.01227764\n",
      "Iteration 31, loss = 4.54886060\n",
      "Iteration 40, loss = 5.09250850\n",
      "Iteration 32, loss = 4.58995319\n",
      "Iteration 41, loss = 5.17203291\n",
      "Iteration 33, loss = 4.19561112\n",
      "Iteration 42, loss = 5.05590206\n",
      "Iteration 34, loss = 4.00238566\n",
      "Iteration 43, loss = 4.42984977\n",
      "Iteration 35, loss = 4.31613318\n",
      "Iteration 44, loss = 4.54878069\n",
      "Iteration 36, loss = 4.47384051\n",
      "Iteration 45, loss = 4.86334885\n",
      "Iteration 37, loss = 4.86712313\n",
      "Iteration 46, loss = 4.78510584\n",
      "Iteration 38, loss = 4.47535515\n",
      "Iteration 47, loss = 4.94152480\n",
      "Iteration 39, loss = 4.60714453\n",
      "Iteration 48, loss = 4.47089352\n",
      "Iteration 40, loss = 4.16162238\n",
      "Iteration 49, loss = 4.13097776\n",
      "Iteration 41, loss = 4.12268347\n",
      "Iteration 50, loss = 4.00075191\n",
      "Iteration 42, loss = 4.71119655\n",
      "Iteration 51, loss = 4.70726307\n",
      "Iteration 43, loss = 4.51500791\n",
      "Iteration 52, loss = 4.70781205\n",
      "Iteration 44, loss = 5.29458020\n",
      "Iteration 53, loss = 4.51211142\n",
      "Iteration 45, loss = 5.53287989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 4.61766257\n",
      "Iteration 55, loss = 5.02150547\n",
      "Iteration 56, loss = 4.74664305\n",
      "Iteration 57, loss = 4.58923832\n",
      "Iteration 58, loss = 4.43239030\n",
      "Iteration 59, loss = 4.27564547\n",
      "Iteration 60, loss = 3.92295608\n",
      "Iteration 61, loss = 4.15982115\n",
      "Iteration 62, loss = 5.13912482\n",
      "Iteration 63, loss = 5.68818595\n",
      "Iteration 64, loss = 4.98202482\n",
      "Iteration 65, loss = 4.27559082\n",
      "Iteration 66, loss = 4.19768706\n",
      "Iteration 67, loss = 4.23719992\n",
      "Iteration 68, loss = 4.15888814\n",
      "Iteration 69, loss = 4.13873881\n",
      "Iteration 70, loss = 3.92346758\n",
      "Iteration 71, loss = 4.35445835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.42356348\n",
      "Iteration 2, loss = 16.39517945\n",
      "Iteration 3, loss = 9.74726054\n",
      "Iteration 4, loss = 9.58120631\n",
      "Iteration 5, loss = 9.03157726\n",
      "Iteration 6, loss = 7.05219016\n",
      "Iteration 7, loss = 8.99608079\n",
      "Iteration 8, loss = 13.51892448\n",
      "Iteration 9, loss = 8.35213538\n",
      "Iteration 10, loss = 6.58243076\n",
      "Iteration 11, loss = 10.10352419\n",
      "Iteration 12, loss = 8.64258100\n",
      "Iteration 13, loss = 8.34701406\n",
      "Iteration 14, loss = 7.29547177\n",
      "Iteration 15, loss = 5.92098548\n",
      "Iteration 16, loss = 6.88874659\n",
      "Iteration 17, loss = 7.14323159\n",
      "Iteration 18, loss = 5.74977403\n",
      "Iteration 1, loss = 16.47652104\n",
      "Iteration 19, loss = 5.80646387\n",
      "Iteration 2, loss = 15.69579955\n",
      "Iteration 20, loss = 6.72384491\n",
      "Iteration 3, loss = 12.55785472\n",
      "Iteration 21, loss = 6.38138603\n",
      "Iteration 4, loss = 10.84578432\n",
      "Iteration 22, loss = 6.07409073\n",
      "Iteration 5, loss = 8.86153004\n",
      "Iteration 23, loss = 7.92110389\n",
      "Iteration 6, loss = 11.67263659\n",
      "Iteration 24, loss = 7.21873969\n",
      "Iteration 7, loss = 9.96986315\n",
      "Iteration 25, loss = 6.12278351\n",
      "Iteration 8, loss = 12.75424047\n",
      "Iteration 26, loss = 5.06499342\n",
      "Iteration 9, loss = 12.94788062\n",
      "Iteration 27, loss = 5.57616110\n",
      "Iteration 10, loss = 8.89522405\n",
      "Iteration 28, loss = 5.26426856\n",
      "Iteration 11, loss = 8.61777810\n",
      "Iteration 29, loss = 5.10888355\n",
      "Iteration 12, loss = 7.66673642\n",
      "Iteration 30, loss = 5.07118080\n",
      "Iteration 13, loss = 10.97604132\n",
      "Iteration 31, loss = 5.07283432\n",
      "Iteration 14, loss = 10.28094524\n",
      "Iteration 32, loss = 5.30928475\n",
      "Iteration 15, loss = 12.99565736\n",
      "Iteration 33, loss = 4.87851564\n",
      "Iteration 16, loss = 9.39110675\n",
      "Iteration 34, loss = 5.42758245\n",
      "Iteration 17, loss = 11.39896685\n",
      "Iteration 35, loss = 4.32021460\n",
      "Iteration 18, loss = 7.17814067\n",
      "Iteration 36, loss = 4.60385915\n",
      "Iteration 19, loss = 8.33205106\n",
      "Iteration 37, loss = 4.76059728\n",
      "Iteration 20, loss = 6.94892991\n",
      "Iteration 38, loss = 4.64308260\n",
      "Iteration 21, loss = 8.24847918\n",
      "Iteration 39, loss = 4.29010955\n",
      "Iteration 22, loss = 8.82416271\n",
      "Iteration 40, loss = 4.68206454\n",
      "Iteration 23, loss = 7.42609520\n",
      "Iteration 41, loss = 4.09326321\n",
      "Iteration 24, loss = 8.53347020\n",
      "Iteration 42, loss = 4.09311034\n",
      "Iteration 43, loss = 4.53497164\n",
      "Iteration 25, loss = 8.58310576\n",
      "Iteration 44, loss = 4.32840269\n",
      "Iteration 26, loss = 8.12308525\n",
      "Iteration 45, loss = 4.53035187\n",
      "Iteration 27, loss = 9.31325734\n",
      "Iteration 46, loss = 4.64921654\n",
      "Iteration 28, loss = 8.81646125\n",
      "Iteration 47, loss = 4.41455978\n",
      "Iteration 29, loss = 7.17898126\n",
      "Iteration 48, loss = 4.61134618\n",
      "Iteration 30, loss = 6.36880489\n",
      "Iteration 49, loss = 4.77666716\n",
      "Iteration 31, loss = 6.69428624\n",
      "Iteration 50, loss = 4.77003692\n",
      "Iteration 32, loss = 6.07652463\n",
      "Iteration 51, loss = 4.75305649\n",
      "Iteration 33, loss = 5.69343718\n",
      "Iteration 52, loss = 4.77062048\n",
      "Iteration 34, loss = 5.62438977\n",
      "Iteration 53, loss = 4.45650883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 5.51324598\n",
      "Iteration 36, loss = 5.76230690\n",
      "Iteration 37, loss = 6.30907543\n",
      "Iteration 38, loss = 5.13505835\n",
      "Iteration 39, loss = 5.25300732\n",
      "Iteration 40, loss = 5.33227550\n",
      "Iteration 41, loss = 5.53769801\n",
      "Iteration 42, loss = 6.04249896\n",
      "Iteration 43, loss = 5.44402664\n",
      "Iteration 44, loss = 5.65550007\n",
      "Iteration 45, loss = 5.61775862\n",
      "Iteration 46, loss = 6.20920434\n",
      "Iteration 47, loss = 5.64336249\n",
      "Iteration 48, loss = 5.70236869\n",
      "Iteration 49, loss = 4.87797159\n",
      "Iteration 50, loss = 5.35022765\n",
      "Iteration 51, loss = 5.43085158\n",
      "Iteration 52, loss = 5.51241266\n",
      "Iteration 1, loss = 17.62086917\n",
      "Iteration 53, loss = 6.08286802\n",
      "Iteration 2, loss = 15.93023969\n",
      "Iteration 54, loss = 5.69677867\n",
      "Iteration 3, loss = 15.67133371\n",
      "Iteration 55, loss = 5.12910801\n",
      "Iteration 4, loss = 11.10360056\n",
      "Iteration 56, loss = 5.16778359\n",
      "Iteration 5, loss = 9.61631409\n",
      "Iteration 57, loss = 4.72738420\n",
      "Iteration 6, loss = 9.44069063\n",
      "Iteration 58, loss = 4.97389719\n",
      "Iteration 7, loss = 7.89463088\n",
      "Iteration 59, loss = 4.86960476\n",
      "Iteration 8, loss = 6.92783339\n",
      "Iteration 60, loss = 4.74087612\n",
      "Iteration 9, loss = 9.41481571\n",
      "Iteration 61, loss = 4.42738798\n",
      "Iteration 10, loss = 8.78522893\n",
      "Iteration 62, loss = 4.58438427\n",
      "Iteration 11, loss = 10.06242395\n",
      "Iteration 63, loss = 4.46650699\n",
      "Iteration 12, loss = 12.41643726\n",
      "Iteration 64, loss = 4.27033914\n",
      "Iteration 13, loss = 12.46346133\n",
      "Iteration 65, loss = 4.78080629\n",
      "Iteration 14, loss = 10.70250052\n",
      "Iteration 66, loss = 4.60365728\n",
      "Iteration 15, loss = 8.39409978\n",
      "Iteration 67, loss = 4.68658084\n",
      "Iteration 16, loss = 7.34458914\n",
      "Iteration 68, loss = 4.34830461\n",
      "Iteration 17, loss = 7.21889960\n",
      "Iteration 69, loss = 4.10999816\n",
      "Iteration 18, loss = 7.58317720\n",
      "Iteration 70, loss = 4.46572878\n",
      "Iteration 19, loss = 6.87349856\n",
      "Iteration 71, loss = 4.66157221\n",
      "Iteration 20, loss = 11.64305786\n",
      "Iteration 72, loss = 4.46479234\n",
      "Iteration 21, loss = 8.94383746\n",
      "Iteration 73, loss = 5.25055116\n",
      "Iteration 22, loss = 8.24584498\n",
      "Iteration 74, loss = 4.77974186\n",
      "Iteration 23, loss = 7.92153088\n",
      "Iteration 75, loss = 5.62326863\n",
      "Iteration 24, loss = 6.88554049\n",
      "Iteration 76, loss = 5.25099139\n",
      "Iteration 25, loss = 7.14851852\n",
      "Iteration 77, loss = 4.74013144\n",
      "Iteration 26, loss = 7.13292536\n",
      "Iteration 78, loss = 4.11146356\n",
      "Iteration 27, loss = 7.19103744\n",
      "Iteration 79, loss = 4.11103803\n",
      "Iteration 28, loss = 6.93695510\n",
      "Iteration 80, loss = 4.20966386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 5.85349931\n",
      "Iteration 30, loss = 5.98290871\n",
      "Iteration 31, loss = 5.55918248\n",
      "Iteration 32, loss = 5.13072024\n",
      "Iteration 33, loss = 5.95595269\n",
      "Iteration 34, loss = 5.96733105\n",
      "Iteration 35, loss = 5.29349689\n",
      "Iteration 36, loss = 5.64732279\n",
      "Iteration 37, loss = 5.53006826\n",
      "Iteration 38, loss = 4.94165666\n",
      "Iteration 39, loss = 4.66753190\n",
      "Iteration 40, loss = 5.21731278\n",
      "Iteration 41, loss = 5.53101629\n",
      "Iteration 42, loss = 5.92332880\n",
      "Iteration 43, loss = 5.21711996\n",
      "Iteration 44, loss = 5.17798132\n",
      "Iteration 45, loss = 4.51132196\n",
      "Iteration 46, loss = 5.09998208\n",
      "Iteration 1, loss = 18.70115552\n",
      "Iteration 47, loss = 4.70826721\n",
      "Iteration 2, loss = 16.37028840\n",
      "Iteration 48, loss = 4.90456208\n",
      "Iteration 3, loss = 14.72481817\n",
      "Iteration 49, loss = 4.74757776\n",
      "Iteration 4, loss = 8.21850732\n",
      "Iteration 5, loss = 9.95073671\n",
      "Iteration 50, loss = 5.17910156\n",
      "Iteration 6, loss = 9.18792868\n",
      "Iteration 51, loss = 4.59029377\n",
      "Iteration 7, loss = 8.73453818Iteration 52, loss = 5.41356890\n",
      "\n",
      "Iteration 8, loss = 9.18207255\n",
      "Iteration 53, loss = 5.21738282\n",
      "Iteration 54, loss = 5.09938828\n",
      "Iteration 9, loss = 7.31579736\n",
      "Iteration 55, loss = 5.80477404\n",
      "Iteration 10, loss = 6.56084298\n",
      "Iteration 11, loss = 8.60747489\n",
      "Iteration 56, loss = 5.60826226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 9.45937762\n",
      "Iteration 13, loss = 7.59027950\n",
      "Iteration 14, loss = 7.03701379\n",
      "Iteration 15, loss = 7.28415996\n",
      "Iteration 16, loss = 7.82321365\n",
      "Iteration 17, loss = 7.94991541\n",
      "Iteration 18, loss = 7.01295465\n",
      "Iteration 19, loss = 7.36898357\n",
      "Iteration 20, loss = 6.50679993\n",
      "Iteration 21, loss = 6.38980424\n",
      "Iteration 22, loss = 6.58632374\n",
      "Iteration 23, loss = 6.35196030\n",
      "Iteration 24, loss = 6.69112651\n",
      "Iteration 25, loss = 6.39024143\n",
      "Iteration 26, loss = 5.64423885\n",
      "Iteration 27, loss = 5.52631296\n",
      "Iteration 28, loss = 6.27176909\n",
      "Iteration 1, loss = 20.19179417\n",
      "Iteration 29, loss = 6.78210258\n",
      "Iteration 2, loss = 19.13703522\n",
      "Iteration 30, loss = 7.99713322\n",
      "Iteration 3, loss = 16.47092826\n",
      "Iteration 31, loss = 6.07296684\n",
      "Iteration 4, loss = 13.64606428\n",
      "Iteration 32, loss = 7.24460680\n",
      "Iteration 5, loss = 10.85748887\n",
      "Iteration 33, loss = 5.68524923\n",
      "Iteration 6, loss = 8.90756279\n",
      "Iteration 34, loss = 5.61203120\n",
      "Iteration 7, loss = 11.22069057\n",
      "Iteration 35, loss = 5.06713832\n",
      "Iteration 36, loss = 6.20813067\n",
      "Iteration 8, loss = 9.41609690\n",
      "Iteration 37, loss = 6.25187318\n",
      "Iteration 9, loss = 9.11868774\n",
      "Iteration 38, loss = 5.86299342\n",
      "Iteration 10, loss = 9.40414344\n",
      "Iteration 39, loss = 9.47195830\n",
      "Iteration 11, loss = 8.35706206\n",
      "Iteration 40, loss = 4.92166706\n",
      "Iteration 12, loss = 7.87478931\n",
      "Iteration 41, loss = 5.44558949\n",
      "Iteration 13, loss = 8.09177731\n",
      "Iteration 42, loss = 6.68223483\n",
      "Iteration 14, loss = 7.85975725\n",
      "Iteration 43, loss = 7.91733808\n",
      "Iteration 15, loss = 8.35095418\n",
      "Iteration 44, loss = 8.83222427\n",
      "Iteration 16, loss = 9.64810599\n",
      "Iteration 45, loss = 8.25244709\n",
      "Iteration 17, loss = 7.28623315\n",
      "Iteration 46, loss = 7.75745459\n",
      "Iteration 18, loss = 6.63898849\n",
      "Iteration 47, loss = 7.97195716\n",
      "Iteration 19, loss = 5.86749253\n",
      "Iteration 48, loss = 7.64672994\n",
      "Iteration 20, loss = 7.44749926\n",
      "Iteration 49, loss = 6.18800167\n",
      "Iteration 21, loss = 6.55317188\n",
      "Iteration 50, loss = 5.34944617\n",
      "Iteration 22, loss = 5.61816089\n",
      "Iteration 51, loss = 6.93626205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 5.70570863\n",
      "Iteration 24, loss = 5.59749374\n",
      "Iteration 25, loss = 5.60484332\n",
      "Iteration 26, loss = 5.88847414\n",
      "Iteration 27, loss = 5.69976270\n",
      "Iteration 28, loss = 5.86205745\n",
      "Iteration 29, loss = 5.98137432\n",
      "Iteration 30, loss = 7.59029535\n",
      "Iteration 31, loss = 6.72692340\n",
      "Iteration 32, loss = 6.09460100\n",
      "Iteration 33, loss = 6.01107364\n",
      "Iteration 34, loss = 7.46154935\n",
      "Iteration 35, loss = 5.96782446\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 21.44613666\n",
      "Iteration 2, loss = 17.42678521\n",
      "Iteration 3, loss = 8.85993860\n",
      "Iteration 4, loss = 12.47349076\n",
      "Iteration 5, loss = 16.20253367\n",
      "Iteration 6, loss = 10.03595588\n",
      "Iteration 7, loss = 12.29385925\n",
      "Iteration 8, loss = 10.86668629\n",
      "Iteration 9, loss = 10.64182621\n",
      "Iteration 10, loss = 8.30710833\n",
      "Iteration 11, loss = 7.53726531\n",
      "Iteration 12, loss = 8.24437630\n",
      "Iteration 13, loss = 8.46450216\n",
      "Iteration 1, loss = 17.69944082\n",
      "Iteration 14, loss = 8.10165268\n",
      "Iteration 2, loss = 14.96317966\n",
      "Iteration 15, loss = 8.00966045\n",
      "Iteration 3, loss = 10.55528028\n",
      "Iteration 16, loss = 8.14654656\n",
      "Iteration 4, loss = 10.44696715\n",
      "Iteration 17, loss = 7.19160917\n",
      "Iteration 5, loss = 8.87421611\n",
      "Iteration 18, loss = 7.94073117\n",
      "Iteration 6, loss = 9.26649196\n",
      "Iteration 19, loss = 8.40045548\n",
      "Iteration 20, loss = 9.63388559\n",
      "Iteration 7, loss = 7.35941689\n",
      "Iteration 21, loss = 8.35005756\n",
      "Iteration 8, loss = 7.90028126\n",
      "Iteration 22, loss = 8.27652893\n",
      "Iteration 9, loss = 9.46980197\n",
      "Iteration 23, loss = 7.06258702\n",
      "Iteration 10, loss = 7.91869333\n",
      "Iteration 24, loss = 7.53256100\n",
      "Iteration 11, loss = 8.59060074\n",
      "Iteration 25, loss = 7.37523136\n",
      "Iteration 12, loss = 7.09720907\n",
      "Iteration 26, loss = 6.51458950\n",
      "Iteration 13, loss = 10.06976548\n",
      "Iteration 27, loss = 6.75411455\n",
      "Iteration 14, loss = 8.80212427\n",
      "Iteration 28, loss = 7.15242061\n",
      "Iteration 15, loss = 7.13693965\n",
      "Iteration 29, loss = 7.39071936\n",
      "Iteration 16, loss = 7.55030042\n",
      "Iteration 30, loss = 8.44778044\n",
      "Iteration 17, loss = 8.66551533\n",
      "Iteration 31, loss = 8.76049369\n",
      "Iteration 18, loss = 6.83076851\n",
      "Iteration 32, loss = 6.95816436\n",
      "Iteration 19, loss = 6.60074191\n",
      "Iteration 33, loss = 6.52891555\n",
      "Iteration 20, loss = 6.09723537\n",
      "Iteration 34, loss = 6.73045050\n",
      "Iteration 21, loss = 6.29824736\n",
      "Iteration 35, loss = 6.93164283\n",
      "Iteration 22, loss = 6.18543082\n",
      "Iteration 36, loss = 6.89556867\n",
      "Iteration 23, loss = 5.96340077\n",
      "Iteration 37, loss = 6.11179988\n",
      "Iteration 24, loss = 5.05755088\n",
      "Iteration 38, loss = 6.10977410\n",
      "Iteration 25, loss = 5.17962664\n",
      "Iteration 26, loss = 5.30069220\n",
      "Iteration 39, loss = 5.91685241\n",
      "Iteration 40, loss = 5.99771513Iteration 27, loss = 5.18452019\n",
      "\n",
      "Iteration 28, loss = 5.18620679\n",
      "Iteration 41, loss = 6.35176891\n",
      "Iteration 42, loss = 6.15834246\n",
      "Iteration 29, loss = 5.34362579\n",
      "Iteration 43, loss = 5.88468408\n",
      "Iteration 30, loss = 4.79420957\n",
      "Iteration 44, loss = 5.84615661\n",
      "Iteration 31, loss = 4.36367377\n",
      "Iteration 45, loss = 5.29706576\n",
      "Iteration 32, loss = 4.52097475\n",
      "Iteration 46, loss = 5.76688604\n",
      "Iteration 33, loss = 4.28572159\n",
      "Iteration 47, loss = 5.72695915\n",
      "Iteration 34, loss = 4.75733186\n",
      "Iteration 35, loss = 4.75843751\n",
      "Iteration 48, loss = 5.37267992\n",
      "Iteration 36, loss = 4.21043821\n",
      "Iteration 49, loss = 5.29349314\n",
      "Iteration 37, loss = 4.43501933\n",
      "Iteration 50, loss = 5.33223314\n",
      "Iteration 38, loss = 4.52566885\n",
      "Iteration 51, loss = 5.37084018\n",
      "Iteration 39, loss = 4.36900911\n",
      "Iteration 52, loss = 5.68658785\n",
      "Iteration 40, loss = 4.33068538\n",
      "Iteration 53, loss = 6.15981214\n",
      "Iteration 41, loss = 4.52865986\n",
      "Iteration 54, loss = 5.96798607\n",
      "Iteration 42, loss = 4.33366016\n",
      "Iteration 55, loss = 5.34406084\n",
      "Iteration 43, loss = 4.41321112\n",
      "Iteration 56, loss = 5.54183720\n",
      "Iteration 44, loss = 4.33443676\n",
      "Iteration 57, loss = 5.07242678\n",
      "Iteration 45, loss = 4.25515384\n",
      "Iteration 58, loss = 5.03458278\n",
      "Iteration 46, loss = 4.58385559\n",
      "Iteration 59, loss = 5.19350022\n",
      "Iteration 47, loss = 4.29384788\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 5.38976976\n",
      "Iteration 61, loss = 5.78158969\n",
      "Iteration 62, loss = 5.35292729\n",
      "Iteration 63, loss = 5.51337488\n",
      "Iteration 64, loss = 5.32280207\n",
      "Iteration 65, loss = 5.91695572\n",
      "Iteration 66, loss = 5.68488767\n",
      "Iteration 67, loss = 6.11840227\n",
      "Iteration 68, loss = 5.64800765\n",
      "Iteration 69, loss = 5.06115105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.53079817\n",
      "Iteration 2, loss = 14.96110415\n",
      "Iteration 3, loss = 10.61402354\n",
      "Iteration 4, loss = 10.72288497\n",
      "Iteration 5, loss = 9.01320711\n",
      "Iteration 6, loss = 11.87739064\n",
      "Iteration 7, loss = 10.25608632\n",
      "Iteration 8, loss = 9.59740432\n",
      "Iteration 9, loss = 8.50925702\n",
      "Iteration 10, loss = 9.08547052\n",
      "Iteration 1, loss = 17.16661054\n",
      "Iteration 11, loss = 7.13836887\n",
      "Iteration 12, loss = 7.91550511\n",
      "Iteration 2, loss = 11.21581154\n",
      "Iteration 13, loss = 7.28360351\n",
      "Iteration 3, loss = 14.59221338\n",
      "Iteration 14, loss = 8.76218972\n",
      "Iteration 4, loss = 10.69992384\n",
      "Iteration 5, loss = 8.49940083\n",
      "Iteration 15, loss = 6.81328817\n",
      "Iteration 6, loss = 9.64705269\n",
      "Iteration 16, loss = 5.80323658\n",
      "Iteration 7, loss = 9.51322920\n",
      "Iteration 17, loss = 5.30170061\n",
      "Iteration 8, loss = 7.80022628\n",
      "Iteration 18, loss = 6.28918843\n",
      "Iteration 9, loss = 7.37711073\n",
      "Iteration 19, loss = 6.13897346\n",
      "Iteration 20, loss = 7.04584506\n",
      "Iteration 10, loss = 8.28813885\n",
      "Iteration 21, loss = 5.79390093\n",
      "Iteration 11, loss = 8.97222848\n",
      "Iteration 22, loss = 5.32534421\n",
      "Iteration 12, loss = 9.17035740\n",
      "Iteration 13, loss = 8.34323975\n",
      "Iteration 23, loss = 5.79796935\n",
      "Iteration 14, loss = 6.72332920\n",
      "Iteration 24, loss = 6.85899411\n",
      "Iteration 15, loss = 7.13882236\n",
      "Iteration 25, loss = 5.52772506\n",
      "Iteration 16, loss = 6.37569620\n",
      "Iteration 26, loss = 5.25436844\n",
      "Iteration 17, loss = 6.47197967\n",
      "Iteration 27, loss = 6.00083341\n",
      "Iteration 18, loss = 6.68448409\n",
      "Iteration 28, loss = 5.61015138\n",
      "Iteration 19, loss = 7.05108682\n",
      "Iteration 29, loss = 6.23204324\n",
      "Iteration 20, loss = 7.45158788\n",
      "Iteration 30, loss = 5.72928449\n",
      "Iteration 31, loss = 5.29778795\n",
      "Iteration 21, loss = 6.78975034\n",
      "Iteration 32, loss = 4.82687633\n",
      "Iteration 22, loss = 6.51849121\n",
      "Iteration 33, loss = 5.35054747\n",
      "Iteration 23, loss = 6.99067514\n",
      "Iteration 34, loss = 5.03886733\n",
      "Iteration 24, loss = 6.95383168\n",
      "Iteration 35, loss = 5.68869713\n",
      "Iteration 25, loss = 6.76083954\n",
      "Iteration 36, loss = 5.32348954\n",
      "Iteration 26, loss = 6.21503463\n",
      "Iteration 37, loss = 5.77687589\n",
      "Iteration 27, loss = 5.86499041\n",
      "Iteration 38, loss = 5.32836310\n",
      "Iteration 28, loss = 5.71070108\n",
      "Iteration 39, loss = 5.10004602\n",
      "Iteration 29, loss = 5.83149924\n",
      "Iteration 40, loss = 5.72672235\n",
      "Iteration 30, loss = 4.89373428\n",
      "Iteration 31, loss = 4.81869285\n",
      "Iteration 41, loss = 5.92593227\n",
      "Iteration 32, loss = 4.70387684\n",
      "Iteration 42, loss = 6.12061072\n",
      "Iteration 33, loss = 4.55014422\n",
      "Iteration 43, loss = 5.09893246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 4.59789253\n",
      "Iteration 35, loss = 4.71109375\n",
      "Iteration 36, loss = 4.55560002\n",
      "Iteration 37, loss = 4.51732366\n",
      "Iteration 38, loss = 4.69098360\n",
      "Iteration 39, loss = 4.51848661\n",
      "Iteration 40, loss = 4.39912190\n",
      "Iteration 41, loss = 4.19052269\n",
      "Iteration 42, loss = 5.17353305\n",
      "Iteration 43, loss = 4.79235747\n",
      "Iteration 44, loss = 4.34298230\n",
      "Iteration 45, loss = 4.24396885\n",
      "Iteration 46, loss = 4.12418748\n",
      "Iteration 47, loss = 4.68827148\n",
      "Iteration 48, loss = 4.32614448\n",
      "Iteration 49, loss = 4.20972595\n",
      "Iteration 50, loss = 3.89629260\n",
      "Iteration 1, loss = 21.81449849\n",
      "Iteration 51, loss = 4.05698159\n",
      "Iteration 2, loss = 23.67784350\n",
      "Iteration 52, loss = 4.44520714\n",
      "Iteration 3, loss = 13.92151896\n",
      "Iteration 53, loss = 4.32734695\n",
      "Iteration 4, loss = 11.22151303\n",
      "Iteration 54, loss = 4.45598540\n",
      "Iteration 5, loss = 11.92922296\n",
      "Iteration 55, loss = 4.52339706\n",
      "Iteration 6, loss = 10.23559240\n",
      "Iteration 56, loss = 3.89426406\n",
      "Iteration 7, loss = 8.48585741\n",
      "Iteration 57, loss = 4.20804526\n",
      "Iteration 8, loss = 8.90601692\n",
      "Iteration 58, loss = 4.36484424\n",
      "Iteration 9, loss = 8.28996404\n",
      "Iteration 59, loss = 4.36492370\n",
      "Iteration 10, loss = 6.93425817\n",
      "Iteration 60, loss = 4.44381541\n",
      "Iteration 11, loss = 10.09122691\n",
      "Iteration 61, loss = 4.32915310\n",
      "Iteration 12, loss = 8.70893254\n",
      "Iteration 62, loss = 4.33143983\n",
      "Iteration 13, loss = 7.30709619\n",
      "Iteration 63, loss = 4.05807227\n",
      "Iteration 14, loss = 7.23404396\n",
      "Iteration 64, loss = 4.13693323\n",
      "Iteration 15, loss = 6.68064941\n",
      "Iteration 16, loss = 6.14752082\n",
      "Iteration 65, loss = 3.82347490\n",
      "Iteration 17, loss = 6.15687956\n",
      "Iteration 66, loss = 3.98018809\n",
      "Iteration 18, loss = 7.76662892\n",
      "Iteration 67, loss = 4.00646582\n",
      "Iteration 68, loss = 4.09805385\n",
      "Iteration 69, loss = 3.82343717\n",
      "Iteration 19, loss = 8.42025395\n",
      "Iteration 70, loss = 4.02108622\n",
      "Iteration 20, loss = 6.09052256\n",
      "Iteration 21, loss = 5.84005444\n",
      "Iteration 22, loss = 5.23108994\n",
      "Iteration 71, loss = 4.09483683\n",
      "Iteration 72, loss = 3.67004901\n",
      "Iteration 23, loss = 5.21453639\n",
      "Iteration 73, loss = 4.25995464\n",
      "Iteration 24, loss = 4.65572122\n",
      "Iteration 74, loss = 4.18494963\n",
      "Iteration 25, loss = 4.41580758\n",
      "Iteration 75, loss = 4.22764659\n",
      "Iteration 26, loss = 4.73556683\n",
      "Iteration 76, loss = 3.83986795\n",
      "Iteration 27, loss = 4.35189251\n",
      "Iteration 77, loss = 3.92124185\n",
      "Iteration 28, loss = 4.71646483\n",
      "Iteration 78, loss = 4.15428654\n",
      "Iteration 29, loss = 4.41135565\n",
      "Iteration 79, loss = 4.47259197\n",
      "Iteration 30, loss = 4.76877258\n",
      "Iteration 80, loss = 3.41363107\n",
      "Iteration 31, loss = 4.37718433\n",
      "Iteration 32, loss = 4.15529636\n",
      "Iteration 81, loss = 3.68692566\n",
      "Iteration 33, loss = 3.98475766\n",
      "Iteration 82, loss = 3.88411956\n",
      "Iteration 34, loss = 4.23863292\n",
      "Iteration 83, loss = 3.58003430\n",
      "Iteration 35, loss = 4.33859412\n",
      "Iteration 84, loss = 3.68948817\n",
      "Iteration 36, loss = 4.29977092\n",
      "Iteration 85, loss = 3.53279482\n",
      "Iteration 37, loss = 4.33818274\n",
      "Iteration 86, loss = 3.85207114\n",
      "Iteration 38, loss = 4.72831370\n",
      "Iteration 87, loss = 3.51751021\n",
      "Iteration 39, loss = 4.37351653\n",
      "Iteration 88, loss = 3.62830820\n",
      "Iteration 40, loss = 4.17587439\n",
      "Iteration 89, loss = 3.53383553\n",
      "Iteration 41, loss = 4.56607188\n",
      "Iteration 90, loss = 3.37798459\n",
      "Iteration 42, loss = 4.09395349\n",
      "Iteration 91, loss = 3.34386422\n",
      "Iteration 43, loss = 4.21082934\n",
      "Iteration 92, loss = 3.26680961\n",
      "Iteration 44, loss = 3.81769969\n",
      "Iteration 93, loss = 3.38322980\n",
      "Iteration 45, loss = 3.77761110\n",
      "Iteration 94, loss = 3.20344680\n",
      "Iteration 46, loss = 3.89469859\n",
      "Iteration 95, loss = 3.13841850\n",
      "Iteration 47, loss = 4.09006142\n",
      "Iteration 96, loss = 3.14822124\n",
      "Iteration 48, loss = 3.89964976\n",
      "Iteration 97, loss = 3.34604762\n",
      "Iteration 49, loss = 3.77537076\n",
      "Iteration 98, loss = 3.26600884\n",
      "Iteration 50, loss = 4.08871755\n",
      "Iteration 99, loss = 3.54056581\n",
      "Iteration 51, loss = 4.48046447\n",
      "Iteration 100, loss = 3.63691751\n",
      "Iteration 52, loss = 4.01035261\n",
      "Iteration 101, loss = 3.81480477\n",
      "Iteration 102, loss = 3.50078677\n",
      "Iteration 53, loss = 3.74032932\n",
      "Iteration 103, loss = 3.54018280\n",
      "Iteration 54, loss = 4.28437536\n",
      "Iteration 104, loss = 3.65764860\n",
      "Iteration 55, loss = 4.47937584\n",
      "Iteration 105, loss = 4.04978949\n",
      "Iteration 56, loss = 4.36041751\n",
      "Iteration 106, loss = 3.14776249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 4.51670024\n",
      "Iteration 58, loss = 4.43790116\n",
      "Iteration 59, loss = 4.28010825\n",
      "Iteration 60, loss = 4.23985488\n",
      "Iteration 61, loss = 4.08258745\n",
      "Iteration 62, loss = 4.04338550\n",
      "Iteration 63, loss = 4.39664057\n",
      "Iteration 64, loss = 4.35797646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.10264726\n",
      "Iteration 2, loss = 15.99955040\n",
      "Iteration 3, loss = 13.96186854\n",
      "Iteration 4, loss = 13.44508441\n",
      "Iteration 5, loss = 10.74053261\n",
      "Iteration 6, loss = 9.81076657\n",
      "Iteration 7, loss = 8.97141912\n",
      "Iteration 1, loss = 19.54457643\n",
      "Iteration 8, loss = 9.67682772\n",
      "Iteration 2, loss = 19.01738687\n",
      "Iteration 9, loss = 8.89971791\n",
      "Iteration 3, loss = 18.34692957\n",
      "Iteration 10, loss = 7.85361059\n",
      "Iteration 4, loss = 10.68830325\n",
      "Iteration 11, loss = 6.89476360\n",
      "Iteration 5, loss = 8.41722310\n",
      "Iteration 12, loss = 6.58343312\n",
      "Iteration 6, loss = 8.44645475\n",
      "Iteration 13, loss = 7.24263194\n",
      "Iteration 14, loss = 6.99067268\n",
      "Iteration 7, loss = 8.17980229\n",
      "Iteration 15, loss = 7.79008260\n",
      "Iteration 8, loss = 11.93143326\n",
      "Iteration 16, loss = 6.30956000\n",
      "Iteration 9, loss = 10.96285866\n",
      "Iteration 17, loss = 6.11943850\n",
      "Iteration 10, loss = 10.15166659\n",
      "Iteration 18, loss = 6.12231912\n",
      "Iteration 11, loss = 8.30617889\n",
      "Iteration 19, loss = 6.94707177\n",
      "Iteration 12, loss = 8.50572998\n",
      "Iteration 20, loss = 6.83067666\n",
      "Iteration 13, loss = 9.68125381\n",
      "Iteration 21, loss = 6.00901114\n",
      "Iteration 14, loss = 8.16782450\n",
      "Iteration 22, loss = 6.32306683\n",
      "Iteration 15, loss = 7.09272943\n",
      "Iteration 23, loss = 5.77266740\n",
      "Iteration 16, loss = 6.98091728\n",
      "Iteration 24, loss = 6.36205035\n",
      "Iteration 17, loss = 6.93430162\n",
      "Iteration 25, loss = 5.93822776\n",
      "Iteration 18, loss = 6.95535579\n",
      "Iteration 26, loss = 5.67004674\n",
      "Iteration 19, loss = 6.45797815\n",
      "Iteration 27, loss = 5.63907061\n",
      "Iteration 20, loss = 5.64050817\n",
      "Iteration 28, loss = 5.95950148\n",
      "Iteration 21, loss = 6.27303827\n",
      "Iteration 29, loss = 5.57095775\n",
      "Iteration 22, loss = 5.92169553\n",
      "Iteration 30, loss = 6.43597337\n",
      "Iteration 23, loss = 5.76531445\n",
      "Iteration 31, loss = 5.69219647\n",
      "Iteration 24, loss = 5.44455475\n",
      "Iteration 32, loss = 5.53743129\n",
      "Iteration 25, loss = 5.13750601\n",
      "Iteration 33, loss = 5.67714685\n",
      "Iteration 26, loss = 6.00090264\n",
      "Iteration 34, loss = 5.30793332\n",
      "Iteration 27, loss = 5.60722075\n",
      "Iteration 35, loss = 4.76210013\n",
      "Iteration 28, loss = 5.88117580\n",
      "Iteration 36, loss = 4.88247276\n",
      "Iteration 29, loss = 5.43208678\n",
      "Iteration 37, loss = 4.80648343\n",
      "Iteration 30, loss = 5.52664222\n",
      "Iteration 38, loss = 5.51548979\n",
      "Iteration 31, loss = 4.97653034\n",
      "Iteration 39, loss = 5.75404896\n",
      "Iteration 32, loss = 6.27150526\n",
      "Iteration 40, loss = 5.16670094\n",
      "Iteration 33, loss = 6.03558996\n",
      "Iteration 41, loss = 5.48110866\n",
      "Iteration 34, loss = 5.45100856\n",
      "Iteration 42, loss = 5.32541041\n",
      "Iteration 35, loss = 5.20966770\n",
      "Iteration 43, loss = 4.85649387\n",
      "Iteration 36, loss = 5.24817904\n",
      "Iteration 44, loss = 4.89658702\n",
      "Iteration 37, loss = 5.09054629\n",
      "Iteration 45, loss = 4.81804448\n",
      "Iteration 38, loss = 6.34651365\n",
      "Iteration 46, loss = 4.66098979\n",
      "Iteration 39, loss = 6.31044415\n",
      "Iteration 47, loss = 4.54375562\n",
      "Iteration 40, loss = 5.44170947\n",
      "Iteration 48, loss = 4.97535010\n",
      "Iteration 41, loss = 5.24424335\n",
      "Iteration 49, loss = 4.73968401\n",
      "Iteration 42, loss = 5.24290635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 5.01449898\n",
      "Iteration 51, loss = 5.29719675\n",
      "Iteration 52, loss = 5.76484901\n",
      "Iteration 53, loss = 5.49250630\n",
      "Iteration 54, loss = 5.49295592\n",
      "Iteration 55, loss = 5.02283418\n",
      "Iteration 56, loss = 5.18118763\n",
      "Iteration 57, loss = 4.94826397\n",
      "Iteration 58, loss = 4.28556419\n",
      "Iteration 59, loss = 4.76003192\n",
      "Iteration 60, loss = 4.48796100\n",
      "Iteration 61, loss = 4.76444375\n",
      "Iteration 62, loss = 4.68754681\n",
      "Iteration 63, loss = 4.02089048\n",
      "Iteration 64, loss = 4.37454762\n",
      "Iteration 65, loss = 4.29797722\n",
      "Iteration 66, loss = 4.41782260\n",
      "Iteration 67, loss = 3.83116363\n",
      "Iteration 68, loss = 4.55598371\n",
      "Iteration 1, loss = 17.05873594\n",
      "Iteration 69, loss = 4.38266680\n",
      "Iteration 2, loss = 18.54413706\n",
      "Iteration 70, loss = 3.91060614\n",
      "Iteration 3, loss = 13.70221816\n",
      "Iteration 71, loss = 3.79523860\n",
      "Iteration 4, loss = 11.17934797\n",
      "Iteration 72, loss = 4.30596601\n",
      "Iteration 5, loss = 10.20437000\n",
      "Iteration 73, loss = 4.11084748\n",
      "Iteration 6, loss = 12.54673847\n",
      "Iteration 74, loss = 4.50507010\n",
      "Iteration 7, loss = 13.31893547\n",
      "Iteration 75, loss = 4.74157605\n",
      "Iteration 8, loss = 8.53627642\n",
      "Iteration 76, loss = 4.15378231\n",
      "Iteration 9, loss = 10.50993203\n",
      "Iteration 77, loss = 4.54624651\n",
      "Iteration 10, loss = 8.29202865\n",
      "Iteration 78, loss = 4.78155261\n",
      "Iteration 11, loss = 11.66941768\n",
      "Iteration 79, loss = 4.82041340\n",
      "Iteration 12, loss = 9.43049276\n",
      "Iteration 80, loss = 5.01632116\n",
      "Iteration 13, loss = 8.94372994\n",
      "Iteration 81, loss = 4.35180233\n",
      "Iteration 14, loss = 10.39933741\n",
      "Iteration 82, loss = 4.70793211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 9.13260013\n",
      "Iteration 16, loss = 10.70095485\n",
      "Iteration 17, loss = 7.32981060\n",
      "Iteration 18, loss = 7.37799786\n",
      "Iteration 19, loss = 7.72690494\n",
      "Iteration 20, loss = 9.74949564\n",
      "Iteration 21, loss = 10.26078225\n",
      "Iteration 22, loss = 15.32208532\n",
      "Iteration 23, loss = 9.47258688\n",
      "Iteration 24, loss = 8.91591979\n",
      "Iteration 25, loss = 9.96584829\n",
      "Iteration 26, loss = 9.56826291\n",
      "Iteration 27, loss = 8.88967709\n",
      "Iteration 28, loss = 8.89677700\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.86062821\n",
      "Iteration 2, loss = 19.17722686\n",
      "Iteration 3, loss = 14.53850065\n",
      "Iteration 4, loss = 15.65589700\n",
      "Iteration 5, loss = 12.85265487\n",
      "Iteration 6, loss = 9.85940792\n",
      "Iteration 7, loss = 10.03346034\n",
      "Iteration 8, loss = 8.44747638\n",
      "Iteration 9, loss = 7.62028315\n",
      "Iteration 10, loss = 8.40600039\n",
      "Iteration 11, loss = 9.29583982\n",
      "Iteration 12, loss = 8.92403804\n",
      "Iteration 13, loss = 7.57785828\n",
      "Iteration 14, loss = 8.36817406\n",
      "Iteration 1, loss = 20.36146258\n",
      "Iteration 15, loss = 8.23809474\n",
      "Iteration 2, loss = 19.12706773\n",
      "Iteration 16, loss = 7.74435084\n",
      "Iteration 3, loss = 15.81017100\n",
      "Iteration 17, loss = 7.03585943\n",
      "Iteration 4, loss = 9.53001874\n",
      "Iteration 18, loss = 6.97675144\n",
      "Iteration 5, loss = 8.41178197\n",
      "Iteration 19, loss = 6.47531640\n",
      "Iteration 6, loss = 7.09808809\n",
      "Iteration 20, loss = 7.97352910\n",
      "Iteration 7, loss = 7.95431928\n",
      "Iteration 21, loss = 7.31476025\n",
      "Iteration 8, loss = 7.70469753\n",
      "Iteration 22, loss = 6.92263221\n",
      "Iteration 9, loss = 8.08854392\n",
      "Iteration 23, loss = 6.45494123\n",
      "Iteration 10, loss = 11.26598814\n",
      "Iteration 24, loss = 7.38859751\n",
      "Iteration 11, loss = 10.89129467\n",
      "Iteration 25, loss = 6.53756528\n",
      "Iteration 12, loss = 6.54024410\n",
      "Iteration 26, loss = 5.55731904\n",
      "Iteration 13, loss = 8.22465258\n",
      "Iteration 27, loss = 5.91079084\n",
      "Iteration 14, loss = 6.29906176\n",
      "Iteration 28, loss = 5.99053638\n",
      "Iteration 15, loss = 5.70539667\n",
      "Iteration 29, loss = 6.06984082\n",
      "Iteration 16, loss = 4.90815422\n",
      "Iteration 30, loss = 6.18862141\n",
      "Iteration 17, loss = 5.32307592\n",
      "Iteration 31, loss = 6.31151633\n",
      "Iteration 18, loss = 4.94794956\n",
      "Iteration 32, loss = 6.54255206\n",
      "Iteration 19, loss = 5.43705995\n",
      "Iteration 33, loss = 6.50353435\n",
      "Iteration 20, loss = 5.10458130\n",
      "Iteration 34, loss = 6.46452498\n",
      "Iteration 21, loss = 4.72794057\n",
      "Iteration 35, loss = 6.38566950\n",
      "Iteration 22, loss = 5.13169550\n",
      "Iteration 36, loss = 5.87524973\n",
      "Iteration 23, loss = 4.78806565\n",
      "Iteration 37, loss = 5.40456079\n",
      "Iteration 24, loss = 4.36238283\n",
      "Iteration 38, loss = 5.52273108\n",
      "Iteration 25, loss = 4.36651023\n",
      "Iteration 39, loss = 5.38149862\n",
      "Iteration 26, loss = 5.74254585\n",
      "Iteration 40, loss = 5.99440903\n",
      "Iteration 27, loss = 5.31364964\n",
      "Iteration 41, loss = 5.56595661\n",
      "Iteration 28, loss = 5.66747179\n",
      "Iteration 42, loss = 5.99516971\n",
      "Iteration 29, loss = 5.07988615\n",
      "Iteration 43, loss = 5.76039552\n",
      "Iteration 30, loss = 5.20935906\n",
      "Iteration 44, loss = 5.48633262\n",
      "Iteration 31, loss = 5.12354813\n",
      "Iteration 45, loss = 5.52599359\n",
      "Iteration 32, loss = 5.04379349\n",
      "Iteration 46, loss = 6.11434983\n",
      "Iteration 33, loss = 5.23714272\n",
      "Iteration 47, loss = 5.56517176\n",
      "Iteration 34, loss = 5.35511955\n",
      "Iteration 48, loss = 6.09135995\n",
      "Iteration 35, loss = 4.73279536\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 5.74556610\n",
      "Iteration 50, loss = 6.38852641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.94136133\n",
      "Iteration 2, loss = 12.14026595\n",
      "Iteration 1, loss = 20.92330167\n",
      "Iteration 3, loss = 13.21503063\n",
      "Iteration 2, loss = 19.01781637\n",
      "Iteration 4, loss = 12.22606246\n",
      "Iteration 3, loss = 14.42273539\n",
      "Iteration 5, loss = 11.02479624\n",
      "Iteration 4, loss = 12.85338143\n",
      "Iteration 6, loss = 8.01005364\n",
      "Iteration 5, loss = 11.22102988\n",
      "Iteration 7, loss = 6.39870882\n",
      "Iteration 6, loss = 12.29514026\n",
      "Iteration 8, loss = 7.46558618\n",
      "Iteration 7, loss = 7.95503418\n",
      "Iteration 9, loss = 9.20341826\n",
      "Iteration 8, loss = 7.51611879\n",
      "Iteration 10, loss = 6.72938983\n",
      "Iteration 9, loss = 7.36028920\n",
      "Iteration 11, loss = 6.08989455\n",
      "Iteration 10, loss = 8.87554335\n",
      "Iteration 12, loss = 7.28818908\n",
      "Iteration 11, loss = 7.79154289\n",
      "Iteration 13, loss = 7.81781944\n",
      "Iteration 12, loss = 7.15362472\n",
      "Iteration 14, loss = 7.47066249\n",
      "Iteration 13, loss = 8.26068835\n",
      "Iteration 15, loss = 5.78028801\n",
      "Iteration 14, loss = 9.82309281\n",
      "Iteration 16, loss = 5.10412582\n",
      "Iteration 15, loss = 9.05972347\n",
      "Iteration 17, loss = 6.02854295\n",
      "Iteration 16, loss = 7.73896464\n",
      "Iteration 18, loss = 5.29609815\n",
      "Iteration 17, loss = 6.92586896\n",
      "Iteration 19, loss = 5.97335277\n",
      "Iteration 18, loss = 6.34472617\n",
      "Iteration 20, loss = 5.11743583\n",
      "Iteration 19, loss = 8.15330084\n",
      "Iteration 21, loss = 6.88742469\n",
      "Iteration 20, loss = 6.86141007\n",
      "Iteration 22, loss = 6.06671461\n",
      "Iteration 21, loss = 5.80404759\n",
      "Iteration 23, loss = 5.79825089\n",
      "Iteration 22, loss = 6.82399912\n",
      "Iteration 24, loss = 5.53941883\n",
      "Iteration 23, loss = 7.64706244\n",
      "Iteration 25, loss = 4.85110806\n",
      "Iteration 24, loss = 6.15450962\n",
      "Iteration 26, loss = 4.78511984\n",
      "Iteration 25, loss = 5.52498492\n",
      "Iteration 27, loss = 4.67574939\n",
      "Iteration 26, loss = 6.42534466\n",
      "Iteration 28, loss = 5.07588255\n",
      "Iteration 27, loss = 5.01280208\n",
      "Iteration 29, loss = 4.49325507\n",
      "Iteration 28, loss = 4.73822779\n",
      "Iteration 30, loss = 4.80756480\n",
      "Iteration 29, loss = 5.81285951\n",
      "Iteration 31, loss = 4.88516244\n",
      "Iteration 30, loss = 7.00907444\n",
      "Iteration 32, loss = 4.37686121\n",
      "Iteration 31, loss = 7.20866303\n",
      "Iteration 33, loss = 4.14201743\n",
      "Iteration 32, loss = 5.40441238\n",
      "Iteration 34, loss = 4.41585415\n",
      "Iteration 33, loss = 5.48278412\n",
      "Iteration 35, loss = 4.65029160\n",
      "Iteration 34, loss = 6.16221433\n",
      "Iteration 36, loss = 5.15978778\n",
      "Iteration 35, loss = 6.07000742\n",
      "Iteration 37, loss = 4.86959751\n",
      "Iteration 36, loss = 5.67501741\n",
      "Iteration 38, loss = 4.88900994\n",
      "Iteration 37, loss = 4.85302127\n",
      "Iteration 39, loss = 4.34323073\n",
      "Iteration 38, loss = 5.48098602\n",
      "Iteration 40, loss = 4.62011917\n",
      "Iteration 39, loss = 5.52013173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 5.09099711\n",
      "Iteration 42, loss = 4.10906573\n",
      "Iteration 43, loss = 4.74294050\n",
      "Iteration 44, loss = 5.73389053\n",
      "Iteration 45, loss = 5.19040837\n",
      "Iteration 46, loss = 6.40841381\n",
      "Iteration 47, loss = 5.54338662\n",
      "Iteration 48, loss = 5.69743142\n",
      "Iteration 49, loss = 5.70017713\n",
      "Iteration 50, loss = 4.44575412\n",
      "Iteration 51, loss = 4.76619052\n",
      "Iteration 52, loss = 4.22520725\n",
      "Iteration 53, loss = 3.95253118\n",
      "Iteration 54, loss = 4.03603724\n",
      "Iteration 55, loss = 3.88682013\n",
      "Iteration 56, loss = 3.73490468\n",
      "Iteration 57, loss = 3.87359822\n",
      "Iteration 58, loss = 4.05691511\n",
      "Iteration 59, loss = 3.68193678\n",
      "Iteration 1, loss = 20.33327198\n",
      "Iteration 60, loss = 4.49600995\n",
      "Iteration 2, loss = 17.80759532\n",
      "Iteration 61, loss = 4.07125717\n",
      "Iteration 3, loss = 15.38733390\n",
      "Iteration 62, loss = 4.70041959\n",
      "Iteration 4, loss = 13.19767572\n",
      "Iteration 63, loss = 4.89723145\n",
      "Iteration 5, loss = 11.20515687\n",
      "Iteration 64, loss = 4.43050440\n",
      "Iteration 6, loss = 8.37836469\n",
      "Iteration 65, loss = 4.76327292\n",
      "Iteration 7, loss = 7.58607008\n",
      "Iteration 66, loss = 4.37140016\n",
      "Iteration 8, loss = 10.44651354\n",
      "Iteration 67, loss = 4.72385673\n",
      "Iteration 9, loss = 12.30008048\n",
      "Iteration 68, loss = 5.44129378\n",
      "Iteration 10, loss = 8.41005006\n",
      "Iteration 69, loss = 4.50918148\n",
      "Iteration 11, loss = 8.72194563\n",
      "Iteration 70, loss = 3.93039274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 8.83495445\n",
      "Iteration 13, loss = 10.86137872\n",
      "Iteration 14, loss = 9.18294750\n",
      "Iteration 15, loss = 7.20965254\n",
      "Iteration 16, loss = 7.49275210\n",
      "Iteration 17, loss = 7.52874391\n",
      "Iteration 18, loss = 7.04822478\n",
      "Iteration 19, loss = 8.75380778\n",
      "Iteration 20, loss = 11.23531509\n",
      "Iteration 21, loss = 7.59533062\n",
      "Iteration 22, loss = 7.17612357\n",
      "Iteration 23, loss = 6.83776532\n",
      "Iteration 24, loss = 9.52061735\n",
      "Iteration 25, loss = 7.93120202\n",
      "Iteration 26, loss = 7.32596918\n",
      "Iteration 27, loss = 6.69132449\n",
      "Iteration 28, loss = 7.03023398\n",
      "Iteration 29, loss = 6.93042980\n",
      "Iteration 1, loss = 13.84462336\n",
      "Iteration 30, loss = 8.86507393\n",
      "Iteration 2, loss = 15.12485194\n",
      "Iteration 31, loss = 7.34431502\n",
      "Iteration 3, loss = 11.77915366\n",
      "Iteration 32, loss = 7.39186541\n",
      "Iteration 4, loss = 8.03154281\n",
      "Iteration 33, loss = 7.55627918\n",
      "Iteration 5, loss = 7.85943515\n",
      "Iteration 34, loss = 5.60258015\n",
      "Iteration 6, loss = 10.03171766\n",
      "Iteration 35, loss = 5.64826917\n",
      "Iteration 7, loss = 11.35026605\n",
      "Iteration 36, loss = 5.92695894\n",
      "Iteration 8, loss = 7.86928116\n",
      "Iteration 37, loss = 6.24529950\n",
      "Iteration 9, loss = 11.39863439\n",
      "Iteration 38, loss = 5.89791290\n",
      "Iteration 10, loss = 8.77861817\n",
      "Iteration 39, loss = 6.08198134\n",
      "Iteration 11, loss = 8.66399432\n",
      "Iteration 40, loss = 5.43103663\n",
      "Iteration 12, loss = 8.09452668\n",
      "Iteration 41, loss = 5.54801908\n",
      "Iteration 13, loss = 7.53834730\n",
      "Iteration 42, loss = 5.82287734\n",
      "Iteration 14, loss = 7.00558838\n",
      "Iteration 43, loss = 6.54022585\n",
      "Iteration 15, loss = 7.79446084\n",
      "Iteration 44, loss = 5.82332261\n",
      "Iteration 16, loss = 7.72609513\n",
      "Iteration 45, loss = 5.86226516\n",
      "Iteration 17, loss = 6.26418351\n",
      "Iteration 46, loss = 5.23482342\n",
      "Iteration 18, loss = 5.77648592\n",
      "Iteration 47, loss = 5.62770401\n",
      "Iteration 19, loss = 7.16420362\n",
      "Iteration 48, loss = 5.94166900\n",
      "Iteration 20, loss = 7.33129052\n",
      "Iteration 49, loss = 5.47020642\n",
      "Iteration 21, loss = 6.51438658\n",
      "Iteration 50, loss = 5.74425745\n",
      "Iteration 22, loss = 5.06938656\n",
      "Iteration 51, loss = 5.19532108\n",
      "Iteration 23, loss = 5.07475031\n",
      "Iteration 52, loss = 5.03618520\n",
      "Iteration 24, loss = 5.15153425\n",
      "Iteration 53, loss = 5.03996280\n",
      "Iteration 25, loss = 5.04143811\n",
      "Iteration 54, loss = 4.80496744\n",
      "Iteration 26, loss = 4.53279968\n",
      "Iteration 55, loss = 4.56887029\n",
      "Iteration 27, loss = 5.24208590\n",
      "Iteration 56, loss = 4.88336820\n",
      "Iteration 28, loss = 4.93292354\n",
      "Iteration 57, loss = 5.43228271\n",
      "Iteration 29, loss = 4.50451430\n",
      "Iteration 58, loss = 5.35363885\n",
      "Iteration 30, loss = 4.27173983\n",
      "Iteration 59, loss = 4.90684059\n",
      "Iteration 31, loss = 4.31329771\n",
      "Iteration 60, loss = 4.80471152\n",
      "Iteration 32, loss = 3.90840385\n",
      "Iteration 61, loss = 5.47212677\n",
      "Iteration 33, loss = 4.17094173\n",
      "Iteration 62, loss = 5.09528099\n",
      "Iteration 34, loss = 4.05880849\n",
      "Iteration 63, loss = 5.35506797\n",
      "Iteration 35, loss = 4.21960198\n",
      "Iteration 64, loss = 4.96289092\n",
      "Iteration 36, loss = 4.41783800\n",
      "Iteration 65, loss = 4.76671303\n",
      "Iteration 37, loss = 4.61499860\n",
      "Iteration 66, loss = 5.51164726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 4.65479426\n",
      "Iteration 39, loss = 4.77352852\n",
      "Iteration 40, loss = 5.00975263\n",
      "Iteration 41, loss = 3.83282402\n",
      "Iteration 42, loss = 4.69572281\n",
      "Iteration 43, loss = 4.57963160\n",
      "Iteration 44, loss = 4.40818902\n",
      "Iteration 45, loss = 4.04255545\n",
      "Iteration 46, loss = 3.95760923\n",
      "Iteration 47, loss = 3.95830664\n",
      "Iteration 48, loss = 4.54835906\n",
      "Iteration 49, loss = 4.62694621\n",
      "Iteration 50, loss = 4.35275271\n",
      "Iteration 51, loss = 4.39214239\n",
      "Iteration 52, loss = 3.88135303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.17071902\n",
      "Iteration 2, loss = 14.71518334\n",
      "Iteration 3, loss = 13.75180092\n",
      "Iteration 4, loss = 10.16614210\n",
      "Iteration 5, loss = 10.64723876\n",
      "Iteration 6, loss = 9.40761086\n",
      "Iteration 7, loss = 10.21193333\n",
      "Iteration 8, loss = 7.49147589\n",
      "Iteration 9, loss = 7.45431036\n",
      "Iteration 10, loss = 7.70694552\n",
      "Iteration 11, loss = 8.25431977\n",
      "Iteration 12, loss = 7.18209585\n",
      "Iteration 13, loss = 6.80347646\n",
      "Iteration 14, loss = 6.47700808\n",
      "Iteration 15, loss = 7.18985245\n",
      "Iteration 1, loss = 15.47279092\n",
      "Iteration 16, loss = 6.09820084\n",
      "Iteration 2, loss = 18.68039722\n",
      "Iteration 17, loss = 6.61123166\n",
      "Iteration 3, loss = 12.77726612\n",
      "Iteration 18, loss = 6.22657453\n",
      "Iteration 4, loss = 14.87794950\n",
      "Iteration 19, loss = 7.43757348\n",
      "Iteration 5, loss = 11.51469175\n",
      "Iteration 20, loss = 6.22217493\n",
      "Iteration 6, loss = 8.43391629\n",
      "Iteration 21, loss = 5.43796954\n",
      "Iteration 7, loss = 9.97648065\n",
      "Iteration 22, loss = 5.82965283\n",
      "Iteration 8, loss = 11.31175174\n",
      "Iteration 9, loss = 10.28794833\n",
      "Iteration 23, loss = 6.57413393\n",
      "Iteration 10, loss = 10.35291528\n",
      "Iteration 24, loss = 6.18172658\n",
      "Iteration 11, loss = 9.54796610\n",
      "Iteration 25, loss = 6.65184833\n",
      "Iteration 12, loss = 8.12942414\n",
      "Iteration 26, loss = 6.21981816\n",
      "Iteration 13, loss = 7.55603162\n",
      "Iteration 27, loss = 6.45476455\n",
      "Iteration 14, loss = 7.08387966\n",
      "Iteration 28, loss = 5.08114860\n",
      "Iteration 15, loss = 7.62734209\n",
      "Iteration 29, loss = 5.78763665\n",
      "Iteration 16, loss = 6.94336553\n",
      "Iteration 30, loss = 6.10279326\n",
      "Iteration 17, loss = 7.70466140\n",
      "Iteration 31, loss = 5.79017186\n",
      "Iteration 18, loss = 7.32236939\n",
      "Iteration 32, loss = 6.41832668\n",
      "Iteration 19, loss = 9.25147963\n",
      "Iteration 33, loss = 6.73334791\n",
      "Iteration 20, loss = 7.57401296\n",
      "Iteration 34, loss = 5.63527745\n",
      "Iteration 21, loss = 6.41248878\n",
      "Iteration 35, loss = 5.04793428\n",
      "Iteration 22, loss = 6.22867168\n",
      "Iteration 36, loss = 4.30461997\n",
      "Iteration 23, loss = 6.90400401\n",
      "Iteration 37, loss = 5.05111637\n",
      "Iteration 24, loss = 7.61620104\n",
      "Iteration 38, loss = 5.09185157\n",
      "Iteration 25, loss = 6.48328276\n",
      "Iteration 39, loss = 4.46509854\n",
      "Iteration 26, loss = 6.91749799\n",
      "Iteration 40, loss = 5.40692454\n",
      "Iteration 27, loss = 6.25290896\n",
      "Iteration 41, loss = 5.40676414\n",
      "Iteration 28, loss = 5.52054928\n",
      "Iteration 42, loss = 5.71995904\n",
      "Iteration 29, loss = 6.29358753\n",
      "Iteration 43, loss = 5.91467035\n",
      "Iteration 30, loss = 5.39116853\n",
      "Iteration 44, loss = 5.40248940\n",
      "Iteration 31, loss = 5.93988873\n",
      "Iteration 45, loss = 5.63531549\n",
      "Iteration 32, loss = 6.64501692\n",
      "Iteration 46, loss = 4.53611987\n",
      "Iteration 33, loss = 5.78084921\n",
      "Iteration 47, loss = 4.22162875\n",
      "Iteration 34, loss = 6.87815148\n",
      "Iteration 48, loss = 4.69264001\n",
      "Iteration 35, loss = 6.09261886\n",
      "Iteration 49, loss = 5.07004770\n",
      "Iteration 36, loss = 6.52294165\n",
      "Iteration 50, loss = 5.32174116\n",
      "Iteration 37, loss = 6.40446741\n",
      "Iteration 51, loss = 5.87049548\n",
      "Iteration 38, loss = 5.89340227\n",
      "Iteration 52, loss = 4.81138169\n",
      "Iteration 39, loss = 5.94008335\n",
      "Iteration 53, loss = 4.49822527\n",
      "Iteration 40, loss = 6.16551890\n",
      "Iteration 54, loss = 5.00813881\n",
      "Iteration 41, loss = 5.51086888\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 4.37973663\n",
      "Iteration 56, loss = 4.10411414\n",
      "Iteration 57, loss = 4.22165506\n",
      "Iteration 58, loss = 3.92117632\n",
      "Iteration 59, loss = 4.21806742\n",
      "Iteration 60, loss = 4.14256602\n",
      "Iteration 61, loss = 3.86769472\n",
      "Iteration 62, loss = 4.48507321\n",
      "Iteration 63, loss = 4.80777182\n",
      "Iteration 64, loss = 4.64532550\n",
      "Iteration 65, loss = 4.45385606\n",
      "Iteration 66, loss = 3.78766285\n",
      "Iteration 67, loss = 3.46977571\n",
      "Iteration 68, loss = 4.02379160\n",
      "Iteration 69, loss = 4.14190732\n",
      "Iteration 70, loss = 5.08341585\n",
      "Iteration 71, loss = 4.69073307\n",
      "Iteration 72, loss = 4.06250936\n",
      "Iteration 73, loss = 4.41288213\n",
      "Iteration 74, loss = 4.72677220\n",
      "Iteration 75, loss = 4.21928664\n",
      "Iteration 76, loss = 4.18236555\n",
      "Iteration 77, loss = 4.18396454\n",
      "Iteration 78, loss = 4.06720527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.23650073\n",
      "Iteration 2, loss = 12.94179749\n",
      "Iteration 3, loss = 14.63867557\n",
      "Iteration 4, loss = 10.00536302\n",
      "Iteration 5, loss = 11.82599038\n",
      "Iteration 6, loss = 10.86158371\n",
      "Iteration 7, loss = 10.76882701\n",
      "Iteration 8, loss = 9.54129515\n",
      "Iteration 9, loss = 6.73493287\n",
      "Iteration 10, loss = 9.00412411\n",
      "Iteration 11, loss = 7.02112018\n",
      "Iteration 12, loss = 8.26628335\n",
      "Iteration 13, loss = 5.96318115\n",
      "Iteration 14, loss = 6.13802255\n",
      "Iteration 15, loss = 7.18471931\n",
      "Iteration 16, loss = 6.92250780\n",
      "Iteration 17, loss = 5.94573569\n",
      "Iteration 18, loss = 6.90616149\n",
      "Iteration 19, loss = 5.04882770\n",
      "Iteration 20, loss = 4.99501218\n",
      "Iteration 21, loss = 4.44591799\n",
      "Iteration 22, loss = 7.11040630\n",
      "Iteration 23, loss = 4.84818636\n",
      "Iteration 24, loss = 4.62675190\n",
      "Iteration 25, loss = 3.77866853\n",
      "Iteration 26, loss = 5.32825817\n",
      "Iteration 27, loss = 4.22677489\n",
      "Iteration 28, loss = 4.05075611\n",
      "Iteration 29, loss = 4.03929219\n",
      "Iteration 30, loss = 3.64161620\n",
      "Iteration 31, loss = 3.75615588\n",
      "Iteration 32, loss = 3.26903395\n",
      "Iteration 33, loss = 3.27527768\n",
      "Iteration 34, loss = 3.10059287\n",
      "Iteration 35, loss = 3.24698864\n",
      "Iteration 36, loss = 2.33199012\n",
      "Iteration 37, loss = 2.32635543\n",
      "Iteration 38, loss = 2.73359717\n",
      "Iteration 39, loss = 2.98380165\n",
      "Iteration 40, loss = 1.88238607\n",
      "Iteration 41, loss = 2.99343633\n",
      "Iteration 42, loss = 3.92497310\n",
      "Iteration 43, loss = 3.58603785\n",
      "Iteration 44, loss = 3.41468514\n",
      "Iteration 45, loss = 3.48485420\n",
      "Iteration 46, loss = 2.84741655\n",
      "Iteration 47, loss = 3.96472200\n",
      "Iteration 48, loss = 2.72205016\n",
      "Iteration 49, loss = 3.33931561\n",
      "Iteration 50, loss = 3.72096309\n",
      "Iteration 51, loss = 2.67117398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                                        ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                          transformers=[(&#x27;Variables &#x27;\n",
       "                                                                         &#x27;Categóricas&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                         [&#x27;MSZoning&#x27;,\n",
       "                                                                          &#x27;Street&#x27;,\n",
       "                                                                          &#x27;Alley&#x27;,\n",
       "                                                                          &#x27;LotShape&#x27;,\n",
       "                                                                          &#x27;LandContour&#x27;,\n",
       "                                                                          &#x27;Utilities&#x27;,\n",
       "                                                                          &#x27;LotConfig&#x27;,\n",
       "                                                                          &#x27;LandSlope&#x27;,\n",
       "                                                                          &#x27;Neighborhood&#x27;,\n",
       "                                                                          &#x27;Condition1&#x27;,\n",
       "                                                                          &#x27;Condition2&#x27;,\n",
       "                                                                          &#x27;BldgType&#x27;,\n",
       "                                                                          &#x27;HouseStyle&#x27;,\n",
       "                                                                          &#x27;RoofSty...\n",
       "                                                                          &#x27;GarageArea&#x27;,\n",
       "                                                                          &#x27;WoodDeckSF&#x27;,\n",
       "                                                                          &#x27;OpenPorchSF&#x27;,\n",
       "                                                                          &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                                       (&#x27;mlpclassifier&#x27;,\n",
       "                                        MLPClassifier(activation=&#x27;identity&#x27;,\n",
       "                                                      hidden_layer_sizes=(),\n",
       "                                                      max_iter=300,\n",
       "                                                      verbose=True))]),\n",
       "             n_jobs=2,\n",
       "             param_grid={&#x27;mlpclassifier__hidden_layer_sizes&#x27;: ((30, 20),\n",
       "                                                               (50, 30),\n",
       "                                                               (10, 20)),\n",
       "                         &#x27;mlpclassifier__learning_rate_init&#x27;: (0.01, 0.001, 1,\n",
       "                                                               10),\n",
       "                         &#x27;mlpclassifier__max_iter&#x27;: (100, 200, 500, 1000)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                                        ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                          transformers=[(&#x27;Variables &#x27;\n",
       "                                                                         &#x27;Categóricas&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                         [&#x27;MSZoning&#x27;,\n",
       "                                                                          &#x27;Street&#x27;,\n",
       "                                                                          &#x27;Alley&#x27;,\n",
       "                                                                          &#x27;LotShape&#x27;,\n",
       "                                                                          &#x27;LandContour&#x27;,\n",
       "                                                                          &#x27;Utilities&#x27;,\n",
       "                                                                          &#x27;LotConfig&#x27;,\n",
       "                                                                          &#x27;LandSlope&#x27;,\n",
       "                                                                          &#x27;Neighborhood&#x27;,\n",
       "                                                                          &#x27;Condition1&#x27;,\n",
       "                                                                          &#x27;Condition2&#x27;,\n",
       "                                                                          &#x27;BldgType&#x27;,\n",
       "                                                                          &#x27;HouseStyle&#x27;,\n",
       "                                                                          &#x27;RoofSty...\n",
       "                                                                          &#x27;GarageArea&#x27;,\n",
       "                                                                          &#x27;WoodDeckSF&#x27;,\n",
       "                                                                          &#x27;OpenPorchSF&#x27;,\n",
       "                                                                          &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                                       (&#x27;mlpclassifier&#x27;,\n",
       "                                        MLPClassifier(activation=&#x27;identity&#x27;,\n",
       "                                                      hidden_layer_sizes=(),\n",
       "                                                      max_iter=300,\n",
       "                                                      verbose=True))]),\n",
       "             n_jobs=2,\n",
       "             param_grid={&#x27;mlpclassifier__hidden_layer_sizes&#x27;: ((30, 20),\n",
       "                                                               (50, 30),\n",
       "                                                               (10, 20)),\n",
       "                         &#x27;mlpclassifier__learning_rate_init&#x27;: (0.01, 0.001, 1,\n",
       "                                                               10),\n",
       "                         &#x27;mlpclassifier__max_iter&#x27;: (100, 200, 500, 1000)})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(), max_iter=300,\n",
       "              verbose=True)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('Variables '\n",
       "                                                                         'Categóricas',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         ['MSZoning',\n",
       "                                                                          'Street',\n",
       "                                                                          'Alley',\n",
       "                                                                          'LotShape',\n",
       "                                                                          'LandContour',\n",
       "                                                                          'Utilities',\n",
       "                                                                          'LotConfig',\n",
       "                                                                          'LandSlope',\n",
       "                                                                          'Neighborhood',\n",
       "                                                                          'Condition1',\n",
       "                                                                          'Condition2',\n",
       "                                                                          'BldgType',\n",
       "                                                                          'HouseStyle',\n",
       "                                                                          'RoofSty...\n",
       "                                                                          'GarageArea',\n",
       "                                                                          'WoodDeckSF',\n",
       "                                                                          'OpenPorchSF',\n",
       "                                                                          'EnclosedPorch', ...])])),\n",
       "                                       ('mlpclassifier',\n",
       "                                        MLPClassifier(activation='identity',\n",
       "                                                      hidden_layer_sizes=(),\n",
       "                                                      max_iter=300,\n",
       "                                                      verbose=True))]),\n",
       "             n_jobs=2,\n",
       "             param_grid={'mlpclassifier__hidden_layer_sizes': ((30, 20),\n",
       "                                                               (50, 30),\n",
       "                                                               (10, 20)),\n",
       "                         'mlpclassifier__learning_rate_init': (0.01, 0.001, 1,\n",
       "                                                               10),\n",
       "                         'mlpclassifier__max_iter': (100, 200, 500, 1000)})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametros_tun  = param_grid = {\n",
    "    'mlpclassifier__max_iter': (100,200,500,1000),\n",
    "    'mlpclassifier__learning_rate_init': (0.01, 0.001, 1,10),\n",
    "    'mlpclassifier__hidden_layer_sizes':((30,20),(50,30),(10,20))}\n",
    "model_grid_search = GridSearchCV(modelo1, param_grid=parametros_tun,\n",
    "                                 n_jobs=2, cv=10) #Vamos a usar dos procesadores(n_jobs), y 10 k-folds\n",
    "model_grid_search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlpclassifier__hidden_layer_sizes': (30, 20),\n",
       " 'mlpclassifier__learning_rate_init': 1,\n",
       " 'mlpclassifier__max_iter': 100}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_grid_search.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      " [[150   0   4]\n",
      " [  1 130  10]\n",
      " [ 32  28  83]]\n",
      "Accuracy:  0.8287671232876712\n",
      "Precision:  0.8287671232876712\n",
      "recall:  0.8287671232876712\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(target_test,pred)\n",
    "accuracy=accuracy_score(target_test,pred)\n",
    "precision =precision_score(target_test,pred,average='micro')\n",
    "recall =  recall_score(target_test,pred,average='micro')\n",
    "f1 = f1_score(target_test,pred,average='micro')\n",
    "print('Matriz de confusión\\n',cm)\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('recall: ',recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo la variable respuesta SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_df = pd.read_csv('train.csv')\n",
    "y = houses_df.pop(\"SalePrice\")\n",
    "X = houses_df\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador_categorico = Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")), (\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "preprocesador_numerico = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador = ColumnTransformer([\n",
    "    ('Variables Categóricas',preprocesador_categorico, categorical_columns),\n",
    "    ('Variables Numéricas',preprocesador_numerico, numerical_columns)\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1436    120500\n",
       "358     130000\n",
       "870     109500\n",
       "6       307000\n",
       "252     173000\n",
       "         ...  \n",
       "952     133900\n",
       "123     153900\n",
       "990     348000\n",
       "565     128000\n",
       "1113    134500\n",
       "Name: SalePrice, Length: 1021, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Categóricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" ><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Numéricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre></pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a></label><div class=\"sk-toggleable__content \"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   '...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpregressor',\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_pipeline(preprocesador, MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=350, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-63\" type=\"checkbox\" ><label for=\"sk-estimator-id-63\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\" ><label for=\"sk-estimator-id-64\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\" ><label for=\"sk-estimator-id-65\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;Id&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   '...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpregressor',\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustar el modelo\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (test): 1137188367.107484\n",
      "MAE (test): 21593.293365825324\n",
      "RMSE (test): 33722.22363824017\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, pred1)\n",
    "print(\"MSE (test):\", mse)\n",
    "mae_test = mean_absolute_error(y_test, pred1)\n",
    "print(\"MAE (test):\", mae_test)\n",
    "rmse_test = np.sqrt(mse)\n",
    "print(\"RMSE (test):\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear rangos de tamaños de entrenamiento\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calcular las curvas de aprendizaje\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, train_sizes=train_sizes, cv=5)\n",
    "\n",
    "# Calcular las medias y desviaciones estándar de los puntajes de entrenamiento y prueba\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Dibujar las curvas de aprendizaje\n",
    "plt.plot(train_sizes, train_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='#DDDDDD')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='#DDDDDD')\n",
    "\n",
    "# Crear la leyenda y los títulos\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 2 SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.001, epsilon=1e-20,\n",
       "                              hidden_layer_sizes=(100, 50),\n",
       "                              learning_rate_init=0.0002, max_iter=500,\n",
       "                              verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.001, epsilon=1e-20,\n",
       "                              hidden_layer_sizes=(100, 50),\n",
       "                              learning_rate_init=0.0002, max_iter=500,\n",
       "                              verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Categóricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Numéricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\" ><label for=\"sk-estimator-id-82\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre>[&#x27;Id&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\" ><label for=\"sk-estimator-id-83\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\" ><label for=\"sk-estimator-id-84\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a></label><div class=\"sk-toggleable__content \"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.001, epsilon=1e-20,\n",
       "             hidden_layer_sizes=(100, 50), learning_rate_init=0.0002,\n",
       "             max_iter=500, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   '...\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpregressor',\n",
       "                 MLPRegressor(activation='logistic', alpha=0.001, epsilon=1e-20,\n",
       "                              hidden_layer_sizes=(100, 50),\n",
       "                              learning_rate_init=0.0002, max_iter=500,\n",
       "                              verbose=True))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = make_pipeline(preprocesador, MLPRegressor(hidden_layer_sizes=(100, 50), activation='logistic', solver='adam', alpha=0.001, batch_size='auto', learning_rate='constant', learning_rate_init=0.0002, max_iter=500, shuffle=True, random_state=None, tol=0.0001, verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-20, n_iter_no_change=10))\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19401000151.59289169\n",
      "Iteration 2, loss = 19400978525.21993256\n",
      "Iteration 3, loss = 19400957145.03267288\n",
      "Iteration 4, loss = 19400935895.54676056\n",
      "Iteration 5, loss = 19400914222.47246933\n",
      "Iteration 6, loss = 19400891617.38912582\n",
      "Iteration 7, loss = 19400868491.16460419\n",
      "Iteration 8, loss = 19400845384.28848648\n",
      "Iteration 9, loss = 19400821829.91747665\n",
      "Iteration 10, loss = 19400797888.90268707\n",
      "Iteration 11, loss = 19400771683.47286224\n",
      "Iteration 12, loss = 19400746158.27952576\n",
      "Iteration 13, loss = 19400722012.02410507\n",
      "Iteration 14, loss = 19400696704.42447662\n",
      "Iteration 15, loss = 19400668728.10109711\n",
      "Iteration 16, loss = 19400641713.63415527\n",
      "Iteration 17, loss = 19400616931.25100708\n",
      "Iteration 18, loss = 19400594387.23678970\n",
      "Iteration 19, loss = 19400572884.27419281\n",
      "Iteration 20, loss = 19400550677.15783691\n",
      "Iteration 21, loss = 19400529594.29872513\n",
      "Iteration 22, loss = 19400508399.34207153\n",
      "Iteration 23, loss = 19400484923.13929367\n",
      "Iteration 24, loss = 19400458949.80401993\n",
      "Iteration 25, loss = 19400438318.85162735\n",
      "Iteration 26, loss = 19400420755.21176910\n",
      "Iteration 27, loss = 19400404258.10702515\n",
      "Iteration 28, loss = 19400387099.76898193\n",
      "Iteration 29, loss = 19400372763.17317581\n",
      "Iteration 30, loss = 19400359541.70751953\n",
      "Iteration 31, loss = 19400347198.47381210\n",
      "Iteration 32, loss = 19400335247.14938736\n",
      "Iteration 33, loss = 19400322828.61653137\n",
      "Iteration 34, loss = 19400310180.61958313\n",
      "Iteration 35, loss = 19400299134.87143326\n",
      "Iteration 36, loss = 19400288327.38798523\n",
      "Iteration 37, loss = 19400278136.94595718\n",
      "Iteration 38, loss = 19400268432.74150085\n",
      "Iteration 39, loss = 19400258854.50157547\n",
      "Iteration 40, loss = 19400249179.10565186\n",
      "Iteration 41, loss = 19400239447.17842102\n",
      "Iteration 42, loss = 19400230256.56364441\n",
      "Iteration 43, loss = 19400221299.35502625\n",
      "Iteration 44, loss = 19400212253.30040359\n",
      "Iteration 45, loss = 19400203257.15324402\n",
      "Iteration 46, loss = 19400194175.01772690\n",
      "Iteration 47, loss = 19400184971.05931854\n",
      "Iteration 48, loss = 19400175220.82430267\n",
      "Iteration 49, loss = 19400166005.00509262\n",
      "Iteration 50, loss = 19400157151.23831558\n",
      "Iteration 51, loss = 19400148431.50476837\n",
      "Iteration 52, loss = 19400139832.09208298\n",
      "Iteration 53, loss = 19400131323.87142944\n",
      "Iteration 54, loss = 19400122925.94730759\n",
      "Iteration 55, loss = 19400114575.62984848\n",
      "Iteration 56, loss = 19400106341.65916061\n",
      "Iteration 57, loss = 19400098121.83309937\n",
      "Iteration 58, loss = 19400090033.99412155\n",
      "Iteration 59, loss = 19400081876.31354141\n",
      "Iteration 60, loss = 19400073741.71212387\n",
      "Iteration 61, loss = 19400065711.59333038\n",
      "Iteration 62, loss = 19400057761.87062073\n",
      "Iteration 63, loss = 19400049913.90671921\n",
      "Iteration 64, loss = 19400041990.48449326\n",
      "Iteration 65, loss = 19400034058.91348267\n",
      "Iteration 66, loss = 19400026193.10690308\n",
      "Iteration 67, loss = 19400018373.27421570\n",
      "Iteration 68, loss = 19400010576.38797379\n",
      "Iteration 69, loss = 19400002796.71906662\n",
      "Iteration 70, loss = 19399995062.51328278\n",
      "Iteration 71, loss = 19399987422.32953644\n",
      "Iteration 72, loss = 19399979681.49542999\n",
      "Iteration 73, loss = 19399972001.29669571\n",
      "Iteration 74, loss = 19399964338.71406937\n",
      "Iteration 75, loss = 19399956647.47390747\n",
      "Iteration 76, loss = 19399948983.44971848\n",
      "Iteration 77, loss = 19399941449.67148209\n",
      "Iteration 78, loss = 19399933705.57492065\n",
      "Iteration 79, loss = 19399925960.84133148\n",
      "Iteration 80, loss = 19399918105.15741730\n",
      "Iteration 81, loss = 19399910149.82685089\n",
      "Iteration 82, loss = 19399902167.88593674\n",
      "Iteration 83, loss = 19399894003.08097839\n",
      "Iteration 84, loss = 19399885753.30582047\n",
      "Iteration 85, loss = 19399877348.65248489\n",
      "Iteration 86, loss = 19399868961.29230118\n",
      "Iteration 87, loss = 19399860667.71099854\n",
      "Iteration 88, loss = 19399852445.34920883\n",
      "Iteration 89, loss = 19399844276.37964630\n",
      "Iteration 90, loss = 19399836103.86746216\n",
      "Iteration 91, loss = 19399828014.85196304\n",
      "Iteration 92, loss = 19399819917.94963074\n",
      "Iteration 93, loss = 19399811907.24018860\n",
      "Iteration 94, loss = 19399803931.28470993\n",
      "Iteration 95, loss = 19399796081.16544342\n",
      "Iteration 96, loss = 19399788274.13917160\n",
      "Iteration 97, loss = 19399780477.52345657\n",
      "Iteration 98, loss = 19399772646.30588531\n",
      "Iteration 99, loss = 19399764826.11768723\n",
      "Iteration 100, loss = 19399757054.78992462\n",
      "Iteration 101, loss = 19399749306.69480133\n",
      "Iteration 102, loss = 19399741520.19419098\n",
      "Iteration 103, loss = 19399733753.22290802\n",
      "Iteration 104, loss = 19399726037.28337479\n",
      "Iteration 105, loss = 19399718414.79562378\n",
      "Iteration 106, loss = 19399710689.02603912\n",
      "Iteration 107, loss = 19399702959.78184509\n",
      "Iteration 108, loss = 19399695223.39431763\n",
      "Iteration 109, loss = 19399687461.29632950\n",
      "Iteration 110, loss = 19399679909.54893494\n",
      "Iteration 111, loss = 19399672325.13898468\n",
      "Iteration 112, loss = 19399664702.97652054\n",
      "Iteration 113, loss = 19399657124.36297989\n",
      "Iteration 114, loss = 19399649496.66847992\n",
      "Iteration 115, loss = 19399641892.54343796\n",
      "Iteration 116, loss = 19399634323.58860016\n",
      "Iteration 117, loss = 19399626698.96193695\n",
      "Iteration 118, loss = 19399619183.55209732\n",
      "Iteration 119, loss = 19399611629.83242035\n",
      "Iteration 120, loss = 19399604040.49464417\n",
      "Iteration 121, loss = 19399596540.29398346\n",
      "Iteration 122, loss = 19399589047.93197250\n",
      "Iteration 123, loss = 19399581608.86071396\n",
      "Iteration 124, loss = 19399574152.42158127\n",
      "Iteration 125, loss = 19399566727.32302475\n",
      "Iteration 126, loss = 19399559338.94982147\n",
      "Iteration 127, loss = 19399551886.85422897\n",
      "Iteration 128, loss = 19399544362.22692108\n",
      "Iteration 129, loss = 19399536907.69947052\n",
      "Iteration 130, loss = 19399529524.78809738\n",
      "Iteration 131, loss = 19399522082.78707123\n",
      "Iteration 132, loss = 19399514646.10042191\n",
      "Iteration 133, loss = 19399507250.43141556\n",
      "Iteration 134, loss = 19399499913.22543716\n",
      "Iteration 135, loss = 19399492605.11896515\n",
      "Iteration 136, loss = 19399485393.44273376\n",
      "Iteration 137, loss = 19399478071.65739441\n",
      "Iteration 138, loss = 19399470738.01736450\n",
      "Iteration 139, loss = 19399463383.31064606\n",
      "Iteration 140, loss = 19399456030.70396042\n",
      "Iteration 141, loss = 19399448846.49478912\n",
      "Iteration 142, loss = 19399441592.07624054\n",
      "Iteration 143, loss = 19399434383.87926102\n",
      "Iteration 144, loss = 19399427160.96878052\n",
      "Iteration 145, loss = 19399419871.82233429\n",
      "Iteration 146, loss = 19399412552.21313095\n",
      "Iteration 147, loss = 19399405295.15115738\n",
      "Iteration 148, loss = 19399397999.10233688\n",
      "Iteration 149, loss = 19399390645.43747330\n",
      "Iteration 150, loss = 19399383253.56394577\n",
      "Iteration 151, loss = 19399375872.38499832\n",
      "Iteration 152, loss = 19399368526.18896484\n",
      "Iteration 153, loss = 19399361268.21749115\n",
      "Iteration 154, loss = 19399354007.95337677\n",
      "Iteration 155, loss = 19399346776.14899826\n",
      "Iteration 156, loss = 19399339633.63362122\n",
      "Iteration 157, loss = 19399332358.85771942\n",
      "Iteration 158, loss = 19399325053.71653366\n",
      "Iteration 159, loss = 19399317797.96611023\n",
      "Iteration 160, loss = 19399310506.98344421\n",
      "Iteration 161, loss = 19399303142.83616257\n",
      "Iteration 162, loss = 19399295814.52121353\n",
      "Iteration 163, loss = 19399288391.98482132\n",
      "Iteration 164, loss = 19399280869.57117462\n",
      "Iteration 165, loss = 19399273169.29703140\n",
      "Iteration 166, loss = 19399265402.90935516\n",
      "Iteration 167, loss = 19399257377.41085815\n",
      "Iteration 168, loss = 19399249245.63701630\n",
      "Iteration 169, loss = 19399241026.66579437\n",
      "Iteration 170, loss = 19399232800.29966736\n",
      "Iteration 171, loss = 19399224549.41416550\n",
      "Iteration 172, loss = 19399216384.46235275\n",
      "Iteration 173, loss = 19399208185.34899521\n",
      "Iteration 174, loss = 19399200111.35728455\n",
      "Iteration 175, loss = 19399192041.89815140\n",
      "Iteration 176, loss = 19399183987.35980225\n",
      "Iteration 177, loss = 19399175917.95299911\n",
      "Iteration 178, loss = 19399168005.45664978\n",
      "Iteration 179, loss = 19399160087.90766144\n",
      "Iteration 180, loss = 19399152206.84170151\n",
      "Iteration 181, loss = 19399144251.68388748\n",
      "Iteration 182, loss = 19399136211.46848679\n",
      "Iteration 183, loss = 19399128155.01602554\n",
      "Iteration 184, loss = 19399119870.35011673\n",
      "Iteration 185, loss = 19399111391.52187729\n",
      "Iteration 186, loss = 19399102740.99030685\n",
      "Iteration 187, loss = 19399093779.85259247\n",
      "Iteration 188, loss = 19399084758.31109238\n",
      "Iteration 189, loss = 19399075735.85861969\n",
      "Iteration 190, loss = 19399066867.86956787\n",
      "Iteration 191, loss = 19399058022.47785187\n",
      "Iteration 192, loss = 19399049381.94764328\n",
      "Iteration 193, loss = 19399040680.32996750\n",
      "Iteration 194, loss = 19399032149.48363876\n",
      "Iteration 195, loss = 19399023674.05747986\n",
      "Iteration 196, loss = 19399015270.23842239\n",
      "Iteration 197, loss = 19399006895.79518890\n",
      "Iteration 198, loss = 19398998575.04731750\n",
      "Iteration 199, loss = 19398990314.16609955\n",
      "Iteration 200, loss = 19398982119.02418137\n",
      "Iteration 201, loss = 19398973933.62232971\n",
      "Iteration 202, loss = 19398965746.03632355\n",
      "Iteration 203, loss = 19398957491.36022186\n",
      "Iteration 204, loss = 19398949433.67823410\n",
      "Iteration 205, loss = 19398941421.22007370\n",
      "Iteration 206, loss = 19398933415.44283295\n",
      "Iteration 207, loss = 19398925406.23966980\n",
      "Iteration 208, loss = 19398917509.82745743\n",
      "Iteration 209, loss = 19398909499.08019638\n",
      "Iteration 210, loss = 19398901506.40111542\n",
      "Iteration 211, loss = 19398893423.13452148\n",
      "Iteration 212, loss = 19398885462.38071060\n",
      "Iteration 213, loss = 19398877430.91280746\n",
      "Iteration 214, loss = 19398869520.35676575\n",
      "Iteration 215, loss = 19398861484.39196014\n",
      "Iteration 216, loss = 19398853503.04856110\n",
      "Iteration 217, loss = 19398845496.91890717\n",
      "Iteration 218, loss = 19398837510.44891739\n",
      "Iteration 219, loss = 19398829651.29744339\n",
      "Iteration 220, loss = 19398821740.15369415\n",
      "Iteration 221, loss = 19398813876.71176910\n",
      "Iteration 222, loss = 19398806079.01828384\n",
      "Iteration 223, loss = 19398798297.05709457\n",
      "Iteration 224, loss = 19398790473.70450211\n",
      "Iteration 225, loss = 19398782707.26322174\n",
      "Iteration 226, loss = 19398774811.28518295\n",
      "Iteration 227, loss = 19398766998.20030594\n",
      "Iteration 228, loss = 19398759142.77084732\n",
      "Iteration 229, loss = 19398751325.61679077\n",
      "Iteration 230, loss = 19398743506.51287079\n",
      "Iteration 231, loss = 19398735619.97816086\n",
      "Iteration 232, loss = 19398727864.04502106\n",
      "Iteration 233, loss = 19398720029.55254364\n",
      "Iteration 234, loss = 19398712127.08379364\n",
      "Iteration 235, loss = 19398704247.18391800\n",
      "Iteration 236, loss = 19398696447.04183960\n",
      "Iteration 237, loss = 19398688627.16544342\n",
      "Iteration 238, loss = 19398680898.33331680\n",
      "Iteration 239, loss = 19398673134.66600037\n",
      "Iteration 240, loss = 19398665371.81560898\n",
      "Iteration 241, loss = 19398657671.98170471\n",
      "Iteration 242, loss = 19398649935.96193695\n",
      "Iteration 243, loss = 19398642172.10314560\n",
      "Iteration 244, loss = 19398634475.61904144\n",
      "Iteration 245, loss = 19398626815.32283783\n",
      "Iteration 246, loss = 19398619076.65776443\n",
      "Iteration 247, loss = 19398611343.41676331\n",
      "Iteration 248, loss = 19398603579.17005157\n",
      "Iteration 249, loss = 19398595826.17720413\n",
      "Iteration 250, loss = 19398588074.89253998\n",
      "Iteration 251, loss = 19398580438.11946106\n",
      "Iteration 252, loss = 19398572745.41907120\n",
      "Iteration 253, loss = 19398565073.30067062\n",
      "Iteration 254, loss = 19398557345.74108505\n",
      "Iteration 255, loss = 19398549768.40114975\n",
      "Iteration 256, loss = 19398542117.95830536\n",
      "Iteration 257, loss = 19398534471.18094254\n",
      "Iteration 258, loss = 19398526781.39808655\n",
      "Iteration 259, loss = 19398519183.18260956\n",
      "Iteration 260, loss = 19398511529.55584335\n",
      "Iteration 261, loss = 19398504004.03648758\n",
      "Iteration 262, loss = 19398496387.41606140\n",
      "Iteration 263, loss = 19398488743.63682938\n",
      "Iteration 264, loss = 19398481176.07378769\n",
      "Iteration 265, loss = 19398473612.79809952\n",
      "Iteration 266, loss = 19398466018.94538879\n",
      "Iteration 267, loss = 19398458350.54414749\n",
      "Iteration 268, loss = 19398450598.30074310\n",
      "Iteration 269, loss = 19398442898.26492691\n",
      "Iteration 270, loss = 19398435238.06898499\n",
      "Iteration 271, loss = 19398427500.47009277\n",
      "Iteration 272, loss = 19398419893.71585846\n",
      "Iteration 273, loss = 19398412169.61463547\n",
      "Iteration 274, loss = 19398404544.47765732\n",
      "Iteration 275, loss = 19398396947.31673431\n",
      "Iteration 276, loss = 19398389401.83995819\n",
      "Iteration 277, loss = 19398381804.93582916\n",
      "Iteration 278, loss = 19398374249.69686508\n",
      "Iteration 279, loss = 19398366537.52071381\n",
      "Iteration 280, loss = 19398358882.39371109\n",
      "Iteration 281, loss = 19398351243.77055740\n",
      "Iteration 282, loss = 19398343590.72988129\n",
      "Iteration 283, loss = 19398335957.30974197\n",
      "Iteration 284, loss = 19398328321.29982376\n",
      "Iteration 285, loss = 19398320704.16781998\n",
      "Iteration 286, loss = 19398313101.85568619\n",
      "Iteration 287, loss = 19398305495.72158813\n",
      "Iteration 288, loss = 19398297908.92567062\n",
      "Iteration 289, loss = 19398290301.62900543\n",
      "Iteration 290, loss = 19398282673.77951813\n",
      "Iteration 291, loss = 19398275092.94012833\n",
      "Iteration 292, loss = 19398267494.07204056\n",
      "Iteration 293, loss = 19398259973.51625443\n",
      "Iteration 294, loss = 19398252469.29294968\n",
      "Iteration 295, loss = 19398244942.75211334\n",
      "Iteration 296, loss = 19398237389.60113144\n",
      "Iteration 297, loss = 19398229750.29130554\n",
      "Iteration 298, loss = 19398222168.74360657\n",
      "Iteration 299, loss = 19398214453.29930115\n",
      "Iteration 300, loss = 19398206706.86188507\n",
      "Iteration 301, loss = 19398198893.17627335\n",
      "Iteration 302, loss = 19398190995.02996063\n",
      "Iteration 303, loss = 19398182716.04319382\n",
      "Iteration 304, loss = 19398174291.27637482\n",
      "Iteration 305, loss = 19398165444.46834564\n",
      "Iteration 306, loss = 19398156393.87628555\n",
      "Iteration 307, loss = 19398147339.75395966\n",
      "Iteration 308, loss = 19398138406.69043732\n",
      "Iteration 309, loss = 19398129555.90079498\n",
      "Iteration 310, loss = 19398120857.65113831\n",
      "Iteration 311, loss = 19398112328.70663452\n",
      "Iteration 312, loss = 19398103811.27005005\n",
      "Iteration 313, loss = 19398095304.68143463\n",
      "Iteration 314, loss = 19398087021.32952499\n",
      "Iteration 315, loss = 19398078625.17969131\n",
      "Iteration 316, loss = 19398070270.58213043\n",
      "Iteration 317, loss = 19398061931.00829315\n",
      "Iteration 318, loss = 19398053605.92934418\n",
      "Iteration 319, loss = 19398045293.10356522\n",
      "Iteration 320, loss = 19398037011.86661148\n",
      "Iteration 321, loss = 19398028868.51538849\n",
      "Iteration 322, loss = 19398020766.52479935\n",
      "Iteration 323, loss = 19398012682.74711609\n",
      "Iteration 324, loss = 19398004570.72281265\n",
      "Iteration 325, loss = 19397996545.21391296\n",
      "Iteration 326, loss = 19397988478.39169693\n",
      "Iteration 327, loss = 19397980412.32370758\n",
      "Iteration 328, loss = 19397972265.21171188\n",
      "Iteration 329, loss = 19397964067.11669159\n",
      "Iteration 330, loss = 19397955889.55382919\n",
      "Iteration 331, loss = 19397947733.53502274\n",
      "Iteration 332, loss = 19397939653.53203201\n",
      "Iteration 333, loss = 19397931651.10668182\n",
      "Iteration 334, loss = 19397923597.57274628\n",
      "Iteration 335, loss = 19397915642.67349243\n",
      "Iteration 336, loss = 19397907685.72618103\n",
      "Iteration 337, loss = 19397899762.77370453\n",
      "Iteration 338, loss = 19397891797.91633224\n",
      "Iteration 339, loss = 19397883867.52553177\n",
      "Iteration 340, loss = 19397875913.65443420\n",
      "Iteration 341, loss = 19397867927.91407776\n",
      "Iteration 342, loss = 19397860005.65957642\n",
      "Iteration 343, loss = 19397852004.78962326\n",
      "Iteration 344, loss = 19397843985.59859848\n",
      "Iteration 345, loss = 19397836070.11441040\n",
      "Iteration 346, loss = 19397828133.13604355\n",
      "Iteration 347, loss = 19397820280.86748123\n",
      "Iteration 348, loss = 19397812381.13268661\n",
      "Iteration 349, loss = 19397804394.42628860\n",
      "Iteration 350, loss = 19397796514.75143814\n",
      "Iteration 351, loss = 19397788676.10815811\n",
      "Iteration 352, loss = 19397780847.32402802\n",
      "Iteration 353, loss = 19397772956.59603882\n",
      "Iteration 354, loss = 19397765072.63140106\n",
      "Iteration 355, loss = 19397757180.54023361\n",
      "Iteration 356, loss = 19397749299.77860260\n",
      "Iteration 357, loss = 19397741486.00541687\n",
      "Iteration 358, loss = 19397733647.88721848\n",
      "Iteration 359, loss = 19397725816.64235687\n",
      "Iteration 360, loss = 19397718010.68254471\n",
      "Iteration 361, loss = 19397710238.42259979\n",
      "Iteration 362, loss = 19397702380.90458298\n",
      "Iteration 363, loss = 19397694510.95288849\n",
      "Iteration 364, loss = 19397686694.71909332\n",
      "Iteration 365, loss = 19397678805.79087067\n",
      "Iteration 366, loss = 19397670929.30969238\n",
      "Iteration 367, loss = 19397663113.79196548\n",
      "Iteration 368, loss = 19397655285.53311920\n",
      "Iteration 369, loss = 19397647466.78117752\n",
      "Iteration 370, loss = 19397639659.71023560\n",
      "Iteration 371, loss = 19397631817.71321106\n",
      "Iteration 372, loss = 19397624073.24496841\n",
      "Iteration 373, loss = 19397616286.99324036\n",
      "Iteration 374, loss = 19397608548.40538406\n",
      "Iteration 375, loss = 19397600823.99955368\n",
      "Iteration 376, loss = 19397593080.78585434\n",
      "Iteration 377, loss = 19397585289.18225861\n",
      "Iteration 378, loss = 19397577543.50149536\n",
      "Iteration 379, loss = 19397569689.95563126\n",
      "Iteration 380, loss = 19397561837.61986923\n",
      "Iteration 381, loss = 19397554012.80315781\n",
      "Iteration 382, loss = 19397546262.82999420\n",
      "Iteration 383, loss = 19397538556.88288116\n",
      "Iteration 384, loss = 19397530760.76370621\n",
      "Iteration 385, loss = 19397522961.23137283\n",
      "Iteration 386, loss = 19397515193.83544540\n",
      "Iteration 387, loss = 19397507392.12425995\n",
      "Iteration 388, loss = 19397499575.75843430\n",
      "Iteration 389, loss = 19397491791.97232056\n",
      "Iteration 390, loss = 19397484119.99978256\n",
      "Iteration 391, loss = 19397476360.79209518\n",
      "Iteration 392, loss = 19397468634.26153564\n",
      "Iteration 393, loss = 19397460873.83345413\n",
      "Iteration 394, loss = 19397453116.81268692\n",
      "Iteration 395, loss = 19397445438.37427902\n",
      "Iteration 396, loss = 19397437673.93199539\n",
      "Iteration 397, loss = 19397429980.92027283\n",
      "Iteration 398, loss = 19397422195.54616165\n",
      "Iteration 399, loss = 19397414405.20319366\n",
      "Iteration 400, loss = 19397406595.45114517\n",
      "Iteration 401, loss = 19397398799.38299179\n",
      "Iteration 402, loss = 19397391051.59119034\n",
      "Iteration 403, loss = 19397383328.81964874\n",
      "Iteration 404, loss = 19397375713.09611511\n",
      "Iteration 405, loss = 19397368017.80768967\n",
      "Iteration 406, loss = 19397360274.18379593\n",
      "Iteration 407, loss = 19397352567.95041275\n",
      "Iteration 408, loss = 19397344815.01871872\n",
      "Iteration 409, loss = 19397337121.66032791\n",
      "Iteration 410, loss = 19397329487.37694550\n",
      "Iteration 411, loss = 19397321859.70752335\n",
      "Iteration 412, loss = 19397314241.06985855\n",
      "Iteration 413, loss = 19397306531.22545242\n",
      "Iteration 414, loss = 19397298842.17393112\n",
      "Iteration 415, loss = 19397291121.00314331\n",
      "Iteration 416, loss = 19397283402.27315903\n",
      "Iteration 417, loss = 19397275670.58562851\n",
      "Iteration 418, loss = 19397268022.63109970\n",
      "Iteration 419, loss = 19397260271.56944656\n",
      "Iteration 420, loss = 19397252579.76202774\n",
      "Iteration 421, loss = 19397244897.24189758\n",
      "Iteration 422, loss = 19397237195.90776443\n",
      "Iteration 423, loss = 19397229549.06905365\n",
      "Iteration 424, loss = 19397221902.91862488\n",
      "Iteration 425, loss = 19397214262.14934158\n",
      "Iteration 426, loss = 19397206554.18965530\n",
      "Iteration 427, loss = 19397198899.92025757\n",
      "Iteration 428, loss = 19397191135.19260025\n",
      "Iteration 429, loss = 19397183478.44137573\n",
      "Iteration 430, loss = 19397175821.85541534\n",
      "Iteration 431, loss = 19397168200.73614120\n",
      "Iteration 432, loss = 19397160591.67457581\n",
      "Iteration 433, loss = 19397152910.79336166\n",
      "Iteration 434, loss = 19397145234.25175095\n",
      "Iteration 435, loss = 19397137603.76030731\n",
      "Iteration 436, loss = 19397129916.72021866\n",
      "Iteration 437, loss = 19397122294.35066986\n",
      "Iteration 438, loss = 19397114665.02091980\n",
      "Iteration 439, loss = 19397107065.46051407\n",
      "Iteration 440, loss = 19397099461.14130783\n",
      "Iteration 441, loss = 19397091835.85972595\n",
      "Iteration 442, loss = 19397084116.84327316\n",
      "Iteration 443, loss = 19397076484.10337067\n",
      "Iteration 444, loss = 19397068812.86711121\n",
      "Iteration 445, loss = 19397061094.82638168\n",
      "Iteration 446, loss = 19397053369.83855438\n",
      "Iteration 447, loss = 19397045598.13393784\n",
      "Iteration 448, loss = 19397037912.37001419\n",
      "Iteration 449, loss = 19397030229.87130356\n",
      "Iteration 450, loss = 19397022572.25080109\n",
      "Iteration 451, loss = 19397014950.37822723\n",
      "Iteration 452, loss = 19397007284.58720016\n",
      "Iteration 453, loss = 19396999604.81262589\n",
      "Iteration 454, loss = 19396991954.34432983\n",
      "Iteration 455, loss = 19396984317.77925110\n",
      "Iteration 456, loss = 19396976584.46548080\n",
      "Iteration 457, loss = 19396968897.49606705\n",
      "Iteration 458, loss = 19396961231.97941971\n",
      "Iteration 459, loss = 19396953521.91961670\n",
      "Iteration 460, loss = 19396945862.94095230\n",
      "Iteration 461, loss = 19396938104.22891617\n",
      "Iteration 462, loss = 19396930440.05519867\n",
      "Iteration 463, loss = 19396922688.97754288\n",
      "Iteration 464, loss = 19396915062.51792908\n",
      "Iteration 465, loss = 19396907381.24842072\n",
      "Iteration 466, loss = 19396899784.62426758\n",
      "Iteration 467, loss = 19396892088.96935272\n",
      "Iteration 468, loss = 19396884402.09256363\n",
      "Iteration 469, loss = 19396876773.05712128\n",
      "Iteration 470, loss = 19396869061.39007568\n",
      "Iteration 471, loss = 19396861343.45672226\n",
      "Iteration 472, loss = 19396853617.99726105\n",
      "Iteration 473, loss = 19396846062.47496033\n",
      "Iteration 474, loss = 19396838416.34903717\n",
      "Iteration 475, loss = 19396830788.31143951\n",
      "Iteration 476, loss = 19396823118.65166092\n",
      "Iteration 477, loss = 19396815481.94257736\n",
      "Iteration 478, loss = 19396807817.41300201\n",
      "Iteration 479, loss = 19396800211.41631699\n",
      "Iteration 480, loss = 19396792560.33772278\n",
      "Iteration 481, loss = 19396784966.00125504\n",
      "Iteration 482, loss = 19396777276.62803268\n",
      "Iteration 483, loss = 19396769661.71104050\n",
      "Iteration 484, loss = 19396762065.67023468\n",
      "Iteration 485, loss = 19396754438.83455658\n",
      "Iteration 486, loss = 19396746783.07401276\n",
      "Iteration 487, loss = 19396739190.29632950\n",
      "Iteration 488, loss = 19396731593.65439987\n",
      "Iteration 489, loss = 19396723852.52972794\n",
      "Iteration 490, loss = 19396716286.11412048\n",
      "Iteration 491, loss = 19396708676.21798325\n",
      "Iteration 492, loss = 19396701060.58008575\n",
      "Iteration 493, loss = 19396693485.55314636\n",
      "Iteration 494, loss = 19396685902.05113983\n",
      "Iteration 495, loss = 19396678325.63818741\n",
      "Iteration 496, loss = 19396670765.37465668\n",
      "Iteration 497, loss = 19396663139.37524414\n",
      "Iteration 498, loss = 19396655513.08762741\n",
      "Iteration 499, loss = 19396647872.39725876\n",
      "Iteration 500, loss = 19396640245.04803848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.001, epsilon=1e-20,\n",
       "                              hidden_layer_sizes=(100, 50),\n",
       "                              learning_rate_init=0.0002, max_iter=500,\n",
       "                              verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-85\" type=\"checkbox\" ><label for=\"sk-estimator-id-85\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.001, epsilon=1e-20,\n",
       "                              hidden_layer_sizes=(100, 50),\n",
       "                              learning_rate_init=0.0002, max_iter=500,\n",
       "                              verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-86\" type=\"checkbox\" ><label for=\"sk-estimator-id-86\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-87\" type=\"checkbox\" ><label for=\"sk-estimator-id-87\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-88\" type=\"checkbox\" ><label for=\"sk-estimator-id-88\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-89\" type=\"checkbox\" ><label for=\"sk-estimator-id-89\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-90\" type=\"checkbox\" ><label for=\"sk-estimator-id-90\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-91\" type=\"checkbox\" ><label for=\"sk-estimator-id-91\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-92\" type=\"checkbox\" ><label for=\"sk-estimator-id-92\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-93\" type=\"checkbox\" ><label for=\"sk-estimator-id-93\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;Id&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-94\" type=\"checkbox\" ><label for=\"sk-estimator-id-94\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-95\" type=\"checkbox\" ><label for=\"sk-estimator-id-95\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(activation=&#x27;logistic&#x27;, alpha=0.001, epsilon=1e-20,\n",
       "             hidden_layer_sizes=(100, 50), learning_rate_init=0.0002,\n",
       "             max_iter=500, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   '...\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpregressor',\n",
       "                 MLPRegressor(activation='logistic', alpha=0.001, epsilon=1e-20,\n",
       "                              hidden_layer_sizes=(100, 50),\n",
       "                              learning_rate_init=0.0002, max_iter=500,\n",
       "                              verbose=True))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustar el modelo\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (test): 39556769162.791405\n",
      "MAE (test): 183315.31077316497\n",
      "RMSE (test): 198888.83619447172\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, pred2)\n",
    "print(\"MSE (test):\", mse)\n",
    "mae_test = mean_absolute_error(y_test, pred2)\n",
    "print(\"MAE (test):\", mae_test)\n",
    "rmse_test = np.sqrt(mse)\n",
    "print(\"RMSE (test):\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 20632638753.01309586\n",
      "Iteration 2, loss = 20632635289.55812836\n",
      "Iteration 3, loss = 20632631826.79347992\n",
      "Iteration 4, loss = 20632628365.10167313\n",
      "Iteration 5, loss = 20632624904.81596756\n",
      "Iteration 6, loss = 20632621446.20607758\n",
      "Iteration 7, loss = 20632617989.46562195\n",
      "Iteration 8, loss = 20632614534.71850586\n",
      "Iteration 9, loss = 20632611082.04491043\n",
      "Iteration 10, loss = 20632607631.49577713\n",
      "Iteration 11, loss = 20632604183.08618546\n",
      "Iteration 12, loss = 20632600736.78420258\n",
      "Iteration 13, loss = 20632597292.50434113\n",
      "Iteration 14, loss = 20632593850.10543442\n",
      "Iteration 15, loss = 20632590409.39192963\n",
      "Iteration 16, loss = 20632586970.11746979\n",
      "Iteration 17, loss = 20632583531.99072647\n",
      "Iteration 18, loss = 20632580094.68300629\n",
      "Iteration 19, loss = 20632576657.83756256\n",
      "Iteration 20, loss = 20632573221.08028793\n",
      "Iteration 21, loss = 20632569784.03158188\n",
      "Iteration 22, loss = 20632566346.31881332\n",
      "Iteration 23, loss = 20632562907.58901596\n",
      "Iteration 24, loss = 20632559467.52124023\n",
      "Iteration 25, loss = 20632556025.83797455\n",
      "Iteration 26, loss = 20632552582.31512833\n",
      "Iteration 27, loss = 20632549136.79018402\n",
      "Iteration 28, loss = 20632545689.16806030\n",
      "Iteration 29, loss = 20632542239.42435837\n",
      "Iteration 30, loss = 20632538787.60574722\n",
      "Iteration 31, loss = 20632535333.82744980\n",
      "Iteration 32, loss = 20632531878.26763535\n",
      "Iteration 33, loss = 20632528421.15865326\n",
      "Iteration 34, loss = 20632524962.77542496\n",
      "Iteration 35, loss = 20632521503.42105484\n",
      "Iteration 36, loss = 20632518043.41012192\n",
      "Iteration 37, loss = 20632514583.05002594\n",
      "Iteration 38, loss = 20632511122.62125397\n",
      "Iteration 39, loss = 20632507662.35688782\n",
      "Iteration 40, loss = 20632504202.42247772\n",
      "Iteration 41, loss = 20632500742.89672089\n",
      "Iteration 42, loss = 20632497283.75405121\n",
      "Iteration 43, loss = 20632493824.84958267\n",
      "Iteration 44, loss = 20632490365.90722656\n",
      "Iteration 45, loss = 20632486906.51062393\n",
      "Iteration 46, loss = 20632483446.09716415\n",
      "Iteration 47, loss = 20632479983.95405960\n",
      "Iteration 48, loss = 20632476519.21569443\n",
      "Iteration 49, loss = 20632473050.86106110\n",
      "Iteration 50, loss = 20632469577.71028519\n",
      "Iteration 51, loss = 20632466098.41976929\n",
      "Iteration 52, loss = 20632462611.47619247\n",
      "Iteration 53, loss = 20632459115.19045639\n",
      "Iteration 54, loss = 20632455607.69387054\n",
      "Iteration 55, loss = 20632452086.93958664\n",
      "Iteration 56, loss = 20632448550.71315765\n",
      "Iteration 57, loss = 20632444996.65610123\n",
      "Iteration 58, loss = 20632441422.30675507\n",
      "Iteration 59, loss = 20632437825.16136169\n",
      "Iteration 60, loss = 20632434202.75681305\n",
      "Iteration 61, loss = 20632430552.77474594\n",
      "Iteration 62, loss = 20632426873.16384125\n",
      "Iteration 63, loss = 20632423162.27410507\n",
      "Iteration 64, loss = 20632419418.99419403\n",
      "Iteration 65, loss = 20632415642.88102722\n",
      "Iteration 66, loss = 20632411834.26916122\n",
      "Iteration 67, loss = 20632407994.34674454\n",
      "Iteration 68, loss = 20632404125.18527603\n",
      "Iteration 69, loss = 20632400229.71232986\n",
      "Iteration 70, loss = 20632396311.62107086\n",
      "Iteration 71, loss = 20632392375.21663284\n",
      "Iteration 72, loss = 20632388425.20544052\n",
      "Iteration 73, loss = 20632384466.43922043\n",
      "Iteration 74, loss = 20632380503.62691498\n",
      "Iteration 75, loss = 20632376541.03080368\n",
      "Iteration 76, loss = 20632372582.17123795\n",
      "Iteration 77, loss = 20632368629.58250809\n",
      "Iteration 78, loss = 20632364684.66982269\n",
      "Iteration 79, loss = 20632360747.68053055\n",
      "Iteration 80, loss = 20632356817.76195526\n",
      "Iteration 81, loss = 20632352893.09333038\n",
      "Iteration 82, loss = 20632348971.10148621\n",
      "Iteration 83, loss = 20632345048.75812149\n",
      "Iteration 84, loss = 20632341122.93564606\n",
      "Iteration 85, loss = 20632337190.78838730\n",
      "Iteration 86, loss = 20632333250.12294006\n",
      "Iteration 87, loss = 20632329299.72220612\n",
      "Iteration 88, loss = 20632325339.59056854\n",
      "Iteration 89, loss = 20632321371.09168243\n",
      "Iteration 90, loss = 20632317396.95860291\n",
      "Iteration 91, loss = 20632313421.16730499\n",
      "Iteration 92, loss = 20632309448.66953278\n",
      "Iteration 93, loss = 20632305484.98749542\n",
      "Iteration 94, loss = 20632301535.70106125\n",
      "Iteration 95, loss = 20632297605.88219452\n",
      "Iteration 96, loss = 20632293699.53768921\n",
      "Iteration 97, loss = 20632289819.11811447\n",
      "Iteration 98, loss = 20632285965.14234543\n",
      "Iteration 99, loss = 20632282135.97234726\n",
      "Iteration 100, loss = 20632278327.75543594\n",
      "Iteration 101, loss = 20632274534.53261566\n",
      "Iteration 102, loss = 20632270748.49551392\n",
      "Iteration 103, loss = 20632266960.36149216\n",
      "Iteration 104, loss = 20632263159.83089828\n",
      "Iteration 105, loss = 20632259336.09253693\n",
      "Iteration 106, loss = 20632255478.34860611\n",
      "Iteration 107, loss = 20632251576.33487320\n",
      "Iteration 108, loss = 20632247620.81547928\n",
      "Iteration 109, loss = 20632243604.03403473\n",
      "Iteration 110, loss = 20632239520.10077667\n",
      "Iteration 111, loss = 20632235365.29438782\n",
      "Iteration 112, loss = 20632231138.25944138\n",
      "Iteration 113, loss = 20632226840.08568954\n",
      "Iteration 114, loss = 20632222474.26433945\n",
      "Iteration 115, loss = 20632218046.53253937\n",
      "Iteration 116, loss = 20632213564.63251114\n",
      "Iteration 117, loss = 20632209038.02096176\n",
      "Iteration 118, loss = 20632204477.56281662\n",
      "Iteration 119, loss = 20632199895.23007965\n",
      "Iteration 120, loss = 20632195303.80854797\n",
      "Iteration 121, loss = 20632190716.59130859\n",
      "Iteration 122, loss = 20632186147.02150726\n",
      "Iteration 123, loss = 20632181608.25695419\n",
      "Iteration 124, loss = 20632177112.65943527\n",
      "Iteration 125, loss = 20632172671.23013687\n",
      "Iteration 126, loss = 20632168293.04250336\n",
      "Iteration 127, loss = 20632163984.75673676\n",
      "Iteration 128, loss = 20632159750.30120468\n",
      "Iteration 129, loss = 20632155590.77987671\n",
      "Iteration 130, loss = 20632151504.62748337\n",
      "Iteration 131, loss = 20632147487.99247742\n",
      "Iteration 132, loss = 20632143535.29450607\n",
      "Iteration 133, loss = 20632139639.88331604\n",
      "Iteration 134, loss = 20632135794.72080994\n",
      "Iteration 135, loss = 20632131993.01821899\n",
      "Iteration 136, loss = 20632128228.77445221\n",
      "Iteration 137, loss = 20632124497.17488098\n",
      "Iteration 138, loss = 20632120794.82298279\n",
      "Iteration 139, loss = 20632117119.79851151\n",
      "Iteration 140, loss = 20632113471.55454254\n",
      "Iteration 141, loss = 20632109850.68056488\n",
      "Iteration 142, loss = 20632106258.57461548\n",
      "Iteration 143, loss = 20632102697.08423996\n",
      "Iteration 144, loss = 20632099168.17434311\n",
      "Iteration 145, loss = 20632095673.66800308\n",
      "Iteration 146, loss = 20632092215.08850098\n",
      "Iteration 147, loss = 20632088793.61056137\n",
      "Iteration 148, loss = 20632085410.11029816\n",
      "Iteration 149, loss = 20632082065.29159164\n",
      "Iteration 150, loss = 20632078759.85945511\n",
      "Iteration 151, loss = 20632075494.70546722\n",
      "Iteration 152, loss = 20632072271.06630707\n",
      "Iteration 153, loss = 20632069090.61569977\n",
      "Iteration 154, loss = 20632065955.45944977\n",
      "Iteration 155, loss = 20632062868.02276993\n",
      "Iteration 156, loss = 20632059830.84261703\n",
      "Iteration 157, loss = 20632056846.29798508\n",
      "Iteration 158, loss = 20632053916.32367706\n",
      "Iteration 159, loss = 20632051042.15740204\n",
      "Iteration 160, loss = 20632048224.16563797\n",
      "Iteration 161, loss = 20632045461.78290176\n",
      "Iteration 162, loss = 20632042753.58097076\n",
      "Iteration 163, loss = 20632040097.46370697\n",
      "Iteration 164, loss = 20632037490.96049881\n",
      "Iteration 165, loss = 20632034931.57197571\n",
      "Iteration 166, loss = 20632032417.11043549\n",
      "Iteration 167, loss = 20632029945.97869110\n",
      "Iteration 168, loss = 20632027517.34427643\n",
      "Iteration 169, loss = 20632025131.18548965\n",
      "Iteration 170, loss = 20632022788.20928192\n",
      "Iteration 171, loss = 20632020489.66081238\n",
      "Iteration 172, loss = 20632018237.05882645\n",
      "Iteration 173, loss = 20632016031.89675140\n",
      "Iteration 174, loss = 20632013875.34856415\n",
      "Iteration 175, loss = 20632011768.01187134\n",
      "Iteration 176, loss = 20632009709.71397400\n",
      "Iteration 177, loss = 20632007699.39969254\n",
      "Iteration 178, loss = 20632005735.11112595\n",
      "Iteration 179, loss = 20632003814.05904770\n",
      "Iteration 180, loss = 20632001932.77429199\n",
      "Iteration 181, loss = 20632000087.31596375\n",
      "Iteration 182, loss = 20631998273.50661469\n",
      "Iteration 183, loss = 20631996487.16350174\n",
      "Iteration 184, loss = 20631994724.30136871\n",
      "Iteration 185, loss = 20631992981.29285431\n",
      "Iteration 186, loss = 20631991254.98358917\n",
      "Iteration 187, loss = 20631989542.76413727\n",
      "Iteration 188, loss = 20631987842.60495758\n",
      "Iteration 189, loss = 20631986153.06124115\n",
      "Iteration 190, loss = 20631984473.25017929\n",
      "Iteration 191, loss = 20631982802.80318069\n",
      "Iteration 192, loss = 20631981141.79474640\n",
      "Iteration 193, loss = 20631979490.64964676\n",
      "Iteration 194, loss = 20631977850.03180313\n",
      "Iteration 195, loss = 20631976220.72100449\n",
      "Iteration 196, loss = 20631974603.48728180\n",
      "Iteration 197, loss = 20631972998.97528458\n",
      "Iteration 198, loss = 20631971407.61278152\n",
      "Iteration 199, loss = 20631969829.55450058\n",
      "Iteration 200, loss = 20631968264.66633987\n",
      "Iteration 201, loss = 20631966712.54760361\n",
      "Iteration 202, loss = 20631965172.58186722\n",
      "Iteration 203, loss = 20631963644.00315094\n",
      "Iteration 204, loss = 20631962125.96438980\n",
      "Iteration 205, loss = 20631960617.59744263\n",
      "Iteration 206, loss = 20631959118.05912399\n",
      "Iteration 207, loss = 20631957626.56135941\n",
      "Iteration 208, loss = 20631956142.38714981\n",
      "Iteration 209, loss = 20631954664.89534378\n",
      "Iteration 210, loss = 20631953193.51758957\n",
      "Iteration 211, loss = 20631951727.75045776\n",
      "Iteration 212, loss = 20631950267.14516449\n",
      "Iteration 213, loss = 20631948811.29613876\n",
      "Iteration 214, loss = 20631947359.82970047\n",
      "Iteration 215, loss = 20631945912.39303970\n",
      "Iteration 216, loss = 20631944468.64370728\n",
      "Iteration 217, loss = 20631943028.24005890\n",
      "Iteration 218, loss = 20631941590.83236313\n",
      "Iteration 219, loss = 20631940156.05516434\n",
      "Iteration 220, loss = 20631938723.52114868\n",
      "Iteration 221, loss = 20631937292.81738281\n",
      "Iteration 222, loss = 20631935863.50484467\n",
      "Iteration 223, loss = 20631934435.12235641\n",
      "Iteration 224, loss = 20631933007.19611740\n",
      "Iteration 225, loss = 20631931579.25561142\n",
      "Iteration 226, loss = 20631930150.85603714\n",
      "Iteration 227, loss = 20631928721.60754395\n",
      "Iteration 228, loss = 20631927291.20937347\n",
      "Iteration 229, loss = 20631925859.48574448\n",
      "Iteration 230, loss = 20631924426.42103195\n",
      "Iteration 231, loss = 20631922992.19047546\n",
      "Iteration 232, loss = 20631921557.18122482\n",
      "Iteration 233, loss = 20631920121.99714279\n",
      "Iteration 234, loss = 20631918687.44180679\n",
      "Iteration 235, loss = 20631917254.47828674\n",
      "Iteration 236, loss = 20631915824.16740036\n",
      "Iteration 237, loss = 20631914397.58952713\n",
      "Iteration 238, loss = 20631912975.75785446\n",
      "Iteration 239, loss = 20631911559.53373718\n",
      "Iteration 240, loss = 20631910149.55738449\n",
      "Iteration 241, loss = 20631908746.20660019\n",
      "Iteration 242, loss = 20631907349.59102631\n",
      "Iteration 243, loss = 20631905959.58094406\n",
      "Iteration 244, loss = 20631904575.86104584\n",
      "Iteration 245, loss = 20631903197.99528122\n",
      "Iteration 246, loss = 20631901825.48923492\n",
      "Iteration 247, loss = 20631900457.84079361\n",
      "Iteration 248, loss = 20631899094.57514572\n",
      "Iteration 249, loss = 20631897735.26494980\n",
      "Iteration 250, loss = 20631896379.53860474\n",
      "Iteration 251, loss = 20631895027.08029938\n",
      "Iteration 252, loss = 20631893677.62568283\n",
      "Iteration 253, loss = 20631892330.95511627\n",
      "Iteration 254, loss = 20631890986.88665009\n",
      "Iteration 255, loss = 20631889645.26918030\n",
      "Iteration 256, loss = 20631888305.97663116\n",
      "Iteration 257, loss = 20631886968.90290451\n",
      "Iteration 258, loss = 20631885633.95793152\n",
      "Iteration 259, loss = 20631884301.06435776\n",
      "Iteration 260, loss = 20631882970.15499878\n",
      "Iteration 261, loss = 20631881641.17084885\n",
      "Iteration 262, loss = 20631880314.05944443\n",
      "Iteration 263, loss = 20631878988.77366638\n",
      "Iteration 264, loss = 20631877665.27080536\n",
      "Iteration 265, loss = 20631876343.51171494\n",
      "Iteration 266, loss = 20631875023.46030045\n",
      "Iteration 267, loss = 20631873705.08295059\n",
      "Iteration 268, loss = 20631872388.34825134\n",
      "Iteration 269, loss = 20631871073.22662735\n",
      "Iteration 270, loss = 20631869759.69011688\n",
      "Iteration 271, loss = 20631868447.71213531\n",
      "Iteration 272, loss = 20631867137.26737595\n",
      "Iteration 273, loss = 20631865828.33160019\n",
      "Iteration 274, loss = 20631864520.88161469\n",
      "Iteration 275, loss = 20631863214.89509583\n",
      "Iteration 276, loss = 20631861910.35052109\n",
      "Iteration 277, loss = 20631860607.22716904\n",
      "Iteration 278, loss = 20631859305.50496292\n",
      "Iteration 279, loss = 20631858005.16449356\n",
      "Iteration 280, loss = 20631856706.18695450\n",
      "Iteration 281, loss = 20631855408.55408096\n",
      "Iteration 282, loss = 20631854112.24812317\n",
      "Iteration 283, loss = 20631852817.25184631\n",
      "Iteration 284, loss = 20631851523.54848480\n",
      "Iteration 285, loss = 20631850231.12167358\n",
      "Iteration 286, loss = 20631848939.95550156\n",
      "Iteration 287, loss = 20631847650.03443909\n",
      "Iteration 288, loss = 20631846361.34333038\n",
      "Iteration 289, loss = 20631845073.86737442\n",
      "Iteration 290, loss = 20631843787.59213638\n",
      "Iteration 291, loss = 20631842502.50347519\n",
      "Iteration 292, loss = 20631841218.58758926\n",
      "Iteration 293, loss = 20631839935.83096313\n",
      "Iteration 294, loss = 20631838654.22038651\n",
      "Iteration 295, loss = 20631837373.74290848\n",
      "Iteration 296, loss = 20631836094.38585281\n",
      "Iteration 297, loss = 20631834816.13680267\n",
      "Iteration 298, loss = 20631833538.98358536\n",
      "Iteration 299, loss = 20631832262.91427612\n",
      "Iteration 300, loss = 20631830987.91716003\n",
      "Iteration 301, loss = 20631829713.98077011\n",
      "Iteration 302, loss = 20631828441.09383774\n",
      "Iteration 303, loss = 20631827169.24531937\n",
      "Iteration 304, loss = 20631825898.42433929\n",
      "Iteration 305, loss = 20631824628.62026215\n",
      "Iteration 306, loss = 20631823359.82260895\n",
      "Iteration 307, loss = 20631822092.02111053\n",
      "Iteration 308, loss = 20631820825.20565796\n",
      "Iteration 309, loss = 20631819559.36630249\n",
      "Iteration 310, loss = 20631818294.49329758\n",
      "Iteration 311, loss = 20631817030.57703400\n",
      "Iteration 312, loss = 20631815767.60807800\n",
      "Iteration 313, loss = 20631814505.57714844\n",
      "Iteration 314, loss = 20631813244.47510529\n",
      "Iteration 315, loss = 20631811984.29296875\n",
      "Iteration 316, loss = 20631810725.02187347\n",
      "Iteration 317, loss = 20631809466.65314102\n",
      "Iteration 318, loss = 20631808209.17819595\n",
      "Iteration 319, loss = 20631806952.58858109\n",
      "Iteration 320, loss = 20631805696.87600327\n",
      "Iteration 321, loss = 20631804442.03230667\n",
      "Iteration 322, loss = 20631803188.04940033\n",
      "Iteration 323, loss = 20631801934.91935730\n",
      "Iteration 324, loss = 20631800682.63437653\n",
      "Iteration 325, loss = 20631799431.18672562\n",
      "Iteration 326, loss = 20631798180.56883240\n",
      "Iteration 327, loss = 20631796930.77322006\n",
      "Iteration 328, loss = 20631795681.79249573\n",
      "Iteration 329, loss = 20631794433.61941910\n",
      "Iteration 330, loss = 20631793186.24680328\n",
      "Iteration 331, loss = 20631791939.66759491\n",
      "Iteration 332, loss = 20631790693.87482834\n",
      "Iteration 333, loss = 20631789448.86163712\n",
      "Iteration 334, loss = 20631788204.62124252\n",
      "Iteration 335, loss = 20631786961.14696121\n",
      "Iteration 336, loss = 20631785718.43222046\n",
      "Iteration 337, loss = 20631784476.47050476\n",
      "Iteration 338, loss = 20631783235.25540924\n",
      "Iteration 339, loss = 20631781994.78060913\n",
      "Iteration 340, loss = 20631780755.03985214\n",
      "Iteration 341, loss = 20631779516.02698898\n",
      "Iteration 342, loss = 20631778277.73593140\n",
      "Iteration 343, loss = 20631777040.16068268\n",
      "Iteration 344, loss = 20631775803.29533768\n",
      "Iteration 345, loss = 20631774567.13402176\n",
      "Iteration 346, loss = 20631773331.67098618\n",
      "Iteration 347, loss = 20631772096.90053558\n",
      "Iteration 348, loss = 20631770862.81702423\n",
      "Iteration 349, loss = 20631769629.41490936\n",
      "Iteration 350, loss = 20631768396.68872833\n",
      "Iteration 351, loss = 20631767164.63304520\n",
      "Iteration 352, loss = 20631765933.24250793\n",
      "Iteration 353, loss = 20631764702.51185608\n",
      "Iteration 354, loss = 20631763472.43585587\n",
      "Iteration 355, loss = 20631762243.00935745\n",
      "Iteration 356, loss = 20631761014.22727966\n",
      "Iteration 357, loss = 20631759786.08458328\n",
      "Iteration 358, loss = 20631758558.57631683\n",
      "Iteration 359, loss = 20631757331.69755173\n",
      "Iteration 360, loss = 20631756105.44345856\n",
      "Iteration 361, loss = 20631754879.80923843\n",
      "Iteration 362, loss = 20631753654.79014969\n",
      "Iteration 363, loss = 20631752430.38152313\n",
      "Iteration 364, loss = 20631751206.57872772\n",
      "Iteration 365, loss = 20631749983.37719727\n",
      "Iteration 366, loss = 20631748760.77242661\n",
      "Iteration 367, loss = 20631747538.75992966\n",
      "Iteration 368, loss = 20631746317.33529663\n",
      "Iteration 369, loss = 20631745096.49417114\n",
      "Iteration 370, loss = 20631743876.23224258\n",
      "Iteration 371, loss = 20631742656.54523849\n",
      "Iteration 372, loss = 20631741437.42894745\n",
      "Iteration 373, loss = 20631740218.87919998\n",
      "Iteration 374, loss = 20631739000.89187622\n",
      "Iteration 375, loss = 20631737783.46290207\n",
      "Iteration 376, loss = 20631736566.58823395\n",
      "Iteration 377, loss = 20631735350.26390076\n",
      "Iteration 378, loss = 20631734134.48594666\n",
      "Iteration 379, loss = 20631732919.25048828\n",
      "Iteration 380, loss = 20631731704.55365372\n",
      "Iteration 381, loss = 20631730490.39163589\n",
      "Iteration 382, loss = 20631729276.76066589\n",
      "Iteration 383, loss = 20631728063.65700531\n",
      "Iteration 384, loss = 20631726851.07695007\n",
      "Iteration 385, loss = 20631725639.01687241\n",
      "Iteration 386, loss = 20631724427.47314453\n",
      "Iteration 387, loss = 20631723216.44219208\n",
      "Iteration 388, loss = 20631722005.92046738\n",
      "Iteration 389, loss = 20631720795.90449142\n",
      "Iteration 390, loss = 20631719586.39077759\n",
      "Iteration 391, loss = 20631718377.37590790\n",
      "Iteration 392, loss = 20631717168.85649872\n",
      "Iteration 393, loss = 20631715960.82917023\n",
      "Iteration 394, loss = 20631714753.29062653\n",
      "Iteration 395, loss = 20631713546.23757172\n",
      "Iteration 396, loss = 20631712339.66673279\n",
      "Iteration 397, loss = 20631711133.57490540\n",
      "Iteration 398, loss = 20631709927.95890045\n",
      "Iteration 399, loss = 20631708722.81555176\n",
      "Iteration 400, loss = 20631707518.14176178\n",
      "Iteration 401, loss = 20631706313.93440628\n",
      "Iteration 402, loss = 20631705110.19042969\n",
      "Iteration 403, loss = 20631703906.90681076\n",
      "Iteration 404, loss = 20631702704.08054352\n",
      "Iteration 405, loss = 20631701501.70865250\n",
      "Iteration 406, loss = 20631700299.78820038\n",
      "Iteration 407, loss = 20631699098.31628036\n",
      "Iteration 408, loss = 20631697897.28999329\n",
      "Iteration 409, loss = 20631696696.70648575\n",
      "Iteration 410, loss = 20631695496.56293869\n",
      "Iteration 411, loss = 20631694296.85654068\n",
      "Iteration 412, loss = 20631693097.58451843\n",
      "Iteration 413, loss = 20631691898.74413681\n",
      "Iteration 414, loss = 20631690700.33266449\n",
      "Iteration 415, loss = 20631689502.34738922\n",
      "Iteration 416, loss = 20631688304.78567505\n",
      "Iteration 417, loss = 20631687107.64486313\n",
      "Iteration 418, loss = 20631685910.92232895\n",
      "Iteration 419, loss = 20631684714.61548233\n",
      "Iteration 420, loss = 20631683518.72175980\n",
      "Iteration 421, loss = 20631682323.23860550\n",
      "Iteration 422, loss = 20631681128.16350937\n",
      "Iteration 423, loss = 20631679933.49395370\n",
      "Iteration 424, loss = 20631678739.22748566\n",
      "Iteration 425, loss = 20631677545.36164474\n",
      "Iteration 426, loss = 20631676351.89399338\n",
      "Iteration 427, loss = 20631675158.82213211\n",
      "Iteration 428, loss = 20631673966.14367294\n",
      "Iteration 429, loss = 20631672773.85624313\n",
      "Iteration 430, loss = 20631671581.95751953\n",
      "Iteration 431, loss = 20631670390.44516754\n",
      "Iteration 432, loss = 20631669199.31689453\n",
      "Iteration 433, loss = 20631668008.57042313\n",
      "Iteration 434, loss = 20631666818.20348358\n",
      "Iteration 435, loss = 20631665628.21384430\n",
      "Iteration 436, loss = 20631664438.59928513\n",
      "Iteration 437, loss = 20631663249.35761642\n",
      "Iteration 438, loss = 20631662060.48664474\n",
      "Iteration 439, loss = 20631660871.98421860\n",
      "Iteration 440, loss = 20631659683.84820175\n",
      "Iteration 441, loss = 20631658496.07645798\n",
      "Iteration 442, loss = 20631657308.66690063\n",
      "Iteration 443, loss = 20631656121.61742783\n",
      "Iteration 444, loss = 20631654934.92598343\n",
      "Iteration 445, loss = 20631653748.59052277\n",
      "Iteration 446, loss = 20631652562.60900497\n",
      "Iteration 447, loss = 20631651376.97941589\n",
      "Iteration 448, loss = 20631650191.69977188\n",
      "Iteration 449, loss = 20631649006.76807785\n",
      "Iteration 450, loss = 20631647822.18237686\n",
      "Iteration 451, loss = 20631646637.94072342\n",
      "Iteration 452, loss = 20631645454.04120255\n",
      "Iteration 453, loss = 20631644270.48188019\n",
      "Iteration 454, loss = 20631643087.26086807\n",
      "Iteration 455, loss = 20631641904.37629318\n",
      "Iteration 456, loss = 20631640721.82627106\n",
      "Iteration 457, loss = 20631639539.60897446\n",
      "Iteration 458, loss = 20631638357.72256088\n",
      "Iteration 459, loss = 20631637176.16520691\n",
      "Iteration 460, loss = 20631635994.93511581\n",
      "Iteration 461, loss = 20631634814.03049469\n",
      "Iteration 462, loss = 20631633633.44957352\n",
      "Iteration 463, loss = 20631632453.19059372\n",
      "Iteration 464, loss = 20631631273.25179672\n",
      "Iteration 465, loss = 20631630093.63146973\n",
      "Iteration 466, loss = 20631628914.32788467\n",
      "Iteration 467, loss = 20631627735.33935165\n",
      "Iteration 468, loss = 20631626556.66416931\n",
      "Iteration 469, loss = 20631625378.30067062\n",
      "Iteration 470, loss = 20631624200.24718857\n",
      "Iteration 471, loss = 20631623022.50207138\n",
      "Iteration 472, loss = 20631621845.06369400\n",
      "Iteration 473, loss = 20631620667.93042374\n",
      "Iteration 474, loss = 20631619491.10065842\n",
      "Iteration 475, loss = 20631618314.57279968\n",
      "Iteration 476, loss = 20631617138.34526062\n",
      "Iteration 477, loss = 20631615962.41647339\n",
      "Iteration 478, loss = 20631614786.78487778\n",
      "Iteration 479, loss = 20631613611.44893646\n",
      "Iteration 480, loss = 20631612436.40708923\n",
      "Iteration 481, loss = 20631611261.65783691\n",
      "Iteration 482, loss = 20631610087.19965363\n",
      "Iteration 483, loss = 20631608913.03104782\n",
      "Iteration 484, loss = 20631607739.15053558\n",
      "Iteration 485, loss = 20631606565.55662918\n",
      "Iteration 486, loss = 20631605392.24785995\n",
      "Iteration 487, loss = 20631604219.22279358\n",
      "Iteration 488, loss = 20631603046.47997665\n",
      "Iteration 489, loss = 20631601874.01797104\n",
      "Iteration 490, loss = 20631600701.83536148\n",
      "Iteration 491, loss = 20631599529.93073654\n",
      "Iteration 492, loss = 20631598358.30270386\n",
      "Iteration 493, loss = 20631597186.94985580\n",
      "Iteration 494, loss = 20631596015.87083435\n",
      "Iteration 495, loss = 20631594845.06425476\n",
      "Iteration 496, loss = 20631593674.52877426\n",
      "Iteration 497, loss = 20631592504.26302338\n",
      "Iteration 498, loss = 20631591334.26567841\n",
      "Iteration 499, loss = 20631590164.53541183\n",
      "Iteration 500, loss = 20631588995.07089996\n",
      "Iteration 1, loss = 20336603680.85223007\n",
      "Iteration 2, loss = 20336596886.27704620\n",
      "Iteration 3, loss = 20336589992.10367584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 20336583131.57483673\n",
      "Iteration 5, loss = 20336576336.79987717\n",
      "Iteration 6, loss = 20336569416.16609573\n",
      "Iteration 7, loss = 20336562544.64186478\n",
      "Iteration 8, loss = 20336555589.98603821\n",
      "Iteration 9, loss = 20336548698.99325180\n",
      "Iteration 10, loss = 20336541754.44895554\n",
      "Iteration 11, loss = 20336534741.88965607\n",
      "Iteration 12, loss = 20336527806.21246338\n",
      "Iteration 13, loss = 20336520750.28981018\n",
      "Iteration 14, loss = 20336513760.74702454\n",
      "Iteration 15, loss = 20336506738.36261368\n",
      "Iteration 16, loss = 20336499704.76295853\n",
      "Iteration 17, loss = 20336492624.49753189\n",
      "Iteration 18, loss = 20336485596.24277496\n",
      "Iteration 19, loss = 20336478566.91542816\n",
      "Iteration 20, loss = 20336471451.98649216\n",
      "Iteration 21, loss = 20336464412.57891464\n",
      "Iteration 22, loss = 20336457287.49838257\n",
      "Iteration 23, loss = 20336450111.43387604\n",
      "Iteration 24, loss = 20336442936.53740692\n",
      "Iteration 25, loss = 20336435769.56435394\n",
      "Iteration 26, loss = 20336428436.19527054\n",
      "Iteration 27, loss = 20336421047.45994949\n",
      "Iteration 28, loss = 20336413758.01395416\n",
      "Iteration 29, loss = 20336406278.72311020\n",
      "Iteration 30, loss = 20336398943.22211838\n",
      "Iteration 31, loss = 20336391497.19723892\n",
      "Iteration 32, loss = 20336384052.64228439\n",
      "Iteration 33, loss = 20336376713.76554871\n",
      "Iteration 34, loss = 20336369325.37727737\n",
      "Iteration 35, loss = 20336361787.47510529\n",
      "Iteration 36, loss = 20336354145.78920364\n",
      "Iteration 37, loss = 20336346612.66806793\n",
      "Iteration 38, loss = 20336338862.07697678\n",
      "Iteration 39, loss = 20336331123.74917984\n",
      "Iteration 40, loss = 20336323317.25755310\n",
      "Iteration 41, loss = 20336315741.67244339\n",
      "Iteration 42, loss = 20336308130.54977798\n",
      "Iteration 43, loss = 20336300664.65493011\n",
      "Iteration 44, loss = 20336293319.78239441\n",
      "Iteration 45, loss = 20336286199.16318512\n",
      "Iteration 46, loss = 20336279110.59329605\n",
      "Iteration 47, loss = 20336272290.34690857\n",
      "Iteration 48, loss = 20336265591.58962250\n",
      "Iteration 49, loss = 20336259102.78748703\n",
      "Iteration 50, loss = 20336252867.85267258\n",
      "Iteration 51, loss = 20336246808.39845657\n",
      "Iteration 52, loss = 20336240920.89845276\n",
      "Iteration 53, loss = 20336235189.56029129\n",
      "Iteration 54, loss = 20336229477.06945038\n",
      "Iteration 55, loss = 20336223845.70895004\n",
      "Iteration 56, loss = 20336218131.01851273\n",
      "Iteration 57, loss = 20336212365.51691437\n",
      "Iteration 58, loss = 20336206514.08332825\n",
      "Iteration 59, loss = 20336200297.90413284\n",
      "Iteration 60, loss = 20336194071.98437500\n",
      "Iteration 61, loss = 20336187594.58473587\n",
      "Iteration 62, loss = 20336180924.56285477\n",
      "Iteration 63, loss = 20336174192.68943787\n",
      "Iteration 64, loss = 20336167380.21452332\n",
      "Iteration 65, loss = 20336160711.60088730\n",
      "Iteration 66, loss = 20336154024.21788788\n",
      "Iteration 67, loss = 20336147562.74422455\n",
      "Iteration 68, loss = 20336141008.26442719\n",
      "Iteration 69, loss = 20336134443.10953522\n",
      "Iteration 70, loss = 20336127778.70512390\n",
      "Iteration 71, loss = 20336121075.23773193\n",
      "Iteration 72, loss = 20336114389.53831100\n",
      "Iteration 73, loss = 20336107695.35715485\n",
      "Iteration 74, loss = 20336101069.15319443\n",
      "Iteration 75, loss = 20336094560.67127991\n",
      "Iteration 76, loss = 20336088059.65381241\n",
      "Iteration 77, loss = 20336081663.83993530\n",
      "Iteration 78, loss = 20336075325.31135178\n",
      "Iteration 79, loss = 20336068968.63734055\n",
      "Iteration 80, loss = 20336062740.58486557\n",
      "Iteration 81, loss = 20336056624.39236450\n",
      "Iteration 82, loss = 20336050623.15816116\n",
      "Iteration 83, loss = 20336044873.83110046\n",
      "Iteration 84, loss = 20336039333.35264206\n",
      "Iteration 85, loss = 20336034112.89855957\n",
      "Iteration 86, loss = 20336029234.70660019\n",
      "Iteration 87, loss = 20336024707.48324585\n",
      "Iteration 88, loss = 20336020437.64544678\n",
      "Iteration 89, loss = 20336016434.97052002\n",
      "Iteration 90, loss = 20336012656.98636627\n",
      "Iteration 91, loss = 20336008942.23253250\n",
      "Iteration 92, loss = 20336005382.34553909\n",
      "Iteration 93, loss = 20336001899.98999786\n",
      "Iteration 94, loss = 20335998494.81096649\n",
      "Iteration 95, loss = 20335995119.95757675\n",
      "Iteration 96, loss = 20335991775.72537231\n",
      "Iteration 97, loss = 20335988510.61050797\n",
      "Iteration 98, loss = 20335985219.34330750\n",
      "Iteration 99, loss = 20335981931.71403885\n",
      "Iteration 100, loss = 20335978678.63934326\n",
      "Iteration 101, loss = 20335975383.74699783\n",
      "Iteration 102, loss = 20335972115.19365311\n",
      "Iteration 103, loss = 20335968799.12036514\n",
      "Iteration 104, loss = 20335965512.82694244\n",
      "Iteration 105, loss = 20335962179.06602097\n",
      "Iteration 106, loss = 20335958843.91376495\n",
      "Iteration 107, loss = 20335955516.46303177\n",
      "Iteration 108, loss = 20335952255.65093231\n",
      "Iteration 109, loss = 20335948969.40976334\n",
      "Iteration 110, loss = 20335945710.20207596\n",
      "Iteration 111, loss = 20335942508.89849854\n",
      "Iteration 112, loss = 20335939322.23638535\n",
      "Iteration 113, loss = 20335936178.03200912\n",
      "Iteration 114, loss = 20335933072.85123825\n",
      "Iteration 115, loss = 20335929987.16638184\n",
      "Iteration 116, loss = 20335926926.09070587\n",
      "Iteration 117, loss = 20335923891.74495697\n",
      "Iteration 118, loss = 20335920822.33691788\n",
      "Iteration 119, loss = 20335917804.08157730\n",
      "Iteration 120, loss = 20335914777.03842163\n",
      "Iteration 121, loss = 20335911741.28576660\n",
      "Iteration 122, loss = 20335908694.75439072\n",
      "Iteration 123, loss = 20335905709.10114670\n",
      "Iteration 124, loss = 20335902705.74890900\n",
      "Iteration 125, loss = 20335899695.73898697\n",
      "Iteration 126, loss = 20335896739.96903229\n",
      "Iteration 127, loss = 20335893738.82264709\n",
      "Iteration 128, loss = 20335890693.25140762\n",
      "Iteration 129, loss = 20335887721.54185867\n",
      "Iteration 130, loss = 20335884693.66386032\n",
      "Iteration 131, loss = 20335881717.29979706\n",
      "Iteration 132, loss = 20335878760.56261826\n",
      "Iteration 133, loss = 20335875835.71709061\n",
      "Iteration 134, loss = 20335872938.62609863\n",
      "Iteration 135, loss = 20335870048.37043762\n",
      "Iteration 136, loss = 20335867179.44156647\n",
      "Iteration 137, loss = 20335864326.78997040\n",
      "Iteration 138, loss = 20335861480.97443008\n",
      "Iteration 139, loss = 20335858672.24892044\n",
      "Iteration 140, loss = 20335855866.08308411\n",
      "Iteration 141, loss = 20335853075.60643387\n",
      "Iteration 142, loss = 20335850276.08340454\n",
      "Iteration 143, loss = 20335847525.83254623\n",
      "Iteration 144, loss = 20335844752.51154327\n",
      "Iteration 145, loss = 20335841976.14200974\n",
      "Iteration 146, loss = 20335839203.20910263\n",
      "Iteration 147, loss = 20335836460.26976776\n",
      "Iteration 148, loss = 20335833666.16514969\n",
      "Iteration 149, loss = 20335830934.17296982\n",
      "Iteration 150, loss = 20335828176.55726242\n",
      "Iteration 151, loss = 20335825424.37065506\n",
      "Iteration 152, loss = 20335822707.08580017\n",
      "Iteration 153, loss = 20335819980.87492371\n",
      "Iteration 154, loss = 20335817255.57582092\n",
      "Iteration 155, loss = 20335814545.32888794\n",
      "Iteration 156, loss = 20335811873.52867889\n",
      "Iteration 157, loss = 20335809159.60460663\n",
      "Iteration 158, loss = 20335806497.82014465\n",
      "Iteration 159, loss = 20335803819.75899506\n",
      "Iteration 160, loss = 20335801145.41222763\n",
      "Iteration 161, loss = 20335798470.46518707\n",
      "Iteration 162, loss = 20335795796.56920624\n",
      "Iteration 163, loss = 20335793092.56338120\n",
      "Iteration 164, loss = 20335790415.18655014\n",
      "Iteration 165, loss = 20335787734.75746536\n",
      "Iteration 166, loss = 20335785033.15107346\n",
      "Iteration 167, loss = 20335782341.55043793\n",
      "Iteration 168, loss = 20335779662.79217148\n",
      "Iteration 169, loss = 20335776993.03438187\n",
      "Iteration 170, loss = 20335774295.07860947\n",
      "Iteration 171, loss = 20335771595.26414871\n",
      "Iteration 172, loss = 20335768932.51697540\n",
      "Iteration 173, loss = 20335766215.96900558\n",
      "Iteration 174, loss = 20335763510.15000153\n",
      "Iteration 175, loss = 20335760794.25551605\n",
      "Iteration 176, loss = 20335758073.30125809\n",
      "Iteration 177, loss = 20335755351.81926727\n",
      "Iteration 178, loss = 20335752636.52989960\n",
      "Iteration 179, loss = 20335749920.35615540\n",
      "Iteration 180, loss = 20335747202.94224930\n",
      "Iteration 181, loss = 20335744491.27019882\n",
      "Iteration 182, loss = 20335741814.43250656\n",
      "Iteration 183, loss = 20335739080.95348358\n",
      "Iteration 184, loss = 20335736377.75394821\n",
      "Iteration 185, loss = 20335733645.32282639\n",
      "Iteration 186, loss = 20335730935.30260086\n",
      "Iteration 187, loss = 20335728205.97940826\n",
      "Iteration 188, loss = 20335725455.23900223\n",
      "Iteration 189, loss = 20335722711.88341522\n",
      "Iteration 190, loss = 20335719959.36000824\n",
      "Iteration 191, loss = 20335717211.32843399\n",
      "Iteration 192, loss = 20335714453.12604904\n",
      "Iteration 193, loss = 20335711696.52715683\n",
      "Iteration 194, loss = 20335708960.69530106\n",
      "Iteration 195, loss = 20335706228.34301376\n",
      "Iteration 196, loss = 20335703489.33494186\n",
      "Iteration 197, loss = 20335700756.15672684\n",
      "Iteration 198, loss = 20335698014.50950623\n",
      "Iteration 199, loss = 20335695243.91775894\n",
      "Iteration 200, loss = 20335692494.27033997\n",
      "Iteration 201, loss = 20335689733.23081207\n",
      "Iteration 202, loss = 20335687014.61334991\n",
      "Iteration 203, loss = 20335684261.87649155\n",
      "Iteration 204, loss = 20335681582.49815750\n",
      "Iteration 205, loss = 20335678866.15494919\n",
      "Iteration 206, loss = 20335676168.62027359\n",
      "Iteration 207, loss = 20335673481.67069244\n",
      "Iteration 208, loss = 20335670743.41070938\n",
      "Iteration 209, loss = 20335668050.26000977\n",
      "Iteration 210, loss = 20335665307.25041199\n",
      "Iteration 211, loss = 20335662578.02011871\n",
      "Iteration 212, loss = 20335659835.41058731\n",
      "Iteration 213, loss = 20335657074.63725281\n",
      "Iteration 214, loss = 20335654348.05744553\n",
      "Iteration 215, loss = 20335651583.92105484\n",
      "Iteration 216, loss = 20335648821.28294754\n",
      "Iteration 217, loss = 20335646107.10886002\n",
      "Iteration 218, loss = 20335643344.38663864\n",
      "Iteration 219, loss = 20335640624.75397491\n",
      "Iteration 220, loss = 20335637898.22853088\n",
      "Iteration 221, loss = 20335635180.20897293\n",
      "Iteration 222, loss = 20335632465.24941254\n",
      "Iteration 223, loss = 20335629745.10569000\n",
      "Iteration 224, loss = 20335627003.06096268\n",
      "Iteration 225, loss = 20335624295.78594589\n",
      "Iteration 226, loss = 20335621541.87976456\n",
      "Iteration 227, loss = 20335618814.46358490\n",
      "Iteration 228, loss = 20335616064.20633316\n",
      "Iteration 229, loss = 20335613313.63257217\n",
      "Iteration 230, loss = 20335610614.26696777\n",
      "Iteration 231, loss = 20335607904.94685364\n",
      "Iteration 232, loss = 20335605229.84140778\n",
      "Iteration 233, loss = 20335602582.54203796\n",
      "Iteration 234, loss = 20335599923.05480576\n",
      "Iteration 235, loss = 20335597283.62107468\n",
      "Iteration 236, loss = 20335594672.83691025\n",
      "Iteration 237, loss = 20335592037.35602188\n",
      "Iteration 238, loss = 20335589402.63155365\n",
      "Iteration 239, loss = 20335586774.33278656\n",
      "Iteration 240, loss = 20335584134.52093124\n",
      "Iteration 241, loss = 20335581520.20430756\n",
      "Iteration 242, loss = 20335578854.76350784\n",
      "Iteration 243, loss = 20335576222.10845947\n",
      "Iteration 244, loss = 20335573567.51056671\n",
      "Iteration 245, loss = 20335570917.70637894\n",
      "Iteration 246, loss = 20335568289.33648682\n",
      "Iteration 247, loss = 20335565681.71683502\n",
      "Iteration 248, loss = 20335563079.67772675\n",
      "Iteration 249, loss = 20335560475.01623154\n",
      "Iteration 250, loss = 20335557871.24566650\n",
      "Iteration 251, loss = 20335555270.61470795\n",
      "Iteration 252, loss = 20335552648.98910522\n",
      "Iteration 253, loss = 20335550018.99610138\n",
      "Iteration 254, loss = 20335547399.39223862\n",
      "Iteration 255, loss = 20335544749.17197800\n",
      "Iteration 256, loss = 20335542101.14385605\n",
      "Iteration 257, loss = 20335539470.19969940\n",
      "Iteration 258, loss = 20335536788.92988205\n",
      "Iteration 259, loss = 20335534146.56479645\n",
      "Iteration 260, loss = 20335531494.73002625\n",
      "Iteration 261, loss = 20335528855.48739624\n",
      "Iteration 262, loss = 20335526223.85913086\n",
      "Iteration 263, loss = 20335523592.94760895\n",
      "Iteration 264, loss = 20335520976.58304214\n",
      "Iteration 265, loss = 20335518325.60506821\n",
      "Iteration 266, loss = 20335515712.98810959\n",
      "Iteration 267, loss = 20335513076.75284958\n",
      "Iteration 268, loss = 20335510436.75805283\n",
      "Iteration 269, loss = 20335507806.62342453\n",
      "Iteration 270, loss = 20335505159.07351303\n",
      "Iteration 271, loss = 20335502570.62724304\n",
      "Iteration 272, loss = 20335499937.93353271\n",
      "Iteration 273, loss = 20335497341.68721008\n",
      "Iteration 274, loss = 20335494779.77711868\n",
      "Iteration 275, loss = 20335492193.21118546\n",
      "Iteration 276, loss = 20335489627.17214203\n",
      "Iteration 277, loss = 20335487095.81990433\n",
      "Iteration 278, loss = 20335484527.41024399\n",
      "Iteration 279, loss = 20335482005.80400848\n",
      "Iteration 280, loss = 20335479457.88729477\n",
      "Iteration 281, loss = 20335476930.26650620\n",
      "Iteration 282, loss = 20335474401.59996033\n",
      "Iteration 283, loss = 20335471883.39636230\n",
      "Iteration 284, loss = 20335469395.91733932\n",
      "Iteration 285, loss = 20335466872.11089706\n",
      "Iteration 286, loss = 20335464373.29395294\n",
      "Iteration 287, loss = 20335461874.86212158\n",
      "Iteration 288, loss = 20335459355.16514206\n",
      "Iteration 289, loss = 20335456835.12973785\n",
      "Iteration 290, loss = 20335454313.91897202\n",
      "Iteration 291, loss = 20335451768.81579590\n",
      "Iteration 292, loss = 20335449232.52697372\n",
      "Iteration 293, loss = 20335446678.42927170\n",
      "Iteration 294, loss = 20335444138.48542404\n",
      "Iteration 295, loss = 20335441602.85220337\n",
      "Iteration 296, loss = 20335439045.13307571\n",
      "Iteration 297, loss = 20335436509.50110626\n",
      "Iteration 298, loss = 20335433959.32966995\n",
      "Iteration 299, loss = 20335431412.64037323\n",
      "Iteration 300, loss = 20335428864.34311295\n",
      "Iteration 301, loss = 20335426320.35813522\n",
      "Iteration 302, loss = 20335423757.49786377\n",
      "Iteration 303, loss = 20335421243.95771027\n",
      "Iteration 304, loss = 20335418665.89345169\n",
      "Iteration 305, loss = 20335416125.78432083\n",
      "Iteration 306, loss = 20335413586.95083237\n",
      "Iteration 307, loss = 20335411053.60605621\n",
      "Iteration 308, loss = 20335408514.13956070\n",
      "Iteration 309, loss = 20335405980.63827896\n",
      "Iteration 310, loss = 20335403478.50628662\n",
      "Iteration 311, loss = 20335400923.71555710\n",
      "Iteration 312, loss = 20335398418.38428497\n",
      "Iteration 313, loss = 20335395863.93099594\n",
      "Iteration 314, loss = 20335393367.13225174\n",
      "Iteration 315, loss = 20335390810.92418671\n",
      "Iteration 316, loss = 20335388303.70504761\n",
      "Iteration 317, loss = 20335385775.02771378\n",
      "Iteration 318, loss = 20335383223.10733032\n",
      "Iteration 319, loss = 20335380736.84941101\n",
      "Iteration 320, loss = 20335378201.27350616\n",
      "Iteration 321, loss = 20335375681.58319092\n",
      "Iteration 322, loss = 20335373174.42464066\n",
      "Iteration 323, loss = 20335370659.11041260\n",
      "Iteration 324, loss = 20335368155.85273743\n",
      "Iteration 325, loss = 20335365657.16763306\n",
      "Iteration 326, loss = 20335363171.42344284\n",
      "Iteration 327, loss = 20335360668.74856567\n",
      "Iteration 328, loss = 20335358217.09758377\n",
      "Iteration 329, loss = 20335355693.03550339\n",
      "Iteration 330, loss = 20335353180.99583435\n",
      "Iteration 331, loss = 20335350683.17866135\n",
      "Iteration 332, loss = 20335348142.64435196\n",
      "Iteration 333, loss = 20335345631.28744888\n",
      "Iteration 334, loss = 20335343106.00827026\n",
      "Iteration 335, loss = 20335340581.76287460\n",
      "Iteration 336, loss = 20335338056.13318634\n",
      "Iteration 337, loss = 20335335543.12402725\n",
      "Iteration 338, loss = 20335333034.05467224\n",
      "Iteration 339, loss = 20335330510.15068054\n",
      "Iteration 340, loss = 20335327985.22147369\n",
      "Iteration 341, loss = 20335325480.87154770\n",
      "Iteration 342, loss = 20335322945.32743073\n",
      "Iteration 343, loss = 20335320453.75520706\n",
      "Iteration 344, loss = 20335317944.22032928\n",
      "Iteration 345, loss = 20335315415.55878067\n",
      "Iteration 346, loss = 20335312933.90993500\n",
      "Iteration 347, loss = 20335310409.53782654\n",
      "Iteration 348, loss = 20335307897.22014999\n",
      "Iteration 349, loss = 20335305390.05928802\n",
      "Iteration 350, loss = 20335302900.70605850\n",
      "Iteration 351, loss = 20335300396.05935669\n",
      "Iteration 352, loss = 20335297911.95130157\n",
      "Iteration 353, loss = 20335295453.64361572\n",
      "Iteration 354, loss = 20335292955.46466827\n",
      "Iteration 355, loss = 20335290498.90821838\n",
      "Iteration 356, loss = 20335288016.32807159\n",
      "Iteration 357, loss = 20335285529.05709839\n",
      "Iteration 358, loss = 20335283050.99932480\n",
      "Iteration 359, loss = 20335280551.81004715\n",
      "Iteration 360, loss = 20335278053.03852463\n",
      "Iteration 361, loss = 20335275561.90238953\n",
      "Iteration 362, loss = 20335273047.28270340\n",
      "Iteration 363, loss = 20335270545.50438690\n",
      "Iteration 364, loss = 20335268052.14371490\n",
      "Iteration 365, loss = 20335265538.82719040\n",
      "Iteration 366, loss = 20335263016.71866226\n",
      "Iteration 367, loss = 20335260509.06369019\n",
      "Iteration 368, loss = 20335257974.26222992\n",
      "Iteration 369, loss = 20335255455.12552643\n",
      "Iteration 370, loss = 20335252941.98328781\n",
      "Iteration 371, loss = 20335250441.66061401\n",
      "Iteration 372, loss = 20335247915.20532990\n",
      "Iteration 373, loss = 20335245445.03374481\n",
      "Iteration 374, loss = 20335242953.46194839\n",
      "Iteration 375, loss = 20335240482.05911636\n",
      "Iteration 376, loss = 20335238002.21077347\n",
      "Iteration 377, loss = 20335235532.90619659\n",
      "Iteration 378, loss = 20335233072.04623413\n",
      "Iteration 379, loss = 20335230577.04860687\n",
      "Iteration 380, loss = 20335228123.61069870\n",
      "Iteration 381, loss = 20335225630.41767120\n",
      "Iteration 382, loss = 20335223139.88887405\n",
      "Iteration 383, loss = 20335220659.55127335\n",
      "Iteration 384, loss = 20335218140.89461517\n",
      "Iteration 385, loss = 20335215668.41355896\n",
      "Iteration 386, loss = 20335213173.47982025\n",
      "Iteration 387, loss = 20335210674.08288193\n",
      "Iteration 388, loss = 20335208198.78245163\n",
      "Iteration 389, loss = 20335205676.30574799\n",
      "Iteration 390, loss = 20335203195.02782059\n",
      "Iteration 391, loss = 20335200667.61130905\n",
      "Iteration 392, loss = 20335198157.42444992\n",
      "Iteration 393, loss = 20335195573.44540787\n",
      "Iteration 394, loss = 20335193016.58849716\n",
      "Iteration 395, loss = 20335190436.29487228\n",
      "Iteration 396, loss = 20335187849.21498871\n",
      "Iteration 397, loss = 20335185263.04748535\n",
      "Iteration 398, loss = 20335182690.77267075\n",
      "Iteration 399, loss = 20335180102.94442368\n",
      "Iteration 400, loss = 20335177509.40494156\n",
      "Iteration 401, loss = 20335174940.16073990\n",
      "Iteration 402, loss = 20335172342.74356079\n",
      "Iteration 403, loss = 20335169755.32217407\n",
      "Iteration 404, loss = 20335167175.36683273\n",
      "Iteration 405, loss = 20335164560.87259674\n",
      "Iteration 406, loss = 20335161927.88985443\n",
      "Iteration 407, loss = 20335159311.96080399\n",
      "Iteration 408, loss = 20335156620.04236221\n",
      "Iteration 409, loss = 20335153906.23865891\n",
      "Iteration 410, loss = 20335151159.42041397\n",
      "Iteration 411, loss = 20335148415.39978790\n",
      "Iteration 412, loss = 20335145609.38469696\n",
      "Iteration 413, loss = 20335142782.19870758\n",
      "Iteration 414, loss = 20335139947.85306168\n",
      "Iteration 415, loss = 20335137126.48695374\n",
      "Iteration 416, loss = 20335134268.41460037\n",
      "Iteration 417, loss = 20335131419.28720856\n",
      "Iteration 418, loss = 20335128571.71960831\n",
      "Iteration 419, loss = 20335125754.61879730\n",
      "Iteration 420, loss = 20335122947.96603394\n",
      "Iteration 421, loss = 20335120143.80945587\n",
      "Iteration 422, loss = 20335117328.98229599\n",
      "Iteration 423, loss = 20335114586.48901367\n",
      "Iteration 424, loss = 20335111766.97592163\n",
      "Iteration 425, loss = 20335109014.21309280\n",
      "Iteration 426, loss = 20335106218.66160202\n",
      "Iteration 427, loss = 20335103456.63757706\n",
      "Iteration 428, loss = 20335100669.41834259\n",
      "Iteration 429, loss = 20335097950.22936630\n",
      "Iteration 430, loss = 20335095186.39266205\n",
      "Iteration 431, loss = 20335092419.57195282\n",
      "Iteration 432, loss = 20335089707.23022842\n",
      "Iteration 433, loss = 20335086972.42496490\n",
      "Iteration 434, loss = 20335084218.04484940\n",
      "Iteration 435, loss = 20335081518.90764236\n",
      "Iteration 436, loss = 20335078799.55117416\n",
      "Iteration 437, loss = 20335076080.00102997\n",
      "Iteration 438, loss = 20335073372.56824875\n",
      "Iteration 439, loss = 20335070707.60755157\n",
      "Iteration 440, loss = 20335067992.85924530\n",
      "Iteration 441, loss = 20335065296.77735901\n",
      "Iteration 442, loss = 20335062615.99628830\n",
      "Iteration 443, loss = 20335059919.59558868\n",
      "Iteration 444, loss = 20335057212.57395935\n",
      "Iteration 445, loss = 20335054556.24382019\n",
      "Iteration 446, loss = 20335051866.72311020\n",
      "Iteration 447, loss = 20335049180.78742981\n",
      "Iteration 448, loss = 20335046550.02424622\n",
      "Iteration 449, loss = 20335043830.02651215\n",
      "Iteration 450, loss = 20335041182.08749771\n",
      "Iteration 451, loss = 20335038483.93305588\n",
      "Iteration 452, loss = 20335035846.65994644\n",
      "Iteration 453, loss = 20335033150.15538788\n",
      "Iteration 454, loss = 20335030493.67439651\n",
      "Iteration 455, loss = 20335027821.90832901\n",
      "Iteration 456, loss = 20335025180.62792206\n",
      "Iteration 457, loss = 20335022500.89598083\n",
      "Iteration 458, loss = 20335019859.49383926\n",
      "Iteration 459, loss = 20335017212.99458313\n",
      "Iteration 460, loss = 20335014557.97066116\n",
      "Iteration 461, loss = 20335011930.16590118\n",
      "Iteration 462, loss = 20335009280.54128265\n",
      "Iteration 463, loss = 20335006649.61858749\n",
      "Iteration 464, loss = 20335004035.40659714\n",
      "Iteration 465, loss = 20335001392.74226379\n",
      "Iteration 466, loss = 20334998771.13793945\n",
      "Iteration 467, loss = 20334996144.83074951\n",
      "Iteration 468, loss = 20334993496.89564896\n",
      "Iteration 469, loss = 20334990880.82257843\n",
      "Iteration 470, loss = 20334988226.35119247\n",
      "Iteration 471, loss = 20334985565.57495499\n",
      "Iteration 472, loss = 20334982956.14155960\n",
      "Iteration 473, loss = 20334980321.24236298\n",
      "Iteration 474, loss = 20334977645.47065735\n",
      "Iteration 475, loss = 20334975047.40517044\n",
      "Iteration 476, loss = 20334972417.15867996\n",
      "Iteration 477, loss = 20334969764.32228088\n",
      "Iteration 478, loss = 20334967144.55854034\n",
      "Iteration 479, loss = 20334964537.95378113\n",
      "Iteration 480, loss = 20334961897.36526489\n",
      "Iteration 481, loss = 20334959278.88195038\n",
      "Iteration 482, loss = 20334956668.56620789\n",
      "Iteration 483, loss = 20334954005.04619598\n",
      "Iteration 484, loss = 20334951435.26235199\n",
      "Iteration 485, loss = 20334948802.20012665\n",
      "Iteration 486, loss = 20334946189.97446823\n",
      "Iteration 487, loss = 20334943560.98881912\n",
      "Iteration 488, loss = 20334940982.77420807\n",
      "Iteration 489, loss = 20334938360.10208130\n",
      "Iteration 490, loss = 20334935753.92998505\n",
      "Iteration 491, loss = 20334933146.72013855\n",
      "Iteration 492, loss = 20334930543.83834076\n",
      "Iteration 493, loss = 20334927986.02601242\n",
      "Iteration 494, loss = 20334925383.06139755\n",
      "Iteration 495, loss = 20334922829.75908661\n",
      "Iteration 496, loss = 20334920256.75342178\n",
      "Iteration 497, loss = 20334917665.90776443\n",
      "Iteration 498, loss = 20334915101.56864929\n",
      "Iteration 499, loss = 20334912482.21950531\n",
      "Iteration 500, loss = 20334909879.95656586\n",
      "Iteration 1, loss = 19975137616.27192688\n",
      "Iteration 2, loss = 19975130054.24019623\n",
      "Iteration 3, loss = 19975122431.32664490\n",
      "Iteration 4, loss = 19975114953.93644333\n",
      "Iteration 5, loss = 19975107239.68377686\n",
      "Iteration 6, loss = 19975099851.66853714\n",
      "Iteration 7, loss = 19975092281.40551758\n",
      "Iteration 8, loss = 19975084742.62062073\n",
      "Iteration 9, loss = 19975077312.67702103\n",
      "Iteration 10, loss = 19975069983.88134384\n",
      "Iteration 11, loss = 19975062483.95554733\n",
      "Iteration 12, loss = 19975055142.46988297\n",
      "Iteration 13, loss = 19975047722.11004639\n",
      "Iteration 14, loss = 19975040484.68429184\n",
      "Iteration 15, loss = 19975033117.35664749\n",
      "Iteration 16, loss = 19975025666.50882721\n",
      "Iteration 17, loss = 19975018306.30414200\n",
      "Iteration 18, loss = 19975010933.04853058\n",
      "Iteration 19, loss = 19975003571.26678848\n",
      "Iteration 20, loss = 19974996048.60251236\n",
      "Iteration 21, loss = 19974988597.61898422\n",
      "Iteration 22, loss = 19974981227.27007675\n",
      "Iteration 23, loss = 19974973803.25937271\n",
      "Iteration 24, loss = 19974966482.73758316\n",
      "Iteration 25, loss = 19974959263.07219696\n",
      "Iteration 26, loss = 19974951939.86242676\n",
      "Iteration 27, loss = 19974944885.97902298\n",
      "Iteration 28, loss = 19974937747.19292831\n",
      "Iteration 29, loss = 19974930615.17378998\n",
      "Iteration 30, loss = 19974923484.26619720\n",
      "Iteration 31, loss = 19974916502.06036377\n",
      "Iteration 32, loss = 19974909225.90970230\n",
      "Iteration 33, loss = 19974901965.63468170\n",
      "Iteration 34, loss = 19974894661.50271606\n",
      "Iteration 35, loss = 19974887271.17368317\n",
      "Iteration 36, loss = 19974879741.38001633\n",
      "Iteration 37, loss = 19974872267.72968674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 19974864630.77791977\n",
      "Iteration 39, loss = 19974857164.93875122\n",
      "Iteration 40, loss = 19974849872.16511536\n",
      "Iteration 41, loss = 19974842541.47419357\n",
      "Iteration 42, loss = 19974835518.28575897\n",
      "Iteration 43, loss = 19974828378.45299911\n",
      "Iteration 44, loss = 19974821226.47838211\n",
      "Iteration 45, loss = 19974814078.12045288\n",
      "Iteration 46, loss = 19974806849.12601471\n",
      "Iteration 47, loss = 19974799526.96113586\n",
      "Iteration 48, loss = 19974792066.81649399\n",
      "Iteration 49, loss = 19974784554.24045181\n",
      "Iteration 50, loss = 19974776834.70552826\n",
      "Iteration 51, loss = 19974769059.36693192\n",
      "Iteration 52, loss = 19974760962.62613678\n",
      "Iteration 53, loss = 19974752797.36404037\n",
      "Iteration 54, loss = 19974744731.65302277\n",
      "Iteration 55, loss = 19974736511.34195328\n",
      "Iteration 56, loss = 19974728168.85870361\n",
      "Iteration 57, loss = 19974719584.36518860\n",
      "Iteration 58, loss = 19974710692.34634781\n",
      "Iteration 59, loss = 19974701453.07207108\n",
      "Iteration 60, loss = 19974692050.67278671\n",
      "Iteration 61, loss = 19974682743.90083313\n",
      "Iteration 62, loss = 19974673679.53218079\n",
      "Iteration 63, loss = 19974665232.10258865\n",
      "Iteration 64, loss = 19974657200.01661682\n",
      "Iteration 65, loss = 19974649291.62489319\n",
      "Iteration 66, loss = 19974641749.39690781\n",
      "Iteration 67, loss = 19974634278.72319031\n",
      "Iteration 68, loss = 19974626897.58304977\n",
      "Iteration 69, loss = 19974619673.29893875\n",
      "Iteration 70, loss = 19974612474.28035736\n",
      "Iteration 71, loss = 19974605349.08393860\n",
      "Iteration 72, loss = 19974598128.72770309\n",
      "Iteration 73, loss = 19974590826.17435455\n",
      "Iteration 74, loss = 19974583401.20979309\n",
      "Iteration 75, loss = 19974575841.16912842\n",
      "Iteration 76, loss = 19974568408.36211395\n",
      "Iteration 77, loss = 19974561168.86063766\n",
      "Iteration 78, loss = 19974553852.90708542\n",
      "Iteration 79, loss = 19974546828.44556427\n",
      "Iteration 80, loss = 19974539595.71979523\n",
      "Iteration 81, loss = 19974532500.15053558\n",
      "Iteration 82, loss = 19974525010.18819809\n",
      "Iteration 83, loss = 19974517520.58481598\n",
      "Iteration 84, loss = 19974509778.34339142\n",
      "Iteration 85, loss = 19974502007.51468277\n",
      "Iteration 86, loss = 19974494513.64888000\n",
      "Iteration 87, loss = 19974487445.15737534\n",
      "Iteration 88, loss = 19974480874.18346405\n",
      "Iteration 89, loss = 19974474747.50964737\n",
      "Iteration 90, loss = 19974469000.50433731\n",
      "Iteration 91, loss = 19974463779.10119247\n",
      "Iteration 92, loss = 19974458717.07432175\n",
      "Iteration 93, loss = 19974453991.68536758\n",
      "Iteration 94, loss = 19974449362.23567581\n",
      "Iteration 95, loss = 19974444923.81821060\n",
      "Iteration 96, loss = 19974440471.82172394\n",
      "Iteration 97, loss = 19974436065.29113007\n",
      "Iteration 98, loss = 19974431727.89976120\n",
      "Iteration 99, loss = 19974427499.79949951\n",
      "Iteration 100, loss = 19974423407.80722809\n",
      "Iteration 101, loss = 19974419393.41136551\n",
      "Iteration 102, loss = 19974415583.87337494\n",
      "Iteration 103, loss = 19974411710.76475525\n",
      "Iteration 104, loss = 19974407898.51891708\n",
      "Iteration 105, loss = 19974404144.04161072\n",
      "Iteration 106, loss = 19974400341.66029739\n",
      "Iteration 107, loss = 19974396663.46862793\n",
      "Iteration 108, loss = 19974393046.15547943\n",
      "Iteration 109, loss = 19974389427.66999054\n",
      "Iteration 110, loss = 19974385967.91462326\n",
      "Iteration 111, loss = 19974382614.28143692\n",
      "Iteration 112, loss = 19974379265.51054764\n",
      "Iteration 113, loss = 19974376021.90649414\n",
      "Iteration 114, loss = 19974372838.84290314\n",
      "Iteration 115, loss = 19974369628.65205765\n",
      "Iteration 116, loss = 19974366490.19520187\n",
      "Iteration 117, loss = 19974363377.42423630\n",
      "Iteration 118, loss = 19974360265.95312500\n",
      "Iteration 119, loss = 19974357208.48540878\n",
      "Iteration 120, loss = 19974354090.27728653\n",
      "Iteration 121, loss = 19974351046.42841339\n",
      "Iteration 122, loss = 19974347962.32249451\n",
      "Iteration 123, loss = 19974344967.61482620\n",
      "Iteration 124, loss = 19974341891.92478561\n",
      "Iteration 125, loss = 19974338855.08862686\n",
      "Iteration 126, loss = 19974335739.30377960\n",
      "Iteration 127, loss = 19974332696.15164185\n",
      "Iteration 128, loss = 19974329602.80430222\n",
      "Iteration 129, loss = 19974326459.94165802\n",
      "Iteration 130, loss = 19974323408.44491577\n",
      "Iteration 131, loss = 19974320375.61100006\n",
      "Iteration 132, loss = 19974317411.75889587\n",
      "Iteration 133, loss = 19974314451.65196228\n",
      "Iteration 134, loss = 19974311513.49013519\n",
      "Iteration 135, loss = 19974308645.78470993\n",
      "Iteration 136, loss = 19974305750.49896622\n",
      "Iteration 137, loss = 19974302852.35947418\n",
      "Iteration 138, loss = 19974299992.31432724\n",
      "Iteration 139, loss = 19974297101.57111740\n",
      "Iteration 140, loss = 19974294265.25228500\n",
      "Iteration 141, loss = 19974291400.27301788\n",
      "Iteration 142, loss = 19974288551.99585342\n",
      "Iteration 143, loss = 19974285686.95708084\n",
      "Iteration 144, loss = 19974282820.92631531\n",
      "Iteration 145, loss = 19974279923.85047531\n",
      "Iteration 146, loss = 19974277025.13080597\n",
      "Iteration 147, loss = 19974274137.97425461\n",
      "Iteration 148, loss = 19974271222.61863708\n",
      "Iteration 149, loss = 19974268350.46668625\n",
      "Iteration 150, loss = 19974265508.39000702\n",
      "Iteration 151, loss = 19974262729.80797958\n",
      "Iteration 152, loss = 19974259931.65335464\n",
      "Iteration 153, loss = 19974257122.82330322\n",
      "Iteration 154, loss = 19974254394.04048920\n",
      "Iteration 155, loss = 19974251605.87659073\n",
      "Iteration 156, loss = 19974248848.19306183\n",
      "Iteration 157, loss = 19974246100.55824661\n",
      "Iteration 158, loss = 19974243351.24379349\n",
      "Iteration 159, loss = 19974240645.35258865\n",
      "Iteration 160, loss = 19974237897.40831375\n",
      "Iteration 161, loss = 19974235163.79961395\n",
      "Iteration 162, loss = 19974232439.47836685\n",
      "Iteration 163, loss = 19974229732.08945847\n",
      "Iteration 164, loss = 19974227023.59760284\n",
      "Iteration 165, loss = 19974224307.87034988\n",
      "Iteration 166, loss = 19974221611.15906143\n",
      "Iteration 167, loss = 19974218914.85063553\n",
      "Iteration 168, loss = 19974216244.22922897\n",
      "Iteration 169, loss = 19974213530.72407532\n",
      "Iteration 170, loss = 19974210876.08645630\n",
      "Iteration 171, loss = 19974208205.25590515\n",
      "Iteration 172, loss = 19974205521.68851089\n",
      "Iteration 173, loss = 19974202829.46983719\n",
      "Iteration 174, loss = 19974200165.73886871\n",
      "Iteration 175, loss = 19974197531.33835220\n",
      "Iteration 176, loss = 19974194863.92667389\n",
      "Iteration 177, loss = 19974192223.59049225\n",
      "Iteration 178, loss = 19974189552.02746582\n",
      "Iteration 179, loss = 19974186942.90114975\n",
      "Iteration 180, loss = 19974184263.56820297\n",
      "Iteration 181, loss = 19974181636.77299118\n",
      "Iteration 182, loss = 19974179023.06037521\n",
      "Iteration 183, loss = 19974176375.80842209\n",
      "Iteration 184, loss = 19974173747.63620377\n",
      "Iteration 185, loss = 19974171095.46952438\n",
      "Iteration 186, loss = 19974168487.96228409\n",
      "Iteration 187, loss = 19974165873.25430298\n",
      "Iteration 188, loss = 19974163228.07232285\n",
      "Iteration 189, loss = 19974160643.54693604\n",
      "Iteration 190, loss = 19974158015.40761948\n",
      "Iteration 191, loss = 19974155397.93548965\n",
      "Iteration 192, loss = 19974152781.91047287\n",
      "Iteration 193, loss = 19974150185.44364166\n",
      "Iteration 194, loss = 19974147562.08814621\n",
      "Iteration 195, loss = 19974144979.37753296\n",
      "Iteration 196, loss = 19974142376.61024857\n",
      "Iteration 197, loss = 19974139736.07760620\n",
      "Iteration 198, loss = 19974137182.48907089\n",
      "Iteration 199, loss = 19974134573.32068253\n",
      "Iteration 200, loss = 19974131979.58418655\n",
      "Iteration 201, loss = 19974129387.88516998\n",
      "Iteration 202, loss = 19974126796.41310120\n",
      "Iteration 203, loss = 19974124198.48358154\n",
      "Iteration 204, loss = 19974121623.47126007\n",
      "Iteration 205, loss = 19974119012.35996628\n",
      "Iteration 206, loss = 19974116422.68605423\n",
      "Iteration 207, loss = 19974113864.11912155\n",
      "Iteration 208, loss = 19974111289.49148941\n",
      "Iteration 209, loss = 19974108702.12499619\n",
      "Iteration 210, loss = 19974106114.69159698\n",
      "Iteration 211, loss = 19974103517.53754807\n",
      "Iteration 212, loss = 19974100971.94501114\n",
      "Iteration 213, loss = 19974098365.52541351\n",
      "Iteration 214, loss = 19974095765.22043991\n",
      "Iteration 215, loss = 19974093224.93662262\n",
      "Iteration 216, loss = 19974090598.46854782\n",
      "Iteration 217, loss = 19974088028.85795212\n",
      "Iteration 218, loss = 19974085475.15454865\n",
      "Iteration 219, loss = 19974082856.03981400\n",
      "Iteration 220, loss = 19974080287.70364380\n",
      "Iteration 221, loss = 19974077711.56253815\n",
      "Iteration 222, loss = 19974075111.59777832\n",
      "Iteration 223, loss = 19974072482.26628876\n",
      "Iteration 224, loss = 19974069906.30982590\n",
      "Iteration 225, loss = 19974067301.76206970\n",
      "Iteration 226, loss = 19974064734.39342117\n",
      "Iteration 227, loss = 19974062097.77845001\n",
      "Iteration 228, loss = 19974059462.65916824\n",
      "Iteration 229, loss = 19974056877.40826416\n",
      "Iteration 230, loss = 19974054249.39587021\n",
      "Iteration 231, loss = 19974051602.81587219\n",
      "Iteration 232, loss = 19974048999.76179504\n",
      "Iteration 233, loss = 19974046357.16933060\n",
      "Iteration 234, loss = 19974043721.74115372\n",
      "Iteration 235, loss = 19974041071.71669388\n",
      "Iteration 236, loss = 19974038415.44013214\n",
      "Iteration 237, loss = 19974035775.91026688\n",
      "Iteration 238, loss = 19974033085.04381561\n",
      "Iteration 239, loss = 19974030443.48781967\n",
      "Iteration 240, loss = 19974027771.63068390\n",
      "Iteration 241, loss = 19974025097.54528427\n",
      "Iteration 242, loss = 19974022449.83644485\n",
      "Iteration 243, loss = 19974019771.94111633\n",
      "Iteration 244, loss = 19974017093.97925186\n",
      "Iteration 245, loss = 19974014405.98532867\n",
      "Iteration 246, loss = 19974011705.00877762\n",
      "Iteration 247, loss = 19974009029.80447388\n",
      "Iteration 248, loss = 19974006380.37532806\n",
      "Iteration 249, loss = 19974003696.58290482\n",
      "Iteration 250, loss = 19974001014.44347382\n",
      "Iteration 251, loss = 19973998321.25828552\n",
      "Iteration 252, loss = 19973995653.97198486\n",
      "Iteration 253, loss = 19973992976.61124039\n",
      "Iteration 254, loss = 19973990288.72311783\n",
      "Iteration 255, loss = 19973987593.74551392\n",
      "Iteration 256, loss = 19973984953.70285797\n",
      "Iteration 257, loss = 19973982254.44040298\n",
      "Iteration 258, loss = 19973979594.70406342\n",
      "Iteration 259, loss = 19973976948.39657211\n",
      "Iteration 260, loss = 19973974257.83433533\n",
      "Iteration 261, loss = 19973971583.36737061\n",
      "Iteration 262, loss = 19973968903.74978256\n",
      "Iteration 263, loss = 19973966278.03638458\n",
      "Iteration 264, loss = 19973963625.01949692\n",
      "Iteration 265, loss = 19973960964.83085251\n",
      "Iteration 266, loss = 19973958309.35530853\n",
      "Iteration 267, loss = 19973955650.79874802\n",
      "Iteration 268, loss = 19973952985.77420044\n",
      "Iteration 269, loss = 19973950363.36175919\n",
      "Iteration 270, loss = 19973947696.61658096\n",
      "Iteration 271, loss = 19973945085.80086136\n",
      "Iteration 272, loss = 19973942426.60677338\n",
      "Iteration 273, loss = 19973939837.75410843\n",
      "Iteration 274, loss = 19973937170.20510864\n",
      "Iteration 275, loss = 19973934566.09955215\n",
      "Iteration 276, loss = 19973931920.87659454\n",
      "Iteration 277, loss = 19973929295.57928085\n",
      "Iteration 278, loss = 19973926669.83777618\n",
      "Iteration 279, loss = 19973924058.77033615\n",
      "Iteration 280, loss = 19973921471.27696609\n",
      "Iteration 281, loss = 19973918817.23159409\n",
      "Iteration 282, loss = 19973916232.92948532\n",
      "Iteration 283, loss = 19973913611.98976517\n",
      "Iteration 284, loss = 19973911015.60473251\n",
      "Iteration 285, loss = 19973908418.14054871\n",
      "Iteration 286, loss = 19973905811.00213623\n",
      "Iteration 287, loss = 19973903204.42816925\n",
      "Iteration 288, loss = 19973900613.66152191\n",
      "Iteration 289, loss = 19973898040.91949081\n",
      "Iteration 290, loss = 19973895423.29237747\n",
      "Iteration 291, loss = 19973892833.04116440\n",
      "Iteration 292, loss = 19973890238.35876465\n",
      "Iteration 293, loss = 19973887656.89732742\n",
      "Iteration 294, loss = 19973885082.89868927\n",
      "Iteration 295, loss = 19973882481.28126526\n",
      "Iteration 296, loss = 19973879909.07451248\n",
      "Iteration 297, loss = 19973877330.52757263\n",
      "Iteration 298, loss = 19973874762.02888489\n",
      "Iteration 299, loss = 19973872180.31259537\n",
      "Iteration 300, loss = 19973869617.82298660\n",
      "Iteration 301, loss = 19973866997.85232162\n",
      "Iteration 302, loss = 19973864434.21121216\n",
      "Iteration 303, loss = 19973861863.95861435\n",
      "Iteration 304, loss = 19973859301.55661011\n",
      "Iteration 305, loss = 19973856730.51787567\n",
      "Iteration 306, loss = 19973854169.94488144\n",
      "Iteration 307, loss = 19973851616.65260696\n",
      "Iteration 308, loss = 19973849021.43705750\n",
      "Iteration 309, loss = 19973846469.51735306\n",
      "Iteration 310, loss = 19973843916.44446564\n",
      "Iteration 311, loss = 19973841347.71501160\n",
      "Iteration 312, loss = 19973838794.38257980\n",
      "Iteration 313, loss = 19973836236.27658081\n",
      "Iteration 314, loss = 19973833655.57776642\n",
      "Iteration 315, loss = 19973831136.59513855\n",
      "Iteration 316, loss = 19973828583.99212265\n",
      "Iteration 317, loss = 19973826009.06230545\n",
      "Iteration 318, loss = 19973823453.93292236\n",
      "Iteration 319, loss = 19973820930.72496414\n",
      "Iteration 320, loss = 19973818377.30260086\n",
      "Iteration 321, loss = 19973815802.45864868\n",
      "Iteration 322, loss = 19973813275.14893341\n",
      "Iteration 323, loss = 19973810717.10538864\n",
      "Iteration 324, loss = 19973808180.24556351\n",
      "Iteration 325, loss = 19973805661.87009430\n",
      "Iteration 326, loss = 19973803075.55446243\n",
      "Iteration 327, loss = 19973800553.11699295\n",
      "Iteration 328, loss = 19973797990.19306946\n",
      "Iteration 329, loss = 19973795492.36808395\n",
      "Iteration 330, loss = 19973792926.85163498\n",
      "Iteration 331, loss = 19973790392.03131485\n",
      "Iteration 332, loss = 19973787864.44833755\n",
      "Iteration 333, loss = 19973785314.08526230\n",
      "Iteration 334, loss = 19973782800.45885468\n",
      "Iteration 335, loss = 19973780247.42026901\n",
      "Iteration 336, loss = 19973777718.74053955\n",
      "Iteration 337, loss = 19973775216.26650238\n",
      "Iteration 338, loss = 19973772653.07442093\n",
      "Iteration 339, loss = 19973770154.49139786\n",
      "Iteration 340, loss = 19973767615.54041290\n",
      "Iteration 341, loss = 19973765104.12740326\n",
      "Iteration 342, loss = 19973762568.49978638\n",
      "Iteration 343, loss = 19973760032.91285706\n",
      "Iteration 344, loss = 19973757531.57218933\n",
      "Iteration 345, loss = 19973755016.88728333\n",
      "Iteration 346, loss = 19973752470.54845047\n",
      "Iteration 347, loss = 19973749968.44462585\n",
      "Iteration 348, loss = 19973747452.16971588\n",
      "Iteration 349, loss = 19973744945.69360352\n",
      "Iteration 350, loss = 19973742408.78530502\n",
      "Iteration 351, loss = 19973739894.42366028\n",
      "Iteration 352, loss = 19973737406.51503754\n",
      "Iteration 353, loss = 19973734878.59585190\n",
      "Iteration 354, loss = 19973732359.87718201\n",
      "Iteration 355, loss = 19973729799.39064026\n",
      "Iteration 356, loss = 19973727346.05300903\n",
      "Iteration 357, loss = 19973724839.58313751\n",
      "Iteration 358, loss = 19973722305.72517395\n",
      "Iteration 359, loss = 19973719826.21554947\n",
      "Iteration 360, loss = 19973717282.60625458\n",
      "Iteration 361, loss = 19973714773.28487015\n",
      "Iteration 362, loss = 19973712278.92440796\n",
      "Iteration 363, loss = 19973709773.42950439\n",
      "Iteration 364, loss = 19973707271.23503494\n",
      "Iteration 365, loss = 19973704791.11157990\n",
      "Iteration 366, loss = 19973702279.47671127\n",
      "Iteration 367, loss = 19973699766.37504578\n",
      "Iteration 368, loss = 19973697255.78982925\n",
      "Iteration 369, loss = 19973694774.30073166\n",
      "Iteration 370, loss = 19973692242.79102707\n",
      "Iteration 371, loss = 19973689759.79159546\n",
      "Iteration 372, loss = 19973687268.97601700\n",
      "Iteration 373, loss = 19973684788.89957428\n",
      "Iteration 374, loss = 19973682290.47092819\n",
      "Iteration 375, loss = 19973679793.63412476\n",
      "Iteration 376, loss = 19973677275.94139099\n",
      "Iteration 377, loss = 19973674789.30332565\n",
      "Iteration 378, loss = 19973672302.38282776\n",
      "Iteration 379, loss = 19973669813.95707703\n",
      "Iteration 380, loss = 19973667329.89641571\n",
      "Iteration 381, loss = 19973664834.53677368\n",
      "Iteration 382, loss = 19973662336.38030243\n",
      "Iteration 383, loss = 19973659820.56990051\n",
      "Iteration 384, loss = 19973657361.58603668\n",
      "Iteration 385, loss = 19973654850.07973862\n",
      "Iteration 386, loss = 19973652391.15578842\n",
      "Iteration 387, loss = 19973649883.38948059\n",
      "Iteration 388, loss = 19973647390.22972107\n",
      "Iteration 389, loss = 19973644908.95138550\n",
      "Iteration 390, loss = 19973642430.05129623\n",
      "Iteration 391, loss = 19973639940.28397751\n",
      "Iteration 392, loss = 19973637437.78015518\n",
      "Iteration 393, loss = 19973634982.48566437\n",
      "Iteration 394, loss = 19973632493.83769989\n",
      "Iteration 395, loss = 19973629980.95603943\n",
      "Iteration 396, loss = 19973627533.47713852\n",
      "Iteration 397, loss = 19973625011.16268539\n",
      "Iteration 398, loss = 19973622512.22195435\n",
      "Iteration 399, loss = 19973619990.63145447\n",
      "Iteration 400, loss = 19973617510.14613724\n",
      "Iteration 401, loss = 19973614996.71716690\n",
      "Iteration 402, loss = 19973612524.72412491\n",
      "Iteration 403, loss = 19973610010.20433807\n",
      "Iteration 404, loss = 19973607535.53243637\n",
      "Iteration 405, loss = 19973605066.11231613\n",
      "Iteration 406, loss = 19973602596.42835236\n",
      "Iteration 407, loss = 19973600116.14620590\n",
      "Iteration 408, loss = 19973597651.10059357\n",
      "Iteration 409, loss = 19973595161.93970871\n",
      "Iteration 410, loss = 19973592696.51710892\n",
      "Iteration 411, loss = 19973590225.48816681\n",
      "Iteration 412, loss = 19973587720.91174698\n",
      "Iteration 413, loss = 19973585230.44096375\n",
      "Iteration 414, loss = 19973582791.06868362\n",
      "Iteration 415, loss = 19973580282.61300659\n",
      "Iteration 416, loss = 19973577769.28529358\n",
      "Iteration 417, loss = 19973575265.85151291\n",
      "Iteration 418, loss = 19973572727.62537384\n",
      "Iteration 419, loss = 19973570233.35399628\n",
      "Iteration 420, loss = 19973567744.37825394\n",
      "Iteration 421, loss = 19973565323.60435104\n",
      "Iteration 422, loss = 19973562824.84265900\n",
      "Iteration 423, loss = 19973560353.45328140\n",
      "Iteration 424, loss = 19973557872.67766190\n",
      "Iteration 425, loss = 19973555393.28613281\n",
      "Iteration 426, loss = 19973552911.08387375\n",
      "Iteration 427, loss = 19973550458.31249619\n",
      "Iteration 428, loss = 19973548007.22644424\n",
      "Iteration 429, loss = 19973545530.32079697\n",
      "Iteration 430, loss = 19973543041.23988724\n",
      "Iteration 431, loss = 19973540583.23177338\n",
      "Iteration 432, loss = 19973538148.70175552\n",
      "Iteration 433, loss = 19973535683.43746567\n",
      "Iteration 434, loss = 19973533183.45552826\n",
      "Iteration 435, loss = 19973530719.29885483\n",
      "Iteration 436, loss = 19973528283.74341202\n",
      "Iteration 437, loss = 19973525819.56065369\n",
      "Iteration 438, loss = 19973523356.56236649\n",
      "Iteration 439, loss = 19973520892.98285294\n",
      "Iteration 440, loss = 19973518437.34598160\n",
      "Iteration 441, loss = 19973515982.36285019\n",
      "Iteration 442, loss = 19973513490.53184128\n",
      "Iteration 443, loss = 19973511062.19712448\n",
      "Iteration 444, loss = 19973508573.89482880\n",
      "Iteration 445, loss = 19973506119.47995377\n",
      "Iteration 446, loss = 19973503652.44485092\n",
      "Iteration 447, loss = 19973501187.51644516\n",
      "Iteration 448, loss = 19973498701.52319336\n",
      "Iteration 449, loss = 19973496220.19784164\n",
      "Iteration 450, loss = 19973493726.57796478\n",
      "Iteration 451, loss = 19973491271.79710770\n",
      "Iteration 452, loss = 19973488774.56250381\n",
      "Iteration 453, loss = 19973486342.85465240\n",
      "Iteration 454, loss = 19973483893.71867752\n",
      "Iteration 455, loss = 19973481417.16027832\n",
      "Iteration 456, loss = 19973478964.80280685\n",
      "Iteration 457, loss = 19973476512.41723251\n",
      "Iteration 458, loss = 19973474064.90620422\n",
      "Iteration 459, loss = 19973471609.31869888\n",
      "Iteration 460, loss = 19973469166.17591095\n",
      "Iteration 461, loss = 19973466689.15614319\n",
      "Iteration 462, loss = 19973464275.69627380\n",
      "Iteration 463, loss = 19973461799.93643570\n",
      "Iteration 464, loss = 19973459348.95968628\n",
      "Iteration 465, loss = 19973456926.29486084\n",
      "Iteration 466, loss = 19973454436.50754929\n",
      "Iteration 467, loss = 19973451974.95609283\n",
      "Iteration 468, loss = 19973449551.39937592\n",
      "Iteration 469, loss = 19973447125.94790649\n",
      "Iteration 470, loss = 19973444642.29546738\n",
      "Iteration 471, loss = 19973442188.26062775\n",
      "Iteration 472, loss = 19973439761.07896423\n",
      "Iteration 473, loss = 19973437311.21895218\n",
      "Iteration 474, loss = 19973434846.17313766\n",
      "Iteration 475, loss = 19973432410.15456009\n",
      "Iteration 476, loss = 19973429980.92029572\n",
      "Iteration 477, loss = 19973427496.08232117\n",
      "Iteration 478, loss = 19973425082.60216522\n",
      "Iteration 479, loss = 19973422613.24818039\n",
      "Iteration 480, loss = 19973420188.93389511\n",
      "Iteration 481, loss = 19973417711.19087982\n",
      "Iteration 482, loss = 19973415303.79331207\n",
      "Iteration 483, loss = 19973412848.48676300\n",
      "Iteration 484, loss = 19973410382.43582916\n",
      "Iteration 485, loss = 19973407958.32893753\n",
      "Iteration 486, loss = 19973405511.06021500\n",
      "Iteration 487, loss = 19973403061.23829651\n",
      "Iteration 488, loss = 19973400591.66915512\n",
      "Iteration 489, loss = 19973398162.50088882\n",
      "Iteration 490, loss = 19973395717.17803955\n",
      "Iteration 491, loss = 19973393269.35806274\n",
      "Iteration 492, loss = 19973390808.22581100\n",
      "Iteration 493, loss = 19973388363.31541824\n",
      "Iteration 494, loss = 19973385926.16257095\n",
      "Iteration 495, loss = 19973383444.36731339\n",
      "Iteration 496, loss = 19973381009.81955338\n",
      "Iteration 497, loss = 19973378551.81070328\n",
      "Iteration 498, loss = 19973376101.25628662\n",
      "Iteration 499, loss = 19973373602.73698044\n",
      "Iteration 500, loss = 19973371189.58709335\n",
      "Iteration 1, loss = 20536520823.87548065\n",
      "Iteration 2, loss = 20536509902.83473587\n",
      "Iteration 3, loss = 20536499013.97705460\n",
      "Iteration 4, loss = 20536488189.65998077\n",
      "Iteration 5, loss = 20536477370.80781555\n",
      "Iteration 6, loss = 20536466441.20952988\n",
      "Iteration 7, loss = 20536455300.83057022\n",
      "Iteration 8, loss = 20536444110.98351288\n",
      "Iteration 9, loss = 20536432594.68900299\n",
      "Iteration 10, loss = 20536421053.31692123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 20536409314.49314880\n",
      "Iteration 12, loss = 20536397455.28472519\n",
      "Iteration 13, loss = 20536385560.51794052\n",
      "Iteration 14, loss = 20536373796.05026245\n",
      "Iteration 15, loss = 20536362012.74294281\n",
      "Iteration 16, loss = 20536350554.48775482\n",
      "Iteration 17, loss = 20536338813.17647171\n",
      "Iteration 18, loss = 20536327182.38404465\n",
      "Iteration 19, loss = 20536315174.63798523\n",
      "Iteration 20, loss = 20536303118.42318344\n",
      "Iteration 21, loss = 20536290247.43939590\n",
      "Iteration 22, loss = 20536277211.00455093\n",
      "Iteration 23, loss = 20536263635.22103119\n",
      "Iteration 24, loss = 20536249625.01258087\n",
      "Iteration 25, loss = 20536235383.82621002\n",
      "Iteration 26, loss = 20536220932.15052032\n",
      "Iteration 27, loss = 20536205522.96800232\n",
      "Iteration 28, loss = 20536189417.59002686\n",
      "Iteration 29, loss = 20536172493.94004822\n",
      "Iteration 30, loss = 20536155102.28503418\n",
      "Iteration 31, loss = 20536137497.42429352\n",
      "Iteration 32, loss = 20536119885.86545944\n",
      "Iteration 33, loss = 20536102874.80537415\n",
      "Iteration 34, loss = 20536086373.61760712\n",
      "Iteration 35, loss = 20536071344.75422287\n",
      "Iteration 36, loss = 20536056875.50135422\n",
      "Iteration 37, loss = 20536043477.53739929\n",
      "Iteration 38, loss = 20536030523.90513611\n",
      "Iteration 39, loss = 20536018081.08351517\n",
      "Iteration 40, loss = 20536006262.59331512\n",
      "Iteration 41, loss = 20535994749.58630371\n",
      "Iteration 42, loss = 20535984086.11250305\n",
      "Iteration 43, loss = 20535974132.84680557\n",
      "Iteration 44, loss = 20535964266.67565536\n",
      "Iteration 45, loss = 20535954583.95281982\n",
      "Iteration 46, loss = 20535944259.24060822\n",
      "Iteration 47, loss = 20535933485.43770981\n",
      "Iteration 48, loss = 20535922600.92266464\n",
      "Iteration 49, loss = 20535912054.44021988\n",
      "Iteration 50, loss = 20535902112.03858948\n",
      "Iteration 51, loss = 20535893111.21345520\n",
      "Iteration 52, loss = 20535884491.65166855\n",
      "Iteration 53, loss = 20535875862.34426498\n",
      "Iteration 54, loss = 20535867124.43169403\n",
      "Iteration 55, loss = 20535858005.58245850\n",
      "Iteration 56, loss = 20535849163.14498138\n",
      "Iteration 57, loss = 20535840893.96569824\n",
      "Iteration 58, loss = 20535833050.27742767\n",
      "Iteration 59, loss = 20535825819.36141205\n",
      "Iteration 60, loss = 20535819107.29403687\n",
      "Iteration 61, loss = 20535812718.63580704\n",
      "Iteration 62, loss = 20535806415.19581604\n",
      "Iteration 63, loss = 20535800111.44343185\n",
      "Iteration 64, loss = 20535793630.70399094\n",
      "Iteration 65, loss = 20535787413.75981140\n",
      "Iteration 66, loss = 20535781508.29681396\n",
      "Iteration 67, loss = 20535775907.91922379\n",
      "Iteration 68, loss = 20535770726.89499283\n",
      "Iteration 69, loss = 20535765610.98277664\n",
      "Iteration 70, loss = 20535760542.60786438\n",
      "Iteration 71, loss = 20535755552.13575745\n",
      "Iteration 72, loss = 20535750610.97307587\n",
      "Iteration 73, loss = 20535745600.76657867\n",
      "Iteration 74, loss = 20535740674.95171738\n",
      "Iteration 75, loss = 20535735534.82065201\n",
      "Iteration 76, loss = 20535730323.99615479\n",
      "Iteration 77, loss = 20535725098.70672607\n",
      "Iteration 78, loss = 20535720000.83147812\n",
      "Iteration 79, loss = 20535714925.52031708\n",
      "Iteration 80, loss = 20535709839.30928040\n",
      "Iteration 81, loss = 20535704851.24888992\n",
      "Iteration 82, loss = 20535700076.83519363\n",
      "Iteration 83, loss = 20535695434.30355835\n",
      "Iteration 84, loss = 20535690894.95894241\n",
      "Iteration 85, loss = 20535686350.78224564\n",
      "Iteration 86, loss = 20535681868.79890442\n",
      "Iteration 87, loss = 20535677438.38414001\n",
      "Iteration 88, loss = 20535673018.24433899\n",
      "Iteration 89, loss = 20535668645.20196533\n",
      "Iteration 90, loss = 20535664350.23502731\n",
      "Iteration 91, loss = 20535659934.79759979\n",
      "Iteration 92, loss = 20535655730.09283829\n",
      "Iteration 93, loss = 20535651441.22074127\n",
      "Iteration 94, loss = 20535647186.87999725\n",
      "Iteration 95, loss = 20535642979.41283035\n",
      "Iteration 96, loss = 20535638793.38229370\n",
      "Iteration 97, loss = 20535634592.00511551\n",
      "Iteration 98, loss = 20535630509.79884720\n",
      "Iteration 99, loss = 20535626327.59799957\n",
      "Iteration 100, loss = 20535622222.21394730\n",
      "Iteration 101, loss = 20535618121.85463333\n",
      "Iteration 102, loss = 20535614027.64279938\n",
      "Iteration 103, loss = 20535609930.57153320\n",
      "Iteration 104, loss = 20535605907.12532043\n",
      "Iteration 105, loss = 20535601836.29236984\n",
      "Iteration 106, loss = 20535597818.87002563\n",
      "Iteration 107, loss = 20535593777.33793640\n",
      "Iteration 108, loss = 20535589753.32609558\n",
      "Iteration 109, loss = 20535585726.72526169\n",
      "Iteration 110, loss = 20535581719.80125809\n",
      "Iteration 111, loss = 20535577685.67661667\n",
      "Iteration 112, loss = 20535573718.58176804\n",
      "Iteration 113, loss = 20535569730.34158325\n",
      "Iteration 114, loss = 20535565756.39816284\n",
      "Iteration 115, loss = 20535561838.98790359\n",
      "Iteration 116, loss = 20535557819.85350800\n",
      "Iteration 117, loss = 20535553896.64596176\n",
      "Iteration 118, loss = 20535549966.55952454\n",
      "Iteration 119, loss = 20535546052.85645676\n",
      "Iteration 120, loss = 20535542124.68018341\n",
      "Iteration 121, loss = 20535538278.49818039\n",
      "Iteration 122, loss = 20535534321.87116241\n",
      "Iteration 123, loss = 20535530487.64514160\n",
      "Iteration 124, loss = 20535526579.60913849\n",
      "Iteration 125, loss = 20535522719.84124374\n",
      "Iteration 126, loss = 20535518826.25851059\n",
      "Iteration 127, loss = 20535514959.48421478\n",
      "Iteration 128, loss = 20535511116.28855896\n",
      "Iteration 129, loss = 20535507262.21194839\n",
      "Iteration 130, loss = 20535503460.79869080\n",
      "Iteration 131, loss = 20535499591.96911621\n",
      "Iteration 132, loss = 20535495749.57962036\n",
      "Iteration 133, loss = 20535491896.88249969\n",
      "Iteration 134, loss = 20535488000.43970871\n",
      "Iteration 135, loss = 20535484096.19491196\n",
      "Iteration 136, loss = 20535480207.51615906\n",
      "Iteration 137, loss = 20535476202.47873306\n",
      "Iteration 138, loss = 20535472223.45781708\n",
      "Iteration 139, loss = 20535468246.77009964\n",
      "Iteration 140, loss = 20535464375.47349548\n",
      "Iteration 141, loss = 20535460535.62181854\n",
      "Iteration 142, loss = 20535456706.77946091\n",
      "Iteration 143, loss = 20535452908.94713211\n",
      "Iteration 144, loss = 20535449145.01870728\n",
      "Iteration 145, loss = 20535445366.50235367\n",
      "Iteration 146, loss = 20535441607.14284515\n",
      "Iteration 147, loss = 20535437834.85514832\n",
      "Iteration 148, loss = 20535434068.73081970\n",
      "Iteration 149, loss = 20535430307.97492599\n",
      "Iteration 150, loss = 20535426561.65350342\n",
      "Iteration 151, loss = 20535422786.76722336\n",
      "Iteration 152, loss = 20535419042.24393082\n",
      "Iteration 153, loss = 20535415300.98957825\n",
      "Iteration 154, loss = 20535411556.03244781\n",
      "Iteration 155, loss = 20535407845.80879974\n",
      "Iteration 156, loss = 20535404097.31185532\n",
      "Iteration 157, loss = 20535400392.59311295\n",
      "Iteration 158, loss = 20535396661.50346756\n",
      "Iteration 159, loss = 20535392921.25377274\n",
      "Iteration 160, loss = 20535389235.63301086\n",
      "Iteration 161, loss = 20535385504.49460602\n",
      "Iteration 162, loss = 20535381823.34513474\n",
      "Iteration 163, loss = 20535378124.20079803\n",
      "Iteration 164, loss = 20535374444.12519836\n",
      "Iteration 165, loss = 20535370794.89969635\n",
      "Iteration 166, loss = 20535367117.47636414\n",
      "Iteration 167, loss = 20535363482.03379440\n",
      "Iteration 168, loss = 20535359843.12488937\n",
      "Iteration 169, loss = 20535356198.63595581\n",
      "Iteration 170, loss = 20535352579.37734985\n",
      "Iteration 171, loss = 20535348985.67517853\n",
      "Iteration 172, loss = 20535345388.98907471\n",
      "Iteration 173, loss = 20535341803.51705551\n",
      "Iteration 174, loss = 20535338163.32828140\n",
      "Iteration 175, loss = 20535334547.14111710\n",
      "Iteration 176, loss = 20535330924.70278931\n",
      "Iteration 177, loss = 20535327324.49619293\n",
      "Iteration 178, loss = 20535323671.17206192\n",
      "Iteration 179, loss = 20535320047.13684082\n",
      "Iteration 180, loss = 20535316378.12104416\n",
      "Iteration 181, loss = 20535312800.03192902\n",
      "Iteration 182, loss = 20535309161.13872910\n",
      "Iteration 183, loss = 20535305561.76099777\n",
      "Iteration 184, loss = 20535301965.34319687\n",
      "Iteration 185, loss = 20535298388.36235046\n",
      "Iteration 186, loss = 20535294768.24886322\n",
      "Iteration 187, loss = 20535291191.62001801\n",
      "Iteration 188, loss = 20535287554.25790405\n",
      "Iteration 189, loss = 20535283969.52101898\n",
      "Iteration 190, loss = 20535280372.44430161\n",
      "Iteration 191, loss = 20535276758.49518204\n",
      "Iteration 192, loss = 20535273176.05751801\n",
      "Iteration 193, loss = 20535269578.40208054\n",
      "Iteration 194, loss = 20535265975.88402557\n",
      "Iteration 195, loss = 20535262432.95278549\n",
      "Iteration 196, loss = 20535258826.69688416\n",
      "Iteration 197, loss = 20535255237.33247375\n",
      "Iteration 198, loss = 20535251650.63904572\n",
      "Iteration 199, loss = 20535248104.97446442\n",
      "Iteration 200, loss = 20535244526.06966019\n",
      "Iteration 201, loss = 20535240924.56892776\n",
      "Iteration 202, loss = 20535237355.52861023\n",
      "Iteration 203, loss = 20535233778.69595337\n",
      "Iteration 204, loss = 20535230228.61089325\n",
      "Iteration 205, loss = 20535226654.91355133\n",
      "Iteration 206, loss = 20535223082.50387192\n",
      "Iteration 207, loss = 20535219494.19702911\n",
      "Iteration 208, loss = 20535215931.70186996\n",
      "Iteration 209, loss = 20535212381.46002579\n",
      "Iteration 210, loss = 20535208851.30482101\n",
      "Iteration 211, loss = 20535205297.01332474\n",
      "Iteration 212, loss = 20535201740.39759445\n",
      "Iteration 213, loss = 20535198238.39654541\n",
      "Iteration 214, loss = 20535194689.33314514\n",
      "Iteration 215, loss = 20535191188.14424515\n",
      "Iteration 216, loss = 20535187626.43908691\n",
      "Iteration 217, loss = 20535184048.84084320\n",
      "Iteration 218, loss = 20535180461.26160431\n",
      "Iteration 219, loss = 20535176822.72684860\n",
      "Iteration 220, loss = 20535173238.89097595\n",
      "Iteration 221, loss = 20535169648.51984024\n",
      "Iteration 222, loss = 20535166110.29218292\n",
      "Iteration 223, loss = 20535162572.01769257\n",
      "Iteration 224, loss = 20535159037.20766830\n",
      "Iteration 225, loss = 20535155490.04370117\n",
      "Iteration 226, loss = 20535151970.86658478\n",
      "Iteration 227, loss = 20535148438.61970139\n",
      "Iteration 228, loss = 20535144925.62540436\n",
      "Iteration 229, loss = 20535141437.99071121\n",
      "Iteration 230, loss = 20535137908.75191879\n",
      "Iteration 231, loss = 20535134402.38177872\n",
      "Iteration 232, loss = 20535130866.43661499\n",
      "Iteration 233, loss = 20535127391.75421906\n",
      "Iteration 234, loss = 20535123888.39855576\n",
      "Iteration 235, loss = 20535120392.25049591\n",
      "Iteration 236, loss = 20535116871.52011490\n",
      "Iteration 237, loss = 20535113383.92990494\n",
      "Iteration 238, loss = 20535109872.61753845\n",
      "Iteration 239, loss = 20535106398.20248032\n",
      "Iteration 240, loss = 20535102893.69392014\n",
      "Iteration 241, loss = 20535099393.14617920\n",
      "Iteration 242, loss = 20535095928.63881302\n",
      "Iteration 243, loss = 20535092415.90655136\n",
      "Iteration 244, loss = 20535088897.45946121\n",
      "Iteration 245, loss = 20535085412.88304138\n",
      "Iteration 246, loss = 20535081893.54567337\n",
      "Iteration 247, loss = 20535078357.61414719\n",
      "Iteration 248, loss = 20535074847.22582626\n",
      "Iteration 249, loss = 20535071354.00379181\n",
      "Iteration 250, loss = 20535067861.06600571\n",
      "Iteration 251, loss = 20535064349.65151596\n",
      "Iteration 252, loss = 20535060866.37877274\n",
      "Iteration 253, loss = 20535057389.00490570\n",
      "Iteration 254, loss = 20535053934.46557999\n",
      "Iteration 255, loss = 20535050414.50325775\n",
      "Iteration 256, loss = 20535046996.01087570\n",
      "Iteration 257, loss = 20535043507.64754105\n",
      "Iteration 258, loss = 20535040004.85937500\n",
      "Iteration 259, loss = 20535036572.17780304\n",
      "Iteration 260, loss = 20535033099.27383804\n",
      "Iteration 261, loss = 20535029617.11236954\n",
      "Iteration 262, loss = 20535026216.46793365\n",
      "Iteration 263, loss = 20535022732.14001465\n",
      "Iteration 264, loss = 20535019282.08179474\n",
      "Iteration 265, loss = 20535015831.66814804\n",
      "Iteration 266, loss = 20535012381.17677307\n",
      "Iteration 267, loss = 20535008895.37452698\n",
      "Iteration 268, loss = 20535005422.09072876\n",
      "Iteration 269, loss = 20535001959.69196701\n",
      "Iteration 270, loss = 20534998502.63798523\n",
      "Iteration 271, loss = 20534995052.89786530\n",
      "Iteration 272, loss = 20534991598.09056854\n",
      "Iteration 273, loss = 20534988127.46954346\n",
      "Iteration 274, loss = 20534984666.87472534\n",
      "Iteration 275, loss = 20534981226.59850693\n",
      "Iteration 276, loss = 20534977733.84909058\n",
      "Iteration 277, loss = 20534974287.93937683\n",
      "Iteration 278, loss = 20534970777.25011063\n",
      "Iteration 279, loss = 20534967314.56424332\n",
      "Iteration 280, loss = 20534963881.52648926\n",
      "Iteration 281, loss = 20534960419.70565033\n",
      "Iteration 282, loss = 20534956967.04674530\n",
      "Iteration 283, loss = 20534953576.84890366\n",
      "Iteration 284, loss = 20534950137.66626740\n",
      "Iteration 285, loss = 20534946700.27272415\n",
      "Iteration 286, loss = 20534943278.71930313\n",
      "Iteration 287, loss = 20534939866.97127533\n",
      "Iteration 288, loss = 20534936430.06254196\n",
      "Iteration 289, loss = 20534933007.24882889\n",
      "Iteration 290, loss = 20534929554.40230942\n",
      "Iteration 291, loss = 20534926162.15463257\n",
      "Iteration 292, loss = 20534922692.64781189\n",
      "Iteration 293, loss = 20534919232.13528442\n",
      "Iteration 294, loss = 20534915792.86338425\n",
      "Iteration 295, loss = 20534912321.95330811\n",
      "Iteration 296, loss = 20534908889.42747879\n",
      "Iteration 297, loss = 20534905409.19564056\n",
      "Iteration 298, loss = 20534901967.99891281\n",
      "Iteration 299, loss = 20534898480.77629471\n",
      "Iteration 300, loss = 20534895005.84433365\n",
      "Iteration 301, loss = 20534891563.19195175\n",
      "Iteration 302, loss = 20534888126.60998917\n",
      "Iteration 303, loss = 20534884620.92790604\n",
      "Iteration 304, loss = 20534881220.07311630\n",
      "Iteration 305, loss = 20534877779.22278976\n",
      "Iteration 306, loss = 20534874312.12231827\n",
      "Iteration 307, loss = 20534870913.82486725\n",
      "Iteration 308, loss = 20534867475.81085968\n",
      "Iteration 309, loss = 20534864031.16110611\n",
      "Iteration 310, loss = 20534860622.95149231\n",
      "Iteration 311, loss = 20534857180.54033661\n",
      "Iteration 312, loss = 20534853752.82746506\n",
      "Iteration 313, loss = 20534850295.29467010\n",
      "Iteration 314, loss = 20534846899.56914520\n",
      "Iteration 315, loss = 20534843429.63724518\n",
      "Iteration 316, loss = 20534839989.49687958\n",
      "Iteration 317, loss = 20534836605.37709045\n",
      "Iteration 318, loss = 20534833152.95804596\n",
      "Iteration 319, loss = 20534829739.63550949\n",
      "Iteration 320, loss = 20534826315.77736282\n",
      "Iteration 321, loss = 20534822875.06623459\n",
      "Iteration 322, loss = 20534819474.07222366\n",
      "Iteration 323, loss = 20534816012.80350876\n",
      "Iteration 324, loss = 20534812632.80953217\n",
      "Iteration 325, loss = 20534809222.17024231\n",
      "Iteration 326, loss = 20534805841.64798737\n",
      "Iteration 327, loss = 20534802419.04877472\n",
      "Iteration 328, loss = 20534798990.24849319\n",
      "Iteration 329, loss = 20534795622.89178467\n",
      "Iteration 330, loss = 20534792187.57534027\n",
      "Iteration 331, loss = 20534788800.65095901\n",
      "Iteration 332, loss = 20534785395.83113861\n",
      "Iteration 333, loss = 20534781987.25108719\n",
      "Iteration 334, loss = 20534778574.59132767\n",
      "Iteration 335, loss = 20534775198.90200806\n",
      "Iteration 336, loss = 20534771748.70763397\n",
      "Iteration 337, loss = 20534768339.01462555\n",
      "Iteration 338, loss = 20534764960.16015244\n",
      "Iteration 339, loss = 20534761529.33669281\n",
      "Iteration 340, loss = 20534758092.33629608\n",
      "Iteration 341, loss = 20534754687.28762817\n",
      "Iteration 342, loss = 20534751283.70979309\n",
      "Iteration 343, loss = 20534747888.05491638\n",
      "Iteration 344, loss = 20534744439.85832214\n",
      "Iteration 345, loss = 20534741030.50159836\n",
      "Iteration 346, loss = 20534737599.86995316\n",
      "Iteration 347, loss = 20534734173.75192261\n",
      "Iteration 348, loss = 20534730745.04099655\n",
      "Iteration 349, loss = 20534727308.76510239\n",
      "Iteration 350, loss = 20534723909.54198456\n",
      "Iteration 351, loss = 20534720466.14914703\n",
      "Iteration 352, loss = 20534717022.16804123\n",
      "Iteration 353, loss = 20534713624.97842026\n",
      "Iteration 354, loss = 20534710220.11380005\n",
      "Iteration 355, loss = 20534706815.38124466\n",
      "Iteration 356, loss = 20534703420.05368423\n",
      "Iteration 357, loss = 20534700025.60127258\n",
      "Iteration 358, loss = 20534696600.80957794\n",
      "Iteration 359, loss = 20534693232.41684341\n",
      "Iteration 360, loss = 20534689776.96954346\n",
      "Iteration 361, loss = 20534686394.10486603\n",
      "Iteration 362, loss = 20534682986.36337662\n",
      "Iteration 363, loss = 20534679590.92223358\n",
      "Iteration 364, loss = 20534676135.31827927\n",
      "Iteration 365, loss = 20534672715.62414932\n",
      "Iteration 366, loss = 20534669287.30582047\n",
      "Iteration 367, loss = 20534665896.92952728\n",
      "Iteration 368, loss = 20534662443.37760925\n",
      "Iteration 369, loss = 20534659070.59786606\n",
      "Iteration 370, loss = 20534655617.16506195\n",
      "Iteration 371, loss = 20534652239.00851822\n",
      "Iteration 372, loss = 20534648833.51075363\n",
      "Iteration 373, loss = 20534645407.49804688\n",
      "Iteration 374, loss = 20534642038.54675293\n",
      "Iteration 375, loss = 20534638620.62203598\n",
      "Iteration 376, loss = 20534635195.95468521\n",
      "Iteration 377, loss = 20534631806.33217621\n",
      "Iteration 378, loss = 20534628410.26126862\n",
      "Iteration 379, loss = 20534625011.42987823\n",
      "Iteration 380, loss = 20534621590.06409454\n",
      "Iteration 381, loss = 20534618195.40742874\n",
      "Iteration 382, loss = 20534614817.21986008\n",
      "Iteration 383, loss = 20534611394.40724182\n",
      "Iteration 384, loss = 20534608034.36196899\n",
      "Iteration 385, loss = 20534604588.60307312\n",
      "Iteration 386, loss = 20534601191.17102432\n",
      "Iteration 387, loss = 20534597812.12583160\n",
      "Iteration 388, loss = 20534594429.12444687\n",
      "Iteration 389, loss = 20534591002.30581665\n",
      "Iteration 390, loss = 20534587636.34973907\n",
      "Iteration 391, loss = 20534584247.67123795\n",
      "Iteration 392, loss = 20534580863.09024811\n",
      "Iteration 393, loss = 20534577502.73364258\n",
      "Iteration 394, loss = 20534574106.83427811\n",
      "Iteration 395, loss = 20534570730.97012329\n",
      "Iteration 396, loss = 20534567353.26309204\n",
      "Iteration 397, loss = 20534564009.55420685\n",
      "Iteration 398, loss = 20534560560.19450760\n",
      "Iteration 399, loss = 20534557222.35064697\n",
      "Iteration 400, loss = 20534553788.13391495\n",
      "Iteration 401, loss = 20534550363.64836502\n",
      "Iteration 402, loss = 20534546952.69132233\n",
      "Iteration 403, loss = 20534543541.58596420\n",
      "Iteration 404, loss = 20534540175.00763321\n",
      "Iteration 405, loss = 20534536770.12581635\n",
      "Iteration 406, loss = 20534533347.56776810\n",
      "Iteration 407, loss = 20534529952.10221481\n",
      "Iteration 408, loss = 20534526554.28447723\n",
      "Iteration 409, loss = 20534523172.71055984\n",
      "Iteration 410, loss = 20534519772.28877640\n",
      "Iteration 411, loss = 20534516365.32426834\n",
      "Iteration 412, loss = 20534512977.53329468\n",
      "Iteration 413, loss = 20534509600.35222244\n",
      "Iteration 414, loss = 20534506210.21095276\n",
      "Iteration 415, loss = 20534502811.47946167\n",
      "Iteration 416, loss = 20534499449.07589722\n",
      "Iteration 417, loss = 20534496043.06087875\n",
      "Iteration 418, loss = 20534492682.44837570\n",
      "Iteration 419, loss = 20534489285.06181717\n",
      "Iteration 420, loss = 20534485895.76383591\n",
      "Iteration 421, loss = 20534482556.65380859\n",
      "Iteration 422, loss = 20534479105.60131454\n",
      "Iteration 423, loss = 20534475758.18563843\n",
      "Iteration 424, loss = 20534472344.69778442\n",
      "Iteration 425, loss = 20534468971.57313919\n",
      "Iteration 426, loss = 20534465567.74764633\n",
      "Iteration 427, loss = 20534462209.95368576\n",
      "Iteration 428, loss = 20534458827.35419464\n",
      "Iteration 429, loss = 20534455455.81585693\n",
      "Iteration 430, loss = 20534452037.20226669\n",
      "Iteration 431, loss = 20534448679.11809540\n",
      "Iteration 432, loss = 20534445322.25459671\n",
      "Iteration 433, loss = 20534441923.06674576\n",
      "Iteration 434, loss = 20534438539.90984726\n",
      "Iteration 435, loss = 20534435148.44393921\n",
      "Iteration 436, loss = 20534431754.34711838\n",
      "Iteration 437, loss = 20534428396.66165161\n",
      "Iteration 438, loss = 20534425009.80405045\n",
      "Iteration 439, loss = 20534421639.12540054\n",
      "Iteration 440, loss = 20534418261.85073090\n",
      "Iteration 441, loss = 20534414900.31497574\n",
      "Iteration 442, loss = 20534411525.10255814\n",
      "Iteration 443, loss = 20534408128.84693527\n",
      "Iteration 444, loss = 20534404760.62595749\n",
      "Iteration 445, loss = 20534401369.37464142\n",
      "Iteration 446, loss = 20534397984.25159836\n",
      "Iteration 447, loss = 20534394626.75843048\n",
      "Iteration 448, loss = 20534391222.17287827\n",
      "Iteration 449, loss = 20534387870.63518143\n",
      "Iteration 450, loss = 20534384436.93728256\n",
      "Iteration 451, loss = 20534381059.20666504\n",
      "Iteration 452, loss = 20534377655.76035690\n",
      "Iteration 453, loss = 20534374262.93545914\n",
      "Iteration 454, loss = 20534370876.92595291\n",
      "Iteration 455, loss = 20534367521.14482498\n",
      "Iteration 456, loss = 20534364122.49646759\n",
      "Iteration 457, loss = 20534360749.83283615\n",
      "Iteration 458, loss = 20534357408.81840134\n",
      "Iteration 459, loss = 20534354032.07012177\n",
      "Iteration 460, loss = 20534350659.50187683\n",
      "Iteration 461, loss = 20534347311.49421692\n",
      "Iteration 462, loss = 20534343921.52624512\n",
      "Iteration 463, loss = 20534340574.40884399\n",
      "Iteration 464, loss = 20534337186.95938873\n",
      "Iteration 465, loss = 20534333845.65107346\n",
      "Iteration 466, loss = 20534330473.31873703\n",
      "Iteration 467, loss = 20534327074.29130554\n",
      "Iteration 468, loss = 20534323739.75199509\n",
      "Iteration 469, loss = 20534320324.84251022\n",
      "Iteration 470, loss = 20534316971.98814011\n",
      "Iteration 471, loss = 20534313599.73138428\n",
      "Iteration 472, loss = 20534310221.37470627\n",
      "Iteration 473, loss = 20534306822.43776321\n",
      "Iteration 474, loss = 20534303439.04608917\n",
      "Iteration 475, loss = 20534300051.65293884\n",
      "Iteration 476, loss = 20534296655.77000809\n",
      "Iteration 477, loss = 20534293237.33334732\n",
      "Iteration 478, loss = 20534289869.79393768\n",
      "Iteration 479, loss = 20534286397.71210861\n",
      "Iteration 480, loss = 20534283010.50243759\n",
      "Iteration 481, loss = 20534279575.46036148\n",
      "Iteration 482, loss = 20534276136.16596603\n",
      "Iteration 483, loss = 20534272690.92227936\n",
      "Iteration 484, loss = 20534269198.54988098\n",
      "Iteration 485, loss = 20534265724.89773560\n",
      "Iteration 486, loss = 20534262179.60204697\n",
      "Iteration 487, loss = 20534258621.25028992\n",
      "Iteration 488, loss = 20534254927.76446152\n",
      "Iteration 489, loss = 20534251271.83030319\n",
      "Iteration 490, loss = 20534247516.19771194\n",
      "Iteration 491, loss = 20534243670.88732147\n",
      "Iteration 492, loss = 20534239756.17342377\n",
      "Iteration 493, loss = 20534235805.52030182\n",
      "Iteration 494, loss = 20534231828.58642197\n",
      "Iteration 495, loss = 20534227774.93020248\n",
      "Iteration 496, loss = 20534223711.40231705\n",
      "Iteration 497, loss = 20534219702.39228439\n",
      "Iteration 498, loss = 20534215606.56604767\n",
      "Iteration 499, loss = 20534211619.08469391\n",
      "Iteration 500, loss = 20534207609.55984497\n",
      "Iteration 1, loss = 20424237596.72030258\n",
      "Iteration 2, loss = 20424227629.68276215\n",
      "Iteration 3, loss = 20424217721.44435120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 20424207639.07934570\n",
      "Iteration 5, loss = 20424197836.74355698\n",
      "Iteration 6, loss = 20424187956.12644577\n",
      "Iteration 7, loss = 20424177852.24545288\n",
      "Iteration 8, loss = 20424167865.75755692\n",
      "Iteration 9, loss = 20424157804.93844223\n",
      "Iteration 10, loss = 20424147731.89659882\n",
      "Iteration 11, loss = 20424137545.48403549\n",
      "Iteration 12, loss = 20424127332.09767151\n",
      "Iteration 13, loss = 20424116966.03123474\n",
      "Iteration 14, loss = 20424106806.49534607\n",
      "Iteration 15, loss = 20424096431.45946121\n",
      "Iteration 16, loss = 20424086133.02346039\n",
      "Iteration 17, loss = 20424075834.74794006\n",
      "Iteration 18, loss = 20424065651.02134323\n",
      "Iteration 19, loss = 20424055117.92479324\n",
      "Iteration 20, loss = 20424044298.73371506\n",
      "Iteration 21, loss = 20424032934.13531494\n",
      "Iteration 22, loss = 20424021043.27559662\n",
      "Iteration 23, loss = 20424008682.36227417\n",
      "Iteration 24, loss = 20423995537.02050018\n",
      "Iteration 25, loss = 20423982143.54685211\n",
      "Iteration 26, loss = 20423968224.88473129\n",
      "Iteration 27, loss = 20423954493.13950729\n",
      "Iteration 28, loss = 20423941285.15748215\n",
      "Iteration 29, loss = 20423929155.58549500\n",
      "Iteration 30, loss = 20423917938.85125351\n",
      "Iteration 31, loss = 20423907115.83090591\n",
      "Iteration 32, loss = 20423896398.44667053\n",
      "Iteration 33, loss = 20423885354.51652908\n",
      "Iteration 34, loss = 20423873950.25040054\n",
      "Iteration 35, loss = 20423862275.95768738\n",
      "Iteration 36, loss = 20423849716.25614166\n",
      "Iteration 37, loss = 20423836418.00035858\n",
      "Iteration 38, loss = 20423822591.90032196\n",
      "Iteration 39, loss = 20423808753.69628906\n",
      "Iteration 40, loss = 20423795285.43235779\n",
      "Iteration 41, loss = 20423782684.96097183\n",
      "Iteration 42, loss = 20423770638.74870300\n",
      "Iteration 43, loss = 20423758161.72479248\n",
      "Iteration 44, loss = 20423745105.57613754\n",
      "Iteration 45, loss = 20423732166.23559952\n",
      "Iteration 46, loss = 20423720259.82645798\n",
      "Iteration 47, loss = 20423709481.22087860\n",
      "Iteration 48, loss = 20423699642.58023453\n",
      "Iteration 49, loss = 20423690177.53998566\n",
      "Iteration 50, loss = 20423680411.86087418\n",
      "Iteration 51, loss = 20423670164.20889664\n",
      "Iteration 52, loss = 20423660368.98591995\n",
      "Iteration 53, loss = 20423650867.84591675\n",
      "Iteration 54, loss = 20423641525.65276337\n",
      "Iteration 55, loss = 20423632014.37927628\n",
      "Iteration 56, loss = 20423622625.50928497\n",
      "Iteration 57, loss = 20423613958.46333694\n",
      "Iteration 58, loss = 20423606135.83658600\n",
      "Iteration 59, loss = 20423599312.99865723\n",
      "Iteration 60, loss = 20423593093.72500229\n",
      "Iteration 61, loss = 20423587165.72690582\n",
      "Iteration 62, loss = 20423581293.18050003\n",
      "Iteration 63, loss = 20423575311.07921219\n",
      "Iteration 64, loss = 20423569513.13117218\n",
      "Iteration 65, loss = 20423564169.54164124\n",
      "Iteration 66, loss = 20423559148.96095657\n",
      "Iteration 67, loss = 20423554364.19450760\n",
      "Iteration 68, loss = 20423549528.58858490\n",
      "Iteration 69, loss = 20423544865.56901169\n",
      "Iteration 70, loss = 20423540196.95661926\n",
      "Iteration 71, loss = 20423535543.82238007\n",
      "Iteration 72, loss = 20423530904.63946152\n",
      "Iteration 73, loss = 20423526214.14159775\n",
      "Iteration 74, loss = 20423521420.79632950\n",
      "Iteration 75, loss = 20423516624.08502197\n",
      "Iteration 76, loss = 20423511930.13517380\n",
      "Iteration 77, loss = 20423507421.95087814\n",
      "Iteration 78, loss = 20423502967.86790085\n",
      "Iteration 79, loss = 20423498437.08039856\n",
      "Iteration 80, loss = 20423493876.05985641\n",
      "Iteration 81, loss = 20423489319.40284348\n",
      "Iteration 82, loss = 20423484917.42085648\n",
      "Iteration 83, loss = 20423480624.14420319\n",
      "Iteration 84, loss = 20423476359.00419617\n",
      "Iteration 85, loss = 20423472225.37770081\n",
      "Iteration 86, loss = 20423468070.78583527\n",
      "Iteration 87, loss = 20423463891.52251053\n",
      "Iteration 88, loss = 20423459812.54816437\n",
      "Iteration 89, loss = 20423455710.00741196\n",
      "Iteration 90, loss = 20423451580.18907166\n",
      "Iteration 91, loss = 20423447557.91709900\n",
      "Iteration 92, loss = 20423443521.95842743\n",
      "Iteration 93, loss = 20423439493.17319489\n",
      "Iteration 94, loss = 20423435459.90972900\n",
      "Iteration 95, loss = 20423431446.25357819\n",
      "Iteration 96, loss = 20423427455.42391205\n",
      "Iteration 97, loss = 20423423469.69828415\n",
      "Iteration 98, loss = 20423419447.27471542\n",
      "Iteration 99, loss = 20423415420.25016785\n",
      "Iteration 100, loss = 20423411269.72841644\n",
      "Iteration 101, loss = 20423407122.31210327\n",
      "Iteration 102, loss = 20423403011.89765549\n",
      "Iteration 103, loss = 20423398983.59449768\n",
      "Iteration 104, loss = 20423394942.28746796\n",
      "Iteration 105, loss = 20423390925.67778015\n",
      "Iteration 106, loss = 20423386955.06986237\n",
      "Iteration 107, loss = 20423383018.81415176\n",
      "Iteration 108, loss = 20423379175.79954910\n",
      "Iteration 109, loss = 20423375243.40184021\n",
      "Iteration 110, loss = 20423371385.63378143\n",
      "Iteration 111, loss = 20423367422.51918793\n",
      "Iteration 112, loss = 20423363466.32368469\n",
      "Iteration 113, loss = 20423359478.46887970\n",
      "Iteration 114, loss = 20423355501.00664520\n",
      "Iteration 115, loss = 20423351600.14863205\n",
      "Iteration 116, loss = 20423347801.56209946\n",
      "Iteration 117, loss = 20423343964.28620529\n",
      "Iteration 118, loss = 20423340150.00650024\n",
      "Iteration 119, loss = 20423336392.67674255\n",
      "Iteration 120, loss = 20423332592.17651367\n",
      "Iteration 121, loss = 20423328821.46987152\n",
      "Iteration 122, loss = 20423325044.96966553\n",
      "Iteration 123, loss = 20423321249.58581924\n",
      "Iteration 124, loss = 20423317550.79054642\n",
      "Iteration 125, loss = 20423313756.74074936\n",
      "Iteration 126, loss = 20423310007.11120224\n",
      "Iteration 127, loss = 20423306298.79533005\n",
      "Iteration 128, loss = 20423302575.07013321\n",
      "Iteration 129, loss = 20423298867.78077316\n",
      "Iteration 130, loss = 20423295119.13031769\n",
      "Iteration 131, loss = 20423291421.99745941\n",
      "Iteration 132, loss = 20423287718.16741562\n",
      "Iteration 133, loss = 20423283989.37376404\n",
      "Iteration 134, loss = 20423280335.52310944\n",
      "Iteration 135, loss = 20423276613.19513702\n",
      "Iteration 136, loss = 20423272972.55706024\n",
      "Iteration 137, loss = 20423269240.87743378\n",
      "Iteration 138, loss = 20423265563.89845657\n",
      "Iteration 139, loss = 20423261937.62069702\n",
      "Iteration 140, loss = 20423258213.70169830\n",
      "Iteration 141, loss = 20423254532.77849579\n",
      "Iteration 142, loss = 20423250914.05234909\n",
      "Iteration 143, loss = 20423247266.24483109\n",
      "Iteration 144, loss = 20423243594.62058258\n",
      "Iteration 145, loss = 20423239971.89349747\n",
      "Iteration 146, loss = 20423236320.64234924\n",
      "Iteration 147, loss = 20423232642.88115692\n",
      "Iteration 148, loss = 20423228996.48583603\n",
      "Iteration 149, loss = 20423225384.35325623\n",
      "Iteration 150, loss = 20423221777.84001541\n",
      "Iteration 151, loss = 20423218134.15562439\n",
      "Iteration 152, loss = 20423214489.38367081\n",
      "Iteration 153, loss = 20423210877.63893890\n",
      "Iteration 154, loss = 20423207245.15935135\n",
      "Iteration 155, loss = 20423203662.45621490\n",
      "Iteration 156, loss = 20423200058.14937973\n",
      "Iteration 157, loss = 20423196419.21643066\n",
      "Iteration 158, loss = 20423192815.87821198\n",
      "Iteration 159, loss = 20423189222.41163635\n",
      "Iteration 160, loss = 20423185593.59442902\n",
      "Iteration 161, loss = 20423181971.22827148\n",
      "Iteration 162, loss = 20423178413.27506256\n",
      "Iteration 163, loss = 20423174823.70652008\n",
      "Iteration 164, loss = 20423171251.67372513\n",
      "Iteration 165, loss = 20423167638.11137390\n",
      "Iteration 166, loss = 20423164039.94153595\n",
      "Iteration 167, loss = 20423160471.87107849\n",
      "Iteration 168, loss = 20423156897.36108017\n",
      "Iteration 169, loss = 20423153320.92620468\n",
      "Iteration 170, loss = 20423149734.91472626\n",
      "Iteration 171, loss = 20423146158.56579590\n",
      "Iteration 172, loss = 20423142576.04628372\n",
      "Iteration 173, loss = 20423139016.48596954\n",
      "Iteration 174, loss = 20423135457.17484665\n",
      "Iteration 175, loss = 20423131887.54367828\n",
      "Iteration 176, loss = 20423128323.31068420\n",
      "Iteration 177, loss = 20423124742.71584320\n",
      "Iteration 178, loss = 20423121207.36952209\n",
      "Iteration 179, loss = 20423117670.54115295\n",
      "Iteration 180, loss = 20423114107.77395248\n",
      "Iteration 181, loss = 20423110530.78863144\n",
      "Iteration 182, loss = 20423107002.36052704\n",
      "Iteration 183, loss = 20423103438.13544083\n",
      "Iteration 184, loss = 20423099946.57358932\n",
      "Iteration 185, loss = 20423096355.71441650\n",
      "Iteration 186, loss = 20423092824.65942764\n",
      "Iteration 187, loss = 20423089271.57874298\n",
      "Iteration 188, loss = 20423085746.46251678\n",
      "Iteration 189, loss = 20423082205.90868759\n",
      "Iteration 190, loss = 20423078695.07502747\n",
      "Iteration 191, loss = 20423075131.61508179\n",
      "Iteration 192, loss = 20423071623.33225250\n",
      "Iteration 193, loss = 20423068085.31541443\n",
      "Iteration 194, loss = 20423064533.67150116\n",
      "Iteration 195, loss = 20423061047.01622772\n",
      "Iteration 196, loss = 20423057516.75915146\n",
      "Iteration 197, loss = 20423053985.13566971\n",
      "Iteration 198, loss = 20423050471.70816040\n",
      "Iteration 199, loss = 20423046931.51965332\n",
      "Iteration 200, loss = 20423043469.76750565\n",
      "Iteration 201, loss = 20423039957.76471710\n",
      "Iteration 202, loss = 20423036392.87619019\n",
      "Iteration 203, loss = 20423032901.28969955\n",
      "Iteration 204, loss = 20423029349.34392548\n",
      "Iteration 205, loss = 20423025875.32300186\n",
      "Iteration 206, loss = 20423022394.53920746\n",
      "Iteration 207, loss = 20423018843.68743134\n",
      "Iteration 208, loss = 20423015330.68587875\n",
      "Iteration 209, loss = 20423011858.04273605\n",
      "Iteration 210, loss = 20423008363.38241577\n",
      "Iteration 211, loss = 20423004861.02366638\n",
      "Iteration 212, loss = 20423001356.11870575\n",
      "Iteration 213, loss = 20422997856.30556870\n",
      "Iteration 214, loss = 20422994358.68337631\n",
      "Iteration 215, loss = 20422990836.23919678\n",
      "Iteration 216, loss = 20422987370.53853989\n",
      "Iteration 217, loss = 20422983890.91545868\n",
      "Iteration 218, loss = 20422980399.90217972\n",
      "Iteration 219, loss = 20422976890.39428329\n",
      "Iteration 220, loss = 20422973412.84688568\n",
      "Iteration 221, loss = 20422969894.14921188\n",
      "Iteration 222, loss = 20422966444.71499252\n",
      "Iteration 223, loss = 20422962957.47119904\n",
      "Iteration 224, loss = 20422959449.95302963\n",
      "Iteration 225, loss = 20422956000.13697815\n",
      "Iteration 226, loss = 20422952463.42495346\n",
      "Iteration 227, loss = 20422949038.91476822\n",
      "Iteration 228, loss = 20422945519.87744522\n",
      "Iteration 229, loss = 20422942073.94412994\n",
      "Iteration 230, loss = 20422938601.59367371\n",
      "Iteration 231, loss = 20422935098.68685913\n",
      "Iteration 232, loss = 20422931628.10306168\n",
      "Iteration 233, loss = 20422928162.99909973\n",
      "Iteration 234, loss = 20422924708.73786163\n",
      "Iteration 235, loss = 20422921208.28044128\n",
      "Iteration 236, loss = 20422917732.85429001\n",
      "Iteration 237, loss = 20422914309.72938538\n",
      "Iteration 238, loss = 20422910804.88946152\n",
      "Iteration 239, loss = 20422907334.36695862\n",
      "Iteration 240, loss = 20422903878.69526672\n",
      "Iteration 241, loss = 20422900408.29086304\n",
      "Iteration 242, loss = 20422896937.44497299\n",
      "Iteration 243, loss = 20422893482.14859772\n",
      "Iteration 244, loss = 20422890042.25069046\n",
      "Iteration 245, loss = 20422886589.67290115\n",
      "Iteration 246, loss = 20422883084.04751587\n",
      "Iteration 247, loss = 20422879635.83246613\n",
      "Iteration 248, loss = 20422876187.50676727\n",
      "Iteration 249, loss = 20422872729.15830231\n",
      "Iteration 250, loss = 20422869261.20112991\n",
      "Iteration 251, loss = 20422865826.57807922\n",
      "Iteration 252, loss = 20422862380.59870148\n",
      "Iteration 253, loss = 20422858911.70117188\n",
      "Iteration 254, loss = 20422855479.34143448\n",
      "Iteration 255, loss = 20422851993.98980713\n",
      "Iteration 256, loss = 20422848577.55701828\n",
      "Iteration 257, loss = 20422845060.90060425\n",
      "Iteration 258, loss = 20422841670.16740799\n",
      "Iteration 259, loss = 20422838194.50207520\n",
      "Iteration 260, loss = 20422834741.75023270\n",
      "Iteration 261, loss = 20422831278.96614456\n",
      "Iteration 262, loss = 20422827782.11806870\n",
      "Iteration 263, loss = 20422824272.81719208\n",
      "Iteration 264, loss = 20422820809.59772491\n",
      "Iteration 265, loss = 20422817356.72129059\n",
      "Iteration 266, loss = 20422813950.15652466\n",
      "Iteration 267, loss = 20422810474.91920471\n",
      "Iteration 268, loss = 20422807029.02367401\n",
      "Iteration 269, loss = 20422803606.50701523\n",
      "Iteration 270, loss = 20422800132.35927963\n",
      "Iteration 271, loss = 20422796695.12483215\n",
      "Iteration 272, loss = 20422793249.77461243\n",
      "Iteration 273, loss = 20422789783.25695801\n",
      "Iteration 274, loss = 20422786369.45693970\n",
      "Iteration 275, loss = 20422782937.65946198\n",
      "Iteration 276, loss = 20422779510.28915405\n",
      "Iteration 277, loss = 20422776050.97222519\n",
      "Iteration 278, loss = 20422772626.97524261\n",
      "Iteration 279, loss = 20422769177.69654846\n",
      "Iteration 280, loss = 20422765749.16421509\n",
      "Iteration 281, loss = 20422762319.82433319\n",
      "Iteration 282, loss = 20422758895.15449524\n",
      "Iteration 283, loss = 20422755426.52062988\n",
      "Iteration 284, loss = 20422752031.89777756\n",
      "Iteration 285, loss = 20422748604.91568756\n",
      "Iteration 286, loss = 20422745126.23427963\n",
      "Iteration 287, loss = 20422741697.77111053\n",
      "Iteration 288, loss = 20422738308.33220673\n",
      "Iteration 289, loss = 20422734895.58412552\n",
      "Iteration 290, loss = 20422731454.93057632\n",
      "Iteration 291, loss = 20422728017.86720657\n",
      "Iteration 292, loss = 20422724562.91689682\n",
      "Iteration 293, loss = 20422721183.52873993\n",
      "Iteration 294, loss = 20422717751.24588394\n",
      "Iteration 295, loss = 20422714332.65036392\n",
      "Iteration 296, loss = 20422710890.17331314\n",
      "Iteration 297, loss = 20422707449.03014755\n",
      "Iteration 298, loss = 20422704058.38592911\n",
      "Iteration 299, loss = 20422700633.63298416\n",
      "Iteration 300, loss = 20422697217.08305359\n",
      "Iteration 301, loss = 20422693793.05912781\n",
      "Iteration 302, loss = 20422690373.16287994\n",
      "Iteration 303, loss = 20422686974.70425797\n",
      "Iteration 304, loss = 20422683568.11773300\n",
      "Iteration 305, loss = 20422680106.24106216\n",
      "Iteration 306, loss = 20422676703.89941406\n",
      "Iteration 307, loss = 20422673280.89011002\n",
      "Iteration 308, loss = 20422669874.08758163\n",
      "Iteration 309, loss = 20422666445.35239792\n",
      "Iteration 310, loss = 20422663069.54701614\n",
      "Iteration 311, loss = 20422659613.74148178\n",
      "Iteration 312, loss = 20422656213.76329803\n",
      "Iteration 313, loss = 20422652846.50986481\n",
      "Iteration 314, loss = 20422649373.63491821\n",
      "Iteration 315, loss = 20422645885.43869400\n",
      "Iteration 316, loss = 20422642507.56909561\n",
      "Iteration 317, loss = 20422639027.72622299\n",
      "Iteration 318, loss = 20422635652.57467270\n",
      "Iteration 319, loss = 20422632209.39846802\n",
      "Iteration 320, loss = 20422628855.50775146\n",
      "Iteration 321, loss = 20422625413.99659729\n",
      "Iteration 322, loss = 20422621998.37823868\n",
      "Iteration 323, loss = 20422618578.09139633\n",
      "Iteration 324, loss = 20422615181.91407776\n",
      "Iteration 325, loss = 20422611779.68826294\n",
      "Iteration 326, loss = 20422608345.36915588\n",
      "Iteration 327, loss = 20422604943.52981567\n",
      "Iteration 328, loss = 20422601563.71315002\n",
      "Iteration 329, loss = 20422598150.54862976\n",
      "Iteration 330, loss = 20422594736.89793015\n",
      "Iteration 331, loss = 20422591338.74133301\n",
      "Iteration 332, loss = 20422587958.45233154\n",
      "Iteration 333, loss = 20422584541.61003113\n",
      "Iteration 334, loss = 20422581113.23308563\n",
      "Iteration 335, loss = 20422577736.72673798\n",
      "Iteration 336, loss = 20422574313.30947113\n",
      "Iteration 337, loss = 20422570930.33233643\n",
      "Iteration 338, loss = 20422567529.26803207\n",
      "Iteration 339, loss = 20422564122.90390396\n",
      "Iteration 340, loss = 20422560730.20951462\n",
      "Iteration 341, loss = 20422557339.25141144\n",
      "Iteration 342, loss = 20422553921.98251724\n",
      "Iteration 343, loss = 20422550544.76623154\n",
      "Iteration 344, loss = 20422547143.21089172\n",
      "Iteration 345, loss = 20422543748.27739716\n",
      "Iteration 346, loss = 20422540347.87192917\n",
      "Iteration 347, loss = 20422536934.32144928\n",
      "Iteration 348, loss = 20422533555.30316544\n",
      "Iteration 349, loss = 20422530158.02429962\n",
      "Iteration 350, loss = 20422526762.79054260\n",
      "Iteration 351, loss = 20422523384.10833740\n",
      "Iteration 352, loss = 20422519990.35134888\n",
      "Iteration 353, loss = 20422516584.72673798\n",
      "Iteration 354, loss = 20422513186.08687973\n",
      "Iteration 355, loss = 20422509800.83096695\n",
      "Iteration 356, loss = 20422506390.62468719\n",
      "Iteration 357, loss = 20422503035.77487946\n",
      "Iteration 358, loss = 20422499615.09222794\n",
      "Iteration 359, loss = 20422496252.89075089\n",
      "Iteration 360, loss = 20422492852.97779083\n",
      "Iteration 361, loss = 20422489450.77606964\n",
      "Iteration 362, loss = 20422486070.58411026\n",
      "Iteration 363, loss = 20422482696.80537796\n",
      "Iteration 364, loss = 20422479289.76653290\n",
      "Iteration 365, loss = 20422475901.79063416\n",
      "Iteration 366, loss = 20422472522.38183594\n",
      "Iteration 367, loss = 20422469136.38434601\n",
      "Iteration 368, loss = 20422465754.47789383\n",
      "Iteration 369, loss = 20422462382.21949768\n",
      "Iteration 370, loss = 20422458975.78002167\n",
      "Iteration 371, loss = 20422455570.58305359\n",
      "Iteration 372, loss = 20422452215.66688538\n",
      "Iteration 373, loss = 20422448815.39097595\n",
      "Iteration 374, loss = 20422445441.96385956\n",
      "Iteration 375, loss = 20422442047.29075241\n",
      "Iteration 376, loss = 20422438670.62850571\n",
      "Iteration 377, loss = 20422435284.56199265\n",
      "Iteration 378, loss = 20422431899.55072021\n",
      "Iteration 379, loss = 20422428534.04827881\n",
      "Iteration 380, loss = 20422425140.32254028\n",
      "Iteration 381, loss = 20422421780.19748688\n",
      "Iteration 382, loss = 20422418399.18089676\n",
      "Iteration 383, loss = 20422415001.37963104\n",
      "Iteration 384, loss = 20422411602.37778473\n",
      "Iteration 385, loss = 20422408235.91306305\n",
      "Iteration 386, loss = 20422404869.31540680\n",
      "Iteration 387, loss = 20422401478.08578873\n",
      "Iteration 388, loss = 20422398114.24453354\n",
      "Iteration 389, loss = 20422394733.37558365\n",
      "Iteration 390, loss = 20422391372.69498444\n",
      "Iteration 391, loss = 20422387959.36380768\n",
      "Iteration 392, loss = 20422384574.19831085\n",
      "Iteration 393, loss = 20422381231.98769379\n",
      "Iteration 394, loss = 20422377829.92517853\n",
      "Iteration 395, loss = 20422374461.14707947\n",
      "Iteration 396, loss = 20422371084.47909164\n",
      "Iteration 397, loss = 20422367710.88139725\n",
      "Iteration 398, loss = 20422364344.55654144\n",
      "Iteration 399, loss = 20422360971.84321594\n",
      "Iteration 400, loss = 20422357587.20213699\n",
      "Iteration 401, loss = 20422354225.20619583\n",
      "Iteration 402, loss = 20422350825.35923386\n",
      "Iteration 403, loss = 20422347454.17279434\n",
      "Iteration 404, loss = 20422344070.01059341\n",
      "Iteration 405, loss = 20422340712.71364594\n",
      "Iteration 406, loss = 20422337325.17131424\n",
      "Iteration 407, loss = 20422333988.08527374\n",
      "Iteration 408, loss = 20422330609.68425369\n",
      "Iteration 409, loss = 20422327245.37294388\n",
      "Iteration 410, loss = 20422323815.64697647\n",
      "Iteration 411, loss = 20422320493.63860321\n",
      "Iteration 412, loss = 20422317089.43821716\n",
      "Iteration 413, loss = 20422313692.81825638\n",
      "Iteration 414, loss = 20422310353.28345490\n",
      "Iteration 415, loss = 20422307004.50407028\n",
      "Iteration 416, loss = 20422303644.68134308\n",
      "Iteration 417, loss = 20422300253.85306931\n",
      "Iteration 418, loss = 20422296853.17460632\n",
      "Iteration 419, loss = 20422293525.53725815\n",
      "Iteration 420, loss = 20422290149.70594025\n",
      "Iteration 421, loss = 20422286781.53276825\n",
      "Iteration 422, loss = 20422283428.25243378\n",
      "Iteration 423, loss = 20422280010.52548599\n",
      "Iteration 424, loss = 20422276710.99187469\n",
      "Iteration 425, loss = 20422273292.02036667\n",
      "Iteration 426, loss = 20422269939.22089005\n",
      "Iteration 427, loss = 20422266578.68840408\n",
      "Iteration 428, loss = 20422263192.01873398\n",
      "Iteration 429, loss = 20422259814.44554520\n",
      "Iteration 430, loss = 20422256448.89825058\n",
      "Iteration 431, loss = 20422253128.11326599\n",
      "Iteration 432, loss = 20422249731.97842407\n",
      "Iteration 433, loss = 20422246367.00391769\n",
      "Iteration 434, loss = 20422243016.43875504\n",
      "Iteration 435, loss = 20422239633.70021820\n",
      "Iteration 436, loss = 20422236291.22888565\n",
      "Iteration 437, loss = 20422232895.82160187\n",
      "Iteration 438, loss = 20422229549.12034607\n",
      "Iteration 439, loss = 20422226208.21303940\n",
      "Iteration 440, loss = 20422222782.55490875\n",
      "Iteration 441, loss = 20422219459.57914734\n",
      "Iteration 442, loss = 20422216093.49445724\n",
      "Iteration 443, loss = 20422212720.03428650\n",
      "Iteration 444, loss = 20422209350.12487793\n",
      "Iteration 445, loss = 20422206006.60427856\n",
      "Iteration 446, loss = 20422202636.12311935\n",
      "Iteration 447, loss = 20422199281.95185852\n",
      "Iteration 448, loss = 20422195896.37397385\n",
      "Iteration 449, loss = 20422192560.85408401\n",
      "Iteration 450, loss = 20422189162.36112595\n",
      "Iteration 451, loss = 20422185812.18047714\n",
      "Iteration 452, loss = 20422182459.06988907\n",
      "Iteration 453, loss = 20422179133.70071411\n",
      "Iteration 454, loss = 20422175756.04311752\n",
      "Iteration 455, loss = 20422172414.01447296\n",
      "Iteration 456, loss = 20422168981.83062744\n",
      "Iteration 457, loss = 20422165653.43900299\n",
      "Iteration 458, loss = 20422162311.08649445\n",
      "Iteration 459, loss = 20422158937.40319824\n",
      "Iteration 460, loss = 20422155574.33394241\n",
      "Iteration 461, loss = 20422152241.26346970\n",
      "Iteration 462, loss = 20422148850.76128006\n",
      "Iteration 463, loss = 20422145502.48043442\n",
      "Iteration 464, loss = 20422142124.63051987\n",
      "Iteration 465, loss = 20422138799.87479782\n",
      "Iteration 466, loss = 20422135442.82565308\n",
      "Iteration 467, loss = 20422132083.47041321\n",
      "Iteration 468, loss = 20422128719.28264236\n",
      "Iteration 469, loss = 20422125346.75959778\n",
      "Iteration 470, loss = 20422122011.85140991\n",
      "Iteration 471, loss = 20422118649.37776184\n",
      "Iteration 472, loss = 20422115286.79639435\n",
      "Iteration 473, loss = 20422111898.19264984\n",
      "Iteration 474, loss = 20422108591.27795029\n",
      "Iteration 475, loss = 20422105223.11076736\n",
      "Iteration 476, loss = 20422101862.19367981\n",
      "Iteration 477, loss = 20422098503.59240723\n",
      "Iteration 478, loss = 20422095157.24399185\n",
      "Iteration 479, loss = 20422091800.48982239\n",
      "Iteration 480, loss = 20422088436.08124542\n",
      "Iteration 481, loss = 20422085091.70105743\n",
      "Iteration 482, loss = 20422081733.45972443\n",
      "Iteration 483, loss = 20422078372.61279297\n",
      "Iteration 484, loss = 20422075042.34884262\n",
      "Iteration 485, loss = 20422071656.54281616\n",
      "Iteration 486, loss = 20422068305.06518936\n",
      "Iteration 487, loss = 20422064956.79114532\n",
      "Iteration 488, loss = 20422061617.68197632\n",
      "Iteration 489, loss = 20422058238.10139084\n",
      "Iteration 490, loss = 20422054906.81100464\n",
      "Iteration 491, loss = 20422051573.09111786\n",
      "Iteration 492, loss = 20422048211.84848404\n",
      "Iteration 493, loss = 20422044840.29899597\n",
      "Iteration 494, loss = 20422041500.70258713\n",
      "Iteration 495, loss = 20422038097.48901749\n",
      "Iteration 496, loss = 20422034777.86457825\n",
      "Iteration 497, loss = 20422031430.92767334\n",
      "Iteration 498, loss = 20422028085.12748337\n",
      "Iteration 499, loss = 20422024724.96283722\n",
      "Iteration 500, loss = 20422021416.47715759\n",
      "Iteration 1, loss = 20078969626.81136322\n",
      "Iteration 2, loss = 20078957204.85225677\n",
      "Iteration 3, loss = 20078944517.46028900\n",
      "Iteration 4, loss = 20078931916.00821304\n",
      "Iteration 5, loss = 20078919212.86320496\n",
      "Iteration 6, loss = 20078906369.69925690\n",
      "Iteration 7, loss = 20078893792.41946411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 20078881559.80739212\n",
      "Iteration 9, loss = 20078869391.06755066\n",
      "Iteration 10, loss = 20078857108.24632263\n",
      "Iteration 11, loss = 20078844900.28237534\n",
      "Iteration 12, loss = 20078831874.53274918\n",
      "Iteration 13, loss = 20078817979.40587616\n",
      "Iteration 14, loss = 20078803356.30852127\n",
      "Iteration 15, loss = 20078788637.40092850\n",
      "Iteration 16, loss = 20078774650.13951492\n",
      "Iteration 17, loss = 20078761279.26527786\n",
      "Iteration 18, loss = 20078747626.16677094\n",
      "Iteration 19, loss = 20078733229.03605652\n",
      "Iteration 20, loss = 20078718680.46402359\n",
      "Iteration 21, loss = 20078705056.88214111\n",
      "Iteration 22, loss = 20078692085.71418381\n",
      "Iteration 23, loss = 20078679116.45850372\n",
      "Iteration 24, loss = 20078665548.26457977\n",
      "Iteration 25, loss = 20078652057.74121857\n",
      "Iteration 26, loss = 20078639922.41952896\n",
      "Iteration 27, loss = 20078627871.63809967\n",
      "Iteration 28, loss = 20078615678.42396164\n",
      "Iteration 29, loss = 20078602500.90915298\n",
      "Iteration 30, loss = 20078587469.99647141\n",
      "Iteration 31, loss = 20078570801.23615646\n",
      "Iteration 32, loss = 20078555388.41640472\n",
      "Iteration 33, loss = 20078542004.10639954\n",
      "Iteration 34, loss = 20078529075.85987854\n",
      "Iteration 35, loss = 20078516238.36412811\n",
      "Iteration 36, loss = 20078504076.75310898\n",
      "Iteration 37, loss = 20078493228.29244232\n",
      "Iteration 38, loss = 20078483320.16495514\n",
      "Iteration 39, loss = 20078473714.78209686\n",
      "Iteration 40, loss = 20078464758.23510361\n",
      "Iteration 41, loss = 20078456815.12793350\n",
      "Iteration 42, loss = 20078449442.26817703\n",
      "Iteration 43, loss = 20078442201.78708267\n",
      "Iteration 44, loss = 20078435125.01691818\n",
      "Iteration 45, loss = 20078428729.56492996\n",
      "Iteration 46, loss = 20078422779.89458847\n",
      "Iteration 47, loss = 20078417198.38786316\n",
      "Iteration 48, loss = 20078411970.43175888\n",
      "Iteration 49, loss = 20078406751.84806442\n",
      "Iteration 50, loss = 20078401579.81726456\n",
      "Iteration 51, loss = 20078396497.15547180\n",
      "Iteration 52, loss = 20078391539.73943710\n",
      "Iteration 53, loss = 20078386726.78220749\n",
      "Iteration 54, loss = 20078381978.80651855\n",
      "Iteration 55, loss = 20078377210.60100555\n",
      "Iteration 56, loss = 20078372524.73847580\n",
      "Iteration 57, loss = 20078367890.61322021\n",
      "Iteration 58, loss = 20078363274.65799713\n",
      "Iteration 59, loss = 20078358726.20238495\n",
      "Iteration 60, loss = 20078354170.53155899\n",
      "Iteration 61, loss = 20078349695.26737976\n",
      "Iteration 62, loss = 20078345171.63874054\n",
      "Iteration 63, loss = 20078340726.52978897\n",
      "Iteration 64, loss = 20078336282.08389282\n",
      "Iteration 65, loss = 20078331873.18178940\n",
      "Iteration 66, loss = 20078327464.46699524\n",
      "Iteration 67, loss = 20078323075.53475571\n",
      "Iteration 68, loss = 20078318741.13743973\n",
      "Iteration 69, loss = 20078314401.37080002\n",
      "Iteration 70, loss = 20078310038.80098724\n",
      "Iteration 71, loss = 20078305729.65356064\n",
      "Iteration 72, loss = 20078301390.27120590\n",
      "Iteration 73, loss = 20078297130.06718063\n",
      "Iteration 74, loss = 20078292831.11767960\n",
      "Iteration 75, loss = 20078288552.82156754\n",
      "Iteration 76, loss = 20078284340.61303711\n",
      "Iteration 77, loss = 20078280093.47684479\n",
      "Iteration 78, loss = 20078275866.60391617\n",
      "Iteration 79, loss = 20078271666.07439804\n",
      "Iteration 80, loss = 20078267469.46626282\n",
      "Iteration 81, loss = 20078263282.49775696\n",
      "Iteration 82, loss = 20078259063.96596146\n",
      "Iteration 83, loss = 20078254913.87715530\n",
      "Iteration 84, loss = 20078250774.34822083\n",
      "Iteration 85, loss = 20078246605.78999329\n",
      "Iteration 86, loss = 20078242493.37275696\n",
      "Iteration 87, loss = 20078238345.02126694\n",
      "Iteration 88, loss = 20078234218.56157303\n",
      "Iteration 89, loss = 20078230088.40945816\n",
      "Iteration 90, loss = 20078225999.98074722\n",
      "Iteration 91, loss = 20078221870.68335724\n",
      "Iteration 92, loss = 20078217762.39020538\n",
      "Iteration 93, loss = 20078213665.93571472\n",
      "Iteration 94, loss = 20078209577.08396912\n",
      "Iteration 95, loss = 20078205473.92299652\n",
      "Iteration 96, loss = 20078201382.85505676\n",
      "Iteration 97, loss = 20078197312.99578857\n",
      "Iteration 98, loss = 20078193244.59330750\n",
      "Iteration 99, loss = 20078189170.69844437\n",
      "Iteration 100, loss = 20078185120.54754639\n",
      "Iteration 101, loss = 20078181082.12500381\n",
      "Iteration 102, loss = 20078176982.19516754\n",
      "Iteration 103, loss = 20078172935.61640930\n",
      "Iteration 104, loss = 20078168829.98381042\n",
      "Iteration 105, loss = 20078164674.56847000\n",
      "Iteration 106, loss = 20078160533.84546280\n",
      "Iteration 107, loss = 20078156518.85358429\n",
      "Iteration 108, loss = 20078152513.20348358\n",
      "Iteration 109, loss = 20078148443.49808502\n",
      "Iteration 110, loss = 20078144438.76625443\n",
      "Iteration 111, loss = 20078140427.52947235\n",
      "Iteration 112, loss = 20078136403.91578293\n",
      "Iteration 113, loss = 20078132352.70147324\n",
      "Iteration 114, loss = 20078128370.25844193\n",
      "Iteration 115, loss = 20078124335.05835342\n",
      "Iteration 116, loss = 20078120350.24614334\n",
      "Iteration 117, loss = 20078116314.86012650\n",
      "Iteration 118, loss = 20078112290.17369461\n",
      "Iteration 119, loss = 20078108220.91548920\n",
      "Iteration 120, loss = 20078104175.70896912\n",
      "Iteration 121, loss = 20078100117.23986435\n",
      "Iteration 122, loss = 20078095997.04028320\n",
      "Iteration 123, loss = 20078091825.19815063\n",
      "Iteration 124, loss = 20078087675.20997620\n",
      "Iteration 125, loss = 20078083440.32850266\n",
      "Iteration 126, loss = 20078079204.94268417\n",
      "Iteration 127, loss = 20078074913.32656860\n",
      "Iteration 128, loss = 20078070541.05968857\n",
      "Iteration 129, loss = 20078066217.46400452\n",
      "Iteration 130, loss = 20078061863.11724091\n",
      "Iteration 131, loss = 20078057409.23990250\n",
      "Iteration 132, loss = 20078053011.67894363\n",
      "Iteration 133, loss = 20078048650.34616852\n",
      "Iteration 134, loss = 20078044270.16818237\n",
      "Iteration 135, loss = 20078039883.79199600\n",
      "Iteration 136, loss = 20078035528.48774719\n",
      "Iteration 137, loss = 20078031183.51851273\n",
      "Iteration 138, loss = 20078026816.83834457\n",
      "Iteration 139, loss = 20078022512.47512436\n",
      "Iteration 140, loss = 20078018204.60989761\n",
      "Iteration 141, loss = 20078013869.92972565\n",
      "Iteration 142, loss = 20078009567.95981216\n",
      "Iteration 143, loss = 20078005294.09479141\n",
      "Iteration 144, loss = 20078000972.60921478\n",
      "Iteration 145, loss = 20077996688.52026367\n",
      "Iteration 146, loss = 20077992308.29847336\n",
      "Iteration 147, loss = 20077987974.36210251\n",
      "Iteration 148, loss = 20077983693.37796021\n",
      "Iteration 149, loss = 20077979412.73768234\n",
      "Iteration 150, loss = 20077975147.08144760\n",
      "Iteration 151, loss = 20077970899.08125305\n",
      "Iteration 152, loss = 20077966568.47452164\n",
      "Iteration 153, loss = 20077962289.44609833\n",
      "Iteration 154, loss = 20077957952.64860535\n",
      "Iteration 155, loss = 20077953611.37195969\n",
      "Iteration 156, loss = 20077949275.63091278\n",
      "Iteration 157, loss = 20077944826.91191101\n",
      "Iteration 158, loss = 20077940306.90035248\n",
      "Iteration 159, loss = 20077935751.73708725\n",
      "Iteration 160, loss = 20077931143.19573212\n",
      "Iteration 161, loss = 20077926376.27083588\n",
      "Iteration 162, loss = 20077921557.47511292\n",
      "Iteration 163, loss = 20077916600.04860306\n",
      "Iteration 164, loss = 20077911566.59687042\n",
      "Iteration 165, loss = 20077906392.73592758\n",
      "Iteration 166, loss = 20077901147.51813507\n",
      "Iteration 167, loss = 20077895886.91660309\n",
      "Iteration 168, loss = 20077890516.86331558\n",
      "Iteration 169, loss = 20077885163.70088577\n",
      "Iteration 170, loss = 20077879836.77382660\n",
      "Iteration 171, loss = 20077874552.60070038\n",
      "Iteration 172, loss = 20077869263.65884018\n",
      "Iteration 173, loss = 20077864007.40576553\n",
      "Iteration 174, loss = 20077858804.98057938\n",
      "Iteration 175, loss = 20077853673.15499878\n",
      "Iteration 176, loss = 20077848587.15119934\n",
      "Iteration 177, loss = 20077843509.68385696\n",
      "Iteration 178, loss = 20077838445.25127411\n",
      "Iteration 179, loss = 20077833448.64017487\n",
      "Iteration 180, loss = 20077828485.44750977\n",
      "Iteration 181, loss = 20077823423.16500092\n",
      "Iteration 182, loss = 20077818475.68646622\n",
      "Iteration 183, loss = 20077813573.85212708\n",
      "Iteration 184, loss = 20077808683.46245956\n",
      "Iteration 185, loss = 20077803876.53099823\n",
      "Iteration 186, loss = 20077799029.03723907\n",
      "Iteration 187, loss = 20077794229.63500595\n",
      "Iteration 188, loss = 20077789485.08589554\n",
      "Iteration 189, loss = 20077784683.80137634\n",
      "Iteration 190, loss = 20077779925.87565231\n",
      "Iteration 191, loss = 20077775186.78007507\n",
      "Iteration 192, loss = 20077770402.66981506\n",
      "Iteration 193, loss = 20077765667.27933884\n",
      "Iteration 194, loss = 20077760927.28353119\n",
      "Iteration 195, loss = 20077756255.24792862\n",
      "Iteration 196, loss = 20077751606.41053009\n",
      "Iteration 197, loss = 20077746942.94430923\n",
      "Iteration 198, loss = 20077742270.04977417\n",
      "Iteration 199, loss = 20077737668.97407150\n",
      "Iteration 200, loss = 20077733039.53251266\n",
      "Iteration 201, loss = 20077728433.23627090\n",
      "Iteration 202, loss = 20077723813.64876556\n",
      "Iteration 203, loss = 20077719239.25923920\n",
      "Iteration 204, loss = 20077714660.16965485\n",
      "Iteration 205, loss = 20077710071.09467697\n",
      "Iteration 206, loss = 20077705476.38416672\n",
      "Iteration 207, loss = 20077700939.69449234\n",
      "Iteration 208, loss = 20077696394.35292435\n",
      "Iteration 209, loss = 20077691851.30326080\n",
      "Iteration 210, loss = 20077687272.92748642\n",
      "Iteration 211, loss = 20077682777.11301422\n",
      "Iteration 212, loss = 20077678200.04946518\n",
      "Iteration 213, loss = 20077673646.71809006\n",
      "Iteration 214, loss = 20077669126.68123627\n",
      "Iteration 215, loss = 20077664616.08651733\n",
      "Iteration 216, loss = 20077660072.33996201\n",
      "Iteration 217, loss = 20077655574.74746704\n",
      "Iteration 218, loss = 20077651077.39478683\n",
      "Iteration 219, loss = 20077646551.36722183\n",
      "Iteration 220, loss = 20077642082.52486038\n",
      "Iteration 221, loss = 20077637564.95339584\n",
      "Iteration 222, loss = 20077633120.48186493\n",
      "Iteration 223, loss = 20077628652.99481583\n",
      "Iteration 224, loss = 20077624179.40020752\n",
      "Iteration 225, loss = 20077619656.89733124\n",
      "Iteration 226, loss = 20077615255.89996719\n",
      "Iteration 227, loss = 20077610748.89730072\n",
      "Iteration 228, loss = 20077606314.39172745\n",
      "Iteration 229, loss = 20077601837.63222122\n",
      "Iteration 230, loss = 20077597408.62347794\n",
      "Iteration 231, loss = 20077592988.30526352\n",
      "Iteration 232, loss = 20077588477.97608185\n",
      "Iteration 233, loss = 20077583988.94324112\n",
      "Iteration 234, loss = 20077579563.67813110\n",
      "Iteration 235, loss = 20077575141.02339172\n",
      "Iteration 236, loss = 20077570726.28116608\n",
      "Iteration 237, loss = 20077566324.71286011\n",
      "Iteration 238, loss = 20077561878.61619568\n",
      "Iteration 239, loss = 20077557479.29640579\n",
      "Iteration 240, loss = 20077553089.69237900\n",
      "Iteration 241, loss = 20077548691.80471039\n",
      "Iteration 242, loss = 20077544295.73715591\n",
      "Iteration 243, loss = 20077539959.90118790\n",
      "Iteration 244, loss = 20077535534.95746231\n",
      "Iteration 245, loss = 20077531160.18194199\n",
      "Iteration 246, loss = 20077526814.04522324\n",
      "Iteration 247, loss = 20077522436.76285553\n",
      "Iteration 248, loss = 20077518091.00180817\n",
      "Iteration 249, loss = 20077513719.47814178\n",
      "Iteration 250, loss = 20077509385.67147827\n",
      "Iteration 251, loss = 20077505002.21623993\n",
      "Iteration 252, loss = 20077500647.96154785\n",
      "Iteration 253, loss = 20077496296.69781494\n",
      "Iteration 254, loss = 20077491935.97342300\n",
      "Iteration 255, loss = 20077487583.76256180\n",
      "Iteration 256, loss = 20077483206.33569717\n",
      "Iteration 257, loss = 20077478835.06596375\n",
      "Iteration 258, loss = 20077474522.74281693\n",
      "Iteration 259, loss = 20077470175.42191696\n",
      "Iteration 260, loss = 20077465760.30274963\n",
      "Iteration 261, loss = 20077461482.41716003\n",
      "Iteration 262, loss = 20077457116.23885345\n",
      "Iteration 263, loss = 20077452764.47568512\n",
      "Iteration 264, loss = 20077448459.81304550\n",
      "Iteration 265, loss = 20077444121.75653076\n",
      "Iteration 266, loss = 20077439764.22541428\n",
      "Iteration 267, loss = 20077435417.76636505\n",
      "Iteration 268, loss = 20077431092.15145874\n",
      "Iteration 269, loss = 20077426776.14185333\n",
      "Iteration 270, loss = 20077422463.50767136\n",
      "Iteration 271, loss = 20077418113.27719498\n",
      "Iteration 272, loss = 20077413803.32604599\n",
      "Iteration 273, loss = 20077409469.07848358\n",
      "Iteration 274, loss = 20077405113.37127686\n",
      "Iteration 275, loss = 20077400788.56023407\n",
      "Iteration 276, loss = 20077396512.48586655\n",
      "Iteration 277, loss = 20077392159.23479843\n",
      "Iteration 278, loss = 20077387846.69434357\n",
      "Iteration 279, loss = 20077383523.23760986\n",
      "Iteration 280, loss = 20077379217.24890137\n",
      "Iteration 281, loss = 20077374894.52994537\n",
      "Iteration 282, loss = 20077370592.84449768\n",
      "Iteration 283, loss = 20077366275.35642624\n",
      "Iteration 284, loss = 20077362021.75837708\n",
      "Iteration 285, loss = 20077357673.36421967\n",
      "Iteration 286, loss = 20077353356.35786819\n",
      "Iteration 287, loss = 20077349092.29029083\n",
      "Iteration 288, loss = 20077344788.31266022\n",
      "Iteration 289, loss = 20077340486.46931458\n",
      "Iteration 290, loss = 20077336177.62600708\n",
      "Iteration 291, loss = 20077331901.84551239\n",
      "Iteration 292, loss = 20077327605.65992355\n",
      "Iteration 293, loss = 20077323325.46695709\n",
      "Iteration 294, loss = 20077319076.50538635\n",
      "Iteration 295, loss = 20077314794.10449600\n",
      "Iteration 296, loss = 20077310495.41474152\n",
      "Iteration 297, loss = 20077306252.21559143\n",
      "Iteration 298, loss = 20077302022.36315155\n",
      "Iteration 299, loss = 20077297709.82135773\n",
      "Iteration 300, loss = 20077293440.91912842\n",
      "Iteration 301, loss = 20077289178.75687027\n",
      "Iteration 302, loss = 20077284931.45250320\n",
      "Iteration 303, loss = 20077280664.81117630\n",
      "Iteration 304, loss = 20077276435.64038467\n",
      "Iteration 305, loss = 20077272151.74944305\n",
      "Iteration 306, loss = 20077267870.86960220\n",
      "Iteration 307, loss = 20077263614.57094193\n",
      "Iteration 308, loss = 20077259385.33723450\n",
      "Iteration 309, loss = 20077255138.97912979\n",
      "Iteration 310, loss = 20077250850.88431931\n",
      "Iteration 311, loss = 20077246651.65199280\n",
      "Iteration 312, loss = 20077242369.26560593\n",
      "Iteration 313, loss = 20077238092.01956940\n",
      "Iteration 314, loss = 20077233854.47367859\n",
      "Iteration 315, loss = 20077229605.01234818\n",
      "Iteration 316, loss = 20077225306.83288193\n",
      "Iteration 317, loss = 20077221076.04695129\n",
      "Iteration 318, loss = 20077216845.71182632\n",
      "Iteration 319, loss = 20077212593.48897171\n",
      "Iteration 320, loss = 20077208327.65231705\n",
      "Iteration 321, loss = 20077204114.84386063\n",
      "Iteration 322, loss = 20077199860.08639908\n",
      "Iteration 323, loss = 20077195638.23545837\n",
      "Iteration 324, loss = 20077191411.63232040\n",
      "Iteration 325, loss = 20077187172.10940170\n",
      "Iteration 326, loss = 20077182975.15516663\n",
      "Iteration 327, loss = 20077178728.11196518\n",
      "Iteration 328, loss = 20077174497.27781677\n",
      "Iteration 329, loss = 20077170241.32728958\n",
      "Iteration 330, loss = 20077166044.91294479\n",
      "Iteration 331, loss = 20077161793.81660843\n",
      "Iteration 332, loss = 20077157583.22616196\n",
      "Iteration 333, loss = 20077153398.68740463\n",
      "Iteration 334, loss = 20077149194.92039490\n",
      "Iteration 335, loss = 20077144950.88977432\n",
      "Iteration 336, loss = 20077140731.41193008\n",
      "Iteration 337, loss = 20077136487.81007385\n",
      "Iteration 338, loss = 20077132302.50513077\n",
      "Iteration 339, loss = 20077128060.90477371\n",
      "Iteration 340, loss = 20077123837.85550308\n",
      "Iteration 341, loss = 20077119587.42812347\n",
      "Iteration 342, loss = 20077115382.98748398\n",
      "Iteration 343, loss = 20077111135.57709122\n",
      "Iteration 344, loss = 20077106883.17651367\n",
      "Iteration 345, loss = 20077102690.34382629\n",
      "Iteration 346, loss = 20077098468.64728928\n",
      "Iteration 347, loss = 20077094264.88476944\n",
      "Iteration 348, loss = 20077090044.91475677\n",
      "Iteration 349, loss = 20077085836.14448547\n",
      "Iteration 350, loss = 20077081622.02152634\n",
      "Iteration 351, loss = 20077077447.53887177\n",
      "Iteration 352, loss = 20077073249.39189529\n",
      "Iteration 353, loss = 20077069109.38380432\n",
      "Iteration 354, loss = 20077064870.67524719\n",
      "Iteration 355, loss = 20077060666.53574371\n",
      "Iteration 356, loss = 20077056458.12468338\n",
      "Iteration 357, loss = 20077052266.49654388\n",
      "Iteration 358, loss = 20077048066.74627304\n",
      "Iteration 359, loss = 20077043847.32659149\n",
      "Iteration 360, loss = 20077039640.62505722\n",
      "Iteration 361, loss = 20077035452.50656128\n",
      "Iteration 362, loss = 20077031214.56146240\n",
      "Iteration 363, loss = 20077027034.72885895\n",
      "Iteration 364, loss = 20077022790.01780319\n",
      "Iteration 365, loss = 20077018642.87331390\n",
      "Iteration 366, loss = 20077014385.31425476\n",
      "Iteration 367, loss = 20077010182.38040543\n",
      "Iteration 368, loss = 20077005967.48254013\n",
      "Iteration 369, loss = 20077001765.93004608\n",
      "Iteration 370, loss = 20076997538.11391830\n",
      "Iteration 371, loss = 20076993338.59257889\n",
      "Iteration 372, loss = 20076989089.83791733\n",
      "Iteration 373, loss = 20076984912.49505997\n",
      "Iteration 374, loss = 20076980702.93093872\n",
      "Iteration 375, loss = 20076976462.38559723\n",
      "Iteration 376, loss = 20076972306.44733810\n",
      "Iteration 377, loss = 20076968081.67212296\n",
      "Iteration 378, loss = 20076963850.43322754\n",
      "Iteration 379, loss = 20076959652.22267151\n",
      "Iteration 380, loss = 20076955455.17863846\n",
      "Iteration 381, loss = 20076951267.63492966\n",
      "Iteration 382, loss = 20076947087.75505066\n",
      "Iteration 383, loss = 20076942910.36930847\n",
      "Iteration 384, loss = 20076938716.73891830\n",
      "Iteration 385, loss = 20076934539.60390472\n",
      "Iteration 386, loss = 20076930336.75360870\n",
      "Iteration 387, loss = 20076926165.85860062\n",
      "Iteration 388, loss = 20076921991.86240387\n",
      "Iteration 389, loss = 20076917781.81970596\n",
      "Iteration 390, loss = 20076913597.45603943\n",
      "Iteration 391, loss = 20076909427.26606369\n",
      "Iteration 392, loss = 20076905221.55855560\n",
      "Iteration 393, loss = 20076901059.37133026\n",
      "Iteration 394, loss = 20076896851.18637466\n",
      "Iteration 395, loss = 20076892664.33252335\n",
      "Iteration 396, loss = 20076888475.17655563\n",
      "Iteration 397, loss = 20076884286.41000748\n",
      "Iteration 398, loss = 20076880141.01046371\n",
      "Iteration 399, loss = 20076875924.94801331\n",
      "Iteration 400, loss = 20076871764.77508926\n",
      "Iteration 401, loss = 20076867540.81861496\n",
      "Iteration 402, loss = 20076863354.10821915\n",
      "Iteration 403, loss = 20076859196.61652756\n",
      "Iteration 404, loss = 20076855008.90773010\n",
      "Iteration 405, loss = 20076850816.63204956\n",
      "Iteration 406, loss = 20076846658.22044754\n",
      "Iteration 407, loss = 20076842493.71641541\n",
      "Iteration 408, loss = 20076838299.24313736\n",
      "Iteration 409, loss = 20076834133.31682968\n",
      "Iteration 410, loss = 20076829994.71666336\n",
      "Iteration 411, loss = 20076825793.07628632\n",
      "Iteration 412, loss = 20076821595.66796494\n",
      "Iteration 413, loss = 20076817407.60750961\n",
      "Iteration 414, loss = 20076813264.17392731\n",
      "Iteration 415, loss = 20076809087.16288376\n",
      "Iteration 416, loss = 20076804890.49992752\n",
      "Iteration 417, loss = 20076800677.28351974\n",
      "Iteration 418, loss = 20076796517.54830170\n",
      "Iteration 419, loss = 20076792360.27069855\n",
      "Iteration 420, loss = 20076788129.68759918\n",
      "Iteration 421, loss = 20076784011.90221405\n",
      "Iteration 422, loss = 20076779796.52367020\n",
      "Iteration 423, loss = 20076775590.88485336\n",
      "Iteration 424, loss = 20076771430.08292007\n",
      "Iteration 425, loss = 20076767241.40542603\n",
      "Iteration 426, loss = 20076763050.13587952\n",
      "Iteration 427, loss = 20076758862.68876648\n",
      "Iteration 428, loss = 20076754688.90148926\n",
      "Iteration 429, loss = 20076750545.42877197\n",
      "Iteration 430, loss = 20076746330.16855240\n",
      "Iteration 431, loss = 20076742199.93066025\n",
      "Iteration 432, loss = 20076738023.37189102\n",
      "Iteration 433, loss = 20076733842.67259598\n",
      "Iteration 434, loss = 20076729717.64537048\n",
      "Iteration 435, loss = 20076725545.92514801\n",
      "Iteration 436, loss = 20076721393.75367355\n",
      "Iteration 437, loss = 20076717224.19128799\n",
      "Iteration 438, loss = 20076713083.02013779\n",
      "Iteration 439, loss = 20076708890.79991531\n",
      "Iteration 440, loss = 20076704724.92454147\n",
      "Iteration 441, loss = 20076700546.34883499\n",
      "Iteration 442, loss = 20076696383.51060867\n",
      "Iteration 443, loss = 20076692235.65709686\n",
      "Iteration 444, loss = 20076688073.88993454\n",
      "Iteration 445, loss = 20076683912.63999939\n",
      "Iteration 446, loss = 20076679733.63890457\n",
      "Iteration 447, loss = 20076675572.57001495\n",
      "Iteration 448, loss = 20076671417.57562637\n",
      "Iteration 449, loss = 20076667230.14971161\n",
      "Iteration 450, loss = 20076663074.04818344\n",
      "Iteration 451, loss = 20076658883.53987885\n",
      "Iteration 452, loss = 20076654790.11648560\n",
      "Iteration 453, loss = 20076650593.64333725\n",
      "Iteration 454, loss = 20076646426.07325363\n",
      "Iteration 455, loss = 20076642258.70306778\n",
      "Iteration 456, loss = 20076638066.11903000\n",
      "Iteration 457, loss = 20076633967.50122833\n",
      "Iteration 458, loss = 20076629757.30081940\n",
      "Iteration 459, loss = 20076625577.27943802\n",
      "Iteration 460, loss = 20076621432.21895218\n",
      "Iteration 461, loss = 20076617253.93378448\n",
      "Iteration 462, loss = 20076613045.81498718\n",
      "Iteration 463, loss = 20076608893.02983856\n",
      "Iteration 464, loss = 20076604741.40651321\n",
      "Iteration 465, loss = 20076600546.77182388\n",
      "Iteration 466, loss = 20076596358.92954636\n",
      "Iteration 467, loss = 20076592247.67053604\n",
      "Iteration 468, loss = 20076588068.02115631\n",
      "Iteration 469, loss = 20076583916.64907074\n",
      "Iteration 470, loss = 20076579783.03775024\n",
      "Iteration 471, loss = 20076575626.44440460\n",
      "Iteration 472, loss = 20076571486.07220840\n",
      "Iteration 473, loss = 20076567328.45148087\n",
      "Iteration 474, loss = 20076563180.85781479\n",
      "Iteration 475, loss = 20076559040.83918762\n",
      "Iteration 476, loss = 20076554910.19244003\n",
      "Iteration 477, loss = 20076550706.44696808\n",
      "Iteration 478, loss = 20076546594.94421768\n",
      "Iteration 479, loss = 20076542438.06466675\n",
      "Iteration 480, loss = 20076538298.56650162\n",
      "Iteration 481, loss = 20076534124.28239822\n",
      "Iteration 482, loss = 20076529948.39976120\n",
      "Iteration 483, loss = 20076525833.18249893\n",
      "Iteration 484, loss = 20076521673.34309006\n",
      "Iteration 485, loss = 20076517526.29277039\n",
      "Iteration 486, loss = 20076513374.27075577\n",
      "Iteration 487, loss = 20076509287.78520203\n",
      "Iteration 488, loss = 20076505136.01670074\n",
      "Iteration 489, loss = 20076501009.08536148\n",
      "Iteration 490, loss = 20076496903.55144501\n",
      "Iteration 491, loss = 20076492701.72375107\n",
      "Iteration 492, loss = 20076488602.03238678\n",
      "Iteration 493, loss = 20076484487.80380630\n",
      "Iteration 494, loss = 20076480331.57778931\n",
      "Iteration 495, loss = 20076476167.96862030\n",
      "Iteration 496, loss = 20076472062.37909698\n",
      "Iteration 497, loss = 20076467879.38716888\n",
      "Iteration 498, loss = 20076463777.32113647\n",
      "Iteration 499, loss = 20076459629.94542313\n",
      "Iteration 500, loss = 20076455478.93864059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19680465254.68727875\n",
      "Iteration 2, loss = 19680450451.87554932\n",
      "Iteration 3, loss = 19680435453.99296188\n",
      "Iteration 4, loss = 19680420098.38835526\n",
      "Iteration 5, loss = 19680404273.52161407\n",
      "Iteration 6, loss = 19680388186.40615845\n",
      "Iteration 7, loss = 19680372570.43691635\n",
      "Iteration 8, loss = 19680357275.28140640\n",
      "Iteration 9, loss = 19680342504.15334702\n",
      "Iteration 10, loss = 19680327537.54987335\n",
      "Iteration 11, loss = 19680311833.85990524\n",
      "Iteration 12, loss = 19680295259.87276077\n",
      "Iteration 13, loss = 19680278555.27003860\n",
      "Iteration 14, loss = 19680262434.06859589\n",
      "Iteration 15, loss = 19680247182.47871399\n",
      "Iteration 16, loss = 19680230521.17057800\n",
      "Iteration 17, loss = 19680211744.56276321\n",
      "Iteration 18, loss = 19680192702.66510391\n",
      "Iteration 19, loss = 19680174677.34383774\n",
      "Iteration 20, loss = 19680156735.15647125\n",
      "Iteration 21, loss = 19680136459.88061905\n",
      "Iteration 22, loss = 19680114134.25431442\n",
      "Iteration 23, loss = 19680093993.07303619\n",
      "Iteration 24, loss = 19680075425.57659149\n",
      "Iteration 25, loss = 19680057656.57819748\n",
      "Iteration 26, loss = 19680040095.13968277\n",
      "Iteration 27, loss = 19680021736.26618958\n",
      "Iteration 28, loss = 19680001711.75733948\n",
      "Iteration 29, loss = 19679981775.16982651\n",
      "Iteration 30, loss = 19679962752.42805481\n",
      "Iteration 31, loss = 19679944934.52329636\n",
      "Iteration 32, loss = 19679930148.05094528\n",
      "Iteration 33, loss = 19679917221.40686035\n",
      "Iteration 34, loss = 19679904043.70939255\n",
      "Iteration 35, loss = 19679890085.39119720\n",
      "Iteration 36, loss = 19679877363.51105499\n",
      "Iteration 37, loss = 19679865052.89077759\n",
      "Iteration 38, loss = 19679854871.80490112\n",
      "Iteration 39, loss = 19679845852.37216187\n",
      "Iteration 40, loss = 19679836983.38646698\n",
      "Iteration 41, loss = 19679828555.24230576\n",
      "Iteration 42, loss = 19679820577.34139633\n",
      "Iteration 43, loss = 19679812534.22596741\n",
      "Iteration 44, loss = 19679804434.26999283\n",
      "Iteration 45, loss = 19679796698.62454605\n",
      "Iteration 46, loss = 19679789181.07643127\n",
      "Iteration 47, loss = 19679781698.37913895\n",
      "Iteration 48, loss = 19679774379.69842148\n",
      "Iteration 49, loss = 19679767036.00362015\n",
      "Iteration 50, loss = 19679759905.44067383\n",
      "Iteration 51, loss = 19679752639.90979385\n",
      "Iteration 52, loss = 19679745367.67878342\n",
      "Iteration 53, loss = 19679737871.18500519\n",
      "Iteration 54, loss = 19679730543.12007904\n",
      "Iteration 55, loss = 19679723315.20464706\n",
      "Iteration 56, loss = 19679716039.90118790\n",
      "Iteration 57, loss = 19679708953.95786285\n",
      "Iteration 58, loss = 19679702036.38176346\n",
      "Iteration 59, loss = 19679695299.99053192\n",
      "Iteration 60, loss = 19679688424.24865341\n",
      "Iteration 61, loss = 19679681581.36733627\n",
      "Iteration 62, loss = 19679674709.74442291\n",
      "Iteration 63, loss = 19679668040.57220459\n",
      "Iteration 64, loss = 19679661415.02188110\n",
      "Iteration 65, loss = 19679654775.71411133\n",
      "Iteration 66, loss = 19679648219.64611816\n",
      "Iteration 67, loss = 19679641539.26330948\n",
      "Iteration 68, loss = 19679634984.36193466\n",
      "Iteration 69, loss = 19679628473.59118271\n",
      "Iteration 70, loss = 19679621965.58029175\n",
      "Iteration 71, loss = 19679615522.02808380\n",
      "Iteration 72, loss = 19679609099.46095276\n",
      "Iteration 73, loss = 19679602712.17069244\n",
      "Iteration 74, loss = 19679596393.61355209\n",
      "Iteration 75, loss = 19679590064.48340607\n",
      "Iteration 76, loss = 19679583722.76836777\n",
      "Iteration 77, loss = 19679577441.55811691\n",
      "Iteration 78, loss = 19679571133.36500549\n",
      "Iteration 79, loss = 19679564855.70411301\n",
      "Iteration 80, loss = 19679558528.11642075\n",
      "Iteration 81, loss = 19679552309.29350281\n",
      "Iteration 82, loss = 19679546076.39105606\n",
      "Iteration 83, loss = 19679539829.17834473\n",
      "Iteration 84, loss = 19679533610.41380310\n",
      "Iteration 85, loss = 19679527376.86596298\n",
      "Iteration 86, loss = 19679521236.42371750\n",
      "Iteration 87, loss = 19679515021.11514282\n",
      "Iteration 88, loss = 19679508876.53514481\n",
      "Iteration 89, loss = 19679502686.16507339\n",
      "Iteration 90, loss = 19679496468.67016602\n",
      "Iteration 91, loss = 19679490238.49897385\n",
      "Iteration 92, loss = 19679484066.16076279\n",
      "Iteration 93, loss = 19679477916.65123367\n",
      "Iteration 94, loss = 19679471778.16836166\n",
      "Iteration 95, loss = 19679465666.31011963\n",
      "Iteration 96, loss = 19679459563.33039093\n",
      "Iteration 97, loss = 19679453468.66405487\n",
      "Iteration 98, loss = 19679447374.99478912\n",
      "Iteration 99, loss = 19679441264.34199905\n",
      "Iteration 100, loss = 19679435174.56828690\n",
      "Iteration 101, loss = 19679429069.75618362\n",
      "Iteration 102, loss = 19679422975.92852020\n",
      "Iteration 103, loss = 19679416887.56407928\n",
      "Iteration 104, loss = 19679410841.87564468\n",
      "Iteration 105, loss = 19679404740.00883102\n",
      "Iteration 106, loss = 19679398655.18935394\n",
      "Iteration 107, loss = 19679392583.63909912\n",
      "Iteration 108, loss = 19679386567.59803009\n",
      "Iteration 109, loss = 19679380481.62089157\n",
      "Iteration 110, loss = 19679374412.68711090\n",
      "Iteration 111, loss = 19679368345.03637314\n",
      "Iteration 112, loss = 19679362343.35519791\n",
      "Iteration 113, loss = 19679356324.04570770\n",
      "Iteration 114, loss = 19679350345.30694580\n",
      "Iteration 115, loss = 19679344381.99560928\n",
      "Iteration 116, loss = 19679338355.35082245\n",
      "Iteration 117, loss = 19679332415.45928192\n",
      "Iteration 118, loss = 19679326404.90778351\n",
      "Iteration 119, loss = 19679320392.71193314\n",
      "Iteration 120, loss = 19679314375.29257965\n",
      "Iteration 121, loss = 19679308457.21726227\n",
      "Iteration 122, loss = 19679302496.07046890\n",
      "Iteration 123, loss = 19679296475.36349869\n",
      "Iteration 124, loss = 19679290444.24768066\n",
      "Iteration 125, loss = 19679284444.02127075\n",
      "Iteration 126, loss = 19679278457.94795990\n",
      "Iteration 127, loss = 19679272493.31094360\n",
      "Iteration 128, loss = 19679266513.23508072\n",
      "Iteration 129, loss = 19679260587.17689896\n",
      "Iteration 130, loss = 19679254648.04897690\n",
      "Iteration 131, loss = 19679248767.14186478\n",
      "Iteration 132, loss = 19679242791.48379898\n",
      "Iteration 133, loss = 19679236809.21745300\n",
      "Iteration 134, loss = 19679230794.00926208\n",
      "Iteration 135, loss = 19679224838.40266418\n",
      "Iteration 136, loss = 19679218825.70912933\n",
      "Iteration 137, loss = 19679212886.95999146\n",
      "Iteration 138, loss = 19679206945.72782516\n",
      "Iteration 139, loss = 19679200944.44406891\n",
      "Iteration 140, loss = 19679194964.71276474\n",
      "Iteration 141, loss = 19679189101.43482208\n",
      "Iteration 142, loss = 19679183234.06190872\n",
      "Iteration 143, loss = 19679177356.65932846\n",
      "Iteration 144, loss = 19679171444.36125565\n",
      "Iteration 145, loss = 19679165534.35946655\n",
      "Iteration 146, loss = 19679159591.52709961\n",
      "Iteration 147, loss = 19679153714.97066116\n",
      "Iteration 148, loss = 19679147713.50811005\n",
      "Iteration 149, loss = 19679141842.52058029\n",
      "Iteration 150, loss = 19679136005.16973495\n",
      "Iteration 151, loss = 19679130091.64329147\n",
      "Iteration 152, loss = 19679124145.55015182\n",
      "Iteration 153, loss = 19679118251.97747040\n",
      "Iteration 154, loss = 19679112410.41030884\n",
      "Iteration 155, loss = 19679106627.10395050\n",
      "Iteration 156, loss = 19679100850.01713943\n",
      "Iteration 157, loss = 19679095067.18155289\n",
      "Iteration 158, loss = 19679089347.11628342\n",
      "Iteration 159, loss = 19679083580.96697998\n",
      "Iteration 160, loss = 19679077860.76531601\n",
      "Iteration 161, loss = 19679072108.28221893\n",
      "Iteration 162, loss = 19679066410.90505600\n",
      "Iteration 163, loss = 19679060586.47273636\n",
      "Iteration 164, loss = 19679054743.48038101\n",
      "Iteration 165, loss = 19679048938.93053055\n",
      "Iteration 166, loss = 19679043111.18885040\n",
      "Iteration 167, loss = 19679037350.33349228\n",
      "Iteration 168, loss = 19679031533.36719513\n",
      "Iteration 169, loss = 19679025717.29384232\n",
      "Iteration 170, loss = 19679019929.76742172\n",
      "Iteration 171, loss = 19679014138.44186401\n",
      "Iteration 172, loss = 19679008325.45339584\n",
      "Iteration 173, loss = 19679002550.93381500\n",
      "Iteration 174, loss = 19678996764.78345490\n",
      "Iteration 175, loss = 19678990965.43943024\n",
      "Iteration 176, loss = 19678985129.77245331\n",
      "Iteration 177, loss = 19678979285.96551895\n",
      "Iteration 178, loss = 19678973453.05904770\n",
      "Iteration 179, loss = 19678967640.80273056\n",
      "Iteration 180, loss = 19678961838.79975510\n",
      "Iteration 181, loss = 19678955984.37126541\n",
      "Iteration 182, loss = 19678950166.79407883\n",
      "Iteration 183, loss = 19678944390.34593582\n",
      "Iteration 184, loss = 19678938555.95493317\n",
      "Iteration 185, loss = 19678932784.40879059\n",
      "Iteration 186, loss = 19678926994.64238739\n",
      "Iteration 187, loss = 19678921279.57828140\n",
      "Iteration 188, loss = 19678915537.71195984\n",
      "Iteration 189, loss = 19678909865.38030624\n",
      "Iteration 190, loss = 19678904088.58546829\n",
      "Iteration 191, loss = 19678898255.93993759\n",
      "Iteration 192, loss = 19678892451.81412506\n",
      "Iteration 193, loss = 19678886645.14404297\n",
      "Iteration 194, loss = 19678880793.65513611\n",
      "Iteration 195, loss = 19678875004.57859039\n",
      "Iteration 196, loss = 19678869260.33940887\n",
      "Iteration 197, loss = 19678863515.62693787\n",
      "Iteration 198, loss = 19678857779.84244537\n",
      "Iteration 199, loss = 19678851915.78631592\n",
      "Iteration 200, loss = 19678846144.57252884\n",
      "Iteration 201, loss = 19678840287.13560486\n",
      "Iteration 202, loss = 19678834371.50315857\n",
      "Iteration 203, loss = 19678828494.33245850\n",
      "Iteration 204, loss = 19678822590.32456589\n",
      "Iteration 205, loss = 19678816659.80801010\n",
      "Iteration 206, loss = 19678810753.57369614\n",
      "Iteration 207, loss = 19678804787.59885788\n",
      "Iteration 208, loss = 19678798774.81631851\n",
      "Iteration 209, loss = 19678792701.88758850\n",
      "Iteration 210, loss = 19678786571.12345505\n",
      "Iteration 211, loss = 19678780309.87593842\n",
      "Iteration 212, loss = 19678773801.71232605\n",
      "Iteration 213, loss = 19678767099.80744553\n",
      "Iteration 214, loss = 19678760246.31386566\n",
      "Iteration 215, loss = 19678753423.39329910\n",
      "Iteration 216, loss = 19678746571.42463303\n",
      "Iteration 217, loss = 19678739878.49695206\n",
      "Iteration 218, loss = 19678733202.87632751\n",
      "Iteration 219, loss = 19678726541.66996002\n",
      "Iteration 220, loss = 19678719972.69760895\n",
      "Iteration 221, loss = 19678713463.44816971\n",
      "Iteration 222, loss = 19678706999.58571243\n",
      "Iteration 223, loss = 19678700517.92041016\n",
      "Iteration 224, loss = 19678694168.21086502\n",
      "Iteration 225, loss = 19678687779.61208725\n",
      "Iteration 226, loss = 19678681362.44372940\n",
      "Iteration 227, loss = 19678675039.76151657\n",
      "Iteration 228, loss = 19678668666.27253723\n",
      "Iteration 229, loss = 19678662370.01062393\n",
      "Iteration 230, loss = 19678656069.62121201\n",
      "Iteration 231, loss = 19678649795.55760956\n",
      "Iteration 232, loss = 19678643459.67884064\n",
      "Iteration 233, loss = 19678637245.90921783\n",
      "Iteration 234, loss = 19678630929.35009766\n",
      "Iteration 235, loss = 19678624668.09909439\n",
      "Iteration 236, loss = 19678618419.06898499\n",
      "Iteration 237, loss = 19678612143.10361481\n",
      "Iteration 238, loss = 19678605901.71723557\n",
      "Iteration 239, loss = 19678599577.08730316\n",
      "Iteration 240, loss = 19678593317.69606400\n",
      "Iteration 241, loss = 19678587018.39036560\n",
      "Iteration 242, loss = 19678580808.35590363\n",
      "Iteration 243, loss = 19678574600.22784042\n",
      "Iteration 244, loss = 19678568516.40335846\n",
      "Iteration 245, loss = 19678562405.36970901\n",
      "Iteration 246, loss = 19678556314.40507507\n",
      "Iteration 247, loss = 19678550262.09120178\n",
      "Iteration 248, loss = 19678544185.33853149\n",
      "Iteration 249, loss = 19678538109.47319031\n",
      "Iteration 250, loss = 19678532109.05492783\n",
      "Iteration 251, loss = 19678526114.80012894\n",
      "Iteration 252, loss = 19678520070.95273209\n",
      "Iteration 253, loss = 19678513964.67834854\n",
      "Iteration 254, loss = 19678507971.14277649\n",
      "Iteration 255, loss = 19678501945.79417801\n",
      "Iteration 256, loss = 19678495926.55259323\n",
      "Iteration 257, loss = 19678489903.52878952\n",
      "Iteration 258, loss = 19678483905.53830338\n",
      "Iteration 259, loss = 19678477815.46083832\n",
      "Iteration 260, loss = 19678471859.33892441\n",
      "Iteration 261, loss = 19678465863.30101776\n",
      "Iteration 262, loss = 19678459862.24514389\n",
      "Iteration 263, loss = 19678453839.61535263\n",
      "Iteration 264, loss = 19678447873.69986725\n",
      "Iteration 265, loss = 19678441790.18431473\n",
      "Iteration 266, loss = 19678435751.07422638\n",
      "Iteration 267, loss = 19678429703.27075958\n",
      "Iteration 268, loss = 19678423615.45356750\n",
      "Iteration 269, loss = 19678417581.69555283\n",
      "Iteration 270, loss = 19678411578.58652496\n",
      "Iteration 271, loss = 19678405554.08734131\n",
      "Iteration 272, loss = 19678399514.55255890\n",
      "Iteration 273, loss = 19678393478.86999893\n",
      "Iteration 274, loss = 19678387385.59283447\n",
      "Iteration 275, loss = 19678381382.73299026\n",
      "Iteration 276, loss = 19678375300.35389709\n",
      "Iteration 277, loss = 19678369206.19375229\n",
      "Iteration 278, loss = 19678363171.09540558\n",
      "Iteration 279, loss = 19678357177.11026001\n",
      "Iteration 280, loss = 19678351269.61145782\n",
      "Iteration 281, loss = 19678345294.98532486\n",
      "Iteration 282, loss = 19678339309.63526917\n",
      "Iteration 283, loss = 19678333177.56511688\n",
      "Iteration 284, loss = 19678327185.78879166\n",
      "Iteration 285, loss = 19678321197.10367584\n",
      "Iteration 286, loss = 19678315234.48337936\n",
      "Iteration 287, loss = 19678309207.86814499\n",
      "Iteration 288, loss = 19678303310.04116440\n",
      "Iteration 289, loss = 19678297336.57550430\n",
      "Iteration 290, loss = 19678291342.61699295\n",
      "Iteration 291, loss = 19678285334.56415558\n",
      "Iteration 292, loss = 19678279255.95123291\n",
      "Iteration 293, loss = 19678273241.01002502\n",
      "Iteration 294, loss = 19678267232.04979324\n",
      "Iteration 295, loss = 19678261190.48258591\n",
      "Iteration 296, loss = 19678255205.34729385\n",
      "Iteration 297, loss = 19678249190.20862961\n",
      "Iteration 298, loss = 19678243260.41888046\n",
      "Iteration 299, loss = 19678237270.96369171\n",
      "Iteration 300, loss = 19678231281.43226242\n",
      "Iteration 301, loss = 19678225314.47543335\n",
      "Iteration 302, loss = 19678219349.28820038\n",
      "Iteration 303, loss = 19678213452.28150940\n",
      "Iteration 304, loss = 19678207532.69058990\n",
      "Iteration 305, loss = 19678201628.78345490\n",
      "Iteration 306, loss = 19678195705.94484329\n",
      "Iteration 307, loss = 19678189766.46224213\n",
      "Iteration 308, loss = 19678183762.21198654\n",
      "Iteration 309, loss = 19678177760.27348328\n",
      "Iteration 310, loss = 19678171760.32706070\n",
      "Iteration 311, loss = 19678165791.20169067\n",
      "Iteration 312, loss = 19678159691.69610977\n",
      "Iteration 313, loss = 19678153687.34780121\n",
      "Iteration 314, loss = 19678147661.61020279\n",
      "Iteration 315, loss = 19678141636.55838013\n",
      "Iteration 316, loss = 19678135541.34892654\n",
      "Iteration 317, loss = 19678129456.13759995\n",
      "Iteration 318, loss = 19678123458.36629868\n",
      "Iteration 319, loss = 19678117524.74627686\n",
      "Iteration 320, loss = 19678111612.92882538\n",
      "Iteration 321, loss = 19678105783.83279419\n",
      "Iteration 322, loss = 19678099944.45266724\n",
      "Iteration 323, loss = 19678094069.72177124\n",
      "Iteration 324, loss = 19678088203.97029495\n",
      "Iteration 325, loss = 19678082330.47246170\n",
      "Iteration 326, loss = 19678076521.94748688\n",
      "Iteration 327, loss = 19678070624.65421677\n",
      "Iteration 328, loss = 19678064791.06346130\n",
      "Iteration 329, loss = 19678058938.38623047\n",
      "Iteration 330, loss = 19678053096.76853561\n",
      "Iteration 331, loss = 19678047256.27896881\n",
      "Iteration 332, loss = 19678041412.68054199\n",
      "Iteration 333, loss = 19678035652.12630463\n",
      "Iteration 334, loss = 19678029783.78566742\n",
      "Iteration 335, loss = 19678023920.82744980\n",
      "Iteration 336, loss = 19678018093.78010559\n",
      "Iteration 337, loss = 19678012173.89525986\n",
      "Iteration 338, loss = 19678006303.59576035\n",
      "Iteration 339, loss = 19678000401.99163818\n",
      "Iteration 340, loss = 19677994569.91863251\n",
      "Iteration 341, loss = 19677988735.78481674\n",
      "Iteration 342, loss = 19677982877.01366043\n",
      "Iteration 343, loss = 19677976957.01879501\n",
      "Iteration 344, loss = 19677971033.11187744\n",
      "Iteration 345, loss = 19677965104.46092606\n",
      "Iteration 346, loss = 19677959193.74163437\n",
      "Iteration 347, loss = 19677953281.20724106\n",
      "Iteration 348, loss = 19677947423.72874451\n",
      "Iteration 349, loss = 19677941574.34475327\n",
      "Iteration 350, loss = 19677935683.18078613\n",
      "Iteration 351, loss = 19677929782.34528351\n",
      "Iteration 352, loss = 19677923882.02700424\n",
      "Iteration 353, loss = 19677918001.47109604\n",
      "Iteration 354, loss = 19677912097.17993546\n",
      "Iteration 355, loss = 19677906264.61875153\n",
      "Iteration 356, loss = 19677900397.01185608\n",
      "Iteration 357, loss = 19677894523.57769775\n",
      "Iteration 358, loss = 19677888648.80941772\n",
      "Iteration 359, loss = 19677882778.10198212\n",
      "Iteration 360, loss = 19677876851.01164246\n",
      "Iteration 361, loss = 19677870997.38647842\n",
      "Iteration 362, loss = 19677865062.01432800\n",
      "Iteration 363, loss = 19677859111.85004044\n",
      "Iteration 364, loss = 19677853118.89071274\n",
      "Iteration 365, loss = 19677847238.54677200\n",
      "Iteration 366, loss = 19677841369.25750351\n",
      "Iteration 367, loss = 19677835514.15726471\n",
      "Iteration 368, loss = 19677829697.24202728\n",
      "Iteration 369, loss = 19677823903.00719833\n",
      "Iteration 370, loss = 19677818090.97697067\n",
      "Iteration 371, loss = 19677812192.40781021\n",
      "Iteration 372, loss = 19677806392.56271362\n",
      "Iteration 373, loss = 19677800574.74534225\n",
      "Iteration 374, loss = 19677794769.96723938\n",
      "Iteration 375, loss = 19677788947.01592255\n",
      "Iteration 376, loss = 19677783062.74966431\n",
      "Iteration 377, loss = 19677777210.37150955\n",
      "Iteration 378, loss = 19677771357.01568222\n",
      "Iteration 379, loss = 19677765524.29589462\n",
      "Iteration 380, loss = 19677759608.56284332\n",
      "Iteration 381, loss = 19677753714.40341568\n",
      "Iteration 382, loss = 19677747801.41093445\n",
      "Iteration 383, loss = 19677741887.10646439\n",
      "Iteration 384, loss = 19677735893.13482666\n",
      "Iteration 385, loss = 19677729974.83344269\n",
      "Iteration 386, loss = 19677724006.45339966\n",
      "Iteration 387, loss = 19677718014.97632599\n",
      "Iteration 388, loss = 19677712059.77301025\n",
      "Iteration 389, loss = 19677706108.55850983\n",
      "Iteration 390, loss = 19677700178.92832565\n",
      "Iteration 391, loss = 19677694287.39306641\n",
      "Iteration 392, loss = 19677688360.42952728\n",
      "Iteration 393, loss = 19677682487.84820175\n",
      "Iteration 394, loss = 19677676596.87508011\n",
      "Iteration 395, loss = 19677670670.40555573\n",
      "Iteration 396, loss = 19677664726.29751587\n",
      "Iteration 397, loss = 19677658836.08253098\n",
      "Iteration 398, loss = 19677652881.32180786\n",
      "Iteration 399, loss = 19677647020.05243683\n",
      "Iteration 400, loss = 19677641226.71873474\n",
      "Iteration 401, loss = 19677635394.18844986\n",
      "Iteration 402, loss = 19677629555.76707077\n",
      "Iteration 403, loss = 19677623664.17475128\n",
      "Iteration 404, loss = 19677617744.12203217\n",
      "Iteration 405, loss = 19677611819.23568344\n",
      "Iteration 406, loss = 19677605864.31304169\n",
      "Iteration 407, loss = 19677599933.97378540\n",
      "Iteration 408, loss = 19677594039.83496094\n",
      "Iteration 409, loss = 19677588213.83482361\n",
      "Iteration 410, loss = 19677582376.37484741\n",
      "Iteration 411, loss = 19677576523.57203674\n",
      "Iteration 412, loss = 19677570708.91295624\n",
      "Iteration 413, loss = 19677564882.04962158\n",
      "Iteration 414, loss = 19677559146.91532135\n",
      "Iteration 415, loss = 19677553316.37359619\n",
      "Iteration 416, loss = 19677547529.99156952\n",
      "Iteration 417, loss = 19677541784.92424011\n",
      "Iteration 418, loss = 19677536046.57746124\n",
      "Iteration 419, loss = 19677530237.39985657\n",
      "Iteration 420, loss = 19677524401.63142776\n",
      "Iteration 421, loss = 19677518510.44647980\n",
      "Iteration 422, loss = 19677512603.09129333\n",
      "Iteration 423, loss = 19677506742.15282059\n",
      "Iteration 424, loss = 19677500864.76986694\n",
      "Iteration 425, loss = 19677495034.35686111\n",
      "Iteration 426, loss = 19677489281.90998459\n",
      "Iteration 427, loss = 19677483460.39545059\n",
      "Iteration 428, loss = 19677477680.36297607\n",
      "Iteration 429, loss = 19677471872.10266876\n",
      "Iteration 430, loss = 19677466016.37662888\n",
      "Iteration 431, loss = 19677460193.54092407\n",
      "Iteration 432, loss = 19677454287.88403320\n",
      "Iteration 433, loss = 19677448349.48104858\n",
      "Iteration 434, loss = 19677442395.72616959\n",
      "Iteration 435, loss = 19677436518.07625961\n",
      "Iteration 436, loss = 19677430640.94048309\n",
      "Iteration 437, loss = 19677424807.27501678\n",
      "Iteration 438, loss = 19677419009.20079041\n",
      "Iteration 439, loss = 19677413157.09640503\n",
      "Iteration 440, loss = 19677407250.47861099\n",
      "Iteration 441, loss = 19677401409.84159470\n",
      "Iteration 442, loss = 19677395530.90550995\n",
      "Iteration 443, loss = 19677389689.30037308\n",
      "Iteration 444, loss = 19677383861.60816193\n",
      "Iteration 445, loss = 19677377993.43632889\n",
      "Iteration 446, loss = 19677372093.97207260\n",
      "Iteration 447, loss = 19677366200.02283859\n",
      "Iteration 448, loss = 19677360352.53694153\n",
      "Iteration 449, loss = 19677354512.84195328\n",
      "Iteration 450, loss = 19677348563.44495010\n",
      "Iteration 451, loss = 19677342603.49612427\n",
      "Iteration 452, loss = 19677336676.35519028\n",
      "Iteration 453, loss = 19677330753.44532394\n",
      "Iteration 454, loss = 19677324913.46809769\n",
      "Iteration 455, loss = 19677319036.22053528\n",
      "Iteration 456, loss = 19677313151.47365952\n",
      "Iteration 457, loss = 19677307320.29669571\n",
      "Iteration 458, loss = 19677301439.58343124\n",
      "Iteration 459, loss = 19677295586.15219879\n",
      "Iteration 460, loss = 19677289719.06521988\n",
      "Iteration 461, loss = 19677283807.11262894\n",
      "Iteration 462, loss = 19677277943.29722977\n",
      "Iteration 463, loss = 19677272085.41622543\n",
      "Iteration 464, loss = 19677266222.13343811\n",
      "Iteration 465, loss = 19677260353.08845901\n",
      "Iteration 466, loss = 19677254496.92569733\n",
      "Iteration 467, loss = 19677248672.11249542\n",
      "Iteration 468, loss = 19677242872.67440033\n",
      "Iteration 469, loss = 19677237086.06247711\n",
      "Iteration 470, loss = 19677231290.08467102\n",
      "Iteration 471, loss = 19677225457.63968658\n",
      "Iteration 472, loss = 19677219562.73717117\n",
      "Iteration 473, loss = 19677213678.47251511\n",
      "Iteration 474, loss = 19677207805.41399384\n",
      "Iteration 475, loss = 19677201929.20809937\n",
      "Iteration 476, loss = 19677196100.73198318\n",
      "Iteration 477, loss = 19677190309.01654434\n",
      "Iteration 478, loss = 19677184533.19281006\n",
      "Iteration 479, loss = 19677178721.41783142\n",
      "Iteration 480, loss = 19677172891.67089081\n",
      "Iteration 481, loss = 19677167044.58062744\n",
      "Iteration 482, loss = 19677161191.99635315\n",
      "Iteration 483, loss = 19677155360.73674011\n",
      "Iteration 484, loss = 19677149498.83938599\n",
      "Iteration 485, loss = 19677143737.09867477\n",
      "Iteration 486, loss = 19677137864.08338928\n",
      "Iteration 487, loss = 19677132041.13535690\n",
      "Iteration 488, loss = 19677126219.25709152\n",
      "Iteration 489, loss = 19677120384.05481720\n",
      "Iteration 490, loss = 19677114537.59154510\n",
      "Iteration 491, loss = 19677108734.97066879\n",
      "Iteration 492, loss = 19677102913.29505157\n",
      "Iteration 493, loss = 19677097175.45898438\n",
      "Iteration 494, loss = 19677091294.18234253\n",
      "Iteration 495, loss = 19677085524.02243042\n",
      "Iteration 496, loss = 19677079584.26556778\n",
      "Iteration 497, loss = 19677073708.71700668\n",
      "Iteration 498, loss = 19677067772.20643234\n",
      "Iteration 499, loss = 19677061855.19293213\n",
      "Iteration 500, loss = 19677055947.86655807\n",
      "Iteration 1, loss = 19707824550.34996033\n",
      "Iteration 2, loss = 19707809357.29524231\n",
      "Iteration 3, loss = 19707794104.15966797\n",
      "Iteration 4, loss = 19707778838.69297791\n",
      "Iteration 5, loss = 19707763460.28213501\n",
      "Iteration 6, loss = 19707747447.46366882\n",
      "Iteration 7, loss = 19707730567.27236176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 19707712111.36474609\n",
      "Iteration 9, loss = 19707692202.34388351\n",
      "Iteration 10, loss = 19707671506.61727142\n",
      "Iteration 11, loss = 19707650253.61833572\n",
      "Iteration 12, loss = 19707630743.34643173\n",
      "Iteration 13, loss = 19707612479.36899185\n",
      "Iteration 14, loss = 19707594006.40234756\n",
      "Iteration 15, loss = 19707576209.30529785\n",
      "Iteration 16, loss = 19707559308.23118973\n",
      "Iteration 17, loss = 19707542484.82682419\n",
      "Iteration 18, loss = 19707524890.25414276\n",
      "Iteration 19, loss = 19707503108.61790085\n",
      "Iteration 20, loss = 19707474934.46420670\n",
      "Iteration 21, loss = 19707443077.30986404\n",
      "Iteration 22, loss = 19707414859.29569626\n",
      "Iteration 23, loss = 19707391177.36176300\n",
      "Iteration 24, loss = 19707369998.51835251\n",
      "Iteration 25, loss = 19707350123.11136246\n",
      "Iteration 26, loss = 19707329760.82427597\n",
      "Iteration 27, loss = 19707307602.89912415\n",
      "Iteration 28, loss = 19707288910.20776749\n",
      "Iteration 29, loss = 19707272946.12252426\n",
      "Iteration 30, loss = 19707257906.45249939\n",
      "Iteration 31, loss = 19707242891.81561279\n",
      "Iteration 32, loss = 19707228127.95512772\n",
      "Iteration 33, loss = 19707214056.31848526\n",
      "Iteration 34, loss = 19707200850.08861542\n",
      "Iteration 35, loss = 19707187964.75536346\n",
      "Iteration 36, loss = 19707176243.39229965\n",
      "Iteration 37, loss = 19707166601.99749374\n",
      "Iteration 38, loss = 19707158493.31462860\n",
      "Iteration 39, loss = 19707150933.90616226\n",
      "Iteration 40, loss = 19707143649.96161652\n",
      "Iteration 41, loss = 19707136711.59499359\n",
      "Iteration 42, loss = 19707129951.28412247\n",
      "Iteration 43, loss = 19707123346.44348907\n",
      "Iteration 44, loss = 19707116761.38480377\n",
      "Iteration 45, loss = 19707110312.33148575\n",
      "Iteration 46, loss = 19707103814.01699829\n",
      "Iteration 47, loss = 19707097179.37623978\n",
      "Iteration 48, loss = 19707090634.91732407\n",
      "Iteration 49, loss = 19707084428.81422424\n",
      "Iteration 50, loss = 19707078378.65643692\n",
      "Iteration 51, loss = 19707072281.09623337\n",
      "Iteration 52, loss = 19707066280.07846832\n",
      "Iteration 53, loss = 19707060375.42343903\n",
      "Iteration 54, loss = 19707054460.57548904\n",
      "Iteration 55, loss = 19707048574.50348663\n",
      "Iteration 56, loss = 19707042775.90491486\n",
      "Iteration 57, loss = 19707036925.65948105\n",
      "Iteration 58, loss = 19707030975.09173965\n",
      "Iteration 59, loss = 19707024958.87308121\n",
      "Iteration 60, loss = 19707019127.69791031\n",
      "Iteration 61, loss = 19707013308.69384766\n",
      "Iteration 62, loss = 19707007402.65806580\n",
      "Iteration 63, loss = 19707001774.69846725\n",
      "Iteration 64, loss = 19706996164.23059464\n",
      "Iteration 65, loss = 19706990560.17419434\n",
      "Iteration 66, loss = 19706985019.49449921\n",
      "Iteration 67, loss = 19706979491.24666977\n",
      "Iteration 68, loss = 19706973999.05960083\n",
      "Iteration 69, loss = 19706968516.46795654\n",
      "Iteration 70, loss = 19706963063.94240189\n",
      "Iteration 71, loss = 19706957639.20985031\n",
      "Iteration 72, loss = 19706952244.16323853\n",
      "Iteration 73, loss = 19706946837.24161911\n",
      "Iteration 74, loss = 19706941403.59272766\n",
      "Iteration 75, loss = 19706936051.36362457\n",
      "Iteration 76, loss = 19706930717.07935333\n",
      "Iteration 77, loss = 19706925361.08667755\n",
      "Iteration 78, loss = 19706920041.94744873\n",
      "Iteration 79, loss = 19706914699.79914093\n",
      "Iteration 80, loss = 19706909406.02087784\n",
      "Iteration 81, loss = 19706904112.29942322\n",
      "Iteration 82, loss = 19706898833.08892822\n",
      "Iteration 83, loss = 19706893526.49790955\n",
      "Iteration 84, loss = 19706888269.30458832\n",
      "Iteration 85, loss = 19706883083.83206558\n",
      "Iteration 86, loss = 19706877791.18983841\n",
      "Iteration 87, loss = 19706872579.24484253\n",
      "Iteration 88, loss = 19706867383.45001221\n",
      "Iteration 89, loss = 19706862181.94064331\n",
      "Iteration 90, loss = 19706856960.01511002\n",
      "Iteration 91, loss = 19706851791.81767654\n",
      "Iteration 92, loss = 19706846625.29771042\n",
      "Iteration 93, loss = 19706841473.79929352\n",
      "Iteration 94, loss = 19706836271.19588089\n",
      "Iteration 95, loss = 19706831147.55950546\n",
      "Iteration 96, loss = 19706825984.54362869\n",
      "Iteration 97, loss = 19706820826.24363327\n",
      "Iteration 98, loss = 19706815704.72143555\n",
      "Iteration 99, loss = 19706810581.54870224\n",
      "Iteration 100, loss = 19706805427.71767044\n",
      "Iteration 101, loss = 19706800306.47486115\n",
      "Iteration 102, loss = 19706795219.39368057\n",
      "Iteration 103, loss = 19706790120.02962875\n",
      "Iteration 104, loss = 19706785020.62716293\n",
      "Iteration 105, loss = 19706779894.16943741\n",
      "Iteration 106, loss = 19706774846.51856232\n",
      "Iteration 107, loss = 19706769720.56354904\n",
      "Iteration 108, loss = 19706764668.03892517\n",
      "Iteration 109, loss = 19706759577.94552612\n",
      "Iteration 110, loss = 19706754529.35805511\n",
      "Iteration 111, loss = 19706749432.43725204\n",
      "Iteration 112, loss = 19706744407.40610504\n",
      "Iteration 113, loss = 19706739354.52799606\n",
      "Iteration 114, loss = 19706734333.06339264\n",
      "Iteration 115, loss = 19706729270.84235382\n",
      "Iteration 116, loss = 19706724220.53677750\n",
      "Iteration 117, loss = 19706719188.81140137\n",
      "Iteration 118, loss = 19706714185.38351059\n",
      "Iteration 119, loss = 19706709140.77291107\n",
      "Iteration 120, loss = 19706704136.12041092\n",
      "Iteration 121, loss = 19706699126.12604141\n",
      "Iteration 122, loss = 19706694104.18000031\n",
      "Iteration 123, loss = 19706689113.55419540\n",
      "Iteration 124, loss = 19706684105.31604385\n",
      "Iteration 125, loss = 19706679110.21604919\n",
      "Iteration 126, loss = 19706674114.64757538\n",
      "Iteration 127, loss = 19706669118.20078278\n",
      "Iteration 128, loss = 19706664157.32895279\n",
      "Iteration 129, loss = 19706659149.62210083\n",
      "Iteration 130, loss = 19706654189.78022385\n",
      "Iteration 131, loss = 19706649239.69223404\n",
      "Iteration 132, loss = 19706644241.81073380\n",
      "Iteration 133, loss = 19706639315.92682648\n",
      "Iteration 134, loss = 19706634314.32770157\n",
      "Iteration 135, loss = 19706629369.39727020\n",
      "Iteration 136, loss = 19706624418.43610001\n",
      "Iteration 137, loss = 19706619456.99177933\n",
      "Iteration 138, loss = 19706614510.51493454\n",
      "Iteration 139, loss = 19706609564.06869125\n",
      "Iteration 140, loss = 19706604645.45229721\n",
      "Iteration 141, loss = 19706599694.07077789\n",
      "Iteration 142, loss = 19706594703.22835541\n",
      "Iteration 143, loss = 19706589795.32307816\n",
      "Iteration 144, loss = 19706584864.73405457\n",
      "Iteration 145, loss = 19706579885.68889618\n",
      "Iteration 146, loss = 19706574970.60984421\n",
      "Iteration 147, loss = 19706570054.93081665\n",
      "Iteration 148, loss = 19706565117.86454391\n",
      "Iteration 149, loss = 19706560162.32067490\n",
      "Iteration 150, loss = 19706555274.16128540\n",
      "Iteration 151, loss = 19706550331.39513397\n",
      "Iteration 152, loss = 19706545387.55400085\n",
      "Iteration 153, loss = 19706540438.06010056\n",
      "Iteration 154, loss = 19706535501.70957565\n",
      "Iteration 155, loss = 19706530584.10227203\n",
      "Iteration 156, loss = 19706525616.31023788\n",
      "Iteration 157, loss = 19706520641.99872971\n",
      "Iteration 158, loss = 19706515638.11081696\n",
      "Iteration 159, loss = 19706510596.39783859\n",
      "Iteration 160, loss = 19706505550.05551529\n",
      "Iteration 161, loss = 19706500407.05474472\n",
      "Iteration 162, loss = 19706495209.48009491\n",
      "Iteration 163, loss = 19706489856.54877853\n",
      "Iteration 164, loss = 19706484414.82981491\n",
      "Iteration 165, loss = 19706478874.40304947\n",
      "Iteration 166, loss = 19706473303.02929688\n",
      "Iteration 167, loss = 19706467629.49898529\n",
      "Iteration 168, loss = 19706461916.16605759\n",
      "Iteration 169, loss = 19706456205.27822876\n",
      "Iteration 170, loss = 19706450568.99444580\n",
      "Iteration 171, loss = 19706444948.52327728\n",
      "Iteration 172, loss = 19706439350.75201035\n",
      "Iteration 173, loss = 19706433741.51558685\n",
      "Iteration 174, loss = 19706428210.94363403\n",
      "Iteration 175, loss = 19706422703.09376907\n",
      "Iteration 176, loss = 19706417271.78705215\n",
      "Iteration 177, loss = 19706411790.56127548\n",
      "Iteration 178, loss = 19706406330.43214798\n",
      "Iteration 179, loss = 19706400956.28334808\n",
      "Iteration 180, loss = 19706395551.47546005\n",
      "Iteration 181, loss = 19706390124.11820984\n",
      "Iteration 182, loss = 19706384829.65818405\n",
      "Iteration 183, loss = 19706379471.22614670\n",
      "Iteration 184, loss = 19706374100.00445175\n",
      "Iteration 185, loss = 19706368779.64694977\n",
      "Iteration 186, loss = 19706363469.42210770\n",
      "Iteration 187, loss = 19706358144.68970871\n",
      "Iteration 188, loss = 19706352889.17135620\n",
      "Iteration 189, loss = 19706347581.15882492\n",
      "Iteration 190, loss = 19706342299.90640640\n",
      "Iteration 191, loss = 19706337071.42077637\n",
      "Iteration 192, loss = 19706331791.76240540\n",
      "Iteration 193, loss = 19706326555.39125061\n",
      "Iteration 194, loss = 19706321316.20085526\n",
      "Iteration 195, loss = 19706316072.33907700\n",
      "Iteration 196, loss = 19706310878.52364731\n",
      "Iteration 197, loss = 19706305634.17759323\n",
      "Iteration 198, loss = 19706300423.18215179\n",
      "Iteration 199, loss = 19706295216.44496918\n",
      "Iteration 200, loss = 19706290035.02630234\n",
      "Iteration 201, loss = 19706284848.79843140\n",
      "Iteration 202, loss = 19706279650.48073578\n",
      "Iteration 203, loss = 19706274433.58272934\n",
      "Iteration 204, loss = 19706269285.20862198\n",
      "Iteration 205, loss = 19706264109.72224045\n",
      "Iteration 206, loss = 19706258903.78929901\n",
      "Iteration 207, loss = 19706253753.52227020\n",
      "Iteration 208, loss = 19706248590.00091171\n",
      "Iteration 209, loss = 19706243412.29144669\n",
      "Iteration 210, loss = 19706238302.94300842\n",
      "Iteration 211, loss = 19706233083.47737503\n",
      "Iteration 212, loss = 19706228003.45072556\n",
      "Iteration 213, loss = 19706222831.41557693\n",
      "Iteration 214, loss = 19706217698.75508499\n",
      "Iteration 215, loss = 19706212588.41346741\n",
      "Iteration 216, loss = 19706207480.88727570\n",
      "Iteration 217, loss = 19706202304.33967590\n",
      "Iteration 218, loss = 19706197205.41588211\n",
      "Iteration 219, loss = 19706192091.04508209\n",
      "Iteration 220, loss = 19706186959.32222748\n",
      "Iteration 221, loss = 19706181853.97194672\n",
      "Iteration 222, loss = 19706176745.74734879\n",
      "Iteration 223, loss = 19706171628.10610199\n",
      "Iteration 224, loss = 19706166495.58017731\n",
      "Iteration 225, loss = 19706161403.90615463\n",
      "Iteration 226, loss = 19706156255.42694092\n",
      "Iteration 227, loss = 19706151174.99130249\n",
      "Iteration 228, loss = 19706146075.02962875\n",
      "Iteration 229, loss = 19706140942.66048813\n",
      "Iteration 230, loss = 19706135860.54504776\n",
      "Iteration 231, loss = 19706130778.43925095\n",
      "Iteration 232, loss = 19706125656.52113342\n",
      "Iteration 233, loss = 19706120570.65442276\n",
      "Iteration 234, loss = 19706115487.86403275\n",
      "Iteration 235, loss = 19706110391.19194412\n",
      "Iteration 236, loss = 19706105305.41890717\n",
      "Iteration 237, loss = 19706100258.21120834\n",
      "Iteration 238, loss = 19706095128.30077362\n",
      "Iteration 239, loss = 19706090089.96001053\n",
      "Iteration 240, loss = 19706084992.57582855\n",
      "Iteration 241, loss = 19706079944.95182800\n",
      "Iteration 242, loss = 19706074870.81346130\n",
      "Iteration 243, loss = 19706069743.22730637\n",
      "Iteration 244, loss = 19706064708.03457260\n",
      "Iteration 245, loss = 19706059690.73422623\n",
      "Iteration 246, loss = 19706054567.99673843\n",
      "Iteration 247, loss = 19706049536.35300446\n",
      "Iteration 248, loss = 19706044461.51069641\n",
      "Iteration 249, loss = 19706039429.10570145\n",
      "Iteration 250, loss = 19706034364.73506165\n",
      "Iteration 251, loss = 19706029298.28226089\n",
      "Iteration 252, loss = 19706024241.83076859\n",
      "Iteration 253, loss = 19706019196.78234482\n",
      "Iteration 254, loss = 19706014173.94104767\n",
      "Iteration 255, loss = 19706009078.39614868\n",
      "Iteration 256, loss = 19706004039.71450424\n",
      "Iteration 257, loss = 19705999016.15395355\n",
      "Iteration 258, loss = 19705993937.44479370\n",
      "Iteration 259, loss = 19705988889.30836868\n",
      "Iteration 260, loss = 19705983920.81850052\n",
      "Iteration 261, loss = 19705978824.52456665\n",
      "Iteration 262, loss = 19705973788.03577423\n",
      "Iteration 263, loss = 19705968773.44002533\n",
      "Iteration 264, loss = 19705963751.77228928\n",
      "Iteration 265, loss = 19705958717.71927643\n",
      "Iteration 266, loss = 19705953665.62207413\n",
      "Iteration 267, loss = 19705948647.43432999\n",
      "Iteration 268, loss = 19705943645.03464890\n",
      "Iteration 269, loss = 19705938617.55456924\n",
      "Iteration 270, loss = 19705933550.30433655\n",
      "Iteration 271, loss = 19705928558.93144226\n",
      "Iteration 272, loss = 19705923521.12631226\n",
      "Iteration 273, loss = 19705918523.89729691\n",
      "Iteration 274, loss = 19705913488.16053009\n",
      "Iteration 275, loss = 19705908441.10327911\n",
      "Iteration 276, loss = 19705903376.09551620\n",
      "Iteration 277, loss = 19705898335.64111710\n",
      "Iteration 278, loss = 19705893326.65737915\n",
      "Iteration 279, loss = 19705888296.53897095\n",
      "Iteration 280, loss = 19705883239.82574844\n",
      "Iteration 281, loss = 19705878257.78844452\n",
      "Iteration 282, loss = 19705873241.04652023\n",
      "Iteration 283, loss = 19705868224.45746613\n",
      "Iteration 284, loss = 19705863188.37789154\n",
      "Iteration 285, loss = 19705858193.11196518\n",
      "Iteration 286, loss = 19705853242.08809662\n",
      "Iteration 287, loss = 19705848134.98823547\n",
      "Iteration 288, loss = 19705843171.84475327\n",
      "Iteration 289, loss = 19705838193.61790848\n",
      "Iteration 290, loss = 19705833185.23733521\n",
      "Iteration 291, loss = 19705828175.35538483\n",
      "Iteration 292, loss = 19705823201.99331665\n",
      "Iteration 293, loss = 19705818180.19270706\n",
      "Iteration 294, loss = 19705813220.92743683\n",
      "Iteration 295, loss = 19705808189.48712158\n",
      "Iteration 296, loss = 19705803203.76546097\n",
      "Iteration 297, loss = 19705798182.12928772\n",
      "Iteration 298, loss = 19705793207.85054016\n",
      "Iteration 299, loss = 19705788215.35556793\n",
      "Iteration 300, loss = 19705783206.41037750\n",
      "Iteration 301, loss = 19705778235.07338715\n",
      "Iteration 302, loss = 19705773247.58184814\n",
      "Iteration 303, loss = 19705768261.55072784\n",
      "Iteration 304, loss = 19705763257.82483292\n",
      "Iteration 305, loss = 19705758294.44943237\n",
      "Iteration 306, loss = 19705753283.11424637\n",
      "Iteration 307, loss = 19705748281.21975327\n",
      "Iteration 308, loss = 19705743308.33536911\n",
      "Iteration 309, loss = 19705738304.52504349\n",
      "Iteration 310, loss = 19705733364.73529434\n",
      "Iteration 311, loss = 19705728367.30941010\n",
      "Iteration 312, loss = 19705723350.02655029\n",
      "Iteration 313, loss = 19705718366.39307404\n",
      "Iteration 314, loss = 19705713426.45576859\n",
      "Iteration 315, loss = 19705708401.84777832\n",
      "Iteration 316, loss = 19705703402.81219864\n",
      "Iteration 317, loss = 19705698466.89726257\n",
      "Iteration 318, loss = 19705693482.54452515\n",
      "Iteration 319, loss = 19705688477.93477249\n",
      "Iteration 320, loss = 19705683502.93685913\n",
      "Iteration 321, loss = 19705678501.22318649\n",
      "Iteration 322, loss = 19705673551.54921341\n",
      "Iteration 323, loss = 19705668585.86952972\n",
      "Iteration 324, loss = 19705663609.22190094\n",
      "Iteration 325, loss = 19705658592.84158707\n",
      "Iteration 326, loss = 19705653664.45597839\n",
      "Iteration 327, loss = 19705648690.11618423\n",
      "Iteration 328, loss = 19705643725.70925903\n",
      "Iteration 329, loss = 19705638738.62848663\n",
      "Iteration 330, loss = 19705633797.40783691\n",
      "Iteration 331, loss = 19705628833.81739426\n",
      "Iteration 332, loss = 19705623867.11145782\n",
      "Iteration 333, loss = 19705618919.17745972\n",
      "Iteration 334, loss = 19705613969.68254471\n",
      "Iteration 335, loss = 19705609007.87121582\n",
      "Iteration 336, loss = 19705604017.54246521\n",
      "Iteration 337, loss = 19705599033.86457062\n",
      "Iteration 338, loss = 19705594113.33528137\n",
      "Iteration 339, loss = 19705589154.67412949\n",
      "Iteration 340, loss = 19705584184.51578140\n",
      "Iteration 341, loss = 19705579218.79161072\n",
      "Iteration 342, loss = 19705574246.69629288\n",
      "Iteration 343, loss = 19705569298.66371536\n",
      "Iteration 344, loss = 19705564346.91654587\n",
      "Iteration 345, loss = 19705559386.00173187\n",
      "Iteration 346, loss = 19705554413.08676910\n",
      "Iteration 347, loss = 19705549446.94678879\n",
      "Iteration 348, loss = 19705544505.50107956\n",
      "Iteration 349, loss = 19705539570.54541397\n",
      "Iteration 350, loss = 19705534581.78353500\n",
      "Iteration 351, loss = 19705529663.55562973\n",
      "Iteration 352, loss = 19705524657.72946930\n",
      "Iteration 353, loss = 19705519736.38294220\n",
      "Iteration 354, loss = 19705514816.16997147\n",
      "Iteration 355, loss = 19705509874.45157623\n",
      "Iteration 356, loss = 19705504887.37905502\n",
      "Iteration 357, loss = 19705499952.07655334\n",
      "Iteration 358, loss = 19705494985.98143387\n",
      "Iteration 359, loss = 19705490066.58791733\n",
      "Iteration 360, loss = 19705485116.29539490\n",
      "Iteration 361, loss = 19705480165.99532700\n",
      "Iteration 362, loss = 19705475213.85762024\n",
      "Iteration 363, loss = 19705470263.35066986\n",
      "Iteration 364, loss = 19705465343.74102402\n",
      "Iteration 365, loss = 19705460353.51342010\n",
      "Iteration 366, loss = 19705455404.23410797\n",
      "Iteration 367, loss = 19705450471.54987717\n",
      "Iteration 368, loss = 19705445504.20931244\n",
      "Iteration 369, loss = 19705440581.02657318\n",
      "Iteration 370, loss = 19705435615.59349060\n",
      "Iteration 371, loss = 19705430663.93425369\n",
      "Iteration 372, loss = 19705425728.13856888\n",
      "Iteration 373, loss = 19705420791.06128693\n",
      "Iteration 374, loss = 19705415835.91918945\n",
      "Iteration 375, loss = 19705410839.41817856\n",
      "Iteration 376, loss = 19705405928.98291779\n",
      "Iteration 377, loss = 19705401004.89244461\n",
      "Iteration 378, loss = 19705396033.65403366\n",
      "Iteration 379, loss = 19705391094.30221939\n",
      "Iteration 380, loss = 19705386118.57448578\n",
      "Iteration 381, loss = 19705381206.54462051\n",
      "Iteration 382, loss = 19705376243.77068710\n",
      "Iteration 383, loss = 19705371293.67397308\n",
      "Iteration 384, loss = 19705366382.36796951\n",
      "Iteration 385, loss = 19705361419.81007767\n",
      "Iteration 386, loss = 19705356477.40500259\n",
      "Iteration 387, loss = 19705351563.03221512\n",
      "Iteration 388, loss = 19705346616.84597397\n",
      "Iteration 389, loss = 19705341639.69688797\n",
      "Iteration 390, loss = 19705336719.35628891\n",
      "Iteration 391, loss = 19705331782.93140793\n",
      "Iteration 392, loss = 19705326878.33940887\n",
      "Iteration 393, loss = 19705321915.00017166\n",
      "Iteration 394, loss = 19705316999.94808960\n",
      "Iteration 395, loss = 19705312053.82353210\n",
      "Iteration 396, loss = 19705307121.21570206\n",
      "Iteration 397, loss = 19705302206.28453827\n",
      "Iteration 398, loss = 19705297272.36046600\n",
      "Iteration 399, loss = 19705292343.10136414\n",
      "Iteration 400, loss = 19705287410.78484344\n",
      "Iteration 401, loss = 19705282490.01650238\n",
      "Iteration 402, loss = 19705277574.22503281\n",
      "Iteration 403, loss = 19705272634.65075684\n",
      "Iteration 404, loss = 19705267731.97774124\n",
      "Iteration 405, loss = 19705262780.22943497\n",
      "Iteration 406, loss = 19705257826.16997147\n",
      "Iteration 407, loss = 19705252947.29946899\n",
      "Iteration 408, loss = 19705247983.97963333\n",
      "Iteration 409, loss = 19705243070.56749344\n",
      "Iteration 410, loss = 19705238124.99816132\n",
      "Iteration 411, loss = 19705233223.56615829\n",
      "Iteration 412, loss = 19705228288.18458176\n",
      "Iteration 413, loss = 19705223326.21694183\n",
      "Iteration 414, loss = 19705218410.20051193\n",
      "Iteration 415, loss = 19705213465.21489716\n",
      "Iteration 416, loss = 19705208582.30091476\n",
      "Iteration 417, loss = 19705203614.76806259\n",
      "Iteration 418, loss = 19705198676.32865524\n",
      "Iteration 419, loss = 19705193738.24490738\n",
      "Iteration 420, loss = 19705188841.84806061\n",
      "Iteration 421, loss = 19705183902.41731644\n",
      "Iteration 422, loss = 19705178967.00050354\n",
      "Iteration 423, loss = 19705174030.78894043\n",
      "Iteration 424, loss = 19705169124.62520218\n",
      "Iteration 425, loss = 19705164197.59040070\n",
      "Iteration 426, loss = 19705159293.73804474\n",
      "Iteration 427, loss = 19705154369.56143188\n",
      "Iteration 428, loss = 19705149403.60220337\n",
      "Iteration 429, loss = 19705144521.68800735\n",
      "Iteration 430, loss = 19705139570.33373642\n",
      "Iteration 431, loss = 19705134644.44257355\n",
      "Iteration 432, loss = 19705129733.77388763\n",
      "Iteration 433, loss = 19705124759.51385880\n",
      "Iteration 434, loss = 19705119900.82567978\n",
      "Iteration 435, loss = 19705114925.54586792\n",
      "Iteration 436, loss = 19705110030.79140472\n",
      "Iteration 437, loss = 19705105104.36233902\n",
      "Iteration 438, loss = 19705100178.84530258\n",
      "Iteration 439, loss = 19705095284.47719574\n",
      "Iteration 440, loss = 19705090332.38933182\n",
      "Iteration 441, loss = 19705085414.98076248\n",
      "Iteration 442, loss = 19705080515.35440826\n",
      "Iteration 443, loss = 19705075583.55115509\n",
      "Iteration 444, loss = 19705070604.64672852\n",
      "Iteration 445, loss = 19705065762.78438187\n",
      "Iteration 446, loss = 19705060819.33190918\n",
      "Iteration 447, loss = 19705055890.73835373\n",
      "Iteration 448, loss = 19705050980.29484940\n",
      "Iteration 449, loss = 19705046037.21391678\n",
      "Iteration 450, loss = 19705041152.68872070\n",
      "Iteration 451, loss = 19705036210.77886200\n",
      "Iteration 452, loss = 19705031294.60152054\n",
      "Iteration 453, loss = 19705026401.91349030\n",
      "Iteration 454, loss = 19705021441.12941742\n",
      "Iteration 455, loss = 19705016589.95375824\n",
      "Iteration 456, loss = 19705011657.31796646\n",
      "Iteration 457, loss = 19705006738.66503143\n",
      "Iteration 458, loss = 19705001821.76033783\n",
      "Iteration 459, loss = 19704996929.62160492\n",
      "Iteration 460, loss = 19704991974.81591034\n",
      "Iteration 461, loss = 19704987118.66350937\n",
      "Iteration 462, loss = 19704982154.18238449\n",
      "Iteration 463, loss = 19704977291.95632553\n",
      "Iteration 464, loss = 19704972358.25341415\n",
      "Iteration 465, loss = 19704967452.91398621\n",
      "Iteration 466, loss = 19704962545.55280304\n",
      "Iteration 467, loss = 19704957632.90316772\n",
      "Iteration 468, loss = 19704952732.81671524\n",
      "Iteration 469, loss = 19704947801.88918304\n",
      "Iteration 470, loss = 19704942883.08295822\n",
      "Iteration 471, loss = 19704937962.82196426\n",
      "Iteration 472, loss = 19704933061.05222702\n",
      "Iteration 473, loss = 19704928176.33513260\n",
      "Iteration 474, loss = 19704923272.19751740\n",
      "Iteration 475, loss = 19704918346.90800476\n",
      "Iteration 476, loss = 19704913416.55475998\n",
      "Iteration 477, loss = 19704908520.31254959\n",
      "Iteration 478, loss = 19704903589.89236832\n",
      "Iteration 479, loss = 19704898681.39509964\n",
      "Iteration 480, loss = 19704893774.89622116\n",
      "Iteration 481, loss = 19704888861.17507935\n",
      "Iteration 482, loss = 19704883911.95498276\n",
      "Iteration 483, loss = 19704879039.39868927\n",
      "Iteration 484, loss = 19704874106.76873398\n",
      "Iteration 485, loss = 19704869194.61496353\n",
      "Iteration 486, loss = 19704864307.74835587\n",
      "Iteration 487, loss = 19704859399.28356934\n",
      "Iteration 488, loss = 19704854479.19974518\n",
      "Iteration 489, loss = 19704849588.04750061\n",
      "Iteration 490, loss = 19704844666.07854462\n",
      "Iteration 491, loss = 19704839765.99179459\n",
      "Iteration 492, loss = 19704834881.35688782\n",
      "Iteration 493, loss = 19704829963.53397751\n",
      "Iteration 494, loss = 19704825035.98911667\n",
      "Iteration 495, loss = 19704820164.70819855\n",
      "Iteration 496, loss = 19704815263.92266083\n",
      "Iteration 497, loss = 19704810320.43445587\n",
      "Iteration 498, loss = 19704805420.42063141\n",
      "Iteration 499, loss = 19704800528.58904648\n",
      "Iteration 500, loss = 19704795619.19241333\n",
      "Iteration 1, loss = 19632749543.82701874\n",
      "Iteration 2, loss = 19632728056.34615326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 19632705622.70125198\n",
      "Iteration 4, loss = 19632682898.50147629\n",
      "Iteration 5, loss = 19632661179.61519623\n",
      "Iteration 6, loss = 19632640427.31684494\n",
      "Iteration 7, loss = 19632619970.69192505\n",
      "Iteration 8, loss = 19632598062.50653458\n",
      "Iteration 9, loss = 19632574387.42738342\n",
      "Iteration 10, loss = 19632548713.63397980\n",
      "Iteration 11, loss = 19632522444.22057343\n",
      "Iteration 12, loss = 19632495640.76118851\n",
      "Iteration 13, loss = 19632467484.27603149\n",
      "Iteration 14, loss = 19632441824.33712387\n",
      "Iteration 15, loss = 19632416073.91521072\n",
      "Iteration 16, loss = 19632392313.82824707\n",
      "Iteration 17, loss = 19632369139.72177124\n",
      "Iteration 18, loss = 19632346690.42776489\n",
      "Iteration 19, loss = 19632326367.50830841\n",
      "Iteration 20, loss = 19632305374.28705978\n",
      "Iteration 21, loss = 19632281256.89511490\n",
      "Iteration 22, loss = 19632259704.01482773\n",
      "Iteration 23, loss = 19632242139.52097702\n",
      "Iteration 24, loss = 19632226667.65719223\n",
      "Iteration 25, loss = 19632210891.09212875\n",
      "Iteration 26, loss = 19632194490.78774261\n",
      "Iteration 27, loss = 19632177084.32078934\n",
      "Iteration 28, loss = 19632160241.64909744\n",
      "Iteration 29, loss = 19632142609.10308456\n",
      "Iteration 30, loss = 19632127963.81613922\n",
      "Iteration 31, loss = 19632116946.05165482\n",
      "Iteration 32, loss = 19632106433.94919205\n",
      "Iteration 33, loss = 19632095763.91658401\n",
      "Iteration 34, loss = 19632085157.39583969\n",
      "Iteration 35, loss = 19632075596.15042877\n",
      "Iteration 36, loss = 19632066333.74825668\n",
      "Iteration 37, loss = 19632057142.28600693\n",
      "Iteration 38, loss = 19632048193.75532913\n",
      "Iteration 39, loss = 19632039330.54919815\n",
      "Iteration 40, loss = 19632030573.27176285\n",
      "Iteration 41, loss = 19632021935.85260010\n",
      "Iteration 42, loss = 19632013329.53539276\n",
      "Iteration 43, loss = 19632004853.09551239\n",
      "Iteration 44, loss = 19631996523.24952698\n",
      "Iteration 45, loss = 19631988170.75166321\n",
      "Iteration 46, loss = 19631979883.54731750\n",
      "Iteration 47, loss = 19631971616.33015823\n",
      "Iteration 48, loss = 19631963169.53358459\n",
      "Iteration 49, loss = 19631954944.47303391\n",
      "Iteration 50, loss = 19631946838.68368912\n",
      "Iteration 51, loss = 19631938860.66906738\n",
      "Iteration 52, loss = 19631930969.43527603\n",
      "Iteration 53, loss = 19631923029.29492950\n",
      "Iteration 54, loss = 19631915221.57522583\n",
      "Iteration 55, loss = 19631907447.87449646\n",
      "Iteration 56, loss = 19631899584.21582794\n",
      "Iteration 57, loss = 19631891814.71258163\n",
      "Iteration 58, loss = 19631883944.26413727\n",
      "Iteration 59, loss = 19631875878.95766449\n",
      "Iteration 60, loss = 19631868111.92144775\n",
      "Iteration 61, loss = 19631860407.18278503\n",
      "Iteration 62, loss = 19631852714.33917236\n",
      "Iteration 63, loss = 19631845061.24855804\n",
      "Iteration 64, loss = 19631837438.81104279\n",
      "Iteration 65, loss = 19631829793.83557129\n",
      "Iteration 66, loss = 19631822226.35463715\n",
      "Iteration 67, loss = 19631814615.54000473\n",
      "Iteration 68, loss = 19631807074.67190552\n",
      "Iteration 69, loss = 19631799473.71457672\n",
      "Iteration 70, loss = 19631791829.78331375\n",
      "Iteration 71, loss = 19631784232.04694366\n",
      "Iteration 72, loss = 19631776526.30862045\n",
      "Iteration 73, loss = 19631768794.80810165\n",
      "Iteration 74, loss = 19631761105.66709137\n",
      "Iteration 75, loss = 19631753372.06755829\n",
      "Iteration 76, loss = 19631745638.49066162\n",
      "Iteration 77, loss = 19631737932.04285431\n",
      "Iteration 78, loss = 19631730183.82226181\n",
      "Iteration 79, loss = 19631722501.73168945\n",
      "Iteration 80, loss = 19631714833.98199844\n",
      "Iteration 81, loss = 19631707140.57337189\n",
      "Iteration 82, loss = 19631699506.80426407\n",
      "Iteration 83, loss = 19631691921.55095291\n",
      "Iteration 84, loss = 19631684330.28885269\n",
      "Iteration 85, loss = 19631676686.49679947\n",
      "Iteration 86, loss = 19631669138.56270218\n",
      "Iteration 87, loss = 19631661532.95135880\n",
      "Iteration 88, loss = 19631654009.69608307\n",
      "Iteration 89, loss = 19631646489.85824585\n",
      "Iteration 90, loss = 19631639086.73554611\n",
      "Iteration 91, loss = 19631631556.61937332\n",
      "Iteration 92, loss = 19631624124.71377945\n",
      "Iteration 93, loss = 19631616715.70172501\n",
      "Iteration 94, loss = 19631609289.81523132\n",
      "Iteration 95, loss = 19631601878.91463089\n",
      "Iteration 96, loss = 19631594490.91722870\n",
      "Iteration 97, loss = 19631587063.49591064\n",
      "Iteration 98, loss = 19631579640.62549591\n",
      "Iteration 99, loss = 19631572230.59227371\n",
      "Iteration 100, loss = 19631564854.54927063\n",
      "Iteration 101, loss = 19631557466.38409805\n",
      "Iteration 102, loss = 19631550148.34311676\n",
      "Iteration 103, loss = 19631542781.24237442\n",
      "Iteration 104, loss = 19631535444.96283340\n",
      "Iteration 105, loss = 19631528105.13495255\n",
      "Iteration 106, loss = 19631520785.37424469\n",
      "Iteration 107, loss = 19631513441.06894302\n",
      "Iteration 108, loss = 19631506176.83468246\n",
      "Iteration 109, loss = 19631498924.71949768\n",
      "Iteration 110, loss = 19631491660.79273224\n",
      "Iteration 111, loss = 19631484449.73791504\n",
      "Iteration 112, loss = 19631477223.63616943\n",
      "Iteration 113, loss = 19631469986.08288193\n",
      "Iteration 114, loss = 19631462788.16205978\n",
      "Iteration 115, loss = 19631455543.45795441\n",
      "Iteration 116, loss = 19631448265.13405991\n",
      "Iteration 117, loss = 19631441103.88608551\n",
      "Iteration 118, loss = 19631433892.76263809\n",
      "Iteration 119, loss = 19631426639.82358932\n",
      "Iteration 120, loss = 19631419490.32377625\n",
      "Iteration 121, loss = 19631412344.44775772\n",
      "Iteration 122, loss = 19631405134.12898636\n",
      "Iteration 123, loss = 19631398013.53091049\n",
      "Iteration 124, loss = 19631390765.03578949\n",
      "Iteration 125, loss = 19631383623.53334808\n",
      "Iteration 126, loss = 19631376438.80529022\n",
      "Iteration 127, loss = 19631369260.50573349\n",
      "Iteration 128, loss = 19631362158.70843124\n",
      "Iteration 129, loss = 19631355014.98051071\n",
      "Iteration 130, loss = 19631347834.38538361\n",
      "Iteration 131, loss = 19631340786.47551727\n",
      "Iteration 132, loss = 19631333613.78934860\n",
      "Iteration 133, loss = 19631326647.06226349\n",
      "Iteration 134, loss = 19631319565.32779694\n",
      "Iteration 135, loss = 19631312478.55994797\n",
      "Iteration 136, loss = 19631305435.47920227\n",
      "Iteration 137, loss = 19631298389.69468307\n",
      "Iteration 138, loss = 19631291273.97956848\n",
      "Iteration 139, loss = 19631284279.62792587\n",
      "Iteration 140, loss = 19631277133.22768402\n",
      "Iteration 141, loss = 19631270004.47537231\n",
      "Iteration 142, loss = 19631262906.45557404\n",
      "Iteration 143, loss = 19631255866.52710342\n",
      "Iteration 144, loss = 19631248738.60872269\n",
      "Iteration 145, loss = 19631241681.27521133\n",
      "Iteration 146, loss = 19631234606.05290222\n",
      "Iteration 147, loss = 19631227581.59601212\n",
      "Iteration 148, loss = 19631220518.41621017\n",
      "Iteration 149, loss = 19631213391.78384399\n",
      "Iteration 150, loss = 19631206355.44861221\n",
      "Iteration 151, loss = 19631199299.97745132\n",
      "Iteration 152, loss = 19631192287.00574875\n",
      "Iteration 153, loss = 19631185234.91936111\n",
      "Iteration 154, loss = 19631178231.57414627\n",
      "Iteration 155, loss = 19631171203.60796356\n",
      "Iteration 156, loss = 19631164197.66475296\n",
      "Iteration 157, loss = 19631157129.12669754\n",
      "Iteration 158, loss = 19631149973.34816742\n",
      "Iteration 159, loss = 19631142986.53968430\n",
      "Iteration 160, loss = 19631135992.45879364\n",
      "Iteration 161, loss = 19631129028.47717285\n",
      "Iteration 162, loss = 19631121991.11464691\n",
      "Iteration 163, loss = 19631115046.32458496\n",
      "Iteration 164, loss = 19631108023.81444168\n",
      "Iteration 165, loss = 19631101052.60533905\n",
      "Iteration 166, loss = 19631094098.79507446\n",
      "Iteration 167, loss = 19631087044.76659012\n",
      "Iteration 168, loss = 19631080096.93069458\n",
      "Iteration 169, loss = 19631073013.43844223\n",
      "Iteration 170, loss = 19631066075.79780579\n",
      "Iteration 171, loss = 19631059131.15019608\n",
      "Iteration 172, loss = 19631052162.88622284\n",
      "Iteration 173, loss = 19631045211.41341400\n",
      "Iteration 174, loss = 19631038260.87446976\n",
      "Iteration 175, loss = 19631031167.39611435\n",
      "Iteration 176, loss = 19631024206.11431122\n",
      "Iteration 177, loss = 19631017203.87891769\n",
      "Iteration 178, loss = 19631010214.79560089\n",
      "Iteration 179, loss = 19631003240.32114792\n",
      "Iteration 180, loss = 19630996272.37454987\n",
      "Iteration 181, loss = 19630989338.03268051\n",
      "Iteration 182, loss = 19630982414.88680649\n",
      "Iteration 183, loss = 19630975470.91032028\n",
      "Iteration 184, loss = 19630968504.62051010\n",
      "Iteration 185, loss = 19630961596.66661453\n",
      "Iteration 186, loss = 19630954585.76979065\n",
      "Iteration 187, loss = 19630947697.57394409\n",
      "Iteration 188, loss = 19630940719.45146179\n",
      "Iteration 189, loss = 19630933765.82884979\n",
      "Iteration 190, loss = 19630926843.21083069\n",
      "Iteration 191, loss = 19630919900.49615097\n",
      "Iteration 192, loss = 19630912921.88913727\n",
      "Iteration 193, loss = 19630905957.09762573\n",
      "Iteration 194, loss = 19630899017.50553513\n",
      "Iteration 195, loss = 19630892038.50340652\n",
      "Iteration 196, loss = 19630885109.66061783\n",
      "Iteration 197, loss = 19630878086.63325882\n",
      "Iteration 198, loss = 19630871126.09067917\n",
      "Iteration 199, loss = 19630864143.19364929\n",
      "Iteration 200, loss = 19630857093.01077271\n",
      "Iteration 201, loss = 19630850056.29655075\n",
      "Iteration 202, loss = 19630842928.05425644\n",
      "Iteration 203, loss = 19630835735.07427979\n",
      "Iteration 204, loss = 19630828420.64614868\n",
      "Iteration 205, loss = 19630820964.42859650\n",
      "Iteration 206, loss = 19630813358.44646835\n",
      "Iteration 207, loss = 19630805577.32241058\n",
      "Iteration 208, loss = 19630797575.42426300\n",
      "Iteration 209, loss = 19630789529.63700104\n",
      "Iteration 210, loss = 19630781411.73672867\n",
      "Iteration 211, loss = 19630773360.61786652\n",
      "Iteration 212, loss = 19630765290.51201248\n",
      "Iteration 213, loss = 19630757319.54157639\n",
      "Iteration 214, loss = 19630749463.82971954\n",
      "Iteration 215, loss = 19630741539.63388443\n",
      "Iteration 216, loss = 19630733630.93885422\n",
      "Iteration 217, loss = 19630725880.95497131\n",
      "Iteration 218, loss = 19630718217.52182007\n",
      "Iteration 219, loss = 19630710557.25069809\n",
      "Iteration 220, loss = 19630702945.67077255\n",
      "Iteration 221, loss = 19630695385.58452988\n",
      "Iteration 222, loss = 19630687836.63423157\n",
      "Iteration 223, loss = 19630680323.07901001\n",
      "Iteration 224, loss = 19630672790.76245117\n",
      "Iteration 225, loss = 19630665325.41790390\n",
      "Iteration 226, loss = 19630657832.65313339\n",
      "Iteration 227, loss = 19630650416.99591446\n",
      "Iteration 228, loss = 19630642941.81936264\n",
      "Iteration 229, loss = 19630635573.27804184\n",
      "Iteration 230, loss = 19630628135.20366669\n",
      "Iteration 231, loss = 19630620746.66631699\n",
      "Iteration 232, loss = 19630613402.00885773\n",
      "Iteration 233, loss = 19630606037.06611633\n",
      "Iteration 234, loss = 19630598707.12205124\n",
      "Iteration 235, loss = 19630591385.81927109\n",
      "Iteration 236, loss = 19630584115.47529221\n",
      "Iteration 237, loss = 19630576739.05803299\n",
      "Iteration 238, loss = 19630569468.28819656\n",
      "Iteration 239, loss = 19630562134.93772888\n",
      "Iteration 240, loss = 19630554832.61700439\n",
      "Iteration 241, loss = 19630547512.42442322\n",
      "Iteration 242, loss = 19630540242.82475662\n",
      "Iteration 243, loss = 19630532903.70582962\n",
      "Iteration 244, loss = 19630525617.73756790\n",
      "Iteration 245, loss = 19630518332.66189575\n",
      "Iteration 246, loss = 19630511094.93529510\n",
      "Iteration 247, loss = 19630503832.54627228\n",
      "Iteration 248, loss = 19630496549.75839996\n",
      "Iteration 249, loss = 19630489314.31791306\n",
      "Iteration 250, loss = 19630482052.31110764\n",
      "Iteration 251, loss = 19630474827.86569595\n",
      "Iteration 252, loss = 19630467514.81075287\n",
      "Iteration 253, loss = 19630460302.37659836\n",
      "Iteration 254, loss = 19630453012.94778824\n",
      "Iteration 255, loss = 19630445832.01243973\n",
      "Iteration 256, loss = 19630438636.76042557\n",
      "Iteration 257, loss = 19630431488.05486679\n",
      "Iteration 258, loss = 19630424228.99711227\n",
      "Iteration 259, loss = 19630417045.66378403\n",
      "Iteration 260, loss = 19630409848.15050125\n",
      "Iteration 261, loss = 19630402691.23698044\n",
      "Iteration 262, loss = 19630395458.84391022\n",
      "Iteration 263, loss = 19630388283.90085220\n",
      "Iteration 264, loss = 19630381045.28279495\n",
      "Iteration 265, loss = 19630373836.02154160\n",
      "Iteration 266, loss = 19630366594.60241318\n",
      "Iteration 267, loss = 19630359446.04150009\n",
      "Iteration 268, loss = 19630352206.40482712\n",
      "Iteration 269, loss = 19630344984.28723907\n",
      "Iteration 270, loss = 19630337803.84767151\n",
      "Iteration 271, loss = 19630330616.14465332\n",
      "Iteration 272, loss = 19630323415.39298248\n",
      "Iteration 273, loss = 19630316314.74027252\n",
      "Iteration 274, loss = 19630309167.43386078\n",
      "Iteration 275, loss = 19630301940.50319672\n",
      "Iteration 276, loss = 19630294835.56811905\n",
      "Iteration 277, loss = 19630287639.59484863\n",
      "Iteration 278, loss = 19630280484.78099060\n",
      "Iteration 279, loss = 19630273246.32414246\n",
      "Iteration 280, loss = 19630265958.49608994\n",
      "Iteration 281, loss = 19630258700.99139786\n",
      "Iteration 282, loss = 19630251411.99705887\n",
      "Iteration 283, loss = 19630243993.99670029\n",
      "Iteration 284, loss = 19630236388.28308105\n",
      "Iteration 285, loss = 19630228572.29572678\n",
      "Iteration 286, loss = 19630220386.10198212\n",
      "Iteration 287, loss = 19630211879.48538208\n",
      "Iteration 288, loss = 19630203187.05158997\n",
      "Iteration 289, loss = 19630194526.49002457\n",
      "Iteration 290, loss = 19630185979.80032730\n",
      "Iteration 291, loss = 19630177482.93936920\n",
      "Iteration 292, loss = 19630169178.24450684\n",
      "Iteration 293, loss = 19630160889.54466629\n",
      "Iteration 294, loss = 19630152745.04040146\n",
      "Iteration 295, loss = 19630144586.04874420\n",
      "Iteration 296, loss = 19630136564.34203720\n",
      "Iteration 297, loss = 19630128540.62020493\n",
      "Iteration 298, loss = 19630120556.20709991\n",
      "Iteration 299, loss = 19630112565.95950317\n",
      "Iteration 300, loss = 19630104654.49613953\n",
      "Iteration 301, loss = 19630096742.27571869\n",
      "Iteration 302, loss = 19630088901.26922989\n",
      "Iteration 303, loss = 19630081115.42545700\n",
      "Iteration 304, loss = 19630073341.71405029\n",
      "Iteration 305, loss = 19630065691.92819977\n",
      "Iteration 306, loss = 19630057977.46666718\n",
      "Iteration 307, loss = 19630050229.04021835\n",
      "Iteration 308, loss = 19630042626.06903839\n",
      "Iteration 309, loss = 19630034917.67681122\n",
      "Iteration 310, loss = 19630027254.69938278\n",
      "Iteration 311, loss = 19630019645.49288177\n",
      "Iteration 312, loss = 19630012009.53964996\n",
      "Iteration 313, loss = 19630004414.33767700\n",
      "Iteration 314, loss = 19629996711.07497025\n",
      "Iteration 315, loss = 19629989099.47636414\n",
      "Iteration 316, loss = 19629981412.74660110\n",
      "Iteration 317, loss = 19629973786.03542328\n",
      "Iteration 318, loss = 19629966216.36570358\n",
      "Iteration 319, loss = 19629958619.42148972\n",
      "Iteration 320, loss = 19629951028.93589020\n",
      "Iteration 321, loss = 19629943505.89801025\n",
      "Iteration 322, loss = 19629935960.54059219\n",
      "Iteration 323, loss = 19629928443.41081238\n",
      "Iteration 324, loss = 19629920822.61663055\n",
      "Iteration 325, loss = 19629913247.43808746\n",
      "Iteration 326, loss = 19629905713.42400742\n",
      "Iteration 327, loss = 19629898205.02702332\n",
      "Iteration 328, loss = 19629890577.27975464\n",
      "Iteration 329, loss = 19629883091.72947693\n",
      "Iteration 330, loss = 19629875581.83044815\n",
      "Iteration 331, loss = 19629868057.82113647\n",
      "Iteration 332, loss = 19629860560.19166946\n",
      "Iteration 333, loss = 19629853097.69697571\n",
      "Iteration 334, loss = 19629845526.60344315\n",
      "Iteration 335, loss = 19629838074.26253891\n",
      "Iteration 336, loss = 19629830626.14566803\n",
      "Iteration 337, loss = 19629823096.15024567\n",
      "Iteration 338, loss = 19629815650.16518784\n",
      "Iteration 339, loss = 19629808226.00897980\n",
      "Iteration 340, loss = 19629800762.83279800\n",
      "Iteration 341, loss = 19629793324.06488037\n",
      "Iteration 342, loss = 19629785835.81170654\n",
      "Iteration 343, loss = 19629778406.15746307\n",
      "Iteration 344, loss = 19629770976.21332550\n",
      "Iteration 345, loss = 19629763528.99098206\n",
      "Iteration 346, loss = 19629756122.42696381\n",
      "Iteration 347, loss = 19629748722.98936081\n",
      "Iteration 348, loss = 19629741306.49487305\n",
      "Iteration 349, loss = 19629733912.95722961\n",
      "Iteration 350, loss = 19629726524.49452209\n",
      "Iteration 351, loss = 19629719157.20397568\n",
      "Iteration 352, loss = 19629711758.21611786\n",
      "Iteration 353, loss = 19629704343.89944077\n",
      "Iteration 354, loss = 19629696939.70960617\n",
      "Iteration 355, loss = 19629689558.65548706\n",
      "Iteration 356, loss = 19629682160.41201019\n",
      "Iteration 357, loss = 19629674754.87265396\n",
      "Iteration 358, loss = 19629667377.15293503\n",
      "Iteration 359, loss = 19629660047.86117172\n",
      "Iteration 360, loss = 19629652664.52470016\n",
      "Iteration 361, loss = 19629645264.59902191\n",
      "Iteration 362, loss = 19629637941.64674759\n",
      "Iteration 363, loss = 19629630535.97709656\n",
      "Iteration 364, loss = 19629623182.51844406\n",
      "Iteration 365, loss = 19629615758.98224640\n",
      "Iteration 366, loss = 19629608402.44408417\n",
      "Iteration 367, loss = 19629601081.26253891\n",
      "Iteration 368, loss = 19629593708.35802841\n",
      "Iteration 369, loss = 19629586326.08953476\n",
      "Iteration 370, loss = 19629578923.70737076\n",
      "Iteration 371, loss = 19629571574.54077530\n",
      "Iteration 372, loss = 19629564169.35290909\n",
      "Iteration 373, loss = 19629556879.83971405\n",
      "Iteration 374, loss = 19629549501.66724396\n",
      "Iteration 375, loss = 19629542105.45780182\n",
      "Iteration 376, loss = 19629534777.90503693\n",
      "Iteration 377, loss = 19629527306.47099686\n",
      "Iteration 378, loss = 19629519947.13924789\n",
      "Iteration 379, loss = 19629512553.18646240\n",
      "Iteration 380, loss = 19629505246.29841995\n",
      "Iteration 381, loss = 19629497873.85360336\n",
      "Iteration 382, loss = 19629490663.48202133\n",
      "Iteration 383, loss = 19629483257.95560455\n",
      "Iteration 384, loss = 19629475924.38661194\n",
      "Iteration 385, loss = 19629468582.90596771\n",
      "Iteration 386, loss = 19629461264.23040009\n",
      "Iteration 387, loss = 19629453920.56776047\n",
      "Iteration 388, loss = 19629446617.03937912\n",
      "Iteration 389, loss = 19629439302.30821609\n",
      "Iteration 390, loss = 19629431938.70659637\n",
      "Iteration 391, loss = 19629424638.57318115\n",
      "Iteration 392, loss = 19629417295.04387283\n",
      "Iteration 393, loss = 19629409945.14013672\n",
      "Iteration 394, loss = 19629402616.31940079\n",
      "Iteration 395, loss = 19629395314.45327377\n",
      "Iteration 396, loss = 19629387984.48774719\n",
      "Iteration 397, loss = 19629380679.62701035\n",
      "Iteration 398, loss = 19629373363.74172592\n",
      "Iteration 399, loss = 19629366098.80900574\n",
      "Iteration 400, loss = 19629358781.40229034\n",
      "Iteration 401, loss = 19629351437.68745422\n",
      "Iteration 402, loss = 19629344113.35865784\n",
      "Iteration 403, loss = 19629336872.05403519\n",
      "Iteration 404, loss = 19629329527.05607224\n",
      "Iteration 405, loss = 19629322185.33676147\n",
      "Iteration 406, loss = 19629314936.91146851\n",
      "Iteration 407, loss = 19629307627.64844131\n",
      "Iteration 408, loss = 19629300314.11934662\n",
      "Iteration 409, loss = 19629293071.66255569\n",
      "Iteration 410, loss = 19629285728.50267029\n",
      "Iteration 411, loss = 19629278437.60668564\n",
      "Iteration 412, loss = 19629271119.38028717\n",
      "Iteration 413, loss = 19629263813.64578247\n",
      "Iteration 414, loss = 19629256522.68790817\n",
      "Iteration 415, loss = 19629249218.36170197\n",
      "Iteration 416, loss = 19629241892.38745117\n",
      "Iteration 417, loss = 19629234579.86978912\n",
      "Iteration 418, loss = 19629227303.04416275\n",
      "Iteration 419, loss = 19629220031.47515106\n",
      "Iteration 420, loss = 19629212746.70844269\n",
      "Iteration 421, loss = 19629205501.81594086\n",
      "Iteration 422, loss = 19629198271.96147156\n",
      "Iteration 423, loss = 19629191014.85615921\n",
      "Iteration 424, loss = 19629183710.03455734\n",
      "Iteration 425, loss = 19629176490.96144485\n",
      "Iteration 426, loss = 19629169186.02500916\n",
      "Iteration 427, loss = 19629161927.13231659\n",
      "Iteration 428, loss = 19629154623.78480530\n",
      "Iteration 429, loss = 19629147318.64541245\n",
      "Iteration 430, loss = 19629140055.18490982\n",
      "Iteration 431, loss = 19629132757.82396698\n",
      "Iteration 432, loss = 19629125519.60895920\n",
      "Iteration 433, loss = 19629118279.92906570\n",
      "Iteration 434, loss = 19629111051.82259369\n",
      "Iteration 435, loss = 19629103753.42167664\n",
      "Iteration 436, loss = 19629096543.32912827\n",
      "Iteration 437, loss = 19629089337.12297440\n",
      "Iteration 438, loss = 19629082045.43470383\n",
      "Iteration 439, loss = 19629074764.32398987\n",
      "Iteration 440, loss = 19629067502.73686600\n",
      "Iteration 441, loss = 19629060248.96690369\n",
      "Iteration 442, loss = 19629052981.24705887\n",
      "Iteration 443, loss = 19629045741.84633636\n",
      "Iteration 444, loss = 19629038476.53965759\n",
      "Iteration 445, loss = 19629031229.43357849\n",
      "Iteration 446, loss = 19629024018.51352692\n",
      "Iteration 447, loss = 19629016811.64870071\n",
      "Iteration 448, loss = 19629009629.54670715\n",
      "Iteration 449, loss = 19629002438.62532806\n",
      "Iteration 450, loss = 19628995224.15329742\n",
      "Iteration 451, loss = 19628987976.42297745\n",
      "Iteration 452, loss = 19628980723.04463577\n",
      "Iteration 453, loss = 19628973500.00091553\n",
      "Iteration 454, loss = 19628966271.05141830\n",
      "Iteration 455, loss = 19628959037.35723114\n",
      "Iteration 456, loss = 19628951818.56177139\n",
      "Iteration 457, loss = 19628944557.59393311\n",
      "Iteration 458, loss = 19628937395.25024414\n",
      "Iteration 459, loss = 19628930121.15912247\n",
      "Iteration 460, loss = 19628922918.69749451\n",
      "Iteration 461, loss = 19628915673.91532898\n",
      "Iteration 462, loss = 19628908421.25542831\n",
      "Iteration 463, loss = 19628901202.55333710\n",
      "Iteration 464, loss = 19628893876.13404846\n",
      "Iteration 465, loss = 19628886652.95839310\n",
      "Iteration 466, loss = 19628879379.01406860\n",
      "Iteration 467, loss = 19628872152.20439148\n",
      "Iteration 468, loss = 19628864896.14910507\n",
      "Iteration 469, loss = 19628857653.56832123\n",
      "Iteration 470, loss = 19628850394.11343384\n",
      "Iteration 471, loss = 19628843202.34818649\n",
      "Iteration 472, loss = 19628835941.13722229\n",
      "Iteration 473, loss = 19628828688.22626114\n",
      "Iteration 474, loss = 19628821429.46228790\n",
      "Iteration 475, loss = 19628814185.93482971\n",
      "Iteration 476, loss = 19628806938.75485611\n",
      "Iteration 477, loss = 19628799658.56793213\n",
      "Iteration 478, loss = 19628792486.75311661\n",
      "Iteration 479, loss = 19628785192.64175034\n",
      "Iteration 480, loss = 19628777968.72868347\n",
      "Iteration 481, loss = 19628770742.07887650\n",
      "Iteration 482, loss = 19628763543.33168030\n",
      "Iteration 483, loss = 19628756341.68230820\n",
      "Iteration 484, loss = 19628749212.99236679\n",
      "Iteration 485, loss = 19628742029.12107849\n",
      "Iteration 486, loss = 19628734762.54732132\n",
      "Iteration 487, loss = 19628727580.07438278\n",
      "Iteration 488, loss = 19628720360.13076401\n",
      "Iteration 489, loss = 19628713108.62308121\n",
      "Iteration 490, loss = 19628705876.94583893\n",
      "Iteration 491, loss = 19628698646.33544540\n",
      "Iteration 492, loss = 19628691467.98909760\n",
      "Iteration 493, loss = 19628684247.17824554\n",
      "Iteration 494, loss = 19628677005.70615005\n",
      "Iteration 495, loss = 19628669824.51489258\n",
      "Iteration 496, loss = 19628662529.14794540\n",
      "Iteration 497, loss = 19628655325.06869507\n",
      "Iteration 498, loss = 19628648073.53591537\n",
      "Iteration 499, loss = 19628640842.47004318\n",
      "Iteration 500, loss = 19628633614.15398026\n",
      "Iteration 1, loss = 19579359157.13061523\n",
      "Iteration 2, loss = 19579336039.05994034\n",
      "Iteration 3, loss = 19579312772.80260849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 19579289663.79628754\n",
      "Iteration 5, loss = 19579266638.71693802\n",
      "Iteration 6, loss = 19579244455.08314514\n",
      "Iteration 7, loss = 19579222542.44089508\n",
      "Iteration 8, loss = 19579200345.25845718\n",
      "Iteration 9, loss = 19579177542.15697098\n",
      "Iteration 10, loss = 19579154921.91152954\n",
      "Iteration 11, loss = 19579130691.20666504\n",
      "Iteration 12, loss = 19579108718.25055695\n",
      "Iteration 13, loss = 19579087966.96644211\n",
      "Iteration 14, loss = 19579065398.12862015\n",
      "Iteration 15, loss = 19579039254.73528290\n",
      "Iteration 16, loss = 19579012558.21730042\n",
      "Iteration 17, loss = 19578988691.15987778\n",
      "Iteration 18, loss = 19578967163.14011765\n",
      "Iteration 19, loss = 19578943627.61467361\n",
      "Iteration 20, loss = 19578920542.17760468\n",
      "Iteration 21, loss = 19578897742.53639221\n",
      "Iteration 22, loss = 19578875430.18182373\n",
      "Iteration 23, loss = 19578857038.81950378\n",
      "Iteration 24, loss = 19578840556.95048141\n",
      "Iteration 25, loss = 19578825846.15983963\n",
      "Iteration 26, loss = 19578811398.20244217\n",
      "Iteration 27, loss = 19578795907.90987396\n",
      "Iteration 28, loss = 19578780143.49238968\n",
      "Iteration 29, loss = 19578767316.58723068\n",
      "Iteration 30, loss = 19578755170.22965622\n",
      "Iteration 31, loss = 19578743545.75014877\n",
      "Iteration 32, loss = 19578732753.54621124\n",
      "Iteration 33, loss = 19578721720.70388031\n",
      "Iteration 34, loss = 19578710860.13472748\n",
      "Iteration 35, loss = 19578700639.68883896\n",
      "Iteration 36, loss = 19578691365.33922195\n",
      "Iteration 37, loss = 19578682264.92825699\n",
      "Iteration 38, loss = 19578673322.92953110\n",
      "Iteration 39, loss = 19578664399.88022995\n",
      "Iteration 40, loss = 19578655640.29884338\n",
      "Iteration 41, loss = 19578646887.67946625\n",
      "Iteration 42, loss = 19578638215.66274643\n",
      "Iteration 43, loss = 19578629547.62092590\n",
      "Iteration 44, loss = 19578620945.85447693\n",
      "Iteration 45, loss = 19578612415.78988647\n",
      "Iteration 46, loss = 19578603849.56460190\n",
      "Iteration 47, loss = 19578595328.46432877\n",
      "Iteration 48, loss = 19578586901.60078812\n",
      "Iteration 49, loss = 19578578460.45479584\n",
      "Iteration 50, loss = 19578570130.36020279\n",
      "Iteration 51, loss = 19578561791.64159012\n",
      "Iteration 52, loss = 19578553599.65229034\n",
      "Iteration 53, loss = 19578545376.95896912\n",
      "Iteration 54, loss = 19578537027.11594391\n",
      "Iteration 55, loss = 19578528339.83171082\n",
      "Iteration 56, loss = 19578519871.83786774\n",
      "Iteration 57, loss = 19578511799.39897537\n",
      "Iteration 58, loss = 19578503787.34748459\n",
      "Iteration 59, loss = 19578495796.52790451\n",
      "Iteration 60, loss = 19578487836.96427536\n",
      "Iteration 61, loss = 19578479821.60236359\n",
      "Iteration 62, loss = 19578471856.24932098\n",
      "Iteration 63, loss = 19578463780.77803802\n",
      "Iteration 64, loss = 19578455786.37417221\n",
      "Iteration 65, loss = 19578447677.92365265\n",
      "Iteration 66, loss = 19578439548.74617767\n",
      "Iteration 67, loss = 19578431431.41259384\n",
      "Iteration 68, loss = 19578423340.19619751\n",
      "Iteration 69, loss = 19578415233.52712631\n",
      "Iteration 70, loss = 19578407211.58840561\n",
      "Iteration 71, loss = 19578399094.04604340\n",
      "Iteration 72, loss = 19578390987.87387085\n",
      "Iteration 73, loss = 19578382941.75868225\n",
      "Iteration 74, loss = 19578375035.73053741\n",
      "Iteration 75, loss = 19578367108.86639404\n",
      "Iteration 76, loss = 19578358888.31698608\n",
      "Iteration 77, loss = 19578350831.40507126\n",
      "Iteration 78, loss = 19578343069.86703491\n",
      "Iteration 79, loss = 19578335363.56297302\n",
      "Iteration 80, loss = 19578327633.21083069\n",
      "Iteration 81, loss = 19578319982.42297363\n",
      "Iteration 82, loss = 19578312367.34363937\n",
      "Iteration 83, loss = 19578304758.32518387\n",
      "Iteration 84, loss = 19578297198.35029984\n",
      "Iteration 85, loss = 19578289691.11949158\n",
      "Iteration 86, loss = 19578282166.13071060\n",
      "Iteration 87, loss = 19578274682.55146027\n",
      "Iteration 88, loss = 19578267170.93800735\n",
      "Iteration 89, loss = 19578259721.56118774\n",
      "Iteration 90, loss = 19578252242.02156830\n",
      "Iteration 91, loss = 19578244824.07603073\n",
      "Iteration 92, loss = 19578237312.89076614\n",
      "Iteration 93, loss = 19578229867.50350952\n",
      "Iteration 94, loss = 19578222393.85908890\n",
      "Iteration 95, loss = 19578214881.87609100\n",
      "Iteration 96, loss = 19578207296.64335251\n",
      "Iteration 97, loss = 19578199659.56468201\n",
      "Iteration 98, loss = 19578192002.82994080\n",
      "Iteration 99, loss = 19578184194.22124100\n",
      "Iteration 100, loss = 19578176314.97579193\n",
      "Iteration 101, loss = 19578168473.89125824\n",
      "Iteration 102, loss = 19578160545.98215103\n",
      "Iteration 103, loss = 19578152547.46911621\n",
      "Iteration 104, loss = 19578144620.08377457\n",
      "Iteration 105, loss = 19578136756.43783951\n",
      "Iteration 106, loss = 19578128847.41703415\n",
      "Iteration 107, loss = 19578121003.55107880\n",
      "Iteration 108, loss = 19578113212.89529419\n",
      "Iteration 109, loss = 19578105465.36587906\n",
      "Iteration 110, loss = 19578097717.03045654\n",
      "Iteration 111, loss = 19578089985.88000488\n",
      "Iteration 112, loss = 19578082351.06737900\n",
      "Iteration 113, loss = 19578074679.77850723\n",
      "Iteration 114, loss = 19578067061.79983139\n",
      "Iteration 115, loss = 19578059485.78720474\n",
      "Iteration 116, loss = 19578051896.49987411\n",
      "Iteration 117, loss = 19578044216.05397415\n",
      "Iteration 118, loss = 19578036481.86521530\n",
      "Iteration 119, loss = 19578028981.74314880\n",
      "Iteration 120, loss = 19578021414.51010513\n",
      "Iteration 121, loss = 19578013933.42503357\n",
      "Iteration 122, loss = 19578006448.82409668\n",
      "Iteration 123, loss = 19577999017.24676514\n",
      "Iteration 124, loss = 19577991561.14712906\n",
      "Iteration 125, loss = 19577984154.42156219\n",
      "Iteration 126, loss = 19577976736.75744629\n",
      "Iteration 127, loss = 19577969293.15700912\n",
      "Iteration 128, loss = 19577961951.03409958\n",
      "Iteration 129, loss = 19577954560.65256500\n",
      "Iteration 130, loss = 19577947179.20007324\n",
      "Iteration 131, loss = 19577939833.60526657\n",
      "Iteration 132, loss = 19577932497.37837601\n",
      "Iteration 133, loss = 19577925211.89828873\n",
      "Iteration 134, loss = 19577917864.32380295\n",
      "Iteration 135, loss = 19577910592.39147186\n",
      "Iteration 136, loss = 19577903234.58018112\n",
      "Iteration 137, loss = 19577895982.14978409\n",
      "Iteration 138, loss = 19577888733.76258850\n",
      "Iteration 139, loss = 19577881419.10683823\n",
      "Iteration 140, loss = 19577874203.22706985\n",
      "Iteration 141, loss = 19577866929.84938812\n",
      "Iteration 142, loss = 19577859651.53214264\n",
      "Iteration 143, loss = 19577852429.79170990\n",
      "Iteration 144, loss = 19577845166.06919098\n",
      "Iteration 145, loss = 19577838000.06352615\n",
      "Iteration 146, loss = 19577830796.85284042\n",
      "Iteration 147, loss = 19577823538.54752731\n",
      "Iteration 148, loss = 19577816404.41846085\n",
      "Iteration 149, loss = 19577809167.40741348\n",
      "Iteration 150, loss = 19577802007.29185104\n",
      "Iteration 151, loss = 19577794831.60439301\n",
      "Iteration 152, loss = 19577787656.70352554\n",
      "Iteration 153, loss = 19577780443.54838181\n",
      "Iteration 154, loss = 19577773305.09589386\n",
      "Iteration 155, loss = 19577766163.04916000\n",
      "Iteration 156, loss = 19577758953.21936035\n",
      "Iteration 157, loss = 19577751856.87855148\n",
      "Iteration 158, loss = 19577744739.54541779\n",
      "Iteration 159, loss = 19577737598.35314560\n",
      "Iteration 160, loss = 19577730462.88106918\n",
      "Iteration 161, loss = 19577723351.94568253\n",
      "Iteration 162, loss = 19577716214.26826859\n",
      "Iteration 163, loss = 19577709094.61677170\n",
      "Iteration 164, loss = 19577702028.86424255\n",
      "Iteration 165, loss = 19577694917.41934586\n",
      "Iteration 166, loss = 19577687754.98374939\n",
      "Iteration 167, loss = 19577680684.20175171\n",
      "Iteration 168, loss = 19577673621.94968414\n",
      "Iteration 169, loss = 19577666465.22032166\n",
      "Iteration 170, loss = 19577659452.56679153\n",
      "Iteration 171, loss = 19577652313.20464706\n",
      "Iteration 172, loss = 19577645274.59344101\n",
      "Iteration 173, loss = 19577638208.31037140\n",
      "Iteration 174, loss = 19577631153.09175110\n",
      "Iteration 175, loss = 19577624028.30625916\n",
      "Iteration 176, loss = 19577617027.55221939\n",
      "Iteration 177, loss = 19577609952.75685120\n",
      "Iteration 178, loss = 19577602904.40167618\n",
      "Iteration 179, loss = 19577595779.44054031\n",
      "Iteration 180, loss = 19577588708.24687576\n",
      "Iteration 181, loss = 19577581710.66098785\n",
      "Iteration 182, loss = 19577574582.49250031\n",
      "Iteration 183, loss = 19577567581.98356247\n",
      "Iteration 184, loss = 19577560542.77437210\n",
      "Iteration 185, loss = 19577553475.76395035\n",
      "Iteration 186, loss = 19577546469.67372894\n",
      "Iteration 187, loss = 19577539421.70913696\n",
      "Iteration 188, loss = 19577532413.50952530\n",
      "Iteration 189, loss = 19577525413.48968124\n",
      "Iteration 190, loss = 19577518320.74467468\n",
      "Iteration 191, loss = 19577511370.30213928\n",
      "Iteration 192, loss = 19577504368.74157715\n",
      "Iteration 193, loss = 19577497296.10026932\n",
      "Iteration 194, loss = 19577490338.57746124\n",
      "Iteration 195, loss = 19577483328.84254074\n",
      "Iteration 196, loss = 19577476338.54375458\n",
      "Iteration 197, loss = 19577469359.86536789\n",
      "Iteration 198, loss = 19577462389.66218185\n",
      "Iteration 199, loss = 19577455351.02092361\n",
      "Iteration 200, loss = 19577448376.92919159\n",
      "Iteration 201, loss = 19577441405.09063721\n",
      "Iteration 202, loss = 19577434423.70024490\n",
      "Iteration 203, loss = 19577427447.49272156\n",
      "Iteration 204, loss = 19577420431.56910706\n",
      "Iteration 205, loss = 19577413396.35900116\n",
      "Iteration 206, loss = 19577406403.16324234\n",
      "Iteration 207, loss = 19577399453.86297607\n",
      "Iteration 208, loss = 19577392451.87913895\n",
      "Iteration 209, loss = 19577385459.40373993\n",
      "Iteration 210, loss = 19577378516.81156158\n",
      "Iteration 211, loss = 19577371564.89208984\n",
      "Iteration 212, loss = 19577364594.56681061\n",
      "Iteration 213, loss = 19577357648.42491913\n",
      "Iteration 214, loss = 19577350693.67458344\n",
      "Iteration 215, loss = 19577343707.65743637\n",
      "Iteration 216, loss = 19577336818.02673340\n",
      "Iteration 217, loss = 19577329817.49298859\n",
      "Iteration 218, loss = 19577322886.62322235\n",
      "Iteration 219, loss = 19577315949.38940048\n",
      "Iteration 220, loss = 19577309023.73373413\n",
      "Iteration 221, loss = 19577302094.02805710\n",
      "Iteration 222, loss = 19577295134.08197784\n",
      "Iteration 223, loss = 19577288229.80242157\n",
      "Iteration 224, loss = 19577281297.08816147\n",
      "Iteration 225, loss = 19577274341.86468887\n",
      "Iteration 226, loss = 19577267435.21601486\n",
      "Iteration 227, loss = 19577260504.74355698\n",
      "Iteration 228, loss = 19577253558.40241241\n",
      "Iteration 229, loss = 19577246675.77635193\n",
      "Iteration 230, loss = 19577239722.77056885\n",
      "Iteration 231, loss = 19577232807.03380966\n",
      "Iteration 232, loss = 19577225872.64839554\n",
      "Iteration 233, loss = 19577218978.11397934\n",
      "Iteration 234, loss = 19577212081.66643906\n",
      "Iteration 235, loss = 19577205154.17139053\n",
      "Iteration 236, loss = 19577198277.56598663\n",
      "Iteration 237, loss = 19577191346.37395859\n",
      "Iteration 238, loss = 19577184428.31870651\n",
      "Iteration 239, loss = 19577177493.51012421\n",
      "Iteration 240, loss = 19577170644.52434921\n",
      "Iteration 241, loss = 19577163758.29756546\n",
      "Iteration 242, loss = 19577156805.62086105\n",
      "Iteration 243, loss = 19577149940.26528931\n",
      "Iteration 244, loss = 19577143072.37322617\n",
      "Iteration 245, loss = 19577136191.70468521\n",
      "Iteration 246, loss = 19577129225.95101547\n",
      "Iteration 247, loss = 19577122364.34178162\n",
      "Iteration 248, loss = 19577115479.89464569\n",
      "Iteration 249, loss = 19577108592.47140503\n",
      "Iteration 250, loss = 19577101699.81037140\n",
      "Iteration 251, loss = 19577094857.87110519\n",
      "Iteration 252, loss = 19577087956.08535767\n",
      "Iteration 253, loss = 19577081066.54529953\n",
      "Iteration 254, loss = 19577074142.59784317\n",
      "Iteration 255, loss = 19577067333.86463165\n",
      "Iteration 256, loss = 19577060400.31137466\n",
      "Iteration 257, loss = 19577053540.16936874\n",
      "Iteration 258, loss = 19577046667.27194595\n",
      "Iteration 259, loss = 19577039833.58000946\n",
      "Iteration 260, loss = 19577032937.87554932\n",
      "Iteration 261, loss = 19577026001.40839767\n",
      "Iteration 262, loss = 19577019175.21609879\n",
      "Iteration 263, loss = 19577012313.49869537\n",
      "Iteration 264, loss = 19577005426.35319138\n",
      "Iteration 265, loss = 19576998531.10273743\n",
      "Iteration 266, loss = 19576991718.14829254\n",
      "Iteration 267, loss = 19576984835.13604736\n",
      "Iteration 268, loss = 19576977972.64873886\n",
      "Iteration 269, loss = 19576971113.22954559\n",
      "Iteration 270, loss = 19576964225.65834045\n",
      "Iteration 271, loss = 19576957368.20416641\n",
      "Iteration 272, loss = 19576950540.41071320\n",
      "Iteration 273, loss = 19576943651.05709076\n",
      "Iteration 274, loss = 19576936817.82313156\n",
      "Iteration 275, loss = 19576929945.06475449\n",
      "Iteration 276, loss = 19576923053.85821152\n",
      "Iteration 277, loss = 19576916219.64433670\n",
      "Iteration 278, loss = 19576909357.97014236\n",
      "Iteration 279, loss = 19576902535.46622849\n",
      "Iteration 280, loss = 19576895647.84972763\n",
      "Iteration 281, loss = 19576888872.31378555\n",
      "Iteration 282, loss = 19576881972.82482910\n",
      "Iteration 283, loss = 19576875130.25728226\n",
      "Iteration 284, loss = 19576868282.46208191\n",
      "Iteration 285, loss = 19576861479.91337585\n",
      "Iteration 286, loss = 19576854623.78199768\n",
      "Iteration 287, loss = 19576847759.98968124\n",
      "Iteration 288, loss = 19576840943.56607437\n",
      "Iteration 289, loss = 19576834080.09267807\n",
      "Iteration 290, loss = 19576827232.67766571\n",
      "Iteration 291, loss = 19576820377.96507645\n",
      "Iteration 292, loss = 19576813523.20859146\n",
      "Iteration 293, loss = 19576806660.21100616\n",
      "Iteration 294, loss = 19576799857.84477615\n",
      "Iteration 295, loss = 19576792976.42850876\n",
      "Iteration 296, loss = 19576786189.16933823\n",
      "Iteration 297, loss = 19576779311.37419128\n",
      "Iteration 298, loss = 19576772493.66422272\n",
      "Iteration 299, loss = 19576765609.30733109\n",
      "Iteration 300, loss = 19576758820.30746078\n",
      "Iteration 301, loss = 19576751926.56775665\n",
      "Iteration 302, loss = 19576745199.18098068\n",
      "Iteration 303, loss = 19576738312.88846970\n",
      "Iteration 304, loss = 19576731485.54024124\n",
      "Iteration 305, loss = 19576724596.33337021\n",
      "Iteration 306, loss = 19576717815.38170624\n",
      "Iteration 307, loss = 19576710981.71990204\n",
      "Iteration 308, loss = 19576704189.98993301\n",
      "Iteration 309, loss = 19576697313.57476044\n",
      "Iteration 310, loss = 19576690513.37726593\n",
      "Iteration 311, loss = 19576683675.90865707\n",
      "Iteration 312, loss = 19576676851.90942383\n",
      "Iteration 313, loss = 19576670007.91889191\n",
      "Iteration 314, loss = 19576663208.11932373\n",
      "Iteration 315, loss = 19576656393.98565292\n",
      "Iteration 316, loss = 19576649549.88393784\n",
      "Iteration 317, loss = 19576642749.19035721\n",
      "Iteration 318, loss = 19576635950.13240051\n",
      "Iteration 319, loss = 19576629129.57973862\n",
      "Iteration 320, loss = 19576622261.92219162\n",
      "Iteration 321, loss = 19576615461.83073807\n",
      "Iteration 322, loss = 19576608627.56012344\n",
      "Iteration 323, loss = 19576601853.05617523\n",
      "Iteration 324, loss = 19576594998.31633377\n",
      "Iteration 325, loss = 19576588193.98684311\n",
      "Iteration 326, loss = 19576581398.77540970\n",
      "Iteration 327, loss = 19576574576.32334900\n",
      "Iteration 328, loss = 19576567776.08969498\n",
      "Iteration 329, loss = 19576560940.11154175\n",
      "Iteration 330, loss = 19576554146.41553116\n",
      "Iteration 331, loss = 19576547315.02144241\n",
      "Iteration 332, loss = 19576540509.80577850\n",
      "Iteration 333, loss = 19576533684.50720978\n",
      "Iteration 334, loss = 19576526910.56151962\n",
      "Iteration 335, loss = 19576520120.23395538\n",
      "Iteration 336, loss = 19576513262.96675873\n",
      "Iteration 337, loss = 19576506497.68317795\n",
      "Iteration 338, loss = 19576499689.63408661\n",
      "Iteration 339, loss = 19576492876.87285614\n",
      "Iteration 340, loss = 19576486088.64641953\n",
      "Iteration 341, loss = 19576479289.32200623\n",
      "Iteration 342, loss = 19576472463.52270889\n",
      "Iteration 343, loss = 19576465682.15349579\n",
      "Iteration 344, loss = 19576458882.81219101\n",
      "Iteration 345, loss = 19576452064.44149017\n",
      "Iteration 346, loss = 19576445286.85705948\n",
      "Iteration 347, loss = 19576438442.88741684\n",
      "Iteration 348, loss = 19576431676.83985901\n",
      "Iteration 349, loss = 19576424871.91336823\n",
      "Iteration 350, loss = 19576418080.76951599\n",
      "Iteration 351, loss = 19576411273.81962967\n",
      "Iteration 352, loss = 19576404460.73435593\n",
      "Iteration 353, loss = 19576397632.72497177\n",
      "Iteration 354, loss = 19576390897.54037857\n",
      "Iteration 355, loss = 19576384068.53259277\n",
      "Iteration 356, loss = 19576377256.49451065\n",
      "Iteration 357, loss = 19576370507.97274017\n",
      "Iteration 358, loss = 19576363715.87970352\n",
      "Iteration 359, loss = 19576356898.51221848\n",
      "Iteration 360, loss = 19576350111.90011597\n",
      "Iteration 361, loss = 19576343274.69829941\n",
      "Iteration 362, loss = 19576336532.40291977\n",
      "Iteration 363, loss = 19576329726.74753952\n",
      "Iteration 364, loss = 19576322940.79083633\n",
      "Iteration 365, loss = 19576316158.99982071\n",
      "Iteration 366, loss = 19576309359.67265701\n",
      "Iteration 367, loss = 19576302609.55154800\n",
      "Iteration 368, loss = 19576295776.55569458\n",
      "Iteration 369, loss = 19576289021.05880356\n",
      "Iteration 370, loss = 19576282217.04447937\n",
      "Iteration 371, loss = 19576275434.99706650\n",
      "Iteration 372, loss = 19576268657.12796021\n",
      "Iteration 373, loss = 19576261879.19271088\n",
      "Iteration 374, loss = 19576255089.90942001\n",
      "Iteration 375, loss = 19576248275.42084885\n",
      "Iteration 376, loss = 19576241545.99959564\n",
      "Iteration 377, loss = 19576234724.36692429\n",
      "Iteration 378, loss = 19576227909.88841629\n",
      "Iteration 379, loss = 19576221165.31970978\n",
      "Iteration 380, loss = 19576214375.69858170\n",
      "Iteration 381, loss = 19576207638.74565125\n",
      "Iteration 382, loss = 19576200824.77152634\n",
      "Iteration 383, loss = 19576194029.03337860\n",
      "Iteration 384, loss = 19576187244.00594330\n",
      "Iteration 385, loss = 19576180475.25089645\n",
      "Iteration 386, loss = 19576173657.10764313\n",
      "Iteration 387, loss = 19576166941.93035126\n",
      "Iteration 388, loss = 19576160110.29449844\n",
      "Iteration 389, loss = 19576153352.10290527\n",
      "Iteration 390, loss = 19576146573.64196777\n",
      "Iteration 391, loss = 19576139773.15139771\n",
      "Iteration 392, loss = 19576132994.43936157\n",
      "Iteration 393, loss = 19576126227.75194168\n",
      "Iteration 394, loss = 19576119474.83494568\n",
      "Iteration 395, loss = 19576112683.49361038\n",
      "Iteration 396, loss = 19576105906.80066681\n",
      "Iteration 397, loss = 19576099118.11609650\n",
      "Iteration 398, loss = 19576092345.17848206\n",
      "Iteration 399, loss = 19576085601.91222382\n",
      "Iteration 400, loss = 19576078775.00429535\n",
      "Iteration 401, loss = 19576072004.52792740\n",
      "Iteration 402, loss = 19576065297.26374054\n",
      "Iteration 403, loss = 19576058486.04784393\n",
      "Iteration 404, loss = 19576051687.77616501\n",
      "Iteration 405, loss = 19576044923.06229401\n",
      "Iteration 406, loss = 19576038180.41261292\n",
      "Iteration 407, loss = 19576031403.63955307\n",
      "Iteration 408, loss = 19576024612.27784348\n",
      "Iteration 409, loss = 19576017852.05387115\n",
      "Iteration 410, loss = 19576011105.08736420\n",
      "Iteration 411, loss = 19576004307.38324356\n",
      "Iteration 412, loss = 19575997575.94470215\n",
      "Iteration 413, loss = 19575990760.44451523\n",
      "Iteration 414, loss = 19575984009.62688828\n",
      "Iteration 415, loss = 19575977237.19606018\n",
      "Iteration 416, loss = 19575970482.21216583\n",
      "Iteration 417, loss = 19575963714.70573425\n",
      "Iteration 418, loss = 19575956959.15048218\n",
      "Iteration 419, loss = 19575950142.21517944\n",
      "Iteration 420, loss = 19575943432.53627777\n",
      "Iteration 421, loss = 19575936631.50623322\n",
      "Iteration 422, loss = 19575929843.17144012\n",
      "Iteration 423, loss = 19575923082.52789307\n",
      "Iteration 424, loss = 19575916337.37113571\n",
      "Iteration 425, loss = 19575909617.34022141\n",
      "Iteration 426, loss = 19575902776.51325226\n",
      "Iteration 427, loss = 19575896047.04814529\n",
      "Iteration 428, loss = 19575889274.82550812\n",
      "Iteration 429, loss = 19575882485.06518173\n",
      "Iteration 430, loss = 19575875724.87401581\n",
      "Iteration 431, loss = 19575868963.37687302\n",
      "Iteration 432, loss = 19575862226.27510452\n",
      "Iteration 433, loss = 19575855470.95557404\n",
      "Iteration 434, loss = 19575848643.45757294\n",
      "Iteration 435, loss = 19575841919.03530884\n",
      "Iteration 436, loss = 19575835167.46280670\n",
      "Iteration 437, loss = 19575828384.88446426\n",
      "Iteration 438, loss = 19575821645.21605682\n",
      "Iteration 439, loss = 19575814840.59226608\n",
      "Iteration 440, loss = 19575808119.43306732\n",
      "Iteration 441, loss = 19575801350.05243301\n",
      "Iteration 442, loss = 19575794605.19325256\n",
      "Iteration 443, loss = 19575787773.37000275\n",
      "Iteration 444, loss = 19575781087.11669540\n",
      "Iteration 445, loss = 19575774305.11788940\n",
      "Iteration 446, loss = 19575767526.40551376\n",
      "Iteration 447, loss = 19575760785.65768051\n",
      "Iteration 448, loss = 19575754025.20904160\n",
      "Iteration 449, loss = 19575747235.93385315\n",
      "Iteration 450, loss = 19575740479.44457245\n",
      "Iteration 451, loss = 19575733770.93252182\n",
      "Iteration 452, loss = 19575727021.77274323\n",
      "Iteration 453, loss = 19575720207.73151398\n",
      "Iteration 454, loss = 19575713446.82600021\n",
      "Iteration 455, loss = 19575706753.75973892\n",
      "Iteration 456, loss = 19575699909.25687408\n",
      "Iteration 457, loss = 19575693197.26029968\n",
      "Iteration 458, loss = 19575686459.56332779\n",
      "Iteration 459, loss = 19575679677.79089355\n",
      "Iteration 460, loss = 19575672954.70898438\n",
      "Iteration 461, loss = 19575666134.49886322\n",
      "Iteration 462, loss = 19575659458.70260239\n",
      "Iteration 463, loss = 19575652661.41314316\n",
      "Iteration 464, loss = 19575645904.66019821\n",
      "Iteration 465, loss = 19575639155.05714035\n",
      "Iteration 466, loss = 19575632409.25822067\n",
      "Iteration 467, loss = 19575625627.75556183\n",
      "Iteration 468, loss = 19575618940.92734909\n",
      "Iteration 469, loss = 19575612137.47771072\n",
      "Iteration 470, loss = 19575605410.79054260\n",
      "Iteration 471, loss = 19575598639.43458939\n",
      "Iteration 472, loss = 19575591881.78035355\n",
      "Iteration 473, loss = 19575585134.29462433\n",
      "Iteration 474, loss = 19575578370.42799377\n",
      "Iteration 475, loss = 19575571655.58418655\n",
      "Iteration 476, loss = 19575564891.93767548\n",
      "Iteration 477, loss = 19575558137.15178680\n",
      "Iteration 478, loss = 19575551402.81082916\n",
      "Iteration 479, loss = 19575544590.57529831\n",
      "Iteration 480, loss = 19575537882.28231049\n",
      "Iteration 481, loss = 19575531122.70691681\n",
      "Iteration 482, loss = 19575524375.40629196\n",
      "Iteration 483, loss = 19575517634.89149475\n",
      "Iteration 484, loss = 19575510888.39310837\n",
      "Iteration 485, loss = 19575504136.63918686\n",
      "Iteration 486, loss = 19575497351.68321991\n",
      "Iteration 487, loss = 19575490619.57151413\n",
      "Iteration 488, loss = 19575483885.74404144\n",
      "Iteration 489, loss = 19575477151.35112381\n",
      "Iteration 490, loss = 19575470380.48844147\n",
      "Iteration 491, loss = 19575463627.06910706\n",
      "Iteration 492, loss = 19575456887.47538757\n",
      "Iteration 493, loss = 19575450154.40290451\n",
      "Iteration 494, loss = 19575443375.49668884\n",
      "Iteration 495, loss = 19575436658.21817780\n",
      "Iteration 496, loss = 19575429918.53142548\n",
      "Iteration 497, loss = 19575423131.74650192\n",
      "Iteration 498, loss = 19575416404.60808945\n",
      "Iteration 499, loss = 19575409642.01529694\n",
      "Iteration 500, loss = 19575402912.92561722\n",
      "Iteration 1, loss = 17987551528.09331131\n",
      "Iteration 2, loss = 17987547973.54391098\n",
      "Iteration 3, loss = 17987544418.00574493\n",
      "Iteration 4, loss = 17987540861.47351837\n",
      "Iteration 5, loss = 17987537303.75194931\n",
      "Iteration 6, loss = 17987533744.66218185\n",
      "Iteration 7, loss = 17987530184.06537628\n",
      "Iteration 8, loss = 17987526621.79823303\n",
      "Iteration 9, loss = 17987523057.63866425\n",
      "Iteration 10, loss = 17987519491.30263901\n",
      "Iteration 11, loss = 17987515922.46219635\n",
      "Iteration 12, loss = 17987512350.77105713\n",
      "Iteration 13, loss = 17987508775.88902283\n",
      "Iteration 14, loss = 17987505197.49769974\n",
      "Iteration 15, loss = 17987501615.30337524\n",
      "Iteration 16, loss = 17987498029.03001404\n",
      "Iteration 17, loss = 17987494438.40897751\n",
      "Iteration 18, loss = 17987490843.17045975\n",
      "Iteration 19, loss = 17987487243.03868103\n",
      "Iteration 20, loss = 17987483637.73076630\n",
      "Iteration 21, loss = 17987480026.95875168\n",
      "Iteration 22, loss = 17987476410.43354416\n",
      "Iteration 23, loss = 17987472787.86993027\n",
      "Iteration 24, loss = 17987469158.99158096\n",
      "Iteration 25, loss = 17987465523.53536987\n",
      "Iteration 26, loss = 17987461881.25466537\n",
      "Iteration 27, loss = 17987458231.92134476\n",
      "Iteration 28, loss = 17987454575.32680130\n",
      "Iteration 29, loss = 17987450911.28216553\n",
      "Iteration 30, loss = 17987447239.61792374\n",
      "Iteration 31, loss = 17987443560.18330002\n",
      "Iteration 32, loss = 17987439872.84541702\n",
      "Iteration 33, loss = 17987436177.48854828\n",
      "Iteration 34, loss = 17987432474.01335526\n",
      "Iteration 35, loss = 17987428762.33631516\n",
      "Iteration 36, loss = 17987425042.38925552\n",
      "Iteration 37, loss = 17987421314.11906815\n",
      "Iteration 38, loss = 17987417577.48757172\n",
      "Iteration 39, loss = 17987413832.47150803\n",
      "Iteration 40, loss = 17987410079.06268311\n",
      "Iteration 41, loss = 17987406317.26816559\n",
      "Iteration 42, loss = 17987402547.11066437\n",
      "Iteration 43, loss = 17987398768.62882614\n",
      "Iteration 44, loss = 17987394981.87772369\n",
      "Iteration 45, loss = 17987391186.92923355\n",
      "Iteration 46, loss = 17987387383.87247086\n",
      "Iteration 47, loss = 17987383572.81414413\n",
      "Iteration 48, loss = 17987379753.87889099\n",
      "Iteration 49, loss = 17987375927.20950699\n",
      "Iteration 50, loss = 17987372092.96707916\n",
      "Iteration 51, loss = 17987368251.33115005\n",
      "Iteration 52, loss = 17987364402.49960709\n",
      "Iteration 53, loss = 17987360546.68868256\n",
      "Iteration 54, loss = 17987356684.13273621\n",
      "Iteration 55, loss = 17987352815.08403397\n",
      "Iteration 56, loss = 17987348939.81245422\n",
      "Iteration 57, loss = 17987345058.60514069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58, loss = 17987341171.76605606\n",
      "Iteration 59, loss = 17987337279.61559296\n",
      "Iteration 60, loss = 17987333382.49005508\n",
      "Iteration 61, loss = 17987329480.74117279\n",
      "Iteration 62, loss = 17987325574.73553467\n",
      "Iteration 63, loss = 17987321664.85406113\n",
      "Iteration 64, loss = 17987317751.49137115\n",
      "Iteration 65, loss = 17987313835.05527115\n",
      "Iteration 66, loss = 17987309915.96604919\n",
      "Iteration 67, loss = 17987305994.65589142\n",
      "Iteration 68, loss = 17987302071.56824875\n",
      "Iteration 69, loss = 17987298147.15717697\n",
      "Iteration 70, loss = 17987294221.88668060\n",
      "Iteration 71, loss = 17987290296.23004532\n",
      "Iteration 72, loss = 17987286370.66917801\n",
      "Iteration 73, loss = 17987282445.69391251\n",
      "Iteration 74, loss = 17987278521.80134201\n",
      "Iteration 75, loss = 17987274599.49514771\n",
      "Iteration 76, loss = 17987270679.28488541\n",
      "Iteration 77, loss = 17987266761.68531418\n",
      "Iteration 78, loss = 17987262847.21569824\n",
      "Iteration 79, loss = 17987258936.39915085\n",
      "Iteration 80, loss = 17987255029.76187515\n",
      "Iteration 81, loss = 17987251127.83255386\n",
      "Iteration 82, loss = 17987247231.14163589\n",
      "Iteration 83, loss = 17987243340.22063446\n",
      "Iteration 84, loss = 17987239455.60148239\n",
      "Iteration 85, loss = 17987235577.81586075\n",
      "Iteration 86, loss = 17987231707.39452744\n",
      "Iteration 87, loss = 17987227844.86670303\n",
      "Iteration 88, loss = 17987223990.75942612\n",
      "Iteration 89, loss = 17987220145.59697342\n",
      "Iteration 90, loss = 17987216309.90026093\n",
      "Iteration 91, loss = 17987212484.18633652\n",
      "Iteration 92, loss = 17987208668.96785736\n",
      "Iteration 93, loss = 17987204864.75261688\n",
      "Iteration 94, loss = 17987201072.04314041\n",
      "Iteration 95, loss = 17987197291.33628082\n",
      "Iteration 96, loss = 17987193523.12286377\n",
      "Iteration 97, loss = 17987189767.88739014\n",
      "Iteration 98, loss = 17987186026.10774994\n",
      "Iteration 99, loss = 17987182298.25498962\n",
      "Iteration 100, loss = 17987178584.79307938\n",
      "Iteration 101, loss = 17987174886.17872238\n",
      "Iteration 102, loss = 17987171202.86121750\n",
      "Iteration 103, loss = 17987167535.28231812\n",
      "Iteration 104, loss = 17987163883.87606812\n",
      "Iteration 105, loss = 17987160249.06877518\n",
      "Iteration 106, loss = 17987156631.27880096\n",
      "Iteration 107, loss = 17987153030.91653824\n",
      "Iteration 108, loss = 17987149448.38425064\n",
      "Iteration 109, loss = 17987145884.07594299\n",
      "Iteration 110, loss = 17987142338.37722778\n",
      "Iteration 111, loss = 17987138811.66513824\n",
      "Iteration 112, loss = 17987135304.30794144\n",
      "Iteration 113, loss = 17987131816.66491318\n",
      "Iteration 114, loss = 17987128349.08605957\n",
      "Iteration 115, loss = 17987124901.91179276\n",
      "Iteration 116, loss = 17987121475.47259140\n",
      "Iteration 117, loss = 17987118070.08861542\n",
      "Iteration 118, loss = 17987114686.06925964\n",
      "Iteration 119, loss = 17987111323.71266937\n",
      "Iteration 120, loss = 17987107983.30523300\n",
      "Iteration 121, loss = 17987104665.12106705\n",
      "Iteration 122, loss = 17987101369.42141724\n",
      "Iteration 123, loss = 17987098096.45408630\n",
      "Iteration 124, loss = 17987094846.45285034\n",
      "Iteration 125, loss = 17987091619.63684082\n",
      "Iteration 126, loss = 17987088416.20994949\n",
      "Iteration 127, loss = 17987085236.36026001\n",
      "Iteration 128, loss = 17987082080.25946045\n",
      "Iteration 129, loss = 17987078948.06230164\n",
      "Iteration 130, loss = 17987075839.90614319\n",
      "Iteration 131, loss = 17987072755.91048050\n",
      "Iteration 132, loss = 17987069696.17659760\n",
      "Iteration 133, loss = 17987066660.78723145\n",
      "Iteration 134, loss = 17987063649.80635452\n",
      "Iteration 135, loss = 17987060663.27902222\n",
      "Iteration 136, loss = 17987057701.23129654\n",
      "Iteration 137, loss = 17987054763.67025757\n",
      "Iteration 138, loss = 17987051850.58408356\n",
      "Iteration 139, loss = 17987048961.94224167\n",
      "Iteration 140, loss = 17987046097.69571686\n",
      "Iteration 141, loss = 17987043257.77729034\n",
      "Iteration 142, loss = 17987040442.10197830\n",
      "Iteration 143, loss = 17987037650.56743240\n",
      "Iteration 144, loss = 17987034883.05441284\n",
      "Iteration 145, loss = 17987032139.42736053\n",
      "Iteration 146, loss = 17987029419.53494644\n",
      "Iteration 147, loss = 17987026723.21069717\n",
      "Iteration 148, loss = 17987024050.27362823\n",
      "Iteration 149, loss = 17987021400.52890778\n",
      "Iteration 150, loss = 17987018773.76853561\n",
      "Iteration 151, loss = 17987016169.77199936\n",
      "Iteration 152, loss = 17987013588.30702209\n",
      "Iteration 153, loss = 17987011029.13018417\n",
      "Iteration 154, loss = 17987008491.98766708\n",
      "Iteration 155, loss = 17987005976.61586380\n",
      "Iteration 156, loss = 17987003482.74212265\n",
      "Iteration 157, loss = 17987001010.08535004\n",
      "Iteration 158, loss = 17986998558.35668945\n",
      "Iteration 159, loss = 17986996127.26011276\n",
      "Iteration 160, loss = 17986993716.49308395\n",
      "Iteration 161, loss = 17986991325.74713516\n",
      "Iteration 162, loss = 17986988954.70846558\n",
      "Iteration 163, loss = 17986986603.05853271\n",
      "Iteration 164, loss = 17986984270.47458267\n",
      "Iteration 165, loss = 17986981956.63025284\n",
      "Iteration 166, loss = 17986979661.19607162\n",
      "Iteration 167, loss = 17986977383.84001541\n",
      "Iteration 168, loss = 17986975124.22797394\n",
      "Iteration 169, loss = 17986972882.02431107\n",
      "Iteration 170, loss = 17986970656.89231491\n",
      "Iteration 171, loss = 17986968448.49466705\n",
      "Iteration 172, loss = 17986966256.49389648\n",
      "Iteration 173, loss = 17986964080.55282593\n",
      "Iteration 174, loss = 17986961920.33496857\n",
      "Iteration 175, loss = 17986959775.50492859\n",
      "Iteration 176, loss = 17986957645.72876358\n",
      "Iteration 177, loss = 17986955530.67436600\n",
      "Iteration 178, loss = 17986953430.01173019\n",
      "Iteration 179, loss = 17986951343.41328812\n",
      "Iteration 180, loss = 17986949270.55420303\n",
      "Iteration 181, loss = 17986947211.11256027\n",
      "Iteration 182, loss = 17986945164.76964569\n",
      "Iteration 183, loss = 17986943131.21012497\n",
      "Iteration 184, loss = 17986941110.12223053\n",
      "Iteration 185, loss = 17986939101.19792938\n",
      "Iteration 186, loss = 17986937104.13305283\n",
      "Iteration 187, loss = 17986935118.62745285\n",
      "Iteration 188, loss = 17986933144.38509369\n",
      "Iteration 189, loss = 17986931181.11420059\n",
      "Iteration 190, loss = 17986929228.52734375\n",
      "Iteration 191, loss = 17986927286.34154892\n",
      "Iteration 192, loss = 17986925354.27840042\n",
      "Iteration 193, loss = 17986923432.06417084\n",
      "Iteration 194, loss = 17986921519.42989731\n",
      "Iteration 195, loss = 17986919616.11154175\n",
      "Iteration 196, loss = 17986917721.85008240\n",
      "Iteration 197, loss = 17986915836.39166260\n",
      "Iteration 198, loss = 17986913959.48772430\n",
      "Iteration 199, loss = 17986912090.89512253\n",
      "Iteration 200, loss = 17986910230.37630463\n",
      "Iteration 201, loss = 17986908377.69945145\n",
      "Iteration 202, loss = 17986906532.63859940\n",
      "Iteration 203, loss = 17986904694.97380066\n",
      "Iteration 204, loss = 17986902864.49125671\n",
      "Iteration 205, loss = 17986901040.98345947\n",
      "Iteration 206, loss = 17986899224.24929810\n",
      "Iteration 207, loss = 17986897414.09418488\n",
      "Iteration 208, loss = 17986895610.33012772\n",
      "Iteration 209, loss = 17986893812.77585220\n",
      "Iteration 210, loss = 17986892021.25681305\n",
      "Iteration 211, loss = 17986890235.60527420\n",
      "Iteration 212, loss = 17986888455.66030884\n",
      "Iteration 213, loss = 17986886681.26781464\n",
      "Iteration 214, loss = 17986884912.28045273\n",
      "Iteration 215, loss = 17986883148.55764008\n",
      "Iteration 216, loss = 17986881389.96543503\n",
      "Iteration 217, loss = 17986879636.37644196\n",
      "Iteration 218, loss = 17986877887.66967773\n",
      "Iteration 219, loss = 17986876143.73042679\n",
      "Iteration 220, loss = 17986874404.45003510\n",
      "Iteration 221, loss = 17986872669.72571182\n",
      "Iteration 222, loss = 17986870939.46027374\n",
      "Iteration 223, loss = 17986869213.56190109\n",
      "Iteration 224, loss = 17986867491.94386292\n",
      "Iteration 225, loss = 17986865774.52419281\n",
      "Iteration 226, loss = 17986864061.22537613\n",
      "Iteration 227, loss = 17986862351.97401428\n",
      "Iteration 228, loss = 17986860646.70048904\n",
      "Iteration 229, loss = 17986858945.33856964\n",
      "Iteration 230, loss = 17986857247.82508087\n",
      "Iteration 231, loss = 17986855554.09952164\n",
      "Iteration 232, loss = 17986853864.10368347\n",
      "Iteration 233, loss = 17986852177.78132248\n",
      "Iteration 234, loss = 17986850495.07775116\n",
      "Iteration 235, loss = 17986848815.93953705\n",
      "Iteration 236, loss = 17986847140.31411743\n",
      "Iteration 237, loss = 17986845468.14954758\n",
      "Iteration 238, loss = 17986843799.39414215\n",
      "Iteration 239, loss = 17986842133.99622726\n",
      "Iteration 240, loss = 17986840471.90389633\n",
      "Iteration 241, loss = 17986838813.06476212\n",
      "Iteration 242, loss = 17986837157.42578506\n",
      "Iteration 243, loss = 17986835504.93308640\n",
      "Iteration 244, loss = 17986833855.53183365\n",
      "Iteration 245, loss = 17986832209.16613770\n",
      "Iteration 246, loss = 17986830565.77896881\n",
      "Iteration 247, loss = 17986828925.31214905\n",
      "Iteration 248, loss = 17986827287.70633698\n",
      "Iteration 249, loss = 17986825652.90106201\n",
      "Iteration 250, loss = 17986824020.83478928\n",
      "Iteration 251, loss = 17986822391.44503403\n",
      "Iteration 252, loss = 17986820764.66849518\n",
      "Iteration 253, loss = 17986819140.44120026\n",
      "Iteration 254, loss = 17986817518.69870758\n",
      "Iteration 255, loss = 17986815899.37631607\n",
      "Iteration 256, loss = 17986814282.40933609\n",
      "Iteration 257, loss = 17986812667.73331451\n",
      "Iteration 258, loss = 17986811055.28432465\n",
      "Iteration 259, loss = 17986809444.99930573\n",
      "Iteration 260, loss = 17986807836.81632233\n",
      "Iteration 261, loss = 17986806230.67492676\n",
      "Iteration 262, loss = 17986804626.51644897\n",
      "Iteration 263, loss = 17986803024.28435898\n",
      "Iteration 264, loss = 17986801423.92454910\n",
      "Iteration 265, loss = 17986799825.38569641\n",
      "Iteration 266, loss = 17986798228.61950684\n",
      "Iteration 267, loss = 17986796633.58105087\n",
      "Iteration 268, loss = 17986795040.22898102\n",
      "Iteration 269, loss = 17986793448.52581406\n",
      "Iteration 270, loss = 17986791858.43808365\n",
      "Iteration 271, loss = 17986790269.93656158\n",
      "Iteration 272, loss = 17986788682.99634171\n",
      "Iteration 273, loss = 17986787097.59697342\n",
      "Iteration 274, loss = 17986785513.72249603\n",
      "Iteration 275, loss = 17986783931.36144257\n",
      "Iteration 276, loss = 17986782350.50682831\n",
      "Iteration 277, loss = 17986780771.15605545\n",
      "Iteration 278, loss = 17986779193.31078720\n",
      "Iteration 279, loss = 17986777616.97679901\n",
      "Iteration 280, loss = 17986776042.16378021\n",
      "Iteration 281, loss = 17986774468.88509369\n",
      "Iteration 282, loss = 17986772897.15750122\n",
      "Iteration 283, loss = 17986771327.00090027\n",
      "Iteration 284, loss = 17986769758.43795395\n",
      "Iteration 285, loss = 17986768191.49382401\n",
      "Iteration 286, loss = 17986766626.19577408\n",
      "Iteration 287, loss = 17986765062.57281876\n",
      "Iteration 288, loss = 17986763500.65538406\n",
      "Iteration 289, loss = 17986761940.47493744\n",
      "Iteration 290, loss = 17986760382.06364441\n",
      "Iteration 291, loss = 17986758825.45401764\n",
      "Iteration 292, loss = 17986757270.67860031\n",
      "Iteration 293, loss = 17986755717.76966476\n",
      "Iteration 294, loss = 17986754166.75889206\n",
      "Iteration 295, loss = 17986752617.67716217\n",
      "Iteration 296, loss = 17986751070.55426025\n",
      "Iteration 297, loss = 17986749525.41869736\n",
      "Iteration 298, loss = 17986747982.29749298\n",
      "Iteration 299, loss = 17986746441.21604156\n",
      "Iteration 300, loss = 17986744902.19795990\n",
      "Iteration 301, loss = 17986743365.26498032\n",
      "Iteration 302, loss = 17986741830.43686676\n",
      "Iteration 303, loss = 17986740297.73135757\n",
      "Iteration 304, loss = 17986738767.16410065\n",
      "Iteration 305, loss = 17986737238.74866867\n",
      "Iteration 306, loss = 17986735712.49654007\n",
      "Iteration 307, loss = 17986734188.41709900\n",
      "Iteration 308, loss = 17986732666.51770020\n",
      "Iteration 309, loss = 17986731146.80368805\n",
      "Iteration 310, loss = 17986729629.27844238\n",
      "Iteration 311, loss = 17986728113.94347000\n",
      "Iteration 312, loss = 17986726600.79846191\n",
      "Iteration 313, loss = 17986725089.84135818\n",
      "Iteration 314, loss = 17986723581.06845093\n",
      "Iteration 315, loss = 17986722074.47446060\n",
      "Iteration 316, loss = 17986720570.05260468\n",
      "Iteration 317, loss = 17986719067.79473877\n",
      "Iteration 318, loss = 17986717567.69138336\n",
      "Iteration 319, loss = 17986716069.73185349\n",
      "Iteration 320, loss = 17986714573.90433121\n",
      "Iteration 321, loss = 17986713080.19594955\n",
      "Iteration 322, loss = 17986711588.59288788\n",
      "Iteration 323, loss = 17986710099.08043289\n",
      "Iteration 324, loss = 17986708611.64306641\n",
      "Iteration 325, loss = 17986707126.26453781\n",
      "Iteration 326, loss = 17986705642.92792892\n",
      "Iteration 327, loss = 17986704161.61573792\n",
      "Iteration 328, loss = 17986702682.30992126\n",
      "Iteration 329, loss = 17986701204.99195480\n",
      "Iteration 330, loss = 17986699729.64289474\n",
      "Iteration 331, loss = 17986698256.24343491\n",
      "Iteration 332, loss = 17986696784.77395248\n",
      "Iteration 333, loss = 17986695315.21453857\n",
      "Iteration 334, loss = 17986693847.54504395\n",
      "Iteration 335, loss = 17986692381.74514008\n",
      "Iteration 336, loss = 17986690917.79431915\n",
      "Iteration 337, loss = 17986689455.67195129\n",
      "Iteration 338, loss = 17986687995.35730743\n",
      "Iteration 339, loss = 17986686536.82956696\n",
      "Iteration 340, loss = 17986685080.06787491\n",
      "Iteration 341, loss = 17986683625.05133057\n",
      "Iteration 342, loss = 17986682171.75904083\n",
      "Iteration 343, loss = 17986680720.17009735\n",
      "Iteration 344, loss = 17986679270.26360321\n",
      "Iteration 345, loss = 17986677822.01871109\n",
      "Iteration 346, loss = 17986676375.41462326\n",
      "Iteration 347, loss = 17986674930.43055725\n",
      "Iteration 348, loss = 17986673487.04582214\n",
      "Iteration 349, loss = 17986672045.23977280\n",
      "Iteration 350, loss = 17986670604.99183273\n",
      "Iteration 351, loss = 17986669166.28149796\n",
      "Iteration 352, loss = 17986667729.08833313\n",
      "Iteration 353, loss = 17986666293.39197159\n",
      "Iteration 354, loss = 17986664859.17210770\n",
      "Iteration 355, loss = 17986663426.40851593\n",
      "Iteration 356, loss = 17986661995.08102036\n",
      "Iteration 357, loss = 17986660565.16949844\n",
      "Iteration 358, loss = 17986659136.65388489\n",
      "Iteration 359, loss = 17986657709.51414490\n",
      "Iteration 360, loss = 17986656283.73027420\n",
      "Iteration 361, loss = 17986654859.28230286\n",
      "Iteration 362, loss = 17986653436.15024567\n",
      "Iteration 363, loss = 17986652014.31412506\n",
      "Iteration 364, loss = 17986650593.75394821\n",
      "Iteration 365, loss = 17986649174.44967651\n",
      "Iteration 366, loss = 17986647756.38122559\n",
      "Iteration 367, loss = 17986646339.52844238\n",
      "Iteration 368, loss = 17986644923.87107849\n",
      "Iteration 369, loss = 17986643509.38878632\n",
      "Iteration 370, loss = 17986642096.06107712\n",
      "Iteration 371, loss = 17986640683.86731339\n",
      "Iteration 372, loss = 17986639272.78667068\n",
      "Iteration 373, loss = 17986637862.79812241\n",
      "Iteration 374, loss = 17986636453.88040543\n",
      "Iteration 375, loss = 17986635046.01200104\n",
      "Iteration 376, loss = 17986633639.17110062\n",
      "Iteration 377, loss = 17986632233.33555603\n",
      "Iteration 378, loss = 17986630828.48287964\n",
      "Iteration 379, loss = 17986629424.59017563\n",
      "Iteration 380, loss = 17986628021.63413620\n",
      "Iteration 381, loss = 17986626619.59096527\n",
      "Iteration 382, loss = 17986625218.43638229\n",
      "Iteration 383, loss = 17986623818.14555359\n",
      "Iteration 384, loss = 17986622418.69302368\n",
      "Iteration 385, loss = 17986621020.05274582\n",
      "Iteration 386, loss = 17986619622.19795609\n",
      "Iteration 387, loss = 17986618225.10117722\n",
      "Iteration 388, loss = 17986616828.73412323\n",
      "Iteration 389, loss = 17986615433.06768036\n",
      "Iteration 390, loss = 17986614038.07182312\n",
      "Iteration 391, loss = 17986612643.71556473\n",
      "Iteration 392, loss = 17986611249.96690750\n",
      "Iteration 393, loss = 17986609856.79273605\n",
      "Iteration 394, loss = 17986608464.15878677\n",
      "Iteration 395, loss = 17986607072.02951813\n",
      "Iteration 396, loss = 17986605680.36809540\n",
      "Iteration 397, loss = 17986604289.13626099\n",
      "Iteration 398, loss = 17986602898.29423904\n",
      "Iteration 399, loss = 17986601507.80066681\n",
      "Iteration 400, loss = 17986600117.61247253\n",
      "Iteration 401, loss = 17986598727.68476486\n",
      "Iteration 402, loss = 17986597337.97074127\n",
      "Iteration 403, loss = 17986595948.42156982\n",
      "Iteration 404, loss = 17986594558.98624802\n",
      "Iteration 405, loss = 17986593169.61152649\n",
      "Iteration 406, loss = 17986591780.24172592\n",
      "Iteration 407, loss = 17986590390.81871033\n",
      "Iteration 408, loss = 17986589001.28168106\n",
      "Iteration 409, loss = 17986587611.56715012\n",
      "Iteration 410, loss = 17986586221.60880280\n",
      "Iteration 411, loss = 17986584831.33743668\n",
      "Iteration 412, loss = 17986583440.68090820\n",
      "Iteration 413, loss = 17986582049.56408310\n",
      "Iteration 414, loss = 17986580657.90887070\n",
      "Iteration 415, loss = 17986579265.63419724\n",
      "Iteration 416, loss = 17986577872.65615463\n",
      "Iteration 417, loss = 17986576478.88806915\n",
      "Iteration 418, loss = 17986575084.24069977\n",
      "Iteration 419, loss = 17986573688.62246704\n",
      "Iteration 420, loss = 17986572291.93977356\n",
      "Iteration 421, loss = 17986570894.09735489\n",
      "Iteration 422, loss = 17986569494.99874878\n",
      "Iteration 423, loss = 17986568094.54681396\n",
      "Iteration 424, loss = 17986566692.64435959\n",
      "Iteration 425, loss = 17986565289.19481659\n",
      "Iteration 426, loss = 17986563884.10304642\n",
      "Iteration 427, loss = 17986562477.27617645\n",
      "Iteration 428, loss = 17986561068.62454987\n",
      "Iteration 429, loss = 17986559658.06270981\n",
      "Iteration 430, loss = 17986558245.51045990\n",
      "Iteration 431, loss = 17986556830.89395523\n",
      "Iteration 432, loss = 17986555414.14681625\n",
      "Iteration 433, loss = 17986553995.21123886\n",
      "Iteration 434, loss = 17986552574.03909302\n",
      "Iteration 435, loss = 17986551150.59297562\n",
      "Iteration 436, loss = 17986549724.84717560\n",
      "Iteration 437, loss = 17986548296.78853226\n",
      "Iteration 438, loss = 17986546866.41722107\n",
      "Iteration 439, loss = 17986545433.74731445\n",
      "Iteration 440, loss = 17986543998.80720139\n",
      "Iteration 441, loss = 17986542561.63982391\n",
      "Iteration 442, loss = 17986541122.30267334\n",
      "Iteration 443, loss = 17986539680.86757660\n",
      "Iteration 444, loss = 17986538237.42029953\n",
      "Iteration 445, loss = 17986536792.05984116\n",
      "Iteration 446, loss = 17986535344.89762497\n",
      "Iteration 447, loss = 17986533896.05643082\n",
      "Iteration 448, loss = 17986532445.66915512\n",
      "Iteration 449, loss = 17986530993.87747574\n",
      "Iteration 450, loss = 17986529540.83036041\n",
      "Iteration 451, loss = 17986528086.68253708\n",
      "Iteration 452, loss = 17986526631.59287262\n",
      "Iteration 453, loss = 17986525175.72281647\n",
      "Iteration 454, loss = 17986523719.23481750\n",
      "Iteration 455, loss = 17986522262.29081345\n",
      "Iteration 456, loss = 17986520805.05084229\n",
      "Iteration 457, loss = 17986519347.67174530\n",
      "Iteration 458, loss = 17986517890.30596542\n",
      "Iteration 459, loss = 17986516433.10057831\n",
      "Iteration 460, loss = 17986514976.19643021\n",
      "Iteration 461, loss = 17986513519.72739029\n",
      "Iteration 462, loss = 17986512063.81984711\n",
      "Iteration 463, loss = 17986510608.59231186\n",
      "Iteration 464, loss = 17986509154.15515137\n",
      "Iteration 465, loss = 17986507700.61050034\n",
      "Iteration 466, loss = 17986506248.05223083\n",
      "Iteration 467, loss = 17986504796.56608582\n",
      "Iteration 468, loss = 17986503346.22983551\n",
      "Iteration 469, loss = 17986501897.11357880\n",
      "Iteration 470, loss = 17986500449.28003311\n",
      "Iteration 471, loss = 17986499002.78489685\n",
      "Iteration 472, loss = 17986497557.67724609\n",
      "Iteration 473, loss = 17986496113.99992371\n",
      "Iteration 474, loss = 17986494671.78993225\n",
      "Iteration 475, loss = 17986493231.07886124\n",
      "Iteration 476, loss = 17986491791.89326096\n",
      "Iteration 477, loss = 17986490354.25501251\n",
      "Iteration 478, loss = 17986488918.18169785\n",
      "Iteration 479, loss = 17986487483.68690872\n",
      "Iteration 480, loss = 17986486050.78060150\n",
      "Iteration 481, loss = 17986484619.46933365\n",
      "Iteration 482, loss = 17986483189.75658035\n",
      "Iteration 483, loss = 17986481761.64296722\n",
      "Iteration 484, loss = 17986480335.12648773\n",
      "Iteration 485, loss = 17986478910.20276260\n",
      "Iteration 486, loss = 17986477486.86521530\n",
      "Iteration 487, loss = 17986476065.10527802\n",
      "Iteration 488, loss = 17986474644.91258621\n",
      "Iteration 489, loss = 17986473226.27514648\n",
      "Iteration 490, loss = 17986471809.17951584\n",
      "Iteration 491, loss = 17986470393.61095810\n",
      "Iteration 492, loss = 17986468979.55363464\n",
      "Iteration 493, loss = 17986467566.99069595\n",
      "Iteration 494, loss = 17986466155.90447998\n",
      "Iteration 495, loss = 17986464746.27663422\n",
      "Iteration 496, loss = 17986463338.08820343\n",
      "Iteration 497, loss = 17986461931.31983185\n",
      "Iteration 498, loss = 17986460525.95178986\n",
      "Iteration 499, loss = 17986459121.96413422\n",
      "Iteration 500, loss = 17986457719.33677673\n",
      "Iteration 1, loss = 19463558809.88616943\n",
      "Iteration 2, loss = 19463552476.01522446\n",
      "Iteration 3, loss = 19463545990.04340744\n",
      "Iteration 4, loss = 19463539693.81074524\n",
      "Iteration 5, loss = 19463533217.56350708\n",
      "Iteration 6, loss = 19463526775.71786118\n",
      "Iteration 7, loss = 19463520403.91820908\n",
      "Iteration 8, loss = 19463513903.85745239\n",
      "Iteration 9, loss = 19463507452.36679840\n",
      "Iteration 10, loss = 19463500959.27313995\n",
      "Iteration 11, loss = 19463494413.25248718\n",
      "Iteration 12, loss = 19463487920.84011459\n",
      "Iteration 13, loss = 19463481367.07439423\n",
      "Iteration 14, loss = 19463474765.82349777\n",
      "Iteration 15, loss = 19463468238.31759644\n",
      "Iteration 16, loss = 19463461583.35334396\n",
      "Iteration 17, loss = 19463454979.40324402\n",
      "Iteration 18, loss = 19463448339.07572937\n",
      "Iteration 19, loss = 19463441603.31435776\n",
      "Iteration 20, loss = 19463434911.14579391\n",
      "Iteration 21, loss = 19463428140.63665009\n",
      "Iteration 22, loss = 19463421366.32485199\n",
      "Iteration 23, loss = 19463414522.44604874\n",
      "Iteration 24, loss = 19463407690.07781982\n",
      "Iteration 25, loss = 19463400779.97443390\n",
      "Iteration 26, loss = 19463393908.28192902\n",
      "Iteration 27, loss = 19463386935.87324142\n",
      "Iteration 28, loss = 19463379961.95435715\n",
      "Iteration 29, loss = 19463372940.89129257\n",
      "Iteration 30, loss = 19463365856.77574921\n",
      "Iteration 31, loss = 19463358768.36774826\n",
      "Iteration 32, loss = 19463351645.84498978\n",
      "Iteration 33, loss = 19463344420.58635330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 19463337216.27350998\n",
      "Iteration 35, loss = 19463329967.17918015\n",
      "Iteration 36, loss = 19463322792.47525406\n",
      "Iteration 37, loss = 19463315487.37727737\n",
      "Iteration 38, loss = 19463308329.98976898\n",
      "Iteration 39, loss = 19463301186.40396500\n",
      "Iteration 40, loss = 19463294120.69759750\n",
      "Iteration 41, loss = 19463287066.63666916\n",
      "Iteration 42, loss = 19463280083.11867523\n",
      "Iteration 43, loss = 19463273162.35197830\n",
      "Iteration 44, loss = 19463266239.82275772\n",
      "Iteration 45, loss = 19463259474.67157364\n",
      "Iteration 46, loss = 19463252698.45542145\n",
      "Iteration 47, loss = 19463245917.99810791\n",
      "Iteration 48, loss = 19463239353.03229904\n",
      "Iteration 49, loss = 19463232722.37239075\n",
      "Iteration 50, loss = 19463226191.50002670\n",
      "Iteration 51, loss = 19463219770.00920105\n",
      "Iteration 52, loss = 19463213318.15002060\n",
      "Iteration 53, loss = 19463206937.64693451\n",
      "Iteration 54, loss = 19463200577.04048920\n",
      "Iteration 55, loss = 19463194299.53387833\n",
      "Iteration 56, loss = 19463188044.90900803\n",
      "Iteration 57, loss = 19463181813.87345505\n",
      "Iteration 58, loss = 19463175583.44276047\n",
      "Iteration 59, loss = 19463169482.14514923\n",
      "Iteration 60, loss = 19463163308.98946381\n",
      "Iteration 61, loss = 19463157284.04903030\n",
      "Iteration 62, loss = 19463151136.12525940\n",
      "Iteration 63, loss = 19463145187.03634262\n",
      "Iteration 64, loss = 19463139211.46733093\n",
      "Iteration 65, loss = 19463133266.67677689\n",
      "Iteration 66, loss = 19463127448.51733780\n",
      "Iteration 67, loss = 19463121639.15664291\n",
      "Iteration 68, loss = 19463115864.07132721\n",
      "Iteration 69, loss = 19463110261.48662567\n",
      "Iteration 70, loss = 19463104595.11493301\n",
      "Iteration 71, loss = 19463099048.19963074\n",
      "Iteration 72, loss = 19463093583.30392075\n",
      "Iteration 73, loss = 19463088209.54862976\n",
      "Iteration 74, loss = 19463082878.54108810\n",
      "Iteration 75, loss = 19463077714.00075531\n",
      "Iteration 76, loss = 19463072592.11732101\n",
      "Iteration 77, loss = 19463067632.75382996\n",
      "Iteration 78, loss = 19463062702.04909897\n",
      "Iteration 79, loss = 19463057934.15779495\n",
      "Iteration 80, loss = 19463053190.60223389\n",
      "Iteration 81, loss = 19463048571.54544830\n",
      "Iteration 82, loss = 19463043967.73973083\n",
      "Iteration 83, loss = 19463039494.30648804\n",
      "Iteration 84, loss = 19463035067.93155670\n",
      "Iteration 85, loss = 19463030681.75487137\n",
      "Iteration 86, loss = 19463026342.51715851\n",
      "Iteration 87, loss = 19463022073.69707870\n",
      "Iteration 88, loss = 19463017839.10704041\n",
      "Iteration 89, loss = 19463013612.45306778\n",
      "Iteration 90, loss = 19463009435.01731110\n",
      "Iteration 91, loss = 19463005308.65714645\n",
      "Iteration 92, loss = 19463001218.32444000\n",
      "Iteration 93, loss = 19462997203.71116638\n",
      "Iteration 94, loss = 19462993204.24825668\n",
      "Iteration 95, loss = 19462989315.94982147\n",
      "Iteration 96, loss = 19462985469.48762894\n",
      "Iteration 97, loss = 19462981647.97211838\n",
      "Iteration 98, loss = 19462977934.06422806\n",
      "Iteration 99, loss = 19462974224.24749374\n",
      "Iteration 100, loss = 19462970568.85086441\n",
      "Iteration 101, loss = 19462966942.07055664\n",
      "Iteration 102, loss = 19462963362.91155624\n",
      "Iteration 103, loss = 19462959792.86476135\n",
      "Iteration 104, loss = 19462956286.79274368\n",
      "Iteration 105, loss = 19462952781.51621628\n",
      "Iteration 106, loss = 19462949318.70933533\n",
      "Iteration 107, loss = 19462945870.41320038\n",
      "Iteration 108, loss = 19462942458.82762909\n",
      "Iteration 109, loss = 19462939056.36149597\n",
      "Iteration 110, loss = 19462935698.71543503\n",
      "Iteration 111, loss = 19462932330.95341873\n",
      "Iteration 112, loss = 19462929031.36119843\n",
      "Iteration 113, loss = 19462925715.22863770\n",
      "Iteration 114, loss = 19462922476.43180847\n",
      "Iteration 115, loss = 19462919210.95045471\n",
      "Iteration 116, loss = 19462915959.01534271\n",
      "Iteration 117, loss = 19462912735.24701309\n",
      "Iteration 118, loss = 19462909531.33331680\n",
      "Iteration 119, loss = 19462906328.57578278\n",
      "Iteration 120, loss = 19462903138.49595261\n",
      "Iteration 121, loss = 19462899948.50577545\n",
      "Iteration 122, loss = 19462896789.43740463\n",
      "Iteration 123, loss = 19462893645.65993881\n",
      "Iteration 124, loss = 19462890516.24634933\n",
      "Iteration 125, loss = 19462887358.49943924\n",
      "Iteration 126, loss = 19462884247.14053345\n",
      "Iteration 127, loss = 19462881162.08188629\n",
      "Iteration 128, loss = 19462878037.79313660\n",
      "Iteration 129, loss = 19462874948.01165009\n",
      "Iteration 130, loss = 19462871865.89576721\n",
      "Iteration 131, loss = 19462868778.24054337\n",
      "Iteration 132, loss = 19462865740.79138947\n",
      "Iteration 133, loss = 19462862686.68568039\n",
      "Iteration 134, loss = 19462859626.42111969\n",
      "Iteration 135, loss = 19462856622.55947495\n",
      "Iteration 136, loss = 19462853583.15149307\n",
      "Iteration 137, loss = 19462850564.12740707\n",
      "Iteration 138, loss = 19462847547.62896729\n",
      "Iteration 139, loss = 19462844508.98228073\n",
      "Iteration 140, loss = 19462841492.80463409\n",
      "Iteration 141, loss = 19462838448.88100433\n",
      "Iteration 142, loss = 19462835471.93684387\n",
      "Iteration 143, loss = 19462832399.24654388\n",
      "Iteration 144, loss = 19462829408.07468414\n",
      "Iteration 145, loss = 19462826384.70673370\n",
      "Iteration 146, loss = 19462823345.63873291\n",
      "Iteration 147, loss = 19462820328.66700363\n",
      "Iteration 148, loss = 19462817331.11599350\n",
      "Iteration 149, loss = 19462814298.56941223\n",
      "Iteration 150, loss = 19462811328.00633240\n",
      "Iteration 151, loss = 19462808307.08318710\n",
      "Iteration 152, loss = 19462805272.05261993\n",
      "Iteration 153, loss = 19462802292.16184998\n",
      "Iteration 154, loss = 19462799252.93521118\n",
      "Iteration 155, loss = 19462796233.37660980\n",
      "Iteration 156, loss = 19462793244.93444824\n",
      "Iteration 157, loss = 19462790241.48353195\n",
      "Iteration 158, loss = 19462787249.77682877\n",
      "Iteration 159, loss = 19462784277.82151031\n",
      "Iteration 160, loss = 19462781307.76378250\n",
      "Iteration 161, loss = 19462778326.29034805\n",
      "Iteration 162, loss = 19462775330.28247070\n",
      "Iteration 163, loss = 19462772385.51466751\n",
      "Iteration 164, loss = 19462769404.63957596\n",
      "Iteration 165, loss = 19462766475.82654190\n",
      "Iteration 166, loss = 19462763516.37785339\n",
      "Iteration 167, loss = 19462760568.86176300\n",
      "Iteration 168, loss = 19462757618.98970795\n",
      "Iteration 169, loss = 19462754680.43619156\n",
      "Iteration 170, loss = 19462751721.16369629\n",
      "Iteration 171, loss = 19462748780.47933960\n",
      "Iteration 172, loss = 19462745841.08975983\n",
      "Iteration 173, loss = 19462742918.57552719\n",
      "Iteration 174, loss = 19462739984.24298859\n",
      "Iteration 175, loss = 19462737072.20276642\n",
      "Iteration 176, loss = 19462734170.94475937\n",
      "Iteration 177, loss = 19462731259.72662354\n",
      "Iteration 178, loss = 19462728375.16501236\n",
      "Iteration 179, loss = 19462725490.16229248\n",
      "Iteration 180, loss = 19462722600.28644180\n",
      "Iteration 181, loss = 19462719716.45052719\n",
      "Iteration 182, loss = 19462716853.52701950\n",
      "Iteration 183, loss = 19462713953.27371979\n",
      "Iteration 184, loss = 19462711096.96812820\n",
      "Iteration 185, loss = 19462708181.01553726\n",
      "Iteration 186, loss = 19462705309.93083954\n",
      "Iteration 187, loss = 19462702397.57642746\n",
      "Iteration 188, loss = 19462699493.10710144\n",
      "Iteration 189, loss = 19462696603.99422073\n",
      "Iteration 190, loss = 19462693670.72667313\n",
      "Iteration 191, loss = 19462690776.55442047\n",
      "Iteration 192, loss = 19462687889.03421402\n",
      "Iteration 193, loss = 19462684970.68830109\n",
      "Iteration 194, loss = 19462682057.52022552\n",
      "Iteration 195, loss = 19462679189.45871735\n",
      "Iteration 196, loss = 19462676266.52290344\n",
      "Iteration 197, loss = 19462673395.02756882\n",
      "Iteration 198, loss = 19462670465.66471100\n",
      "Iteration 199, loss = 19462667639.47465897\n",
      "Iteration 200, loss = 19462664730.33573532\n",
      "Iteration 201, loss = 19462661854.03047180\n",
      "Iteration 202, loss = 19462658989.77400208\n",
      "Iteration 203, loss = 19462656135.92880249\n",
      "Iteration 204, loss = 19462653257.44570541\n",
      "Iteration 205, loss = 19462650439.04573822\n",
      "Iteration 206, loss = 19462647585.53620529\n",
      "Iteration 207, loss = 19462644742.52803802\n",
      "Iteration 208, loss = 19462641937.37894058\n",
      "Iteration 209, loss = 19462639126.46500778\n",
      "Iteration 210, loss = 19462636334.82102585\n",
      "Iteration 211, loss = 19462633523.72171783\n",
      "Iteration 212, loss = 19462630747.97153854\n",
      "Iteration 213, loss = 19462627957.39717484\n",
      "Iteration 214, loss = 19462625151.66612625\n",
      "Iteration 215, loss = 19462622345.58883667\n",
      "Iteration 216, loss = 19462619569.59567642\n",
      "Iteration 217, loss = 19462616755.30719376\n",
      "Iteration 218, loss = 19462613937.14274597\n",
      "Iteration 219, loss = 19462611143.98846817\n",
      "Iteration 220, loss = 19462608353.76676559\n",
      "Iteration 221, loss = 19462605564.01491928\n",
      "Iteration 222, loss = 19462602743.18585968\n",
      "Iteration 223, loss = 19462599964.49297714\n",
      "Iteration 224, loss = 19462597173.73602676\n",
      "Iteration 225, loss = 19462594386.73682785\n",
      "Iteration 226, loss = 19462591604.23365784\n",
      "Iteration 227, loss = 19462588837.11213684\n",
      "Iteration 228, loss = 19462586088.77323532\n",
      "Iteration 229, loss = 19462583302.77462006\n",
      "Iteration 230, loss = 19462580553.76974106\n",
      "Iteration 231, loss = 19462577783.69770050\n",
      "Iteration 232, loss = 19462574982.33559799\n",
      "Iteration 233, loss = 19462572223.36758041\n",
      "Iteration 234, loss = 19462569410.68784332\n",
      "Iteration 235, loss = 19462566643.71624756\n",
      "Iteration 236, loss = 19462563856.25574493\n",
      "Iteration 237, loss = 19462561061.38507462\n",
      "Iteration 238, loss = 19462558301.14550400\n",
      "Iteration 239, loss = 19462555510.36246490\n",
      "Iteration 240, loss = 19462552754.33966827\n",
      "Iteration 241, loss = 19462549992.90908051\n",
      "Iteration 242, loss = 19462547240.01510620\n",
      "Iteration 243, loss = 19462544504.70394135\n",
      "Iteration 244, loss = 19462541750.17163467\n",
      "Iteration 245, loss = 19462539010.41961288\n",
      "Iteration 246, loss = 19462536256.45346451\n",
      "Iteration 247, loss = 19462533473.69806290\n",
      "Iteration 248, loss = 19462530711.97833633\n",
      "Iteration 249, loss = 19462527919.64325333\n",
      "Iteration 250, loss = 19462525131.73582077\n",
      "Iteration 251, loss = 19462522325.09363937\n",
      "Iteration 252, loss = 19462519564.82487106\n",
      "Iteration 253, loss = 19462516752.74755096\n",
      "Iteration 254, loss = 19462513940.01539993\n",
      "Iteration 255, loss = 19462511177.18423843\n",
      "Iteration 256, loss = 19462508326.05398178\n",
      "Iteration 257, loss = 19462505535.41831589\n",
      "Iteration 258, loss = 19462502685.36082077\n",
      "Iteration 259, loss = 19462499866.39876556\n",
      "Iteration 260, loss = 19462496985.81507111\n",
      "Iteration 261, loss = 19462494136.13540649\n",
      "Iteration 262, loss = 19462491262.17366791\n",
      "Iteration 263, loss = 19462488364.82176971\n",
      "Iteration 264, loss = 19462485454.92518234\n",
      "Iteration 265, loss = 19462482542.94224548\n",
      "Iteration 266, loss = 19462479580.09725571\n",
      "Iteration 267, loss = 19462476635.62836075\n",
      "Iteration 268, loss = 19462473654.39848709\n",
      "Iteration 269, loss = 19462470657.01102066\n",
      "Iteration 270, loss = 19462467666.68162155\n",
      "Iteration 271, loss = 19462464659.63649368\n",
      "Iteration 272, loss = 19462461660.57359695\n",
      "Iteration 273, loss = 19462458637.75644302\n",
      "Iteration 274, loss = 19462455622.87301636\n",
      "Iteration 275, loss = 19462452603.42620850\n",
      "Iteration 276, loss = 19462449586.26301956\n",
      "Iteration 277, loss = 19462446579.58038712\n",
      "Iteration 278, loss = 19462443541.00222015\n",
      "Iteration 279, loss = 19462440547.21608734\n",
      "Iteration 280, loss = 19462437550.37919235\n",
      "Iteration 281, loss = 19462434531.76279068\n",
      "Iteration 282, loss = 19462431566.83862686\n",
      "Iteration 283, loss = 19462428557.92116928\n",
      "Iteration 284, loss = 19462425603.01395416\n",
      "Iteration 285, loss = 19462422601.32008362\n",
      "Iteration 286, loss = 19462419630.77972412\n",
      "Iteration 287, loss = 19462416662.99188614\n",
      "Iteration 288, loss = 19462413680.34363937\n",
      "Iteration 289, loss = 19462410736.34955978\n",
      "Iteration 290, loss = 19462407794.04948044\n",
      "Iteration 291, loss = 19462404879.06149292\n",
      "Iteration 292, loss = 19462401983.74465179\n",
      "Iteration 293, loss = 19462399088.75475693\n",
      "Iteration 294, loss = 19462396218.77423859\n",
      "Iteration 295, loss = 19462393304.00837326\n",
      "Iteration 296, loss = 19462390456.02770996\n",
      "Iteration 297, loss = 19462387569.82828903\n",
      "Iteration 298, loss = 19462384703.87720108\n",
      "Iteration 299, loss = 19462381826.85134888\n",
      "Iteration 300, loss = 19462378987.11588287\n",
      "Iteration 301, loss = 19462376124.57353210\n",
      "Iteration 302, loss = 19462373286.43373489\n",
      "Iteration 303, loss = 19462370429.91364288\n",
      "Iteration 304, loss = 19462367593.97673416\n",
      "Iteration 305, loss = 19462364744.73079300\n",
      "Iteration 306, loss = 19462361919.57991028\n",
      "Iteration 307, loss = 19462359091.75985718\n",
      "Iteration 308, loss = 19462356286.31480026\n",
      "Iteration 309, loss = 19462353469.90721512\n",
      "Iteration 310, loss = 19462350658.99840546\n",
      "Iteration 311, loss = 19462347849.22265625\n",
      "Iteration 312, loss = 19462345055.25673294\n",
      "Iteration 313, loss = 19462342211.51170349\n",
      "Iteration 314, loss = 19462339426.94095612\n",
      "Iteration 315, loss = 19462336616.10538483\n",
      "Iteration 316, loss = 19462333831.69825745\n",
      "Iteration 317, loss = 19462331018.30772018\n",
      "Iteration 318, loss = 19462328240.29038239\n",
      "Iteration 319, loss = 19462325460.87341309\n",
      "Iteration 320, loss = 19462322664.15647125\n",
      "Iteration 321, loss = 19462319888.00972748\n",
      "Iteration 322, loss = 19462317126.01953506\n",
      "Iteration 323, loss = 19462314317.89490128\n",
      "Iteration 324, loss = 19462311524.81951904\n",
      "Iteration 325, loss = 19462308733.34487915\n",
      "Iteration 326, loss = 19462305918.59413910\n",
      "Iteration 327, loss = 19462303136.65480042\n",
      "Iteration 328, loss = 19462300295.32911301\n",
      "Iteration 329, loss = 19462297533.73807907\n",
      "Iteration 330, loss = 19462294742.23352814\n",
      "Iteration 331, loss = 19462291953.71955109\n",
      "Iteration 332, loss = 19462289143.65602875\n",
      "Iteration 333, loss = 19462286371.62363052\n",
      "Iteration 334, loss = 19462283574.17978668\n",
      "Iteration 335, loss = 19462280815.91014099\n",
      "Iteration 336, loss = 19462278034.23101425\n",
      "Iteration 337, loss = 19462275272.23535538\n",
      "Iteration 338, loss = 19462272524.89728165\n",
      "Iteration 339, loss = 19462269793.51879883\n",
      "Iteration 340, loss = 19462267047.31121826\n",
      "Iteration 341, loss = 19462264324.88526917\n",
      "Iteration 342, loss = 19462261585.70116425\n",
      "Iteration 343, loss = 19462258860.20912933\n",
      "Iteration 344, loss = 19462256113.92613983\n",
      "Iteration 345, loss = 19462253354.62872314\n",
      "Iteration 346, loss = 19462250620.45858765\n",
      "Iteration 347, loss = 19462247873.99230576\n",
      "Iteration 348, loss = 19462245109.71546936\n",
      "Iteration 349, loss = 19462242379.12212753\n",
      "Iteration 350, loss = 19462239613.26362991\n",
      "Iteration 351, loss = 19462236863.53671646\n",
      "Iteration 352, loss = 19462234104.66785431\n",
      "Iteration 353, loss = 19462231352.34635544\n",
      "Iteration 354, loss = 19462228563.66224289\n",
      "Iteration 355, loss = 19462225791.81669998\n",
      "Iteration 356, loss = 19462223020.33934021\n",
      "Iteration 357, loss = 19462220248.64491653\n",
      "Iteration 358, loss = 19462217442.72540665\n",
      "Iteration 359, loss = 19462214680.29195786\n",
      "Iteration 360, loss = 19462211890.18464279\n",
      "Iteration 361, loss = 19462209087.31232071\n",
      "Iteration 362, loss = 19462206325.86087036\n",
      "Iteration 363, loss = 19462203520.42891693\n",
      "Iteration 364, loss = 19462200731.73950577\n",
      "Iteration 365, loss = 19462197930.89339066\n",
      "Iteration 366, loss = 19462195115.12489700\n",
      "Iteration 367, loss = 19462192331.08145142\n",
      "Iteration 368, loss = 19462189508.46429062\n",
      "Iteration 369, loss = 19462186668.08623886\n",
      "Iteration 370, loss = 19462183877.60129929\n",
      "Iteration 371, loss = 19462181040.92251968\n",
      "Iteration 372, loss = 19462178207.83678818\n",
      "Iteration 373, loss = 19462175375.95236969\n",
      "Iteration 374, loss = 19462172534.23411942\n",
      "Iteration 375, loss = 19462169647.67066956\n",
      "Iteration 376, loss = 19462166757.57017136\n",
      "Iteration 377, loss = 19462163887.05094147\n",
      "Iteration 378, loss = 19462160993.42177582\n",
      "Iteration 379, loss = 19462158077.43421555\n",
      "Iteration 380, loss = 19462155193.96805954\n",
      "Iteration 381, loss = 19462152296.60446548\n",
      "Iteration 382, loss = 19462149391.48431778\n",
      "Iteration 383, loss = 19462146500.70647430\n",
      "Iteration 384, loss = 19462143570.00969696\n",
      "Iteration 385, loss = 19462140659.31317139\n",
      "Iteration 386, loss = 19462137737.77143478\n",
      "Iteration 387, loss = 19462134797.62577057\n",
      "Iteration 388, loss = 19462131839.83177185\n",
      "Iteration 389, loss = 19462128883.09114456\n",
      "Iteration 390, loss = 19462125911.59080124\n",
      "Iteration 391, loss = 19462122925.47905350\n",
      "Iteration 392, loss = 19462119933.54162216\n",
      "Iteration 393, loss = 19462116941.40953445\n",
      "Iteration 394, loss = 19462113974.77254105\n",
      "Iteration 395, loss = 19462110960.79376984\n",
      "Iteration 396, loss = 19462107972.63350296\n",
      "Iteration 397, loss = 19462105010.33179092\n",
      "Iteration 398, loss = 19462102002.93111420\n",
      "Iteration 399, loss = 19462099002.15324402\n",
      "Iteration 400, loss = 19462096036.25548172\n",
      "Iteration 401, loss = 19462093067.05416489\n",
      "Iteration 402, loss = 19462090113.85008240\n",
      "Iteration 403, loss = 19462087153.24672699\n",
      "Iteration 404, loss = 19462084212.61488342\n",
      "Iteration 405, loss = 19462081256.37631607\n",
      "Iteration 406, loss = 19462078321.48373032\n",
      "Iteration 407, loss = 19462075360.90245819\n",
      "Iteration 408, loss = 19462072407.02198410\n",
      "Iteration 409, loss = 19462069460.85927963\n",
      "Iteration 410, loss = 19462066481.02328110\n",
      "Iteration 411, loss = 19462063564.14936447\n",
      "Iteration 412, loss = 19462060615.66172409\n",
      "Iteration 413, loss = 19462057681.21703720\n",
      "Iteration 414, loss = 19462054769.49775696\n",
      "Iteration 415, loss = 19462051868.33713150\n",
      "Iteration 416, loss = 19462048903.06285477\n",
      "Iteration 417, loss = 19462045981.54972458\n",
      "Iteration 418, loss = 19462043039.15460968\n",
      "Iteration 419, loss = 19462040089.83717728\n",
      "Iteration 420, loss = 19462037156.90615463\n",
      "Iteration 421, loss = 19462034208.35996628\n",
      "Iteration 422, loss = 19462031310.14806366\n",
      "Iteration 423, loss = 19462028379.04348373\n",
      "Iteration 424, loss = 19462025496.88589859\n",
      "Iteration 425, loss = 19462022612.54030228\n",
      "Iteration 426, loss = 19462019731.96389389\n",
      "Iteration 427, loss = 19462016869.46715546\n",
      "Iteration 428, loss = 19462014009.75049973\n",
      "Iteration 429, loss = 19462011125.60892487\n",
      "Iteration 430, loss = 19462008280.53800964\n",
      "Iteration 431, loss = 19462005387.13845062\n",
      "Iteration 432, loss = 19462002529.34498215\n",
      "Iteration 433, loss = 19461999653.83810806\n",
      "Iteration 434, loss = 19461996812.54146957\n",
      "Iteration 435, loss = 19461993915.95663834\n",
      "Iteration 436, loss = 19461991103.02612686\n",
      "Iteration 437, loss = 19461988214.32318878\n",
      "Iteration 438, loss = 19461985342.88219070\n",
      "Iteration 439, loss = 19461982531.72160721\n",
      "Iteration 440, loss = 19461979605.56777573\n",
      "Iteration 441, loss = 19461976767.59867477\n",
      "Iteration 442, loss = 19461973882.62930679\n",
      "Iteration 443, loss = 19461971004.73430252\n",
      "Iteration 444, loss = 19461968143.74141693\n",
      "Iteration 445, loss = 19461965283.70113373\n",
      "Iteration 446, loss = 19461962428.76282120\n",
      "Iteration 447, loss = 19461959566.63969421\n",
      "Iteration 448, loss = 19461956739.96752167\n",
      "Iteration 449, loss = 19461953881.87277603\n",
      "Iteration 450, loss = 19461951043.43457031\n",
      "Iteration 451, loss = 19461948192.35128403\n",
      "Iteration 452, loss = 19461945338.37828064\n",
      "Iteration 453, loss = 19461942452.81341553\n",
      "Iteration 454, loss = 19461939589.23786163\n",
      "Iteration 455, loss = 19461936730.56261826\n",
      "Iteration 456, loss = 19461933848.94889069\n",
      "Iteration 457, loss = 19461930973.78419113\n",
      "Iteration 458, loss = 19461928131.61265564\n",
      "Iteration 459, loss = 19461925320.87858200\n",
      "Iteration 460, loss = 19461922519.43792343\n",
      "Iteration 461, loss = 19461919697.91650009\n",
      "Iteration 462, loss = 19461916910.65367508\n",
      "Iteration 463, loss = 19461914121.37695694\n",
      "Iteration 464, loss = 19461911318.91357803\n",
      "Iteration 465, loss = 19461908518.94469070\n",
      "Iteration 466, loss = 19461905703.57154083\n",
      "Iteration 467, loss = 19461902883.73533249\n",
      "Iteration 468, loss = 19461900088.12057495\n",
      "Iteration 469, loss = 19461897263.14478302\n",
      "Iteration 470, loss = 19461894446.90299606\n",
      "Iteration 471, loss = 19461891631.35740280\n",
      "Iteration 472, loss = 19461888817.67554092\n",
      "Iteration 473, loss = 19461885993.97742081\n",
      "Iteration 474, loss = 19461883181.19097519\n",
      "Iteration 475, loss = 19461880375.73498535\n",
      "Iteration 476, loss = 19461877564.47956467\n",
      "Iteration 477, loss = 19461874722.00301743\n",
      "Iteration 478, loss = 19461871935.18035126\n",
      "Iteration 479, loss = 19461869128.65829849\n",
      "Iteration 480, loss = 19461866307.77925873\n",
      "Iteration 481, loss = 19461863509.47633743\n",
      "Iteration 482, loss = 19461860695.41871643\n",
      "Iteration 483, loss = 19461857901.23219299\n",
      "Iteration 484, loss = 19461855085.67514038\n",
      "Iteration 485, loss = 19461852295.66520691\n",
      "Iteration 486, loss = 19461849492.27964783\n",
      "Iteration 487, loss = 19461846702.84857178\n",
      "Iteration 488, loss = 19461843935.62222672\n",
      "Iteration 489, loss = 19461841145.51405334\n",
      "Iteration 490, loss = 19461838358.31446457\n",
      "Iteration 491, loss = 19461835597.83019638\n",
      "Iteration 492, loss = 19461832802.94333267\n",
      "Iteration 493, loss = 19461830030.44335175\n",
      "Iteration 494, loss = 19461827238.10493088\n",
      "Iteration 495, loss = 19461824452.74563980\n",
      "Iteration 496, loss = 19461821665.13290787\n",
      "Iteration 497, loss = 19461818871.52823639\n",
      "Iteration 498, loss = 19461816089.66790771\n",
      "Iteration 499, loss = 19461813327.58158112\n",
      "Iteration 500, loss = 19461810548.38937759\n",
      "Iteration 1, loss = 19353801330.30305862\n",
      "Iteration 2, loss = 19353793192.82464981\n",
      "Iteration 3, loss = 19353785097.54885101\n",
      "Iteration 4, loss = 19353776991.24497986\n",
      "Iteration 5, loss = 19353768841.65711594\n",
      "Iteration 6, loss = 19353760665.19791412\n",
      "Iteration 7, loss = 19353752451.12313461\n",
      "Iteration 8, loss = 19353744191.57173538\n",
      "Iteration 9, loss = 19353735970.79575729\n",
      "Iteration 10, loss = 19353727622.80626297\n",
      "Iteration 11, loss = 19353719335.82472610\n",
      "Iteration 12, loss = 19353710967.78126526\n",
      "Iteration 13, loss = 19353702590.34683609\n",
      "Iteration 14, loss = 19353694239.18771362\n",
      "Iteration 15, loss = 19353685783.34316254\n",
      "Iteration 16, loss = 19353677350.65371704\n",
      "Iteration 17, loss = 19353668868.64532471\n",
      "Iteration 18, loss = 19353660498.19667435\n",
      "Iteration 19, loss = 19353652034.39085770\n",
      "Iteration 20, loss = 19353643439.15168381\n",
      "Iteration 21, loss = 19353634981.36064529\n",
      "Iteration 22, loss = 19353626241.73317337\n",
      "Iteration 23, loss = 19353617535.49143600\n",
      "Iteration 24, loss = 19353608677.63323593\n",
      "Iteration 25, loss = 19353599647.28306961\n",
      "Iteration 26, loss = 19353590618.60186386\n",
      "Iteration 27, loss = 19353581481.11038971\n",
      "Iteration 28, loss = 19353572340.96317673\n",
      "Iteration 29, loss = 19353562861.51569366\n",
      "Iteration 30, loss = 19353553660.46574402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 19353544471.24983215\n",
      "Iteration 32, loss = 19353535193.20143509\n",
      "Iteration 33, loss = 19353525931.88705826\n",
      "Iteration 34, loss = 19353516629.84339905\n",
      "Iteration 35, loss = 19353507396.15962219\n",
      "Iteration 36, loss = 19353498147.26670837\n",
      "Iteration 37, loss = 19353488803.35081100\n",
      "Iteration 38, loss = 19353479472.60750580\n",
      "Iteration 39, loss = 19353469953.95492172\n",
      "Iteration 40, loss = 19353460527.94155121\n",
      "Iteration 41, loss = 19353451025.25856018\n",
      "Iteration 42, loss = 19353441423.11613464\n",
      "Iteration 43, loss = 19353432037.33214188\n",
      "Iteration 44, loss = 19353422562.31039047\n",
      "Iteration 45, loss = 19353413187.07041168\n",
      "Iteration 46, loss = 19353403781.06526947\n",
      "Iteration 47, loss = 19353394398.44264221\n",
      "Iteration 48, loss = 19353385292.19700241\n",
      "Iteration 49, loss = 19353376138.12405014\n",
      "Iteration 50, loss = 19353367042.48860168\n",
      "Iteration 51, loss = 19353357948.75527573\n",
      "Iteration 52, loss = 19353349025.34273911\n",
      "Iteration 53, loss = 19353340217.08311844\n",
      "Iteration 54, loss = 19353331412.69580841\n",
      "Iteration 55, loss = 19353322734.52477264\n",
      "Iteration 56, loss = 19353314371.77442551\n",
      "Iteration 57, loss = 19353305909.33631516\n",
      "Iteration 58, loss = 19353297738.47884369\n",
      "Iteration 59, loss = 19353289652.88459778\n",
      "Iteration 60, loss = 19353281704.15538406\n",
      "Iteration 61, loss = 19353273826.17514801\n",
      "Iteration 62, loss = 19353266046.83769989\n",
      "Iteration 63, loss = 19353258166.20959854\n",
      "Iteration 64, loss = 19353250550.78532028\n",
      "Iteration 65, loss = 19353243018.17187500\n",
      "Iteration 66, loss = 19353235372.02063751\n",
      "Iteration 67, loss = 19353227824.01163101\n",
      "Iteration 68, loss = 19353220233.55382156\n",
      "Iteration 69, loss = 19353212763.56528854\n",
      "Iteration 70, loss = 19353205333.84469223\n",
      "Iteration 71, loss = 19353198071.90929794\n",
      "Iteration 72, loss = 19353191037.00591660\n",
      "Iteration 73, loss = 19353184038.37403870\n",
      "Iteration 74, loss = 19353177270.10230637\n",
      "Iteration 75, loss = 19353170611.76948547\n",
      "Iteration 76, loss = 19353164087.34518814\n",
      "Iteration 77, loss = 19353157747.19115829\n",
      "Iteration 78, loss = 19353151288.52092743\n",
      "Iteration 79, loss = 19353145115.18261719\n",
      "Iteration 80, loss = 19353138957.33900833\n",
      "Iteration 81, loss = 19353132963.73472214\n",
      "Iteration 82, loss = 19353127140.77975082\n",
      "Iteration 83, loss = 19353121486.77886200\n",
      "Iteration 84, loss = 19353116029.11557770\n",
      "Iteration 85, loss = 19353110506.81583405\n",
      "Iteration 86, loss = 19353105200.99390793\n",
      "Iteration 87, loss = 19353099855.40920639\n",
      "Iteration 88, loss = 19353094606.74891663\n",
      "Iteration 89, loss = 19353089401.31448364\n",
      "Iteration 90, loss = 19353084344.82270050\n",
      "Iteration 91, loss = 19353079495.43725586\n",
      "Iteration 92, loss = 19353074622.87397766\n",
      "Iteration 93, loss = 19353070008.18518829\n",
      "Iteration 94, loss = 19353065356.00174713\n",
      "Iteration 95, loss = 19353060828.46090317\n",
      "Iteration 96, loss = 19353056331.16423035\n",
      "Iteration 97, loss = 19353051860.18443680\n",
      "Iteration 98, loss = 19353047422.35390472\n",
      "Iteration 99, loss = 19353043095.79401398\n",
      "Iteration 100, loss = 19353038779.48722839\n",
      "Iteration 101, loss = 19353034491.11438751\n",
      "Iteration 102, loss = 19353030308.03174591\n",
      "Iteration 103, loss = 19353026116.50979233\n",
      "Iteration 104, loss = 19353022063.34402847\n",
      "Iteration 105, loss = 19353018087.16810226\n",
      "Iteration 106, loss = 19353014155.95839691\n",
      "Iteration 107, loss = 19353010299.56980515\n",
      "Iteration 108, loss = 19353006451.76239014\n",
      "Iteration 109, loss = 19353002687.70724869\n",
      "Iteration 110, loss = 19352998973.55395508\n",
      "Iteration 111, loss = 19352995229.25423050\n",
      "Iteration 112, loss = 19352991532.29240036\n",
      "Iteration 113, loss = 19352987880.87167358\n",
      "Iteration 114, loss = 19352984218.15441513\n",
      "Iteration 115, loss = 19352980563.64884567\n",
      "Iteration 116, loss = 19352976914.96349716\n",
      "Iteration 117, loss = 19352973324.70746231\n",
      "Iteration 118, loss = 19352969705.71421432\n",
      "Iteration 119, loss = 19352966125.29395294\n",
      "Iteration 120, loss = 19352962551.45479965\n",
      "Iteration 121, loss = 19352958996.06831360\n",
      "Iteration 122, loss = 19352955500.55479813\n",
      "Iteration 123, loss = 19352952021.00307083\n",
      "Iteration 124, loss = 19352948547.18828964\n",
      "Iteration 125, loss = 19352945099.35219955\n",
      "Iteration 126, loss = 19352941650.17435837\n",
      "Iteration 127, loss = 19352938220.32849884\n",
      "Iteration 128, loss = 19352934801.97214890\n",
      "Iteration 129, loss = 19352931413.30251694\n",
      "Iteration 130, loss = 19352928039.12696457\n",
      "Iteration 131, loss = 19352924697.28523254\n",
      "Iteration 132, loss = 19352921324.73525620\n",
      "Iteration 133, loss = 19352918009.82649612\n",
      "Iteration 134, loss = 19352914719.67956161\n",
      "Iteration 135, loss = 19352911439.18891525\n",
      "Iteration 136, loss = 19352908141.46304703\n",
      "Iteration 137, loss = 19352904889.07633209\n",
      "Iteration 138, loss = 19352901671.70793533\n",
      "Iteration 139, loss = 19352898427.65888977\n",
      "Iteration 140, loss = 19352895191.57939148\n",
      "Iteration 141, loss = 19352891942.42847824\n",
      "Iteration 142, loss = 19352888773.68141174\n",
      "Iteration 143, loss = 19352885552.43268585\n",
      "Iteration 144, loss = 19352882406.61456299\n",
      "Iteration 145, loss = 19352879224.74770737\n",
      "Iteration 146, loss = 19352876054.31989288\n",
      "Iteration 147, loss = 19352872855.10761261\n",
      "Iteration 148, loss = 19352869758.44681931\n",
      "Iteration 149, loss = 19352866603.30448532\n",
      "Iteration 150, loss = 19352863449.16622925\n",
      "Iteration 151, loss = 19352860347.18560791\n",
      "Iteration 152, loss = 19352857226.07046127\n",
      "Iteration 153, loss = 19352854086.19741058\n",
      "Iteration 154, loss = 19352850990.27503586\n",
      "Iteration 155, loss = 19352847892.72145081\n",
      "Iteration 156, loss = 19352844819.37819672\n",
      "Iteration 157, loss = 19352841734.00930786\n",
      "Iteration 158, loss = 19352838644.42118454\n",
      "Iteration 159, loss = 19352835584.20688248\n",
      "Iteration 160, loss = 19352832508.33272934\n",
      "Iteration 161, loss = 19352829464.93859482\n",
      "Iteration 162, loss = 19352826366.42787170\n",
      "Iteration 163, loss = 19352823327.78202438\n",
      "Iteration 164, loss = 19352820292.77672577\n",
      "Iteration 165, loss = 19352817315.23427582\n",
      "Iteration 166, loss = 19352814257.53818512\n",
      "Iteration 167, loss = 19352811213.75334167\n",
      "Iteration 168, loss = 19352808191.31118393\n",
      "Iteration 169, loss = 19352805215.95516205\n",
      "Iteration 170, loss = 19352802179.24921799\n",
      "Iteration 171, loss = 19352799184.18442154\n",
      "Iteration 172, loss = 19352796179.31785583\n",
      "Iteration 173, loss = 19352793158.73573685\n",
      "Iteration 174, loss = 19352790200.78718567\n",
      "Iteration 175, loss = 19352787220.45942688\n",
      "Iteration 176, loss = 19352784269.96969223\n",
      "Iteration 177, loss = 19352781263.53329849\n",
      "Iteration 178, loss = 19352778296.30981064\n",
      "Iteration 179, loss = 19352775319.40802383\n",
      "Iteration 180, loss = 19352772358.25631714\n",
      "Iteration 181, loss = 19352769375.64881134\n",
      "Iteration 182, loss = 19352766453.98711014\n",
      "Iteration 183, loss = 19352763509.76930237\n",
      "Iteration 184, loss = 19352760562.89198303\n",
      "Iteration 185, loss = 19352757573.19139481\n",
      "Iteration 186, loss = 19352754664.85596848\n",
      "Iteration 187, loss = 19352751711.04671860\n",
      "Iteration 188, loss = 19352748777.47719955\n",
      "Iteration 189, loss = 19352745881.97722626\n",
      "Iteration 190, loss = 19352742936.22797394\n",
      "Iteration 191, loss = 19352740003.79148483\n",
      "Iteration 192, loss = 19352737083.60235596\n",
      "Iteration 193, loss = 19352734155.98167038\n",
      "Iteration 194, loss = 19352731262.20269775\n",
      "Iteration 195, loss = 19352728354.34838867\n",
      "Iteration 196, loss = 19352725461.14656830\n",
      "Iteration 197, loss = 19352722520.89599228\n",
      "Iteration 198, loss = 19352719634.60973358\n",
      "Iteration 199, loss = 19352716735.60363007\n",
      "Iteration 200, loss = 19352713859.34274673\n",
      "Iteration 201, loss = 19352710930.52961349\n",
      "Iteration 202, loss = 19352708059.65000534\n",
      "Iteration 203, loss = 19352705159.21590805\n",
      "Iteration 204, loss = 19352702305.73259354\n",
      "Iteration 205, loss = 19352699392.90253830\n",
      "Iteration 206, loss = 19352696516.99307632\n",
      "Iteration 207, loss = 19352693621.50207138\n",
      "Iteration 208, loss = 19352690764.85148621\n",
      "Iteration 209, loss = 19352687890.05638885\n",
      "Iteration 210, loss = 19352684995.97205734\n",
      "Iteration 211, loss = 19352682138.54656219\n",
      "Iteration 212, loss = 19352679266.88132477\n",
      "Iteration 213, loss = 19352676399.23859406\n",
      "Iteration 214, loss = 19352673522.77584076\n",
      "Iteration 215, loss = 19352670637.24066925\n",
      "Iteration 216, loss = 19352667799.72860336\n",
      "Iteration 217, loss = 19352664908.64212036\n",
      "Iteration 218, loss = 19352662029.87858200\n",
      "Iteration 219, loss = 19352659207.88531494\n",
      "Iteration 220, loss = 19352656307.56170273\n",
      "Iteration 221, loss = 19352653474.62660980\n",
      "Iteration 222, loss = 19352650579.41034698\n",
      "Iteration 223, loss = 19352647695.05085754\n",
      "Iteration 224, loss = 19352644827.34313583\n",
      "Iteration 225, loss = 19352641958.64434052\n",
      "Iteration 226, loss = 19352639070.58337784\n",
      "Iteration 227, loss = 19352636193.73352051\n",
      "Iteration 228, loss = 19352633321.16886902\n",
      "Iteration 229, loss = 19352630385.57183456\n",
      "Iteration 230, loss = 19352627530.25480270\n",
      "Iteration 231, loss = 19352624617.42859268\n",
      "Iteration 232, loss = 19352621686.65373993\n",
      "Iteration 233, loss = 19352618735.88035583\n",
      "Iteration 234, loss = 19352615814.17224121\n",
      "Iteration 235, loss = 19352612866.67378998\n",
      "Iteration 236, loss = 19352609903.18661118\n",
      "Iteration 237, loss = 19352606918.58750153\n",
      "Iteration 238, loss = 19352603937.45310211\n",
      "Iteration 239, loss = 19352600950.35712433\n",
      "Iteration 240, loss = 19352597921.42100525\n",
      "Iteration 241, loss = 19352594858.49927521\n",
      "Iteration 242, loss = 19352591811.01232147\n",
      "Iteration 243, loss = 19352588766.56110764\n",
      "Iteration 244, loss = 19352585707.20646286\n",
      "Iteration 245, loss = 19352582625.52997208\n",
      "Iteration 246, loss = 19352579531.63257980\n",
      "Iteration 247, loss = 19352576449.72459412\n",
      "Iteration 248, loss = 19352573339.70037079\n",
      "Iteration 249, loss = 19352570249.97809601\n",
      "Iteration 250, loss = 19352567136.42310715\n",
      "Iteration 251, loss = 19352564038.99675369\n",
      "Iteration 252, loss = 19352560963.20772934\n",
      "Iteration 253, loss = 19352557900.14290237\n",
      "Iteration 254, loss = 19352554777.95236206\n",
      "Iteration 255, loss = 19352551758.00246048\n",
      "Iteration 256, loss = 19352548653.76662445\n",
      "Iteration 257, loss = 19352545568.87152100\n",
      "Iteration 258, loss = 19352542546.65695190\n",
      "Iteration 259, loss = 19352539490.57424545\n",
      "Iteration 260, loss = 19352536428.10994339\n",
      "Iteration 261, loss = 19352533411.99546051\n",
      "Iteration 262, loss = 19352530354.11344910\n",
      "Iteration 263, loss = 19352527368.02862167\n",
      "Iteration 264, loss = 19352524330.42441559\n",
      "Iteration 265, loss = 19352521318.03332520\n",
      "Iteration 266, loss = 19352518302.28155136\n",
      "Iteration 267, loss = 19352515303.66489029\n",
      "Iteration 268, loss = 19352512328.67879105\n",
      "Iteration 269, loss = 19352509300.03173447\n",
      "Iteration 270, loss = 19352506343.47710800\n",
      "Iteration 271, loss = 19352503374.12364578\n",
      "Iteration 272, loss = 19352500393.32735825\n",
      "Iteration 273, loss = 19352497394.78467178\n",
      "Iteration 274, loss = 19352494439.92353821\n",
      "Iteration 275, loss = 19352491479.98157501\n",
      "Iteration 276, loss = 19352488534.05875397\n",
      "Iteration 277, loss = 19352485570.48854446\n",
      "Iteration 278, loss = 19352482629.41848373\n",
      "Iteration 279, loss = 19352479680.68394089\n",
      "Iteration 280, loss = 19352476742.22695923\n",
      "Iteration 281, loss = 19352473803.56173706\n",
      "Iteration 282, loss = 19352470888.25325775\n",
      "Iteration 283, loss = 19352467948.11784363\n",
      "Iteration 284, loss = 19352465001.11587143\n",
      "Iteration 285, loss = 19352462084.72530746\n",
      "Iteration 286, loss = 19352459173.42116547\n",
      "Iteration 287, loss = 19352456267.72825241\n",
      "Iteration 288, loss = 19352453324.91616440\n",
      "Iteration 289, loss = 19352450461.18951035\n",
      "Iteration 290, loss = 19352447517.58522415\n",
      "Iteration 291, loss = 19352444610.74698639\n",
      "Iteration 292, loss = 19352441708.39382935\n",
      "Iteration 293, loss = 19352438819.33638000\n",
      "Iteration 294, loss = 19352435922.34254074\n",
      "Iteration 295, loss = 19352433049.52243805\n",
      "Iteration 296, loss = 19352430129.41526794\n",
      "Iteration 297, loss = 19352427253.20765686\n",
      "Iteration 298, loss = 19352424333.93870163\n",
      "Iteration 299, loss = 19352421497.31190491\n",
      "Iteration 300, loss = 19352418590.00010300\n",
      "Iteration 301, loss = 19352415706.30781555\n",
      "Iteration 302, loss = 19352412836.66094208\n",
      "Iteration 303, loss = 19352409980.93427277\n",
      "Iteration 304, loss = 19352407098.81731415\n",
      "Iteration 305, loss = 19352404219.92866516\n",
      "Iteration 306, loss = 19352401345.54551697\n",
      "Iteration 307, loss = 19352398499.91526794\n",
      "Iteration 308, loss = 19352395600.52180099\n",
      "Iteration 309, loss = 19352392742.70661545\n",
      "Iteration 310, loss = 19352389903.43862534\n",
      "Iteration 311, loss = 19352387027.29756165\n",
      "Iteration 312, loss = 19352384191.08972931\n",
      "Iteration 313, loss = 19352381314.02974701\n",
      "Iteration 314, loss = 19352378459.40200806\n",
      "Iteration 315, loss = 19352375601.95896912\n",
      "Iteration 316, loss = 19352372734.94261169\n",
      "Iteration 317, loss = 19352369908.92402267\n",
      "Iteration 318, loss = 19352367067.34416962\n",
      "Iteration 319, loss = 19352364201.86614990\n",
      "Iteration 320, loss = 19352361312.42269135\n",
      "Iteration 321, loss = 19352358474.51122665\n",
      "Iteration 322, loss = 19352355665.80608368\n",
      "Iteration 323, loss = 19352352778.11848068\n",
      "Iteration 324, loss = 19352349895.41352081\n",
      "Iteration 325, loss = 19352347061.55335617\n",
      "Iteration 326, loss = 19352344206.68059540\n",
      "Iteration 327, loss = 19352341364.91665649\n",
      "Iteration 328, loss = 19352338524.26096725\n",
      "Iteration 329, loss = 19352335681.41060257\n",
      "Iteration 330, loss = 19352332815.66828156\n",
      "Iteration 331, loss = 19352330008.72214508\n",
      "Iteration 332, loss = 19352327122.40910339\n",
      "Iteration 333, loss = 19352324317.17478943\n",
      "Iteration 334, loss = 19352321499.18618393\n",
      "Iteration 335, loss = 19352318651.39658356\n",
      "Iteration 336, loss = 19352315814.04432678\n",
      "Iteration 337, loss = 19352312999.91248322\n",
      "Iteration 338, loss = 19352310162.60064697\n",
      "Iteration 339, loss = 19352307358.26786804\n",
      "Iteration 340, loss = 19352304503.82709885\n",
      "Iteration 341, loss = 19352301686.90968704\n",
      "Iteration 342, loss = 19352298866.75425339\n",
      "Iteration 343, loss = 19352296051.11878967\n",
      "Iteration 344, loss = 19352293247.23334122\n",
      "Iteration 345, loss = 19352290410.04817963\n",
      "Iteration 346, loss = 19352287606.33363724\n",
      "Iteration 347, loss = 19352284779.38513947\n",
      "Iteration 348, loss = 19352281948.39905930\n",
      "Iteration 349, loss = 19352279158.22438431\n",
      "Iteration 350, loss = 19352276343.11460876\n",
      "Iteration 351, loss = 19352273545.47570419\n",
      "Iteration 352, loss = 19352270751.08565903\n",
      "Iteration 353, loss = 19352267893.62483978\n",
      "Iteration 354, loss = 19352265113.53303146\n",
      "Iteration 355, loss = 19352262321.58532333\n",
      "Iteration 356, loss = 19352259498.02933121\n",
      "Iteration 357, loss = 19352256707.94083023\n",
      "Iteration 358, loss = 19352253863.90522003\n",
      "Iteration 359, loss = 19352251113.17255020\n",
      "Iteration 360, loss = 19352248314.23931503\n",
      "Iteration 361, loss = 19352245472.65557098\n",
      "Iteration 362, loss = 19352242677.79475403\n",
      "Iteration 363, loss = 19352239927.42284775\n",
      "Iteration 364, loss = 19352237105.63774109\n",
      "Iteration 365, loss = 19352234312.20911026\n",
      "Iteration 366, loss = 19352231507.25490952\n",
      "Iteration 367, loss = 19352228713.24140167\n",
      "Iteration 368, loss = 19352225938.37975693\n",
      "Iteration 369, loss = 19352223169.54074097\n",
      "Iteration 370, loss = 19352220324.47963333\n",
      "Iteration 371, loss = 19352217564.78755951\n",
      "Iteration 372, loss = 19352214773.38559341\n",
      "Iteration 373, loss = 19352211955.67558289\n",
      "Iteration 374, loss = 19352209203.26406860\n",
      "Iteration 375, loss = 19352206440.12944794\n",
      "Iteration 376, loss = 19352203633.97322464\n",
      "Iteration 377, loss = 19352200860.78461456\n",
      "Iteration 378, loss = 19352198074.38517761\n",
      "Iteration 379, loss = 19352195296.28720856\n",
      "Iteration 380, loss = 19352192505.15768433\n",
      "Iteration 381, loss = 19352189743.08820724\n",
      "Iteration 382, loss = 19352186946.37999725\n",
      "Iteration 383, loss = 19352184211.51227188\n",
      "Iteration 384, loss = 19352181406.00839996\n",
      "Iteration 385, loss = 19352178642.39081573\n",
      "Iteration 386, loss = 19352175867.50979996\n",
      "Iteration 387, loss = 19352173078.28096008\n",
      "Iteration 388, loss = 19352170331.66069031\n",
      "Iteration 389, loss = 19352167535.15639114\n",
      "Iteration 390, loss = 19352164776.34005737\n",
      "Iteration 391, loss = 19352162028.67357635\n",
      "Iteration 392, loss = 19352159226.22160721\n",
      "Iteration 393, loss = 19352156479.49592972\n",
      "Iteration 394, loss = 19352153687.65888977\n",
      "Iteration 395, loss = 19352150974.90802383\n",
      "Iteration 396, loss = 19352148170.81755066\n",
      "Iteration 397, loss = 19352145397.99217606\n",
      "Iteration 398, loss = 19352142636.43317413\n",
      "Iteration 399, loss = 19352139875.63834763\n",
      "Iteration 400, loss = 19352137128.37731171\n",
      "Iteration 401, loss = 19352134358.74433899\n",
      "Iteration 402, loss = 19352131617.40563202\n",
      "Iteration 403, loss = 19352128829.94501495\n",
      "Iteration 404, loss = 19352126121.45124054\n",
      "Iteration 405, loss = 19352123334.18505096\n",
      "Iteration 406, loss = 19352120574.20195770\n",
      "Iteration 407, loss = 19352117806.27326202\n",
      "Iteration 408, loss = 19352115042.09881592\n",
      "Iteration 409, loss = 19352112320.00103378\n",
      "Iteration 410, loss = 19352109571.38980865\n",
      "Iteration 411, loss = 19352106820.54739761\n",
      "Iteration 412, loss = 19352104059.50285339\n",
      "Iteration 413, loss = 19352101286.59074783\n",
      "Iteration 414, loss = 19352098542.18113708\n",
      "Iteration 415, loss = 19352095814.93026733\n",
      "Iteration 416, loss = 19352093042.06562042\n",
      "Iteration 417, loss = 19352090274.83960342\n",
      "Iteration 418, loss = 19352087543.95089340\n",
      "Iteration 419, loss = 19352084812.98136139\n",
      "Iteration 420, loss = 19352082023.13337708\n",
      "Iteration 421, loss = 19352079309.54043579\n",
      "Iteration 422, loss = 19352076564.14146423\n",
      "Iteration 423, loss = 19352073793.99962234\n",
      "Iteration 424, loss = 19352071049.98816299\n",
      "Iteration 425, loss = 19352068282.20716095\n",
      "Iteration 426, loss = 19352065564.18902206\n",
      "Iteration 427, loss = 19352062815.04746628\n",
      "Iteration 428, loss = 19352060062.74943542\n",
      "Iteration 429, loss = 19352057303.40454102\n",
      "Iteration 430, loss = 19352054582.47870636\n",
      "Iteration 431, loss = 19352051812.37823486\n",
      "Iteration 432, loss = 19352049083.46241379\n",
      "Iteration 433, loss = 19352046328.20024490\n",
      "Iteration 434, loss = 19352043583.90404129\n",
      "Iteration 435, loss = 19352040828.28144455\n",
      "Iteration 436, loss = 19352038096.57550049\n",
      "Iteration 437, loss = 19352035355.46699905\n",
      "Iteration 438, loss = 19352032606.64227295\n",
      "Iteration 439, loss = 19352029858.76911926\n",
      "Iteration 440, loss = 19352027122.89561462\n",
      "Iteration 441, loss = 19352024363.64622879\n",
      "Iteration 442, loss = 19352021626.91883850\n",
      "Iteration 443, loss = 19352018906.99052429\n",
      "Iteration 444, loss = 19352016153.14218903\n",
      "Iteration 445, loss = 19352013422.86968613\n",
      "Iteration 446, loss = 19352010664.50025558\n",
      "Iteration 447, loss = 19352007924.13358688\n",
      "Iteration 448, loss = 19352005197.16777420\n",
      "Iteration 449, loss = 19352002489.18817139\n",
      "Iteration 450, loss = 19351999739.01913071\n",
      "Iteration 451, loss = 19351996999.37162018\n",
      "Iteration 452, loss = 19351994254.98020554\n",
      "Iteration 453, loss = 19351991539.10641098\n",
      "Iteration 454, loss = 19351988811.54487610\n",
      "Iteration 455, loss = 19351986070.36319733\n",
      "Iteration 456, loss = 19351983338.53582764\n",
      "Iteration 457, loss = 19351980608.85453033\n",
      "Iteration 458, loss = 19351977895.38100815\n",
      "Iteration 459, loss = 19351975142.20178986\n",
      "Iteration 460, loss = 19351972454.15426254\n",
      "Iteration 461, loss = 19351969698.26311493\n",
      "Iteration 462, loss = 19351966942.98796463\n",
      "Iteration 463, loss = 19351964256.46702194\n",
      "Iteration 464, loss = 19351961524.89391327\n",
      "Iteration 465, loss = 19351958783.32380676\n",
      "Iteration 466, loss = 19351956058.98470306\n",
      "Iteration 467, loss = 19351953365.12309647\n",
      "Iteration 468, loss = 19351950603.13582993\n",
      "Iteration 469, loss = 19351947865.68078232\n",
      "Iteration 470, loss = 19351945150.40882111\n",
      "Iteration 471, loss = 19351942441.22939682\n",
      "Iteration 472, loss = 19351939726.16558838\n",
      "Iteration 473, loss = 19351936987.27688217\n",
      "Iteration 474, loss = 19351934256.41709137\n",
      "Iteration 475, loss = 19351931553.82607651\n",
      "Iteration 476, loss = 19351928804.06568146\n",
      "Iteration 477, loss = 19351926083.11110306\n",
      "Iteration 478, loss = 19351923384.61336899\n",
      "Iteration 479, loss = 19351920662.78709412\n",
      "Iteration 480, loss = 19351917918.26747894\n",
      "Iteration 481, loss = 19351915222.26852798\n",
      "Iteration 482, loss = 19351912500.60065460\n",
      "Iteration 483, loss = 19351909747.79830933\n",
      "Iteration 484, loss = 19351907042.13370132\n",
      "Iteration 485, loss = 19351904337.95689011\n",
      "Iteration 486, loss = 19351901610.44268417\n",
      "Iteration 487, loss = 19351898863.01293564\n",
      "Iteration 488, loss = 19351896143.77817917\n",
      "Iteration 489, loss = 19351893462.96506882\n",
      "Iteration 490, loss = 19351890745.77383804\n",
      "Iteration 491, loss = 19351888022.78200150\n",
      "Iteration 492, loss = 19351885266.76907349\n",
      "Iteration 493, loss = 19351882567.55084991\n",
      "Iteration 494, loss = 19351879861.65208054\n",
      "Iteration 495, loss = 19351877137.17088699\n",
      "Iteration 496, loss = 19351874405.09824753\n",
      "Iteration 497, loss = 19351871682.55070877\n",
      "Iteration 498, loss = 19351869003.14883423\n",
      "Iteration 499, loss = 19351866280.17414474\n",
      "Iteration 500, loss = 19351863546.80168152\n",
      "Iteration 1, loss = 20070789180.29888916\n",
      "Iteration 2, loss = 20070778668.95530319\n",
      "Iteration 3, loss = 20070768074.42161560\n",
      "Iteration 4, loss = 20070757412.45570374\n",
      "Iteration 5, loss = 20070746800.21691895\n",
      "Iteration 6, loss = 20070736179.75137711\n",
      "Iteration 7, loss = 20070725340.63237381\n",
      "Iteration 8, loss = 20070714625.20909882\n",
      "Iteration 9, loss = 20070703573.09730530\n",
      "Iteration 10, loss = 20070692569.54660034\n",
      "Iteration 11, loss = 20070681320.16498184\n",
      "Iteration 12, loss = 20070669898.60767365\n",
      "Iteration 13, loss = 20070658317.87379837\n",
      "Iteration 14, loss = 20070646587.30401993\n",
      "Iteration 15, loss = 20070634985.80194855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 20070623509.10908508\n",
      "Iteration 17, loss = 20070611941.68344498\n",
      "Iteration 18, loss = 20070600546.88348007\n",
      "Iteration 19, loss = 20070589080.84768295\n",
      "Iteration 20, loss = 20070577428.94383240\n",
      "Iteration 21, loss = 20070565621.11481476\n",
      "Iteration 22, loss = 20070553700.49416733\n",
      "Iteration 23, loss = 20070541537.17120743\n",
      "Iteration 24, loss = 20070529194.77476883\n",
      "Iteration 25, loss = 20070516409.75134659\n",
      "Iteration 26, loss = 20070503759.35451126\n",
      "Iteration 27, loss = 20070490979.86036682\n",
      "Iteration 28, loss = 20070478257.22274780\n",
      "Iteration 29, loss = 20070465407.59707260\n",
      "Iteration 30, loss = 20070452382.12775040\n",
      "Iteration 31, loss = 20070439099.82965088\n",
      "Iteration 32, loss = 20070425817.41790009\n",
      "Iteration 33, loss = 20070412424.48216248\n",
      "Iteration 34, loss = 20070399166.47802353\n",
      "Iteration 35, loss = 20070386636.72269821\n",
      "Iteration 36, loss = 20070374388.16509247\n",
      "Iteration 37, loss = 20070362508.58838654\n",
      "Iteration 38, loss = 20070350873.25119781\n",
      "Iteration 39, loss = 20070339469.74811935\n",
      "Iteration 40, loss = 20070328279.98470306\n",
      "Iteration 41, loss = 20070317183.73917770\n",
      "Iteration 42, loss = 20070306418.09609604\n",
      "Iteration 43, loss = 20070295831.12207413\n",
      "Iteration 44, loss = 20070285584.59245682\n",
      "Iteration 45, loss = 20070275823.58798981\n",
      "Iteration 46, loss = 20070266509.58501434\n",
      "Iteration 47, loss = 20070257583.75962448\n",
      "Iteration 48, loss = 20070248738.39785767\n",
      "Iteration 49, loss = 20070240002.98370743\n",
      "Iteration 50, loss = 20070231414.38800812\n",
      "Iteration 51, loss = 20070222888.81546402\n",
      "Iteration 52, loss = 20070214263.60948944\n",
      "Iteration 53, loss = 20070205630.24969101\n",
      "Iteration 54, loss = 20070197110.91233063\n",
      "Iteration 55, loss = 20070189256.90553665\n",
      "Iteration 56, loss = 20070181706.87951279\n",
      "Iteration 57, loss = 20070174543.93025589\n",
      "Iteration 58, loss = 20070167735.03833389\n",
      "Iteration 59, loss = 20070161301.42633820\n",
      "Iteration 60, loss = 20070155038.35656357\n",
      "Iteration 61, loss = 20070148979.97665787\n",
      "Iteration 62, loss = 20070142979.79813004\n",
      "Iteration 63, loss = 20070136956.72002411\n",
      "Iteration 64, loss = 20070131094.17195892\n",
      "Iteration 65, loss = 20070125268.97306442\n",
      "Iteration 66, loss = 20070119601.97006989\n",
      "Iteration 67, loss = 20070113893.49265289\n",
      "Iteration 68, loss = 20070108173.72643661\n",
      "Iteration 69, loss = 20070102592.22843170\n",
      "Iteration 70, loss = 20070097194.49775696\n",
      "Iteration 71, loss = 20070091951.40937042\n",
      "Iteration 72, loss = 20070086831.93878555\n",
      "Iteration 73, loss = 20070081744.17036057\n",
      "Iteration 74, loss = 20070076811.29962158\n",
      "Iteration 75, loss = 20070071978.51327133\n",
      "Iteration 76, loss = 20070067129.69127274\n",
      "Iteration 77, loss = 20070062299.86449051\n",
      "Iteration 78, loss = 20070057475.27076340\n",
      "Iteration 79, loss = 20070052632.63935089\n",
      "Iteration 80, loss = 20070047787.32678604\n",
      "Iteration 81, loss = 20070042978.87625885\n",
      "Iteration 82, loss = 20070038262.96967316\n",
      "Iteration 83, loss = 20070033520.28402710\n",
      "Iteration 84, loss = 20070028782.00682449\n",
      "Iteration 85, loss = 20070024121.04940796\n",
      "Iteration 86, loss = 20070019479.68787766\n",
      "Iteration 87, loss = 20070014853.11243057\n",
      "Iteration 88, loss = 20070010286.03791809\n",
      "Iteration 89, loss = 20070005676.29558945\n",
      "Iteration 90, loss = 20070001152.81723022\n",
      "Iteration 91, loss = 20069996621.90505219\n",
      "Iteration 92, loss = 20069992132.34823227\n",
      "Iteration 93, loss = 20069987596.38523483\n",
      "Iteration 94, loss = 20069983173.70981216\n",
      "Iteration 95, loss = 20069978706.99274826\n",
      "Iteration 96, loss = 20069974276.51987457\n",
      "Iteration 97, loss = 20069969867.14790726\n",
      "Iteration 98, loss = 20069965513.49746323\n",
      "Iteration 99, loss = 20069961068.86697006\n",
      "Iteration 100, loss = 20069956672.59297180\n",
      "Iteration 101, loss = 20069952296.42659760\n",
      "Iteration 102, loss = 20069947881.87542725\n",
      "Iteration 103, loss = 20069943433.20160294\n",
      "Iteration 104, loss = 20069939001.20834732\n",
      "Iteration 105, loss = 20069934510.06870651\n",
      "Iteration 106, loss = 20069930031.74846268\n",
      "Iteration 107, loss = 20069925563.62319183\n",
      "Iteration 108, loss = 20069921187.07345581\n",
      "Iteration 109, loss = 20069916847.80595398\n",
      "Iteration 110, loss = 20069912540.91305542\n",
      "Iteration 111, loss = 20069908236.84942627\n",
      "Iteration 112, loss = 20069903952.92353821\n",
      "Iteration 113, loss = 20069899697.57439804\n",
      "Iteration 114, loss = 20069895428.05762482\n",
      "Iteration 115, loss = 20069891185.72438049\n",
      "Iteration 116, loss = 20069886931.12076187\n",
      "Iteration 117, loss = 20069882727.74583817\n",
      "Iteration 118, loss = 20069878444.04325485\n",
      "Iteration 119, loss = 20069874180.31862259\n",
      "Iteration 120, loss = 20069869963.29601288\n",
      "Iteration 121, loss = 20069865642.17086792\n",
      "Iteration 122, loss = 20069861359.30628967\n",
      "Iteration 123, loss = 20069857067.89222717\n",
      "Iteration 124, loss = 20069852710.87378693\n",
      "Iteration 125, loss = 20069848357.42555237\n",
      "Iteration 126, loss = 20069843924.37816620\n",
      "Iteration 127, loss = 20069839454.96222687\n",
      "Iteration 128, loss = 20069834949.23935699\n",
      "Iteration 129, loss = 20069830393.43164444\n",
      "Iteration 130, loss = 20069825737.49615479\n",
      "Iteration 131, loss = 20069821043.98604965\n",
      "Iteration 132, loss = 20069816311.45793152\n",
      "Iteration 133, loss = 20069811505.30678177\n",
      "Iteration 134, loss = 20069806671.84857178\n",
      "Iteration 135, loss = 20069801773.74323654\n",
      "Iteration 136, loss = 20069796840.94276810\n",
      "Iteration 137, loss = 20069791891.90869141\n",
      "Iteration 138, loss = 20069786913.16341782\n",
      "Iteration 139, loss = 20069781909.28194809\n",
      "Iteration 140, loss = 20069776858.97733688\n",
      "Iteration 141, loss = 20069771889.94228745\n",
      "Iteration 142, loss = 20069766893.01570892\n",
      "Iteration 143, loss = 20069761839.42867661\n",
      "Iteration 144, loss = 20069756870.58419800\n",
      "Iteration 145, loss = 20069751972.04455566\n",
      "Iteration 146, loss = 20069747026.49570084\n",
      "Iteration 147, loss = 20069742122.73296738\n",
      "Iteration 148, loss = 20069737275.94616318\n",
      "Iteration 149, loss = 20069732460.88558960\n",
      "Iteration 150, loss = 20069727643.60833359\n",
      "Iteration 151, loss = 20069722859.04205704\n",
      "Iteration 152, loss = 20069718046.10226440\n",
      "Iteration 153, loss = 20069713349.60482407\n",
      "Iteration 154, loss = 20069708598.81890488\n",
      "Iteration 155, loss = 20069703913.67835617\n",
      "Iteration 156, loss = 20069699219.15626144\n",
      "Iteration 157, loss = 20069694522.70795822\n",
      "Iteration 158, loss = 20069689879.93709946\n",
      "Iteration 159, loss = 20069685202.29487610\n",
      "Iteration 160, loss = 20069680550.38178253\n",
      "Iteration 161, loss = 20069675959.06373215\n",
      "Iteration 162, loss = 20069671324.51795959\n",
      "Iteration 163, loss = 20069666741.97035980\n",
      "Iteration 164, loss = 20069662171.44124603\n",
      "Iteration 165, loss = 20069657611.32084656\n",
      "Iteration 166, loss = 20069653085.47359467\n",
      "Iteration 167, loss = 20069648571.58642578\n",
      "Iteration 168, loss = 20069644027.49968338\n",
      "Iteration 169, loss = 20069639567.32582855\n",
      "Iteration 170, loss = 20069635073.82705307\n",
      "Iteration 171, loss = 20069630533.66452408\n",
      "Iteration 172, loss = 20069626086.30043411\n",
      "Iteration 173, loss = 20069621599.39424515\n",
      "Iteration 174, loss = 20069617160.14610291\n",
      "Iteration 175, loss = 20069612733.92491531\n",
      "Iteration 176, loss = 20069608279.95323563\n",
      "Iteration 177, loss = 20069603851.57204056\n",
      "Iteration 178, loss = 20069599430.88838577\n",
      "Iteration 179, loss = 20069595005.20222855\n",
      "Iteration 180, loss = 20069590574.02737808\n",
      "Iteration 181, loss = 20069586188.78552246\n",
      "Iteration 182, loss = 20069581768.04577255\n",
      "Iteration 183, loss = 20069577358.58072662\n",
      "Iteration 184, loss = 20069573007.89036942\n",
      "Iteration 185, loss = 20069568564.95287323\n",
      "Iteration 186, loss = 20069564204.30203629\n",
      "Iteration 187, loss = 20069559813.08859634\n",
      "Iteration 188, loss = 20069555419.46134567\n",
      "Iteration 189, loss = 20069551052.04106903\n",
      "Iteration 190, loss = 20069546621.30857849\n",
      "Iteration 191, loss = 20069542317.20201111\n",
      "Iteration 192, loss = 20069537899.50012207\n",
      "Iteration 193, loss = 20069533532.97830963\n",
      "Iteration 194, loss = 20069529211.98741150\n",
      "Iteration 195, loss = 20069524852.73641205\n",
      "Iteration 196, loss = 20069520491.55632019\n",
      "Iteration 197, loss = 20069516159.45786285\n",
      "Iteration 198, loss = 20069511846.14170837\n",
      "Iteration 199, loss = 20069507562.43193054\n",
      "Iteration 200, loss = 20069503195.90114594\n",
      "Iteration 201, loss = 20069498900.80578995\n",
      "Iteration 202, loss = 20069494634.24023056\n",
      "Iteration 203, loss = 20069490353.20042038\n",
      "Iteration 204, loss = 20069486066.77951050\n",
      "Iteration 205, loss = 20069481786.68071747\n",
      "Iteration 206, loss = 20069477529.44255447\n",
      "Iteration 207, loss = 20069473286.18929672\n",
      "Iteration 208, loss = 20069468989.32650375\n",
      "Iteration 209, loss = 20069464759.42016602\n",
      "Iteration 210, loss = 20069460530.92710876\n",
      "Iteration 211, loss = 20069456265.34785461\n",
      "Iteration 212, loss = 20069452014.78576660\n",
      "Iteration 213, loss = 20069447768.48844910\n",
      "Iteration 214, loss = 20069443536.62475586\n",
      "Iteration 215, loss = 20069439280.92707825\n",
      "Iteration 216, loss = 20069435015.96432495\n",
      "Iteration 217, loss = 20069430796.12250900\n",
      "Iteration 218, loss = 20069426528.33731842\n",
      "Iteration 219, loss = 20069422324.15161133\n",
      "Iteration 220, loss = 20069418071.10819244\n",
      "Iteration 221, loss = 20069413851.66686630\n",
      "Iteration 222, loss = 20069409615.37196732\n",
      "Iteration 223, loss = 20069405411.78495789\n",
      "Iteration 224, loss = 20069401200.70991516\n",
      "Iteration 225, loss = 20069396989.89302444\n",
      "Iteration 226, loss = 20069392748.37232208\n",
      "Iteration 227, loss = 20069388524.76909256\n",
      "Iteration 228, loss = 20069384343.17176056\n",
      "Iteration 229, loss = 20069380117.30583572\n",
      "Iteration 230, loss = 20069375920.52666473\n",
      "Iteration 231, loss = 20069371695.26739502\n",
      "Iteration 232, loss = 20069367486.23251724\n",
      "Iteration 233, loss = 20069363355.45470047\n",
      "Iteration 234, loss = 20069359152.65103531\n",
      "Iteration 235, loss = 20069354971.81324005\n",
      "Iteration 236, loss = 20069350824.55380249\n",
      "Iteration 237, loss = 20069346661.67134857\n",
      "Iteration 238, loss = 20069342491.48020935\n",
      "Iteration 239, loss = 20069338324.11718369\n",
      "Iteration 240, loss = 20069334157.37262726\n",
      "Iteration 241, loss = 20069329986.91865158\n",
      "Iteration 242, loss = 20069325866.78141785\n",
      "Iteration 243, loss = 20069321670.45411301\n",
      "Iteration 244, loss = 20069317513.15962982\n",
      "Iteration 245, loss = 20069313343.39078522\n",
      "Iteration 246, loss = 20069309223.46678543\n",
      "Iteration 247, loss = 20069305088.27770996\n",
      "Iteration 248, loss = 20069300920.10678482\n",
      "Iteration 249, loss = 20069296810.34284973\n",
      "Iteration 250, loss = 20069292660.40352249\n",
      "Iteration 251, loss = 20069288573.52150345\n",
      "Iteration 252, loss = 20069284404.75004959\n",
      "Iteration 253, loss = 20069280285.16010666\n",
      "Iteration 254, loss = 20069276154.65040970\n",
      "Iteration 255, loss = 20069272034.20959854\n",
      "Iteration 256, loss = 20069267868.76973724\n",
      "Iteration 257, loss = 20069263706.78055573\n",
      "Iteration 258, loss = 20069259599.47460175\n",
      "Iteration 259, loss = 20069255459.06362915\n",
      "Iteration 260, loss = 20069251281.69521332\n",
      "Iteration 261, loss = 20069247182.32381439\n",
      "Iteration 262, loss = 20069243054.41301346\n",
      "Iteration 263, loss = 20069239013.63783264\n",
      "Iteration 264, loss = 20069234834.75045776\n",
      "Iteration 265, loss = 20069230777.87200928\n",
      "Iteration 266, loss = 20069226701.14540482\n",
      "Iteration 267, loss = 20069222615.50323868\n",
      "Iteration 268, loss = 20069218508.69715118\n",
      "Iteration 269, loss = 20069214422.01369095\n",
      "Iteration 270, loss = 20069210331.66200638\n",
      "Iteration 271, loss = 20069206234.57960129\n",
      "Iteration 272, loss = 20069202134.84790421\n",
      "Iteration 273, loss = 20069198062.29163361\n",
      "Iteration 274, loss = 20069193957.27644730\n",
      "Iteration 275, loss = 20069189876.22294998\n",
      "Iteration 276, loss = 20069185793.22920227\n",
      "Iteration 277, loss = 20069181725.77703476\n",
      "Iteration 278, loss = 20069177656.96644592\n",
      "Iteration 279, loss = 20069173570.43198395\n",
      "Iteration 280, loss = 20069169544.59848022\n",
      "Iteration 281, loss = 20069165482.25456619\n",
      "Iteration 282, loss = 20069161406.83021545\n",
      "Iteration 283, loss = 20069157363.98432159\n",
      "Iteration 284, loss = 20069153273.02788925\n",
      "Iteration 285, loss = 20069149217.33497620\n",
      "Iteration 286, loss = 20069145132.92616653\n",
      "Iteration 287, loss = 20069141069.22071457\n",
      "Iteration 288, loss = 20069136984.00118256\n",
      "Iteration 289, loss = 20069132914.03592682\n",
      "Iteration 290, loss = 20069128775.05590820\n",
      "Iteration 291, loss = 20069124743.62109756\n",
      "Iteration 292, loss = 20069120664.85452652\n",
      "Iteration 293, loss = 20069116531.87211990\n",
      "Iteration 294, loss = 20069112515.60007858\n",
      "Iteration 295, loss = 20069108428.83958054\n",
      "Iteration 296, loss = 20069104355.90886688\n",
      "Iteration 297, loss = 20069100324.96358490\n",
      "Iteration 298, loss = 20069096206.93862152\n",
      "Iteration 299, loss = 20069092162.00881577\n",
      "Iteration 300, loss = 20069088096.85570526\n",
      "Iteration 301, loss = 20069084046.31909561\n",
      "Iteration 302, loss = 20069079984.11909485\n",
      "Iteration 303, loss = 20069075949.86075211\n",
      "Iteration 304, loss = 20069071883.62998962\n",
      "Iteration 305, loss = 20069067830.64321518\n",
      "Iteration 306, loss = 20069063765.67561722\n",
      "Iteration 307, loss = 20069059695.62794113\n",
      "Iteration 308, loss = 20069055656.96542740\n",
      "Iteration 309, loss = 20069051606.82433319\n",
      "Iteration 310, loss = 20069047569.92689896\n",
      "Iteration 311, loss = 20069043508.48680878\n",
      "Iteration 312, loss = 20069039490.37097931\n",
      "Iteration 313, loss = 20069035446.72661972\n",
      "Iteration 314, loss = 20069031400.89556503\n",
      "Iteration 315, loss = 20069027340.79481125\n",
      "Iteration 316, loss = 20069023319.29068756\n",
      "Iteration 317, loss = 20069019248.18852997\n",
      "Iteration 318, loss = 20069015217.18509674\n",
      "Iteration 319, loss = 20069011153.20328903\n",
      "Iteration 320, loss = 20069007120.20279312\n",
      "Iteration 321, loss = 20069003082.90481949\n",
      "Iteration 322, loss = 20068999089.91372299\n",
      "Iteration 323, loss = 20068995075.25356293\n",
      "Iteration 324, loss = 20068991047.42356110\n",
      "Iteration 325, loss = 20068987045.85599899\n",
      "Iteration 326, loss = 20068983054.99264908\n",
      "Iteration 327, loss = 20068979021.26894760\n",
      "Iteration 328, loss = 20068974997.36329269\n",
      "Iteration 329, loss = 20068971004.31180191\n",
      "Iteration 330, loss = 20068966947.17905045\n",
      "Iteration 331, loss = 20068962947.45515442\n",
      "Iteration 332, loss = 20068958914.55817413\n",
      "Iteration 333, loss = 20068954885.68024445\n",
      "Iteration 334, loss = 20068950889.54524994\n",
      "Iteration 335, loss = 20068946851.12258911\n",
      "Iteration 336, loss = 20068942798.57754135\n",
      "Iteration 337, loss = 20068938812.55231476\n",
      "Iteration 338, loss = 20068934803.60744858\n",
      "Iteration 339, loss = 20068930783.82513046\n",
      "Iteration 340, loss = 20068926770.31879044\n",
      "Iteration 341, loss = 20068922768.33646011\n",
      "Iteration 342, loss = 20068918709.71350861\n",
      "Iteration 343, loss = 20068914707.64231110\n",
      "Iteration 344, loss = 20068910705.43946838\n",
      "Iteration 345, loss = 20068906643.46741486\n",
      "Iteration 346, loss = 20068902621.58971405\n",
      "Iteration 347, loss = 20068898597.98826981\n",
      "Iteration 348, loss = 20068894558.26120377\n",
      "Iteration 349, loss = 20068890566.47138596\n",
      "Iteration 350, loss = 20068886545.40823364\n",
      "Iteration 351, loss = 20068882533.87855530\n",
      "Iteration 352, loss = 20068878554.86027908\n",
      "Iteration 353, loss = 20068874590.01618576\n",
      "Iteration 354, loss = 20068870619.74334335\n",
      "Iteration 355, loss = 20068866633.05793762\n",
      "Iteration 356, loss = 20068862630.74206543\n",
      "Iteration 357, loss = 20068858651.12305832\n",
      "Iteration 358, loss = 20068854655.27318192\n",
      "Iteration 359, loss = 20068850669.51244736\n",
      "Iteration 360, loss = 20068846591.28021240\n",
      "Iteration 361, loss = 20068842600.55430984\n",
      "Iteration 362, loss = 20068838597.17121506\n",
      "Iteration 363, loss = 20068834556.74255371\n",
      "Iteration 364, loss = 20068830546.84518051\n",
      "Iteration 365, loss = 20068826529.92058945\n",
      "Iteration 366, loss = 20068822518.40976715\n",
      "Iteration 367, loss = 20068818493.31569672\n",
      "Iteration 368, loss = 20068814493.07797623\n",
      "Iteration 369, loss = 20068810485.58138275\n",
      "Iteration 370, loss = 20068806499.30692673\n",
      "Iteration 371, loss = 20068802466.25378799\n",
      "Iteration 372, loss = 20068798513.42900467\n",
      "Iteration 373, loss = 20068794519.71384811\n",
      "Iteration 374, loss = 20068790523.46638870\n",
      "Iteration 375, loss = 20068786534.21650696\n",
      "Iteration 376, loss = 20068782581.67235184\n",
      "Iteration 377, loss = 20068778588.50262070\n",
      "Iteration 378, loss = 20068774622.25750351\n",
      "Iteration 379, loss = 20068770673.73404312\n",
      "Iteration 380, loss = 20068766698.53047943\n",
      "Iteration 381, loss = 20068762726.24636841\n",
      "Iteration 382, loss = 20068758806.20523834\n",
      "Iteration 383, loss = 20068754843.99037933\n",
      "Iteration 384, loss = 20068750849.70418549\n",
      "Iteration 385, loss = 20068746873.72381592\n",
      "Iteration 386, loss = 20068742876.75442123\n",
      "Iteration 387, loss = 20068738888.44986725\n",
      "Iteration 388, loss = 20068734854.77047729\n",
      "Iteration 389, loss = 20068730861.92164993\n",
      "Iteration 390, loss = 20068726854.76709747\n",
      "Iteration 391, loss = 20068722862.55463028\n",
      "Iteration 392, loss = 20068718830.94511414\n",
      "Iteration 393, loss = 20068714831.77825546\n",
      "Iteration 394, loss = 20068710799.20911789\n",
      "Iteration 395, loss = 20068706826.62378693\n",
      "Iteration 396, loss = 20068702806.74136734\n",
      "Iteration 397, loss = 20068698825.43158722\n",
      "Iteration 398, loss = 20068694824.55693436\n",
      "Iteration 399, loss = 20068690854.03790283\n",
      "Iteration 400, loss = 20068686871.67709351\n",
      "Iteration 401, loss = 20068682928.58844757\n",
      "Iteration 402, loss = 20068678992.03236389\n",
      "Iteration 403, loss = 20068674975.82832336\n",
      "Iteration 404, loss = 20068671034.54870605\n",
      "Iteration 405, loss = 20068667064.53885269\n",
      "Iteration 406, loss = 20068663133.40802383\n",
      "Iteration 407, loss = 20068659157.04722595\n",
      "Iteration 408, loss = 20068655168.48558044\n",
      "Iteration 409, loss = 20068651211.53913498\n",
      "Iteration 410, loss = 20068647249.00282669\n",
      "Iteration 411, loss = 20068643298.80527115\n",
      "Iteration 412, loss = 20068639290.56499100\n",
      "Iteration 413, loss = 20068635400.42268753\n",
      "Iteration 414, loss = 20068631432.99575806\n",
      "Iteration 415, loss = 20068627464.83860397\n",
      "Iteration 416, loss = 20068623465.93627930\n",
      "Iteration 417, loss = 20068619537.19404602\n",
      "Iteration 418, loss = 20068615576.45360184\n",
      "Iteration 419, loss = 20068611645.07751846\n",
      "Iteration 420, loss = 20068607667.21892166\n",
      "Iteration 421, loss = 20068603714.80739212\n",
      "Iteration 422, loss = 20068599725.73985672\n",
      "Iteration 423, loss = 20068595749.30945206\n",
      "Iteration 424, loss = 20068591762.67005157\n",
      "Iteration 425, loss = 20068587789.26263809\n",
      "Iteration 426, loss = 20068583813.06379318\n",
      "Iteration 427, loss = 20068579840.21268845\n",
      "Iteration 428, loss = 20068575849.56956482\n",
      "Iteration 429, loss = 20068571905.02785873\n",
      "Iteration 430, loss = 20068567939.48590851\n",
      "Iteration 431, loss = 20068563954.86053085\n",
      "Iteration 432, loss = 20068560006.07785797\n",
      "Iteration 433, loss = 20068556049.20300293\n",
      "Iteration 434, loss = 20068552074.71891785\n",
      "Iteration 435, loss = 20068548102.50143814\n",
      "Iteration 436, loss = 20068544102.86613464\n",
      "Iteration 437, loss = 20068540164.19743347\n",
      "Iteration 438, loss = 20068536210.74227142\n",
      "Iteration 439, loss = 20068532216.46731567\n",
      "Iteration 440, loss = 20068528275.65291595\n",
      "Iteration 441, loss = 20068524318.86323166\n",
      "Iteration 442, loss = 20068520356.70298004\n",
      "Iteration 443, loss = 20068516439.85815811\n",
      "Iteration 444, loss = 20068512513.63065338\n",
      "Iteration 445, loss = 20068508532.51552582\n",
      "Iteration 446, loss = 20068504652.29225922\n",
      "Iteration 447, loss = 20068500671.23098755\n",
      "Iteration 448, loss = 20068496744.74080276\n",
      "Iteration 449, loss = 20068492760.55127335\n",
      "Iteration 450, loss = 20068488809.65790558\n",
      "Iteration 451, loss = 20068484898.41683197\n",
      "Iteration 452, loss = 20068480946.73166656\n",
      "Iteration 453, loss = 20068477034.97731018\n",
      "Iteration 454, loss = 20068473061.55643463\n",
      "Iteration 455, loss = 20068469166.47002411\n",
      "Iteration 456, loss = 20068465213.80064392\n",
      "Iteration 457, loss = 20068461271.35817719\n",
      "Iteration 458, loss = 20068457338.35879135\n",
      "Iteration 459, loss = 20068453445.88084793\n",
      "Iteration 460, loss = 20068449475.42463303\n",
      "Iteration 461, loss = 20068445528.45224762\n",
      "Iteration 462, loss = 20068441620.97503281\n",
      "Iteration 463, loss = 20068437680.79981232\n",
      "Iteration 464, loss = 20068433710.48097992\n",
      "Iteration 465, loss = 20068429769.26056290\n",
      "Iteration 466, loss = 20068425807.11991501\n",
      "Iteration 467, loss = 20068421839.68607712\n",
      "Iteration 468, loss = 20068417931.40074921\n",
      "Iteration 469, loss = 20068413951.43726730\n",
      "Iteration 470, loss = 20068410070.32116699\n",
      "Iteration 471, loss = 20068406113.29775238\n",
      "Iteration 472, loss = 20068402187.64085007\n",
      "Iteration 473, loss = 20068398265.25230789\n",
      "Iteration 474, loss = 20068394338.88723373\n",
      "Iteration 475, loss = 20068390363.71445084\n",
      "Iteration 476, loss = 20068386469.73967743\n",
      "Iteration 477, loss = 20068382577.67357635\n",
      "Iteration 478, loss = 20068378601.11674118\n",
      "Iteration 479, loss = 20068374678.08052444\n",
      "Iteration 480, loss = 20068370745.63634109\n",
      "Iteration 481, loss = 20068366795.34074020\n",
      "Iteration 482, loss = 20068362894.46554947\n",
      "Iteration 483, loss = 20068358898.97774124\n",
      "Iteration 484, loss = 20068354983.05999756\n",
      "Iteration 485, loss = 20068351054.24347687\n",
      "Iteration 486, loss = 20068347100.54037094\n",
      "Iteration 487, loss = 20068343182.67454910\n",
      "Iteration 488, loss = 20068339227.87113190\n",
      "Iteration 489, loss = 20068335316.39483643\n",
      "Iteration 490, loss = 20068331389.11209488\n",
      "Iteration 491, loss = 20068327502.20464706\n",
      "Iteration 492, loss = 20068323566.55907822\n",
      "Iteration 493, loss = 20068319651.07168961\n",
      "Iteration 494, loss = 20068315799.66710281\n",
      "Iteration 495, loss = 20068311890.87121582\n",
      "Iteration 496, loss = 20068307946.10843658\n",
      "Iteration 497, loss = 20068304031.97948074\n",
      "Iteration 498, loss = 20068300163.57187653\n",
      "Iteration 499, loss = 20068296227.51553726\n",
      "Iteration 500, loss = 20068292312.81358719\n",
      "Iteration 1, loss = 20051874882.02486038\n",
      "Iteration 2, loss = 20051864201.12132645\n",
      "Iteration 3, loss = 20051853715.36904907\n",
      "Iteration 4, loss = 20051843118.12553024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 20051832740.35468674\n",
      "Iteration 6, loss = 20051822326.42830276\n",
      "Iteration 7, loss = 20051811887.85482407\n",
      "Iteration 8, loss = 20051801671.95055008\n",
      "Iteration 9, loss = 20051791231.88624573\n",
      "Iteration 10, loss = 20051780770.30600739\n",
      "Iteration 11, loss = 20051770324.37010956\n",
      "Iteration 12, loss = 20051759581.33635712\n",
      "Iteration 13, loss = 20051749035.04626465\n",
      "Iteration 14, loss = 20051738458.25971985\n",
      "Iteration 15, loss = 20051727769.77931976\n",
      "Iteration 16, loss = 20051717283.85611725\n",
      "Iteration 17, loss = 20051706853.58053970\n",
      "Iteration 18, loss = 20051696259.38181686\n",
      "Iteration 19, loss = 20051685784.18964386\n",
      "Iteration 20, loss = 20051675584.34090042\n",
      "Iteration 21, loss = 20051665268.19827652\n",
      "Iteration 22, loss = 20051654906.63253784\n",
      "Iteration 23, loss = 20051644412.77379608\n",
      "Iteration 24, loss = 20051633480.34505463\n",
      "Iteration 25, loss = 20051622476.55678940\n",
      "Iteration 26, loss = 20051610994.12441254\n",
      "Iteration 27, loss = 20051599607.76436234\n",
      "Iteration 28, loss = 20051588248.62314606\n",
      "Iteration 29, loss = 20051576572.66274643\n",
      "Iteration 30, loss = 20051564213.75885010\n",
      "Iteration 31, loss = 20051551606.11415482\n",
      "Iteration 32, loss = 20051538724.52386475\n",
      "Iteration 33, loss = 20051525812.19367218\n",
      "Iteration 34, loss = 20051513046.36412430\n",
      "Iteration 35, loss = 20051500831.96150589\n",
      "Iteration 36, loss = 20051488667.90957260\n",
      "Iteration 37, loss = 20051476436.70115280\n",
      "Iteration 38, loss = 20051464214.55930328\n",
      "Iteration 39, loss = 20051452344.03145981\n",
      "Iteration 40, loss = 20051441279.09782791\n",
      "Iteration 41, loss = 20051430743.12883377\n",
      "Iteration 42, loss = 20051420402.96994019\n",
      "Iteration 43, loss = 20051410373.59654236\n",
      "Iteration 44, loss = 20051400459.57403564\n",
      "Iteration 45, loss = 20051390889.47224426\n",
      "Iteration 46, loss = 20051381142.70333481\n",
      "Iteration 47, loss = 20051371145.85221863\n",
      "Iteration 48, loss = 20051361259.66658020\n",
      "Iteration 49, loss = 20051351757.36957932\n",
      "Iteration 50, loss = 20051342722.69635010\n",
      "Iteration 51, loss = 20051333715.68071747\n",
      "Iteration 52, loss = 20051324373.65110779\n",
      "Iteration 53, loss = 20051315213.04547501\n",
      "Iteration 54, loss = 20051306283.78903198\n",
      "Iteration 55, loss = 20051297585.33720779\n",
      "Iteration 56, loss = 20051289277.68270874\n",
      "Iteration 57, loss = 20051281311.08581161\n",
      "Iteration 58, loss = 20051273670.78310776\n",
      "Iteration 59, loss = 20051266418.89348602\n",
      "Iteration 60, loss = 20051259595.82976913\n",
      "Iteration 61, loss = 20051252802.08618546\n",
      "Iteration 62, loss = 20051245875.41847610\n",
      "Iteration 63, loss = 20051239043.30319977\n",
      "Iteration 64, loss = 20051232523.07183456\n",
      "Iteration 65, loss = 20051226264.64866638\n",
      "Iteration 66, loss = 20051220218.64713287\n",
      "Iteration 67, loss = 20051214320.16560745\n",
      "Iteration 68, loss = 20051208673.55434036\n",
      "Iteration 69, loss = 20051203072.31482315\n",
      "Iteration 70, loss = 20051197458.65653229\n",
      "Iteration 71, loss = 20051191982.58757401\n",
      "Iteration 72, loss = 20051186906.40148163\n",
      "Iteration 73, loss = 20051181917.80119324\n",
      "Iteration 74, loss = 20051177037.51155853\n",
      "Iteration 75, loss = 20051172204.97036743\n",
      "Iteration 76, loss = 20051167427.53520966\n",
      "Iteration 77, loss = 20051162617.73015594\n",
      "Iteration 78, loss = 20051157752.33793259\n",
      "Iteration 79, loss = 20051152813.97307587\n",
      "Iteration 80, loss = 20051147930.05950165\n",
      "Iteration 81, loss = 20051143184.16504669\n",
      "Iteration 82, loss = 20051138514.50650406\n",
      "Iteration 83, loss = 20051133906.63250732\n",
      "Iteration 84, loss = 20051129214.27679443\n",
      "Iteration 85, loss = 20051124596.89887238\n",
      "Iteration 86, loss = 20051120065.64664459\n",
      "Iteration 87, loss = 20051115564.13364792\n",
      "Iteration 88, loss = 20051111115.30666351\n",
      "Iteration 89, loss = 20051106753.88913727\n",
      "Iteration 90, loss = 20051102422.41372681\n",
      "Iteration 91, loss = 20051098058.90505981\n",
      "Iteration 92, loss = 20051093697.50350189\n",
      "Iteration 93, loss = 20051089429.68458557\n",
      "Iteration 94, loss = 20051085124.82365799\n",
      "Iteration 95, loss = 20051080904.48174667\n",
      "Iteration 96, loss = 20051076686.60864639\n",
      "Iteration 97, loss = 20051072488.86573029\n",
      "Iteration 98, loss = 20051068253.40423584\n",
      "Iteration 99, loss = 20051064134.58405304\n",
      "Iteration 100, loss = 20051059936.10681534\n",
      "Iteration 101, loss = 20051055807.97445679\n",
      "Iteration 102, loss = 20051051669.30772781\n",
      "Iteration 103, loss = 20051047586.79718781\n",
      "Iteration 104, loss = 20051043488.61709595\n",
      "Iteration 105, loss = 20051039442.81044769\n",
      "Iteration 106, loss = 20051035342.95402145\n",
      "Iteration 107, loss = 20051031327.94792938\n",
      "Iteration 108, loss = 20051027292.76499176\n",
      "Iteration 109, loss = 20051023256.41476440\n",
      "Iteration 110, loss = 20051019227.00937271\n",
      "Iteration 111, loss = 20051015251.53427887\n",
      "Iteration 112, loss = 20051011269.61499405\n",
      "Iteration 113, loss = 20051007296.94683838\n",
      "Iteration 114, loss = 20051003315.56555176\n",
      "Iteration 115, loss = 20050999377.37886429\n",
      "Iteration 116, loss = 20050995383.74005890\n",
      "Iteration 117, loss = 20050991468.41349411\n",
      "Iteration 118, loss = 20050987543.40957642\n",
      "Iteration 119, loss = 20050983636.75837326\n",
      "Iteration 120, loss = 20050979732.09894562\n",
      "Iteration 121, loss = 20050975795.54293823\n",
      "Iteration 122, loss = 20050971891.10155106\n",
      "Iteration 123, loss = 20050968009.46136475\n",
      "Iteration 124, loss = 20050964094.04507446\n",
      "Iteration 125, loss = 20050960234.18738937\n",
      "Iteration 126, loss = 20050956385.42443085\n",
      "Iteration 127, loss = 20050952490.08314133\n",
      "Iteration 128, loss = 20050948576.14555359\n",
      "Iteration 129, loss = 20050944709.71271133\n",
      "Iteration 130, loss = 20050940834.50382233\n",
      "Iteration 131, loss = 20050936986.05688858\n",
      "Iteration 132, loss = 20050933090.48256683\n",
      "Iteration 133, loss = 20050929177.24008560\n",
      "Iteration 134, loss = 20050925362.95856476\n",
      "Iteration 135, loss = 20050921453.64749527\n",
      "Iteration 136, loss = 20050917537.22235489\n",
      "Iteration 137, loss = 20050913618.31236267\n",
      "Iteration 138, loss = 20050909744.22256088\n",
      "Iteration 139, loss = 20050905752.55347824\n",
      "Iteration 140, loss = 20050901803.51926422\n",
      "Iteration 141, loss = 20050897798.73591614\n",
      "Iteration 142, loss = 20050893789.93689728\n",
      "Iteration 143, loss = 20050889743.62165070\n",
      "Iteration 144, loss = 20050885695.30837250\n",
      "Iteration 145, loss = 20050881633.08476257\n",
      "Iteration 146, loss = 20050877549.72711563\n",
      "Iteration 147, loss = 20050873459.95701599\n",
      "Iteration 148, loss = 20050869306.10543823\n",
      "Iteration 149, loss = 20050865214.70666504\n",
      "Iteration 150, loss = 20050861053.72694778\n",
      "Iteration 151, loss = 20050856889.66362762\n",
      "Iteration 152, loss = 20050852745.11324692\n",
      "Iteration 153, loss = 20050848538.55602646\n",
      "Iteration 154, loss = 20050844355.86754227\n",
      "Iteration 155, loss = 20050840170.82471085\n",
      "Iteration 156, loss = 20050835983.18551254\n",
      "Iteration 157, loss = 20050831791.27420807\n",
      "Iteration 158, loss = 20050827612.86506271\n",
      "Iteration 159, loss = 20050823406.76431274\n",
      "Iteration 160, loss = 20050819202.15479279\n",
      "Iteration 161, loss = 20050814996.96184540\n",
      "Iteration 162, loss = 20050810834.75778961\n",
      "Iteration 163, loss = 20050806632.73528290\n",
      "Iteration 164, loss = 20050802431.41789627\n",
      "Iteration 165, loss = 20050798276.84177017\n",
      "Iteration 166, loss = 20050794030.87188721\n",
      "Iteration 167, loss = 20050789903.06210709\n",
      "Iteration 168, loss = 20050785680.42838287\n",
      "Iteration 169, loss = 20050781546.09736252\n",
      "Iteration 170, loss = 20050777311.83489227\n",
      "Iteration 171, loss = 20050773278.26364136\n",
      "Iteration 172, loss = 20050769116.25088501\n",
      "Iteration 173, loss = 20050764982.73781967\n",
      "Iteration 174, loss = 20050760907.22253418\n",
      "Iteration 175, loss = 20050756762.45353699\n",
      "Iteration 176, loss = 20050752714.10917664\n",
      "Iteration 177, loss = 20050748630.25495148\n",
      "Iteration 178, loss = 20050744559.48858643\n",
      "Iteration 179, loss = 20050740480.76702881\n",
      "Iteration 180, loss = 20050736455.71654129\n",
      "Iteration 181, loss = 20050732363.52339935\n",
      "Iteration 182, loss = 20050728335.25625229\n",
      "Iteration 183, loss = 20050724288.15460205\n",
      "Iteration 184, loss = 20050720248.46054840\n",
      "Iteration 185, loss = 20050716248.57978058\n",
      "Iteration 186, loss = 20050712253.88005447\n",
      "Iteration 187, loss = 20050708233.53815842\n",
      "Iteration 188, loss = 20050704218.94898987\n",
      "Iteration 189, loss = 20050700178.50375366\n",
      "Iteration 190, loss = 20050696207.35657501\n",
      "Iteration 191, loss = 20050692250.61928940\n",
      "Iteration 192, loss = 20050688266.24988556\n",
      "Iteration 193, loss = 20050684269.63341522\n",
      "Iteration 194, loss = 20050680312.46103668\n",
      "Iteration 195, loss = 20050676340.43226242\n",
      "Iteration 196, loss = 20050672399.01927567\n",
      "Iteration 197, loss = 20050668439.56961060\n",
      "Iteration 198, loss = 20050664513.67705154\n",
      "Iteration 199, loss = 20050660554.98626709\n",
      "Iteration 200, loss = 20050656614.04073715\n",
      "Iteration 201, loss = 20050652659.26979065\n",
      "Iteration 202, loss = 20050648759.86511612\n",
      "Iteration 203, loss = 20050644804.60884857\n",
      "Iteration 204, loss = 20050640911.54331589\n",
      "Iteration 205, loss = 20050636980.84494400\n",
      "Iteration 206, loss = 20050633079.01795959\n",
      "Iteration 207, loss = 20050629142.33651733\n",
      "Iteration 208, loss = 20050625245.28226852\n",
      "Iteration 209, loss = 20050621352.99418259\n",
      "Iteration 210, loss = 20050617451.23122406\n",
      "Iteration 211, loss = 20050613562.61608505\n",
      "Iteration 212, loss = 20050609710.17424011\n",
      "Iteration 213, loss = 20050605805.80302811\n",
      "Iteration 214, loss = 20050601927.19359207\n",
      "Iteration 215, loss = 20050598012.49706268\n",
      "Iteration 216, loss = 20050594164.41925812\n",
      "Iteration 217, loss = 20050590316.49199677\n",
      "Iteration 218, loss = 20050586419.59138489\n",
      "Iteration 219, loss = 20050582542.17922211\n",
      "Iteration 220, loss = 20050578696.62318420\n",
      "Iteration 221, loss = 20050574820.27685928\n",
      "Iteration 222, loss = 20050570987.32653809\n",
      "Iteration 223, loss = 20050567164.66187668\n",
      "Iteration 224, loss = 20050563260.21101761\n",
      "Iteration 225, loss = 20050559442.49923706\n",
      "Iteration 226, loss = 20050555546.54051208\n",
      "Iteration 227, loss = 20050551734.53554916\n",
      "Iteration 228, loss = 20050547907.58103561\n",
      "Iteration 229, loss = 20050544088.70775604\n",
      "Iteration 230, loss = 20050540193.22069550\n",
      "Iteration 231, loss = 20050536442.25656891\n",
      "Iteration 232, loss = 20050532574.27772903\n",
      "Iteration 233, loss = 20050528751.45182419\n",
      "Iteration 234, loss = 20050524957.06185913\n",
      "Iteration 235, loss = 20050521114.92556763\n",
      "Iteration 236, loss = 20050517312.82802582\n",
      "Iteration 237, loss = 20050513478.02795792\n",
      "Iteration 238, loss = 20050509685.41080475\n",
      "Iteration 239, loss = 20050505856.17960739\n",
      "Iteration 240, loss = 20050502024.36791611\n",
      "Iteration 241, loss = 20050498243.25886536\n",
      "Iteration 242, loss = 20050494442.42649841\n",
      "Iteration 243, loss = 20050490663.75227356\n",
      "Iteration 244, loss = 20050486825.72897720\n",
      "Iteration 245, loss = 20050483069.67737198\n",
      "Iteration 246, loss = 20050479287.60502625\n",
      "Iteration 247, loss = 20050475467.58185196\n",
      "Iteration 248, loss = 20050471678.27221680\n",
      "Iteration 249, loss = 20050467891.49293518\n",
      "Iteration 250, loss = 20050464112.40230179\n",
      "Iteration 251, loss = 20050460358.59266281\n",
      "Iteration 252, loss = 20050456516.35542679\n",
      "Iteration 253, loss = 20050452776.17834091\n",
      "Iteration 254, loss = 20050449030.60140610\n",
      "Iteration 255, loss = 20050445235.04508972\n",
      "Iteration 256, loss = 20050441416.19235611\n",
      "Iteration 257, loss = 20050437667.83696365\n",
      "Iteration 258, loss = 20050433918.87160110\n",
      "Iteration 259, loss = 20050430142.26469040\n",
      "Iteration 260, loss = 20050426372.25115967\n",
      "Iteration 261, loss = 20050422644.31441879\n",
      "Iteration 262, loss = 20050418864.49491119\n",
      "Iteration 263, loss = 20050415068.54199600\n",
      "Iteration 264, loss = 20050411337.35158157\n",
      "Iteration 265, loss = 20050407598.97911835\n",
      "Iteration 266, loss = 20050403819.49241638\n",
      "Iteration 267, loss = 20050400084.87694550\n",
      "Iteration 268, loss = 20050396308.64751816\n",
      "Iteration 269, loss = 20050392551.81747818\n",
      "Iteration 270, loss = 20050388875.52610779\n",
      "Iteration 271, loss = 20050385084.36210251\n",
      "Iteration 272, loss = 20050381318.71248245\n",
      "Iteration 273, loss = 20050377580.94762802\n",
      "Iteration 274, loss = 20050373837.76616669\n",
      "Iteration 275, loss = 20050370058.89731979\n",
      "Iteration 276, loss = 20050366359.27775955\n",
      "Iteration 277, loss = 20050362630.03920364\n",
      "Iteration 278, loss = 20050358898.39934540\n",
      "Iteration 279, loss = 20050355145.39413452\n",
      "Iteration 280, loss = 20050351417.94871902\n",
      "Iteration 281, loss = 20050347657.74485779\n",
      "Iteration 282, loss = 20050343969.82444763\n",
      "Iteration 283, loss = 20050340226.49647141\n",
      "Iteration 284, loss = 20050336473.03131104\n",
      "Iteration 285, loss = 20050332747.28908920\n",
      "Iteration 286, loss = 20050328993.58530045\n",
      "Iteration 287, loss = 20050325276.76365662\n",
      "Iteration 288, loss = 20050321552.96661377\n",
      "Iteration 289, loss = 20050317806.57849503\n",
      "Iteration 290, loss = 20050314089.69444275\n",
      "Iteration 291, loss = 20050310374.27909470\n",
      "Iteration 292, loss = 20050306635.63608170\n",
      "Iteration 293, loss = 20050302931.00656509\n",
      "Iteration 294, loss = 20050299217.28338242\n",
      "Iteration 295, loss = 20050295500.45696259\n",
      "Iteration 296, loss = 20050291781.59902954\n",
      "Iteration 297, loss = 20050288037.07648468\n",
      "Iteration 298, loss = 20050284378.98735046\n",
      "Iteration 299, loss = 20050280636.15124512\n",
      "Iteration 300, loss = 20050276958.12292099\n",
      "Iteration 301, loss = 20050273234.89800262\n",
      "Iteration 302, loss = 20050269560.04816437\n",
      "Iteration 303, loss = 20050265815.32248688\n",
      "Iteration 304, loss = 20050262137.63279724\n",
      "Iteration 305, loss = 20050258412.65888214\n",
      "Iteration 306, loss = 20050254717.05794144\n",
      "Iteration 307, loss = 20050251057.21210480\n",
      "Iteration 308, loss = 20050247325.48594284\n",
      "Iteration 309, loss = 20050243612.20094299\n",
      "Iteration 310, loss = 20050239938.78371048\n",
      "Iteration 311, loss = 20050236210.56707001\n",
      "Iteration 312, loss = 20050232534.54692459\n",
      "Iteration 313, loss = 20050228854.65778351\n",
      "Iteration 314, loss = 20050225141.80011749\n",
      "Iteration 315, loss = 20050221459.25343323\n",
      "Iteration 316, loss = 20050217775.94910431\n",
      "Iteration 317, loss = 20050214065.65255356\n",
      "Iteration 318, loss = 20050210403.75113297\n",
      "Iteration 319, loss = 20050206715.70396042\n",
      "Iteration 320, loss = 20050203037.83183670\n",
      "Iteration 321, loss = 20050199344.04955673\n",
      "Iteration 322, loss = 20050195653.93116760\n",
      "Iteration 323, loss = 20050191987.25348282\n",
      "Iteration 324, loss = 20050188297.97264099\n",
      "Iteration 325, loss = 20050184591.72534180\n",
      "Iteration 326, loss = 20050180912.03281784\n",
      "Iteration 327, loss = 20050177240.52301788\n",
      "Iteration 328, loss = 20050173560.32661819\n",
      "Iteration 329, loss = 20050169881.86543655\n",
      "Iteration 330, loss = 20050166216.87946320\n",
      "Iteration 331, loss = 20050162542.02838898\n",
      "Iteration 332, loss = 20050158860.13659668\n",
      "Iteration 333, loss = 20050155167.40782928\n",
      "Iteration 334, loss = 20050151495.31941605\n",
      "Iteration 335, loss = 20050147835.45698166\n",
      "Iteration 336, loss = 20050144160.94537354\n",
      "Iteration 337, loss = 20050140503.13466644\n",
      "Iteration 338, loss = 20050136843.98497772\n",
      "Iteration 339, loss = 20050133130.60724640\n",
      "Iteration 340, loss = 20050129497.66215134\n",
      "Iteration 341, loss = 20050125831.38749313\n",
      "Iteration 342, loss = 20050122117.29825974\n",
      "Iteration 343, loss = 20050118475.61508179\n",
      "Iteration 344, loss = 20050114863.06572342\n",
      "Iteration 345, loss = 20050111153.57915115\n",
      "Iteration 346, loss = 20050107482.39086914\n",
      "Iteration 347, loss = 20050103838.69118500\n",
      "Iteration 348, loss = 20050100128.09012222\n",
      "Iteration 349, loss = 20050096520.85845184\n",
      "Iteration 350, loss = 20050092817.95318222\n",
      "Iteration 351, loss = 20050089166.70743179\n",
      "Iteration 352, loss = 20050085523.94178391\n",
      "Iteration 353, loss = 20050081864.14841461\n",
      "Iteration 354, loss = 20050078207.28147888\n",
      "Iteration 355, loss = 20050074559.67747498\n",
      "Iteration 356, loss = 20050070891.39580917\n",
      "Iteration 357, loss = 20050067200.00652313\n",
      "Iteration 358, loss = 20050063587.91199493\n",
      "Iteration 359, loss = 20050059924.74270630\n",
      "Iteration 360, loss = 20050056311.81621552\n",
      "Iteration 361, loss = 20050052588.72620010\n",
      "Iteration 362, loss = 20050048977.26267242\n",
      "Iteration 363, loss = 20050045322.91062546\n",
      "Iteration 364, loss = 20050041662.07289124\n",
      "Iteration 365, loss = 20050038013.22547150\n",
      "Iteration 366, loss = 20050034384.10865784\n",
      "Iteration 367, loss = 20050030725.27202225\n",
      "Iteration 368, loss = 20050027065.57580948\n",
      "Iteration 369, loss = 20050023450.80393982\n",
      "Iteration 370, loss = 20050019762.74118423\n",
      "Iteration 371, loss = 20050016139.15782547\n",
      "Iteration 372, loss = 20050012492.59088898\n",
      "Iteration 373, loss = 20050008834.68787384\n",
      "Iteration 374, loss = 20050005198.30218506\n",
      "Iteration 375, loss = 20050001579.99435425\n",
      "Iteration 376, loss = 20049997939.18320084\n",
      "Iteration 377, loss = 20049994281.34493256\n",
      "Iteration 378, loss = 20049990670.28778458\n",
      "Iteration 379, loss = 20049987012.59345627\n",
      "Iteration 380, loss = 20049983357.56375504\n",
      "Iteration 381, loss = 20049979725.44088745\n",
      "Iteration 382, loss = 20049976089.43189621\n",
      "Iteration 383, loss = 20049972482.58146667\n",
      "Iteration 384, loss = 20049968806.50683594\n",
      "Iteration 385, loss = 20049965174.91548538\n",
      "Iteration 386, loss = 20049961537.46369171\n",
      "Iteration 387, loss = 20049957907.10483170\n",
      "Iteration 388, loss = 20049954258.93307114\n",
      "Iteration 389, loss = 20049950623.41799927\n",
      "Iteration 390, loss = 20049947022.92937469\n",
      "Iteration 391, loss = 20049943376.25535583\n",
      "Iteration 392, loss = 20049939750.05732346\n",
      "Iteration 393, loss = 20049936117.62551880\n",
      "Iteration 394, loss = 20049932503.70899963\n",
      "Iteration 395, loss = 20049928853.85700226\n",
      "Iteration 396, loss = 20049925210.86989594\n",
      "Iteration 397, loss = 20049921602.75642776\n",
      "Iteration 398, loss = 20049917942.99570084\n",
      "Iteration 399, loss = 20049914358.97869110\n",
      "Iteration 400, loss = 20049910709.22862244\n",
      "Iteration 401, loss = 20049907119.09043884\n",
      "Iteration 402, loss = 20049903433.49729156\n",
      "Iteration 403, loss = 20049899833.06907654\n",
      "Iteration 404, loss = 20049896200.53183365\n",
      "Iteration 405, loss = 20049892594.93710709\n",
      "Iteration 406, loss = 20049888988.40591049\n",
      "Iteration 407, loss = 20049885328.16920090\n",
      "Iteration 408, loss = 20049881699.34286880\n",
      "Iteration 409, loss = 20049878075.92084122\n",
      "Iteration 410, loss = 20049874464.17023468\n",
      "Iteration 411, loss = 20049870868.48544693\n",
      "Iteration 412, loss = 20049867220.55767059\n",
      "Iteration 413, loss = 20049863631.52561951\n",
      "Iteration 414, loss = 20049859965.18397903\n",
      "Iteration 415, loss = 20049856379.42229462\n",
      "Iteration 416, loss = 20049852741.92912674\n",
      "Iteration 417, loss = 20049849125.41653061\n",
      "Iteration 418, loss = 20049845499.22854233\n",
      "Iteration 419, loss = 20049841899.89806747\n",
      "Iteration 420, loss = 20049838278.50627518\n",
      "Iteration 421, loss = 20049834656.14138412\n",
      "Iteration 422, loss = 20049831019.63155365\n",
      "Iteration 423, loss = 20049827440.94159317\n",
      "Iteration 424, loss = 20049823834.80055237\n",
      "Iteration 425, loss = 20049820187.17558670\n",
      "Iteration 426, loss = 20049816563.09066010\n",
      "Iteration 427, loss = 20049812993.43140030\n",
      "Iteration 428, loss = 20049809342.23358536\n",
      "Iteration 429, loss = 20049805795.50379944\n",
      "Iteration 430, loss = 20049802133.14443207\n",
      "Iteration 431, loss = 20049798520.56389618\n",
      "Iteration 432, loss = 20049794897.03110504\n",
      "Iteration 433, loss = 20049791269.80307388\n",
      "Iteration 434, loss = 20049787694.98881531\n",
      "Iteration 435, loss = 20049784064.51206207\n",
      "Iteration 436, loss = 20049780435.99311066\n",
      "Iteration 437, loss = 20049776852.68111420\n",
      "Iteration 438, loss = 20049773265.85895157\n",
      "Iteration 439, loss = 20049769617.45952988\n",
      "Iteration 440, loss = 20049766031.54234314\n",
      "Iteration 441, loss = 20049762420.13429642\n",
      "Iteration 442, loss = 20049758793.46836090\n",
      "Iteration 443, loss = 20049755211.10317612\n",
      "Iteration 444, loss = 20049751597.66280365\n",
      "Iteration 445, loss = 20049748001.79972839\n",
      "Iteration 446, loss = 20049744364.12277222\n",
      "Iteration 447, loss = 20049740749.46312332\n",
      "Iteration 448, loss = 20049737164.60540009\n",
      "Iteration 449, loss = 20049733552.98358154\n",
      "Iteration 450, loss = 20049729984.44052124\n",
      "Iteration 451, loss = 20049726339.78411102\n",
      "Iteration 452, loss = 20049722722.99156189\n",
      "Iteration 453, loss = 20049719125.33556366\n",
      "Iteration 454, loss = 20049715552.22684860\n",
      "Iteration 455, loss = 20049711921.84243774\n",
      "Iteration 456, loss = 20049708369.55715942\n",
      "Iteration 457, loss = 20049704758.24078369\n",
      "Iteration 458, loss = 20049701095.19858170\n",
      "Iteration 459, loss = 20049697548.04381180\n",
      "Iteration 460, loss = 20049693940.53537369\n",
      "Iteration 461, loss = 20049690306.92066193\n",
      "Iteration 462, loss = 20049686709.86097336\n",
      "Iteration 463, loss = 20049683097.50379181\n",
      "Iteration 464, loss = 20049679511.66277313\n",
      "Iteration 465, loss = 20049675927.63102722\n",
      "Iteration 466, loss = 20049672315.82488632\n",
      "Iteration 467, loss = 20049668713.74283600\n",
      "Iteration 468, loss = 20049665107.86094666\n",
      "Iteration 469, loss = 20049661489.48297882\n",
      "Iteration 470, loss = 20049657942.17091751\n",
      "Iteration 471, loss = 20049654325.88620758\n",
      "Iteration 472, loss = 20049650708.23071671\n",
      "Iteration 473, loss = 20049647121.43895721\n",
      "Iteration 474, loss = 20049643521.35051727\n",
      "Iteration 475, loss = 20049639926.79222488\n",
      "Iteration 476, loss = 20049636326.77678680\n",
      "Iteration 477, loss = 20049632717.76751328\n",
      "Iteration 478, loss = 20049629129.09407806\n",
      "Iteration 479, loss = 20049625504.84572983\n",
      "Iteration 480, loss = 20049621932.56368256\n",
      "Iteration 481, loss = 20049618322.48457718\n",
      "Iteration 482, loss = 20049614706.13024521\n",
      "Iteration 483, loss = 20049611118.83383560\n",
      "Iteration 484, loss = 20049607507.37921906\n",
      "Iteration 485, loss = 20049603931.43662643\n",
      "Iteration 486, loss = 20049600299.62459183\n",
      "Iteration 487, loss = 20049596720.03747559\n",
      "Iteration 488, loss = 20049593089.68809891\n",
      "Iteration 489, loss = 20049589444.21359634\n",
      "Iteration 490, loss = 20049585871.80457306\n",
      "Iteration 491, loss = 20049582210.08485413\n",
      "Iteration 492, loss = 20049578586.31683731\n",
      "Iteration 493, loss = 20049574932.41131210\n",
      "Iteration 494, loss = 20049571244.65151978\n",
      "Iteration 495, loss = 20049567585.64857864\n",
      "Iteration 496, loss = 20049563851.78934479\n",
      "Iteration 497, loss = 20049560079.40392685\n",
      "Iteration 498, loss = 20049556278.11042404\n",
      "Iteration 499, loss = 20049552398.98197556\n",
      "Iteration 500, loss = 20049548400.22176361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19768196612.52438736\n",
      "Iteration 2, loss = 19768183747.22285843\n",
      "Iteration 3, loss = 19768170807.28268433\n",
      "Iteration 4, loss = 19768157735.38183975\n",
      "Iteration 5, loss = 19768144467.90162659\n",
      "Iteration 6, loss = 19768131166.23758698\n",
      "Iteration 7, loss = 19768117724.88376999\n",
      "Iteration 8, loss = 19768104436.74849319\n",
      "Iteration 9, loss = 19768091256.96015930\n",
      "Iteration 10, loss = 19768078192.30218887\n",
      "Iteration 11, loss = 19768065104.57229614\n",
      "Iteration 12, loss = 19768051685.70129395\n",
      "Iteration 13, loss = 19768038376.58946991\n",
      "Iteration 14, loss = 19768025286.11888504\n",
      "Iteration 15, loss = 19768011914.84448624\n",
      "Iteration 16, loss = 19767998084.08216095\n",
      "Iteration 17, loss = 19767983475.11812592\n",
      "Iteration 18, loss = 19767968239.93444061\n",
      "Iteration 19, loss = 19767952709.68826675\n",
      "Iteration 20, loss = 19767937317.04909134\n",
      "Iteration 21, loss = 19767922448.05238342\n",
      "Iteration 22, loss = 19767907661.35712433\n",
      "Iteration 23, loss = 19767893111.70991516\n",
      "Iteration 24, loss = 19767878820.31183243\n",
      "Iteration 25, loss = 19767863707.40147781\n",
      "Iteration 26, loss = 19767846866.46125793\n",
      "Iteration 27, loss = 19767830199.78521347\n",
      "Iteration 28, loss = 19767815286.33413315\n",
      "Iteration 29, loss = 19767801230.54226685\n",
      "Iteration 30, loss = 19767787224.41723251\n",
      "Iteration 31, loss = 19767773381.04349518\n",
      "Iteration 32, loss = 19767760620.86974335\n",
      "Iteration 33, loss = 19767748512.20650482\n",
      "Iteration 34, loss = 19767736497.16521072\n",
      "Iteration 35, loss = 19767724729.58000183\n",
      "Iteration 36, loss = 19767713829.86262131\n",
      "Iteration 37, loss = 19767703671.82532120\n",
      "Iteration 38, loss = 19767693932.45494461\n",
      "Iteration 39, loss = 19767684404.04244614\n",
      "Iteration 40, loss = 19767675030.83013916\n",
      "Iteration 41, loss = 19767666388.49174881\n",
      "Iteration 42, loss = 19767658291.15652466\n",
      "Iteration 43, loss = 19767650129.68787766\n",
      "Iteration 44, loss = 19767641739.97388458\n",
      "Iteration 45, loss = 19767633546.50333405\n",
      "Iteration 46, loss = 19767625578.63899612\n",
      "Iteration 47, loss = 19767617837.05664444\n",
      "Iteration 48, loss = 19767609893.98807526\n",
      "Iteration 49, loss = 19767602125.31337357\n",
      "Iteration 50, loss = 19767594872.57105637\n",
      "Iteration 51, loss = 19767587915.98347473\n",
      "Iteration 52, loss = 19767581345.18491745\n",
      "Iteration 53, loss = 19767574879.80971146\n",
      "Iteration 54, loss = 19767568367.56401062\n",
      "Iteration 55, loss = 19767561800.06425095\n",
      "Iteration 56, loss = 19767555353.87555695\n",
      "Iteration 57, loss = 19767548990.27409744\n",
      "Iteration 58, loss = 19767542780.81759262\n",
      "Iteration 59, loss = 19767536719.53849792\n",
      "Iteration 60, loss = 19767530715.15012360\n",
      "Iteration 61, loss = 19767524792.13659286\n",
      "Iteration 62, loss = 19767518921.10315323\n",
      "Iteration 63, loss = 19767513048.45632172\n",
      "Iteration 64, loss = 19767507272.59788132\n",
      "Iteration 65, loss = 19767501441.76528168\n",
      "Iteration 66, loss = 19767495653.63631821\n",
      "Iteration 67, loss = 19767489997.50075912\n",
      "Iteration 68, loss = 19767484423.63968277\n",
      "Iteration 69, loss = 19767478857.19985580\n",
      "Iteration 70, loss = 19767473348.72319794\n",
      "Iteration 71, loss = 19767467868.78699875\n",
      "Iteration 72, loss = 19767462406.47285843\n",
      "Iteration 73, loss = 19767457006.11364746\n",
      "Iteration 74, loss = 19767451626.29130173\n",
      "Iteration 75, loss = 19767446181.24660110\n",
      "Iteration 76, loss = 19767440790.33552170\n",
      "Iteration 77, loss = 19767435278.58006287\n",
      "Iteration 78, loss = 19767429905.70214844\n",
      "Iteration 79, loss = 19767424550.02815628\n",
      "Iteration 80, loss = 19767419150.63938522\n",
      "Iteration 81, loss = 19767413801.88537979\n",
      "Iteration 82, loss = 19767408297.60731506\n",
      "Iteration 83, loss = 19767402793.98657990\n",
      "Iteration 84, loss = 19767397276.67243958\n",
      "Iteration 85, loss = 19767391798.03726196\n",
      "Iteration 86, loss = 19767386330.12872314\n",
      "Iteration 87, loss = 19767380791.99673843\n",
      "Iteration 88, loss = 19767375234.47515869\n",
      "Iteration 89, loss = 19767369648.55637741\n",
      "Iteration 90, loss = 19767364024.35008240\n",
      "Iteration 91, loss = 19767358404.03551102\n",
      "Iteration 92, loss = 19767352746.60878754\n",
      "Iteration 93, loss = 19767347018.51383209\n",
      "Iteration 94, loss = 19767341346.73578262\n",
      "Iteration 95, loss = 19767335639.27716064\n",
      "Iteration 96, loss = 19767329876.65836716\n",
      "Iteration 97, loss = 19767324249.27528000\n",
      "Iteration 98, loss = 19767318533.41882324\n",
      "Iteration 99, loss = 19767312900.14898682\n",
      "Iteration 100, loss = 19767307250.75511932\n",
      "Iteration 101, loss = 19767301633.12277222\n",
      "Iteration 102, loss = 19767296049.91337204\n",
      "Iteration 103, loss = 19767290475.39994431\n",
      "Iteration 104, loss = 19767284945.03750229\n",
      "Iteration 105, loss = 19767279425.01762390\n",
      "Iteration 106, loss = 19767273896.68395996\n",
      "Iteration 107, loss = 19767268498.05233383\n",
      "Iteration 108, loss = 19767262979.86695862\n",
      "Iteration 109, loss = 19767257532.88882828\n",
      "Iteration 110, loss = 19767252135.80108261\n",
      "Iteration 111, loss = 19767246773.87602615\n",
      "Iteration 112, loss = 19767241416.95048523\n",
      "Iteration 113, loss = 19767236041.83195877\n",
      "Iteration 114, loss = 19767230767.80316544\n",
      "Iteration 115, loss = 19767225467.12080765\n",
      "Iteration 116, loss = 19767220139.72087097\n",
      "Iteration 117, loss = 19767214881.40759659\n",
      "Iteration 118, loss = 19767209619.55919266\n",
      "Iteration 119, loss = 19767204381.69569016\n",
      "Iteration 120, loss = 19767199135.83241653\n",
      "Iteration 121, loss = 19767193964.90954971\n",
      "Iteration 122, loss = 19767188711.61886597\n",
      "Iteration 123, loss = 19767183539.43653488\n",
      "Iteration 124, loss = 19767178359.57735443\n",
      "Iteration 125, loss = 19767173181.77282333\n",
      "Iteration 126, loss = 19767168019.62435913\n",
      "Iteration 127, loss = 19767162867.49738693\n",
      "Iteration 128, loss = 19767157651.86757660\n",
      "Iteration 129, loss = 19767152533.04634857\n",
      "Iteration 130, loss = 19767147339.24654388\n",
      "Iteration 131, loss = 19767142184.93725586\n",
      "Iteration 132, loss = 19767137042.87985611\n",
      "Iteration 133, loss = 19767131852.02704239\n",
      "Iteration 134, loss = 19767126644.56308746\n",
      "Iteration 135, loss = 19767121503.56565475\n",
      "Iteration 136, loss = 19767116271.80155182\n",
      "Iteration 137, loss = 19767111027.13864517\n",
      "Iteration 138, loss = 19767105770.17191315\n",
      "Iteration 139, loss = 19767100522.06486130\n",
      "Iteration 140, loss = 19767095151.72612762\n",
      "Iteration 141, loss = 19767089797.86736679\n",
      "Iteration 142, loss = 19767084371.89655304\n",
      "Iteration 143, loss = 19767078862.18235397\n",
      "Iteration 144, loss = 19767073348.07555389\n",
      "Iteration 145, loss = 19767067798.23938370\n",
      "Iteration 146, loss = 19767062193.41281128\n",
      "Iteration 147, loss = 19767056630.63251495\n",
      "Iteration 148, loss = 19767050985.16857147\n",
      "Iteration 149, loss = 19767045468.52300644\n",
      "Iteration 150, loss = 19767039921.76670074\n",
      "Iteration 151, loss = 19767034389.53764725\n",
      "Iteration 152, loss = 19767028926.65447235\n",
      "Iteration 153, loss = 19767023476.21408463\n",
      "Iteration 154, loss = 19767018001.50680542\n",
      "Iteration 155, loss = 19767012591.17321014\n",
      "Iteration 156, loss = 19767007213.56153107\n",
      "Iteration 157, loss = 19767001844.81935501\n",
      "Iteration 158, loss = 19766996428.65069962\n",
      "Iteration 159, loss = 19766991156.75251007\n",
      "Iteration 160, loss = 19766985806.57910538\n",
      "Iteration 161, loss = 19766980478.92340851\n",
      "Iteration 162, loss = 19766975192.76615143\n",
      "Iteration 163, loss = 19766969899.18434143\n",
      "Iteration 164, loss = 19766964601.28207016\n",
      "Iteration 165, loss = 19766959305.12774658\n",
      "Iteration 166, loss = 19766954069.97674942\n",
      "Iteration 167, loss = 19766948780.34918594\n",
      "Iteration 168, loss = 19766943566.38003159\n",
      "Iteration 169, loss = 19766938284.68731308\n",
      "Iteration 170, loss = 19766933109.44298935\n",
      "Iteration 171, loss = 19766927884.74248505\n",
      "Iteration 172, loss = 19766922692.16987610\n",
      "Iteration 173, loss = 19766917488.31997299\n",
      "Iteration 174, loss = 19766912300.07682037\n",
      "Iteration 175, loss = 19766907093.40753174\n",
      "Iteration 176, loss = 19766901982.34532547\n",
      "Iteration 177, loss = 19766896782.39634705\n",
      "Iteration 178, loss = 19766891628.01868820\n",
      "Iteration 179, loss = 19766886461.31975555\n",
      "Iteration 180, loss = 19766881356.93508530\n",
      "Iteration 181, loss = 19766876197.31155396\n",
      "Iteration 182, loss = 19766871095.04584122\n",
      "Iteration 183, loss = 19766865973.18471146\n",
      "Iteration 184, loss = 19766860868.71865082\n",
      "Iteration 185, loss = 19766855777.88076782\n",
      "Iteration 186, loss = 19766850695.70901108\n",
      "Iteration 187, loss = 19766845575.12977600\n",
      "Iteration 188, loss = 19766840544.98351669\n",
      "Iteration 189, loss = 19766835471.94113922\n",
      "Iteration 190, loss = 19766830379.31768799\n",
      "Iteration 191, loss = 19766825303.69929123\n",
      "Iteration 192, loss = 19766820285.57952499\n",
      "Iteration 193, loss = 19766815183.89724350\n",
      "Iteration 194, loss = 19766810133.03540802\n",
      "Iteration 195, loss = 19766805089.66636276\n",
      "Iteration 196, loss = 19766800000.28206253\n",
      "Iteration 197, loss = 19766794958.07880020\n",
      "Iteration 198, loss = 19766789906.61936188\n",
      "Iteration 199, loss = 19766784886.04072952\n",
      "Iteration 200, loss = 19766779854.55426788\n",
      "Iteration 201, loss = 19766774810.04505920\n",
      "Iteration 202, loss = 19766769788.06810379\n",
      "Iteration 203, loss = 19766764783.17198563\n",
      "Iteration 204, loss = 19766759762.13656998\n",
      "Iteration 205, loss = 19766754763.62788010\n",
      "Iteration 206, loss = 19766749750.26717758\n",
      "Iteration 207, loss = 19766744687.66981888\n",
      "Iteration 208, loss = 19766739665.39721680\n",
      "Iteration 209, loss = 19766734683.00662231\n",
      "Iteration 210, loss = 19766729690.88874054\n",
      "Iteration 211, loss = 19766724643.95113373\n",
      "Iteration 212, loss = 19766719654.46064758\n",
      "Iteration 213, loss = 19766714701.06232834\n",
      "Iteration 214, loss = 19766709703.32500458\n",
      "Iteration 215, loss = 19766704670.50353622\n",
      "Iteration 216, loss = 19766699749.24668121\n",
      "Iteration 217, loss = 19766694766.31935120\n",
      "Iteration 218, loss = 19766689734.89436340\n",
      "Iteration 219, loss = 19766684801.59251022\n",
      "Iteration 220, loss = 19766679822.64079285\n",
      "Iteration 221, loss = 19766674883.22795105\n",
      "Iteration 222, loss = 19766669917.05396652\n",
      "Iteration 223, loss = 19766664922.81854630\n",
      "Iteration 224, loss = 19766659949.89421463\n",
      "Iteration 225, loss = 19766655032.04950714\n",
      "Iteration 226, loss = 19766650054.78498840\n",
      "Iteration 227, loss = 19766645103.81828308\n",
      "Iteration 228, loss = 19766640177.75376511\n",
      "Iteration 229, loss = 19766635228.39630127\n",
      "Iteration 230, loss = 19766630311.20020676\n",
      "Iteration 231, loss = 19766625380.43084717\n",
      "Iteration 232, loss = 19766620398.62908936\n",
      "Iteration 233, loss = 19766615454.35547638\n",
      "Iteration 234, loss = 19766610522.24393463\n",
      "Iteration 235, loss = 19766605637.07108688\n",
      "Iteration 236, loss = 19766600688.11342621\n",
      "Iteration 237, loss = 19766595763.68841934\n",
      "Iteration 238, loss = 19766590849.20437241\n",
      "Iteration 239, loss = 19766585913.09006500\n",
      "Iteration 240, loss = 19766581036.61523819\n",
      "Iteration 241, loss = 19766576099.64375687\n",
      "Iteration 242, loss = 19766571177.14379883\n",
      "Iteration 243, loss = 19766566288.11902237\n",
      "Iteration 244, loss = 19766561357.04287338\n",
      "Iteration 245, loss = 19766556441.26901627\n",
      "Iteration 246, loss = 19766551566.73632431\n",
      "Iteration 247, loss = 19766546653.97419739\n",
      "Iteration 248, loss = 19766541751.12506866\n",
      "Iteration 249, loss = 19766536840.88686752\n",
      "Iteration 250, loss = 19766531926.29468155\n",
      "Iteration 251, loss = 19766527041.58996201\n",
      "Iteration 252, loss = 19766522180.19810867\n",
      "Iteration 253, loss = 19766517228.42108917\n",
      "Iteration 254, loss = 19766512405.06328583\n",
      "Iteration 255, loss = 19766507503.60902023\n",
      "Iteration 256, loss = 19766502644.23606491\n",
      "Iteration 257, loss = 19766497776.66897583\n",
      "Iteration 258, loss = 19766492932.34633255\n",
      "Iteration 259, loss = 19766488032.46746063\n",
      "Iteration 260, loss = 19766483193.79382706\n",
      "Iteration 261, loss = 19766478288.29311371\n",
      "Iteration 262, loss = 19766473470.76499557\n",
      "Iteration 263, loss = 19766468604.36110306\n",
      "Iteration 264, loss = 19766463747.36704636\n",
      "Iteration 265, loss = 19766458895.44435501\n",
      "Iteration 266, loss = 19766454030.75351715\n",
      "Iteration 267, loss = 19766449145.96830750\n",
      "Iteration 268, loss = 19766444297.38941193\n",
      "Iteration 269, loss = 19766439433.56089401\n",
      "Iteration 270, loss = 19766434555.15464401\n",
      "Iteration 271, loss = 19766429687.63047028\n",
      "Iteration 272, loss = 19766424823.63774872\n",
      "Iteration 273, loss = 19766419948.31879425\n",
      "Iteration 274, loss = 19766415090.04951477\n",
      "Iteration 275, loss = 19766410220.45318222\n",
      "Iteration 276, loss = 19766405365.92564392\n",
      "Iteration 277, loss = 19766400493.53065872\n",
      "Iteration 278, loss = 19766395658.71024704\n",
      "Iteration 279, loss = 19766390803.01968765\n",
      "Iteration 280, loss = 19766385945.34844208\n",
      "Iteration 281, loss = 19766381106.61970520\n",
      "Iteration 282, loss = 19766376262.00202179\n",
      "Iteration 283, loss = 19766371388.67816544\n",
      "Iteration 284, loss = 19766366531.29450607\n",
      "Iteration 285, loss = 19766361760.27487183\n",
      "Iteration 286, loss = 19766356843.75899124\n",
      "Iteration 287, loss = 19766352020.96388626\n",
      "Iteration 288, loss = 19766347199.83906555\n",
      "Iteration 289, loss = 19766342374.23199081\n",
      "Iteration 290, loss = 19766337522.24434662\n",
      "Iteration 291, loss = 19766332683.02502823\n",
      "Iteration 292, loss = 19766327867.48750305\n",
      "Iteration 293, loss = 19766323029.79480362\n",
      "Iteration 294, loss = 19766318217.60960007\n",
      "Iteration 295, loss = 19766313373.41386032\n",
      "Iteration 296, loss = 19766308584.88539505\n",
      "Iteration 297, loss = 19766303735.37737656\n",
      "Iteration 298, loss = 19766298932.61156082\n",
      "Iteration 299, loss = 19766294093.61302567\n",
      "Iteration 300, loss = 19766289297.77037430\n",
      "Iteration 301, loss = 19766284464.73692322\n",
      "Iteration 302, loss = 19766279639.50506592\n",
      "Iteration 303, loss = 19766274824.18424225\n",
      "Iteration 304, loss = 19766270015.90867615\n",
      "Iteration 305, loss = 19766265169.34115982\n",
      "Iteration 306, loss = 19766260440.01420212\n",
      "Iteration 307, loss = 19766255542.91920853\n",
      "Iteration 308, loss = 19766250735.26278305\n",
      "Iteration 309, loss = 19766245925.14780426\n",
      "Iteration 310, loss = 19766241105.70272827\n",
      "Iteration 311, loss = 19766236268.47418976\n",
      "Iteration 312, loss = 19766231478.49890900\n",
      "Iteration 313, loss = 19766226646.83951569\n",
      "Iteration 314, loss = 19766221824.25567627\n",
      "Iteration 315, loss = 19766217039.96074677\n",
      "Iteration 316, loss = 19766212218.25921249\n",
      "Iteration 317, loss = 19766207387.09102631\n",
      "Iteration 318, loss = 19766202592.08335876\n",
      "Iteration 319, loss = 19766197764.01584625\n",
      "Iteration 320, loss = 19766192917.29796219\n",
      "Iteration 321, loss = 19766188120.28905106\n",
      "Iteration 322, loss = 19766183312.10581970\n",
      "Iteration 323, loss = 19766178482.94618988\n",
      "Iteration 324, loss = 19766173675.95869446\n",
      "Iteration 325, loss = 19766168879.96107864\n",
      "Iteration 326, loss = 19766164045.88079834\n",
      "Iteration 327, loss = 19766159241.63894653\n",
      "Iteration 328, loss = 19766154433.97278214\n",
      "Iteration 329, loss = 19766149603.88560486\n",
      "Iteration 330, loss = 19766144846.12548447\n",
      "Iteration 331, loss = 19766140073.03384018\n",
      "Iteration 332, loss = 19766135256.27895355\n",
      "Iteration 333, loss = 19766130506.23855209\n",
      "Iteration 334, loss = 19766125712.13227463\n",
      "Iteration 335, loss = 19766120961.31595612\n",
      "Iteration 336, loss = 19766116155.73601151\n",
      "Iteration 337, loss = 19766111367.32438278\n",
      "Iteration 338, loss = 19766106603.78846359\n",
      "Iteration 339, loss = 19766101857.29450226\n",
      "Iteration 340, loss = 19766097021.10726166\n",
      "Iteration 341, loss = 19766092259.33000183\n",
      "Iteration 342, loss = 19766087483.95544434\n",
      "Iteration 343, loss = 19766082699.19912720\n",
      "Iteration 344, loss = 19766077896.61817932\n",
      "Iteration 345, loss = 19766073135.76369476\n",
      "Iteration 346, loss = 19766068358.09768295\n",
      "Iteration 347, loss = 19766063581.04433823\n",
      "Iteration 348, loss = 19766058807.45418930\n",
      "Iteration 349, loss = 19766054043.59798813\n",
      "Iteration 350, loss = 19766049255.87228775\n",
      "Iteration 351, loss = 19766044494.80317307\n",
      "Iteration 352, loss = 19766039675.34661865\n",
      "Iteration 353, loss = 19766034844.91391373\n",
      "Iteration 354, loss = 19766030128.99848175\n",
      "Iteration 355, loss = 19766025284.44078827\n",
      "Iteration 356, loss = 19766020563.91004181\n",
      "Iteration 357, loss = 19766015743.72243500\n",
      "Iteration 358, loss = 19766010966.83578873\n",
      "Iteration 359, loss = 19766006175.99708176\n",
      "Iteration 360, loss = 19766001427.19426727\n",
      "Iteration 361, loss = 19765996649.88310242\n",
      "Iteration 362, loss = 19765991845.41797638\n",
      "Iteration 363, loss = 19765987104.80086899\n",
      "Iteration 364, loss = 19765982323.11084747\n",
      "Iteration 365, loss = 19765977502.34928131\n",
      "Iteration 366, loss = 19765972755.96096039\n",
      "Iteration 367, loss = 19765967965.49774933\n",
      "Iteration 368, loss = 19765963161.76005554\n",
      "Iteration 369, loss = 19765958392.88535309\n",
      "Iteration 370, loss = 19765953621.42341614\n",
      "Iteration 371, loss = 19765948842.04931641\n",
      "Iteration 372, loss = 19765944056.32184601\n",
      "Iteration 373, loss = 19765939291.25316620\n",
      "Iteration 374, loss = 19765934561.40795898\n",
      "Iteration 375, loss = 19765929764.61957550\n",
      "Iteration 376, loss = 19765925045.95241547\n",
      "Iteration 377, loss = 19765920280.49514008\n",
      "Iteration 378, loss = 19765915489.26919174\n",
      "Iteration 379, loss = 19765910774.90580750\n",
      "Iteration 380, loss = 19765905992.48367310\n",
      "Iteration 381, loss = 19765901268.18013763\n",
      "Iteration 382, loss = 19765896507.10891724\n",
      "Iteration 383, loss = 19765891731.24836731\n",
      "Iteration 384, loss = 19765886969.12491608\n",
      "Iteration 385, loss = 19765882235.58539581\n",
      "Iteration 386, loss = 19765877463.07974243\n",
      "Iteration 387, loss = 19765872690.82350540\n",
      "Iteration 388, loss = 19765867921.74317551\n",
      "Iteration 389, loss = 19765863156.11773300\n",
      "Iteration 390, loss = 19765858383.53863144\n",
      "Iteration 391, loss = 19765853627.68807602\n",
      "Iteration 392, loss = 19765848886.80435944\n",
      "Iteration 393, loss = 19765844084.30769730\n",
      "Iteration 394, loss = 19765839324.58042526\n",
      "Iteration 395, loss = 19765834642.84558487\n",
      "Iteration 396, loss = 19765829846.75738144\n",
      "Iteration 397, loss = 19765825096.38188171\n",
      "Iteration 398, loss = 19765820341.52304840\n",
      "Iteration 399, loss = 19765815583.25918961\n",
      "Iteration 400, loss = 19765810811.37046432\n",
      "Iteration 401, loss = 19765806024.90602493\n",
      "Iteration 402, loss = 19765801347.70254517\n",
      "Iteration 403, loss = 19765796582.20165634\n",
      "Iteration 404, loss = 19765791816.87963867\n",
      "Iteration 405, loss = 19765787077.24612045\n",
      "Iteration 406, loss = 19765782315.32805634\n",
      "Iteration 407, loss = 19765777603.66574478\n",
      "Iteration 408, loss = 19765772849.82620621\n",
      "Iteration 409, loss = 19765768116.91718292\n",
      "Iteration 410, loss = 19765763377.86691284\n",
      "Iteration 411, loss = 19765758626.79829025\n",
      "Iteration 412, loss = 19765753892.94352341\n",
      "Iteration 413, loss = 19765749150.15886688\n",
      "Iteration 414, loss = 19765744388.89196014\n",
      "Iteration 415, loss = 19765739630.28177643\n",
      "Iteration 416, loss = 19765734939.12961578\n",
      "Iteration 417, loss = 19765730201.66808701\n",
      "Iteration 418, loss = 19765725434.96543121\n",
      "Iteration 419, loss = 19765720721.47024536\n",
      "Iteration 420, loss = 19765715980.69909286\n",
      "Iteration 421, loss = 19765711219.32598877\n",
      "Iteration 422, loss = 19765706504.22071075\n",
      "Iteration 423, loss = 19765701761.65945053\n",
      "Iteration 424, loss = 19765696998.34175873\n",
      "Iteration 425, loss = 19765692280.32979965\n",
      "Iteration 426, loss = 19765687523.45046997\n",
      "Iteration 427, loss = 19765682801.35825348\n",
      "Iteration 428, loss = 19765678047.59386063\n",
      "Iteration 429, loss = 19765673283.09576416\n",
      "Iteration 430, loss = 19765668527.42979050\n",
      "Iteration 431, loss = 19765663784.05204010\n",
      "Iteration 432, loss = 19765659033.43840790\n",
      "Iteration 433, loss = 19765654265.22527313\n",
      "Iteration 434, loss = 19765649570.24732971\n",
      "Iteration 435, loss = 19765644789.02313232\n",
      "Iteration 436, loss = 19765640047.75498581\n",
      "Iteration 437, loss = 19765635307.12722397\n",
      "Iteration 438, loss = 19765630568.19783783\n",
      "Iteration 439, loss = 19765625894.37445450\n",
      "Iteration 440, loss = 19765621086.51345825\n",
      "Iteration 441, loss = 19765616392.68944931\n",
      "Iteration 442, loss = 19765611647.43380737\n",
      "Iteration 443, loss = 19765606951.03859711\n",
      "Iteration 444, loss = 19765602184.28149414\n",
      "Iteration 445, loss = 19765597512.33819580\n",
      "Iteration 446, loss = 19765592746.89649963\n",
      "Iteration 447, loss = 19765588048.66127396\n",
      "Iteration 448, loss = 19765583348.47687531\n",
      "Iteration 449, loss = 19765578620.01870728\n",
      "Iteration 450, loss = 19765573867.25619888\n",
      "Iteration 451, loss = 19765569135.59240341\n",
      "Iteration 452, loss = 19765564410.37416840\n",
      "Iteration 453, loss = 19765559701.90237045\n",
      "Iteration 454, loss = 19765554963.73123932\n",
      "Iteration 455, loss = 19765550215.08692169\n",
      "Iteration 456, loss = 19765545498.74987793\n",
      "Iteration 457, loss = 19765540752.41310120\n",
      "Iteration 458, loss = 19765536030.89419937\n",
      "Iteration 459, loss = 19765531266.10815048\n",
      "Iteration 460, loss = 19765526558.04429245\n",
      "Iteration 461, loss = 19765521838.47198868\n",
      "Iteration 462, loss = 19765517118.06567001\n",
      "Iteration 463, loss = 19765512379.69480133\n",
      "Iteration 464, loss = 19765507658.69058990\n",
      "Iteration 465, loss = 19765502916.90420151\n",
      "Iteration 466, loss = 19765498193.67586899\n",
      "Iteration 467, loss = 19765493479.72824097\n",
      "Iteration 468, loss = 19765488748.82471085\n",
      "Iteration 469, loss = 19765483986.90161896\n",
      "Iteration 470, loss = 19765479323.49971390\n",
      "Iteration 471, loss = 19765474605.74922943\n",
      "Iteration 472, loss = 19765469865.77466965\n",
      "Iteration 473, loss = 19765465109.38043976\n",
      "Iteration 474, loss = 19765460418.82929230\n",
      "Iteration 475, loss = 19765455730.73694611\n",
      "Iteration 476, loss = 19765451015.27643967\n",
      "Iteration 477, loss = 19765446292.07844925\n",
      "Iteration 478, loss = 19765441592.57598877\n",
      "Iteration 479, loss = 19765436914.35803604\n",
      "Iteration 480, loss = 19765432172.47176743\n",
      "Iteration 481, loss = 19765427457.86024475\n",
      "Iteration 482, loss = 19765422755.68051910\n",
      "Iteration 483, loss = 19765418035.67731476\n",
      "Iteration 484, loss = 19765413365.23729706\n",
      "Iteration 485, loss = 19765408598.21791077\n",
      "Iteration 486, loss = 19765403931.86810303\n",
      "Iteration 487, loss = 19765399218.03657150\n",
      "Iteration 488, loss = 19765394497.69287872\n",
      "Iteration 489, loss = 19765389785.69359589\n",
      "Iteration 490, loss = 19765385077.58081055\n",
      "Iteration 491, loss = 19765380367.50740814\n",
      "Iteration 492, loss = 19765375690.54859161\n",
      "Iteration 493, loss = 19765370939.75617981\n",
      "Iteration 494, loss = 19765366222.49230576\n",
      "Iteration 495, loss = 19765361524.85630417\n",
      "Iteration 496, loss = 19765356805.06203461\n",
      "Iteration 497, loss = 19765352085.15027237\n",
      "Iteration 498, loss = 19765347366.90941238\n",
      "Iteration 499, loss = 19765342692.07423782\n",
      "Iteration 500, loss = 19765337942.71807861\n",
      "Iteration 1, loss = 19414307480.44296646\n",
      "Iteration 2, loss = 19414288029.39952850\n",
      "Iteration 3, loss = 19414268552.68778229\n",
      "Iteration 4, loss = 19414248949.98061752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 19414229186.43280792\n",
      "Iteration 6, loss = 19414209011.39141846\n",
      "Iteration 7, loss = 19414188890.01157379\n",
      "Iteration 8, loss = 19414168456.12849426\n",
      "Iteration 9, loss = 19414147808.06261063\n",
      "Iteration 10, loss = 19414126658.48350143\n",
      "Iteration 11, loss = 19414105490.49875259\n",
      "Iteration 12, loss = 19414084066.49913788\n",
      "Iteration 13, loss = 19414062196.58309555\n",
      "Iteration 14, loss = 19414041017.78796768\n",
      "Iteration 15, loss = 19414020376.57186890\n",
      "Iteration 16, loss = 19413999487.23495483\n",
      "Iteration 17, loss = 19413978851.91622925\n",
      "Iteration 18, loss = 19413958698.91132355\n",
      "Iteration 19, loss = 19413938655.21179199\n",
      "Iteration 20, loss = 19413917601.59851456\n",
      "Iteration 21, loss = 19413897270.36348343\n",
      "Iteration 22, loss = 19413878033.88345718\n",
      "Iteration 23, loss = 19413858947.58031845\n",
      "Iteration 24, loss = 19413842366.18979263\n",
      "Iteration 25, loss = 19413827062.88560867\n",
      "Iteration 26, loss = 19413812199.12335587\n",
      "Iteration 27, loss = 19413796967.39970779\n",
      "Iteration 28, loss = 19413781189.59442902\n",
      "Iteration 29, loss = 19413764837.25901413\n",
      "Iteration 30, loss = 19413749206.66699219\n",
      "Iteration 31, loss = 19413734911.27854538\n",
      "Iteration 32, loss = 19413721483.48397446\n",
      "Iteration 33, loss = 19413708898.30041122\n",
      "Iteration 34, loss = 19413696888.90433884\n",
      "Iteration 35, loss = 19413684761.01565552\n",
      "Iteration 36, loss = 19413672533.36606598\n",
      "Iteration 37, loss = 19413661498.17187119\n",
      "Iteration 38, loss = 19413651743.90453720\n",
      "Iteration 39, loss = 19413642089.38883972\n",
      "Iteration 40, loss = 19413632624.78103256\n",
      "Iteration 41, loss = 19413623711.64526749\n",
      "Iteration 42, loss = 19413614952.52689743\n",
      "Iteration 43, loss = 19413606737.86641312\n",
      "Iteration 44, loss = 19413598943.70368576\n",
      "Iteration 45, loss = 19413591281.79522705\n",
      "Iteration 46, loss = 19413583751.86422729\n",
      "Iteration 47, loss = 19413576361.57518005\n",
      "Iteration 48, loss = 19413569096.31900406\n",
      "Iteration 49, loss = 19413561764.55661774\n",
      "Iteration 50, loss = 19413554503.87607193\n",
      "Iteration 51, loss = 19413547303.17497253\n",
      "Iteration 52, loss = 19413540217.81496048\n",
      "Iteration 53, loss = 19413533198.75037003\n",
      "Iteration 54, loss = 19413526214.39643478\n",
      "Iteration 55, loss = 19413519041.40686035\n",
      "Iteration 56, loss = 19413511876.81847763\n",
      "Iteration 57, loss = 19413504663.01464462\n",
      "Iteration 58, loss = 19413497518.37055969\n",
      "Iteration 59, loss = 19413490537.28682327\n",
      "Iteration 60, loss = 19413483559.35960388\n",
      "Iteration 61, loss = 19413476703.20575333\n",
      "Iteration 62, loss = 19413469831.64190292\n",
      "Iteration 63, loss = 19413463022.29956818\n",
      "Iteration 64, loss = 19413456158.10759354\n",
      "Iteration 65, loss = 19413449284.25841141\n",
      "Iteration 66, loss = 19413442562.67652893\n",
      "Iteration 67, loss = 19413435814.08019638\n",
      "Iteration 68, loss = 19413429087.98706436\n",
      "Iteration 69, loss = 19413422345.67157364\n",
      "Iteration 70, loss = 19413415652.44202805\n",
      "Iteration 71, loss = 19413408910.77729416\n",
      "Iteration 72, loss = 19413402240.60255814\n",
      "Iteration 73, loss = 19413395748.08549500\n",
      "Iteration 74, loss = 19413389329.47736740\n",
      "Iteration 75, loss = 19413383004.05670547\n",
      "Iteration 76, loss = 19413376663.44406891\n",
      "Iteration 77, loss = 19413370431.73043823\n",
      "Iteration 78, loss = 19413364192.35834885\n",
      "Iteration 79, loss = 19413358006.42821884\n",
      "Iteration 80, loss = 19413351769.91711044\n",
      "Iteration 81, loss = 19413345472.15299606\n",
      "Iteration 82, loss = 19413339239.77394485\n",
      "Iteration 83, loss = 19413332983.43081284\n",
      "Iteration 84, loss = 19413326769.28790665\n",
      "Iteration 85, loss = 19413320494.24813461\n",
      "Iteration 86, loss = 19413314373.11824799\n",
      "Iteration 87, loss = 19413308194.61977386\n",
      "Iteration 88, loss = 19413301981.38315201\n",
      "Iteration 89, loss = 19413295827.32387543\n",
      "Iteration 90, loss = 19413289756.26585388\n",
      "Iteration 91, loss = 19413283692.15953827\n",
      "Iteration 92, loss = 19413277631.44396973\n",
      "Iteration 93, loss = 19413271605.93934631\n",
      "Iteration 94, loss = 19413265666.04121399\n",
      "Iteration 95, loss = 19413259741.94703293\n",
      "Iteration 96, loss = 19413253782.57338333\n",
      "Iteration 97, loss = 19413247916.21328354\n",
      "Iteration 98, loss = 19413241946.88021469\n",
      "Iteration 99, loss = 19413235961.43548203\n",
      "Iteration 100, loss = 19413230084.17239380\n",
      "Iteration 101, loss = 19413224156.60457611\n",
      "Iteration 102, loss = 19413218217.21551895\n",
      "Iteration 103, loss = 19413212299.84844589\n",
      "Iteration 104, loss = 19413206373.74610138\n",
      "Iteration 105, loss = 19413200486.42459488\n",
      "Iteration 106, loss = 19413194607.48941040\n",
      "Iteration 107, loss = 19413188788.06853485\n",
      "Iteration 108, loss = 19413182940.45725632\n",
      "Iteration 109, loss = 19413177017.14796066\n",
      "Iteration 110, loss = 19413171137.02930450\n",
      "Iteration 111, loss = 19413165333.47533035\n",
      "Iteration 112, loss = 19413159527.54995728\n",
      "Iteration 113, loss = 19413153718.43204880\n",
      "Iteration 114, loss = 19413147988.19457245\n",
      "Iteration 115, loss = 19413142230.38452911\n",
      "Iteration 116, loss = 19413136483.65718079\n",
      "Iteration 117, loss = 19413130655.42095566\n",
      "Iteration 118, loss = 19413124915.82711792\n",
      "Iteration 119, loss = 19413119216.29088974\n",
      "Iteration 120, loss = 19413113490.58387375\n",
      "Iteration 121, loss = 19413107745.29128265\n",
      "Iteration 122, loss = 19413102047.88438416\n",
      "Iteration 123, loss = 19413096206.67733002\n",
      "Iteration 124, loss = 19413090451.03245926\n",
      "Iteration 125, loss = 19413084697.17363739\n",
      "Iteration 126, loss = 19413078943.52976608\n",
      "Iteration 127, loss = 19413073283.24767303\n",
      "Iteration 128, loss = 19413067525.20201874\n",
      "Iteration 129, loss = 19413061839.98441696\n",
      "Iteration 130, loss = 19413056122.48817444\n",
      "Iteration 131, loss = 19413050378.63266373\n",
      "Iteration 132, loss = 19413044641.01621246\n",
      "Iteration 133, loss = 19413038875.78187561\n",
      "Iteration 134, loss = 19413033021.13381958\n",
      "Iteration 135, loss = 19413027170.89957809\n",
      "Iteration 136, loss = 19413021151.18011475\n",
      "Iteration 137, loss = 19413015077.84112930\n",
      "Iteration 138, loss = 19413008840.21510315\n",
      "Iteration 139, loss = 19413002543.30617905\n",
      "Iteration 140, loss = 19412996109.32339478\n",
      "Iteration 141, loss = 19412989620.80894089\n",
      "Iteration 142, loss = 19412983142.03847122\n",
      "Iteration 143, loss = 19412976663.89241791\n",
      "Iteration 144, loss = 19412970245.46837997\n",
      "Iteration 145, loss = 19412963888.22366714\n",
      "Iteration 146, loss = 19412957610.19887161\n",
      "Iteration 147, loss = 19412951288.02457047\n",
      "Iteration 148, loss = 19412945039.69150543\n",
      "Iteration 149, loss = 19412938851.22029877\n",
      "Iteration 150, loss = 19412932771.74441147\n",
      "Iteration 151, loss = 19412926625.09211349\n",
      "Iteration 152, loss = 19412920549.42118073\n",
      "Iteration 153, loss = 19412914427.12466812\n",
      "Iteration 154, loss = 19412908366.79226303\n",
      "Iteration 155, loss = 19412902391.12290955\n",
      "Iteration 156, loss = 19412896415.70409012\n",
      "Iteration 157, loss = 19412890376.52629852\n",
      "Iteration 158, loss = 19412884333.18394089\n",
      "Iteration 159, loss = 19412878346.27052307\n",
      "Iteration 160, loss = 19412872413.22518921\n",
      "Iteration 161, loss = 19412866417.11267471\n",
      "Iteration 162, loss = 19412860364.80926132\n",
      "Iteration 163, loss = 19412854431.08873367\n",
      "Iteration 164, loss = 19412848432.60297775\n",
      "Iteration 165, loss = 19412842539.31373215\n",
      "Iteration 166, loss = 19412836661.91691208\n",
      "Iteration 167, loss = 19412830742.73228455\n",
      "Iteration 168, loss = 19412824921.41336060\n",
      "Iteration 169, loss = 19412819063.78327560\n",
      "Iteration 170, loss = 19412813291.04278183\n",
      "Iteration 171, loss = 19412807468.26455688\n",
      "Iteration 172, loss = 19412801681.43397522\n",
      "Iteration 173, loss = 19412795837.98335266\n",
      "Iteration 174, loss = 19412789963.09580994\n",
      "Iteration 175, loss = 19412784068.51474380\n",
      "Iteration 176, loss = 19412778256.91834641\n",
      "Iteration 177, loss = 19412772404.17368317\n",
      "Iteration 178, loss = 19412766589.74204254\n",
      "Iteration 179, loss = 19412760694.32100677\n",
      "Iteration 180, loss = 19412754814.53797150\n",
      "Iteration 181, loss = 19412748982.01022720\n",
      "Iteration 182, loss = 19412743179.87385559\n",
      "Iteration 183, loss = 19412737409.98724747\n",
      "Iteration 184, loss = 19412731672.72433090\n",
      "Iteration 185, loss = 19412725897.65467453\n",
      "Iteration 186, loss = 19412720080.49327469\n",
      "Iteration 187, loss = 19412714305.92238235\n",
      "Iteration 188, loss = 19412708527.31583786\n",
      "Iteration 189, loss = 19412702697.93511200\n",
      "Iteration 190, loss = 19412696861.63405228\n",
      "Iteration 191, loss = 19412691067.56879807\n",
      "Iteration 192, loss = 19412685292.97300720\n",
      "Iteration 193, loss = 19412679470.79980087\n",
      "Iteration 194, loss = 19412673692.82678604\n",
      "Iteration 195, loss = 19412667920.06591797\n",
      "Iteration 196, loss = 19412662221.48576736\n",
      "Iteration 197, loss = 19412656415.18091202\n",
      "Iteration 198, loss = 19412650626.83174896\n",
      "Iteration 199, loss = 19412644778.63110352\n",
      "Iteration 200, loss = 19412638971.77203369\n",
      "Iteration 201, loss = 19412633142.50043869\n",
      "Iteration 202, loss = 19412627209.98025131\n",
      "Iteration 203, loss = 19412621184.56518936\n",
      "Iteration 204, loss = 19412615063.16777420\n",
      "Iteration 205, loss = 19412608819.24895096\n",
      "Iteration 206, loss = 19412602387.93312073\n",
      "Iteration 207, loss = 19412595769.40617371\n",
      "Iteration 208, loss = 19412589153.62125778\n",
      "Iteration 209, loss = 19412582502.61533356\n",
      "Iteration 210, loss = 19412575758.08984375\n",
      "Iteration 211, loss = 19412569075.70782089\n",
      "Iteration 212, loss = 19412562464.66944504\n",
      "Iteration 213, loss = 19412555893.34302902\n",
      "Iteration 214, loss = 19412549371.61287308\n",
      "Iteration 215, loss = 19412542816.47457886\n",
      "Iteration 216, loss = 19412536376.61935425\n",
      "Iteration 217, loss = 19412529940.46553040\n",
      "Iteration 218, loss = 19412523585.11787033\n",
      "Iteration 219, loss = 19412517280.97656631\n",
      "Iteration 220, loss = 19412510963.34252167\n",
      "Iteration 221, loss = 19412504723.46240997\n",
      "Iteration 222, loss = 19412498514.97404099\n",
      "Iteration 223, loss = 19412492352.71080780\n",
      "Iteration 224, loss = 19412486235.53660965\n",
      "Iteration 225, loss = 19412480140.55597305\n",
      "Iteration 226, loss = 19412474025.37184906\n",
      "Iteration 227, loss = 19412467927.34483337\n",
      "Iteration 228, loss = 19412461859.60552979\n",
      "Iteration 229, loss = 19412455770.58576965\n",
      "Iteration 230, loss = 19412449785.19010925\n",
      "Iteration 231, loss = 19412443722.37063599\n",
      "Iteration 232, loss = 19412437706.29197693\n",
      "Iteration 233, loss = 19412431635.77001572\n",
      "Iteration 234, loss = 19412425548.85247421\n",
      "Iteration 235, loss = 19412419405.83551788\n",
      "Iteration 236, loss = 19412413275.55541992\n",
      "Iteration 237, loss = 19412407221.20369339\n",
      "Iteration 238, loss = 19412401199.06701279\n",
      "Iteration 239, loss = 19412395161.55903244\n",
      "Iteration 240, loss = 19412389163.13138962\n",
      "Iteration 241, loss = 19412383134.94683456\n",
      "Iteration 242, loss = 19412377134.32960129\n",
      "Iteration 243, loss = 19412371036.83731079\n",
      "Iteration 244, loss = 19412364955.40279770\n",
      "Iteration 245, loss = 19412358968.74610901\n",
      "Iteration 246, loss = 19412352933.31873703\n",
      "Iteration 247, loss = 19412346943.07302475\n",
      "Iteration 248, loss = 19412340832.94997406\n",
      "Iteration 249, loss = 19412334763.14969635\n",
      "Iteration 250, loss = 19412328634.83810425\n",
      "Iteration 251, loss = 19412322491.68077469\n",
      "Iteration 252, loss = 19412316379.87139130\n",
      "Iteration 253, loss = 19412310134.05246735\n",
      "Iteration 254, loss = 19412303706.07990265\n",
      "Iteration 255, loss = 19412297175.28719711\n",
      "Iteration 256, loss = 19412290298.64016724\n",
      "Iteration 257, loss = 19412283263.98401260\n",
      "Iteration 258, loss = 19412276107.16117477\n",
      "Iteration 259, loss = 19412269005.34217453\n",
      "Iteration 260, loss = 19412261938.28993607\n",
      "Iteration 261, loss = 19412254931.88631058\n",
      "Iteration 262, loss = 19412248003.61450195\n",
      "Iteration 263, loss = 19412241092.65875244\n",
      "Iteration 264, loss = 19412234293.42015457\n",
      "Iteration 265, loss = 19412227594.40050125\n",
      "Iteration 266, loss = 19412220944.22856903\n",
      "Iteration 267, loss = 19412214312.60977554\n",
      "Iteration 268, loss = 19412207717.89526367\n",
      "Iteration 269, loss = 19412201202.13593292\n",
      "Iteration 270, loss = 19412194631.66546249\n",
      "Iteration 271, loss = 19412188142.04296875\n",
      "Iteration 272, loss = 19412181658.41795349\n",
      "Iteration 273, loss = 19412175171.35382843\n",
      "Iteration 274, loss = 19412168683.58967209\n",
      "Iteration 275, loss = 19412162249.83329010\n",
      "Iteration 276, loss = 19412155790.42614365\n",
      "Iteration 277, loss = 19412149371.98458099\n",
      "Iteration 278, loss = 19412142910.07072067\n",
      "Iteration 279, loss = 19412136482.67988968\n",
      "Iteration 280, loss = 19412130003.25835037\n",
      "Iteration 281, loss = 19412123533.19245911\n",
      "Iteration 282, loss = 19412117165.80284119\n",
      "Iteration 283, loss = 19412110752.48269272\n",
      "Iteration 284, loss = 19412104400.00936508\n",
      "Iteration 285, loss = 19412098072.42409134\n",
      "Iteration 286, loss = 19412091733.77039719\n",
      "Iteration 287, loss = 19412085448.51396179\n",
      "Iteration 288, loss = 19412079217.46260452\n",
      "Iteration 289, loss = 19412072953.30541611\n",
      "Iteration 290, loss = 19412066730.68156433\n",
      "Iteration 291, loss = 19412060482.77600479\n",
      "Iteration 292, loss = 19412054237.39044189\n",
      "Iteration 293, loss = 19412048006.73191071\n",
      "Iteration 294, loss = 19412041838.74928665\n",
      "Iteration 295, loss = 19412035603.46001053\n",
      "Iteration 296, loss = 19412029365.44826126\n",
      "Iteration 297, loss = 19412023175.44820404\n",
      "Iteration 298, loss = 19412017023.61711502\n",
      "Iteration 299, loss = 19412010924.01316071\n",
      "Iteration 300, loss = 19412004805.99825668\n",
      "Iteration 301, loss = 19411998670.87712097\n",
      "Iteration 302, loss = 19411992572.46542740\n",
      "Iteration 303, loss = 19411986529.60365295\n",
      "Iteration 304, loss = 19411980446.11008835\n",
      "Iteration 305, loss = 19411974307.47401428\n",
      "Iteration 306, loss = 19411968169.01939392\n",
      "Iteration 307, loss = 19411962089.43992996\n",
      "Iteration 308, loss = 19411955956.89244080\n",
      "Iteration 309, loss = 19411949875.62535477\n",
      "Iteration 310, loss = 19411943771.06000519\n",
      "Iteration 311, loss = 19411937699.46244049\n",
      "Iteration 312, loss = 19411931657.64272308\n",
      "Iteration 313, loss = 19411925624.21287537\n",
      "Iteration 314, loss = 19411919532.81049347\n",
      "Iteration 315, loss = 19411913439.75916672\n",
      "Iteration 316, loss = 19411907224.26102448\n",
      "Iteration 317, loss = 19411901073.60004425\n",
      "Iteration 318, loss = 19411894856.87220001\n",
      "Iteration 319, loss = 19411888693.23754120\n",
      "Iteration 320, loss = 19411882598.04825974\n",
      "Iteration 321, loss = 19411876514.50579453\n",
      "Iteration 322, loss = 19411870416.00880814\n",
      "Iteration 323, loss = 19411864383.31810760\n",
      "Iteration 324, loss = 19411858356.10555649\n",
      "Iteration 325, loss = 19411852312.18582153\n",
      "Iteration 326, loss = 19411846363.44425964\n",
      "Iteration 327, loss = 19411840354.86484909\n",
      "Iteration 328, loss = 19411834248.61810684\n",
      "Iteration 329, loss = 19411828231.84083939\n",
      "Iteration 330, loss = 19411822192.75867462\n",
      "Iteration 331, loss = 19411816214.11433411\n",
      "Iteration 332, loss = 19411810258.81271362\n",
      "Iteration 333, loss = 19411804254.79946136\n",
      "Iteration 334, loss = 19411798226.64730072\n",
      "Iteration 335, loss = 19411792254.32681274\n",
      "Iteration 336, loss = 19411786226.41255569\n",
      "Iteration 337, loss = 19411780217.61378098\n",
      "Iteration 338, loss = 19411774246.83665848\n",
      "Iteration 339, loss = 19411768187.33575439\n",
      "Iteration 340, loss = 19411762130.28101730\n",
      "Iteration 341, loss = 19411756103.75862885\n",
      "Iteration 342, loss = 19411750050.79164505\n",
      "Iteration 343, loss = 19411744037.44761276\n",
      "Iteration 344, loss = 19411737972.21816254\n",
      "Iteration 345, loss = 19411731907.10874939\n",
      "Iteration 346, loss = 19411725895.52221298\n",
      "Iteration 347, loss = 19411719920.00936127\n",
      "Iteration 348, loss = 19411713974.30730057\n",
      "Iteration 349, loss = 19411708006.12249374\n",
      "Iteration 350, loss = 19411702073.81341553\n",
      "Iteration 351, loss = 19411696165.68990707\n",
      "Iteration 352, loss = 19411690230.67549133\n",
      "Iteration 353, loss = 19411684346.38840103\n",
      "Iteration 354, loss = 19411678454.96746445\n",
      "Iteration 355, loss = 19411672476.08148575\n",
      "Iteration 356, loss = 19411666547.87666702\n",
      "Iteration 357, loss = 19411660542.17127609\n",
      "Iteration 358, loss = 19411654645.53986359\n",
      "Iteration 359, loss = 19411648655.85639954\n",
      "Iteration 360, loss = 19411642665.70169830\n",
      "Iteration 361, loss = 19411636716.25204849\n",
      "Iteration 362, loss = 19411630775.32293701\n",
      "Iteration 363, loss = 19411624781.07183838\n",
      "Iteration 364, loss = 19411618873.58823776\n",
      "Iteration 365, loss = 19411612877.74975204\n",
      "Iteration 366, loss = 19411606900.81847382\n",
      "Iteration 367, loss = 19411600929.98826981\n",
      "Iteration 368, loss = 19411595005.27566910\n",
      "Iteration 369, loss = 19411588970.84104538\n",
      "Iteration 370, loss = 19411582967.51956177\n",
      "Iteration 371, loss = 19411576948.98403931\n",
      "Iteration 372, loss = 19411571057.92030334\n",
      "Iteration 373, loss = 19411565141.69720459\n",
      "Iteration 374, loss = 19411559209.43732071\n",
      "Iteration 375, loss = 19411553253.67461395\n",
      "Iteration 376, loss = 19411547335.89322662\n",
      "Iteration 377, loss = 19411541399.28513336\n",
      "Iteration 378, loss = 19411535389.75815582\n",
      "Iteration 379, loss = 19411529415.87196350\n",
      "Iteration 380, loss = 19411523437.37510300\n",
      "Iteration 381, loss = 19411517486.52518845\n",
      "Iteration 382, loss = 19411511526.28619003\n",
      "Iteration 383, loss = 19411505549.61514282\n",
      "Iteration 384, loss = 19411499586.39064789\n",
      "Iteration 385, loss = 19411493688.96058655\n",
      "Iteration 386, loss = 19411487791.00856018\n",
      "Iteration 387, loss = 19411481886.08983231\n",
      "Iteration 388, loss = 19411475889.70729828\n",
      "Iteration 389, loss = 19411469942.85437775\n",
      "Iteration 390, loss = 19411464061.73554611\n",
      "Iteration 391, loss = 19411458144.44635010\n",
      "Iteration 392, loss = 19411452245.46728516\n",
      "Iteration 393, loss = 19411446333.54029465\n",
      "Iteration 394, loss = 19411440425.45464325\n",
      "Iteration 395, loss = 19411434568.77679062\n",
      "Iteration 396, loss = 19411428670.20962906\n",
      "Iteration 397, loss = 19411422663.05310440\n",
      "Iteration 398, loss = 19411416682.21084976\n",
      "Iteration 399, loss = 19411410763.91592407\n",
      "Iteration 400, loss = 19411404856.91029739\n",
      "Iteration 401, loss = 19411399038.87425232\n",
      "Iteration 402, loss = 19411393103.58875275\n",
      "Iteration 403, loss = 19411387185.24544525\n",
      "Iteration 404, loss = 19411381245.74818420\n",
      "Iteration 405, loss = 19411375371.84622192\n",
      "Iteration 406, loss = 19411369456.04217911\n",
      "Iteration 407, loss = 19411363618.26025772\n",
      "Iteration 408, loss = 19411357756.75085449\n",
      "Iteration 409, loss = 19411351773.64819336\n",
      "Iteration 410, loss = 19411345876.22025299\n",
      "Iteration 411, loss = 19411339987.30774307\n",
      "Iteration 412, loss = 19411334015.79585266\n",
      "Iteration 413, loss = 19411328036.69856262\n",
      "Iteration 414, loss = 19411322042.99690628\n",
      "Iteration 415, loss = 19411316009.30521393\n",
      "Iteration 416, loss = 19411310121.48324966\n",
      "Iteration 417, loss = 19411304159.46037674\n",
      "Iteration 418, loss = 19411298269.35789108\n",
      "Iteration 419, loss = 19411292379.57768631\n",
      "Iteration 420, loss = 19411286438.71665192\n",
      "Iteration 421, loss = 19411280440.51185226\n",
      "Iteration 422, loss = 19411274513.39523697\n",
      "Iteration 423, loss = 19411268570.25056458\n",
      "Iteration 424, loss = 19411262657.63802338\n",
      "Iteration 425, loss = 19411256710.04722977\n",
      "Iteration 426, loss = 19411250734.18874741\n",
      "Iteration 427, loss = 19411244841.43262482\n",
      "Iteration 428, loss = 19411238935.14096069\n",
      "Iteration 429, loss = 19411233066.33335876\n",
      "Iteration 430, loss = 19411227176.25216293\n",
      "Iteration 431, loss = 19411221327.15136337\n",
      "Iteration 432, loss = 19411215434.30821991\n",
      "Iteration 433, loss = 19411209509.31357956\n",
      "Iteration 434, loss = 19411203626.18452454\n",
      "Iteration 435, loss = 19411197711.33481216\n",
      "Iteration 436, loss = 19411191826.80216217\n",
      "Iteration 437, loss = 19411185926.81306839\n",
      "Iteration 438, loss = 19411180045.10648727\n",
      "Iteration 439, loss = 19411174232.42117310\n",
      "Iteration 440, loss = 19411168424.59197998\n",
      "Iteration 441, loss = 19411162574.70090485\n",
      "Iteration 442, loss = 19411156694.21651840\n",
      "Iteration 443, loss = 19411150851.03456116\n",
      "Iteration 444, loss = 19411144954.72073746\n",
      "Iteration 445, loss = 19411138968.99646759\n",
      "Iteration 446, loss = 19411133081.61088181\n",
      "Iteration 447, loss = 19411127218.03743362\n",
      "Iteration 448, loss = 19411121349.62680817\n",
      "Iteration 449, loss = 19411115518.20474625\n",
      "Iteration 450, loss = 19411109612.22298431\n",
      "Iteration 451, loss = 19411103699.72812653\n",
      "Iteration 452, loss = 19411097799.71224594\n",
      "Iteration 453, loss = 19411091961.36789703\n",
      "Iteration 454, loss = 19411086158.82978821\n",
      "Iteration 455, loss = 19411080354.71870422\n",
      "Iteration 456, loss = 19411074418.28248596\n",
      "Iteration 457, loss = 19411068586.17286301\n",
      "Iteration 458, loss = 19411062707.29454422\n",
      "Iteration 459, loss = 19411056785.33728027\n",
      "Iteration 460, loss = 19411050907.95088959\n",
      "Iteration 461, loss = 19411044987.95307541\n",
      "Iteration 462, loss = 19411039107.11270142\n",
      "Iteration 463, loss = 19411033296.04589844\n",
      "Iteration 464, loss = 19411027333.73823166\n",
      "Iteration 465, loss = 19411021345.26539612\n",
      "Iteration 466, loss = 19411015378.01546097\n",
      "Iteration 467, loss = 19411009393.77787399\n",
      "Iteration 468, loss = 19411003442.11346054\n",
      "Iteration 469, loss = 19410997593.59482193\n",
      "Iteration 470, loss = 19410991745.96350861\n",
      "Iteration 471, loss = 19410985877.33091354\n",
      "Iteration 472, loss = 19410980034.51855850\n",
      "Iteration 473, loss = 19410974144.10681152\n",
      "Iteration 474, loss = 19410968251.62025070\n",
      "Iteration 475, loss = 19410962337.00456238\n",
      "Iteration 476, loss = 19410956340.10210800\n",
      "Iteration 477, loss = 19410950329.23672104\n",
      "Iteration 478, loss = 19410944376.04636002\n",
      "Iteration 479, loss = 19410938455.38592911\n",
      "Iteration 480, loss = 19410932538.26600647\n",
      "Iteration 481, loss = 19410926737.76356125\n",
      "Iteration 482, loss = 19410920947.09595490\n",
      "Iteration 483, loss = 19410915126.35234070\n",
      "Iteration 484, loss = 19410909355.55257034\n",
      "Iteration 485, loss = 19410903513.12363434\n",
      "Iteration 486, loss = 19410897633.74496460\n",
      "Iteration 487, loss = 19410891729.39587784\n",
      "Iteration 488, loss = 19410885810.48912811\n",
      "Iteration 489, loss = 19410879998.07251358\n",
      "Iteration 490, loss = 19410874071.01652527\n",
      "Iteration 491, loss = 19410868259.14119720\n",
      "Iteration 492, loss = 19410862389.70916367\n",
      "Iteration 493, loss = 19410856491.16997147\n",
      "Iteration 494, loss = 19410850621.51461029\n",
      "Iteration 495, loss = 19410844711.87089157\n",
      "Iteration 496, loss = 19410838888.88460159\n",
      "Iteration 497, loss = 19410832963.60899734\n",
      "Iteration 498, loss = 19410827086.02998734\n",
      "Iteration 499, loss = 19410821225.06410980\n",
      "Iteration 500, loss = 19410815289.16702652\n",
      "Iteration 1, loss = 19474968040.42515945\n",
      "Iteration 2, loss = 19474950354.84182739\n",
      "Iteration 3, loss = 19474931806.78238297\n",
      "Iteration 4, loss = 19474913089.24679947\n",
      "Iteration 5, loss = 19474894276.56557846\n",
      "Iteration 6, loss = 19474875615.02260590\n",
      "Iteration 7, loss = 19474856782.93138885\n",
      "Iteration 8, loss = 19474837646.04251099\n",
      "Iteration 9, loss = 19474818577.11864853\n",
      "Iteration 10, loss = 19474799899.02409744\n",
      "Iteration 11, loss = 19474780770.23137665\n",
      "Iteration 12, loss = 19474759478.08618927\n",
      "Iteration 13, loss = 19474736873.99429703\n",
      "Iteration 14, loss = 19474714952.12509918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 19474695218.10513687\n",
      "Iteration 16, loss = 19474675813.97188950\n",
      "Iteration 17, loss = 19474654287.51887894\n",
      "Iteration 18, loss = 19474631477.80083084\n",
      "Iteration 19, loss = 19474609265.57312775\n",
      "Iteration 20, loss = 19474587495.44168091\n",
      "Iteration 21, loss = 19474564974.11891174\n",
      "Iteration 22, loss = 19474543271.78822327\n",
      "Iteration 23, loss = 19474524617.80777359\n",
      "Iteration 24, loss = 19474507661.73944473\n",
      "Iteration 25, loss = 19474491381.82555771\n",
      "Iteration 26, loss = 19474475330.30582809\n",
      "Iteration 27, loss = 19474460196.34688568\n",
      "Iteration 28, loss = 19474445809.51781464\n",
      "Iteration 29, loss = 19474432029.61388397\n",
      "Iteration 30, loss = 19474418739.55691147\n",
      "Iteration 31, loss = 19474405931.62511444\n",
      "Iteration 32, loss = 19474393952.55177307\n",
      "Iteration 33, loss = 19474383426.44019699\n",
      "Iteration 34, loss = 19474373350.43171692\n",
      "Iteration 35, loss = 19474363241.61344528\n",
      "Iteration 36, loss = 19474353137.61011887\n",
      "Iteration 37, loss = 19474343047.56380844\n",
      "Iteration 38, loss = 19474333284.47405624\n",
      "Iteration 39, loss = 19474323362.47491455\n",
      "Iteration 40, loss = 19474313698.21788788\n",
      "Iteration 41, loss = 19474305148.61007690\n",
      "Iteration 42, loss = 19474297225.17382050\n",
      "Iteration 43, loss = 19474289546.89607620\n",
      "Iteration 44, loss = 19474282127.19342041\n",
      "Iteration 45, loss = 19474275069.68679047\n",
      "Iteration 46, loss = 19474268043.12683487\n",
      "Iteration 47, loss = 19474261112.68273163\n",
      "Iteration 48, loss = 19474254080.56220245\n",
      "Iteration 49, loss = 19474247292.83126450\n",
      "Iteration 50, loss = 19474240652.73081589\n",
      "Iteration 51, loss = 19474234030.42324829\n",
      "Iteration 52, loss = 19474227456.86153030\n",
      "Iteration 53, loss = 19474221129.78332138\n",
      "Iteration 54, loss = 19474214792.72698212\n",
      "Iteration 55, loss = 19474208528.98681259\n",
      "Iteration 56, loss = 19474202300.96063614\n",
      "Iteration 57, loss = 19474196218.54415894\n",
      "Iteration 58, loss = 19474190036.10995102\n",
      "Iteration 59, loss = 19474184041.43954468\n",
      "Iteration 60, loss = 19474178044.41604996\n",
      "Iteration 61, loss = 19474172017.20550919\n",
      "Iteration 62, loss = 19474166088.66724777\n",
      "Iteration 63, loss = 19474160140.33221436\n",
      "Iteration 64, loss = 19474154293.40327454\n",
      "Iteration 65, loss = 19474148457.09239197\n",
      "Iteration 66, loss = 19474142605.96805191\n",
      "Iteration 67, loss = 19474136812.96076965\n",
      "Iteration 68, loss = 19474131069.01482010\n",
      "Iteration 69, loss = 19474125262.89693451\n",
      "Iteration 70, loss = 19474119432.96982574\n",
      "Iteration 71, loss = 19474113612.41862488\n",
      "Iteration 72, loss = 19474107930.69417572\n",
      "Iteration 73, loss = 19474102257.59857178\n",
      "Iteration 74, loss = 19474096587.28918076\n",
      "Iteration 75, loss = 19474090934.18291473\n",
      "Iteration 76, loss = 19474085328.78142166\n",
      "Iteration 77, loss = 19474079707.49677658\n",
      "Iteration 78, loss = 19474074131.71909332\n",
      "Iteration 79, loss = 19474068542.86135483\n",
      "Iteration 80, loss = 19474062945.97981644\n",
      "Iteration 81, loss = 19474057457.76226807\n",
      "Iteration 82, loss = 19474051893.29782104\n",
      "Iteration 83, loss = 19474046410.40200806\n",
      "Iteration 84, loss = 19474040914.05656052\n",
      "Iteration 85, loss = 19474035421.38422012\n",
      "Iteration 86, loss = 19474029924.44333267\n",
      "Iteration 87, loss = 19474024496.87482452\n",
      "Iteration 88, loss = 19474019029.20562363\n",
      "Iteration 89, loss = 19474013640.15338898\n",
      "Iteration 90, loss = 19474008154.02531052\n",
      "Iteration 91, loss = 19474002776.67946243\n",
      "Iteration 92, loss = 19473997352.82004166\n",
      "Iteration 93, loss = 19473991989.08518219\n",
      "Iteration 94, loss = 19473986566.17064667\n",
      "Iteration 95, loss = 19473981219.10555649\n",
      "Iteration 96, loss = 19473975821.50994873\n",
      "Iteration 97, loss = 19473970488.17278290\n",
      "Iteration 98, loss = 19473965122.76514435\n",
      "Iteration 99, loss = 19473959763.39368057\n",
      "Iteration 100, loss = 19473954431.99505997\n",
      "Iteration 101, loss = 19473949127.82438278\n",
      "Iteration 102, loss = 19473943807.46614838\n",
      "Iteration 103, loss = 19473938470.86933136\n",
      "Iteration 104, loss = 19473933151.06181717\n",
      "Iteration 105, loss = 19473927825.07449722\n",
      "Iteration 106, loss = 19473922492.35021210\n",
      "Iteration 107, loss = 19473917113.35663223\n",
      "Iteration 108, loss = 19473911808.01796722\n",
      "Iteration 109, loss = 19473906516.24499130\n",
      "Iteration 110, loss = 19473901215.40460587\n",
      "Iteration 111, loss = 19473895851.05807877\n",
      "Iteration 112, loss = 19473890559.67206573\n",
      "Iteration 113, loss = 19473885247.70546341\n",
      "Iteration 114, loss = 19473879944.56039810\n",
      "Iteration 115, loss = 19473874640.92583466\n",
      "Iteration 116, loss = 19473869328.74634552\n",
      "Iteration 117, loss = 19473864095.34616089\n",
      "Iteration 118, loss = 19473858849.47034073\n",
      "Iteration 119, loss = 19473853605.12311554\n",
      "Iteration 120, loss = 19473848366.67503738\n",
      "Iteration 121, loss = 19473843139.82176208\n",
      "Iteration 122, loss = 19473837899.46778488\n",
      "Iteration 123, loss = 19473832712.45892715\n",
      "Iteration 124, loss = 19473827445.38616943\n",
      "Iteration 125, loss = 19473822249.11020279\n",
      "Iteration 126, loss = 19473817055.45312881\n",
      "Iteration 127, loss = 19473811841.87488174\n",
      "Iteration 128, loss = 19473806639.63153458\n",
      "Iteration 129, loss = 19473801462.91146851\n",
      "Iteration 130, loss = 19473796287.30196381\n",
      "Iteration 131, loss = 19473791128.93772888\n",
      "Iteration 132, loss = 19473785902.34045410\n",
      "Iteration 133, loss = 19473780778.54935837\n",
      "Iteration 134, loss = 19473775618.29327011\n",
      "Iteration 135, loss = 19473770479.88764191\n",
      "Iteration 136, loss = 19473765290.66054916\n",
      "Iteration 137, loss = 19473760144.69231796\n",
      "Iteration 138, loss = 19473755020.76509476\n",
      "Iteration 139, loss = 19473749872.57054520\n",
      "Iteration 140, loss = 19473744750.05392456\n",
      "Iteration 141, loss = 19473739623.42788315\n",
      "Iteration 142, loss = 19473734481.30549240\n",
      "Iteration 143, loss = 19473729351.74819565\n",
      "Iteration 144, loss = 19473724198.62910843\n",
      "Iteration 145, loss = 19473719058.86956406\n",
      "Iteration 146, loss = 19473713935.15449905\n",
      "Iteration 147, loss = 19473708842.56557846\n",
      "Iteration 148, loss = 19473703727.29506302\n",
      "Iteration 149, loss = 19473698617.87828827\n",
      "Iteration 150, loss = 19473693521.34833908\n",
      "Iteration 151, loss = 19473688386.19999313\n",
      "Iteration 152, loss = 19473683255.47399521\n",
      "Iteration 153, loss = 19473678165.67482376\n",
      "Iteration 154, loss = 19473673074.74882889\n",
      "Iteration 155, loss = 19473667974.11433029\n",
      "Iteration 156, loss = 19473662880.77084351\n",
      "Iteration 157, loss = 19473657765.53894806\n",
      "Iteration 158, loss = 19473652704.46512985\n",
      "Iteration 159, loss = 19473647609.89533615\n",
      "Iteration 160, loss = 19473642529.31909561\n",
      "Iteration 161, loss = 19473637470.36656952\n",
      "Iteration 162, loss = 19473632339.86354828\n",
      "Iteration 163, loss = 19473627305.58081055\n",
      "Iteration 164, loss = 19473622244.99718857\n",
      "Iteration 165, loss = 19473617146.98304367\n",
      "Iteration 166, loss = 19473612104.78063583\n",
      "Iteration 167, loss = 19473607040.18529892\n",
      "Iteration 168, loss = 19473601961.85233307\n",
      "Iteration 169, loss = 19473596904.68806076\n",
      "Iteration 170, loss = 19473591840.06096649\n",
      "Iteration 171, loss = 19473586786.88870621\n",
      "Iteration 172, loss = 19473581696.02054214\n",
      "Iteration 173, loss = 19473576677.57080460\n",
      "Iteration 174, loss = 19473571629.37356949\n",
      "Iteration 175, loss = 19473566547.68867111\n",
      "Iteration 176, loss = 19473561523.78845215\n",
      "Iteration 177, loss = 19473556461.57637024\n",
      "Iteration 178, loss = 19473551436.65432739\n",
      "Iteration 179, loss = 19473546390.55609512\n",
      "Iteration 180, loss = 19473541308.68423843\n",
      "Iteration 181, loss = 19473536264.64023209\n",
      "Iteration 182, loss = 19473531269.64795685\n",
      "Iteration 183, loss = 19473526215.72785950\n",
      "Iteration 184, loss = 19473521187.26262283\n",
      "Iteration 185, loss = 19473516164.74161148\n",
      "Iteration 186, loss = 19473511133.89575958\n",
      "Iteration 187, loss = 19473506095.76366043\n",
      "Iteration 188, loss = 19473501048.04500198\n",
      "Iteration 189, loss = 19473496057.93211746\n",
      "Iteration 190, loss = 19473491034.24504089\n",
      "Iteration 191, loss = 19473486029.56552887\n",
      "Iteration 192, loss = 19473480949.08276749\n",
      "Iteration 193, loss = 19473475978.67941666\n",
      "Iteration 194, loss = 19473470916.87193680\n",
      "Iteration 195, loss = 19473465867.93907166\n",
      "Iteration 196, loss = 19473460878.67406082\n",
      "Iteration 197, loss = 19473455873.26298904\n",
      "Iteration 198, loss = 19473450821.55607986\n",
      "Iteration 199, loss = 19473445798.03513718\n",
      "Iteration 200, loss = 19473440789.78173447\n",
      "Iteration 201, loss = 19473435786.02648926\n",
      "Iteration 202, loss = 19473430754.20985413\n",
      "Iteration 203, loss = 19473425744.20691299\n",
      "Iteration 204, loss = 19473420721.31469727\n",
      "Iteration 205, loss = 19473415735.06031799\n",
      "Iteration 206, loss = 19473410706.76564026\n",
      "Iteration 207, loss = 19473405722.80126953\n",
      "Iteration 208, loss = 19473400742.35511780\n",
      "Iteration 209, loss = 19473395718.09068298\n",
      "Iteration 210, loss = 19473390705.79092026\n",
      "Iteration 211, loss = 19473385728.21522141\n",
      "Iteration 212, loss = 19473380716.39075470\n",
      "Iteration 213, loss = 19473375729.16601562\n",
      "Iteration 214, loss = 19473370695.71205521\n",
      "Iteration 215, loss = 19473365719.84507751\n",
      "Iteration 216, loss = 19473360705.04270935\n",
      "Iteration 217, loss = 19473355715.27455902\n",
      "Iteration 218, loss = 19473350686.65455246\n",
      "Iteration 219, loss = 19473345688.54589081\n",
      "Iteration 220, loss = 19473340720.92469025\n",
      "Iteration 221, loss = 19473335689.92148209\n",
      "Iteration 222, loss = 19473330704.25496674\n",
      "Iteration 223, loss = 19473325731.47390366\n",
      "Iteration 224, loss = 19473320742.30446243\n",
      "Iteration 225, loss = 19473315739.73716354\n",
      "Iteration 226, loss = 19473310741.50114441\n",
      "Iteration 227, loss = 19473305773.02716827\n",
      "Iteration 228, loss = 19473300779.34247971\n",
      "Iteration 229, loss = 19473295778.48932266\n",
      "Iteration 230, loss = 19473290779.82027817\n",
      "Iteration 231, loss = 19473285835.92696762\n",
      "Iteration 232, loss = 19473280798.08508301\n",
      "Iteration 233, loss = 19473275811.16146088\n",
      "Iteration 234, loss = 19473270866.52343750\n",
      "Iteration 235, loss = 19473265890.83792114\n",
      "Iteration 236, loss = 19473260932.90272522\n",
      "Iteration 237, loss = 19473255926.56351471\n",
      "Iteration 238, loss = 19473250982.44898605\n",
      "Iteration 239, loss = 19473245990.19298172\n",
      "Iteration 240, loss = 19473241000.94531250\n",
      "Iteration 241, loss = 19473236045.04838181\n",
      "Iteration 242, loss = 19473231098.16315079\n",
      "Iteration 243, loss = 19473226112.12171555\n",
      "Iteration 244, loss = 19473221152.68030930\n",
      "Iteration 245, loss = 19473216168.27357483\n",
      "Iteration 246, loss = 19473211201.20621872\n",
      "Iteration 247, loss = 19473206273.54921722\n",
      "Iteration 248, loss = 19473201298.17098236\n",
      "Iteration 249, loss = 19473196332.98240280\n",
      "Iteration 250, loss = 19473191354.81826401\n",
      "Iteration 251, loss = 19473186384.89562607\n",
      "Iteration 252, loss = 19473181432.84560394\n",
      "Iteration 253, loss = 19473176482.82112503\n",
      "Iteration 254, loss = 19473171508.82446671\n",
      "Iteration 255, loss = 19473166524.95365906\n",
      "Iteration 256, loss = 19473161595.31385422\n",
      "Iteration 257, loss = 19473156590.76370621\n",
      "Iteration 258, loss = 19473151667.12884903\n",
      "Iteration 259, loss = 19473146739.41283417\n",
      "Iteration 260, loss = 19473141730.10771942\n",
      "Iteration 261, loss = 19473136792.95277023\n",
      "Iteration 262, loss = 19473131821.52488327\n",
      "Iteration 263, loss = 19473126883.69702148\n",
      "Iteration 264, loss = 19473121920.49845123\n",
      "Iteration 265, loss = 19473116997.98801804\n",
      "Iteration 266, loss = 19473112011.97435379\n",
      "Iteration 267, loss = 19473107043.56790924\n",
      "Iteration 268, loss = 19473102122.71650696\n",
      "Iteration 269, loss = 19473097133.17396545\n",
      "Iteration 270, loss = 19473092191.55894470\n",
      "Iteration 271, loss = 19473087278.46537399\n",
      "Iteration 272, loss = 19473082310.51550293\n",
      "Iteration 273, loss = 19473077365.52948761\n",
      "Iteration 274, loss = 19473072378.52866364\n",
      "Iteration 275, loss = 19473067488.41317749\n",
      "Iteration 276, loss = 19473062544.20058823\n",
      "Iteration 277, loss = 19473057567.51479340\n",
      "Iteration 278, loss = 19473052641.77085876\n",
      "Iteration 279, loss = 19473047676.50659561\n",
      "Iteration 280, loss = 19473042748.48712158\n",
      "Iteration 281, loss = 19473037790.37359238\n",
      "Iteration 282, loss = 19473032873.16340637\n",
      "Iteration 283, loss = 19473027919.62352753\n",
      "Iteration 284, loss = 19473022962.88969803\n",
      "Iteration 285, loss = 19473018019.68805313\n",
      "Iteration 286, loss = 19473013085.47883606\n",
      "Iteration 287, loss = 19473008096.14126968\n",
      "Iteration 288, loss = 19473003207.84170151\n",
      "Iteration 289, loss = 19472998213.28186798\n",
      "Iteration 290, loss = 19472993278.18793869\n",
      "Iteration 291, loss = 19472988341.08879471\n",
      "Iteration 292, loss = 19472983364.86400223\n",
      "Iteration 293, loss = 19472978485.50275040\n",
      "Iteration 294, loss = 19472973547.16696167\n",
      "Iteration 295, loss = 19472968618.92555618\n",
      "Iteration 296, loss = 19472963664.58160400\n",
      "Iteration 297, loss = 19472958756.21914291\n",
      "Iteration 298, loss = 19472953783.19050217\n",
      "Iteration 299, loss = 19472948877.27994537\n",
      "Iteration 300, loss = 19472943950.84674835\n",
      "Iteration 301, loss = 19472939006.86825562\n",
      "Iteration 302, loss = 19472934084.78639984\n",
      "Iteration 303, loss = 19472929141.23184204\n",
      "Iteration 304, loss = 19472924230.79975128\n",
      "Iteration 305, loss = 19472919269.32408905\n",
      "Iteration 306, loss = 19472914322.08364868\n",
      "Iteration 307, loss = 19472909431.01636505\n",
      "Iteration 308, loss = 19472904455.37113190\n",
      "Iteration 309, loss = 19472899585.05468750\n",
      "Iteration 310, loss = 19472894622.94070435\n",
      "Iteration 311, loss = 19472889700.09968948\n",
      "Iteration 312, loss = 19472884757.64012146\n",
      "Iteration 313, loss = 19472879832.64339828\n",
      "Iteration 314, loss = 19472874911.14540482\n",
      "Iteration 315, loss = 19472869985.31195450\n",
      "Iteration 316, loss = 19472865044.39913177\n",
      "Iteration 317, loss = 19472860086.85321045\n",
      "Iteration 318, loss = 19472855187.47046661\n",
      "Iteration 319, loss = 19472850248.70089722\n",
      "Iteration 320, loss = 19472845298.20235443\n",
      "Iteration 321, loss = 19472840408.43088913\n",
      "Iteration 322, loss = 19472835470.80086517\n",
      "Iteration 323, loss = 19472830563.23324585\n",
      "Iteration 324, loss = 19472825613.96337128\n",
      "Iteration 325, loss = 19472820699.25635147\n",
      "Iteration 326, loss = 19472815759.31632996\n",
      "Iteration 327, loss = 19472810845.53184509\n",
      "Iteration 328, loss = 19472805934.19114685\n",
      "Iteration 329, loss = 19472801006.50183105\n",
      "Iteration 330, loss = 19472796087.21825790\n",
      "Iteration 331, loss = 19472791173.83411407\n",
      "Iteration 332, loss = 19472786213.25515747\n",
      "Iteration 333, loss = 19472781289.17451859\n",
      "Iteration 334, loss = 19472776396.61137390\n",
      "Iteration 335, loss = 19472771484.07423401\n",
      "Iteration 336, loss = 19472766522.14058304\n",
      "Iteration 337, loss = 19472761635.57222748\n",
      "Iteration 338, loss = 19472756698.42954636\n",
      "Iteration 339, loss = 19472751825.22415924\n",
      "Iteration 340, loss = 19472746860.29644012\n",
      "Iteration 341, loss = 19472741936.86091232\n",
      "Iteration 342, loss = 19472737039.52691650\n",
      "Iteration 343, loss = 19472732121.39493942\n",
      "Iteration 344, loss = 19472727197.18556595\n",
      "Iteration 345, loss = 19472722281.27703476\n",
      "Iteration 346, loss = 19472717365.92885971\n",
      "Iteration 347, loss = 19472712450.47855377\n",
      "Iteration 348, loss = 19472707555.50815582\n",
      "Iteration 349, loss = 19472702636.03881073\n",
      "Iteration 350, loss = 19472697703.07052231\n",
      "Iteration 351, loss = 19472692794.85538101\n",
      "Iteration 352, loss = 19472687936.24688721\n",
      "Iteration 353, loss = 19472682972.46804810\n",
      "Iteration 354, loss = 19472678054.89104843\n",
      "Iteration 355, loss = 19472673156.18608475\n",
      "Iteration 356, loss = 19472668248.21051407\n",
      "Iteration 357, loss = 19472663326.87784576\n",
      "Iteration 358, loss = 19472658422.21911621\n",
      "Iteration 359, loss = 19472653496.89410400\n",
      "Iteration 360, loss = 19472648609.43883514\n",
      "Iteration 361, loss = 19472643679.32107544\n",
      "Iteration 362, loss = 19472638743.05148697\n",
      "Iteration 363, loss = 19472633839.23614120\n",
      "Iteration 364, loss = 19472628904.50384521\n",
      "Iteration 365, loss = 19472624015.05396652\n",
      "Iteration 366, loss = 19472619091.05416107\n",
      "Iteration 367, loss = 19472614215.81192780\n",
      "Iteration 368, loss = 19472609270.28958130\n",
      "Iteration 369, loss = 19472604362.21221542\n",
      "Iteration 370, loss = 19472599485.40855789\n",
      "Iteration 371, loss = 19472594524.78751755\n",
      "Iteration 372, loss = 19472589680.68971634\n",
      "Iteration 373, loss = 19472584735.08824921\n",
      "Iteration 374, loss = 19472579837.67176437\n",
      "Iteration 375, loss = 19472574931.86273193\n",
      "Iteration 376, loss = 19472570021.19500351\n",
      "Iteration 377, loss = 19472565124.77322388\n",
      "Iteration 378, loss = 19472560230.54935455\n",
      "Iteration 379, loss = 19472555315.59362793\n",
      "Iteration 380, loss = 19472550396.35716629\n",
      "Iteration 381, loss = 19472545542.29417419\n",
      "Iteration 382, loss = 19472540594.10533142\n",
      "Iteration 383, loss = 19472535648.94696045\n",
      "Iteration 384, loss = 19472530799.54986954\n",
      "Iteration 385, loss = 19472525850.60876465\n",
      "Iteration 386, loss = 19472520959.80970383\n",
      "Iteration 387, loss = 19472516063.25166321\n",
      "Iteration 388, loss = 19472511141.51931000\n",
      "Iteration 389, loss = 19472506261.90603256\n",
      "Iteration 390, loss = 19472501337.51935959\n",
      "Iteration 391, loss = 19472496394.33786011\n",
      "Iteration 392, loss = 19472491552.16882706\n",
      "Iteration 393, loss = 19472486638.40350723\n",
      "Iteration 394, loss = 19472481713.88135529\n",
      "Iteration 395, loss = 19472476843.02091217\n",
      "Iteration 396, loss = 19472471906.39652252\n",
      "Iteration 397, loss = 19472467024.69154739\n",
      "Iteration 398, loss = 19472462104.22356796\n",
      "Iteration 399, loss = 19472457234.11058807\n",
      "Iteration 400, loss = 19472452342.61948395\n",
      "Iteration 401, loss = 19472447424.10256577\n",
      "Iteration 402, loss = 19472442507.85767365\n",
      "Iteration 403, loss = 19472437654.97025681\n",
      "Iteration 404, loss = 19472432716.80627823\n",
      "Iteration 405, loss = 19472427820.67220688\n",
      "Iteration 406, loss = 19472422944.84219742\n",
      "Iteration 407, loss = 19472418041.70674515\n",
      "Iteration 408, loss = 19472413142.77439117\n",
      "Iteration 409, loss = 19472408199.53636169\n",
      "Iteration 410, loss = 19472403334.07599258\n",
      "Iteration 411, loss = 19472398407.98845291\n",
      "Iteration 412, loss = 19472393536.40953445\n",
      "Iteration 413, loss = 19472388634.08646393\n",
      "Iteration 414, loss = 19472383715.34755325\n",
      "Iteration 415, loss = 19472378821.97090149\n",
      "Iteration 416, loss = 19472373918.98052216\n",
      "Iteration 417, loss = 19472368997.60833359\n",
      "Iteration 418, loss = 19472364150.08057404\n",
      "Iteration 419, loss = 19472359221.23065567\n",
      "Iteration 420, loss = 19472354356.83496857\n",
      "Iteration 421, loss = 19472349429.98685074\n",
      "Iteration 422, loss = 19472344530.02618790\n",
      "Iteration 423, loss = 19472339620.23158264\n",
      "Iteration 424, loss = 19472334724.58497238\n",
      "Iteration 425, loss = 19472329843.81465149\n",
      "Iteration 426, loss = 19472324893.80418015\n",
      "Iteration 427, loss = 19472320044.09439850\n",
      "Iteration 428, loss = 19472315117.61242676\n",
      "Iteration 429, loss = 19472310226.54247665\n",
      "Iteration 430, loss = 19472305332.65130234\n",
      "Iteration 431, loss = 19472300428.99020004\n",
      "Iteration 432, loss = 19472295565.47444153\n",
      "Iteration 433, loss = 19472290661.97207642\n",
      "Iteration 434, loss = 19472285752.39384842\n",
      "Iteration 435, loss = 19472280879.97873306\n",
      "Iteration 436, loss = 19472275989.16932297\n",
      "Iteration 437, loss = 19472271079.31290436\n",
      "Iteration 438, loss = 19472266195.56065750\n",
      "Iteration 439, loss = 19472261269.63700485\n",
      "Iteration 440, loss = 19472256393.63846588\n",
      "Iteration 441, loss = 19472251495.49106216\n",
      "Iteration 442, loss = 19472246594.90890121\n",
      "Iteration 443, loss = 19472241693.83658218\n",
      "Iteration 444, loss = 19472236781.46575165\n",
      "Iteration 445, loss = 19472231895.00154877\n",
      "Iteration 446, loss = 19472227041.23203278\n",
      "Iteration 447, loss = 19472222125.07942581\n",
      "Iteration 448, loss = 19472217206.58357620\n",
      "Iteration 449, loss = 19472212343.85215378\n",
      "Iteration 450, loss = 19472207452.28265381\n",
      "Iteration 451, loss = 19472202528.58256149\n",
      "Iteration 452, loss = 19472197675.57635880\n",
      "Iteration 453, loss = 19472192754.61524582\n",
      "Iteration 454, loss = 19472187871.54257965\n",
      "Iteration 455, loss = 19472182991.98967743\n",
      "Iteration 456, loss = 19472178084.57679367\n",
      "Iteration 457, loss = 19472173223.49178314\n",
      "Iteration 458, loss = 19472168313.73803711\n",
      "Iteration 459, loss = 19472163426.64292526\n",
      "Iteration 460, loss = 19472158564.40925598\n",
      "Iteration 461, loss = 19472153618.21583939\n",
      "Iteration 462, loss = 19472148760.69803619\n",
      "Iteration 463, loss = 19472143876.31204224\n",
      "Iteration 464, loss = 19472138965.74907303\n",
      "Iteration 465, loss = 19472134106.76071167\n",
      "Iteration 466, loss = 19472129189.08490753\n",
      "Iteration 467, loss = 19472124318.24644089\n",
      "Iteration 468, loss = 19472119422.31736755\n",
      "Iteration 469, loss = 19472114507.81758499\n",
      "Iteration 470, loss = 19472109660.50348282\n",
      "Iteration 471, loss = 19472104714.74557495\n",
      "Iteration 472, loss = 19472099869.74562836\n",
      "Iteration 473, loss = 19472094957.15943146\n",
      "Iteration 474, loss = 19472090077.43308640\n",
      "Iteration 475, loss = 19472085171.66584396\n",
      "Iteration 476, loss = 19472080300.53457260\n",
      "Iteration 477, loss = 19472075387.71809006\n",
      "Iteration 478, loss = 19472070534.07609558\n",
      "Iteration 479, loss = 19472065672.40862656\n",
      "Iteration 480, loss = 19472060759.66736221\n",
      "Iteration 481, loss = 19472055900.46084595\n",
      "Iteration 482, loss = 19472050997.76335526\n",
      "Iteration 483, loss = 19472046117.78642654\n",
      "Iteration 484, loss = 19472041222.47696686\n",
      "Iteration 485, loss = 19472036317.25380707\n",
      "Iteration 486, loss = 19472031483.95860291\n",
      "Iteration 487, loss = 19472026530.59688950\n",
      "Iteration 488, loss = 19472021667.70609283\n",
      "Iteration 489, loss = 19472016767.04653549\n",
      "Iteration 490, loss = 19472011923.83280563\n",
      "Iteration 491, loss = 19472007005.56138611\n",
      "Iteration 492, loss = 19472002148.18376160\n",
      "Iteration 493, loss = 19471997239.42516327\n",
      "Iteration 494, loss = 19471992350.83150101\n",
      "Iteration 495, loss = 19471987480.07633591\n",
      "Iteration 496, loss = 19471982557.29707336\n",
      "Iteration 497, loss = 19471977677.96591949\n",
      "Iteration 498, loss = 19471972853.01116943\n",
      "Iteration 499, loss = 19471967881.80594635\n",
      "Iteration 500, loss = 19471963043.04219055\n",
      "Iteration 1, loss = 19425739299.22434616\n",
      "Iteration 2, loss = 19425719129.09988022\n",
      "Iteration 3, loss = 19425698334.70383453\n",
      "Iteration 4, loss = 19425676875.49225998\n",
      "Iteration 5, loss = 19425655015.16971970\n",
      "Iteration 6, loss = 19425633717.72889709\n",
      "Iteration 7, loss = 19425612575.65423965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 19425590642.66303253\n",
      "Iteration 9, loss = 19425566770.77719498\n",
      "Iteration 10, loss = 19425542832.64921570\n",
      "Iteration 11, loss = 19425518286.94738388\n",
      "Iteration 12, loss = 19425492963.36310959\n",
      "Iteration 13, loss = 19425468847.20220566\n",
      "Iteration 14, loss = 19425446222.14669037\n",
      "Iteration 15, loss = 19425425257.21216583\n",
      "Iteration 16, loss = 19425404847.02590942\n",
      "Iteration 17, loss = 19425385246.68294525\n",
      "Iteration 18, loss = 19425365862.71534729\n",
      "Iteration 19, loss = 19425345519.88361740\n",
      "Iteration 20, loss = 19425323202.57359314\n",
      "Iteration 21, loss = 19425300499.32593536\n",
      "Iteration 22, loss = 19425280393.32467651\n",
      "Iteration 23, loss = 19425260699.80768204\n",
      "Iteration 24, loss = 19425240776.00478745\n",
      "Iteration 25, loss = 19425221422.89087296\n",
      "Iteration 26, loss = 19425205250.86014175\n",
      "Iteration 27, loss = 19425191220.54944611\n",
      "Iteration 28, loss = 19425176939.62979126\n",
      "Iteration 29, loss = 19425162208.69906616\n",
      "Iteration 30, loss = 19425147554.10058594\n",
      "Iteration 31, loss = 19425132601.29111862\n",
      "Iteration 32, loss = 19425120725.96622849\n",
      "Iteration 33, loss = 19425110084.74722672\n",
      "Iteration 34, loss = 19425099389.13782883\n",
      "Iteration 35, loss = 19425088238.34190750\n",
      "Iteration 36, loss = 19425077158.99973679\n",
      "Iteration 37, loss = 19425067139.40484619\n",
      "Iteration 38, loss = 19425057425.09622574\n",
      "Iteration 39, loss = 19425048038.71252823\n",
      "Iteration 40, loss = 19425038765.02632904\n",
      "Iteration 41, loss = 19425029575.34653854\n",
      "Iteration 42, loss = 19425020665.23785782\n",
      "Iteration 43, loss = 19425011767.21320724\n",
      "Iteration 44, loss = 19425002894.98731995\n",
      "Iteration 45, loss = 19424994331.43898010\n",
      "Iteration 46, loss = 19424985835.88178635\n",
      "Iteration 47, loss = 19424977322.15506744\n",
      "Iteration 48, loss = 19424968697.57752228\n",
      "Iteration 49, loss = 19424959925.59239578\n",
      "Iteration 50, loss = 19424951367.38345718\n",
      "Iteration 51, loss = 19424942846.45580292\n",
      "Iteration 52, loss = 19424934283.12113190\n",
      "Iteration 53, loss = 19424925825.52659988\n",
      "Iteration 54, loss = 19424917446.73018646\n",
      "Iteration 55, loss = 19424909025.52773285\n",
      "Iteration 56, loss = 19424900515.06546783\n",
      "Iteration 57, loss = 19424892093.43332291\n",
      "Iteration 58, loss = 19424883794.97677994\n",
      "Iteration 59, loss = 19424875550.48792648\n",
      "Iteration 60, loss = 19424867161.83659363\n",
      "Iteration 61, loss = 19424858906.76436234\n",
      "Iteration 62, loss = 19424850622.25666046\n",
      "Iteration 63, loss = 19424842397.48685074\n",
      "Iteration 64, loss = 19424834172.22269058\n",
      "Iteration 65, loss = 19424825815.15392685\n",
      "Iteration 66, loss = 19424817423.89752197\n",
      "Iteration 67, loss = 19424809017.27127075\n",
      "Iteration 68, loss = 19424800512.51869965\n",
      "Iteration 69, loss = 19424791987.54819489\n",
      "Iteration 70, loss = 19424783393.96809387\n",
      "Iteration 71, loss = 19424774848.11688232\n",
      "Iteration 72, loss = 19424766402.22001266\n",
      "Iteration 73, loss = 19424757965.99685669\n",
      "Iteration 74, loss = 19424749649.59608459\n",
      "Iteration 75, loss = 19424741269.99672699\n",
      "Iteration 76, loss = 19424733024.24833298\n",
      "Iteration 77, loss = 19424724750.90473175\n",
      "Iteration 78, loss = 19424716537.61615753\n",
      "Iteration 79, loss = 19424708416.70449829\n",
      "Iteration 80, loss = 19424700289.90211487\n",
      "Iteration 81, loss = 19424692234.25376129\n",
      "Iteration 82, loss = 19424684130.83155441\n",
      "Iteration 83, loss = 19424676131.72703934\n",
      "Iteration 84, loss = 19424668056.45302582\n",
      "Iteration 85, loss = 19424660125.27030563\n",
      "Iteration 86, loss = 19424652088.45821762\n",
      "Iteration 87, loss = 19424644184.61364365\n",
      "Iteration 88, loss = 19424636217.56728363\n",
      "Iteration 89, loss = 19424628355.68502426\n",
      "Iteration 90, loss = 19424620450.63933182\n",
      "Iteration 91, loss = 19424612569.52317810\n",
      "Iteration 92, loss = 19424604753.48231125\n",
      "Iteration 93, loss = 19424597026.83367538\n",
      "Iteration 94, loss = 19424589165.46214676\n",
      "Iteration 95, loss = 19424581458.25913620\n",
      "Iteration 96, loss = 19424573726.71667099\n",
      "Iteration 97, loss = 19424566028.38896942\n",
      "Iteration 98, loss = 19424558273.26238632\n",
      "Iteration 99, loss = 19424550587.45604324\n",
      "Iteration 100, loss = 19424542792.79109573\n",
      "Iteration 101, loss = 19424535067.45634460\n",
      "Iteration 102, loss = 19424527383.94032669\n",
      "Iteration 103, loss = 19424519626.26609039\n",
      "Iteration 104, loss = 19424511962.18928909\n",
      "Iteration 105, loss = 19424504218.24916458\n",
      "Iteration 106, loss = 19424496602.25555801\n",
      "Iteration 107, loss = 19424488858.86725998\n",
      "Iteration 108, loss = 19424481199.30086899\n",
      "Iteration 109, loss = 19424473540.54610062\n",
      "Iteration 110, loss = 19424465912.99131393\n",
      "Iteration 111, loss = 19424458290.76601410\n",
      "Iteration 112, loss = 19424450730.38326263\n",
      "Iteration 113, loss = 19424443103.64018250\n",
      "Iteration 114, loss = 19424435516.03986359\n",
      "Iteration 115, loss = 19424427927.43097687\n",
      "Iteration 116, loss = 19424420319.74064255\n",
      "Iteration 117, loss = 19424412698.10651398\n",
      "Iteration 118, loss = 19424405092.96110153\n",
      "Iteration 119, loss = 19424397494.68290329\n",
      "Iteration 120, loss = 19424389906.64010620\n",
      "Iteration 121, loss = 19424382393.35564804\n",
      "Iteration 122, loss = 19424374849.38978577\n",
      "Iteration 123, loss = 19424367262.69191360\n",
      "Iteration 124, loss = 19424359723.78761673\n",
      "Iteration 125, loss = 19424352198.93490219\n",
      "Iteration 126, loss = 19424344678.94892502\n",
      "Iteration 127, loss = 19424337132.17451477\n",
      "Iteration 128, loss = 19424329623.11820221\n",
      "Iteration 129, loss = 19424322049.67780685\n",
      "Iteration 130, loss = 19424314581.54713440\n",
      "Iteration 131, loss = 19424307040.04826355\n",
      "Iteration 132, loss = 19424299583.32015610\n",
      "Iteration 133, loss = 19424292095.51173401\n",
      "Iteration 134, loss = 19424284625.43481445\n",
      "Iteration 135, loss = 19424277192.06951141\n",
      "Iteration 136, loss = 19424269712.85591507\n",
      "Iteration 137, loss = 19424262181.37432480\n",
      "Iteration 138, loss = 19424254783.99273682\n",
      "Iteration 139, loss = 19424247356.69998169\n",
      "Iteration 140, loss = 19424239879.62176132\n",
      "Iteration 141, loss = 19424232467.38225555\n",
      "Iteration 142, loss = 19424224996.63107681\n",
      "Iteration 143, loss = 19424217537.18125534\n",
      "Iteration 144, loss = 19424210089.10718918\n",
      "Iteration 145, loss = 19424202671.77601624\n",
      "Iteration 146, loss = 19424195236.07963943\n",
      "Iteration 147, loss = 19424187748.19923019\n",
      "Iteration 148, loss = 19424180401.55240631\n",
      "Iteration 149, loss = 19424172938.10808945\n",
      "Iteration 150, loss = 19424165598.88237000\n",
      "Iteration 151, loss = 19424158194.25448990\n",
      "Iteration 152, loss = 19424150754.33072281\n",
      "Iteration 153, loss = 19424143344.79597855\n",
      "Iteration 154, loss = 19424135894.15730286\n",
      "Iteration 155, loss = 19424128493.87845230\n",
      "Iteration 156, loss = 19424121034.17329407\n",
      "Iteration 157, loss = 19424113620.78754425\n",
      "Iteration 158, loss = 19424106234.86934280\n",
      "Iteration 159, loss = 19424098800.49793243\n",
      "Iteration 160, loss = 19424091410.35203552\n",
      "Iteration 161, loss = 19424084047.65002441\n",
      "Iteration 162, loss = 19424076613.45879745\n",
      "Iteration 163, loss = 19424069248.17151260\n",
      "Iteration 164, loss = 19424061901.49055481\n",
      "Iteration 165, loss = 19424054464.92250824\n",
      "Iteration 166, loss = 19424047120.74752045\n",
      "Iteration 167, loss = 19424039832.06121445\n",
      "Iteration 168, loss = 19424032449.08260345\n",
      "Iteration 169, loss = 19424025078.82203293\n",
      "Iteration 170, loss = 19424017721.62775803\n",
      "Iteration 171, loss = 19424010378.41600418\n",
      "Iteration 172, loss = 19424003040.00309753\n",
      "Iteration 173, loss = 19423995617.21494293\n",
      "Iteration 174, loss = 19423988287.39450073\n",
      "Iteration 175, loss = 19423980937.96330643\n",
      "Iteration 176, loss = 19423973644.76856232\n",
      "Iteration 177, loss = 19423966270.03100204\n",
      "Iteration 178, loss = 19423958911.83195877\n",
      "Iteration 179, loss = 19423951534.67655182\n",
      "Iteration 180, loss = 19423944179.49359512\n",
      "Iteration 181, loss = 19423936811.21143341\n",
      "Iteration 182, loss = 19423929513.44567490\n",
      "Iteration 183, loss = 19423922110.11846542\n",
      "Iteration 184, loss = 19423914763.23929214\n",
      "Iteration 185, loss = 19423907446.52581024\n",
      "Iteration 186, loss = 19423900138.26713181\n",
      "Iteration 187, loss = 19423892750.89310837\n",
      "Iteration 188, loss = 19423885442.89521027\n",
      "Iteration 189, loss = 19423877999.57986832\n",
      "Iteration 190, loss = 19423870614.79935074\n",
      "Iteration 191, loss = 19423863258.39297104\n",
      "Iteration 192, loss = 19423855908.45145035\n",
      "Iteration 193, loss = 19423848611.07757568\n",
      "Iteration 194, loss = 19423841320.43177795\n",
      "Iteration 195, loss = 19423834019.49660110\n",
      "Iteration 196, loss = 19423826635.41887665\n",
      "Iteration 197, loss = 19423819393.32633972\n",
      "Iteration 198, loss = 19423812071.93297958\n",
      "Iteration 199, loss = 19423804797.19763184\n",
      "Iteration 200, loss = 19423797494.12494659\n",
      "Iteration 201, loss = 19423790197.51873016\n",
      "Iteration 202, loss = 19423782917.81069183\n",
      "Iteration 203, loss = 19423775600.99271774\n",
      "Iteration 204, loss = 19423768332.81086349\n",
      "Iteration 205, loss = 19423760999.08772278\n",
      "Iteration 206, loss = 19423753670.90552139\n",
      "Iteration 207, loss = 19423746342.42741394\n",
      "Iteration 208, loss = 19423739029.04749298\n",
      "Iteration 209, loss = 19423731705.85957718\n",
      "Iteration 210, loss = 19423724404.18262482\n",
      "Iteration 211, loss = 19423717091.90639114\n",
      "Iteration 212, loss = 19423709781.27960587\n",
      "Iteration 213, loss = 19423702500.86087799\n",
      "Iteration 214, loss = 19423695181.06702042\n",
      "Iteration 215, loss = 19423687860.67903137\n",
      "Iteration 216, loss = 19423680591.92047501\n",
      "Iteration 217, loss = 19423673327.68497849\n",
      "Iteration 218, loss = 19423666045.83444595\n",
      "Iteration 219, loss = 19423658863.78797531\n",
      "Iteration 220, loss = 19423651533.99198532\n",
      "Iteration 221, loss = 19423644276.40921783\n",
      "Iteration 222, loss = 19423637010.54831696\n",
      "Iteration 223, loss = 19423629754.72636032\n",
      "Iteration 224, loss = 19423622359.20564651\n",
      "Iteration 225, loss = 19423615112.76256561\n",
      "Iteration 226, loss = 19423607776.00151443\n",
      "Iteration 227, loss = 19423600448.33658981\n",
      "Iteration 228, loss = 19423593210.72270203\n",
      "Iteration 229, loss = 19423585842.18165970\n",
      "Iteration 230, loss = 19423578620.73372269\n",
      "Iteration 231, loss = 19423571276.28907394\n",
      "Iteration 232, loss = 19423564053.59767151\n",
      "Iteration 233, loss = 19423556724.67610550\n",
      "Iteration 234, loss = 19423549501.66604996\n",
      "Iteration 235, loss = 19423542260.05308151\n",
      "Iteration 236, loss = 19423535006.74664688\n",
      "Iteration 237, loss = 19423527673.59651566\n",
      "Iteration 238, loss = 19423520426.55380249\n",
      "Iteration 239, loss = 19423513140.24161148\n",
      "Iteration 240, loss = 19423505960.41321564\n",
      "Iteration 241, loss = 19423498660.38305664\n",
      "Iteration 242, loss = 19423491426.39039230\n",
      "Iteration 243, loss = 19423484212.28927994\n",
      "Iteration 244, loss = 19423477003.80887222\n",
      "Iteration 245, loss = 19423469721.19100189\n",
      "Iteration 246, loss = 19423462466.17757034\n",
      "Iteration 247, loss = 19423455223.22138977\n",
      "Iteration 248, loss = 19423447894.15491486\n",
      "Iteration 249, loss = 19423440651.03700638\n",
      "Iteration 250, loss = 19423433364.91734695\n",
      "Iteration 251, loss = 19423426020.72348785\n",
      "Iteration 252, loss = 19423418720.97258759\n",
      "Iteration 253, loss = 19423411485.35681534\n",
      "Iteration 254, loss = 19423404215.15656662\n",
      "Iteration 255, loss = 19423396855.50727844\n",
      "Iteration 256, loss = 19423389574.45610046\n",
      "Iteration 257, loss = 19423382330.72587967\n",
      "Iteration 258, loss = 19423375026.82943726\n",
      "Iteration 259, loss = 19423367785.38848495\n",
      "Iteration 260, loss = 19423360501.69033051\n",
      "Iteration 261, loss = 19423353268.60852814\n",
      "Iteration 262, loss = 19423345971.51048279\n",
      "Iteration 263, loss = 19423338731.50424576\n",
      "Iteration 264, loss = 19423331485.57253647\n",
      "Iteration 265, loss = 19423324143.28070068\n",
      "Iteration 266, loss = 19423316944.16299057\n",
      "Iteration 267, loss = 19423309621.55004120\n",
      "Iteration 268, loss = 19423302411.72244263\n",
      "Iteration 269, loss = 19423295125.05320740\n",
      "Iteration 270, loss = 19423287867.50215149\n",
      "Iteration 271, loss = 19423280595.81176758\n",
      "Iteration 272, loss = 19423273351.64076996\n",
      "Iteration 273, loss = 19423266089.51052094\n",
      "Iteration 274, loss = 19423258848.04439926\n",
      "Iteration 275, loss = 19423251659.65918732\n",
      "Iteration 276, loss = 19423244454.89131546\n",
      "Iteration 277, loss = 19423237238.55237198\n",
      "Iteration 278, loss = 19423230071.24890518\n",
      "Iteration 279, loss = 19423222815.10748672\n",
      "Iteration 280, loss = 19423215640.11759949\n",
      "Iteration 281, loss = 19423208404.82836151\n",
      "Iteration 282, loss = 19423201233.50241852\n",
      "Iteration 283, loss = 19423194002.46330643\n",
      "Iteration 284, loss = 19423186824.50821304\n",
      "Iteration 285, loss = 19423179619.49163437\n",
      "Iteration 286, loss = 19423172387.84469986\n",
      "Iteration 287, loss = 19423165172.57918549\n",
      "Iteration 288, loss = 19423157960.83363724\n",
      "Iteration 289, loss = 19423150749.39235306\n",
      "Iteration 290, loss = 19423143555.87263870\n",
      "Iteration 291, loss = 19423136376.62382507\n",
      "Iteration 292, loss = 19423129171.61849976\n",
      "Iteration 293, loss = 19423122019.40023804\n",
      "Iteration 294, loss = 19423114806.69760895\n",
      "Iteration 295, loss = 19423107601.87391281\n",
      "Iteration 296, loss = 19423100466.97272110\n",
      "Iteration 297, loss = 19423093229.74399185\n",
      "Iteration 298, loss = 19423086029.28948212\n",
      "Iteration 299, loss = 19423078841.55347824\n",
      "Iteration 300, loss = 19423071614.29972458\n",
      "Iteration 301, loss = 19423064408.48139191\n",
      "Iteration 302, loss = 19423057205.72789764\n",
      "Iteration 303, loss = 19423049994.69675827\n",
      "Iteration 304, loss = 19423042809.91548157\n",
      "Iteration 305, loss = 19423035516.99980927\n",
      "Iteration 306, loss = 19423028341.03260803\n",
      "Iteration 307, loss = 19423021076.92832947\n",
      "Iteration 308, loss = 19423013889.73444748\n",
      "Iteration 309, loss = 19423006637.98677063\n",
      "Iteration 310, loss = 19422999411.89819336\n",
      "Iteration 311, loss = 19422992176.46743774\n",
      "Iteration 312, loss = 19422984964.33222580\n",
      "Iteration 313, loss = 19422977796.69318008\n",
      "Iteration 314, loss = 19422970547.84483719\n",
      "Iteration 315, loss = 19422963355.46704483\n",
      "Iteration 316, loss = 19422956145.44738770\n",
      "Iteration 317, loss = 19422948903.23458099\n",
      "Iteration 318, loss = 19422941675.87668228\n",
      "Iteration 319, loss = 19422934499.80064392\n",
      "Iteration 320, loss = 19422927233.39670563\n",
      "Iteration 321, loss = 19422919968.17440796\n",
      "Iteration 322, loss = 19422912785.02159882\n",
      "Iteration 323, loss = 19422905565.93425751\n",
      "Iteration 324, loss = 19422898410.03043365\n",
      "Iteration 325, loss = 19422891159.25733185\n",
      "Iteration 326, loss = 19422884011.62383652\n",
      "Iteration 327, loss = 19422876860.90469360\n",
      "Iteration 328, loss = 19422869722.13428497\n",
      "Iteration 329, loss = 19422862549.35134888\n",
      "Iteration 330, loss = 19422855324.30845642\n",
      "Iteration 331, loss = 19422848181.43240356\n",
      "Iteration 332, loss = 19422841017.55929947\n",
      "Iteration 333, loss = 19422833812.23249435\n",
      "Iteration 334, loss = 19422826674.40105057\n",
      "Iteration 335, loss = 19422819440.50524902\n",
      "Iteration 336, loss = 19422812269.13807678\n",
      "Iteration 337, loss = 19422805080.54722977\n",
      "Iteration 338, loss = 19422797872.70915222\n",
      "Iteration 339, loss = 19422790631.85172653\n",
      "Iteration 340, loss = 19422783437.73599243\n",
      "Iteration 341, loss = 19422776274.42016220\n",
      "Iteration 342, loss = 19422769130.14067078\n",
      "Iteration 343, loss = 19422761893.10528564\n",
      "Iteration 344, loss = 19422754745.63657761\n",
      "Iteration 345, loss = 19422747540.76167679\n",
      "Iteration 346, loss = 19422740385.88879395\n",
      "Iteration 347, loss = 19422733174.13570023\n",
      "Iteration 348, loss = 19422725986.04408646\n",
      "Iteration 349, loss = 19422718811.15024185\n",
      "Iteration 350, loss = 19422711576.65233231\n",
      "Iteration 351, loss = 19422704434.50069046\n",
      "Iteration 352, loss = 19422697267.14111328\n",
      "Iteration 353, loss = 19422690130.97966003\n",
      "Iteration 354, loss = 19422682951.77817917\n",
      "Iteration 355, loss = 19422675735.86772537\n",
      "Iteration 356, loss = 19422668606.40068054\n",
      "Iteration 357, loss = 19422661342.00251007\n",
      "Iteration 358, loss = 19422654217.78400421\n",
      "Iteration 359, loss = 19422647012.78153229\n",
      "Iteration 360, loss = 19422639808.59614182\n",
      "Iteration 361, loss = 19422632616.41622925\n",
      "Iteration 362, loss = 19422625369.27640915\n",
      "Iteration 363, loss = 19422618172.36626434\n",
      "Iteration 364, loss = 19422610936.47457123\n",
      "Iteration 365, loss = 19422603688.66050339\n",
      "Iteration 366, loss = 19422596399.60173798\n",
      "Iteration 367, loss = 19422589223.17761612\n",
      "Iteration 368, loss = 19422582115.25134659\n",
      "Iteration 369, loss = 19422574874.12339401\n",
      "Iteration 370, loss = 19422567676.61585236\n",
      "Iteration 371, loss = 19422560549.95675278\n",
      "Iteration 372, loss = 19422553408.44948959\n",
      "Iteration 373, loss = 19422546219.83852386\n",
      "Iteration 374, loss = 19422539009.77307892\n",
      "Iteration 375, loss = 19422531852.15079498\n",
      "Iteration 376, loss = 19422524680.39022446\n",
      "Iteration 377, loss = 19422517502.32521439\n",
      "Iteration 378, loss = 19422510314.49720764\n",
      "Iteration 379, loss = 19422503109.64686584\n",
      "Iteration 380, loss = 19422495970.94774246\n",
      "Iteration 381, loss = 19422488776.23909378\n",
      "Iteration 382, loss = 19422481584.50320816\n",
      "Iteration 383, loss = 19422474424.99817276\n",
      "Iteration 384, loss = 19422467256.92261124\n",
      "Iteration 385, loss = 19422460079.99013901\n",
      "Iteration 386, loss = 19422452827.67003632\n",
      "Iteration 387, loss = 19422445642.49154663\n",
      "Iteration 388, loss = 19422438433.41060257\n",
      "Iteration 389, loss = 19422431186.55392075\n",
      "Iteration 390, loss = 19422423896.32914734\n",
      "Iteration 391, loss = 19422416732.92209244\n",
      "Iteration 392, loss = 19422409560.81801987\n",
      "Iteration 393, loss = 19422402384.98988342\n",
      "Iteration 394, loss = 19422395247.74861526\n",
      "Iteration 395, loss = 19422388103.26007462\n",
      "Iteration 396, loss = 19422380956.80671692\n",
      "Iteration 397, loss = 19422373802.31326675\n",
      "Iteration 398, loss = 19422366682.02784729\n",
      "Iteration 399, loss = 19422359532.40277100\n",
      "Iteration 400, loss = 19422352393.97050476\n",
      "Iteration 401, loss = 19422345254.68040848\n",
      "Iteration 402, loss = 19422338122.44052124\n",
      "Iteration 403, loss = 19422330940.24612045\n",
      "Iteration 404, loss = 19422323728.46638489\n",
      "Iteration 405, loss = 19422316588.96639252\n",
      "Iteration 406, loss = 19422309438.64384460\n",
      "Iteration 407, loss = 19422302261.06436920\n",
      "Iteration 408, loss = 19422295159.94557571\n",
      "Iteration 409, loss = 19422288038.91408539\n",
      "Iteration 410, loss = 19422280910.97889328\n",
      "Iteration 411, loss = 19422273722.44869995\n",
      "Iteration 412, loss = 19422266592.60540771\n",
      "Iteration 413, loss = 19422259430.60973740\n",
      "Iteration 414, loss = 19422252250.25797653\n",
      "Iteration 415, loss = 19422245108.61522293\n",
      "Iteration 416, loss = 19422237930.20643234\n",
      "Iteration 417, loss = 19422230760.08957291\n",
      "Iteration 418, loss = 19422223590.19954300\n",
      "Iteration 419, loss = 19422216431.42773056\n",
      "Iteration 420, loss = 19422209284.87352371\n",
      "Iteration 421, loss = 19422202093.38402176\n",
      "Iteration 422, loss = 19422194936.82885361\n",
      "Iteration 423, loss = 19422187752.89157486\n",
      "Iteration 424, loss = 19422180584.88932419\n",
      "Iteration 425, loss = 19422173458.02290344\n",
      "Iteration 426, loss = 19422166226.54413605\n",
      "Iteration 427, loss = 19422159038.85007858\n",
      "Iteration 428, loss = 19422151863.70405579\n",
      "Iteration 429, loss = 19422144690.74549866\n",
      "Iteration 430, loss = 19422137490.95038605\n",
      "Iteration 431, loss = 19422130361.71068954\n",
      "Iteration 432, loss = 19422123140.68336868\n",
      "Iteration 433, loss = 19422115919.97285843\n",
      "Iteration 434, loss = 19422108709.53243256\n",
      "Iteration 435, loss = 19422101434.91409683\n",
      "Iteration 436, loss = 19422094245.43519974\n",
      "Iteration 437, loss = 19422087065.76314545\n",
      "Iteration 438, loss = 19422079856.51171875\n",
      "Iteration 439, loss = 19422072731.44552994\n",
      "Iteration 440, loss = 19422065527.96723175\n",
      "Iteration 441, loss = 19422058364.07742310\n",
      "Iteration 442, loss = 19422051179.89040756\n",
      "Iteration 443, loss = 19422044008.10905838\n",
      "Iteration 444, loss = 19422036807.98444748\n",
      "Iteration 445, loss = 19422029620.14155960\n",
      "Iteration 446, loss = 19422022467.80093384\n",
      "Iteration 447, loss = 19422015264.70805359\n",
      "Iteration 448, loss = 19422008057.59247971\n",
      "Iteration 449, loss = 19422000957.49405289\n",
      "Iteration 450, loss = 19421993790.69454575\n",
      "Iteration 451, loss = 19421986658.28141785\n",
      "Iteration 452, loss = 19421979511.17394257\n",
      "Iteration 453, loss = 19421972387.35743332\n",
      "Iteration 454, loss = 19421965231.73066330\n",
      "Iteration 455, loss = 19421958011.18949127\n",
      "Iteration 456, loss = 19421950833.71244812\n",
      "Iteration 457, loss = 19421943629.94003296\n",
      "Iteration 458, loss = 19421936481.84328461\n",
      "Iteration 459, loss = 19421929326.44878769\n",
      "Iteration 460, loss = 19421922134.04297256\n",
      "Iteration 461, loss = 19421914979.95257187\n",
      "Iteration 462, loss = 19421907758.41918182\n",
      "Iteration 463, loss = 19421900585.61715698\n",
      "Iteration 464, loss = 19421893422.64403534\n",
      "Iteration 465, loss = 19421886206.04961014\n",
      "Iteration 466, loss = 19421879035.03179550\n",
      "Iteration 467, loss = 19421871915.29252243\n",
      "Iteration 468, loss = 19421864744.11819839\n",
      "Iteration 469, loss = 19421857536.60886383\n",
      "Iteration 470, loss = 19421850337.74920273\n",
      "Iteration 471, loss = 19421843117.70055008\n",
      "Iteration 472, loss = 19421835942.59902191\n",
      "Iteration 473, loss = 19421828788.12907410\n",
      "Iteration 474, loss = 19421821603.89139938\n",
      "Iteration 475, loss = 19421814466.54707718\n",
      "Iteration 476, loss = 19421807292.31529617\n",
      "Iteration 477, loss = 19421800137.14940643\n",
      "Iteration 478, loss = 19421792977.42931366\n",
      "Iteration 479, loss = 19421785818.44850159\n",
      "Iteration 480, loss = 19421778671.77184677\n",
      "Iteration 481, loss = 19421771448.43502426\n",
      "Iteration 482, loss = 19421764330.26585007\n",
      "Iteration 483, loss = 19421757096.58135605\n",
      "Iteration 484, loss = 19421749942.51202774\n",
      "Iteration 485, loss = 19421742739.37388229\n",
      "Iteration 486, loss = 19421735655.20090103\n",
      "Iteration 487, loss = 19421728491.01386642\n",
      "Iteration 488, loss = 19421721305.75875092\n",
      "Iteration 489, loss = 19421714160.89572906\n",
      "Iteration 490, loss = 19421707035.30062485\n",
      "Iteration 491, loss = 19421699868.62997818\n",
      "Iteration 492, loss = 19421692707.00056076\n",
      "Iteration 493, loss = 19421685599.25449371\n",
      "Iteration 494, loss = 19421678432.30274200\n",
      "Iteration 495, loss = 19421671349.76890182\n",
      "Iteration 496, loss = 19421664215.01612473\n",
      "Iteration 497, loss = 19421657050.71516800\n",
      "Iteration 498, loss = 19421649907.45677567\n",
      "Iteration 499, loss = 19421642756.23593903\n",
      "Iteration 500, loss = 19421635539.50281906\n",
      "Iteration 1, loss = 19393289803.38052368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 19393269937.93341064\n",
      "Iteration 3, loss = 19393250172.67404175\n",
      "Iteration 4, loss = 19393230329.89141083\n",
      "Iteration 5, loss = 19393210233.53638077\n",
      "Iteration 6, loss = 19393190082.18137360\n",
      "Iteration 7, loss = 19393170206.46842957\n",
      "Iteration 8, loss = 19393149994.77949905\n",
      "Iteration 9, loss = 19393128292.13942719\n",
      "Iteration 10, loss = 19393106979.62553024\n",
      "Iteration 11, loss = 19393086265.55020142\n",
      "Iteration 12, loss = 19393065440.68710709\n",
      "Iteration 13, loss = 19393043949.40111923\n",
      "Iteration 14, loss = 19393020786.35652542\n",
      "Iteration 15, loss = 19392997735.05371094\n",
      "Iteration 16, loss = 19392976089.48819733\n",
      "Iteration 17, loss = 19392954955.89883804\n",
      "Iteration 18, loss = 19392934464.94556808\n",
      "Iteration 19, loss = 19392915057.74631119\n",
      "Iteration 20, loss = 19392897227.18534470\n",
      "Iteration 21, loss = 19392880261.03998184\n",
      "Iteration 22, loss = 19392863024.22808075\n",
      "Iteration 23, loss = 19392843916.26249695\n",
      "Iteration 24, loss = 19392827124.17530823\n",
      "Iteration 25, loss = 19392811183.19968033\n",
      "Iteration 26, loss = 19392794870.20314026\n",
      "Iteration 27, loss = 19392780367.31505966\n",
      "Iteration 28, loss = 19392765930.55963898\n",
      "Iteration 29, loss = 19392751157.01246262\n",
      "Iteration 30, loss = 19392736006.40532303\n",
      "Iteration 31, loss = 19392723377.34806824\n",
      "Iteration 32, loss = 19392711956.16135788\n",
      "Iteration 33, loss = 19392699860.33396149\n",
      "Iteration 34, loss = 19392688485.29974365\n",
      "Iteration 35, loss = 19392678260.53554916\n",
      "Iteration 36, loss = 19392669345.59443665\n",
      "Iteration 37, loss = 19392660803.48755646\n",
      "Iteration 38, loss = 19392652208.83692932\n",
      "Iteration 39, loss = 19392643225.98554993\n",
      "Iteration 40, loss = 19392635070.91121674\n",
      "Iteration 41, loss = 19392627196.38464355\n",
      "Iteration 42, loss = 19392619485.02954102\n",
      "Iteration 43, loss = 19392611789.07503128\n",
      "Iteration 44, loss = 19392603952.21029663\n",
      "Iteration 45, loss = 19392596436.69672012\n",
      "Iteration 46, loss = 19392589095.78926086\n",
      "Iteration 47, loss = 19392581777.26385880\n",
      "Iteration 48, loss = 19392574561.62208557\n",
      "Iteration 49, loss = 19392567378.47637939\n",
      "Iteration 50, loss = 19392560206.60106277\n",
      "Iteration 51, loss = 19392553093.32492065\n",
      "Iteration 52, loss = 19392545827.38117218\n",
      "Iteration 53, loss = 19392538400.51517868\n",
      "Iteration 54, loss = 19392531063.91497040\n",
      "Iteration 55, loss = 19392523817.13155365\n",
      "Iteration 56, loss = 19392516403.67870331\n",
      "Iteration 57, loss = 19392508847.15470505\n",
      "Iteration 58, loss = 19392501233.08155060\n",
      "Iteration 59, loss = 19392493463.06248093\n",
      "Iteration 60, loss = 19392485585.08987045\n",
      "Iteration 61, loss = 19392477619.83018494\n",
      "Iteration 62, loss = 19392469637.67232132\n",
      "Iteration 63, loss = 19392461559.32132721\n",
      "Iteration 64, loss = 19392453482.81129074\n",
      "Iteration 65, loss = 19392445264.67882919\n",
      "Iteration 66, loss = 19392437170.22203064\n",
      "Iteration 67, loss = 19392429075.49026489\n",
      "Iteration 68, loss = 19392421098.88734818\n",
      "Iteration 69, loss = 19392413032.94726944\n",
      "Iteration 70, loss = 19392405059.02920532\n",
      "Iteration 71, loss = 19392397172.81107712\n",
      "Iteration 72, loss = 19392389220.04225159\n",
      "Iteration 73, loss = 19392381399.19566727\n",
      "Iteration 74, loss = 19392373587.34723663\n",
      "Iteration 75, loss = 19392365876.71563721\n",
      "Iteration 76, loss = 19392358166.29599380\n",
      "Iteration 77, loss = 19392350527.68770218\n",
      "Iteration 78, loss = 19392342927.54167938\n",
      "Iteration 79, loss = 19392335429.29639816\n",
      "Iteration 80, loss = 19392327867.04206848\n",
      "Iteration 81, loss = 19392320452.41493225\n",
      "Iteration 82, loss = 19392312953.77699661\n",
      "Iteration 83, loss = 19392305597.47495651\n",
      "Iteration 84, loss = 19392298186.22084427\n",
      "Iteration 85, loss = 19392290893.01851273\n",
      "Iteration 86, loss = 19392283552.38530731\n",
      "Iteration 87, loss = 19392276250.49012756\n",
      "Iteration 88, loss = 19392268973.28763962\n",
      "Iteration 89, loss = 19392261737.73727036\n",
      "Iteration 90, loss = 19392254542.22380066\n",
      "Iteration 91, loss = 19392247370.50789642\n",
      "Iteration 92, loss = 19392240137.23452377\n",
      "Iteration 93, loss = 19392233010.25085068\n",
      "Iteration 94, loss = 19392225829.01247787\n",
      "Iteration 95, loss = 19392218734.98526382\n",
      "Iteration 96, loss = 19392211594.74637985\n",
      "Iteration 97, loss = 19392204490.36734390\n",
      "Iteration 98, loss = 19392197297.61195755\n",
      "Iteration 99, loss = 19392190165.32490158\n",
      "Iteration 100, loss = 19392183114.39931107\n",
      "Iteration 101, loss = 19392176044.57816696\n",
      "Iteration 102, loss = 19392168950.30064392\n",
      "Iteration 103, loss = 19392161902.89638901\n",
      "Iteration 104, loss = 19392154757.03582764\n",
      "Iteration 105, loss = 19392147640.53883362\n",
      "Iteration 106, loss = 19392140452.95771027\n",
      "Iteration 107, loss = 19392133217.98917770\n",
      "Iteration 108, loss = 19392125861.70382309\n",
      "Iteration 109, loss = 19392118441.51691437\n",
      "Iteration 110, loss = 19392110906.11497116\n",
      "Iteration 111, loss = 19392103303.64888382\n",
      "Iteration 112, loss = 19392095679.96816254\n",
      "Iteration 113, loss = 19392088035.42849350\n",
      "Iteration 114, loss = 19392080361.34598541\n",
      "Iteration 115, loss = 19392072713.26989365\n",
      "Iteration 116, loss = 19392065127.09202576\n",
      "Iteration 117, loss = 19392057611.98223877\n",
      "Iteration 118, loss = 19392050073.68516541\n",
      "Iteration 119, loss = 19392042578.65005112\n",
      "Iteration 120, loss = 19392035159.56264496\n",
      "Iteration 121, loss = 19392027744.46637726\n",
      "Iteration 122, loss = 19392020340.43098831\n",
      "Iteration 123, loss = 19392013037.56322098\n",
      "Iteration 124, loss = 19392005622.27894592\n",
      "Iteration 125, loss = 19391998353.04018784\n",
      "Iteration 126, loss = 19391991032.57359314\n",
      "Iteration 127, loss = 19391983749.76272202\n",
      "Iteration 128, loss = 19391976469.49435043\n",
      "Iteration 129, loss = 19391969246.94561768\n",
      "Iteration 130, loss = 19391962031.79208755\n",
      "Iteration 131, loss = 19391954852.00782013\n",
      "Iteration 132, loss = 19391947677.45965195\n",
      "Iteration 133, loss = 19391940499.81411362\n",
      "Iteration 134, loss = 19391933310.69666672\n",
      "Iteration 135, loss = 19391926232.66292572\n",
      "Iteration 136, loss = 19391919050.85630035\n",
      "Iteration 137, loss = 19391911951.55051041\n",
      "Iteration 138, loss = 19391904837.90146255\n",
      "Iteration 139, loss = 19391897750.21186829\n",
      "Iteration 140, loss = 19391890644.79246521\n",
      "Iteration 141, loss = 19391883601.88584137\n",
      "Iteration 142, loss = 19391876555.88970184\n",
      "Iteration 143, loss = 19391869505.42250443\n",
      "Iteration 144, loss = 19391862449.36655045\n",
      "Iteration 145, loss = 19391855410.65261841\n",
      "Iteration 146, loss = 19391848430.09791183\n",
      "Iteration 147, loss = 19391841370.80588913\n",
      "Iteration 148, loss = 19391834393.15797043\n",
      "Iteration 149, loss = 19391827387.45286942\n",
      "Iteration 150, loss = 19391820392.04485321\n",
      "Iteration 151, loss = 19391813389.02807617\n",
      "Iteration 152, loss = 19391806435.33594131\n",
      "Iteration 153, loss = 19391799472.26673126\n",
      "Iteration 154, loss = 19391792496.42579651\n",
      "Iteration 155, loss = 19391785545.24515152\n",
      "Iteration 156, loss = 19391778630.17164230\n",
      "Iteration 157, loss = 19391771675.68750000\n",
      "Iteration 158, loss = 19391764719.36915588\n",
      "Iteration 159, loss = 19391757798.93027115\n",
      "Iteration 160, loss = 19391750871.60803986\n",
      "Iteration 161, loss = 19391743934.87441635\n",
      "Iteration 162, loss = 19391737014.37351227\n",
      "Iteration 163, loss = 19391730122.15497208\n",
      "Iteration 164, loss = 19391723197.11928177\n",
      "Iteration 165, loss = 19391716326.60404968\n",
      "Iteration 166, loss = 19391709442.81665421\n",
      "Iteration 167, loss = 19391702529.81734467\n",
      "Iteration 168, loss = 19391695677.30802155\n",
      "Iteration 169, loss = 19391688788.21680069\n",
      "Iteration 170, loss = 19391681887.03084946\n",
      "Iteration 171, loss = 19391675043.11670685\n",
      "Iteration 172, loss = 19391668218.94231033\n",
      "Iteration 173, loss = 19391661351.44250870\n",
      "Iteration 174, loss = 19391654437.49637985\n",
      "Iteration 175, loss = 19391647618.48542023\n",
      "Iteration 176, loss = 19391640744.58116531\n",
      "Iteration 177, loss = 19391633950.44596100\n",
      "Iteration 178, loss = 19391627108.62352753\n",
      "Iteration 179, loss = 19391620220.48073959\n",
      "Iteration 180, loss = 19391613433.68763351\n",
      "Iteration 181, loss = 19391606613.79875946\n",
      "Iteration 182, loss = 19391599777.89347076\n",
      "Iteration 183, loss = 19391592938.91592026\n",
      "Iteration 184, loss = 19391586156.98645782\n",
      "Iteration 185, loss = 19391579311.97919846\n",
      "Iteration 186, loss = 19391572507.89264679\n",
      "Iteration 187, loss = 19391565725.13026810\n",
      "Iteration 188, loss = 19391558886.78648758\n",
      "Iteration 189, loss = 19391552137.72941971\n",
      "Iteration 190, loss = 19391545340.28844833\n",
      "Iteration 191, loss = 19391538520.61815643\n",
      "Iteration 192, loss = 19391531756.51718521\n",
      "Iteration 193, loss = 19391524905.95722961\n",
      "Iteration 194, loss = 19391518174.38308334\n",
      "Iteration 195, loss = 19391511394.22714615\n",
      "Iteration 196, loss = 19391504607.36695099\n",
      "Iteration 197, loss = 19391497822.13630295\n",
      "Iteration 198, loss = 19391491073.37026978\n",
      "Iteration 199, loss = 19391484277.80450439\n",
      "Iteration 200, loss = 19391477489.53408813\n",
      "Iteration 201, loss = 19391470778.92320633\n",
      "Iteration 202, loss = 19391463963.63512039\n",
      "Iteration 203, loss = 19391457271.11342621\n",
      "Iteration 204, loss = 19391450501.27964020\n",
      "Iteration 205, loss = 19391443722.92464447\n",
      "Iteration 206, loss = 19391437011.98794174\n",
      "Iteration 207, loss = 19391430203.29577637\n",
      "Iteration 208, loss = 19391423478.58668900\n",
      "Iteration 209, loss = 19391416774.03929901\n",
      "Iteration 210, loss = 19391409997.21468735\n",
      "Iteration 211, loss = 19391403282.28309631\n",
      "Iteration 212, loss = 19391396591.32672501\n",
      "Iteration 213, loss = 19391389796.85718918\n",
      "Iteration 214, loss = 19391383129.68305969\n",
      "Iteration 215, loss = 19391376332.84561920\n",
      "Iteration 216, loss = 19391369653.83680344\n",
      "Iteration 217, loss = 19391362953.29334641\n",
      "Iteration 218, loss = 19391356206.63864899\n",
      "Iteration 219, loss = 19391349511.64671707\n",
      "Iteration 220, loss = 19391342750.42411423\n",
      "Iteration 221, loss = 19391336061.78737640\n",
      "Iteration 222, loss = 19391329364.10347748\n",
      "Iteration 223, loss = 19391322644.64358902\n",
      "Iteration 224, loss = 19391315926.90504074\n",
      "Iteration 225, loss = 19391309251.97711945\n",
      "Iteration 226, loss = 19391302517.54682159\n",
      "Iteration 227, loss = 19391295853.75196457\n",
      "Iteration 228, loss = 19391289093.37491608\n",
      "Iteration 229, loss = 19391282434.13013077\n",
      "Iteration 230, loss = 19391275769.99986267\n",
      "Iteration 231, loss = 19391269009.92618179\n",
      "Iteration 232, loss = 19391262342.63281631\n",
      "Iteration 233, loss = 19391255664.28892899\n",
      "Iteration 234, loss = 19391248986.19454193\n",
      "Iteration 235, loss = 19391242261.58605194\n",
      "Iteration 236, loss = 19391235586.03450775\n",
      "Iteration 237, loss = 19391228898.39542007\n",
      "Iteration 238, loss = 19391222235.56663513\n",
      "Iteration 239, loss = 19391215524.22094345\n",
      "Iteration 240, loss = 19391208876.11553955\n",
      "Iteration 241, loss = 19391202212.43375397\n",
      "Iteration 242, loss = 19391195503.04529572\n",
      "Iteration 243, loss = 19391188831.59414291\n",
      "Iteration 244, loss = 19391182157.03511429\n",
      "Iteration 245, loss = 19391175504.15319443\n",
      "Iteration 246, loss = 19391168844.67369843\n",
      "Iteration 247, loss = 19391162123.37895966\n",
      "Iteration 248, loss = 19391155532.96876907\n",
      "Iteration 249, loss = 19391148829.76543427\n",
      "Iteration 250, loss = 19391142151.69084930\n",
      "Iteration 251, loss = 19391135439.20913696\n",
      "Iteration 252, loss = 19391128792.90092087\n",
      "Iteration 253, loss = 19391122223.88468552\n",
      "Iteration 254, loss = 19391115502.33603287\n",
      "Iteration 255, loss = 19391108856.67749405\n",
      "Iteration 256, loss = 19391102218.57884598\n",
      "Iteration 257, loss = 19391095538.73121262\n",
      "Iteration 258, loss = 19391088876.12073898\n",
      "Iteration 259, loss = 19391082258.08215332\n",
      "Iteration 260, loss = 19391075589.54019547\n",
      "Iteration 261, loss = 19391068931.82060242\n",
      "Iteration 262, loss = 19391062300.76437378\n",
      "Iteration 263, loss = 19391055660.04146194\n",
      "Iteration 264, loss = 19391049049.68287277\n",
      "Iteration 265, loss = 19391042340.77851105\n",
      "Iteration 266, loss = 19391035700.19745255\n",
      "Iteration 267, loss = 19391029108.92815781\n",
      "Iteration 268, loss = 19391022401.74840546\n",
      "Iteration 269, loss = 19391015850.96651459\n",
      "Iteration 270, loss = 19391009162.93392944\n",
      "Iteration 271, loss = 19391002553.85499954\n",
      "Iteration 272, loss = 19390995908.74690628\n",
      "Iteration 273, loss = 19390989261.71899414\n",
      "Iteration 274, loss = 19390982654.59658432\n",
      "Iteration 275, loss = 19390975993.02167892\n",
      "Iteration 276, loss = 19390969372.71317291\n",
      "Iteration 277, loss = 19390962753.06454849\n",
      "Iteration 278, loss = 19390956128.49531174\n",
      "Iteration 279, loss = 19390949476.51217270\n",
      "Iteration 280, loss = 19390942883.49944305\n",
      "Iteration 281, loss = 19390936214.51018524\n",
      "Iteration 282, loss = 19390929655.53921127\n",
      "Iteration 283, loss = 19390923007.91564560\n",
      "Iteration 284, loss = 19390916371.52698135\n",
      "Iteration 285, loss = 19390909743.92050552\n",
      "Iteration 286, loss = 19390903151.47417831\n",
      "Iteration 287, loss = 19390896516.97533798\n",
      "Iteration 288, loss = 19390889903.52724838\n",
      "Iteration 289, loss = 19390883297.28660583\n",
      "Iteration 290, loss = 19390876677.97863388\n",
      "Iteration 291, loss = 19390870055.29291534\n",
      "Iteration 292, loss = 19390863443.20471573\n",
      "Iteration 293, loss = 19390856850.12788010\n",
      "Iteration 294, loss = 19390850219.77054977\n",
      "Iteration 295, loss = 19390843595.36109924\n",
      "Iteration 296, loss = 19390837006.36854553\n",
      "Iteration 297, loss = 19390830380.61766434\n",
      "Iteration 298, loss = 19390823795.08641815\n",
      "Iteration 299, loss = 19390817171.40010452\n",
      "Iteration 300, loss = 19390810535.48027802\n",
      "Iteration 301, loss = 19390803913.55863953\n",
      "Iteration 302, loss = 19390797336.99492264\n",
      "Iteration 303, loss = 19390790758.22355270\n",
      "Iteration 304, loss = 19390784120.92661667\n",
      "Iteration 305, loss = 19390777533.93441010\n",
      "Iteration 306, loss = 19390770924.57110596\n",
      "Iteration 307, loss = 19390764335.26398849\n",
      "Iteration 308, loss = 19390757719.72329712\n",
      "Iteration 309, loss = 19390751153.23072815\n",
      "Iteration 310, loss = 19390744522.61898422\n",
      "Iteration 311, loss = 19390737964.56829071\n",
      "Iteration 312, loss = 19390731350.20298004\n",
      "Iteration 313, loss = 19390724757.42557907\n",
      "Iteration 314, loss = 19390718173.46307373\n",
      "Iteration 315, loss = 19390711547.67802048\n",
      "Iteration 316, loss = 19390704989.99089813\n",
      "Iteration 317, loss = 19390698355.76966476\n",
      "Iteration 318, loss = 19390691780.53233337\n",
      "Iteration 319, loss = 19390685190.30134201\n",
      "Iteration 320, loss = 19390678633.88465118\n",
      "Iteration 321, loss = 19390671997.57975006\n",
      "Iteration 322, loss = 19390665447.75405502\n",
      "Iteration 323, loss = 19390658848.72136307\n",
      "Iteration 324, loss = 19390652278.68984604\n",
      "Iteration 325, loss = 19390645665.88724518\n",
      "Iteration 326, loss = 19390639058.43840790\n",
      "Iteration 327, loss = 19390632466.44322586\n",
      "Iteration 328, loss = 19390625929.95482254\n",
      "Iteration 329, loss = 19390619293.91318130\n",
      "Iteration 330, loss = 19390612738.28429413\n",
      "Iteration 331, loss = 19390606159.13324356\n",
      "Iteration 332, loss = 19390599565.82021713\n",
      "Iteration 333, loss = 19390592943.31401825\n",
      "Iteration 334, loss = 19390586429.42609024\n",
      "Iteration 335, loss = 19390579815.52462387\n",
      "Iteration 336, loss = 19390573230.10296249\n",
      "Iteration 337, loss = 19390566652.85275650\n",
      "Iteration 338, loss = 19390560103.75579834\n",
      "Iteration 339, loss = 19390553507.59642410\n",
      "Iteration 340, loss = 19390546907.98853302\n",
      "Iteration 341, loss = 19390540338.71118546\n",
      "Iteration 342, loss = 19390533742.21899414\n",
      "Iteration 343, loss = 19390527167.03530502\n",
      "Iteration 344, loss = 19390520560.59175491\n",
      "Iteration 345, loss = 19390513969.01840973\n",
      "Iteration 346, loss = 19390507415.01260757\n",
      "Iteration 347, loss = 19390500768.79443741\n",
      "Iteration 348, loss = 19390494182.30885696\n",
      "Iteration 349, loss = 19390487596.31987762\n",
      "Iteration 350, loss = 19390480892.29452133\n",
      "Iteration 351, loss = 19390474217.08553696\n",
      "Iteration 352, loss = 19390467495.66624451\n",
      "Iteration 353, loss = 19390460579.28438950\n",
      "Iteration 354, loss = 19390453469.46517563\n",
      "Iteration 355, loss = 19390445906.71233368\n",
      "Iteration 356, loss = 19390437945.68747330\n",
      "Iteration 357, loss = 19390429676.70090103\n",
      "Iteration 358, loss = 19390421410.75835419\n",
      "Iteration 359, loss = 19390413239.69253159\n",
      "Iteration 360, loss = 19390405146.97550583\n",
      "Iteration 361, loss = 19390397342.80317307\n",
      "Iteration 362, loss = 19390389565.93688583\n",
      "Iteration 363, loss = 19390381902.38101196\n",
      "Iteration 364, loss = 19390374348.03011322\n",
      "Iteration 365, loss = 19390366745.84343719\n",
      "Iteration 366, loss = 19390359347.21039581\n",
      "Iteration 367, loss = 19390351950.26877213\n",
      "Iteration 368, loss = 19390344535.50836563\n",
      "Iteration 369, loss = 19390337112.21282196\n",
      "Iteration 370, loss = 19390329836.99692154\n",
      "Iteration 371, loss = 19390322551.89711380\n",
      "Iteration 372, loss = 19390315248.71078873\n",
      "Iteration 373, loss = 19390308015.30538940\n",
      "Iteration 374, loss = 19390300757.68800354\n",
      "Iteration 375, loss = 19390293573.50255966\n",
      "Iteration 376, loss = 19390286357.65315247\n",
      "Iteration 377, loss = 19390279172.64773560\n",
      "Iteration 378, loss = 19390272018.62826920\n",
      "Iteration 379, loss = 19390264906.11938858\n",
      "Iteration 380, loss = 19390257721.49144363\n",
      "Iteration 381, loss = 19390250611.90615082\n",
      "Iteration 382, loss = 19390243476.52251816\n",
      "Iteration 383, loss = 19390236405.11029053\n",
      "Iteration 384, loss = 19390229322.10208893\n",
      "Iteration 385, loss = 19390222209.67504883\n",
      "Iteration 386, loss = 19390215148.46106720\n",
      "Iteration 387, loss = 19390208055.59682465\n",
      "Iteration 388, loss = 19390201048.50883484\n",
      "Iteration 389, loss = 19390193958.29867554\n",
      "Iteration 390, loss = 19390186927.76216125\n",
      "Iteration 391, loss = 19390179881.14350128\n",
      "Iteration 392, loss = 19390172889.85679626\n",
      "Iteration 393, loss = 19390165826.92971420\n",
      "Iteration 394, loss = 19390158804.10940170\n",
      "Iteration 395, loss = 19390151813.15634537\n",
      "Iteration 396, loss = 19390144844.22857666\n",
      "Iteration 397, loss = 19390137774.60836792\n",
      "Iteration 398, loss = 19390130849.75596237\n",
      "Iteration 399, loss = 19390123809.56068039\n",
      "Iteration 400, loss = 19390116862.00254059\n",
      "Iteration 401, loss = 19390109838.36983490\n",
      "Iteration 402, loss = 19390102895.26422119\n",
      "Iteration 403, loss = 19390095897.09854507\n",
      "Iteration 404, loss = 19390088962.87392807\n",
      "Iteration 405, loss = 19390082020.19201279\n",
      "Iteration 406, loss = 19390075043.93367386\n",
      "Iteration 407, loss = 19390068031.93589020\n",
      "Iteration 408, loss = 19390061140.00053024\n",
      "Iteration 409, loss = 19390054183.66614532\n",
      "Iteration 410, loss = 19390047225.63501740\n",
      "Iteration 411, loss = 19390040259.65662384\n",
      "Iteration 412, loss = 19390033399.60602570\n",
      "Iteration 413, loss = 19390026445.99555206\n",
      "Iteration 414, loss = 19390019450.74268341\n",
      "Iteration 415, loss = 19390012552.62421417\n",
      "Iteration 416, loss = 19390005647.93611908\n",
      "Iteration 417, loss = 19389998735.74129105\n",
      "Iteration 418, loss = 19389991793.38888550\n",
      "Iteration 419, loss = 19389984886.09965897\n",
      "Iteration 420, loss = 19389977943.28451920\n",
      "Iteration 421, loss = 19389971058.28130341\n",
      "Iteration 422, loss = 19389964132.83172989\n",
      "Iteration 423, loss = 19389957225.22436905\n",
      "Iteration 424, loss = 19389950351.34646988\n",
      "Iteration 425, loss = 19389943456.01615143\n",
      "Iteration 426, loss = 19389936522.69372559\n",
      "Iteration 427, loss = 19389929612.98664093\n",
      "Iteration 428, loss = 19389922763.35237885\n",
      "Iteration 429, loss = 19389915833.16934967\n",
      "Iteration 430, loss = 19389908943.85261917\n",
      "Iteration 431, loss = 19389902088.11809158\n",
      "Iteration 432, loss = 19389895229.32085419\n",
      "Iteration 433, loss = 19389888284.32069778\n",
      "Iteration 434, loss = 19389881389.91139984\n",
      "Iteration 435, loss = 19389874543.13264084\n",
      "Iteration 436, loss = 19389867675.89455032\n",
      "Iteration 437, loss = 19389860821.93569183\n",
      "Iteration 438, loss = 19389853935.64754486\n",
      "Iteration 439, loss = 19389847058.39885330\n",
      "Iteration 440, loss = 19389840179.77677155\n",
      "Iteration 441, loss = 19389833329.16809845\n",
      "Iteration 442, loss = 19389826461.05095673\n",
      "Iteration 443, loss = 19389819596.24780655\n",
      "Iteration 444, loss = 19389812714.38971710\n",
      "Iteration 445, loss = 19389805881.68056107\n",
      "Iteration 446, loss = 19389799056.87123108\n",
      "Iteration 447, loss = 19389792170.36713409\n",
      "Iteration 448, loss = 19389785317.63892746\n",
      "Iteration 449, loss = 19389778433.28731537\n",
      "Iteration 450, loss = 19389771619.47705460\n",
      "Iteration 451, loss = 19389764760.64112854\n",
      "Iteration 452, loss = 19389757901.75852203\n",
      "Iteration 453, loss = 19389751060.43257904\n",
      "Iteration 454, loss = 19389744232.73022079\n",
      "Iteration 455, loss = 19389737375.71574783\n",
      "Iteration 456, loss = 19389730518.52940369\n",
      "Iteration 457, loss = 19389723724.84318924\n",
      "Iteration 458, loss = 19389716878.46527100\n",
      "Iteration 459, loss = 19389710000.54929352\n",
      "Iteration 460, loss = 19389703165.93447876\n",
      "Iteration 461, loss = 19389696340.55949402\n",
      "Iteration 462, loss = 19389689490.74824142\n",
      "Iteration 463, loss = 19389682667.41925430\n",
      "Iteration 464, loss = 19389675816.57984924\n",
      "Iteration 465, loss = 19389669025.12533188\n",
      "Iteration 466, loss = 19389662143.87236404\n",
      "Iteration 467, loss = 19389655287.29613876\n",
      "Iteration 468, loss = 19389648432.73642349\n",
      "Iteration 469, loss = 19389641561.77070999\n",
      "Iteration 470, loss = 19389634693.19010162\n",
      "Iteration 471, loss = 19389627719.05810165\n",
      "Iteration 472, loss = 19389620654.13320541\n",
      "Iteration 473, loss = 19389613457.00807190\n",
      "Iteration 474, loss = 19389605846.03078079\n",
      "Iteration 475, loss = 19389597590.57840729\n",
      "Iteration 476, loss = 19389588660.06109619\n",
      "Iteration 477, loss = 19389579550.91830826\n",
      "Iteration 478, loss = 19389570525.56406021\n",
      "Iteration 479, loss = 19389561907.33469009\n",
      "Iteration 480, loss = 19389553439.60316849\n",
      "Iteration 481, loss = 19389545301.09610367\n",
      "Iteration 482, loss = 19389537248.26940155\n",
      "Iteration 483, loss = 19389529267.22776794\n",
      "Iteration 484, loss = 19389521386.04452515\n",
      "Iteration 485, loss = 19389513553.36022568\n",
      "Iteration 486, loss = 19389505840.58908081\n",
      "Iteration 487, loss = 19389498139.39703369\n",
      "Iteration 488, loss = 19389490436.87116241\n",
      "Iteration 489, loss = 19389482862.71317673\n",
      "Iteration 490, loss = 19389475273.81546402\n",
      "Iteration 491, loss = 19389467688.72133255\n",
      "Iteration 492, loss = 19389460154.44944763\n",
      "Iteration 493, loss = 19389452622.34024811\n",
      "Iteration 494, loss = 19389445172.95940399\n",
      "Iteration 495, loss = 19389437689.09159470\n",
      "Iteration 496, loss = 19389430267.98575211\n",
      "Iteration 497, loss = 19389422754.97694778\n",
      "Iteration 498, loss = 19389415366.29296875\n",
      "Iteration 499, loss = 19389407959.87596130\n",
      "Iteration 500, loss = 19389400553.91514587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 17987868708.80720520\n",
      "Iteration 2, loss = 17987865341.15207672\n",
      "Iteration 3, loss = 17987861975.84090042\n",
      "Iteration 4, loss = 17987858613.01008606\n",
      "Iteration 5, loss = 17987855252.79813004\n",
      "Iteration 6, loss = 17987851895.30023575\n",
      "Iteration 7, loss = 17987848540.53878403\n",
      "Iteration 8, loss = 17987845188.46746063\n",
      "Iteration 9, loss = 17987841838.99529266\n",
      "Iteration 10, loss = 17987838492.00408936\n",
      "Iteration 11, loss = 17987835147.35228348\n",
      "Iteration 12, loss = 17987831804.87564468\n",
      "Iteration 13, loss = 17987828464.39094925\n",
      "Iteration 14, loss = 17987825125.70275116\n",
      "Iteration 15, loss = 17987821788.61162567\n",
      "Iteration 16, loss = 17987818452.92202759\n",
      "Iteration 17, loss = 17987815118.44860458\n",
      "Iteration 18, loss = 17987811785.02089691\n",
      "Iteration 19, loss = 17987808452.48631287\n",
      "Iteration 20, loss = 17987805120.71129608\n",
      "Iteration 21, loss = 17987801789.58093643\n",
      "Iteration 22, loss = 17987798458.99736786\n",
      "Iteration 23, loss = 17987795128.87774658\n",
      "Iteration 24, loss = 17987791799.15212250\n",
      "Iteration 25, loss = 17987788469.76164246\n",
      "Iteration 26, loss = 17987785140.65705109\n",
      "Iteration 27, loss = 17987781811.79761887\n",
      "Iteration 28, loss = 17987778483.15038300\n",
      "Iteration 29, loss = 17987775154.68972397\n",
      "Iteration 30, loss = 17987771826.39721298\n",
      "Iteration 31, loss = 17987768498.26171112\n",
      "Iteration 32, loss = 17987765170.27963638\n",
      "Iteration 33, loss = 17987761842.45541382\n",
      "Iteration 34, loss = 17987758514.80202484\n",
      "Iteration 35, loss = 17987755187.34164810\n",
      "Iteration 36, loss = 17987751860.10637665\n",
      "Iteration 37, loss = 17987748533.13889313\n",
      "Iteration 38, loss = 17987745206.49319458\n",
      "Iteration 39, loss = 17987741880.23516083\n",
      "Iteration 40, loss = 17987738554.44314194\n",
      "Iteration 41, loss = 17987735229.20829010\n",
      "Iteration 42, loss = 17987731904.63486862\n",
      "Iteration 43, loss = 17987728580.84034348\n",
      "Iteration 44, loss = 17987725257.95531082\n",
      "Iteration 45, loss = 17987721936.12332916\n",
      "Iteration 46, loss = 17987718615.50054550\n",
      "Iteration 47, loss = 17987715296.25524902\n",
      "Iteration 48, loss = 17987711978.56732178\n",
      "Iteration 49, loss = 17987708662.62757874\n",
      "Iteration 50, loss = 17987705348.63707352\n",
      "Iteration 51, loss = 17987702036.80639267\n",
      "Iteration 52, loss = 17987698727.35484314\n",
      "Iteration 53, loss = 17987695420.50971603\n",
      "Iteration 54, loss = 17987692116.50553894\n",
      "Iteration 55, loss = 17987688815.58329391\n",
      "Iteration 56, loss = 17987685517.98973083\n",
      "Iteration 57, loss = 17987682223.97665787\n",
      "Iteration 58, loss = 17987678933.80027771\n",
      "Iteration 59, loss = 17987675647.72058105\n",
      "Iteration 60, loss = 17987672366.00075150\n",
      "Iteration 61, loss = 17987669088.90662384\n",
      "Iteration 62, loss = 17987665816.70618439\n",
      "Iteration 63, loss = 17987662549.66911697\n",
      "Iteration 64, loss = 17987659288.06641006\n",
      "Iteration 65, loss = 17987656032.16996002\n",
      "Iteration 66, loss = 17987652782.25228500\n",
      "Iteration 67, loss = 17987649538.58623123\n",
      "Iteration 68, loss = 17987646301.44473648\n",
      "Iteration 69, loss = 17987643071.10058594\n",
      "Iteration 70, loss = 17987639847.82627869\n",
      "Iteration 71, loss = 17987636631.89385605\n",
      "Iteration 72, loss = 17987633423.57476807\n",
      "Iteration 73, loss = 17987630223.13975525\n",
      "Iteration 74, loss = 17987627030.85876846\n",
      "Iteration 75, loss = 17987623847.00086975\n",
      "Iteration 76, loss = 17987620671.83414841\n",
      "Iteration 77, loss = 17987617505.62565994\n",
      "Iteration 78, loss = 17987614348.64134598\n",
      "Iteration 79, loss = 17987611201.14594269\n",
      "Iteration 80, loss = 17987608063.40293884\n",
      "Iteration 81, loss = 17987604935.67449188\n",
      "Iteration 82, loss = 17987601818.22136688\n",
      "Iteration 83, loss = 17987598711.30285263\n",
      "Iteration 84, loss = 17987595615.17672348\n",
      "Iteration 85, loss = 17987592530.09915543\n",
      "Iteration 86, loss = 17987589456.32465744\n",
      "Iteration 87, loss = 17987586394.10601425\n",
      "Iteration 88, loss = 17987583343.69423294\n",
      "Iteration 89, loss = 17987580305.33845520\n",
      "Iteration 90, loss = 17987577279.28592300\n",
      "Iteration 91, loss = 17987574265.78191376\n",
      "Iteration 92, loss = 17987571265.06967545\n",
      "Iteration 93, loss = 17987568277.39036942\n",
      "Iteration 94, loss = 17987565302.98299026\n",
      "Iteration 95, loss = 17987562342.08431244\n",
      "Iteration 96, loss = 17987559394.92880630\n",
      "Iteration 97, loss = 17987556461.74851608\n",
      "Iteration 98, loss = 17987553542.77296829\n",
      "Iteration 99, loss = 17987550638.22902679\n",
      "Iteration 100, loss = 17987547748.34071732\n",
      "Iteration 101, loss = 17987544873.32909393\n",
      "Iteration 102, loss = 17987542013.41199112\n",
      "Iteration 103, loss = 17987539168.80379868\n",
      "Iteration 104, loss = 17987536339.71521378\n",
      "Iteration 105, loss = 17987533526.35295105\n",
      "Iteration 106, loss = 17987530728.91942215\n",
      "Iteration 107, loss = 17987527947.61241150\n",
      "Iteration 108, loss = 17987525182.62468719\n",
      "Iteration 109, loss = 17987522434.14365387\n",
      "Iteration 110, loss = 17987519702.35089874\n",
      "Iteration 111, loss = 17987516987.42181778\n",
      "Iteration 112, loss = 17987514289.52516937\n",
      "Iteration 113, loss = 17987511608.82261658\n",
      "Iteration 114, loss = 17987508945.46832275\n",
      "Iteration 115, loss = 17987506299.60848618\n",
      "Iteration 116, loss = 17987503671.38093185\n",
      "Iteration 117, loss = 17987501060.91466904\n",
      "Iteration 118, loss = 17987498468.32954025\n",
      "Iteration 119, loss = 17987495893.73579407\n",
      "Iteration 120, loss = 17987493337.23373413\n",
      "Iteration 121, loss = 17987490798.91344452\n",
      "Iteration 122, loss = 17987488278.85444641\n",
      "Iteration 123, loss = 17987485777.12544632\n",
      "Iteration 124, loss = 17987483293.78409958\n",
      "Iteration 125, loss = 17987480828.87682724\n",
      "Iteration 126, loss = 17987478382.43860245\n",
      "Iteration 127, loss = 17987475954.49286270\n",
      "Iteration 128, loss = 17987473545.05137253\n",
      "Iteration 129, loss = 17987471154.11411667\n",
      "Iteration 130, loss = 17987468781.66931152\n",
      "Iteration 131, loss = 17987466427.69332504\n",
      "Iteration 132, loss = 17987464092.15075302\n",
      "Iteration 133, loss = 17987461774.99440002\n",
      "Iteration 134, loss = 17987459476.16540909\n",
      "Iteration 135, loss = 17987457195.59336090\n",
      "Iteration 136, loss = 17987454933.19644165\n",
      "Iteration 137, loss = 17987452688.88162613\n",
      "Iteration 138, loss = 17987450462.54492950\n",
      "Iteration 139, loss = 17987448254.07173157\n",
      "Iteration 140, loss = 17987446063.33706284\n",
      "Iteration 141, loss = 17987443890.20603561\n",
      "Iteration 142, loss = 17987441734.53430939\n",
      "Iteration 143, loss = 17987439596.16850281\n",
      "Iteration 144, loss = 17987437474.94683838\n",
      "Iteration 145, loss = 17987435370.69968033\n",
      "Iteration 146, loss = 17987433283.25018692\n",
      "Iteration 147, loss = 17987431212.41496277\n",
      "Iteration 148, loss = 17987429158.00479507\n",
      "Iteration 149, loss = 17987427119.82538605\n",
      "Iteration 150, loss = 17987425097.67807388\n",
      "Iteration 151, loss = 17987423091.36061859\n",
      "Iteration 152, loss = 17987421100.66793823\n",
      "Iteration 153, loss = 17987419125.39289093\n",
      "Iteration 154, loss = 17987417165.32700729\n",
      "Iteration 155, loss = 17987415220.26117706\n",
      "Iteration 156, loss = 17987413289.98634338\n",
      "Iteration 157, loss = 17987411374.29415131\n",
      "Iteration 158, loss = 17987409472.97753143\n",
      "Iteration 159, loss = 17987407585.83125687\n",
      "Iteration 160, loss = 17987405712.65241623\n",
      "Iteration 161, loss = 17987403853.24085999\n",
      "Iteration 162, loss = 17987402007.39956665\n",
      "Iteration 163, loss = 17987400174.93494415\n",
      "Iteration 164, loss = 17987398355.65708923\n",
      "Iteration 165, loss = 17987396549.37996292\n",
      "Iteration 166, loss = 17987394755.92152786\n",
      "Iteration 167, loss = 17987392975.10382080\n",
      "Iteration 168, loss = 17987391206.75294495\n",
      "Iteration 169, loss = 17987389450.69908142\n",
      "Iteration 170, loss = 17987387706.77635956\n",
      "Iteration 171, loss = 17987385974.82279968\n",
      "Iteration 172, loss = 17987384254.68012238\n",
      "Iteration 173, loss = 17987382546.19360352\n",
      "Iteration 174, loss = 17987380849.21187973\n",
      "Iteration 175, loss = 17987379163.58671570\n",
      "Iteration 176, loss = 17987377489.17285919\n",
      "Iteration 177, loss = 17987375825.82776642\n",
      "Iteration 178, loss = 17987374173.41144943\n",
      "Iteration 179, loss = 17987372531.78624344\n",
      "Iteration 180, loss = 17987370900.81668091\n",
      "Iteration 181, loss = 17987369280.36929321\n",
      "Iteration 182, loss = 17987367670.31250381\n",
      "Iteration 183, loss = 17987366070.51648331\n",
      "Iteration 184, loss = 17987364480.85306549\n",
      "Iteration 185, loss = 17987362901.19569397\n",
      "Iteration 186, loss = 17987361331.41933060\n",
      "Iteration 187, loss = 17987359771.40045166\n",
      "Iteration 188, loss = 17987358221.01700974\n",
      "Iteration 189, loss = 17987356680.14842606\n",
      "Iteration 190, loss = 17987355148.67558670\n",
      "Iteration 191, loss = 17987353626.48084641\n",
      "Iteration 192, loss = 17987352113.44805527\n",
      "Iteration 193, loss = 17987350609.46254349\n",
      "Iteration 194, loss = 17987349114.41112900\n",
      "Iteration 195, loss = 17987347628.18213272\n",
      "Iteration 196, loss = 17987346150.66538239\n",
      "Iteration 197, loss = 17987344681.75217819\n",
      "Iteration 198, loss = 17987343221.33529282\n",
      "Iteration 199, loss = 17987341769.30893707\n",
      "Iteration 200, loss = 17987340325.56874466\n",
      "Iteration 201, loss = 17987338890.01171494\n",
      "Iteration 202, loss = 17987337462.53618240\n",
      "Iteration 203, loss = 17987336043.04178238\n",
      "Iteration 204, loss = 17987334631.42935562\n",
      "Iteration 205, loss = 17987333227.60093307\n",
      "Iteration 206, loss = 17987331831.45967865\n",
      "Iteration 207, loss = 17987330442.90982437\n",
      "Iteration 208, loss = 17987329061.85660934\n",
      "Iteration 209, loss = 17987327688.20624924\n",
      "Iteration 210, loss = 17987326321.86588287\n",
      "Iteration 211, loss = 17987324962.74353027\n",
      "Iteration 212, loss = 17987323610.74805069\n",
      "Iteration 213, loss = 17987322265.78911209\n",
      "Iteration 214, loss = 17987320927.77717209\n",
      "Iteration 215, loss = 17987319596.62344360\n",
      "Iteration 216, loss = 17987318272.23990631\n",
      "Iteration 217, loss = 17987316954.53925705\n",
      "Iteration 218, loss = 17987315643.43494797\n",
      "Iteration 219, loss = 17987314338.84115982\n",
      "Iteration 220, loss = 17987313040.67280579\n",
      "Iteration 221, loss = 17987311748.84555054\n",
      "Iteration 222, loss = 17987310463.27582169\n",
      "Iteration 223, loss = 17987309183.88082123\n",
      "Iteration 224, loss = 17987307910.57853699\n",
      "Iteration 225, loss = 17987306643.28778076\n",
      "Iteration 226, loss = 17987305381.92817307\n",
      "Iteration 227, loss = 17987304126.42020798\n",
      "Iteration 228, loss = 17987302876.68524551\n",
      "Iteration 229, loss = 17987301632.64555359\n",
      "Iteration 230, loss = 17987300394.22430420\n",
      "Iteration 231, loss = 17987299161.34560394\n",
      "Iteration 232, loss = 17987297933.93450165\n",
      "Iteration 233, loss = 17987296711.91700363\n",
      "Iteration 234, loss = 17987295495.22006226\n",
      "Iteration 235, loss = 17987294283.77156830\n",
      "Iteration 236, loss = 17987293077.50035858\n",
      "Iteration 237, loss = 17987291876.33617401\n",
      "Iteration 238, loss = 17987290680.20964813\n",
      "Iteration 239, loss = 17987289489.05226898\n",
      "Iteration 240, loss = 17987288302.79634476\n",
      "Iteration 241, loss = 17987287121.37495422\n",
      "Iteration 242, loss = 17987285944.72192764\n",
      "Iteration 243, loss = 17987284772.77177048\n",
      "Iteration 244, loss = 17987283605.45961380\n",
      "Iteration 245, loss = 17987282442.72123337\n",
      "Iteration 246, loss = 17987281284.49291992\n",
      "Iteration 247, loss = 17987280130.71152115\n",
      "Iteration 248, loss = 17987278981.31436920\n",
      "Iteration 249, loss = 17987277836.23927689\n",
      "Iteration 250, loss = 17987276695.42451477\n",
      "Iteration 251, loss = 17987275558.80881882\n",
      "Iteration 252, loss = 17987274426.33139420\n",
      "Iteration 253, loss = 17987273297.93190002\n",
      "Iteration 254, loss = 17987272173.55051041\n",
      "Iteration 255, loss = 17987271053.12791824\n",
      "Iteration 256, loss = 17987269936.60535049\n",
      "Iteration 257, loss = 17987268823.92463684\n",
      "Iteration 258, loss = 17987267715.02820969\n",
      "Iteration 259, loss = 17987266609.85920334\n",
      "Iteration 260, loss = 17987265508.36143494\n",
      "Iteration 261, loss = 17987264410.47946930\n",
      "Iteration 262, loss = 17987263316.15866852\n",
      "Iteration 263, loss = 17987262225.34520340\n",
      "Iteration 264, loss = 17987261137.98608780\n",
      "Iteration 265, loss = 17987260054.02921295\n",
      "Iteration 266, loss = 17987258973.42336273\n",
      "Iteration 267, loss = 17987257896.11820221\n",
      "Iteration 268, loss = 17987256822.06431580\n",
      "Iteration 269, loss = 17987255751.21317673\n",
      "Iteration 270, loss = 17987254683.51716232\n",
      "Iteration 271, loss = 17987253618.92948914\n",
      "Iteration 272, loss = 17987252557.40426636\n",
      "Iteration 273, loss = 17987251498.89641571\n",
      "Iteration 274, loss = 17987250443.36164474\n",
      "Iteration 275, loss = 17987249390.75643539\n",
      "Iteration 276, loss = 17987248341.03799438\n",
      "Iteration 277, loss = 17987247294.16421890\n",
      "Iteration 278, loss = 17987246250.09365082\n",
      "Iteration 279, loss = 17987245208.78546143\n",
      "Iteration 280, loss = 17987244170.19939804\n",
      "Iteration 281, loss = 17987243134.29575729\n",
      "Iteration 282, loss = 17987242101.03535461\n",
      "Iteration 283, loss = 17987241070.37950516\n",
      "Iteration 284, loss = 17987240042.28998184\n",
      "Iteration 285, loss = 17987239016.72903061\n",
      "Iteration 286, loss = 17987237993.65931702\n",
      "Iteration 287, loss = 17987236973.04396057\n",
      "Iteration 288, loss = 17987235954.84648514\n",
      "Iteration 289, loss = 17987234939.03084946\n",
      "Iteration 290, loss = 17987233925.56144333\n",
      "Iteration 291, loss = 17987232914.40307617\n",
      "Iteration 292, loss = 17987231905.52100754\n",
      "Iteration 293, loss = 17987230898.88093567\n",
      "Iteration 294, loss = 17987229894.44901657\n",
      "Iteration 295, loss = 17987228892.19189072\n",
      "Iteration 296, loss = 17987227892.07666397\n",
      "Iteration 297, loss = 17987226894.07095337\n",
      "Iteration 298, loss = 17987225898.14286804\n",
      "Iteration 299, loss = 17987224904.26104355\n",
      "Iteration 300, loss = 17987223912.39464569\n",
      "Iteration 301, loss = 17987222922.51337051\n",
      "Iteration 302, loss = 17987221934.58746338\n",
      "Iteration 303, loss = 17987220948.58771133\n",
      "Iteration 304, loss = 17987219964.48547363\n",
      "Iteration 305, loss = 17987218982.25264740\n",
      "Iteration 306, loss = 17987218001.86169434\n",
      "Iteration 307, loss = 17987217023.28563309\n",
      "Iteration 308, loss = 17987216046.49802780\n",
      "Iteration 309, loss = 17987215071.47300339\n",
      "Iteration 310, loss = 17987214098.18522263\n",
      "Iteration 311, loss = 17987213126.60985184\n",
      "Iteration 312, loss = 17987212156.72262192\n",
      "Iteration 313, loss = 17987211188.49975967\n",
      "Iteration 314, loss = 17987210221.91797256\n",
      "Iteration 315, loss = 17987209256.95449448\n",
      "Iteration 316, loss = 17987208293.58699799\n",
      "Iteration 317, loss = 17987207331.79363251\n",
      "Iteration 318, loss = 17987206371.55298615\n",
      "Iteration 319, loss = 17987205412.84408188\n",
      "Iteration 320, loss = 17987204455.64633179\n",
      "Iteration 321, loss = 17987203499.93957520\n",
      "Iteration 322, loss = 17987202545.70401001\n",
      "Iteration 323, loss = 17987201592.92020035\n",
      "Iteration 324, loss = 17987200641.56906509\n",
      "Iteration 325, loss = 17987199691.63184738\n",
      "Iteration 326, loss = 17987198743.09011459\n",
      "Iteration 327, loss = 17987197795.92572021\n",
      "Iteration 328, loss = 17987196850.12083054\n",
      "Iteration 329, loss = 17987195905.65786743\n",
      "Iteration 330, loss = 17987194962.51950455\n",
      "Iteration 331, loss = 17987194020.68868256\n",
      "Iteration 332, loss = 17987193080.14855957\n",
      "Iteration 333, loss = 17987192140.88253784\n",
      "Iteration 334, loss = 17987191202.87420654\n",
      "Iteration 335, loss = 17987190266.10737991\n",
      "Iteration 336, loss = 17987189330.56605148\n",
      "Iteration 337, loss = 17987188396.23439407\n",
      "Iteration 338, loss = 17987187463.09678268\n",
      "Iteration 339, loss = 17987186531.13771439\n",
      "Iteration 340, loss = 17987185600.34187698\n",
      "Iteration 341, loss = 17987184670.69410324\n",
      "Iteration 342, loss = 17987183742.17936707\n",
      "Iteration 343, loss = 17987182814.78277969\n",
      "Iteration 344, loss = 17987181888.48958206\n",
      "Iteration 345, loss = 17987180963.28514862\n",
      "Iteration 346, loss = 17987180039.15494537\n",
      "Iteration 347, loss = 17987179116.08459473\n",
      "Iteration 348, loss = 17987178194.05979156\n",
      "Iteration 349, loss = 17987177273.06633377\n",
      "Iteration 350, loss = 17987176353.09012985\n",
      "Iteration 351, loss = 17987175434.11717606\n",
      "Iteration 352, loss = 17987174516.13353729\n",
      "Iteration 353, loss = 17987173599.12536621\n",
      "Iteration 354, loss = 17987172683.07888031\n",
      "Iteration 355, loss = 17987171767.98038101\n",
      "Iteration 356, loss = 17987170853.81620026\n",
      "Iteration 357, loss = 17987169940.57274246\n",
      "Iteration 358, loss = 17987169028.23645020\n",
      "Iteration 359, loss = 17987168116.79380417\n",
      "Iteration 360, loss = 17987167206.23130798\n",
      "Iteration 361, loss = 17987166296.53550720\n",
      "Iteration 362, loss = 17987165387.69294739\n",
      "Iteration 363, loss = 17987164479.69018173\n",
      "Iteration 364, loss = 17987163572.51377106\n",
      "Iteration 365, loss = 17987162666.15024567\n",
      "Iteration 366, loss = 17987161760.58614349\n",
      "Iteration 367, loss = 17987160855.80794525\n",
      "Iteration 368, loss = 17987159951.80210876\n",
      "Iteration 369, loss = 17987159048.55502701\n",
      "Iteration 370, loss = 17987158146.05304718\n",
      "Iteration 371, loss = 17987157244.28241730\n",
      "Iteration 372, loss = 17987156343.22932434\n",
      "Iteration 373, loss = 17987155442.87984848\n",
      "Iteration 374, loss = 17987154543.21994019\n",
      "Iteration 375, loss = 17987153644.23544312\n",
      "Iteration 376, loss = 17987152745.91205215\n",
      "Iteration 377, loss = 17987151848.23530197\n",
      "Iteration 378, loss = 17987150951.19055939\n",
      "Iteration 379, loss = 17987150054.76299667\n",
      "Iteration 380, loss = 17987149158.93757248\n",
      "Iteration 381, loss = 17987148263.69903946\n",
      "Iteration 382, loss = 17987147369.03187180\n",
      "Iteration 383, loss = 17987146474.92031479\n",
      "Iteration 384, loss = 17987145581.34829712\n",
      "Iteration 385, loss = 17987144688.29944611\n",
      "Iteration 386, loss = 17987143795.75706863\n",
      "Iteration 387, loss = 17987142903.70409393\n",
      "Iteration 388, loss = 17987142012.12308121\n",
      "Iteration 389, loss = 17987141120.99617386\n",
      "Iteration 390, loss = 17987140230.30508804\n",
      "Iteration 391, loss = 17987139340.03106308\n",
      "Iteration 392, loss = 17987138450.15484619\n",
      "Iteration 393, loss = 17987137560.65665054\n",
      "Iteration 394, loss = 17987136671.51613235\n",
      "Iteration 395, loss = 17987135782.71234894\n",
      "Iteration 396, loss = 17987134894.22372818\n",
      "Iteration 397, loss = 17987134006.02803040\n",
      "Iteration 398, loss = 17987133118.10230637\n",
      "Iteration 399, loss = 17987132230.42284775\n",
      "Iteration 400, loss = 17987131342.96518326\n",
      "Iteration 401, loss = 17987130455.70398712\n",
      "Iteration 402, loss = 17987129568.61307526\n",
      "Iteration 403, loss = 17987128681.66532516\n",
      "Iteration 404, loss = 17987127794.83266068\n",
      "Iteration 405, loss = 17987126908.08600235\n",
      "Iteration 406, loss = 17987126021.39518738\n",
      "Iteration 407, loss = 17987125134.72896957\n",
      "Iteration 408, loss = 17987124248.05493164\n",
      "Iteration 409, loss = 17987123361.33945847\n",
      "Iteration 410, loss = 17987122474.54769516\n",
      "Iteration 411, loss = 17987121587.64347839\n",
      "Iteration 412, loss = 17987120700.58930969\n",
      "Iteration 413, loss = 17987119813.34630966\n",
      "Iteration 414, loss = 17987118925.87417221\n",
      "Iteration 415, loss = 17987118038.13113785\n",
      "Iteration 416, loss = 17987117150.07395554\n",
      "Iteration 417, loss = 17987116261.65786362\n",
      "Iteration 418, loss = 17987115372.83655167\n",
      "Iteration 419, loss = 17987114483.56216049\n",
      "Iteration 420, loss = 17987113593.78530121\n",
      "Iteration 421, loss = 17987112703.45500565\n",
      "Iteration 422, loss = 17987111812.51879501\n",
      "Iteration 423, loss = 17987110920.92269135\n",
      "Iteration 424, loss = 17987110028.61125183\n",
      "Iteration 425, loss = 17987109135.52766037\n",
      "Iteration 426, loss = 17987108241.61375809\n",
      "Iteration 427, loss = 17987107346.81019974\n",
      "Iteration 428, loss = 17987106451.05651093\n",
      "Iteration 429, loss = 17987105554.29129791\n",
      "Iteration 430, loss = 17987104656.45235825\n",
      "Iteration 431, loss = 17987103757.47688675\n",
      "Iteration 432, loss = 17987102857.30170822\n",
      "Iteration 433, loss = 17987101955.86351013\n",
      "Iteration 434, loss = 17987101053.09910965\n",
      "Iteration 435, loss = 17987100148.94575500\n",
      "Iteration 436, loss = 17987099243.34146881\n",
      "Iteration 437, loss = 17987098336.22533798\n",
      "Iteration 438, loss = 17987097427.53796387\n",
      "Iteration 439, loss = 17987096517.22178268\n",
      "Iteration 440, loss = 17987095605.22150040\n",
      "Iteration 441, loss = 17987094691.48451996\n",
      "Iteration 442, loss = 17987093775.96133041\n",
      "Iteration 443, loss = 17987092858.60595703\n",
      "Iteration 444, loss = 17987091939.37636948\n",
      "Iteration 445, loss = 17987091018.23484039\n",
      "Iteration 446, loss = 17987090095.14837646\n",
      "Iteration 447, loss = 17987089170.08902740\n",
      "Iteration 448, loss = 17987088243.03420639\n",
      "Iteration 449, loss = 17987087313.96692276\n",
      "Iteration 450, loss = 17987086382.87598038\n",
      "Iteration 451, loss = 17987085449.75614548\n",
      "Iteration 452, loss = 17987084514.60815430\n",
      "Iteration 453, loss = 17987083577.43874741\n",
      "Iteration 454, loss = 17987082638.26054001\n",
      "Iteration 455, loss = 17987081697.09186554\n",
      "Iteration 456, loss = 17987080753.95649338\n",
      "Iteration 457, loss = 17987079808.88331985\n",
      "Iteration 458, loss = 17987078861.90589142\n",
      "Iteration 459, loss = 17987077913.06197739\n",
      "Iteration 460, loss = 17987076962.39292145\n",
      "Iteration 461, loss = 17987076009.94309235\n",
      "Iteration 462, loss = 17987075055.75914764\n",
      "Iteration 463, loss = 17987074099.88932037\n",
      "Iteration 464, loss = 17987073142.38270950\n",
      "Iteration 465, loss = 17987072183.28845215\n",
      "Iteration 466, loss = 17987071222.65499115\n",
      "Iteration 467, loss = 17987070260.52928925\n",
      "Iteration 468, loss = 17987069296.95605087\n",
      "Iteration 469, loss = 17987068331.97701263\n",
      "Iteration 470, loss = 17987067365.63028336\n",
      "Iteration 471, loss = 17987066397.94959641\n",
      "Iteration 472, loss = 17987065428.96382523\n",
      "Iteration 473, loss = 17987064458.69636917\n",
      "Iteration 474, loss = 17987063487.16473389\n",
      "Iteration 475, loss = 17987062514.38008118\n",
      "Iteration 476, loss = 17987061540.34696960\n",
      "Iteration 477, loss = 17987060565.06304932\n",
      "Iteration 478, loss = 17987059588.51894760\n",
      "Iteration 479, loss = 17987058610.69815826\n",
      "Iteration 480, loss = 17987057631.57708740\n",
      "Iteration 481, loss = 17987056651.12514114\n",
      "Iteration 482, loss = 17987055669.30490875\n",
      "Iteration 483, loss = 17987054686.07250595\n",
      "Iteration 484, loss = 17987053701.37790298\n",
      "Iteration 485, loss = 17987052715.16547012\n",
      "Iteration 486, loss = 17987051727.37453842\n",
      "Iteration 487, loss = 17987050737.94012833\n",
      "Iteration 488, loss = 17987049746.79373169\n",
      "Iteration 489, loss = 17987048753.86421204\n",
      "Iteration 490, loss = 17987047759.07883835\n",
      "Iteration 491, loss = 17987046762.36434937\n",
      "Iteration 492, loss = 17987045763.64817429\n",
      "Iteration 493, loss = 17987044762.85966110\n",
      "Iteration 494, loss = 17987043759.93143845\n",
      "Iteration 495, loss = 17987042754.80078506\n",
      "Iteration 496, loss = 17987041747.41099930\n",
      "Iteration 497, loss = 17987040737.71286011\n",
      "Iteration 498, loss = 17987039725.66594315\n",
      "Iteration 499, loss = 17987038711.24000168\n",
      "Iteration 500, loss = 17987037694.41615677\n",
      "Iteration 1, loss = 19463707801.80242157\n",
      "Iteration 2, loss = 19463700799.32752228\n",
      "Iteration 3, loss = 19463693771.97151566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 19463686797.30218887\n",
      "Iteration 5, loss = 19463679830.34437561\n",
      "Iteration 6, loss = 19463672784.42751312\n",
      "Iteration 7, loss = 19463665761.31899643\n",
      "Iteration 8, loss = 19463658723.68129730\n",
      "Iteration 9, loss = 19463651686.08979416\n",
      "Iteration 10, loss = 19463644641.75848770\n",
      "Iteration 11, loss = 19463637538.31682205\n",
      "Iteration 12, loss = 19463630399.84965897\n",
      "Iteration 13, loss = 19463623349.15642548\n",
      "Iteration 14, loss = 19463616153.75099182\n",
      "Iteration 15, loss = 19463609001.52485275\n",
      "Iteration 16, loss = 19463601843.47927856\n",
      "Iteration 17, loss = 19463594589.24472046\n",
      "Iteration 18, loss = 19463587368.58481598\n",
      "Iteration 19, loss = 19463580052.16106033\n",
      "Iteration 20, loss = 19463572773.74872589\n",
      "Iteration 21, loss = 19463565369.89645386\n",
      "Iteration 22, loss = 19463558054.38664246\n",
      "Iteration 23, loss = 19463550696.93200684\n",
      "Iteration 24, loss = 19463543245.68175507\n",
      "Iteration 25, loss = 19463535853.69273758\n",
      "Iteration 26, loss = 19463528427.17734909\n",
      "Iteration 27, loss = 19463520997.24376297\n",
      "Iteration 28, loss = 19463513497.12847137\n",
      "Iteration 29, loss = 19463506017.68744278\n",
      "Iteration 30, loss = 19463498431.39929962\n",
      "Iteration 31, loss = 19463490815.94143295\n",
      "Iteration 32, loss = 19463483207.15910339\n",
      "Iteration 33, loss = 19463475524.79393768\n",
      "Iteration 34, loss = 19463467752.66012192\n",
      "Iteration 35, loss = 19463459978.64684296\n",
      "Iteration 36, loss = 19463452175.16214752\n",
      "Iteration 37, loss = 19463444272.26597595\n",
      "Iteration 38, loss = 19463436438.42578125\n",
      "Iteration 39, loss = 19463428501.42733383\n",
      "Iteration 40, loss = 19463420611.47944260\n",
      "Iteration 41, loss = 19463412598.24515915\n",
      "Iteration 42, loss = 19463404679.84834290\n",
      "Iteration 43, loss = 19463396723.26628113\n",
      "Iteration 44, loss = 19463388647.71971512\n",
      "Iteration 45, loss = 19463380753.48960495\n",
      "Iteration 46, loss = 19463372676.56581879\n",
      "Iteration 47, loss = 19463364684.60051346\n",
      "Iteration 48, loss = 19463356652.33606339\n",
      "Iteration 49, loss = 19463348654.46412277\n",
      "Iteration 50, loss = 19463340618.35360336\n",
      "Iteration 51, loss = 19463332646.20476532\n",
      "Iteration 52, loss = 19463324710.84077072\n",
      "Iteration 53, loss = 19463316786.52893066\n",
      "Iteration 54, loss = 19463308900.92521286\n",
      "Iteration 55, loss = 19463301115.58661270\n",
      "Iteration 56, loss = 19463293414.49153137\n",
      "Iteration 57, loss = 19463285803.77235794\n",
      "Iteration 58, loss = 19463278219.10706329\n",
      "Iteration 59, loss = 19463270748.97131348\n",
      "Iteration 60, loss = 19463263474.80266190\n",
      "Iteration 61, loss = 19463256303.18753052\n",
      "Iteration 62, loss = 19463249266.45385742\n",
      "Iteration 63, loss = 19463242365.80797195\n",
      "Iteration 64, loss = 19463235637.08356476\n",
      "Iteration 65, loss = 19463229035.21826935\n",
      "Iteration 66, loss = 19463222603.33911896\n",
      "Iteration 67, loss = 19463216280.83440399\n",
      "Iteration 68, loss = 19463210146.99868011\n",
      "Iteration 69, loss = 19463204143.36914062\n",
      "Iteration 70, loss = 19463198312.41276169\n",
      "Iteration 71, loss = 19463192598.15637207\n",
      "Iteration 72, loss = 19463187051.12742615\n",
      "Iteration 73, loss = 19463181658.19231033\n",
      "Iteration 74, loss = 19463176355.39671326\n",
      "Iteration 75, loss = 19463171200.33737946\n",
      "Iteration 76, loss = 19463166166.37656403\n",
      "Iteration 77, loss = 19463161264.48802948\n",
      "Iteration 78, loss = 19463156507.03988266\n",
      "Iteration 79, loss = 19463151855.30060196\n",
      "Iteration 80, loss = 19463147317.86677933\n",
      "Iteration 81, loss = 19463142933.71638107\n",
      "Iteration 82, loss = 19463138643.99495316\n",
      "Iteration 83, loss = 19463134490.45021439\n",
      "Iteration 84, loss = 19463130391.38700867\n",
      "Iteration 85, loss = 19463126393.09069061\n",
      "Iteration 86, loss = 19463122529.83231735\n",
      "Iteration 87, loss = 19463118678.79177856\n",
      "Iteration 88, loss = 19463114937.99544907\n",
      "Iteration 89, loss = 19463111240.48382568\n",
      "Iteration 90, loss = 19463107624.24618530\n",
      "Iteration 91, loss = 19463104059.49947357\n",
      "Iteration 92, loss = 19463100516.49881744\n",
      "Iteration 93, loss = 19463097088.85066986\n",
      "Iteration 94, loss = 19463093670.99921799\n",
      "Iteration 95, loss = 19463090292.55506897\n",
      "Iteration 96, loss = 19463087006.54886627\n",
      "Iteration 97, loss = 19463083740.64571762\n",
      "Iteration 98, loss = 19463080539.62842941\n",
      "Iteration 99, loss = 19463077334.66715240\n",
      "Iteration 100, loss = 19463074227.54624557\n",
      "Iteration 101, loss = 19463071158.84066772\n",
      "Iteration 102, loss = 19463068102.65972137\n",
      "Iteration 103, loss = 19463065108.53197098\n",
      "Iteration 104, loss = 19463062150.70907211\n",
      "Iteration 105, loss = 19463059254.78248215\n",
      "Iteration 106, loss = 19463056408.50812149\n",
      "Iteration 107, loss = 19463053575.22474670\n",
      "Iteration 108, loss = 19463050811.27799225\n",
      "Iteration 109, loss = 19463048096.79211044\n",
      "Iteration 110, loss = 19463045405.76328278\n",
      "Iteration 111, loss = 19463042777.48804092\n",
      "Iteration 112, loss = 19463040148.12385941\n",
      "Iteration 113, loss = 19463037552.05321121\n",
      "Iteration 114, loss = 19463035024.95881271\n",
      "Iteration 115, loss = 19463032448.47208023\n",
      "Iteration 116, loss = 19463029929.58755493\n",
      "Iteration 117, loss = 19463027409.13060760\n",
      "Iteration 118, loss = 19463024921.28651428\n",
      "Iteration 119, loss = 19463022443.89137650\n",
      "Iteration 120, loss = 19463019986.57577896\n",
      "Iteration 121, loss = 19463017557.53922272\n",
      "Iteration 122, loss = 19463015157.40585709\n",
      "Iteration 123, loss = 19463012752.56716537\n",
      "Iteration 124, loss = 19463010396.87283325\n",
      "Iteration 125, loss = 19463008049.82679367\n",
      "Iteration 126, loss = 19463005727.15401077\n",
      "Iteration 127, loss = 19463003407.23724365\n",
      "Iteration 128, loss = 19463001106.49681473\n",
      "Iteration 129, loss = 19462998812.15625000\n",
      "Iteration 130, loss = 19462996530.42979050\n",
      "Iteration 131, loss = 19462994251.50308609\n",
      "Iteration 132, loss = 19462991974.63972473\n",
      "Iteration 133, loss = 19462989726.78971481\n",
      "Iteration 134, loss = 19462987461.17660522\n",
      "Iteration 135, loss = 19462985245.42358017\n",
      "Iteration 136, loss = 19462983000.71104813\n",
      "Iteration 137, loss = 19462980759.32698059\n",
      "Iteration 138, loss = 19462978565.17782593\n",
      "Iteration 139, loss = 19462976324.22311401\n",
      "Iteration 140, loss = 19462974115.86857986\n",
      "Iteration 141, loss = 19462971893.16942978\n",
      "Iteration 142, loss = 19462969669.92603302\n",
      "Iteration 143, loss = 19462967470.41126251\n",
      "Iteration 144, loss = 19462965242.49641418\n",
      "Iteration 145, loss = 19462963013.64797211\n",
      "Iteration 146, loss = 19462960835.28586197\n",
      "Iteration 147, loss = 19462958627.25307465\n",
      "Iteration 148, loss = 19462956427.62480164\n",
      "Iteration 149, loss = 19462954262.42334366\n",
      "Iteration 150, loss = 19462952097.70179367\n",
      "Iteration 151, loss = 19462949954.34560776\n",
      "Iteration 152, loss = 19462947789.05958557\n",
      "Iteration 153, loss = 19462945653.12362671\n",
      "Iteration 154, loss = 19462943502.54860306\n",
      "Iteration 155, loss = 19462941336.73921204\n",
      "Iteration 156, loss = 19462939166.57068253\n",
      "Iteration 157, loss = 19462936994.71704483\n",
      "Iteration 158, loss = 19462934821.68045425\n",
      "Iteration 159, loss = 19462932641.17294312\n",
      "Iteration 160, loss = 19462930464.49392319\n",
      "Iteration 161, loss = 19462928260.47411728\n",
      "Iteration 162, loss = 19462926078.05714035\n",
      "Iteration 163, loss = 19462923872.16087723\n",
      "Iteration 164, loss = 19462921684.38755035\n",
      "Iteration 165, loss = 19462919441.25828171\n",
      "Iteration 166, loss = 19462917256.36342239\n",
      "Iteration 167, loss = 19462915058.15415192\n",
      "Iteration 168, loss = 19462912791.43119049\n",
      "Iteration 169, loss = 19462910593.54365921\n",
      "Iteration 170, loss = 19462908350.64372635\n",
      "Iteration 171, loss = 19462906119.44134903\n",
      "Iteration 172, loss = 19462903857.53561783\n",
      "Iteration 173, loss = 19462901608.75368500\n",
      "Iteration 174, loss = 19462899333.66997910\n",
      "Iteration 175, loss = 19462897087.85680389\n",
      "Iteration 176, loss = 19462894774.75141144\n",
      "Iteration 177, loss = 19462892507.77592468\n",
      "Iteration 178, loss = 19462890211.40003967\n",
      "Iteration 179, loss = 19462887907.72661591\n",
      "Iteration 180, loss = 19462885638.45856476\n",
      "Iteration 181, loss = 19462883350.04940796\n",
      "Iteration 182, loss = 19462881041.62639618\n",
      "Iteration 183, loss = 19462878777.23686218\n",
      "Iteration 184, loss = 19462876496.03770828\n",
      "Iteration 185, loss = 19462874219.00381088\n",
      "Iteration 186, loss = 19462871952.88951492\n",
      "Iteration 187, loss = 19462869682.22718811\n",
      "Iteration 188, loss = 19462867465.13759232\n",
      "Iteration 189, loss = 19462865226.62971115\n",
      "Iteration 190, loss = 19462863013.15570831\n",
      "Iteration 191, loss = 19462860815.84360886\n",
      "Iteration 192, loss = 19462858602.71323013\n",
      "Iteration 193, loss = 19462856432.34223938\n",
      "Iteration 194, loss = 19462854219.98680878\n",
      "Iteration 195, loss = 19462852030.09431076\n",
      "Iteration 196, loss = 19462849835.53858566\n",
      "Iteration 197, loss = 19462847622.31291580\n",
      "Iteration 198, loss = 19462845393.03738785\n",
      "Iteration 199, loss = 19462843202.22097397\n",
      "Iteration 200, loss = 19462840956.97396469\n",
      "Iteration 201, loss = 19462838740.03643036\n",
      "Iteration 202, loss = 19462836493.33407974\n",
      "Iteration 203, loss = 19462834267.47133255\n",
      "Iteration 204, loss = 19462832012.28129959\n",
      "Iteration 205, loss = 19462829791.69341278\n",
      "Iteration 206, loss = 19462827535.95545197\n",
      "Iteration 207, loss = 19462825308.63451385\n",
      "Iteration 208, loss = 19462823065.22952652\n",
      "Iteration 209, loss = 19462820845.51374054\n",
      "Iteration 210, loss = 19462818628.58823776\n",
      "Iteration 211, loss = 19462816392.93524551\n",
      "Iteration 212, loss = 19462814179.44751358\n",
      "Iteration 213, loss = 19462811914.39256287\n",
      "Iteration 214, loss = 19462809694.86612701\n",
      "Iteration 215, loss = 19462807417.08092117\n",
      "Iteration 216, loss = 19462805184.10525894\n",
      "Iteration 217, loss = 19462802906.99164581\n",
      "Iteration 218, loss = 19462800642.83414459\n",
      "Iteration 219, loss = 19462798383.60866165\n",
      "Iteration 220, loss = 19462796098.81403732\n",
      "Iteration 221, loss = 19462793821.62974167\n",
      "Iteration 222, loss = 19462791541.61919403\n",
      "Iteration 223, loss = 19462789265.73506927\n",
      "Iteration 224, loss = 19462786955.28807449\n",
      "Iteration 225, loss = 19462784676.34210205\n",
      "Iteration 226, loss = 19462782373.78890610\n",
      "Iteration 227, loss = 19462780093.74801636\n",
      "Iteration 228, loss = 19462777791.15002060\n",
      "Iteration 229, loss = 19462775501.90071487\n",
      "Iteration 230, loss = 19462773224.84562683\n",
      "Iteration 231, loss = 19462770933.29597473\n",
      "Iteration 232, loss = 19462768661.52508926\n",
      "Iteration 233, loss = 19462766396.92343903\n",
      "Iteration 234, loss = 19462764103.71011353\n",
      "Iteration 235, loss = 19462761833.50650406\n",
      "Iteration 236, loss = 19462759547.51982880\n",
      "Iteration 237, loss = 19462757268.41036606\n",
      "Iteration 238, loss = 19462754988.99605179\n",
      "Iteration 239, loss = 19462752729.52814484\n",
      "Iteration 240, loss = 19462750489.33499908\n",
      "Iteration 241, loss = 19462748253.98631287\n",
      "Iteration 242, loss = 19462746040.93734741\n",
      "Iteration 243, loss = 19462743838.98882294\n",
      "Iteration 244, loss = 19462741633.33423996\n",
      "Iteration 245, loss = 19462739412.26443100\n",
      "Iteration 246, loss = 19462737213.40970230\n",
      "Iteration 247, loss = 19462735003.75388718\n",
      "Iteration 248, loss = 19462732795.96757126\n",
      "Iteration 249, loss = 19462730607.45665359\n",
      "Iteration 250, loss = 19462728397.14047241\n",
      "Iteration 251, loss = 19462726199.61862183\n",
      "Iteration 252, loss = 19462724009.48146057\n",
      "Iteration 253, loss = 19462721807.15577316\n",
      "Iteration 254, loss = 19462719617.10072708\n",
      "Iteration 255, loss = 19462717422.36047363\n",
      "Iteration 256, loss = 19462715229.07508469\n",
      "Iteration 257, loss = 19462713030.86798859\n",
      "Iteration 258, loss = 19462710842.84909821\n",
      "Iteration 259, loss = 19462708652.65872192\n",
      "Iteration 260, loss = 19462706480.52716827\n",
      "Iteration 261, loss = 19462704299.20269775\n",
      "Iteration 262, loss = 19462702118.06423569\n",
      "Iteration 263, loss = 19462699981.84311295\n",
      "Iteration 264, loss = 19462697810.01834488\n",
      "Iteration 265, loss = 19462695637.90607834\n",
      "Iteration 266, loss = 19462693490.59935760\n",
      "Iteration 267, loss = 19462691319.97812653\n",
      "Iteration 268, loss = 19462689157.43798065\n",
      "Iteration 269, loss = 19462687009.51059723\n",
      "Iteration 270, loss = 19462684859.09041214\n",
      "Iteration 271, loss = 19462682720.29086304\n",
      "Iteration 272, loss = 19462680578.87369156\n",
      "Iteration 273, loss = 19462678433.14052200\n",
      "Iteration 274, loss = 19462676313.69483185\n",
      "Iteration 275, loss = 19462674180.15429688\n",
      "Iteration 276, loss = 19462672063.06790543\n",
      "Iteration 277, loss = 19462669945.67030334\n",
      "Iteration 278, loss = 19462667828.53575134\n",
      "Iteration 279, loss = 19462665734.33691788\n",
      "Iteration 280, loss = 19462663629.98334122\n",
      "Iteration 281, loss = 19462661516.53873444\n",
      "Iteration 282, loss = 19462659415.15842819\n",
      "Iteration 283, loss = 19462657317.09376144\n",
      "Iteration 284, loss = 19462655183.73475647\n",
      "Iteration 285, loss = 19462653073.35412598\n",
      "Iteration 286, loss = 19462650966.82472992\n",
      "Iteration 287, loss = 19462648844.22518921\n",
      "Iteration 288, loss = 19462646732.72396088\n",
      "Iteration 289, loss = 19462644633.50799179\n",
      "Iteration 290, loss = 19462642525.41567993\n",
      "Iteration 291, loss = 19462640432.98867798\n",
      "Iteration 292, loss = 19462638345.84628296\n",
      "Iteration 293, loss = 19462636244.37997818\n",
      "Iteration 294, loss = 19462634177.75298691\n",
      "Iteration 295, loss = 19462632086.58143616\n",
      "Iteration 296, loss = 19462630035.49239731\n",
      "Iteration 297, loss = 19462627955.42697906\n",
      "Iteration 298, loss = 19462625916.01340485\n",
      "Iteration 299, loss = 19462623826.97217941\n",
      "Iteration 300, loss = 19462621782.52016449\n",
      "Iteration 301, loss = 19462619709.90644455\n",
      "Iteration 302, loss = 19462617636.32018280\n",
      "Iteration 303, loss = 19462615557.60317612\n",
      "Iteration 304, loss = 19462613483.32871246\n",
      "Iteration 305, loss = 19462611393.86145782\n",
      "Iteration 306, loss = 19462609327.94327545\n",
      "Iteration 307, loss = 19462607235.10970688\n",
      "Iteration 308, loss = 19462605167.71387482\n",
      "Iteration 309, loss = 19462603094.81067276\n",
      "Iteration 310, loss = 19462601021.63709259\n",
      "Iteration 311, loss = 19462598960.55469894\n",
      "Iteration 312, loss = 19462596898.60548401\n",
      "Iteration 313, loss = 19462594826.34976959\n",
      "Iteration 314, loss = 19462592766.69311523\n",
      "Iteration 315, loss = 19462590673.94222641\n",
      "Iteration 316, loss = 19462588604.26058960\n",
      "Iteration 317, loss = 19462586484.07706070\n",
      "Iteration 318, loss = 19462584416.70843124\n",
      "Iteration 319, loss = 19462582302.96964645\n",
      "Iteration 320, loss = 19462580206.40931320\n",
      "Iteration 321, loss = 19462578137.86739349\n",
      "Iteration 322, loss = 19462576036.04860306\n",
      "Iteration 323, loss = 19462573981.80645752\n",
      "Iteration 324, loss = 19462571910.37317276\n",
      "Iteration 325, loss = 19462569837.28990936\n",
      "Iteration 326, loss = 19462567803.58017731\n",
      "Iteration 327, loss = 19462565734.01344681\n",
      "Iteration 328, loss = 19462563698.57400131\n",
      "Iteration 329, loss = 19462561634.05437469\n",
      "Iteration 330, loss = 19462559596.51930618\n",
      "Iteration 331, loss = 19462557540.07360077\n",
      "Iteration 332, loss = 19462555481.57027054\n",
      "Iteration 333, loss = 19462553424.90742493\n",
      "Iteration 334, loss = 19462551357.92854691\n",
      "Iteration 335, loss = 19462549317.95735931\n",
      "Iteration 336, loss = 19462547255.18923187\n",
      "Iteration 337, loss = 19462545186.63320160\n",
      "Iteration 338, loss = 19462543145.76103210\n",
      "Iteration 339, loss = 19462541078.50522995\n",
      "Iteration 340, loss = 19462539033.04656219\n",
      "Iteration 341, loss = 19462536994.80038452\n",
      "Iteration 342, loss = 19462534925.69487762\n",
      "Iteration 343, loss = 19462532886.15086365\n",
      "Iteration 344, loss = 19462530830.01324844\n",
      "Iteration 345, loss = 19462528797.02351379\n",
      "Iteration 346, loss = 19462526757.77454758\n",
      "Iteration 347, loss = 19462524738.54318619\n",
      "Iteration 348, loss = 19462522724.51098633\n",
      "Iteration 349, loss = 19462520710.12293243\n",
      "Iteration 350, loss = 19462518726.41033173\n",
      "Iteration 351, loss = 19462516716.57098389\n",
      "Iteration 352, loss = 19462514716.53562546\n",
      "Iteration 353, loss = 19462512708.72034454\n",
      "Iteration 354, loss = 19462510712.62105560\n",
      "Iteration 355, loss = 19462508674.35886002\n",
      "Iteration 356, loss = 19462506676.25782394\n",
      "Iteration 357, loss = 19462504666.94573975\n",
      "Iteration 358, loss = 19462502639.19380951\n",
      "Iteration 359, loss = 19462500635.73215866\n",
      "Iteration 360, loss = 19462498596.08784103\n",
      "Iteration 361, loss = 19462496563.64327621\n",
      "Iteration 362, loss = 19462494543.34745789\n",
      "Iteration 363, loss = 19462492492.63816071\n",
      "Iteration 364, loss = 19462490455.64993668\n",
      "Iteration 365, loss = 19462488417.34667206\n",
      "Iteration 366, loss = 19462486363.62029266\n",
      "Iteration 367, loss = 19462484358.88811111\n",
      "Iteration 368, loss = 19462482310.85771561\n",
      "Iteration 369, loss = 19462480300.84144974\n",
      "Iteration 370, loss = 19462478274.28134918\n",
      "Iteration 371, loss = 19462476260.81174850\n",
      "Iteration 372, loss = 19462474259.81164551\n",
      "Iteration 373, loss = 19462472234.33389282\n",
      "Iteration 374, loss = 19462470227.19269180\n",
      "Iteration 375, loss = 19462468210.80281830\n",
      "Iteration 376, loss = 19462466202.60039139\n",
      "Iteration 377, loss = 19462464190.37923431\n",
      "Iteration 378, loss = 19462462196.44596100\n",
      "Iteration 379, loss = 19462460193.45091248\n",
      "Iteration 380, loss = 19462458195.27630615\n",
      "Iteration 381, loss = 19462456210.55299759\n",
      "Iteration 382, loss = 19462454233.81790543\n",
      "Iteration 383, loss = 19462452219.33789825\n",
      "Iteration 384, loss = 19462450240.05436707\n",
      "Iteration 385, loss = 19462448248.48011398\n",
      "Iteration 386, loss = 19462446249.50489426\n",
      "Iteration 387, loss = 19462444265.53319550\n",
      "Iteration 388, loss = 19462442283.94381714\n",
      "Iteration 389, loss = 19462440266.99817276\n",
      "Iteration 390, loss = 19462438279.53591919\n",
      "Iteration 391, loss = 19462436269.60653305\n",
      "Iteration 392, loss = 19462434263.17111206\n",
      "Iteration 393, loss = 19462432236.26274872\n",
      "Iteration 394, loss = 19462430239.78075790\n",
      "Iteration 395, loss = 19462428209.77395630\n",
      "Iteration 396, loss = 19462426194.90946579\n",
      "Iteration 397, loss = 19462424190.34116364\n",
      "Iteration 398, loss = 19462422205.61204910\n",
      "Iteration 399, loss = 19462420180.59184265\n",
      "Iteration 400, loss = 19462418192.03306580\n",
      "Iteration 401, loss = 19462416202.54067230\n",
      "Iteration 402, loss = 19462414191.59360504\n",
      "Iteration 403, loss = 19462412201.47935867\n",
      "Iteration 404, loss = 19462410215.40933990\n",
      "Iteration 405, loss = 19462408201.50021744\n",
      "Iteration 406, loss = 19462406222.22375870\n",
      "Iteration 407, loss = 19462404225.65582275\n",
      "Iteration 408, loss = 19462402255.38927460\n",
      "Iteration 409, loss = 19462400266.76556396\n",
      "Iteration 410, loss = 19462398283.17947769\n",
      "Iteration 411, loss = 19462396291.00814056\n",
      "Iteration 412, loss = 19462394311.53283310\n",
      "Iteration 413, loss = 19462392304.44201660\n",
      "Iteration 414, loss = 19462390308.69406509\n",
      "Iteration 415, loss = 19462388309.53139496\n",
      "Iteration 416, loss = 19462386318.05082321\n",
      "Iteration 417, loss = 19462384314.43107986\n",
      "Iteration 418, loss = 19462382328.82909012\n",
      "Iteration 419, loss = 19462380337.04450226\n",
      "Iteration 420, loss = 19462378331.12341690\n",
      "Iteration 421, loss = 19462376355.67599106\n",
      "Iteration 422, loss = 19462374358.71889114\n",
      "Iteration 423, loss = 19462372391.29545593\n",
      "Iteration 424, loss = 19462370413.79368973\n",
      "Iteration 425, loss = 19462368440.01797104\n",
      "Iteration 426, loss = 19462366475.25748062\n",
      "Iteration 427, loss = 19462364485.48377228\n",
      "Iteration 428, loss = 19462362525.98080063\n",
      "Iteration 429, loss = 19462360538.29630661\n",
      "Iteration 430, loss = 19462358534.88111877\n",
      "Iteration 431, loss = 19462356560.69453430\n",
      "Iteration 432, loss = 19462354555.88684082\n",
      "Iteration 433, loss = 19462352576.57008743\n",
      "Iteration 434, loss = 19462350583.58763885\n",
      "Iteration 435, loss = 19462348579.37641525\n",
      "Iteration 436, loss = 19462346604.38226318\n",
      "Iteration 437, loss = 19462344617.46374893\n",
      "Iteration 438, loss = 19462342627.58274460\n",
      "Iteration 439, loss = 19462340656.14929581\n",
      "Iteration 440, loss = 19462338661.42419434\n",
      "Iteration 441, loss = 19462336677.09738922\n",
      "Iteration 442, loss = 19462334691.35115051\n",
      "Iteration 443, loss = 19462332686.44606400\n",
      "Iteration 444, loss = 19462330705.97227478\n",
      "Iteration 445, loss = 19462328693.61391830\n",
      "Iteration 446, loss = 19462326706.39941406\n",
      "Iteration 447, loss = 19462324686.45999146\n",
      "Iteration 448, loss = 19462322695.86484528\n",
      "Iteration 449, loss = 19462320677.32269669\n",
      "Iteration 450, loss = 19462318665.06399536\n",
      "Iteration 451, loss = 19462316658.50695419\n",
      "Iteration 452, loss = 19462314660.70853424\n",
      "Iteration 453, loss = 19462312637.89250565\n",
      "Iteration 454, loss = 19462310657.77187729\n",
      "Iteration 455, loss = 19462308665.62165833\n",
      "Iteration 456, loss = 19462306692.95899582\n",
      "Iteration 457, loss = 19462304727.78217316\n",
      "Iteration 458, loss = 19462302752.85980606\n",
      "Iteration 459, loss = 19462300806.38602448\n",
      "Iteration 460, loss = 19462298859.16576385\n",
      "Iteration 461, loss = 19462296893.04516983\n",
      "Iteration 462, loss = 19462294934.58595657\n",
      "Iteration 463, loss = 19462292988.00490189\n",
      "Iteration 464, loss = 19462291038.28596878\n",
      "Iteration 465, loss = 19462289069.87102509\n",
      "Iteration 466, loss = 19462287128.31781769\n",
      "Iteration 467, loss = 19462285163.14634323\n",
      "Iteration 468, loss = 19462283185.74012375\n",
      "Iteration 469, loss = 19462281217.28147507\n",
      "Iteration 470, loss = 19462279243.19667053\n",
      "Iteration 471, loss = 19462277279.04724884\n",
      "Iteration 472, loss = 19462275281.11050797\n",
      "Iteration 473, loss = 19462273300.74422455\n",
      "Iteration 474, loss = 19462271308.15298462\n",
      "Iteration 475, loss = 19462269299.85015106\n",
      "Iteration 476, loss = 19462267300.17037964\n",
      "Iteration 477, loss = 19462265271.40820694\n",
      "Iteration 478, loss = 19462263284.71260452\n",
      "Iteration 479, loss = 19462261266.52398300\n",
      "Iteration 480, loss = 19462259269.48331070\n",
      "Iteration 481, loss = 19462257278.53948593\n",
      "Iteration 482, loss = 19462255298.46974564\n",
      "Iteration 483, loss = 19462253317.75916672\n",
      "Iteration 484, loss = 19462251346.45037460\n",
      "Iteration 485, loss = 19462249384.72055817\n",
      "Iteration 486, loss = 19462247411.73381042\n",
      "Iteration 487, loss = 19462245455.44038773\n",
      "Iteration 488, loss = 19462243491.91784668\n",
      "Iteration 489, loss = 19462241528.66821289\n",
      "Iteration 490, loss = 19462239562.10371017\n",
      "Iteration 491, loss = 19462237605.13545227\n",
      "Iteration 492, loss = 19462235636.81121445\n",
      "Iteration 493, loss = 19462233684.78749466\n",
      "Iteration 494, loss = 19462231685.01871109\n",
      "Iteration 495, loss = 19462229725.74538803\n",
      "Iteration 496, loss = 19462227747.06674194\n",
      "Iteration 497, loss = 19462225770.03523254\n",
      "Iteration 498, loss = 19462223800.56999207\n",
      "Iteration 499, loss = 19462221834.38128662\n",
      "Iteration 500, loss = 19462219866.52297974\n",
      "Iteration 1, loss = 20102320279.56298065\n",
      "Iteration 2, loss = 20102313410.99727249\n",
      "Iteration 3, loss = 20102306446.66234970\n",
      "Iteration 4, loss = 20102299505.93606567\n",
      "Iteration 5, loss = 20102292581.86874008\n",
      "Iteration 6, loss = 20102285726.92620087\n",
      "Iteration 7, loss = 20102278735.55044556\n",
      "Iteration 8, loss = 20102271816.51865387\n",
      "Iteration 9, loss = 20102264868.73152161\n",
      "Iteration 10, loss = 20102257896.50319672\n",
      "Iteration 11, loss = 20102250878.00496674\n",
      "Iteration 12, loss = 20102243887.33169937\n",
      "Iteration 13, loss = 20102236795.96305084\n",
      "Iteration 14, loss = 20102229790.98362732\n",
      "Iteration 15, loss = 20102222674.58376694\n",
      "Iteration 16, loss = 20102215641.78951263\n",
      "Iteration 17, loss = 20102208419.60170746\n",
      "Iteration 18, loss = 20102201304.36091614\n",
      "Iteration 19, loss = 20102194047.15577698\n",
      "Iteration 20, loss = 20102186713.38689041\n",
      "Iteration 21, loss = 20102179561.11516571\n",
      "Iteration 22, loss = 20102172067.15693283\n",
      "Iteration 23, loss = 20102164510.48511124\n",
      "Iteration 24, loss = 20102157084.11574173\n",
      "Iteration 25, loss = 20102149572.86066818\n",
      "Iteration 26, loss = 20102141848.58714676\n",
      "Iteration 27, loss = 20102134148.36745453\n",
      "Iteration 28, loss = 20102126290.64809799\n",
      "Iteration 29, loss = 20102118475.38720322\n",
      "Iteration 30, loss = 20102110607.93527985\n",
      "Iteration 31, loss = 20102102495.43933487\n",
      "Iteration 32, loss = 20102094423.37796783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 20102086375.56575775\n",
      "Iteration 34, loss = 20102078160.31543732\n",
      "Iteration 35, loss = 20102069920.17232895\n",
      "Iteration 36, loss = 20102061574.44699097\n",
      "Iteration 37, loss = 20102053272.12621689\n",
      "Iteration 38, loss = 20102045022.09368515\n",
      "Iteration 39, loss = 20102036608.17951584\n",
      "Iteration 40, loss = 20102028288.93171310\n",
      "Iteration 41, loss = 20102019889.80214691\n",
      "Iteration 42, loss = 20102011630.78833008\n",
      "Iteration 43, loss = 20102003360.10615921\n",
      "Iteration 44, loss = 20101995180.54457092\n",
      "Iteration 45, loss = 20101987157.70563507\n",
      "Iteration 46, loss = 20101979180.64173889\n",
      "Iteration 47, loss = 20101971221.65922546\n",
      "Iteration 48, loss = 20101963401.56014633\n",
      "Iteration 49, loss = 20101955682.80260468\n",
      "Iteration 50, loss = 20101948027.26679230\n",
      "Iteration 51, loss = 20101940530.98664856\n",
      "Iteration 52, loss = 20101933046.72742844\n",
      "Iteration 53, loss = 20101925654.94107819\n",
      "Iteration 54, loss = 20101918387.04451752\n",
      "Iteration 55, loss = 20101911074.34202957\n",
      "Iteration 56, loss = 20101903933.56934738\n",
      "Iteration 57, loss = 20101896797.52103806\n",
      "Iteration 58, loss = 20101889817.54654694\n",
      "Iteration 59, loss = 20101882806.26697159\n",
      "Iteration 60, loss = 20101875911.56389618\n",
      "Iteration 61, loss = 20101869214.83025742\n",
      "Iteration 62, loss = 20101862524.81057358\n",
      "Iteration 63, loss = 20101855991.50546265\n",
      "Iteration 64, loss = 20101849527.19643784\n",
      "Iteration 65, loss = 20101843095.91121674\n",
      "Iteration 66, loss = 20101836874.71440887\n",
      "Iteration 67, loss = 20101830649.50650024\n",
      "Iteration 68, loss = 20101824577.06334686\n",
      "Iteration 69, loss = 20101818688.44622040\n",
      "Iteration 70, loss = 20101812743.87691498\n",
      "Iteration 71, loss = 20101807046.14941406\n",
      "Iteration 72, loss = 20101801322.30857849\n",
      "Iteration 73, loss = 20101795836.91404343\n",
      "Iteration 74, loss = 20101790321.11738968\n",
      "Iteration 75, loss = 20101785004.12404251\n",
      "Iteration 76, loss = 20101779692.71370697\n",
      "Iteration 77, loss = 20101774498.21972275\n",
      "Iteration 78, loss = 20101769363.85703278\n",
      "Iteration 79, loss = 20101764268.39094543\n",
      "Iteration 80, loss = 20101759205.86560822\n",
      "Iteration 81, loss = 20101754319.51464462\n",
      "Iteration 82, loss = 20101749385.14799500\n",
      "Iteration 83, loss = 20101744504.22330856\n",
      "Iteration 84, loss = 20101739746.58751678\n",
      "Iteration 85, loss = 20101734971.27568817\n",
      "Iteration 86, loss = 20101730304.45561218\n",
      "Iteration 87, loss = 20101725767.36525726\n",
      "Iteration 88, loss = 20101721268.30090332\n",
      "Iteration 89, loss = 20101716862.95347214\n",
      "Iteration 90, loss = 20101712587.21619034\n",
      "Iteration 91, loss = 20101708361.17830276\n",
      "Iteration 92, loss = 20101704254.64082336\n",
      "Iteration 93, loss = 20101700143.23714828\n",
      "Iteration 94, loss = 20101696132.11449814\n",
      "Iteration 95, loss = 20101692154.13475037\n",
      "Iteration 96, loss = 20101688219.17840576\n",
      "Iteration 97, loss = 20101684331.51543045\n",
      "Iteration 98, loss = 20101680505.89741516\n",
      "Iteration 99, loss = 20101676675.65395737\n",
      "Iteration 100, loss = 20101672908.73680878\n",
      "Iteration 101, loss = 20101669206.19613647\n",
      "Iteration 102, loss = 20101665485.50954819\n",
      "Iteration 103, loss = 20101661813.85060501\n",
      "Iteration 104, loss = 20101658185.82804108\n",
      "Iteration 105, loss = 20101654623.57137299\n",
      "Iteration 106, loss = 20101651046.42310715\n",
      "Iteration 107, loss = 20101647537.37696457\n",
      "Iteration 108, loss = 20101644037.57324982\n",
      "Iteration 109, loss = 20101640559.19000244\n",
      "Iteration 110, loss = 20101637100.52541733\n",
      "Iteration 111, loss = 20101633662.47193527\n",
      "Iteration 112, loss = 20101630194.60324860\n",
      "Iteration 113, loss = 20101626771.96464157\n",
      "Iteration 114, loss = 20101623370.47189713\n",
      "Iteration 115, loss = 20101619998.36230469\n",
      "Iteration 116, loss = 20101616616.21839523\n",
      "Iteration 117, loss = 20101613255.43225861\n",
      "Iteration 118, loss = 20101609902.09834671\n",
      "Iteration 119, loss = 20101606620.44974518\n",
      "Iteration 120, loss = 20101603323.13768005\n",
      "Iteration 121, loss = 20101600059.12795639\n",
      "Iteration 122, loss = 20101596800.20226288\n",
      "Iteration 123, loss = 20101593552.21616745\n",
      "Iteration 124, loss = 20101590356.27653885\n",
      "Iteration 125, loss = 20101587168.81830215\n",
      "Iteration 126, loss = 20101583942.26258850\n",
      "Iteration 127, loss = 20101580786.88631058\n",
      "Iteration 128, loss = 20101577602.62443542\n",
      "Iteration 129, loss = 20101574457.00729370\n",
      "Iteration 130, loss = 20101571316.87558746\n",
      "Iteration 131, loss = 20101568210.76837158\n",
      "Iteration 132, loss = 20101565097.04912567\n",
      "Iteration 133, loss = 20101562023.03755951\n",
      "Iteration 134, loss = 20101558952.22516632\n",
      "Iteration 135, loss = 20101555922.00373459\n",
      "Iteration 136, loss = 20101552857.85424042\n",
      "Iteration 137, loss = 20101549863.39437485\n",
      "Iteration 138, loss = 20101546873.08432388\n",
      "Iteration 139, loss = 20101543921.22882462\n",
      "Iteration 140, loss = 20101540946.90633774\n",
      "Iteration 141, loss = 20101537993.30154037\n",
      "Iteration 142, loss = 20101535040.92420959\n",
      "Iteration 143, loss = 20101532161.59701538\n",
      "Iteration 144, loss = 20101529230.29857254\n",
      "Iteration 145, loss = 20101526364.26174545\n",
      "Iteration 146, loss = 20101523484.22343063\n",
      "Iteration 147, loss = 20101520601.71426392\n",
      "Iteration 148, loss = 20101517752.83299255\n",
      "Iteration 149, loss = 20101514915.22583771\n",
      "Iteration 150, loss = 20101512090.96065140\n",
      "Iteration 151, loss = 20101509242.30776596\n",
      "Iteration 152, loss = 20101506426.10546875\n",
      "Iteration 153, loss = 20101503609.45986176\n",
      "Iteration 154, loss = 20101500795.43726730\n",
      "Iteration 155, loss = 20101497990.66379547\n",
      "Iteration 156, loss = 20101495228.87355423\n",
      "Iteration 157, loss = 20101492424.73621750\n",
      "Iteration 158, loss = 20101489661.93351746\n",
      "Iteration 159, loss = 20101486876.04576874\n",
      "Iteration 160, loss = 20101484117.60005951\n",
      "Iteration 161, loss = 20101481332.61043930\n",
      "Iteration 162, loss = 20101478589.37961960\n",
      "Iteration 163, loss = 20101475847.41596985\n",
      "Iteration 164, loss = 20101473090.97646332\n",
      "Iteration 165, loss = 20101470349.97895050\n",
      "Iteration 166, loss = 20101467641.53134918\n",
      "Iteration 167, loss = 20101464877.47078705\n",
      "Iteration 168, loss = 20101462159.91172028\n",
      "Iteration 169, loss = 20101459441.36274719\n",
      "Iteration 170, loss = 20101456716.57990646\n",
      "Iteration 171, loss = 20101453981.21711349\n",
      "Iteration 172, loss = 20101451275.98800278\n",
      "Iteration 173, loss = 20101448589.59946442\n",
      "Iteration 174, loss = 20101445845.82641983\n",
      "Iteration 175, loss = 20101443138.34305954\n",
      "Iteration 176, loss = 20101440425.87648392\n",
      "Iteration 177, loss = 20101437719.98121643\n",
      "Iteration 178, loss = 20101435035.62373734\n",
      "Iteration 179, loss = 20101432305.03297806\n",
      "Iteration 180, loss = 20101429648.45238113\n",
      "Iteration 181, loss = 20101426959.99742126\n",
      "Iteration 182, loss = 20101424273.32081604\n",
      "Iteration 183, loss = 20101421608.40065002\n",
      "Iteration 184, loss = 20101418954.44552994\n",
      "Iteration 185, loss = 20101416275.90839386\n",
      "Iteration 186, loss = 20101413626.78675079\n",
      "Iteration 187, loss = 20101410977.59974289\n",
      "Iteration 188, loss = 20101408337.73892975\n",
      "Iteration 189, loss = 20101405673.90867615\n",
      "Iteration 190, loss = 20101402998.45600891\n",
      "Iteration 191, loss = 20101400369.72977829\n",
      "Iteration 192, loss = 20101397762.92671204\n",
      "Iteration 193, loss = 20101395081.64915085\n",
      "Iteration 194, loss = 20101392450.43009949\n",
      "Iteration 195, loss = 20101389848.46644974\n",
      "Iteration 196, loss = 20101387223.19738388\n",
      "Iteration 197, loss = 20101384582.27392960\n",
      "Iteration 198, loss = 20101381936.86822510\n",
      "Iteration 199, loss = 20101379293.39523697\n",
      "Iteration 200, loss = 20101376700.02041245\n",
      "Iteration 201, loss = 20101374069.43962479\n",
      "Iteration 202, loss = 20101371458.14927292\n",
      "Iteration 203, loss = 20101368838.75164795\n",
      "Iteration 204, loss = 20101366194.48437119\n",
      "Iteration 205, loss = 20101363626.62454224\n",
      "Iteration 206, loss = 20101360979.38932419\n",
      "Iteration 207, loss = 20101358374.56842041\n",
      "Iteration 208, loss = 20101355790.24653625\n",
      "Iteration 209, loss = 20101353122.55810165\n",
      "Iteration 210, loss = 20101350542.19031143\n",
      "Iteration 211, loss = 20101347941.85439301\n",
      "Iteration 212, loss = 20101345326.53848648\n",
      "Iteration 213, loss = 20101342736.42532349\n",
      "Iteration 214, loss = 20101340081.53435898\n",
      "Iteration 215, loss = 20101337498.74835587\n",
      "Iteration 216, loss = 20101334885.69295883\n",
      "Iteration 217, loss = 20101332255.90876007\n",
      "Iteration 218, loss = 20101329657.20571899\n",
      "Iteration 219, loss = 20101327036.04601669\n",
      "Iteration 220, loss = 20101324422.29320908\n",
      "Iteration 221, loss = 20101321791.28842545\n",
      "Iteration 222, loss = 20101319161.42598343\n",
      "Iteration 223, loss = 20101316540.91519547\n",
      "Iteration 224, loss = 20101313895.56719971\n",
      "Iteration 225, loss = 20101311266.99462509\n",
      "Iteration 226, loss = 20101308637.49233627\n",
      "Iteration 227, loss = 20101306003.12544250\n",
      "Iteration 228, loss = 20101303353.57228088\n",
      "Iteration 229, loss = 20101300666.49056244\n",
      "Iteration 230, loss = 20101298048.86864853\n",
      "Iteration 231, loss = 20101295388.18790436\n",
      "Iteration 232, loss = 20101292719.30088043\n",
      "Iteration 233, loss = 20101290024.97559357\n",
      "Iteration 234, loss = 20101287383.04563522\n",
      "Iteration 235, loss = 20101284677.86030579\n",
      "Iteration 236, loss = 20101281994.65234756\n",
      "Iteration 237, loss = 20101279300.21869659\n",
      "Iteration 238, loss = 20101276622.37203598\n",
      "Iteration 239, loss = 20101273917.36120224\n",
      "Iteration 240, loss = 20101271200.02422333\n",
      "Iteration 241, loss = 20101268503.70554733\n",
      "Iteration 242, loss = 20101265784.03942108\n",
      "Iteration 243, loss = 20101263090.50642776\n",
      "Iteration 244, loss = 20101260376.66211319\n",
      "Iteration 245, loss = 20101257648.21920013\n",
      "Iteration 246, loss = 20101254968.04362106\n",
      "Iteration 247, loss = 20101252198.85494995\n",
      "Iteration 248, loss = 20101249532.48672867\n",
      "Iteration 249, loss = 20101246821.41783142\n",
      "Iteration 250, loss = 20101244091.30941010\n",
      "Iteration 251, loss = 20101241353.27970886\n",
      "Iteration 252, loss = 20101238670.68546677\n",
      "Iteration 253, loss = 20101235936.78274536\n",
      "Iteration 254, loss = 20101233258.00122070\n",
      "Iteration 255, loss = 20101230546.77349854\n",
      "Iteration 256, loss = 20101227807.77958679\n",
      "Iteration 257, loss = 20101225138.59867859\n",
      "Iteration 258, loss = 20101222402.02927399\n",
      "Iteration 259, loss = 20101219709.73107529\n",
      "Iteration 260, loss = 20101217052.10190201\n",
      "Iteration 261, loss = 20101214285.30045319\n",
      "Iteration 262, loss = 20101211581.41863632\n",
      "Iteration 263, loss = 20101208892.97190857\n",
      "Iteration 264, loss = 20101206187.34730911\n",
      "Iteration 265, loss = 20101203503.28095245\n",
      "Iteration 266, loss = 20101200807.64664459\n",
      "Iteration 267, loss = 20101198089.19593811\n",
      "Iteration 268, loss = 20101195380.84700775\n",
      "Iteration 269, loss = 20101192706.08827591\n",
      "Iteration 270, loss = 20101190037.23070145\n",
      "Iteration 271, loss = 20101187341.95756912\n",
      "Iteration 272, loss = 20101184665.04270554\n",
      "Iteration 273, loss = 20101181979.07785797\n",
      "Iteration 274, loss = 20101179313.83152390\n",
      "Iteration 275, loss = 20101176597.96533585\n",
      "Iteration 276, loss = 20101173922.11036301\n",
      "Iteration 277, loss = 20101171246.77637100\n",
      "Iteration 278, loss = 20101168552.52667236\n",
      "Iteration 279, loss = 20101165864.89579391\n",
      "Iteration 280, loss = 20101163176.62960434\n",
      "Iteration 281, loss = 20101160460.57500458\n",
      "Iteration 282, loss = 20101157750.18648911\n",
      "Iteration 283, loss = 20101155063.07781982\n",
      "Iteration 284, loss = 20101152379.29530716\n",
      "Iteration 285, loss = 20101149651.20612335\n",
      "Iteration 286, loss = 20101146956.61218262\n",
      "Iteration 287, loss = 20101144213.78720093\n",
      "Iteration 288, loss = 20101141504.64031982\n",
      "Iteration 289, loss = 20101138741.04151917\n",
      "Iteration 290, loss = 20101136002.11816406\n",
      "Iteration 291, loss = 20101133265.20681763\n",
      "Iteration 292, loss = 20101130536.17378235\n",
      "Iteration 293, loss = 20101127760.27425766\n",
      "Iteration 294, loss = 20101124949.10464096\n",
      "Iteration 295, loss = 20101122216.62850952\n",
      "Iteration 296, loss = 20101119404.29895782\n",
      "Iteration 297, loss = 20101116633.26529694\n",
      "Iteration 298, loss = 20101113834.98679733\n",
      "Iteration 299, loss = 20101110989.04089355\n",
      "Iteration 300, loss = 20101108143.80813599\n",
      "Iteration 301, loss = 20101105376.26013565\n",
      "Iteration 302, loss = 20101102534.18347168\n",
      "Iteration 303, loss = 20101099702.27541733\n",
      "Iteration 304, loss = 20101096830.21842575\n",
      "Iteration 305, loss = 20101093972.48921204\n",
      "Iteration 306, loss = 20101091089.26695251\n",
      "Iteration 307, loss = 20101088237.60850143\n",
      "Iteration 308, loss = 20101085358.29225922\n",
      "Iteration 309, loss = 20101082455.98151779\n",
      "Iteration 310, loss = 20101079585.85359192\n",
      "Iteration 311, loss = 20101076694.88758850\n",
      "Iteration 312, loss = 20101073737.24741745\n",
      "Iteration 313, loss = 20101070823.26180649\n",
      "Iteration 314, loss = 20101067868.88038635\n",
      "Iteration 315, loss = 20101064911.27864075\n",
      "Iteration 316, loss = 20101061946.22121811\n",
      "Iteration 317, loss = 20101058964.29609680\n",
      "Iteration 318, loss = 20101055924.45212173\n",
      "Iteration 319, loss = 20101052946.98138428\n",
      "Iteration 320, loss = 20101049909.91208267\n",
      "Iteration 321, loss = 20101046857.57266617\n",
      "Iteration 322, loss = 20101043783.80302048\n",
      "Iteration 323, loss = 20101040708.88605499\n",
      "Iteration 324, loss = 20101037638.65458679\n",
      "Iteration 325, loss = 20101034552.82754135\n",
      "Iteration 326, loss = 20101031454.25165939\n",
      "Iteration 327, loss = 20101028344.86927414\n",
      "Iteration 328, loss = 20101025260.30185699\n",
      "Iteration 329, loss = 20101022142.12170029\n",
      "Iteration 330, loss = 20101019054.04648590\n",
      "Iteration 331, loss = 20101016014.15104675\n",
      "Iteration 332, loss = 20101012915.58459854\n",
      "Iteration 333, loss = 20101009841.44035721\n",
      "Iteration 334, loss = 20101006756.88040924\n",
      "Iteration 335, loss = 20101003700.14718628\n",
      "Iteration 336, loss = 20101000658.50507736\n",
      "Iteration 337, loss = 20100997631.05678177\n",
      "Iteration 338, loss = 20100994606.58480072\n",
      "Iteration 339, loss = 20100991567.03632736\n",
      "Iteration 340, loss = 20100988591.87125397\n",
      "Iteration 341, loss = 20100985554.40055084\n",
      "Iteration 342, loss = 20100982546.33098221\n",
      "Iteration 343, loss = 20100979595.76086044\n",
      "Iteration 344, loss = 20100976566.07017136\n",
      "Iteration 345, loss = 20100973635.32665634\n",
      "Iteration 346, loss = 20100970636.97254944\n",
      "Iteration 347, loss = 20100967703.75920105\n",
      "Iteration 348, loss = 20100964764.74776840\n",
      "Iteration 349, loss = 20100961787.16234970\n",
      "Iteration 350, loss = 20100958856.92503738\n",
      "Iteration 351, loss = 20100955936.15791702\n",
      "Iteration 352, loss = 20100952989.12421417\n",
      "Iteration 353, loss = 20100950067.50283051\n",
      "Iteration 354, loss = 20100947163.73620605\n",
      "Iteration 355, loss = 20100944263.77298737\n",
      "Iteration 356, loss = 20100941353.98693466\n",
      "Iteration 357, loss = 20100938425.53712082\n",
      "Iteration 358, loss = 20100935561.91729736\n",
      "Iteration 359, loss = 20100932674.37637711\n",
      "Iteration 360, loss = 20100929740.94942093\n",
      "Iteration 361, loss = 20100926923.11359406\n",
      "Iteration 362, loss = 20100924032.12157822\n",
      "Iteration 363, loss = 20100921142.99439240\n",
      "Iteration 364, loss = 20100918275.66736984\n",
      "Iteration 365, loss = 20100915401.10304260\n",
      "Iteration 366, loss = 20100912572.81726074\n",
      "Iteration 367, loss = 20100909676.19005203\n",
      "Iteration 368, loss = 20100906848.24501801\n",
      "Iteration 369, loss = 20100903980.92167282\n",
      "Iteration 370, loss = 20100901134.99720001\n",
      "Iteration 371, loss = 20100898299.58121109\n",
      "Iteration 372, loss = 20100895459.47422028\n",
      "Iteration 373, loss = 20100892587.33304214\n",
      "Iteration 374, loss = 20100889764.63166428\n",
      "Iteration 375, loss = 20100886938.67335892\n",
      "Iteration 376, loss = 20100884091.90064621\n",
      "Iteration 377, loss = 20100881253.06130981\n",
      "Iteration 378, loss = 20100878421.41163635\n",
      "Iteration 379, loss = 20100875607.29317856\n",
      "Iteration 380, loss = 20100872767.86504364\n",
      "Iteration 381, loss = 20100869954.79398346\n",
      "Iteration 382, loss = 20100867143.87135696\n",
      "Iteration 383, loss = 20100864313.50757980\n",
      "Iteration 384, loss = 20100861486.38678741\n",
      "Iteration 385, loss = 20100858678.23635101\n",
      "Iteration 386, loss = 20100855862.89650726\n",
      "Iteration 387, loss = 20100853048.98720551\n",
      "Iteration 388, loss = 20100850247.54665756\n",
      "Iteration 389, loss = 20100847432.81650543\n",
      "Iteration 390, loss = 20100844607.19528961\n",
      "Iteration 391, loss = 20100841831.51156998\n",
      "Iteration 392, loss = 20100839025.64463043\n",
      "Iteration 393, loss = 20100836226.26322937\n",
      "Iteration 394, loss = 20100833417.40570450\n",
      "Iteration 395, loss = 20100830586.41194153\n",
      "Iteration 396, loss = 20100827808.03443527\n",
      "Iteration 397, loss = 20100825021.17387009\n",
      "Iteration 398, loss = 20100822227.12786102\n",
      "Iteration 399, loss = 20100819393.13608932\n",
      "Iteration 400, loss = 20100816603.18405533\n",
      "Iteration 401, loss = 20100813802.88805771\n",
      "Iteration 402, loss = 20100811004.29502106\n",
      "Iteration 403, loss = 20100808176.12910461\n",
      "Iteration 404, loss = 20100805389.98867798\n",
      "Iteration 405, loss = 20100802588.94426346\n",
      "Iteration 406, loss = 20100799750.02378464\n",
      "Iteration 407, loss = 20100796936.08121490\n",
      "Iteration 408, loss = 20100794118.78591919\n",
      "Iteration 409, loss = 20100791291.54757690\n",
      "Iteration 410, loss = 20100788461.84111404\n",
      "Iteration 411, loss = 20100785620.31211472\n",
      "Iteration 412, loss = 20100782775.09635544\n",
      "Iteration 413, loss = 20100779906.22460556\n",
      "Iteration 414, loss = 20100777045.75193405\n",
      "Iteration 415, loss = 20100774155.14967346\n",
      "Iteration 416, loss = 20100771231.38103104\n",
      "Iteration 417, loss = 20100768341.51196289\n",
      "Iteration 418, loss = 20100765395.82877350\n",
      "Iteration 419, loss = 20100762465.25817108\n",
      "Iteration 420, loss = 20100759453.37190247\n",
      "Iteration 421, loss = 20100756478.90306473\n",
      "Iteration 422, loss = 20100753456.62588120\n",
      "Iteration 423, loss = 20100750422.02476120\n",
      "Iteration 424, loss = 20100747370.72007370\n",
      "Iteration 425, loss = 20100744283.44863892\n",
      "Iteration 426, loss = 20100741199.88930893\n",
      "Iteration 427, loss = 20100738123.67100906\n",
      "Iteration 428, loss = 20100734999.61243439\n",
      "Iteration 429, loss = 20100731839.39402008\n",
      "Iteration 430, loss = 20100728770.44899368\n",
      "Iteration 431, loss = 20100725633.67809296\n",
      "Iteration 432, loss = 20100722528.72789383\n",
      "Iteration 433, loss = 20100719402.67366028\n",
      "Iteration 434, loss = 20100716301.66259003\n",
      "Iteration 435, loss = 20100713192.62771225\n",
      "Iteration 436, loss = 20100710121.16272736\n",
      "Iteration 437, loss = 20100707022.36402512\n",
      "Iteration 438, loss = 20100703961.10738754\n",
      "Iteration 439, loss = 20100700880.04722214\n",
      "Iteration 440, loss = 20100697842.52293396\n",
      "Iteration 441, loss = 20100694776.58101273\n",
      "Iteration 442, loss = 20100691745.14613724\n",
      "Iteration 443, loss = 20100688730.00022888\n",
      "Iteration 444, loss = 20100685682.50981903\n",
      "Iteration 445, loss = 20100682655.27519989\n",
      "Iteration 446, loss = 20100679655.29678345\n",
      "Iteration 447, loss = 20100676648.30900574\n",
      "Iteration 448, loss = 20100673646.61454391\n",
      "Iteration 449, loss = 20100670682.27760315\n",
      "Iteration 450, loss = 20100667674.18845367\n",
      "Iteration 451, loss = 20100664709.63386917\n",
      "Iteration 452, loss = 20100661717.07138443\n",
      "Iteration 453, loss = 20100658773.22239304\n",
      "Iteration 454, loss = 20100655808.24230576\n",
      "Iteration 455, loss = 20100652822.68660355\n",
      "Iteration 456, loss = 20100649889.43095016\n",
      "Iteration 457, loss = 20100646938.64885330\n",
      "Iteration 458, loss = 20100643996.90814590\n",
      "Iteration 459, loss = 20100641034.01109695\n",
      "Iteration 460, loss = 20100638117.09409714\n",
      "Iteration 461, loss = 20100635214.07673645\n",
      "Iteration 462, loss = 20100632292.28705978\n",
      "Iteration 463, loss = 20100629332.88311768\n",
      "Iteration 464, loss = 20100626402.25945282\n",
      "Iteration 465, loss = 20100623500.04537964\n",
      "Iteration 466, loss = 20100620625.57563019\n",
      "Iteration 467, loss = 20100617681.04579926\n",
      "Iteration 468, loss = 20100614784.34180832\n",
      "Iteration 469, loss = 20100611893.98694229\n",
      "Iteration 470, loss = 20100608980.19526291\n",
      "Iteration 471, loss = 20100606089.27258682\n",
      "Iteration 472, loss = 20100603226.35462189\n",
      "Iteration 473, loss = 20100600313.43494797\n",
      "Iteration 474, loss = 20100597422.06054688\n",
      "Iteration 475, loss = 20100594539.13540649\n",
      "Iteration 476, loss = 20100591666.09046173\n",
      "Iteration 477, loss = 20100588776.53105164\n",
      "Iteration 478, loss = 20100585932.86256027\n",
      "Iteration 479, loss = 20100583036.94843674\n",
      "Iteration 480, loss = 20100580169.36838913\n",
      "Iteration 481, loss = 20100577294.99296951\n",
      "Iteration 482, loss = 20100574440.09238434\n",
      "Iteration 483, loss = 20100571576.09292221\n",
      "Iteration 484, loss = 20100568731.81767654\n",
      "Iteration 485, loss = 20100565833.78392410\n",
      "Iteration 486, loss = 20100562994.03967285\n",
      "Iteration 487, loss = 20100560155.69149017\n",
      "Iteration 488, loss = 20100557255.58852386\n",
      "Iteration 489, loss = 20100554434.96099472\n",
      "Iteration 490, loss = 20100551580.15644455\n",
      "Iteration 491, loss = 20100548746.55303574\n",
      "Iteration 492, loss = 20100545883.00505447\n",
      "Iteration 493, loss = 20100543046.03102493\n",
      "Iteration 494, loss = 20100540186.97387314\n",
      "Iteration 495, loss = 20100537370.74004364\n",
      "Iteration 496, loss = 20100534525.23917770\n",
      "Iteration 497, loss = 20100531679.21377182\n",
      "Iteration 498, loss = 20100528837.12792969\n",
      "Iteration 499, loss = 20100525990.43786240\n",
      "Iteration 500, loss = 20100523182.98513412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19475845536.95424652\n",
      "Iteration 2, loss = 19475836155.50405121\n",
      "Iteration 3, loss = 19475826720.50008392\n",
      "Iteration 4, loss = 19475817280.48223495\n",
      "Iteration 5, loss = 19475807834.31633377\n",
      "Iteration 6, loss = 19475798356.32258606\n",
      "Iteration 7, loss = 19475788802.94334030\n",
      "Iteration 8, loss = 19475779224.20716095\n",
      "Iteration 9, loss = 19475769557.77396393\n",
      "Iteration 10, loss = 19475759883.39633942\n",
      "Iteration 11, loss = 19475750150.81683731\n",
      "Iteration 12, loss = 19475740126.93750000\n",
      "Iteration 13, loss = 19475730095.65172577\n",
      "Iteration 14, loss = 19475719786.04753113\n",
      "Iteration 15, loss = 19475709409.13853073\n",
      "Iteration 16, loss = 19475698700.09065628\n",
      "Iteration 17, loss = 19475687819.78828430\n",
      "Iteration 18, loss = 19475676587.51718903\n",
      "Iteration 19, loss = 19475665338.49619293\n",
      "Iteration 20, loss = 19475653708.67870712\n",
      "Iteration 21, loss = 19475642101.08198166\n",
      "Iteration 22, loss = 19475630234.48559189\n",
      "Iteration 23, loss = 19475618262.41347122\n",
      "Iteration 24, loss = 19475606384.77935410\n",
      "Iteration 25, loss = 19475594281.72618103\n",
      "Iteration 26, loss = 19475582219.56636429\n",
      "Iteration 27, loss = 19475570028.83392715\n",
      "Iteration 28, loss = 19475558033.62754440\n",
      "Iteration 29, loss = 19475545935.69246674\n",
      "Iteration 30, loss = 19475534084.54454041\n",
      "Iteration 31, loss = 19475522499.61262131\n",
      "Iteration 32, loss = 19475511108.87963867\n",
      "Iteration 33, loss = 19475500016.99098969\n",
      "Iteration 34, loss = 19475489031.40425110\n",
      "Iteration 35, loss = 19475478258.16388702\n",
      "Iteration 36, loss = 19475467567.97985840\n",
      "Iteration 37, loss = 19475457020.89937592\n",
      "Iteration 38, loss = 19475446570.48918533\n",
      "Iteration 39, loss = 19475436219.51527786\n",
      "Iteration 40, loss = 19475425803.54340363\n",
      "Iteration 41, loss = 19475415503.75278473\n",
      "Iteration 42, loss = 19475405292.82244873\n",
      "Iteration 43, loss = 19475395177.61044312\n",
      "Iteration 44, loss = 19475385361.00526047\n",
      "Iteration 45, loss = 19475375795.16170120\n",
      "Iteration 46, loss = 19475366598.27676010\n",
      "Iteration 47, loss = 19475357468.61915970\n",
      "Iteration 48, loss = 19475348660.33741379\n",
      "Iteration 49, loss = 19475339856.30245590\n",
      "Iteration 50, loss = 19475331166.02537155\n",
      "Iteration 51, loss = 19475322363.51819229\n",
      "Iteration 52, loss = 19475313736.36788177\n",
      "Iteration 53, loss = 19475305163.03958511\n",
      "Iteration 54, loss = 19475296634.64454269\n",
      "Iteration 55, loss = 19475288342.65995026\n",
      "Iteration 56, loss = 19475280331.48087692\n",
      "Iteration 57, loss = 19475272612.19263458\n",
      "Iteration 58, loss = 19475265198.55831909\n",
      "Iteration 59, loss = 19475258037.26618195\n",
      "Iteration 60, loss = 19475251033.97496796\n",
      "Iteration 61, loss = 19475244296.57391357\n",
      "Iteration 62, loss = 19475237699.48360443\n",
      "Iteration 63, loss = 19475231455.90840530\n",
      "Iteration 64, loss = 19475225557.28672028\n",
      "Iteration 65, loss = 19475219897.99418259\n",
      "Iteration 66, loss = 19475214512.79428101\n",
      "Iteration 67, loss = 19475209384.23422623\n",
      "Iteration 68, loss = 19475204337.19400787\n",
      "Iteration 69, loss = 19475199412.81897354\n",
      "Iteration 70, loss = 19475194629.66202545\n",
      "Iteration 71, loss = 19475189797.91270065\n",
      "Iteration 72, loss = 19475185092.65585327\n",
      "Iteration 73, loss = 19475180420.33697510\n",
      "Iteration 74, loss = 19475175722.27317810\n",
      "Iteration 75, loss = 19475171142.08898163\n",
      "Iteration 76, loss = 19475166473.14766693\n",
      "Iteration 77, loss = 19475161925.90343475\n",
      "Iteration 78, loss = 19475157441.25848007\n",
      "Iteration 79, loss = 19475152954.23648834\n",
      "Iteration 80, loss = 19475148582.06905746\n",
      "Iteration 81, loss = 19475144142.40463257\n",
      "Iteration 82, loss = 19475139797.75120926\n",
      "Iteration 83, loss = 19475135381.76481247\n",
      "Iteration 84, loss = 19475131031.66703796\n",
      "Iteration 85, loss = 19475126644.31904221\n",
      "Iteration 86, loss = 19475122303.22492599\n",
      "Iteration 87, loss = 19475117949.57110596\n",
      "Iteration 88, loss = 19475113667.65269470\n",
      "Iteration 89, loss = 19475109401.41353607\n",
      "Iteration 90, loss = 19475105206.56592560\n",
      "Iteration 91, loss = 19475101014.99760818\n",
      "Iteration 92, loss = 19475096837.77432251\n",
      "Iteration 93, loss = 19475092663.79495621\n",
      "Iteration 94, loss = 19475088519.82838058\n",
      "Iteration 95, loss = 19475084409.82157898\n",
      "Iteration 96, loss = 19475080271.60667801\n",
      "Iteration 97, loss = 19475076220.36268616\n",
      "Iteration 98, loss = 19475072101.39405441\n",
      "Iteration 99, loss = 19475068048.10455322\n",
      "Iteration 100, loss = 19475064037.41309738\n",
      "Iteration 101, loss = 19475059998.25585556\n",
      "Iteration 102, loss = 19475055989.62474060\n",
      "Iteration 103, loss = 19475052033.89315796\n",
      "Iteration 104, loss = 19475048045.02170944\n",
      "Iteration 105, loss = 19475044095.40467453\n",
      "Iteration 106, loss = 19475040170.20246887\n",
      "Iteration 107, loss = 19475036187.20648193\n",
      "Iteration 108, loss = 19475032251.85169220\n",
      "Iteration 109, loss = 19475028301.77152252\n",
      "Iteration 110, loss = 19475024414.34934616\n",
      "Iteration 111, loss = 19475020544.50218582\n",
      "Iteration 112, loss = 19475016666.57841873\n",
      "Iteration 113, loss = 19475012821.18650818\n",
      "Iteration 114, loss = 19475008973.56754684\n",
      "Iteration 115, loss = 19475005138.72778702\n",
      "Iteration 116, loss = 19475001273.33150482\n",
      "Iteration 117, loss = 19474997424.08505630\n",
      "Iteration 118, loss = 19474993621.65617752\n",
      "Iteration 119, loss = 19474989787.22989273\n",
      "Iteration 120, loss = 19474986034.44998550\n",
      "Iteration 121, loss = 19474982265.80213547\n",
      "Iteration 122, loss = 19474978518.23857117\n",
      "Iteration 123, loss = 19474974771.12259674\n",
      "Iteration 124, loss = 19474971022.36119461\n",
      "Iteration 125, loss = 19474967310.28424072\n",
      "Iteration 126, loss = 19474963596.79751587\n",
      "Iteration 127, loss = 19474959890.21863556\n",
      "Iteration 128, loss = 19474956209.28676987\n",
      "Iteration 129, loss = 19474952537.57198715\n",
      "Iteration 130, loss = 19474948880.32450485\n",
      "Iteration 131, loss = 19474945193.30102539\n",
      "Iteration 132, loss = 19474941535.09158325\n",
      "Iteration 133, loss = 19474937896.33045197\n",
      "Iteration 134, loss = 19474934214.26657867\n",
      "Iteration 135, loss = 19474930595.77660370\n",
      "Iteration 136, loss = 19474926951.40205765\n",
      "Iteration 137, loss = 19474923281.22161102\n",
      "Iteration 138, loss = 19474919655.87274933\n",
      "Iteration 139, loss = 19474916051.25019073\n",
      "Iteration 140, loss = 19474912425.08209229\n",
      "Iteration 141, loss = 19474908805.74347687\n",
      "Iteration 142, loss = 19474905235.82432175\n",
      "Iteration 143, loss = 19474901603.40061188\n",
      "Iteration 144, loss = 19474898016.21355057\n",
      "Iteration 145, loss = 19474894407.82190704\n",
      "Iteration 146, loss = 19474890871.19557953\n",
      "Iteration 147, loss = 19474887255.69232559\n",
      "Iteration 148, loss = 19474883685.93880844\n",
      "Iteration 149, loss = 19474880132.95609283\n",
      "Iteration 150, loss = 19474876542.14474869\n",
      "Iteration 151, loss = 19474872979.73220444\n",
      "Iteration 152, loss = 19474869447.33817673\n",
      "Iteration 153, loss = 19474865820.89466476\n",
      "Iteration 154, loss = 19474862303.86474991\n",
      "Iteration 155, loss = 19474858730.67419052\n",
      "Iteration 156, loss = 19474855188.86575699\n",
      "Iteration 157, loss = 19474851649.46789169\n",
      "Iteration 158, loss = 19474848086.63688278\n",
      "Iteration 159, loss = 19474844593.27301407\n",
      "Iteration 160, loss = 19474841035.80668640\n",
      "Iteration 161, loss = 19474837502.00885773\n",
      "Iteration 162, loss = 19474834018.19900131\n",
      "Iteration 163, loss = 19474830505.68416977\n",
      "Iteration 164, loss = 19474826965.07616425\n",
      "Iteration 165, loss = 19474823464.43258286\n",
      "Iteration 166, loss = 19474819993.54941177\n",
      "Iteration 167, loss = 19474816515.11239624\n",
      "Iteration 168, loss = 19474813010.63005066\n",
      "Iteration 169, loss = 19474809554.15089798\n",
      "Iteration 170, loss = 19474806069.71757507\n",
      "Iteration 171, loss = 19474802585.37868500\n",
      "Iteration 172, loss = 19474799123.83137131\n",
      "Iteration 173, loss = 19474795623.93046188\n",
      "Iteration 174, loss = 19474792179.22348404\n",
      "Iteration 175, loss = 19474788691.76251221\n",
      "Iteration 176, loss = 19474785230.65777588\n",
      "Iteration 177, loss = 19474781775.77752304\n",
      "Iteration 178, loss = 19474778339.66971588\n",
      "Iteration 179, loss = 19474774840.64062119\n",
      "Iteration 180, loss = 19474771428.56997681\n",
      "Iteration 181, loss = 19474767976.59150696\n",
      "Iteration 182, loss = 19474764499.03948975\n",
      "Iteration 183, loss = 19474761071.80213165\n",
      "Iteration 184, loss = 19474757640.92778397\n",
      "Iteration 185, loss = 19474754206.43508530\n",
      "Iteration 186, loss = 19474750754.93152618\n",
      "Iteration 187, loss = 19474747325.54906464\n",
      "Iteration 188, loss = 19474743887.13381577\n",
      "Iteration 189, loss = 19474740435.83745193\n",
      "Iteration 190, loss = 19474737024.45640564\n",
      "Iteration 191, loss = 19474733545.23581314\n",
      "Iteration 192, loss = 19474730130.67672729\n",
      "Iteration 193, loss = 19474726706.09558868\n",
      "Iteration 194, loss = 19474723260.39093781\n",
      "Iteration 195, loss = 19474719847.69616318\n",
      "Iteration 196, loss = 19474716420.80646515\n",
      "Iteration 197, loss = 19474713044.10615158\n",
      "Iteration 198, loss = 19474709626.11239624\n",
      "Iteration 199, loss = 19474706247.54269791\n",
      "Iteration 200, loss = 19474702859.41289139\n",
      "Iteration 201, loss = 19474699461.89105988\n",
      "Iteration 202, loss = 19474696097.29800797\n",
      "Iteration 203, loss = 19474692703.51517487\n",
      "Iteration 204, loss = 19474689361.62539291\n",
      "Iteration 205, loss = 19474685958.55016327\n",
      "Iteration 206, loss = 19474682552.86605072\n",
      "Iteration 207, loss = 19474679186.62674332\n",
      "Iteration 208, loss = 19474675790.50899124\n",
      "Iteration 209, loss = 19474672392.36787033\n",
      "Iteration 210, loss = 19474669020.88892746\n",
      "Iteration 211, loss = 19474665618.23165512\n",
      "Iteration 212, loss = 19474662252.63654709\n",
      "Iteration 213, loss = 19474658878.97174072\n",
      "Iteration 214, loss = 19474655497.72283936\n",
      "Iteration 215, loss = 19474652096.64607620\n",
      "Iteration 216, loss = 19474648727.60055161\n",
      "Iteration 217, loss = 19474645366.82552338\n",
      "Iteration 218, loss = 19474641970.38222885\n",
      "Iteration 219, loss = 19474638600.71528625\n",
      "Iteration 220, loss = 19474635248.83556366\n",
      "Iteration 221, loss = 19474631865.78014755\n",
      "Iteration 222, loss = 19474628499.04037476\n",
      "Iteration 223, loss = 19474625102.41607285\n",
      "Iteration 224, loss = 19474621761.98715210\n",
      "Iteration 225, loss = 19474618380.86800766\n",
      "Iteration 226, loss = 19474614999.40413666\n",
      "Iteration 227, loss = 19474611604.40911865\n",
      "Iteration 228, loss = 19474608269.87722015\n",
      "Iteration 229, loss = 19474604903.79586411\n",
      "Iteration 230, loss = 19474601551.01787567\n",
      "Iteration 231, loss = 19474598181.49155426\n",
      "Iteration 232, loss = 19474594821.20909882\n",
      "Iteration 233, loss = 19474591468.61132050\n",
      "Iteration 234, loss = 19474588091.57091141\n",
      "Iteration 235, loss = 19474584774.02584839\n",
      "Iteration 236, loss = 19474581433.57766724\n",
      "Iteration 237, loss = 19474578093.08074188\n",
      "Iteration 238, loss = 19474574756.37316132\n",
      "Iteration 239, loss = 19474571395.14226532\n",
      "Iteration 240, loss = 19474568093.75770187\n",
      "Iteration 241, loss = 19474564748.25261307\n",
      "Iteration 242, loss = 19474561412.49483490\n",
      "Iteration 243, loss = 19474558061.43742371\n",
      "Iteration 244, loss = 19474554761.18197250\n",
      "Iteration 245, loss = 19474551421.19197464\n",
      "Iteration 246, loss = 19474548092.31612015\n",
      "Iteration 247, loss = 19474544758.37104797\n",
      "Iteration 248, loss = 19474541420.31062698\n",
      "Iteration 249, loss = 19474538134.61746597\n",
      "Iteration 250, loss = 19474534777.10534668\n",
      "Iteration 251, loss = 19474531465.38713837\n",
      "Iteration 252, loss = 19474528133.96916580\n",
      "Iteration 253, loss = 19474524812.97479630\n",
      "Iteration 254, loss = 19474521448.58441544\n",
      "Iteration 255, loss = 19474518137.35209274\n",
      "Iteration 256, loss = 19474514810.97953033\n",
      "Iteration 257, loss = 19474511437.51110077\n",
      "Iteration 258, loss = 19474508146.15242386\n",
      "Iteration 259, loss = 19474504816.67251205\n",
      "Iteration 260, loss = 19474501499.30441284\n",
      "Iteration 261, loss = 19474498172.68714905\n",
      "Iteration 262, loss = 19474494852.63759232\n",
      "Iteration 263, loss = 19474491577.27530670\n",
      "Iteration 264, loss = 19474488228.44018555\n",
      "Iteration 265, loss = 19474484961.60618973\n",
      "Iteration 266, loss = 19474481655.02557755\n",
      "Iteration 267, loss = 19474478362.78348923\n",
      "Iteration 268, loss = 19474475050.27682877\n",
      "Iteration 269, loss = 19474471776.62879562\n",
      "Iteration 270, loss = 19474468440.82331467\n",
      "Iteration 271, loss = 19474465137.11071396\n",
      "Iteration 272, loss = 19474461857.45750046\n",
      "Iteration 273, loss = 19474458533.88706589\n",
      "Iteration 274, loss = 19474455260.50667953\n",
      "Iteration 275, loss = 19474451936.58291626\n",
      "Iteration 276, loss = 19474448646.26524353\n",
      "Iteration 277, loss = 19474445330.78644180\n",
      "Iteration 278, loss = 19474442014.65111542\n",
      "Iteration 279, loss = 19474438702.48597717\n",
      "Iteration 280, loss = 19474435416.40600204\n",
      "Iteration 281, loss = 19474432088.73183823\n",
      "Iteration 282, loss = 19474428830.15807724\n",
      "Iteration 283, loss = 19474425471.55350876\n",
      "Iteration 284, loss = 19474422183.49760056\n",
      "Iteration 285, loss = 19474418893.71739578\n",
      "Iteration 286, loss = 19474415579.26467896\n",
      "Iteration 287, loss = 19474412257.81589890\n",
      "Iteration 288, loss = 19474408958.73839188\n",
      "Iteration 289, loss = 19474405637.86558914\n",
      "Iteration 290, loss = 19474402345.26772308\n",
      "Iteration 291, loss = 19474399051.43606567\n",
      "Iteration 292, loss = 19474395734.15941238\n",
      "Iteration 293, loss = 19474392457.66064453\n",
      "Iteration 294, loss = 19474389158.23546600\n",
      "Iteration 295, loss = 19474385837.75749207\n",
      "Iteration 296, loss = 19474382577.76836395\n",
      "Iteration 297, loss = 19474379272.79475784\n",
      "Iteration 298, loss = 19474375974.31415176\n",
      "Iteration 299, loss = 19474372676.23297119\n",
      "Iteration 300, loss = 19474369381.83881378\n",
      "Iteration 301, loss = 19474366067.97637177\n",
      "Iteration 302, loss = 19474362782.45924759\n",
      "Iteration 303, loss = 19474359490.05176163\n",
      "Iteration 304, loss = 19474356171.93811035\n",
      "Iteration 305, loss = 19474352899.51236343\n",
      "Iteration 306, loss = 19474349576.36671829\n",
      "Iteration 307, loss = 19474346321.29582596\n",
      "Iteration 308, loss = 19474343010.12006378\n",
      "Iteration 309, loss = 19474339718.46397400\n",
      "Iteration 310, loss = 19474336452.33067322\n",
      "Iteration 311, loss = 19474333178.57202911\n",
      "Iteration 312, loss = 19474329882.26997757\n",
      "Iteration 313, loss = 19474326607.71750641\n",
      "Iteration 314, loss = 19474323357.07214737\n",
      "Iteration 315, loss = 19474320059.12258911\n",
      "Iteration 316, loss = 19474316795.52950287\n",
      "Iteration 317, loss = 19474313529.88060379\n",
      "Iteration 318, loss = 19474310225.25543976\n",
      "Iteration 319, loss = 19474306965.88934708\n",
      "Iteration 320, loss = 19474303687.93324661\n",
      "Iteration 321, loss = 19474300404.79963684\n",
      "Iteration 322, loss = 19474297139.71685791\n",
      "Iteration 323, loss = 19474293873.09178162\n",
      "Iteration 324, loss = 19474290604.10083008\n",
      "Iteration 325, loss = 19474287360.98538208\n",
      "Iteration 326, loss = 19474284093.89485931\n",
      "Iteration 327, loss = 19474280862.26271820\n",
      "Iteration 328, loss = 19474277579.97837067\n",
      "Iteration 329, loss = 19474274340.90074539\n",
      "Iteration 330, loss = 19474271084.92222977\n",
      "Iteration 331, loss = 19474267811.97123337\n",
      "Iteration 332, loss = 19474264565.72137070\n",
      "Iteration 333, loss = 19474261297.11919022\n",
      "Iteration 334, loss = 19474258050.60057449\n",
      "Iteration 335, loss = 19474254777.11421967\n",
      "Iteration 336, loss = 19474251533.18430710\n",
      "Iteration 337, loss = 19474248257.31210327\n",
      "Iteration 338, loss = 19474245013.52889633\n",
      "Iteration 339, loss = 19474241763.58401489\n",
      "Iteration 340, loss = 19474238483.19513321\n",
      "Iteration 341, loss = 19474235248.18411255\n",
      "Iteration 342, loss = 19474231960.52256012\n",
      "Iteration 343, loss = 19474228736.73297119\n",
      "Iteration 344, loss = 19474225446.33182907\n",
      "Iteration 345, loss = 19474222217.59352493\n",
      "Iteration 346, loss = 19474218966.28070068\n",
      "Iteration 347, loss = 19474215699.97384262\n",
      "Iteration 348, loss = 19474212463.55530548\n",
      "Iteration 349, loss = 19474209229.09532547\n",
      "Iteration 350, loss = 19474205992.86066055\n",
      "Iteration 351, loss = 19474202706.36328888\n",
      "Iteration 352, loss = 19474199477.67967224\n",
      "Iteration 353, loss = 19474196185.50019836\n",
      "Iteration 354, loss = 19474192930.84665298\n",
      "Iteration 355, loss = 19474189657.78347778\n",
      "Iteration 356, loss = 19474186376.07466125\n",
      "Iteration 357, loss = 19474183108.09453583\n",
      "Iteration 358, loss = 19474179838.22592545\n",
      "Iteration 359, loss = 19474176551.65041733\n",
      "Iteration 360, loss = 19474173292.79795456\n",
      "Iteration 361, loss = 19474170015.86032867\n",
      "Iteration 362, loss = 19474166753.03619766\n",
      "Iteration 363, loss = 19474163494.36706543\n",
      "Iteration 364, loss = 19474160242.22965240\n",
      "Iteration 365, loss = 19474156970.37368011\n",
      "Iteration 366, loss = 19474153758.52657700\n",
      "Iteration 367, loss = 19474150479.04373550\n",
      "Iteration 368, loss = 19474147269.51468277\n",
      "Iteration 369, loss = 19474144038.73746872\n",
      "Iteration 370, loss = 19474140802.43067169\n",
      "Iteration 371, loss = 19474137580.16979599\n",
      "Iteration 372, loss = 19474134306.24798965\n",
      "Iteration 373, loss = 19474131079.03273773\n",
      "Iteration 374, loss = 19474127840.55932236\n",
      "Iteration 375, loss = 19474124596.29974747\n",
      "Iteration 376, loss = 19474121326.06241989\n",
      "Iteration 377, loss = 19474118102.39953232\n",
      "Iteration 378, loss = 19474114830.12962723\n",
      "Iteration 379, loss = 19474111564.23831177\n",
      "Iteration 380, loss = 19474108307.62871933\n",
      "Iteration 381, loss = 19474105025.57893753\n",
      "Iteration 382, loss = 19474101757.77791977\n",
      "Iteration 383, loss = 19474098507.02945709\n",
      "Iteration 384, loss = 19474095227.38642883\n",
      "Iteration 385, loss = 19474091976.62427902\n",
      "Iteration 386, loss = 19474088725.80173492\n",
      "Iteration 387, loss = 19474085472.43978119\n",
      "Iteration 388, loss = 19474082235.25008011\n",
      "Iteration 389, loss = 19474079007.37835312\n",
      "Iteration 390, loss = 19474075748.77943420\n",
      "Iteration 391, loss = 19474072528.36722183\n",
      "Iteration 392, loss = 19474069298.68406677\n",
      "Iteration 393, loss = 19474066050.88690948\n",
      "Iteration 394, loss = 19474062828.81648254\n",
      "Iteration 395, loss = 19474059574.23085785\n",
      "Iteration 396, loss = 19474056369.58625793\n",
      "Iteration 397, loss = 19474053156.01216507\n",
      "Iteration 398, loss = 19474049893.98952484\n",
      "Iteration 399, loss = 19474046663.26276779\n",
      "Iteration 400, loss = 19474043461.40300369\n",
      "Iteration 401, loss = 19474040204.49681854\n",
      "Iteration 402, loss = 19474036965.94704819\n",
      "Iteration 403, loss = 19474033719.77616119\n",
      "Iteration 404, loss = 19474030497.78623962\n",
      "Iteration 405, loss = 19474027263.03644180\n",
      "Iteration 406, loss = 19474024024.69747925\n",
      "Iteration 407, loss = 19474020749.96142197\n",
      "Iteration 408, loss = 19474017547.71758652\n",
      "Iteration 409, loss = 19474014280.79913330\n",
      "Iteration 410, loss = 19474011079.50626755\n",
      "Iteration 411, loss = 19474007849.68465424\n",
      "Iteration 412, loss = 19474004643.25202179\n",
      "Iteration 413, loss = 19474001408.60859680\n",
      "Iteration 414, loss = 19473998188.81235123\n",
      "Iteration 415, loss = 19473994952.69392014\n",
      "Iteration 416, loss = 19473991707.43223572\n",
      "Iteration 417, loss = 19473988461.47459412\n",
      "Iteration 418, loss = 19473985220.68622971\n",
      "Iteration 419, loss = 19473981973.58454895\n",
      "Iteration 420, loss = 19473978759.00953674\n",
      "Iteration 421, loss = 19473975454.46386337\n",
      "Iteration 422, loss = 19473972234.61287689\n",
      "Iteration 423, loss = 19473968987.89536285\n",
      "Iteration 424, loss = 19473965755.40023041\n",
      "Iteration 425, loss = 19473962485.67022324\n",
      "Iteration 426, loss = 19473959280.12080002\n",
      "Iteration 427, loss = 19473956016.43816757\n",
      "Iteration 428, loss = 19473952810.14164734\n",
      "Iteration 429, loss = 19473949557.32631302\n",
      "Iteration 430, loss = 19473946339.17955399\n",
      "Iteration 431, loss = 19473943130.54648972\n",
      "Iteration 432, loss = 19473939870.79091263\n",
      "Iteration 433, loss = 19473936647.52281952\n",
      "Iteration 434, loss = 19473933443.57114410\n",
      "Iteration 435, loss = 19473930203.76653671\n",
      "Iteration 436, loss = 19473926973.34169388\n",
      "Iteration 437, loss = 19473923713.82099152\n",
      "Iteration 438, loss = 19473920483.23407364\n",
      "Iteration 439, loss = 19473917231.73867035\n",
      "Iteration 440, loss = 19473913998.18547440\n",
      "Iteration 441, loss = 19473910767.07339096\n",
      "Iteration 442, loss = 19473907507.93997574\n",
      "Iteration 443, loss = 19473904280.91610718\n",
      "Iteration 444, loss = 19473901061.16388702\n",
      "Iteration 445, loss = 19473897807.99710083\n",
      "Iteration 446, loss = 19473894573.98229599\n",
      "Iteration 447, loss = 19473891372.08078384\n",
      "Iteration 448, loss = 19473888143.22018433\n",
      "Iteration 449, loss = 19473884912.70344543\n",
      "Iteration 450, loss = 19473881717.08614349\n",
      "Iteration 451, loss = 19473878487.22813416\n",
      "Iteration 452, loss = 19473875235.28915405\n",
      "Iteration 453, loss = 19473872050.34737778\n",
      "Iteration 454, loss = 19473868776.63597488\n",
      "Iteration 455, loss = 19473865563.49100113\n",
      "Iteration 456, loss = 19473862338.09682083\n",
      "Iteration 457, loss = 19473859060.93321228\n",
      "Iteration 458, loss = 19473855843.28078461\n",
      "Iteration 459, loss = 19473852595.10327911\n",
      "Iteration 460, loss = 19473849372.56027603\n",
      "Iteration 461, loss = 19473846103.90587616\n",
      "Iteration 462, loss = 19473842877.58844757\n",
      "Iteration 463, loss = 19473839639.80338669\n",
      "Iteration 464, loss = 19473836400.14544296\n",
      "Iteration 465, loss = 19473833164.78134155\n",
      "Iteration 466, loss = 19473829907.38144302\n",
      "Iteration 467, loss = 19473826674.14417648\n",
      "Iteration 468, loss = 19473823448.23227310\n",
      "Iteration 469, loss = 19473820212.33129120\n",
      "Iteration 470, loss = 19473817032.33409500\n",
      "Iteration 471, loss = 19473813841.93523788\n",
      "Iteration 472, loss = 19473810615.22785187\n",
      "Iteration 473, loss = 19473807398.98763275\n",
      "Iteration 474, loss = 19473804218.43495178\n",
      "Iteration 475, loss = 19473800975.02257538\n",
      "Iteration 476, loss = 19473797762.00534058\n",
      "Iteration 477, loss = 19473794553.99603653\n",
      "Iteration 478, loss = 19473791310.12874985\n",
      "Iteration 479, loss = 19473788090.62566376\n",
      "Iteration 480, loss = 19473784877.11839676\n",
      "Iteration 481, loss = 19473781660.82881165\n",
      "Iteration 482, loss = 19473778423.70095062\n",
      "Iteration 483, loss = 19473775185.05611801\n",
      "Iteration 484, loss = 19473771955.20956802\n",
      "Iteration 485, loss = 19473768735.27782440\n",
      "Iteration 486, loss = 19473765505.11552811\n",
      "Iteration 487, loss = 19473762261.56285858\n",
      "Iteration 488, loss = 19473759004.99714279\n",
      "Iteration 489, loss = 19473755775.07685852\n",
      "Iteration 490, loss = 19473752564.56936646\n",
      "Iteration 491, loss = 19473749315.40940094\n",
      "Iteration 492, loss = 19473746080.74714279\n",
      "Iteration 493, loss = 19473742894.80232620\n",
      "Iteration 494, loss = 19473739649.93345642\n",
      "Iteration 495, loss = 19473736416.93629074\n",
      "Iteration 496, loss = 19473733231.46997070\n",
      "Iteration 497, loss = 19473729992.31063843\n",
      "Iteration 498, loss = 19473726759.52067947\n",
      "Iteration 499, loss = 19473723517.43686676\n",
      "Iteration 500, loss = 19473720302.65764999\n",
      "Iteration 1, loss = 19652951085.10974121\n",
      "Iteration 2, loss = 19652941633.62455368\n",
      "Iteration 3, loss = 19652932157.68843842\n",
      "Iteration 4, loss = 19652922667.84599686\n",
      "Iteration 5, loss = 19652913150.29135513\n",
      "Iteration 6, loss = 19652903672.89054489\n",
      "Iteration 7, loss = 19652893951.84454727\n",
      "Iteration 8, loss = 19652884292.66596222\n",
      "Iteration 9, loss = 19652874510.76552200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 19652864504.08049393\n",
      "Iteration 11, loss = 19652854502.34032440\n",
      "Iteration 12, loss = 19652844217.29954910\n",
      "Iteration 13, loss = 19652833865.55110931\n",
      "Iteration 14, loss = 19652823302.97292328\n",
      "Iteration 15, loss = 19652812651.85285187\n",
      "Iteration 16, loss = 19652801703.55297089\n",
      "Iteration 17, loss = 19652790585.15927887\n",
      "Iteration 18, loss = 19652779279.86614990\n",
      "Iteration 19, loss = 19652767375.99906540\n",
      "Iteration 20, loss = 19652755187.61461258\n",
      "Iteration 21, loss = 19652742785.00876236\n",
      "Iteration 22, loss = 19652730299.49724197\n",
      "Iteration 23, loss = 19652717694.10046387\n",
      "Iteration 24, loss = 19652705071.43713379\n",
      "Iteration 25, loss = 19652692432.99133682\n",
      "Iteration 26, loss = 19652679904.35791779\n",
      "Iteration 27, loss = 19652667612.10494232\n",
      "Iteration 28, loss = 19652655389.15862274\n",
      "Iteration 29, loss = 19652643637.37440491\n",
      "Iteration 30, loss = 19652632117.69447708\n",
      "Iteration 31, loss = 19652620743.32094955\n",
      "Iteration 32, loss = 19652609620.93162918\n",
      "Iteration 33, loss = 19652598551.78641510\n",
      "Iteration 34, loss = 19652587545.26227951\n",
      "Iteration 35, loss = 19652576429.99367142\n",
      "Iteration 36, loss = 19652565369.34584427\n",
      "Iteration 37, loss = 19652554274.55190277\n",
      "Iteration 38, loss = 19652543432.14077759\n",
      "Iteration 39, loss = 19652532645.73006058\n",
      "Iteration 40, loss = 19652521488.41577911\n",
      "Iteration 41, loss = 19652510611.19927979\n",
      "Iteration 42, loss = 19652499335.55312347\n",
      "Iteration 43, loss = 19652488476.37423706\n",
      "Iteration 44, loss = 19652477911.18862915\n",
      "Iteration 45, loss = 19652467844.27112961\n",
      "Iteration 46, loss = 19652458152.00603485\n",
      "Iteration 47, loss = 19652448864.46811676\n",
      "Iteration 48, loss = 19652439902.38653183\n",
      "Iteration 49, loss = 19652431199.34698105\n",
      "Iteration 50, loss = 19652422778.19872665\n",
      "Iteration 51, loss = 19652414361.42058563\n",
      "Iteration 52, loss = 19652406275.17071152\n",
      "Iteration 53, loss = 19652398227.47936630\n",
      "Iteration 54, loss = 19652390374.51655960\n",
      "Iteration 55, loss = 19652382794.34333420\n",
      "Iteration 56, loss = 19652375513.92644119\n",
      "Iteration 57, loss = 19652368401.96982956\n",
      "Iteration 58, loss = 19652361681.49969864\n",
      "Iteration 59, loss = 19652355060.61878586\n",
      "Iteration 60, loss = 19652348635.55082321\n",
      "Iteration 61, loss = 19652342054.36279297\n",
      "Iteration 62, loss = 19652335640.83733368\n",
      "Iteration 63, loss = 19652329208.93561172\n",
      "Iteration 64, loss = 19652322789.83055878\n",
      "Iteration 65, loss = 19652316537.88110733\n",
      "Iteration 66, loss = 19652310322.90406418\n",
      "Iteration 67, loss = 19652304325.20668030\n",
      "Iteration 68, loss = 19652298424.13221741\n",
      "Iteration 69, loss = 19652292732.02781677\n",
      "Iteration 70, loss = 19652287294.41540909\n",
      "Iteration 71, loss = 19652281991.35564423\n",
      "Iteration 72, loss = 19652276863.55744171\n",
      "Iteration 73, loss = 19652271731.72575378\n",
      "Iteration 74, loss = 19652266627.96238327\n",
      "Iteration 75, loss = 19652261580.53484726\n",
      "Iteration 76, loss = 19652256688.52764511\n",
      "Iteration 77, loss = 19652251860.49339676\n",
      "Iteration 78, loss = 19652247047.14751053\n",
      "Iteration 79, loss = 19652242327.55718613\n",
      "Iteration 80, loss = 19652237621.35221100\n",
      "Iteration 81, loss = 19652232967.16743088\n",
      "Iteration 82, loss = 19652228325.87347794\n",
      "Iteration 83, loss = 19652223699.37343597\n",
      "Iteration 84, loss = 19652219131.35398483\n",
      "Iteration 85, loss = 19652214578.30576324\n",
      "Iteration 86, loss = 19652210018.04364395\n",
      "Iteration 87, loss = 19652205517.61787033\n",
      "Iteration 88, loss = 19652201004.86918259\n",
      "Iteration 89, loss = 19652196433.77640152\n",
      "Iteration 90, loss = 19652191957.35915375\n",
      "Iteration 91, loss = 19652187422.77411270\n",
      "Iteration 92, loss = 19652182886.83387756\n",
      "Iteration 93, loss = 19652178365.49993134\n",
      "Iteration 94, loss = 19652173846.53265381\n",
      "Iteration 95, loss = 19652169383.46647263\n",
      "Iteration 96, loss = 19652164908.44266891\n",
      "Iteration 97, loss = 19652160455.63952637\n",
      "Iteration 98, loss = 19652155978.12696838\n",
      "Iteration 99, loss = 19652151496.63162613\n",
      "Iteration 100, loss = 19652147067.41389465\n",
      "Iteration 101, loss = 19652142586.54964447\n",
      "Iteration 102, loss = 19652138145.31576920\n",
      "Iteration 103, loss = 19652133674.40840530\n",
      "Iteration 104, loss = 19652129220.31010056\n",
      "Iteration 105, loss = 19652124768.26110077\n",
      "Iteration 106, loss = 19652120332.05804443\n",
      "Iteration 107, loss = 19652115823.88085938\n",
      "Iteration 108, loss = 19652111317.88478088\n",
      "Iteration 109, loss = 19652106827.86355591\n",
      "Iteration 110, loss = 19652102312.46718216\n",
      "Iteration 111, loss = 19652097836.29436493\n",
      "Iteration 112, loss = 19652093385.93275070\n",
      "Iteration 113, loss = 19652088981.17350769\n",
      "Iteration 114, loss = 19652084566.11931229\n",
      "Iteration 115, loss = 19652080190.07731247\n",
      "Iteration 116, loss = 19652075840.26098251\n",
      "Iteration 117, loss = 19652071510.05063248\n",
      "Iteration 118, loss = 19652067144.02474976\n",
      "Iteration 119, loss = 19652062750.60447693\n",
      "Iteration 120, loss = 19652058406.75438690\n",
      "Iteration 121, loss = 19652054078.70247650\n",
      "Iteration 122, loss = 19652049742.33863449\n",
      "Iteration 123, loss = 19652045471.69077301\n",
      "Iteration 124, loss = 19652041173.56964874\n",
      "Iteration 125, loss = 19652036971.42180252\n",
      "Iteration 126, loss = 19652032658.07539749\n",
      "Iteration 127, loss = 19652028433.81298828\n",
      "Iteration 128, loss = 19652024231.18358994\n",
      "Iteration 129, loss = 19652019980.00530243\n",
      "Iteration 130, loss = 19652015829.91806793\n",
      "Iteration 131, loss = 19652011621.84737015\n",
      "Iteration 132, loss = 19652007445.51670074\n",
      "Iteration 133, loss = 19652003234.42195129\n",
      "Iteration 134, loss = 19651999058.78366852\n",
      "Iteration 135, loss = 19651994924.51498413\n",
      "Iteration 136, loss = 19651990756.57826233\n",
      "Iteration 137, loss = 19651986613.32895279\n",
      "Iteration 138, loss = 19651982514.15080643\n",
      "Iteration 139, loss = 19651978329.14680862\n",
      "Iteration 140, loss = 19651974168.84467697\n",
      "Iteration 141, loss = 19651970047.38820648\n",
      "Iteration 142, loss = 19651965929.39102173\n",
      "Iteration 143, loss = 19651961782.03747177\n",
      "Iteration 144, loss = 19651957627.73355103\n",
      "Iteration 145, loss = 19651953540.07749939\n",
      "Iteration 146, loss = 19651949364.89554977\n",
      "Iteration 147, loss = 19651945219.80126190\n",
      "Iteration 148, loss = 19651941061.30556488\n",
      "Iteration 149, loss = 19651936911.98040771\n",
      "Iteration 150, loss = 19651932712.70719147\n",
      "Iteration 151, loss = 19651928524.24030304\n",
      "Iteration 152, loss = 19651924244.87036514\n",
      "Iteration 153, loss = 19651920008.46443176\n",
      "Iteration 154, loss = 19651915802.75695419\n",
      "Iteration 155, loss = 19651911517.31005096\n",
      "Iteration 156, loss = 19651907194.83835983\n",
      "Iteration 157, loss = 19651902952.06888962\n",
      "Iteration 158, loss = 19651898596.64025116\n",
      "Iteration 159, loss = 19651894272.43737793\n",
      "Iteration 160, loss = 19651889914.86061096\n",
      "Iteration 161, loss = 19651885553.83586884\n",
      "Iteration 162, loss = 19651881149.74043655\n",
      "Iteration 163, loss = 19651876740.97885513\n",
      "Iteration 164, loss = 19651872340.44076538\n",
      "Iteration 165, loss = 19651867987.97926331\n",
      "Iteration 166, loss = 19651863617.56925583\n",
      "Iteration 167, loss = 19651859237.58211136\n",
      "Iteration 168, loss = 19651854899.59771347\n",
      "Iteration 169, loss = 19651850542.91627502\n",
      "Iteration 170, loss = 19651846206.44852829\n",
      "Iteration 171, loss = 19651841902.69271851\n",
      "Iteration 172, loss = 19651837588.32645798\n",
      "Iteration 173, loss = 19651833300.28501129\n",
      "Iteration 174, loss = 19651828992.75067902\n",
      "Iteration 175, loss = 19651824712.14212418\n",
      "Iteration 176, loss = 19651820432.11096191\n",
      "Iteration 177, loss = 19651816198.54743195\n",
      "Iteration 178, loss = 19651811978.59598923\n",
      "Iteration 179, loss = 19651807701.06954956\n",
      "Iteration 180, loss = 19651803474.73870087\n",
      "Iteration 181, loss = 19651799245.72888565\n",
      "Iteration 182, loss = 19651795039.42991638\n",
      "Iteration 183, loss = 19651790876.86509323\n",
      "Iteration 184, loss = 19651786643.72244644\n",
      "Iteration 185, loss = 19651782468.74155045\n",
      "Iteration 186, loss = 19651778307.19367218\n",
      "Iteration 187, loss = 19651774097.37912750\n",
      "Iteration 188, loss = 19651769981.81394958\n",
      "Iteration 189, loss = 19651765754.36995697\n",
      "Iteration 190, loss = 19651761608.10688782\n",
      "Iteration 191, loss = 19651757501.30419159\n",
      "Iteration 192, loss = 19651753325.43577576\n",
      "Iteration 193, loss = 19651749195.60622406\n",
      "Iteration 194, loss = 19651745051.95794296\n",
      "Iteration 195, loss = 19651740929.07371521\n",
      "Iteration 196, loss = 19651736842.40989304\n",
      "Iteration 197, loss = 19651732675.52018738\n",
      "Iteration 198, loss = 19651728632.26252747\n",
      "Iteration 199, loss = 19651724474.28186035\n",
      "Iteration 200, loss = 19651720368.14640808\n",
      "Iteration 201, loss = 19651716259.30303574\n",
      "Iteration 202, loss = 19651712188.74247360\n",
      "Iteration 203, loss = 19651708059.16196060\n",
      "Iteration 204, loss = 19651704020.34334946\n",
      "Iteration 205, loss = 19651699949.16576767\n",
      "Iteration 206, loss = 19651695835.42686462\n",
      "Iteration 207, loss = 19651691763.69311905\n",
      "Iteration 208, loss = 19651687677.31074524\n",
      "Iteration 209, loss = 19651683663.21509171\n",
      "Iteration 210, loss = 19651679567.48092270\n",
      "Iteration 211, loss = 19651675510.43542480\n",
      "Iteration 212, loss = 19651671457.97706223\n",
      "Iteration 213, loss = 19651667396.43734741\n",
      "Iteration 214, loss = 19651663366.17938614\n",
      "Iteration 215, loss = 19651659273.15248489\n",
      "Iteration 216, loss = 19651655228.79831314\n",
      "Iteration 217, loss = 19651651223.77195358\n",
      "Iteration 218, loss = 19651647154.58943558\n",
      "Iteration 219, loss = 19651643140.39401627\n",
      "Iteration 220, loss = 19651639111.35369492\n",
      "Iteration 221, loss = 19651635102.57121277\n",
      "Iteration 222, loss = 19651631045.51416779\n",
      "Iteration 223, loss = 19651627004.26023865\n",
      "Iteration 224, loss = 19651623026.51800537\n",
      "Iteration 225, loss = 19651618972.46752548\n",
      "Iteration 226, loss = 19651614978.54547119\n",
      "Iteration 227, loss = 19651610961.60222626\n",
      "Iteration 228, loss = 19651606952.09329605\n",
      "Iteration 229, loss = 19651602914.89137268\n",
      "Iteration 230, loss = 19651598943.05204010\n",
      "Iteration 231, loss = 19651594925.37788773\n",
      "Iteration 232, loss = 19651590935.25817871\n",
      "Iteration 233, loss = 19651586909.73044968\n",
      "Iteration 234, loss = 19651582938.15670776\n",
      "Iteration 235, loss = 19651578945.73309708\n",
      "Iteration 236, loss = 19651574949.53186417\n",
      "Iteration 237, loss = 19651570992.27997971\n",
      "Iteration 238, loss = 19651566941.41701889\n",
      "Iteration 239, loss = 19651562981.97391891\n",
      "Iteration 240, loss = 19651559011.29722977\n",
      "Iteration 241, loss = 19651555026.78179169\n",
      "Iteration 242, loss = 19651551025.49349976\n",
      "Iteration 243, loss = 19651547034.39141464\n",
      "Iteration 244, loss = 19651543092.91510773\n",
      "Iteration 245, loss = 19651539112.28261185\n",
      "Iteration 246, loss = 19651535145.50572968\n",
      "Iteration 247, loss = 19651531181.15645218\n",
      "Iteration 248, loss = 19651527192.07830048\n",
      "Iteration 249, loss = 19651523225.63189316\n",
      "Iteration 250, loss = 19651519281.76125336\n",
      "Iteration 251, loss = 19651515303.79447937\n",
      "Iteration 252, loss = 19651511322.92032242\n",
      "Iteration 253, loss = 19651507425.86751556\n",
      "Iteration 254, loss = 19651503445.40706635\n",
      "Iteration 255, loss = 19651499498.90656662\n",
      "Iteration 256, loss = 19651495508.52436829\n",
      "Iteration 257, loss = 19651491594.84839249\n",
      "Iteration 258, loss = 19651487606.06315613\n",
      "Iteration 259, loss = 19651483648.63275909\n",
      "Iteration 260, loss = 19651479754.14598083\n",
      "Iteration 261, loss = 19651475781.04530716\n",
      "Iteration 262, loss = 19651471829.90970230\n",
      "Iteration 263, loss = 19651467890.57719040\n",
      "Iteration 264, loss = 19651463955.91077805\n",
      "Iteration 265, loss = 19651460005.07085800\n",
      "Iteration 266, loss = 19651456102.94965363\n",
      "Iteration 267, loss = 19651452119.80712891\n",
      "Iteration 268, loss = 19651448252.02471924\n",
      "Iteration 269, loss = 19651444286.07683563\n",
      "Iteration 270, loss = 19651440338.33861160\n",
      "Iteration 271, loss = 19651436413.02240753\n",
      "Iteration 272, loss = 19651432513.65147400\n",
      "Iteration 273, loss = 19651428602.40333557\n",
      "Iteration 274, loss = 19651424613.32374573\n",
      "Iteration 275, loss = 19651420685.14929199\n",
      "Iteration 276, loss = 19651416807.89855576\n",
      "Iteration 277, loss = 19651412890.46393967\n",
      "Iteration 278, loss = 19651408943.77358246\n",
      "Iteration 279, loss = 19651405037.63950348\n",
      "Iteration 280, loss = 19651401100.39333725\n",
      "Iteration 281, loss = 19651397184.82889557\n",
      "Iteration 282, loss = 19651393282.99910736\n",
      "Iteration 283, loss = 19651389380.94060516\n",
      "Iteration 284, loss = 19651385452.23266602\n",
      "Iteration 285, loss = 19651381511.78967285\n",
      "Iteration 286, loss = 19651377619.37517166\n",
      "Iteration 287, loss = 19651373741.97727203\n",
      "Iteration 288, loss = 19651369857.23851395\n",
      "Iteration 289, loss = 19651365902.38519287\n",
      "Iteration 290, loss = 19651362004.94830322\n",
      "Iteration 291, loss = 19651358094.06851959\n",
      "Iteration 292, loss = 19651354160.66703796\n",
      "Iteration 293, loss = 19651350284.63619995\n",
      "Iteration 294, loss = 19651346387.26192474\n",
      "Iteration 295, loss = 19651342470.00755310\n",
      "Iteration 296, loss = 19651338549.63227844\n",
      "Iteration 297, loss = 19651334657.05567932\n",
      "Iteration 298, loss = 19651330699.78506088\n",
      "Iteration 299, loss = 19651326844.68328094\n",
      "Iteration 300, loss = 19651322925.34550858\n",
      "Iteration 301, loss = 19651319043.76759720\n",
      "Iteration 302, loss = 19651315147.22595978\n",
      "Iteration 303, loss = 19651311241.21957016\n",
      "Iteration 304, loss = 19651307384.79681015\n",
      "Iteration 305, loss = 19651303427.12799072\n",
      "Iteration 306, loss = 19651299588.25596619\n",
      "Iteration 307, loss = 19651295662.96537781\n",
      "Iteration 308, loss = 19651291785.61470413\n",
      "Iteration 309, loss = 19651287885.62347412\n",
      "Iteration 310, loss = 19651284007.25624084\n",
      "Iteration 311, loss = 19651280102.44085693\n",
      "Iteration 312, loss = 19651276192.32946777\n",
      "Iteration 313, loss = 19651272374.02272797\n",
      "Iteration 314, loss = 19651268441.87085724\n",
      "Iteration 315, loss = 19651264576.51379395\n",
      "Iteration 316, loss = 19651260703.62245178\n",
      "Iteration 317, loss = 19651256802.21109009\n",
      "Iteration 318, loss = 19651252943.40784073\n",
      "Iteration 319, loss = 19651249057.17533493\n",
      "Iteration 320, loss = 19651245167.69600296\n",
      "Iteration 321, loss = 19651241290.77722931\n",
      "Iteration 322, loss = 19651237432.20740891\n",
      "Iteration 323, loss = 19651233554.82616806\n",
      "Iteration 324, loss = 19651229669.66217804\n",
      "Iteration 325, loss = 19651225790.89850616\n",
      "Iteration 326, loss = 19651221938.13346481\n",
      "Iteration 327, loss = 19651218073.73571014\n",
      "Iteration 328, loss = 19651214180.68095016\n",
      "Iteration 329, loss = 19651210326.93244171\n",
      "Iteration 330, loss = 19651206470.80719376\n",
      "Iteration 331, loss = 19651202579.24124908\n",
      "Iteration 332, loss = 19651198727.60258102\n",
      "Iteration 333, loss = 19651194847.81830215\n",
      "Iteration 334, loss = 19651190986.26463318\n",
      "Iteration 335, loss = 19651187111.95195770\n",
      "Iteration 336, loss = 19651183283.60054398\n",
      "Iteration 337, loss = 19651179366.81320572\n",
      "Iteration 338, loss = 19651175551.64033127\n",
      "Iteration 339, loss = 19651171682.32658005\n",
      "Iteration 340, loss = 19651167801.40087128\n",
      "Iteration 341, loss = 19651163952.39132690\n",
      "Iteration 342, loss = 19651160096.83390808\n",
      "Iteration 343, loss = 19651156235.93070602\n",
      "Iteration 344, loss = 19651152376.00899506\n",
      "Iteration 345, loss = 19651148520.97177124\n",
      "Iteration 346, loss = 19651144662.17096329\n",
      "Iteration 347, loss = 19651140823.37107086\n",
      "Iteration 348, loss = 19651136976.89812088\n",
      "Iteration 349, loss = 19651133081.16651154\n",
      "Iteration 350, loss = 19651129254.93109512\n",
      "Iteration 351, loss = 19651125433.08821869\n",
      "Iteration 352, loss = 19651121524.23764801\n",
      "Iteration 353, loss = 19651117705.98827744\n",
      "Iteration 354, loss = 19651113869.83845901\n",
      "Iteration 355, loss = 19651110007.16686630\n",
      "Iteration 356, loss = 19651106139.05844498\n",
      "Iteration 357, loss = 19651102319.98369217\n",
      "Iteration 358, loss = 19651098447.99465942\n",
      "Iteration 359, loss = 19651094618.24359894\n",
      "Iteration 360, loss = 19651090757.58562088\n",
      "Iteration 361, loss = 19651086930.83766556\n",
      "Iteration 362, loss = 19651083070.46472168\n",
      "Iteration 363, loss = 19651079232.57672882\n",
      "Iteration 364, loss = 19651075373.31385422\n",
      "Iteration 365, loss = 19651071539.43930435\n",
      "Iteration 366, loss = 19651067700.73177719\n",
      "Iteration 367, loss = 19651063895.74751663\n",
      "Iteration 368, loss = 19651060014.38939667\n",
      "Iteration 369, loss = 19651056182.81078720\n",
      "Iteration 370, loss = 19651052338.97704697\n",
      "Iteration 371, loss = 19651048507.40703201\n",
      "Iteration 372, loss = 19651044692.10681915\n",
      "Iteration 373, loss = 19651040828.34468079\n",
      "Iteration 374, loss = 19651036976.54867554\n",
      "Iteration 375, loss = 19651033142.29132080\n",
      "Iteration 376, loss = 19651029336.27478027\n",
      "Iteration 377, loss = 19651025488.26451111\n",
      "Iteration 378, loss = 19651021637.22324371\n",
      "Iteration 379, loss = 19651017810.00222397\n",
      "Iteration 380, loss = 19651013976.17976761\n",
      "Iteration 381, loss = 19651010140.27519608\n",
      "Iteration 382, loss = 19651006304.94480133\n",
      "Iteration 383, loss = 19651002464.79192734\n",
      "Iteration 384, loss = 19650998673.70905685\n",
      "Iteration 385, loss = 19650994805.43553162\n",
      "Iteration 386, loss = 19650990985.74526596\n",
      "Iteration 387, loss = 19650987164.49635696\n",
      "Iteration 388, loss = 19650983288.78291702\n",
      "Iteration 389, loss = 19650979479.53264999\n",
      "Iteration 390, loss = 19650975664.74214935\n",
      "Iteration 391, loss = 19650971809.65940857\n",
      "Iteration 392, loss = 19650968001.46735382\n",
      "Iteration 393, loss = 19650964193.76369095\n",
      "Iteration 394, loss = 19650960358.67006302\n",
      "Iteration 395, loss = 19650956501.27805710\n",
      "Iteration 396, loss = 19650952689.16388321\n",
      "Iteration 397, loss = 19650948893.38586807\n",
      "Iteration 398, loss = 19650945000.27208710\n",
      "Iteration 399, loss = 19650941195.37192917\n",
      "Iteration 400, loss = 19650937401.53465271\n",
      "Iteration 401, loss = 19650933589.54689407\n",
      "Iteration 402, loss = 19650929713.28183365\n",
      "Iteration 403, loss = 19650925912.64448547\n",
      "Iteration 404, loss = 19650922101.91222382\n",
      "Iteration 405, loss = 19650918282.22252655\n",
      "Iteration 406, loss = 19650914413.96942902\n",
      "Iteration 407, loss = 19650910637.54347610\n",
      "Iteration 408, loss = 19650906802.41446686\n",
      "Iteration 409, loss = 19650902937.90089417\n",
      "Iteration 410, loss = 19650899150.62556076\n",
      "Iteration 411, loss = 19650895331.11611176\n",
      "Iteration 412, loss = 19650891488.41076660\n",
      "Iteration 413, loss = 19650887685.79471588\n",
      "Iteration 414, loss = 19650883824.93173599\n",
      "Iteration 415, loss = 19650879988.02820587\n",
      "Iteration 416, loss = 19650876189.73555756\n",
      "Iteration 417, loss = 19650872342.80896759\n",
      "Iteration 418, loss = 19650868522.42216873\n",
      "Iteration 419, loss = 19650864681.39285278\n",
      "Iteration 420, loss = 19650860845.72866821\n",
      "Iteration 421, loss = 19650857026.12034607\n",
      "Iteration 422, loss = 19650853166.07687759\n",
      "Iteration 423, loss = 19650849286.19625092\n",
      "Iteration 424, loss = 19650845428.98989487\n",
      "Iteration 425, loss = 19650841571.24865723\n",
      "Iteration 426, loss = 19650837678.14127350\n",
      "Iteration 427, loss = 19650833766.46105957\n",
      "Iteration 428, loss = 19650829858.98918533\n",
      "Iteration 429, loss = 19650825837.41390991\n",
      "Iteration 430, loss = 19650821906.61765671\n",
      "Iteration 431, loss = 19650817823.15672302\n",
      "Iteration 432, loss = 19650813692.42104340\n",
      "Iteration 433, loss = 19650809564.57449722\n",
      "Iteration 434, loss = 19650805334.36265945\n",
      "Iteration 435, loss = 19650801016.07422638\n",
      "Iteration 436, loss = 19650796628.57055664\n",
      "Iteration 437, loss = 19650792224.35249329\n",
      "Iteration 438, loss = 19650787803.60424423\n",
      "Iteration 439, loss = 19650783326.73246384\n",
      "Iteration 440, loss = 19650778832.77934265\n",
      "Iteration 441, loss = 19650774400.79846573\n",
      "Iteration 442, loss = 19650769991.55756760\n",
      "Iteration 443, loss = 19650765531.42019653\n",
      "Iteration 444, loss = 19650761138.29830933\n",
      "Iteration 445, loss = 19650756808.65555573\n",
      "Iteration 446, loss = 19650752443.93021011\n",
      "Iteration 447, loss = 19650748130.34024811\n",
      "Iteration 448, loss = 19650743798.46396637\n",
      "Iteration 449, loss = 19650739529.59273148\n",
      "Iteration 450, loss = 19650735179.08842087\n",
      "Iteration 451, loss = 19650730945.46792603\n",
      "Iteration 452, loss = 19650726720.74092484\n",
      "Iteration 453, loss = 19650722447.32873535\n",
      "Iteration 454, loss = 19650718225.82036591\n",
      "Iteration 455, loss = 19650714028.41626740\n",
      "Iteration 456, loss = 19650709843.64743423\n",
      "Iteration 457, loss = 19650705581.19904327\n",
      "Iteration 458, loss = 19650701417.02450180\n",
      "Iteration 459, loss = 19650697247.28835297\n",
      "Iteration 460, loss = 19650693052.68978119\n",
      "Iteration 461, loss = 19650688910.13912201\n",
      "Iteration 462, loss = 19650684788.05737305\n",
      "Iteration 463, loss = 19650680629.72698212\n",
      "Iteration 464, loss = 19650676475.97460938\n",
      "Iteration 465, loss = 19650672318.41794968\n",
      "Iteration 466, loss = 19650668186.32408524\n",
      "Iteration 467, loss = 19650664070.55781937\n",
      "Iteration 468, loss = 19650659984.11347580\n",
      "Iteration 469, loss = 19650655822.18336487\n",
      "Iteration 470, loss = 19650651777.34141922\n",
      "Iteration 471, loss = 19650647658.65919495\n",
      "Iteration 472, loss = 19650643549.79817963\n",
      "Iteration 473, loss = 19650639407.97307205\n",
      "Iteration 474, loss = 19650635363.98566437\n",
      "Iteration 475, loss = 19650631239.81391144\n",
      "Iteration 476, loss = 19650627206.85070419\n",
      "Iteration 477, loss = 19650623098.49071121\n",
      "Iteration 478, loss = 19650618998.12998199\n",
      "Iteration 479, loss = 19650614940.15846252\n",
      "Iteration 480, loss = 19650610890.81188965\n",
      "Iteration 481, loss = 19650606817.93206406\n",
      "Iteration 482, loss = 19650602733.43257141\n",
      "Iteration 483, loss = 19650598702.95138168\n",
      "Iteration 484, loss = 19650594609.09946823\n",
      "Iteration 485, loss = 19650590585.32453918\n",
      "Iteration 486, loss = 19650586535.38795471\n",
      "Iteration 487, loss = 19650582468.55957794\n",
      "Iteration 488, loss = 19650578402.97879791\n",
      "Iteration 489, loss = 19650574371.32697296\n",
      "Iteration 490, loss = 19650570327.09395981\n",
      "Iteration 491, loss = 19650566312.79600906\n",
      "Iteration 492, loss = 19650562271.01513672\n",
      "Iteration 493, loss = 19650558229.62534714\n",
      "Iteration 494, loss = 19650554171.93767548\n",
      "Iteration 495, loss = 19650550117.63573456\n",
      "Iteration 496, loss = 19650546133.75590897\n",
      "Iteration 497, loss = 19650542084.66507339\n",
      "Iteration 498, loss = 19650538078.59170151\n",
      "Iteration 499, loss = 19650534015.22969437\n",
      "Iteration 500, loss = 19650530036.83340073\n",
      "Iteration 1, loss = 19435443452.59062576\n",
      "Iteration 2, loss = 19435431339.37545013\n",
      "Iteration 3, loss = 19435419418.29380417\n",
      "Iteration 4, loss = 19435407360.93401337\n",
      "Iteration 5, loss = 19435395520.63541412\n",
      "Iteration 6, loss = 19435383680.90744400\n",
      "Iteration 7, loss = 19435371809.33729172\n",
      "Iteration 8, loss = 19435359915.81117630\n",
      "Iteration 9, loss = 19435347862.37456512\n",
      "Iteration 10, loss = 19435335549.45896912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 19435323076.86759567\n",
      "Iteration 12, loss = 19435310300.62524414\n",
      "Iteration 13, loss = 19435296929.40905762\n",
      "Iteration 14, loss = 19435282933.93177795\n",
      "Iteration 15, loss = 19435268536.59079361\n",
      "Iteration 16, loss = 19435253826.18938065\n",
      "Iteration 17, loss = 19435239183.60539246\n",
      "Iteration 18, loss = 19435224128.59134674\n",
      "Iteration 19, loss = 19435208772.39807892\n",
      "Iteration 20, loss = 19435193473.95656204\n",
      "Iteration 21, loss = 19435177917.20681000\n",
      "Iteration 22, loss = 19435161944.25658035\n",
      "Iteration 23, loss = 19435145151.52419662\n",
      "Iteration 24, loss = 19435128515.49859238\n",
      "Iteration 25, loss = 19435111893.22410583\n",
      "Iteration 26, loss = 19435094912.93298721\n",
      "Iteration 27, loss = 19435077722.90287018\n",
      "Iteration 28, loss = 19435061097.27845001\n",
      "Iteration 29, loss = 19435044824.65966034\n",
      "Iteration 30, loss = 19435028900.18770981\n",
      "Iteration 31, loss = 19435013509.27238083\n",
      "Iteration 32, loss = 19434998865.19968796\n",
      "Iteration 33, loss = 19434984997.02602005\n",
      "Iteration 34, loss = 19434971998.21063995\n",
      "Iteration 35, loss = 19434959525.22796631\n",
      "Iteration 36, loss = 19434947730.25716400\n",
      "Iteration 37, loss = 19434936119.82902527\n",
      "Iteration 38, loss = 19434925393.45839310\n",
      "Iteration 39, loss = 19434915165.89250946\n",
      "Iteration 40, loss = 19434904850.25212479\n",
      "Iteration 41, loss = 19434894798.95549011\n",
      "Iteration 42, loss = 19434884827.40325165\n",
      "Iteration 43, loss = 19434874820.64910507\n",
      "Iteration 44, loss = 19434865219.58644867\n",
      "Iteration 45, loss = 19434855954.83404541\n",
      "Iteration 46, loss = 19434846963.19973373\n",
      "Iteration 47, loss = 19434838443.37354660\n",
      "Iteration 48, loss = 19434830294.62422562\n",
      "Iteration 49, loss = 19434822484.61746597\n",
      "Iteration 50, loss = 19434814801.65039444\n",
      "Iteration 51, loss = 19434807123.21622086\n",
      "Iteration 52, loss = 19434799553.91344833\n",
      "Iteration 53, loss = 19434792131.95121002\n",
      "Iteration 54, loss = 19434784969.02031708\n",
      "Iteration 55, loss = 19434777925.54328156\n",
      "Iteration 56, loss = 19434770939.61509705\n",
      "Iteration 57, loss = 19434764113.72418976\n",
      "Iteration 58, loss = 19434757468.27642441\n",
      "Iteration 59, loss = 19434750821.93441391\n",
      "Iteration 60, loss = 19434744262.60502625\n",
      "Iteration 61, loss = 19434737788.72369766\n",
      "Iteration 62, loss = 19434731338.14019012\n",
      "Iteration 63, loss = 19434724911.17380524\n",
      "Iteration 64, loss = 19434718563.17471695\n",
      "Iteration 65, loss = 19434712240.52713776\n",
      "Iteration 66, loss = 19434705937.13586426\n",
      "Iteration 67, loss = 19434699673.72979355\n",
      "Iteration 68, loss = 19434693379.31298065\n",
      "Iteration 69, loss = 19434687076.88538742\n",
      "Iteration 70, loss = 19434680718.40686798\n",
      "Iteration 71, loss = 19434674497.81271362\n",
      "Iteration 72, loss = 19434668306.69120026\n",
      "Iteration 73, loss = 19434662208.11634064\n",
      "Iteration 74, loss = 19434656147.02001190\n",
      "Iteration 75, loss = 19434650218.21139908\n",
      "Iteration 76, loss = 19434644103.05166626\n",
      "Iteration 77, loss = 19434638130.63287735\n",
      "Iteration 78, loss = 19434632114.50772095\n",
      "Iteration 79, loss = 19434626255.81718063\n",
      "Iteration 80, loss = 19434620359.94246674\n",
      "Iteration 81, loss = 19434614555.79702759\n",
      "Iteration 82, loss = 19434608756.51607132\n",
      "Iteration 83, loss = 19434602990.91935730\n",
      "Iteration 84, loss = 19434597318.38107300\n",
      "Iteration 85, loss = 19434591552.91902542\n",
      "Iteration 86, loss = 19434585863.26374054\n",
      "Iteration 87, loss = 19434580186.37839127\n",
      "Iteration 88, loss = 19434574571.75842285\n",
      "Iteration 89, loss = 19434568877.26171494\n",
      "Iteration 90, loss = 19434563262.42940140\n",
      "Iteration 91, loss = 19434557646.85491562\n",
      "Iteration 92, loss = 19434552022.61449814\n",
      "Iteration 93, loss = 19434546384.85168076\n",
      "Iteration 94, loss = 19434540809.34175873\n",
      "Iteration 95, loss = 19434535187.04872131\n",
      "Iteration 96, loss = 19434529634.16556931\n",
      "Iteration 97, loss = 19434524122.18250656\n",
      "Iteration 98, loss = 19434518616.10899353\n",
      "Iteration 99, loss = 19434513112.89774704\n",
      "Iteration 100, loss = 19434507644.66840363\n",
      "Iteration 101, loss = 19434502157.72222519\n",
      "Iteration 102, loss = 19434496708.93212128\n",
      "Iteration 103, loss = 19434491256.03646469\n",
      "Iteration 104, loss = 19434485810.15118790\n",
      "Iteration 105, loss = 19434480424.27814102\n",
      "Iteration 106, loss = 19434474998.75598145\n",
      "Iteration 107, loss = 19434469620.40959167\n",
      "Iteration 108, loss = 19434464215.37851715\n",
      "Iteration 109, loss = 19434458831.39788055\n",
      "Iteration 110, loss = 19434453469.30148315\n",
      "Iteration 111, loss = 19434448133.80452347\n",
      "Iteration 112, loss = 19434442719.43973541\n",
      "Iteration 113, loss = 19434437422.02891922\n",
      "Iteration 114, loss = 19434432072.10591888\n",
      "Iteration 115, loss = 19434426687.47862244\n",
      "Iteration 116, loss = 19434421316.41516876\n",
      "Iteration 117, loss = 19434415994.92594147\n",
      "Iteration 118, loss = 19434410669.29891205\n",
      "Iteration 119, loss = 19434405346.33423233\n",
      "Iteration 120, loss = 19434400067.98648453\n",
      "Iteration 121, loss = 19434394752.02365112\n",
      "Iteration 122, loss = 19434389520.14923096\n",
      "Iteration 123, loss = 19434384253.54657745\n",
      "Iteration 124, loss = 19434378959.88404465\n",
      "Iteration 125, loss = 19434373737.59434891\n",
      "Iteration 126, loss = 19434368525.94248962\n",
      "Iteration 127, loss = 19434363259.70230865\n",
      "Iteration 128, loss = 19434358061.33484268\n",
      "Iteration 129, loss = 19434352848.43634796\n",
      "Iteration 130, loss = 19434347586.67313004\n",
      "Iteration 131, loss = 19434342422.70870590\n",
      "Iteration 132, loss = 19434337174.47610092\n",
      "Iteration 133, loss = 19434331973.44029999\n",
      "Iteration 134, loss = 19434326800.68132782\n",
      "Iteration 135, loss = 19434321536.19819260\n",
      "Iteration 136, loss = 19434316301.33996582\n",
      "Iteration 137, loss = 19434311124.26720810\n",
      "Iteration 138, loss = 19434305968.81480408\n",
      "Iteration 139, loss = 19434300705.23021317\n",
      "Iteration 140, loss = 19434295544.09316254\n",
      "Iteration 141, loss = 19434290363.84400177\n",
      "Iteration 142, loss = 19434285168.81259918\n",
      "Iteration 143, loss = 19434279994.44922256\n",
      "Iteration 144, loss = 19434274804.23019028\n",
      "Iteration 145, loss = 19434269658.33978271\n",
      "Iteration 146, loss = 19434264447.33085632\n",
      "Iteration 147, loss = 19434259263.61933136\n",
      "Iteration 148, loss = 19434254083.13376236\n",
      "Iteration 149, loss = 19434248924.67082977\n",
      "Iteration 150, loss = 19434243707.01944733\n",
      "Iteration 151, loss = 19434238581.11656189\n",
      "Iteration 152, loss = 19434233425.10105896\n",
      "Iteration 153, loss = 19434228301.34316254\n",
      "Iteration 154, loss = 19434223124.13729477\n",
      "Iteration 155, loss = 19434217974.82272720\n",
      "Iteration 156, loss = 19434212848.29301834\n",
      "Iteration 157, loss = 19434207676.62733078\n",
      "Iteration 158, loss = 19434202532.66856766\n",
      "Iteration 159, loss = 19434197382.77214432\n",
      "Iteration 160, loss = 19434192153.82341385\n",
      "Iteration 161, loss = 19434187010.00701904\n",
      "Iteration 162, loss = 19434181786.28288651\n",
      "Iteration 163, loss = 19434176596.33224487\n",
      "Iteration 164, loss = 19434171367.13327789\n",
      "Iteration 165, loss = 19434166081.05484772\n",
      "Iteration 166, loss = 19434160781.48258591\n",
      "Iteration 167, loss = 19434155447.10257339\n",
      "Iteration 168, loss = 19434150109.74689865\n",
      "Iteration 169, loss = 19434144681.47154617\n",
      "Iteration 170, loss = 19434139247.52445984\n",
      "Iteration 171, loss = 19434133728.80310440\n",
      "Iteration 172, loss = 19434128196.96239471\n",
      "Iteration 173, loss = 19434122665.91471100\n",
      "Iteration 174, loss = 19434117052.74583817\n",
      "Iteration 175, loss = 19434111482.32320786\n",
      "Iteration 176, loss = 19434105918.75973129\n",
      "Iteration 177, loss = 19434100344.02818298\n",
      "Iteration 178, loss = 19434094787.31434250\n",
      "Iteration 179, loss = 19434089212.93016434\n",
      "Iteration 180, loss = 19434083691.38653183\n",
      "Iteration 181, loss = 19434078215.81806183\n",
      "Iteration 182, loss = 19434072649.71830750\n",
      "Iteration 183, loss = 19434067197.38112640\n",
      "Iteration 184, loss = 19434061719.98106003\n",
      "Iteration 185, loss = 19434056256.26662827\n",
      "Iteration 186, loss = 19434050763.68777847\n",
      "Iteration 187, loss = 19434045323.52092361\n",
      "Iteration 188, loss = 19434039897.37380981\n",
      "Iteration 189, loss = 19434034444.61901474\n",
      "Iteration 190, loss = 19434029053.48242950\n",
      "Iteration 191, loss = 19434023636.28744888\n",
      "Iteration 192, loss = 19434018260.62498474\n",
      "Iteration 193, loss = 19434012868.42540359\n",
      "Iteration 194, loss = 19434007486.30566025\n",
      "Iteration 195, loss = 19434002152.81270599\n",
      "Iteration 196, loss = 19433996828.94066238\n",
      "Iteration 197, loss = 19433991464.63872528\n",
      "Iteration 198, loss = 19433986140.81416702\n",
      "Iteration 199, loss = 19433980823.46931839\n",
      "Iteration 200, loss = 19433975529.40101624\n",
      "Iteration 201, loss = 19433970215.81496811\n",
      "Iteration 202, loss = 19433964876.32344437\n",
      "Iteration 203, loss = 19433959595.03584290\n",
      "Iteration 204, loss = 19433954241.45981979\n",
      "Iteration 205, loss = 19433948994.29661179\n",
      "Iteration 206, loss = 19433943685.23231888\n",
      "Iteration 207, loss = 19433938413.11019135\n",
      "Iteration 208, loss = 19433933083.78414154\n",
      "Iteration 209, loss = 19433927853.88139725\n",
      "Iteration 210, loss = 19433922564.82004929\n",
      "Iteration 211, loss = 19433917295.59557343\n",
      "Iteration 212, loss = 19433911982.33665085\n",
      "Iteration 213, loss = 19433906739.65886307\n",
      "Iteration 214, loss = 19433901453.25040054\n",
      "Iteration 215, loss = 19433896187.58234024\n",
      "Iteration 216, loss = 19433890914.49507904\n",
      "Iteration 217, loss = 19433885694.16191483\n",
      "Iteration 218, loss = 19433880406.44994354\n",
      "Iteration 219, loss = 19433875161.22540283\n",
      "Iteration 220, loss = 19433869888.00222778\n",
      "Iteration 221, loss = 19433864649.10067368\n",
      "Iteration 222, loss = 19433859364.79381561\n",
      "Iteration 223, loss = 19433854167.16054535\n",
      "Iteration 224, loss = 19433848930.68763351\n",
      "Iteration 225, loss = 19433843670.50453568\n",
      "Iteration 226, loss = 19433838442.86575317\n",
      "Iteration 227, loss = 19433833221.24740601\n",
      "Iteration 228, loss = 19433828019.99892807\n",
      "Iteration 229, loss = 19433822790.00104904\n",
      "Iteration 230, loss = 19433817602.05233383\n",
      "Iteration 231, loss = 19433812395.33469772\n",
      "Iteration 232, loss = 19433807177.54269028\n",
      "Iteration 233, loss = 19433801981.24980927\n",
      "Iteration 234, loss = 19433796848.42577744\n",
      "Iteration 235, loss = 19433791610.13192368\n",
      "Iteration 236, loss = 19433786407.79612350\n",
      "Iteration 237, loss = 19433781266.00444794\n",
      "Iteration 238, loss = 19433776038.22233963\n",
      "Iteration 239, loss = 19433770864.63013077\n",
      "Iteration 240, loss = 19433765666.52852631\n",
      "Iteration 241, loss = 19433760494.28036499\n",
      "Iteration 242, loss = 19433755314.96180344\n",
      "Iteration 243, loss = 19433750083.84136581\n",
      "Iteration 244, loss = 19433744953.78964233\n",
      "Iteration 245, loss = 19433739755.12754059\n",
      "Iteration 246, loss = 19433734599.62899399\n",
      "Iteration 247, loss = 19433729395.48072815\n",
      "Iteration 248, loss = 19433724255.96682358\n",
      "Iteration 249, loss = 19433719033.46517181\n",
      "Iteration 250, loss = 19433713875.56001663\n",
      "Iteration 251, loss = 19433708662.63101196\n",
      "Iteration 252, loss = 19433703558.49540710\n",
      "Iteration 253, loss = 19433698310.91761017\n",
      "Iteration 254, loss = 19433693146.88374329\n",
      "Iteration 255, loss = 19433688010.05321884\n",
      "Iteration 256, loss = 19433682821.80917358\n",
      "Iteration 257, loss = 19433677633.93737411\n",
      "Iteration 258, loss = 19433672474.71456146\n",
      "Iteration 259, loss = 19433667323.52411652\n",
      "Iteration 260, loss = 19433662216.51823807\n",
      "Iteration 261, loss = 19433657068.27878189\n",
      "Iteration 262, loss = 19433651872.50531006\n",
      "Iteration 263, loss = 19433646735.58194351\n",
      "Iteration 264, loss = 19433641658.50762177\n",
      "Iteration 265, loss = 19433636510.83592224\n",
      "Iteration 266, loss = 19433631361.39069366\n",
      "Iteration 267, loss = 19433626225.04509735\n",
      "Iteration 268, loss = 19433621136.47896576\n",
      "Iteration 269, loss = 19433615947.12571716\n",
      "Iteration 270, loss = 19433610827.12921906\n",
      "Iteration 271, loss = 19433605678.22294998\n",
      "Iteration 272, loss = 19433600587.40088272\n",
      "Iteration 273, loss = 19433595425.47991943\n",
      "Iteration 274, loss = 19433590315.87712860\n",
      "Iteration 275, loss = 19433585177.69512939\n",
      "Iteration 276, loss = 19433580080.68279266\n",
      "Iteration 277, loss = 19433574952.20515442\n",
      "Iteration 278, loss = 19433569802.22767639\n",
      "Iteration 279, loss = 19433564729.64119339\n",
      "Iteration 280, loss = 19433559594.42168808\n",
      "Iteration 281, loss = 19433554442.70122528\n",
      "Iteration 282, loss = 19433549357.81314850\n",
      "Iteration 283, loss = 19433544207.35781097\n",
      "Iteration 284, loss = 19433539130.29868317\n",
      "Iteration 285, loss = 19433533965.03339386\n",
      "Iteration 286, loss = 19433528859.05426407\n",
      "Iteration 287, loss = 19433523798.02615356\n",
      "Iteration 288, loss = 19433518705.34251785\n",
      "Iteration 289, loss = 19433513591.83650970\n",
      "Iteration 290, loss = 19433508428.09703827\n",
      "Iteration 291, loss = 19433503374.23952484\n",
      "Iteration 292, loss = 19433498300.56429291\n",
      "Iteration 293, loss = 19433493144.12599182\n",
      "Iteration 294, loss = 19433488053.52958298\n",
      "Iteration 295, loss = 19433482980.85251617\n",
      "Iteration 296, loss = 19433477902.55891037\n",
      "Iteration 297, loss = 19433472769.51940155\n",
      "Iteration 298, loss = 19433467683.59859848\n",
      "Iteration 299, loss = 19433462629.11444092\n",
      "Iteration 300, loss = 19433457516.02571106\n",
      "Iteration 301, loss = 19433452464.14341354\n",
      "Iteration 302, loss = 19433447354.93802643\n",
      "Iteration 303, loss = 19433442265.40201569\n",
      "Iteration 304, loss = 19433437201.48291016\n",
      "Iteration 305, loss = 19433432103.63227844\n",
      "Iteration 306, loss = 19433427019.14641571\n",
      "Iteration 307, loss = 19433421941.93856430\n",
      "Iteration 308, loss = 19433416874.41977310\n",
      "Iteration 309, loss = 19433411744.81943893\n",
      "Iteration 310, loss = 19433406690.82701492\n",
      "Iteration 311, loss = 19433401615.90956116\n",
      "Iteration 312, loss = 19433396557.85493851\n",
      "Iteration 313, loss = 19433391454.84798813\n",
      "Iteration 314, loss = 19433386363.15111923\n",
      "Iteration 315, loss = 19433381319.77046967\n",
      "Iteration 316, loss = 19433376215.60730362\n",
      "Iteration 317, loss = 19433371196.96394348\n",
      "Iteration 318, loss = 19433366099.27643585\n",
      "Iteration 319, loss = 19433361042.11461639\n",
      "Iteration 320, loss = 19433356000.53089142\n",
      "Iteration 321, loss = 19433350910.66833496\n",
      "Iteration 322, loss = 19433345876.66542816\n",
      "Iteration 323, loss = 19433340803.96957016\n",
      "Iteration 324, loss = 19433335717.72670364\n",
      "Iteration 325, loss = 19433330650.24295807\n",
      "Iteration 326, loss = 19433325570.80421448\n",
      "Iteration 327, loss = 19433320518.65811539\n",
      "Iteration 328, loss = 19433315478.41695404\n",
      "Iteration 329, loss = 19433310378.41757202\n",
      "Iteration 330, loss = 19433305334.41712189\n",
      "Iteration 331, loss = 19433300265.45997238\n",
      "Iteration 332, loss = 19433295175.64387131\n",
      "Iteration 333, loss = 19433290116.23572159\n",
      "Iteration 334, loss = 19433285071.67290878\n",
      "Iteration 335, loss = 19433280005.16308212\n",
      "Iteration 336, loss = 19433274925.16772842\n",
      "Iteration 337, loss = 19433269874.46551895\n",
      "Iteration 338, loss = 19433264794.55609131\n",
      "Iteration 339, loss = 19433259724.96990967\n",
      "Iteration 340, loss = 19433254628.75762939\n",
      "Iteration 341, loss = 19433249614.61401749\n",
      "Iteration 342, loss = 19433244550.15864944\n",
      "Iteration 343, loss = 19433239461.36697388\n",
      "Iteration 344, loss = 19433234377.95290375\n",
      "Iteration 345, loss = 19433229337.94389343\n",
      "Iteration 346, loss = 19433224242.37269592\n",
      "Iteration 347, loss = 19433219250.19211960\n",
      "Iteration 348, loss = 19433214154.47438812\n",
      "Iteration 349, loss = 19433209121.12764740\n",
      "Iteration 350, loss = 19433204072.76546478\n",
      "Iteration 351, loss = 19433199010.17990875\n",
      "Iteration 352, loss = 19433193981.11187363\n",
      "Iteration 353, loss = 19433188950.93811417\n",
      "Iteration 354, loss = 19433183866.58471680\n",
      "Iteration 355, loss = 19433178808.32876968\n",
      "Iteration 356, loss = 19433173773.53233719\n",
      "Iteration 357, loss = 19433168765.41750717\n",
      "Iteration 358, loss = 19433163645.76203156\n",
      "Iteration 359, loss = 19433158666.99670029\n",
      "Iteration 360, loss = 19433153550.91841888\n",
      "Iteration 361, loss = 19433148511.52233505\n",
      "Iteration 362, loss = 19433143461.17959213\n",
      "Iteration 363, loss = 19433138429.54051208\n",
      "Iteration 364, loss = 19433133440.81192398\n",
      "Iteration 365, loss = 19433128364.17847061\n",
      "Iteration 366, loss = 19433123332.85447693\n",
      "Iteration 367, loss = 19433118325.23751831\n",
      "Iteration 368, loss = 19433113299.49849701\n",
      "Iteration 369, loss = 19433108252.15525818\n",
      "Iteration 370, loss = 19433103227.97288895\n",
      "Iteration 371, loss = 19433098193.20318604\n",
      "Iteration 372, loss = 19433093152.95491791\n",
      "Iteration 373, loss = 19433088111.26031113\n",
      "Iteration 374, loss = 19433083085.59849167\n",
      "Iteration 375, loss = 19433078075.08323288\n",
      "Iteration 376, loss = 19433072991.88062668\n",
      "Iteration 377, loss = 19433068003.45043182\n",
      "Iteration 378, loss = 19433062967.01568222\n",
      "Iteration 379, loss = 19433057947.40223312\n",
      "Iteration 380, loss = 19433052928.95421982\n",
      "Iteration 381, loss = 19433047910.41426468\n",
      "Iteration 382, loss = 19433042905.80501938\n",
      "Iteration 383, loss = 19433037896.91423416\n",
      "Iteration 384, loss = 19433032864.36299515\n",
      "Iteration 385, loss = 19433027827.81071091\n",
      "Iteration 386, loss = 19433022844.41283035\n",
      "Iteration 387, loss = 19433017847.44456863\n",
      "Iteration 388, loss = 19433012798.25920486\n",
      "Iteration 389, loss = 19433007793.36614227\n",
      "Iteration 390, loss = 19433002776.45203018\n",
      "Iteration 391, loss = 19432997732.77197647\n",
      "Iteration 392, loss = 19432992778.30928040\n",
      "Iteration 393, loss = 19432987723.82186508\n",
      "Iteration 394, loss = 19432982707.83770752\n",
      "Iteration 395, loss = 19432977687.17344666\n",
      "Iteration 396, loss = 19432972666.16662598\n",
      "Iteration 397, loss = 19432967632.40758514\n",
      "Iteration 398, loss = 19432962636.48505783\n",
      "Iteration 399, loss = 19432957657.45840073\n",
      "Iteration 400, loss = 19432952610.18255997\n",
      "Iteration 401, loss = 19432947633.64918900\n",
      "Iteration 402, loss = 19432942596.31943512\n",
      "Iteration 403, loss = 19432937582.68667984\n",
      "Iteration 404, loss = 19432932570.57887650\n",
      "Iteration 405, loss = 19432927507.66979218\n",
      "Iteration 406, loss = 19432922495.86391068\n",
      "Iteration 407, loss = 19432917488.83673096\n",
      "Iteration 408, loss = 19432912426.81441498\n",
      "Iteration 409, loss = 19432907447.49135208\n",
      "Iteration 410, loss = 19432902374.46464920\n",
      "Iteration 411, loss = 19432897362.52992249\n",
      "Iteration 412, loss = 19432892318.06633377\n",
      "Iteration 413, loss = 19432887290.86374664\n",
      "Iteration 414, loss = 19432882286.64026642\n",
      "Iteration 415, loss = 19432877258.35923386\n",
      "Iteration 416, loss = 19432872212.89888763\n",
      "Iteration 417, loss = 19432867189.92593384\n",
      "Iteration 418, loss = 19432862159.46718216\n",
      "Iteration 419, loss = 19432857181.84365082\n",
      "Iteration 420, loss = 19432852157.16675949\n",
      "Iteration 421, loss = 19432847172.09374237\n",
      "Iteration 422, loss = 19432842178.96654129\n",
      "Iteration 423, loss = 19432837135.34388733\n",
      "Iteration 424, loss = 19432832158.79586029\n",
      "Iteration 425, loss = 19432827128.31773758\n",
      "Iteration 426, loss = 19432822141.93262100\n",
      "Iteration 427, loss = 19432817106.78906631\n",
      "Iteration 428, loss = 19432812122.70162964\n",
      "Iteration 429, loss = 19432807108.67866516\n",
      "Iteration 430, loss = 19432802104.29535675\n",
      "Iteration 431, loss = 19432797070.94869995\n",
      "Iteration 432, loss = 19432792077.78755188\n",
      "Iteration 433, loss = 19432787076.60430908\n",
      "Iteration 434, loss = 19432782043.06483459\n",
      "Iteration 435, loss = 19432777051.27709579\n",
      "Iteration 436, loss = 19432772074.18688202\n",
      "Iteration 437, loss = 19432767028.98067093\n",
      "Iteration 438, loss = 19432762079.92808533\n",
      "Iteration 439, loss = 19432757031.22629929\n",
      "Iteration 440, loss = 19432752028.35871124\n",
      "Iteration 441, loss = 19432747036.57975006\n",
      "Iteration 442, loss = 19432742028.37816238\n",
      "Iteration 443, loss = 19432737042.65500641\n",
      "Iteration 444, loss = 19432732035.42982101\n",
      "Iteration 445, loss = 19432727000.15068436\n",
      "Iteration 446, loss = 19432722034.36676407\n",
      "Iteration 447, loss = 19432717032.75645065\n",
      "Iteration 448, loss = 19432712058.07477188\n",
      "Iteration 449, loss = 19432707038.49447632\n",
      "Iteration 450, loss = 19432702017.62649155\n",
      "Iteration 451, loss = 19432697096.00098801\n",
      "Iteration 452, loss = 19432692057.50883865\n",
      "Iteration 453, loss = 19432687092.12089920\n",
      "Iteration 454, loss = 19432682074.95789337\n",
      "Iteration 455, loss = 19432677084.92625427\n",
      "Iteration 456, loss = 19432672090.39282990\n",
      "Iteration 457, loss = 19432667068.06079483\n",
      "Iteration 458, loss = 19432662095.45893478\n",
      "Iteration 459, loss = 19432657073.10994720\n",
      "Iteration 460, loss = 19432652096.66429520\n",
      "Iteration 461, loss = 19432647120.60178375\n",
      "Iteration 462, loss = 19432642093.28868484\n",
      "Iteration 463, loss = 19432637114.00445175\n",
      "Iteration 464, loss = 19432632067.34754562\n",
      "Iteration 465, loss = 19432627079.24032593\n",
      "Iteration 466, loss = 19432622118.73105240\n",
      "Iteration 467, loss = 19432617093.29317093\n",
      "Iteration 468, loss = 19432612156.44101334\n",
      "Iteration 469, loss = 19432607144.66748047\n",
      "Iteration 470, loss = 19432602119.14595032\n",
      "Iteration 471, loss = 19432597143.27422333\n",
      "Iteration 472, loss = 19432592183.77757645\n",
      "Iteration 473, loss = 19432587160.37454605\n",
      "Iteration 474, loss = 19432582213.78833771\n",
      "Iteration 475, loss = 19432577203.76247025\n",
      "Iteration 476, loss = 19432572246.25019073\n",
      "Iteration 477, loss = 19432567264.79099655\n",
      "Iteration 478, loss = 19432562261.95719147\n",
      "Iteration 479, loss = 19432557308.05159760\n",
      "Iteration 480, loss = 19432552307.83263016\n",
      "Iteration 481, loss = 19432547316.05277252\n",
      "Iteration 482, loss = 19432542343.26261139\n",
      "Iteration 483, loss = 19432537381.67185211\n",
      "Iteration 484, loss = 19432532373.31825256\n",
      "Iteration 485, loss = 19432527408.02826309\n",
      "Iteration 486, loss = 19432522391.19731903\n",
      "Iteration 487, loss = 19432517396.31687164\n",
      "Iteration 488, loss = 19432512441.04910278\n",
      "Iteration 489, loss = 19432507438.66736603\n",
      "Iteration 490, loss = 19432502470.41756821\n",
      "Iteration 491, loss = 19432497489.40515900\n",
      "Iteration 492, loss = 19432492508.67839432\n",
      "Iteration 493, loss = 19432487558.24163055\n",
      "Iteration 494, loss = 19432482580.70959854\n",
      "Iteration 495, loss = 19432477588.28892136\n",
      "Iteration 496, loss = 19432472599.89149094\n",
      "Iteration 497, loss = 19432467629.00048447\n",
      "Iteration 498, loss = 19432462688.81298447\n",
      "Iteration 499, loss = 19432457608.67639542\n",
      "Iteration 500, loss = 19432452678.05548477\n",
      "Iteration 1, loss = 19129035575.56779861\n",
      "Iteration 2, loss = 19129019602.46702957\n",
      "Iteration 3, loss = 19129003627.24793625\n",
      "Iteration 4, loss = 19128987639.20104218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 19128971477.56197739\n",
      "Iteration 6, loss = 19128955114.21975708\n",
      "Iteration 7, loss = 19128938451.66409683\n",
      "Iteration 8, loss = 19128921327.31320572\n",
      "Iteration 9, loss = 19128903707.18842697\n",
      "Iteration 10, loss = 19128885603.04390335\n",
      "Iteration 11, loss = 19128866971.73616028\n",
      "Iteration 12, loss = 19128847500.70093155\n",
      "Iteration 13, loss = 19128828049.51852798\n",
      "Iteration 14, loss = 19128808696.50525665\n",
      "Iteration 15, loss = 19128789015.11434174\n",
      "Iteration 16, loss = 19128769306.28247452\n",
      "Iteration 17, loss = 19128749325.55341721\n",
      "Iteration 18, loss = 19128729378.97643280\n",
      "Iteration 19, loss = 19128709132.05474091\n",
      "Iteration 20, loss = 19128687555.82604599\n",
      "Iteration 21, loss = 19128664833.52944946\n",
      "Iteration 22, loss = 19128643020.56500244\n",
      "Iteration 23, loss = 19128622136.26401520\n",
      "Iteration 24, loss = 19128601198.11251068\n",
      "Iteration 25, loss = 19128578670.58069611\n",
      "Iteration 26, loss = 19128556656.07390976\n",
      "Iteration 27, loss = 19128536914.29952240\n",
      "Iteration 28, loss = 19128517312.73073959\n",
      "Iteration 29, loss = 19128499406.28157425\n",
      "Iteration 30, loss = 19128482695.03308487\n",
      "Iteration 31, loss = 19128465456.61803436\n",
      "Iteration 32, loss = 19128447891.70791245\n",
      "Iteration 33, loss = 19128429984.07941437\n",
      "Iteration 34, loss = 19128413279.01242447\n",
      "Iteration 35, loss = 19128398535.86735916\n",
      "Iteration 36, loss = 19128384283.71491623\n",
      "Iteration 37, loss = 19128371580.99041748\n",
      "Iteration 38, loss = 19128360236.78142548\n",
      "Iteration 39, loss = 19128349820.06837463\n",
      "Iteration 40, loss = 19128340091.46151733\n",
      "Iteration 41, loss = 19128330618.55398560\n",
      "Iteration 42, loss = 19128321231.84430695\n",
      "Iteration 43, loss = 19128312105.37755203\n",
      "Iteration 44, loss = 19128303325.05027771\n",
      "Iteration 45, loss = 19128294580.13362885\n",
      "Iteration 46, loss = 19128285888.19592285\n",
      "Iteration 47, loss = 19128277180.94347763\n",
      "Iteration 48, loss = 19128268746.82115173\n",
      "Iteration 49, loss = 19128260511.90635300\n",
      "Iteration 50, loss = 19128252384.04852295\n",
      "Iteration 51, loss = 19128244322.10019302\n",
      "Iteration 52, loss = 19128236338.56697464\n",
      "Iteration 53, loss = 19128228117.35966873\n",
      "Iteration 54, loss = 19128219889.15121841\n",
      "Iteration 55, loss = 19128211837.42405701\n",
      "Iteration 56, loss = 19128204021.04636002\n",
      "Iteration 57, loss = 19128196231.09096527\n",
      "Iteration 58, loss = 19128188386.41184998\n",
      "Iteration 59, loss = 19128180734.82694244\n",
      "Iteration 60, loss = 19128173165.42756653\n",
      "Iteration 61, loss = 19128165576.92625809\n",
      "Iteration 62, loss = 19128158092.05251694\n",
      "Iteration 63, loss = 19128150616.38198471\n",
      "Iteration 64, loss = 19128143218.88092041\n",
      "Iteration 65, loss = 19128135894.85964966\n",
      "Iteration 66, loss = 19128128513.52852631\n",
      "Iteration 67, loss = 19128121298.53593063\n",
      "Iteration 68, loss = 19128114069.02796173\n",
      "Iteration 69, loss = 19128106932.71918106\n",
      "Iteration 70, loss = 19128099753.51784897\n",
      "Iteration 71, loss = 19128092642.03704834\n",
      "Iteration 72, loss = 19128085536.08118057\n",
      "Iteration 73, loss = 19128078548.98446655\n",
      "Iteration 74, loss = 19128071577.59362411\n",
      "Iteration 75, loss = 19128064718.74328613\n",
      "Iteration 76, loss = 19128057778.53921890\n",
      "Iteration 77, loss = 19128050901.43115234\n",
      "Iteration 78, loss = 19128044005.36549759\n",
      "Iteration 79, loss = 19128037118.02431107\n",
      "Iteration 80, loss = 19128030323.71378708\n",
      "Iteration 81, loss = 19128023474.24238205\n",
      "Iteration 82, loss = 19128016578.51272202\n",
      "Iteration 83, loss = 19128009730.02196121\n",
      "Iteration 84, loss = 19128002935.96036148\n",
      "Iteration 85, loss = 19127996166.54046249\n",
      "Iteration 86, loss = 19127989424.77183151\n",
      "Iteration 87, loss = 19127982780.14458466\n",
      "Iteration 88, loss = 19127976061.05289078\n",
      "Iteration 89, loss = 19127969265.70941925\n",
      "Iteration 90, loss = 19127962635.03246689\n",
      "Iteration 91, loss = 19127955947.49588776\n",
      "Iteration 92, loss = 19127949299.30800247\n",
      "Iteration 93, loss = 19127942602.30334473\n",
      "Iteration 94, loss = 19127935911.16144180\n",
      "Iteration 95, loss = 19127929255.35097122\n",
      "Iteration 96, loss = 19127922578.84077835\n",
      "Iteration 97, loss = 19127915832.67243195\n",
      "Iteration 98, loss = 19127909076.52675629\n",
      "Iteration 99, loss = 19127902364.31822586\n",
      "Iteration 100, loss = 19127895719.83056259\n",
      "Iteration 101, loss = 19127889122.60330200\n",
      "Iteration 102, loss = 19127882572.04999542\n",
      "Iteration 103, loss = 19127876020.29073715\n",
      "Iteration 104, loss = 19127869420.95113754\n",
      "Iteration 105, loss = 19127862850.91180801\n",
      "Iteration 106, loss = 19127856365.37984085\n",
      "Iteration 107, loss = 19127849867.03523636\n",
      "Iteration 108, loss = 19127843457.92312241\n",
      "Iteration 109, loss = 19127836971.63064575\n",
      "Iteration 110, loss = 19127830589.90991592\n",
      "Iteration 111, loss = 19127824167.22114563\n",
      "Iteration 112, loss = 19127817779.13213348\n",
      "Iteration 113, loss = 19127811368.98540115\n",
      "Iteration 114, loss = 19127804997.26247787\n",
      "Iteration 115, loss = 19127798567.44268417\n",
      "Iteration 116, loss = 19127792206.77530289\n",
      "Iteration 117, loss = 19127785810.59762192\n",
      "Iteration 118, loss = 19127779443.98795319\n",
      "Iteration 119, loss = 19127773024.85565948\n",
      "Iteration 120, loss = 19127766564.84096146\n",
      "Iteration 121, loss = 19127760088.59577179\n",
      "Iteration 122, loss = 19127753643.10346222\n",
      "Iteration 123, loss = 19127747193.01651382\n",
      "Iteration 124, loss = 19127740794.82902908\n",
      "Iteration 125, loss = 19127734392.23444366\n",
      "Iteration 126, loss = 19127727952.97008514\n",
      "Iteration 127, loss = 19127721346.86922836\n",
      "Iteration 128, loss = 19127714710.52149200\n",
      "Iteration 129, loss = 19127708052.95679855\n",
      "Iteration 130, loss = 19127701256.89260101\n",
      "Iteration 131, loss = 19127694460.05344772\n",
      "Iteration 132, loss = 19127687508.99843979\n",
      "Iteration 133, loss = 19127680567.80022049\n",
      "Iteration 134, loss = 19127673599.45148087\n",
      "Iteration 135, loss = 19127666531.47761536\n",
      "Iteration 136, loss = 19127659501.60930252\n",
      "Iteration 137, loss = 19127652529.52792740\n",
      "Iteration 138, loss = 19127645560.64879608\n",
      "Iteration 139, loss = 19127638657.96791840\n",
      "Iteration 140, loss = 19127631674.34937286\n",
      "Iteration 141, loss = 19127624723.72940063\n",
      "Iteration 142, loss = 19127617775.33233261\n",
      "Iteration 143, loss = 19127610848.60380173\n",
      "Iteration 144, loss = 19127603982.06803894\n",
      "Iteration 145, loss = 19127597132.39994812\n",
      "Iteration 146, loss = 19127590321.63264465\n",
      "Iteration 147, loss = 19127583595.55846405\n",
      "Iteration 148, loss = 19127576886.27368164\n",
      "Iteration 149, loss = 19127570082.79135132\n",
      "Iteration 150, loss = 19127563367.08156204\n",
      "Iteration 151, loss = 19127556683.93056107\n",
      "Iteration 152, loss = 19127550079.30267715\n",
      "Iteration 153, loss = 19127543520.16690445\n",
      "Iteration 154, loss = 19127536926.13690186\n",
      "Iteration 155, loss = 19127530335.86595154\n",
      "Iteration 156, loss = 19127523706.12562561\n",
      "Iteration 157, loss = 19127517110.68661880\n",
      "Iteration 158, loss = 19127510529.21788406\n",
      "Iteration 159, loss = 19127503842.41838074\n",
      "Iteration 160, loss = 19127497082.83152390\n",
      "Iteration 161, loss = 19127490469.68854141\n",
      "Iteration 162, loss = 19127483739.15921402\n",
      "Iteration 163, loss = 19127477154.32180023\n",
      "Iteration 164, loss = 19127470588.56374741\n",
      "Iteration 165, loss = 19127464100.31722260\n",
      "Iteration 166, loss = 19127457539.26001740\n",
      "Iteration 167, loss = 19127451027.03593826\n",
      "Iteration 168, loss = 19127444597.84096909\n",
      "Iteration 169, loss = 19127438086.80964661\n",
      "Iteration 170, loss = 19127431586.59062195\n",
      "Iteration 171, loss = 19127424989.67124176\n",
      "Iteration 172, loss = 19127418450.70987320\n",
      "Iteration 173, loss = 19127411941.18320465\n",
      "Iteration 174, loss = 19127405431.94068909\n",
      "Iteration 175, loss = 19127398964.78837585\n",
      "Iteration 176, loss = 19127392433.03469849\n",
      "Iteration 177, loss = 19127385978.06885529\n",
      "Iteration 178, loss = 19127379532.89913940\n",
      "Iteration 179, loss = 19127373060.26001358\n",
      "Iteration 180, loss = 19127366518.61069870\n",
      "Iteration 181, loss = 19127359974.34479523\n",
      "Iteration 182, loss = 19127353480.99903870\n",
      "Iteration 183, loss = 19127347004.31898880\n",
      "Iteration 184, loss = 19127340459.34988022\n",
      "Iteration 185, loss = 19127333910.89544296\n",
      "Iteration 186, loss = 19127327414.14483643\n",
      "Iteration 187, loss = 19127320865.00175095\n",
      "Iteration 188, loss = 19127314283.01681137\n",
      "Iteration 189, loss = 19127307722.88416672\n",
      "Iteration 190, loss = 19127301185.26697159\n",
      "Iteration 191, loss = 19127294667.09753036\n",
      "Iteration 192, loss = 19127288133.36696243\n",
      "Iteration 193, loss = 19127281552.17947769\n",
      "Iteration 194, loss = 19127275105.66796112\n",
      "Iteration 195, loss = 19127268640.86926270\n",
      "Iteration 196, loss = 19127262235.24840927\n",
      "Iteration 197, loss = 19127255716.68402100\n",
      "Iteration 198, loss = 19127249238.62968063\n",
      "Iteration 199, loss = 19127242699.96107483\n",
      "Iteration 200, loss = 19127236244.34463501\n",
      "Iteration 201, loss = 19127229786.23891830\n",
      "Iteration 202, loss = 19127223298.47819901\n",
      "Iteration 203, loss = 19127216864.66019821\n",
      "Iteration 204, loss = 19127210444.84750748\n",
      "Iteration 205, loss = 19127204038.06529999\n",
      "Iteration 206, loss = 19127197617.20340347\n",
      "Iteration 207, loss = 19127191209.48223877\n",
      "Iteration 208, loss = 19127184734.65050507\n",
      "Iteration 209, loss = 19127178240.02685547\n",
      "Iteration 210, loss = 19127171825.79958725\n",
      "Iteration 211, loss = 19127165453.51794815\n",
      "Iteration 212, loss = 19127159053.75769043\n",
      "Iteration 213, loss = 19127152773.76122284\n",
      "Iteration 214, loss = 19127146471.90005875\n",
      "Iteration 215, loss = 19127140202.83173370\n",
      "Iteration 216, loss = 19127133936.98114395\n",
      "Iteration 217, loss = 19127127653.40571213\n",
      "Iteration 218, loss = 19127121395.42422867\n",
      "Iteration 219, loss = 19127115197.44543076\n",
      "Iteration 220, loss = 19127108971.22242355\n",
      "Iteration 221, loss = 19127102810.33691025\n",
      "Iteration 222, loss = 19127096519.97893524\n",
      "Iteration 223, loss = 19127090157.54005814\n",
      "Iteration 224, loss = 19127083856.80506134\n",
      "Iteration 225, loss = 19127077526.62975693\n",
      "Iteration 226, loss = 19127071294.79208755\n",
      "Iteration 227, loss = 19127064996.83462906\n",
      "Iteration 228, loss = 19127058684.13609695\n",
      "Iteration 229, loss = 19127052433.78802109\n",
      "Iteration 230, loss = 19127046159.07586288\n",
      "Iteration 231, loss = 19127039821.93284988\n",
      "Iteration 232, loss = 19127033447.74204254\n",
      "Iteration 233, loss = 19127027080.83082962\n",
      "Iteration 234, loss = 19127020787.34780502\n",
      "Iteration 235, loss = 19127014515.04129028\n",
      "Iteration 236, loss = 19127008225.01935959\n",
      "Iteration 237, loss = 19127001930.33593369\n",
      "Iteration 238, loss = 19126995584.84415817\n",
      "Iteration 239, loss = 19126989234.48751068\n",
      "Iteration 240, loss = 19126982843.27982712\n",
      "Iteration 241, loss = 19126976424.98342896\n",
      "Iteration 242, loss = 19126970128.29664230\n",
      "Iteration 243, loss = 19126963801.26204300\n",
      "Iteration 244, loss = 19126957569.70156479\n",
      "Iteration 245, loss = 19126951265.37481689\n",
      "Iteration 246, loss = 19126945045.66544724\n",
      "Iteration 247, loss = 19126938767.96913147\n",
      "Iteration 248, loss = 19126932480.35173798\n",
      "Iteration 249, loss = 19126926199.27400589\n",
      "Iteration 250, loss = 19126919951.39025879\n",
      "Iteration 251, loss = 19126913676.56987000\n",
      "Iteration 252, loss = 19126907519.89532089\n",
      "Iteration 253, loss = 19126901329.78127289\n",
      "Iteration 254, loss = 19126895093.40718842\n",
      "Iteration 255, loss = 19126888900.36867905\n",
      "Iteration 256, loss = 19126882713.78460693\n",
      "Iteration 257, loss = 19126876431.07332611\n",
      "Iteration 258, loss = 19126870233.76210022\n",
      "Iteration 259, loss = 19126863891.98463058\n",
      "Iteration 260, loss = 19126857666.33343506\n",
      "Iteration 261, loss = 19126851333.18765640\n",
      "Iteration 262, loss = 19126845038.15225601\n",
      "Iteration 263, loss = 19126838781.11627960\n",
      "Iteration 264, loss = 19126832485.48556137\n",
      "Iteration 265, loss = 19126826151.97410965\n",
      "Iteration 266, loss = 19126819942.14649582\n",
      "Iteration 267, loss = 19126813709.78256226\n",
      "Iteration 268, loss = 19126807443.56293869\n",
      "Iteration 269, loss = 19126801157.25232315\n",
      "Iteration 270, loss = 19126794902.93736267\n",
      "Iteration 271, loss = 19126788645.73835754\n",
      "Iteration 272, loss = 19126782423.28643799\n",
      "Iteration 273, loss = 19126776129.18809891\n",
      "Iteration 274, loss = 19126769815.42347336\n",
      "Iteration 275, loss = 19126763392.30707932\n",
      "Iteration 276, loss = 19126756908.57445908\n",
      "Iteration 277, loss = 19126750359.45810318\n",
      "Iteration 278, loss = 19126743596.83478165\n",
      "Iteration 279, loss = 19126736679.69884491\n",
      "Iteration 280, loss = 19126729450.69012833\n",
      "Iteration 281, loss = 19126722096.37932587\n",
      "Iteration 282, loss = 19126714660.71967316\n",
      "Iteration 283, loss = 19126707230.28219986\n",
      "Iteration 284, loss = 19126699764.65871048\n",
      "Iteration 285, loss = 19126692389.78757858\n",
      "Iteration 286, loss = 19126685174.36911774\n",
      "Iteration 287, loss = 19126678078.32738495\n",
      "Iteration 288, loss = 19126670967.46000290\n",
      "Iteration 289, loss = 19126663995.38065720\n",
      "Iteration 290, loss = 19126657120.68666840\n",
      "Iteration 291, loss = 19126650210.35077667\n",
      "Iteration 292, loss = 19126643281.61868668\n",
      "Iteration 293, loss = 19126636367.27317810\n",
      "Iteration 294, loss = 19126629459.35731506\n",
      "Iteration 295, loss = 19126622532.84605026\n",
      "Iteration 296, loss = 19126615704.40761566\n",
      "Iteration 297, loss = 19126608841.27434921\n",
      "Iteration 298, loss = 19126602044.21631622\n",
      "Iteration 299, loss = 19126595298.54202652\n",
      "Iteration 300, loss = 19126588532.20240402\n",
      "Iteration 301, loss = 19126581766.60495377\n",
      "Iteration 302, loss = 19126575043.93738937\n",
      "Iteration 303, loss = 19126568333.29722595\n",
      "Iteration 304, loss = 19126561536.74267197\n",
      "Iteration 305, loss = 19126554729.73195648\n",
      "Iteration 306, loss = 19126547849.78178406\n",
      "Iteration 307, loss = 19126540923.58500290\n",
      "Iteration 308, loss = 19126534047.50206757\n",
      "Iteration 309, loss = 19126527243.37119293\n",
      "Iteration 310, loss = 19126520481.90861893\n",
      "Iteration 311, loss = 19126513820.44029617\n",
      "Iteration 312, loss = 19126507183.86900711\n",
      "Iteration 313, loss = 19126500585.23262405\n",
      "Iteration 314, loss = 19126493929.11566162\n",
      "Iteration 315, loss = 19126487338.70256042\n",
      "Iteration 316, loss = 19126480739.76523590\n",
      "Iteration 317, loss = 19126474168.01351547\n",
      "Iteration 318, loss = 19126467599.01383972\n",
      "Iteration 319, loss = 19126460925.91638947\n",
      "Iteration 320, loss = 19126454320.77429962\n",
      "Iteration 321, loss = 19126447695.27189255\n",
      "Iteration 322, loss = 19126441051.87219238\n",
      "Iteration 323, loss = 19126434500.74289322\n",
      "Iteration 324, loss = 19126427849.97561646\n",
      "Iteration 325, loss = 19126421229.85555649\n",
      "Iteration 326, loss = 19126414629.88107681\n",
      "Iteration 327, loss = 19126408036.69160843\n",
      "Iteration 328, loss = 19126401500.55864334\n",
      "Iteration 329, loss = 19126394931.62416458\n",
      "Iteration 330, loss = 19126388390.94962692\n",
      "Iteration 331, loss = 19126381895.09951019\n",
      "Iteration 332, loss = 19126375360.27096939\n",
      "Iteration 333, loss = 19126368884.51586533\n",
      "Iteration 334, loss = 19126362427.40974808\n",
      "Iteration 335, loss = 19126355991.42196274\n",
      "Iteration 336, loss = 19126349484.18658447\n",
      "Iteration 337, loss = 19126342992.32483673\n",
      "Iteration 338, loss = 19126336446.68647385\n",
      "Iteration 339, loss = 19126329901.87741089\n",
      "Iteration 340, loss = 19126323419.15395355\n",
      "Iteration 341, loss = 19126316874.04521179\n",
      "Iteration 342, loss = 19126310289.08808899\n",
      "Iteration 343, loss = 19126303724.04792023\n",
      "Iteration 344, loss = 19126297194.32052612\n",
      "Iteration 345, loss = 19126290685.52563095\n",
      "Iteration 346, loss = 19126284166.34681320\n",
      "Iteration 347, loss = 19126277578.80544662\n",
      "Iteration 348, loss = 19126270943.82264328\n",
      "Iteration 349, loss = 19126264340.71082306\n",
      "Iteration 350, loss = 19126257694.40903091\n",
      "Iteration 351, loss = 19126251033.27036667\n",
      "Iteration 352, loss = 19126244403.66244888\n",
      "Iteration 353, loss = 19126237884.84624481\n",
      "Iteration 354, loss = 19126231403.48381805\n",
      "Iteration 355, loss = 19126224911.20909500\n",
      "Iteration 356, loss = 19126218466.11713409\n",
      "Iteration 357, loss = 19126211972.10036850\n",
      "Iteration 358, loss = 19126205483.30074310\n",
      "Iteration 359, loss = 19126199086.02311707\n",
      "Iteration 360, loss = 19126192594.07536697\n",
      "Iteration 361, loss = 19126186099.59882355\n",
      "Iteration 362, loss = 19126179661.97135162\n",
      "Iteration 363, loss = 19126173180.20236588\n",
      "Iteration 364, loss = 19126166682.00372696\n",
      "Iteration 365, loss = 19126160202.44709015\n",
      "Iteration 366, loss = 19126153625.74525833\n",
      "Iteration 367, loss = 19126147110.72593689\n",
      "Iteration 368, loss = 19126140501.61811066\n",
      "Iteration 369, loss = 19126134024.12797928\n",
      "Iteration 370, loss = 19126127477.62834930\n",
      "Iteration 371, loss = 19126120985.49255753\n",
      "Iteration 372, loss = 19126114542.84439087\n",
      "Iteration 373, loss = 19126108087.03746414\n",
      "Iteration 374, loss = 19126101742.97528458\n",
      "Iteration 375, loss = 19126095295.27466583\n",
      "Iteration 376, loss = 19126088886.37682724\n",
      "Iteration 377, loss = 19126082408.05917358\n",
      "Iteration 378, loss = 19126075943.27353287\n",
      "Iteration 379, loss = 19126069523.44633102\n",
      "Iteration 380, loss = 19126063143.78639221\n",
      "Iteration 381, loss = 19126056734.83328629\n",
      "Iteration 382, loss = 19126050261.64809418\n",
      "Iteration 383, loss = 19126043838.02968979\n",
      "Iteration 384, loss = 19126037371.98334122\n",
      "Iteration 385, loss = 19126030964.87805939\n",
      "Iteration 386, loss = 19126024575.81517792\n",
      "Iteration 387, loss = 19126018191.15384674\n",
      "Iteration 388, loss = 19126011672.22768784\n",
      "Iteration 389, loss = 19126005150.63700485\n",
      "Iteration 390, loss = 19125998600.49172974\n",
      "Iteration 391, loss = 19125992130.73565674\n",
      "Iteration 392, loss = 19125985692.23205185\n",
      "Iteration 393, loss = 19125979252.39801788\n",
      "Iteration 394, loss = 19125972809.32372284\n",
      "Iteration 395, loss = 19125966413.56987000\n",
      "Iteration 396, loss = 19125960022.86556244\n",
      "Iteration 397, loss = 19125953625.91706467\n",
      "Iteration 398, loss = 19125947304.34179306\n",
      "Iteration 399, loss = 19125941033.81268311\n",
      "Iteration 400, loss = 19125934684.61571121\n",
      "Iteration 401, loss = 19125928360.54353714\n",
      "Iteration 402, loss = 19125922083.04730988\n",
      "Iteration 403, loss = 19125915730.07222366\n",
      "Iteration 404, loss = 19125909380.28200912\n",
      "Iteration 405, loss = 19125902995.20922470\n",
      "Iteration 406, loss = 19125896626.76983261\n",
      "Iteration 407, loss = 19125890220.29592896\n",
      "Iteration 408, loss = 19125883718.20746231\n",
      "Iteration 409, loss = 19125877331.17058563\n",
      "Iteration 410, loss = 19125870916.70160294\n",
      "Iteration 411, loss = 19125864449.70261002\n",
      "Iteration 412, loss = 19125858047.71743393\n",
      "Iteration 413, loss = 19125851653.33378983\n",
      "Iteration 414, loss = 19125845232.34187698\n",
      "Iteration 415, loss = 19125838868.97817993\n",
      "Iteration 416, loss = 19125832476.80469131\n",
      "Iteration 417, loss = 19125826132.26359940\n",
      "Iteration 418, loss = 19125819771.16729355\n",
      "Iteration 419, loss = 19125813424.39046478\n",
      "Iteration 420, loss = 19125807115.74667358\n",
      "Iteration 421, loss = 19125800750.68843842\n",
      "Iteration 422, loss = 19125794369.21291733\n",
      "Iteration 423, loss = 19125787984.36199570\n",
      "Iteration 424, loss = 19125781558.11290741\n",
      "Iteration 425, loss = 19125775201.39272690\n",
      "Iteration 426, loss = 19125768761.04423904\n",
      "Iteration 427, loss = 19125762313.51145935\n",
      "Iteration 428, loss = 19125755769.18941879\n",
      "Iteration 429, loss = 19125749326.70846558\n",
      "Iteration 430, loss = 19125742840.76501846\n",
      "Iteration 431, loss = 19125736459.42731857\n",
      "Iteration 432, loss = 19125730148.85610199\n",
      "Iteration 433, loss = 19125723765.42585373\n",
      "Iteration 434, loss = 19125717361.45864105\n",
      "Iteration 435, loss = 19125711059.76711273\n",
      "Iteration 436, loss = 19125704689.12044907\n",
      "Iteration 437, loss = 19125698337.46746445\n",
      "Iteration 438, loss = 19125692015.63605881\n",
      "Iteration 439, loss = 19125685700.37405014\n",
      "Iteration 440, loss = 19125679267.62918472\n",
      "Iteration 441, loss = 19125672806.88928223\n",
      "Iteration 442, loss = 19125666341.42160416\n",
      "Iteration 443, loss = 19125659912.83154297\n",
      "Iteration 444, loss = 19125653557.64461899\n",
      "Iteration 445, loss = 19125647150.73134995\n",
      "Iteration 446, loss = 19125640823.50865555\n",
      "Iteration 447, loss = 19125634439.53073502\n",
      "Iteration 448, loss = 19125628133.90039825\n",
      "Iteration 449, loss = 19125621722.19153595\n",
      "Iteration 450, loss = 19125615348.06644821\n",
      "Iteration 451, loss = 19125609023.46159744\n",
      "Iteration 452, loss = 19125602714.79532242\n",
      "Iteration 453, loss = 19125596369.50476456\n",
      "Iteration 454, loss = 19125589980.46473694\n",
      "Iteration 455, loss = 19125583525.96877289\n",
      "Iteration 456, loss = 19125577126.79003525\n",
      "Iteration 457, loss = 19125570678.69613266\n",
      "Iteration 458, loss = 19125564258.38099670\n",
      "Iteration 459, loss = 19125557826.80212784\n",
      "Iteration 460, loss = 19125551461.98137665\n",
      "Iteration 461, loss = 19125545071.02850723\n",
      "Iteration 462, loss = 19125538723.82881165\n",
      "Iteration 463, loss = 19125532392.46971130\n",
      "Iteration 464, loss = 19125526027.46818161\n",
      "Iteration 465, loss = 19125519761.63362885\n",
      "Iteration 466, loss = 19125513386.47777557\n",
      "Iteration 467, loss = 19125507095.82418442\n",
      "Iteration 468, loss = 19125500806.76171494\n",
      "Iteration 469, loss = 19125494509.76781082\n",
      "Iteration 470, loss = 19125488175.41561127\n",
      "Iteration 471, loss = 19125481793.88205338\n",
      "Iteration 472, loss = 19125475497.96550369\n",
      "Iteration 473, loss = 19125469233.83697891\n",
      "Iteration 474, loss = 19125462981.20541382\n",
      "Iteration 475, loss = 19125456679.15847778\n",
      "Iteration 476, loss = 19125450404.22568130\n",
      "Iteration 477, loss = 19125444102.10701370\n",
      "Iteration 478, loss = 19125437760.46834183\n",
      "Iteration 479, loss = 19125431295.49893188\n",
      "Iteration 480, loss = 19125424887.35524750\n",
      "Iteration 481, loss = 19125418480.99486160\n",
      "Iteration 482, loss = 19125412150.35723495\n",
      "Iteration 483, loss = 19125405696.82390976\n",
      "Iteration 484, loss = 19125399344.41762161\n",
      "Iteration 485, loss = 19125393021.52730942\n",
      "Iteration 486, loss = 19125386605.89464188\n",
      "Iteration 487, loss = 19125380201.21358490\n",
      "Iteration 488, loss = 19125373801.98496628\n",
      "Iteration 489, loss = 19125367396.96133423\n",
      "Iteration 490, loss = 19125361034.27485657\n",
      "Iteration 491, loss = 19125354654.29169846\n",
      "Iteration 492, loss = 19125348284.67076492\n",
      "Iteration 493, loss = 19125341867.72365952\n",
      "Iteration 494, loss = 19125335478.69829178\n",
      "Iteration 495, loss = 19125329131.30188751\n",
      "Iteration 496, loss = 19125322811.87604141\n",
      "Iteration 497, loss = 19125316512.43357468\n",
      "Iteration 498, loss = 19125310154.49235535\n",
      "Iteration 499, loss = 19125303842.01972961\n",
      "Iteration 500, loss = 19125297444.15169525\n",
      "Iteration 1, loss = 19225459109.14673615\n",
      "Iteration 2, loss = 19225443747.06975555\n",
      "Iteration 3, loss = 19225428401.58035278\n",
      "Iteration 4, loss = 19225412661.09363556\n",
      "Iteration 5, loss = 19225396691.43418503\n",
      "Iteration 6, loss = 19225380461.44665146\n",
      "Iteration 7, loss = 19225364878.48326111\n",
      "Iteration 8, loss = 19225349064.66064072\n",
      "Iteration 9, loss = 19225333132.14253616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 19225316891.26707077\n",
      "Iteration 11, loss = 19225300737.91931152\n",
      "Iteration 12, loss = 19225284343.10765839\n",
      "Iteration 13, loss = 19225267486.95941925\n",
      "Iteration 14, loss = 19225250750.65312958\n",
      "Iteration 15, loss = 19225234839.56737900\n",
      "Iteration 16, loss = 19225217790.80046844\n",
      "Iteration 17, loss = 19225200338.63596725\n",
      "Iteration 18, loss = 19225182873.30519867\n",
      "Iteration 19, loss = 19225165331.53217316\n",
      "Iteration 20, loss = 19225147624.90218353\n",
      "Iteration 21, loss = 19225129696.82348251\n",
      "Iteration 22, loss = 19225111265.47605896\n",
      "Iteration 23, loss = 19225092061.43204498\n",
      "Iteration 24, loss = 19225074026.60936356\n",
      "Iteration 25, loss = 19225056890.25037766\n",
      "Iteration 26, loss = 19225040756.63400269\n",
      "Iteration 27, loss = 19225024219.40807343\n",
      "Iteration 28, loss = 19225008094.45530701\n",
      "Iteration 29, loss = 19224992951.91188049\n",
      "Iteration 30, loss = 19224978892.75200272\n",
      "Iteration 31, loss = 19224965096.79926682\n",
      "Iteration 32, loss = 19224951671.65027618\n",
      "Iteration 33, loss = 19224938915.42382050\n",
      "Iteration 34, loss = 19224926317.41091919\n",
      "Iteration 35, loss = 19224914612.13676071\n",
      "Iteration 36, loss = 19224903801.40934753\n",
      "Iteration 37, loss = 19224893750.49770737\n",
      "Iteration 38, loss = 19224883758.12100220\n",
      "Iteration 39, loss = 19224874165.16080856\n",
      "Iteration 40, loss = 19224865409.69691467\n",
      "Iteration 41, loss = 19224857493.46387482\n",
      "Iteration 42, loss = 19224849732.88563156\n",
      "Iteration 43, loss = 19224842253.72267532\n",
      "Iteration 44, loss = 19224834803.59373474\n",
      "Iteration 45, loss = 19224827488.92634201\n",
      "Iteration 46, loss = 19224820229.86059952\n",
      "Iteration 47, loss = 19224813100.12948608\n",
      "Iteration 48, loss = 19224805950.07135391\n",
      "Iteration 49, loss = 19224798726.17129898\n",
      "Iteration 50, loss = 19224791498.24526215\n",
      "Iteration 51, loss = 19224784500.31225204\n",
      "Iteration 52, loss = 19224777595.98571396\n",
      "Iteration 53, loss = 19224770761.03820801\n",
      "Iteration 54, loss = 19224764025.80953217\n",
      "Iteration 55, loss = 19224757240.02096176\n",
      "Iteration 56, loss = 19224750510.49570084\n",
      "Iteration 57, loss = 19224743727.80194473\n",
      "Iteration 58, loss = 19224736901.18310165\n",
      "Iteration 59, loss = 19224729776.79613876\n",
      "Iteration 60, loss = 19224722739.54504776\n",
      "Iteration 61, loss = 19224715899.15784073\n",
      "Iteration 62, loss = 19224709099.06035614\n",
      "Iteration 63, loss = 19224702257.43807602\n",
      "Iteration 64, loss = 19224695407.94856262\n",
      "Iteration 65, loss = 19224688500.04698563\n",
      "Iteration 66, loss = 19224681513.26944351\n",
      "Iteration 67, loss = 19224674519.84949493\n",
      "Iteration 68, loss = 19224667538.75230026\n",
      "Iteration 69, loss = 19224660387.92638016\n",
      "Iteration 70, loss = 19224653157.69326401\n",
      "Iteration 71, loss = 19224645947.65109253\n",
      "Iteration 72, loss = 19224638780.88857651\n",
      "Iteration 73, loss = 19224631810.70660400\n",
      "Iteration 74, loss = 19224624842.40652466\n",
      "Iteration 75, loss = 19224617815.82855988\n",
      "Iteration 76, loss = 19224610900.72837830\n",
      "Iteration 77, loss = 19224604026.57205963\n",
      "Iteration 78, loss = 19224597255.14369583\n",
      "Iteration 79, loss = 19224590602.02317429\n",
      "Iteration 80, loss = 19224583840.65319824\n",
      "Iteration 81, loss = 19224577212.35507965\n",
      "Iteration 82, loss = 19224570612.23678970\n",
      "Iteration 83, loss = 19224563998.48944473\n",
      "Iteration 84, loss = 19224557421.35725021\n",
      "Iteration 85, loss = 19224550881.88155365\n",
      "Iteration 86, loss = 19224544377.84291458\n",
      "Iteration 87, loss = 19224537876.04624176\n",
      "Iteration 88, loss = 19224531380.95255280\n",
      "Iteration 89, loss = 19224524854.89947128\n",
      "Iteration 90, loss = 19224518359.39348602\n",
      "Iteration 91, loss = 19224511816.88914490\n",
      "Iteration 92, loss = 19224505382.89139175\n",
      "Iteration 93, loss = 19224498846.77486038\n",
      "Iteration 94, loss = 19224492399.47116089\n",
      "Iteration 95, loss = 19224485970.64437485\n",
      "Iteration 96, loss = 19224479412.30840302\n",
      "Iteration 97, loss = 19224472869.68054962\n",
      "Iteration 98, loss = 19224466371.30157089\n",
      "Iteration 99, loss = 19224459703.32702255\n",
      "Iteration 100, loss = 19224453025.91275024\n",
      "Iteration 101, loss = 19224446349.77016449\n",
      "Iteration 102, loss = 19224439565.40750122\n",
      "Iteration 103, loss = 19224432837.51217651\n",
      "Iteration 104, loss = 19224426047.77576447\n",
      "Iteration 105, loss = 19224419229.01112747\n",
      "Iteration 106, loss = 19224412477.42670059\n",
      "Iteration 107, loss = 19224405735.13644028\n",
      "Iteration 108, loss = 19224399004.34419632\n",
      "Iteration 109, loss = 19224392300.93407822\n",
      "Iteration 110, loss = 19224385622.43095016\n",
      "Iteration 111, loss = 19224378965.43885040\n",
      "Iteration 112, loss = 19224372340.70359802\n",
      "Iteration 113, loss = 19224365663.54866791\n",
      "Iteration 114, loss = 19224359162.36277771\n",
      "Iteration 115, loss = 19224352565.59875107\n",
      "Iteration 116, loss = 19224346073.28582382\n",
      "Iteration 117, loss = 19224339550.01302338\n",
      "Iteration 118, loss = 19224333044.00164032\n",
      "Iteration 119, loss = 19224326566.46077347\n",
      "Iteration 120, loss = 19224320109.62119293\n",
      "Iteration 121, loss = 19224313632.56949234\n",
      "Iteration 122, loss = 19224307206.52288437\n",
      "Iteration 123, loss = 19224300815.93040848\n",
      "Iteration 124, loss = 19224294358.65827179\n",
      "Iteration 125, loss = 19224287931.43842316\n",
      "Iteration 126, loss = 19224281549.86313629\n",
      "Iteration 127, loss = 19224275143.07442856\n",
      "Iteration 128, loss = 19224268789.85451889\n",
      "Iteration 129, loss = 19224262429.87234116\n",
      "Iteration 130, loss = 19224256059.02290344\n",
      "Iteration 131, loss = 19224249757.54133224\n",
      "Iteration 132, loss = 19224243420.63826752\n",
      "Iteration 133, loss = 19224237105.02512360\n",
      "Iteration 134, loss = 19224230796.82535172\n",
      "Iteration 135, loss = 19224224496.79282761\n",
      "Iteration 136, loss = 19224218211.72444916\n",
      "Iteration 137, loss = 19224211913.94509506\n",
      "Iteration 138, loss = 19224205657.43455505\n",
      "Iteration 139, loss = 19224199354.26502991\n",
      "Iteration 140, loss = 19224193146.81557465\n",
      "Iteration 141, loss = 19224186858.41239548\n",
      "Iteration 142, loss = 19224180550.65831375\n",
      "Iteration 143, loss = 19224174325.66641617\n",
      "Iteration 144, loss = 19224168096.25862503\n",
      "Iteration 145, loss = 19224161804.27206039\n",
      "Iteration 146, loss = 19224155573.07166290\n",
      "Iteration 147, loss = 19224149254.85409164\n",
      "Iteration 148, loss = 19224143015.18892288\n",
      "Iteration 149, loss = 19224136690.17238235\n",
      "Iteration 150, loss = 19224130308.72910690\n",
      "Iteration 151, loss = 19224123865.55149460\n",
      "Iteration 152, loss = 19224117358.53429413\n",
      "Iteration 153, loss = 19224110750.21964645\n",
      "Iteration 154, loss = 19224103969.89956284\n",
      "Iteration 155, loss = 19224097054.82509232\n",
      "Iteration 156, loss = 19224090120.83734131\n",
      "Iteration 157, loss = 19224083054.00315857\n",
      "Iteration 158, loss = 19224075945.37754822\n",
      "Iteration 159, loss = 19224068969.80294800\n",
      "Iteration 160, loss = 19224061886.66318893\n",
      "Iteration 161, loss = 19224054905.24983597\n",
      "Iteration 162, loss = 19224048018.87538910\n",
      "Iteration 163, loss = 19224041125.71596527\n",
      "Iteration 164, loss = 19224034337.07378387\n",
      "Iteration 165, loss = 19224027558.09482574\n",
      "Iteration 166, loss = 19224020816.69974518\n",
      "Iteration 167, loss = 19224014082.31861877\n",
      "Iteration 168, loss = 19224007370.06311035\n",
      "Iteration 169, loss = 19224000746.74114609\n",
      "Iteration 170, loss = 19223994075.35266495\n",
      "Iteration 171, loss = 19223987511.03055573\n",
      "Iteration 172, loss = 19223980872.80566788\n",
      "Iteration 173, loss = 19223974306.91194916\n",
      "Iteration 174, loss = 19223967738.35095215\n",
      "Iteration 175, loss = 19223961162.82210541\n",
      "Iteration 176, loss = 19223954689.43501282\n",
      "Iteration 177, loss = 19223948156.06414795\n",
      "Iteration 178, loss = 19223941619.80348206\n",
      "Iteration 179, loss = 19223935126.60694122\n",
      "Iteration 180, loss = 19223928660.30480576\n",
      "Iteration 181, loss = 19223922163.16452408\n",
      "Iteration 182, loss = 19223915706.13724899\n",
      "Iteration 183, loss = 19223909284.42913055\n",
      "Iteration 184, loss = 19223902826.57453156\n",
      "Iteration 185, loss = 19223896382.44232178\n",
      "Iteration 186, loss = 19223889947.74208450\n",
      "Iteration 187, loss = 19223883605.03166962\n",
      "Iteration 188, loss = 19223877115.34098434\n",
      "Iteration 189, loss = 19223870777.48183823\n",
      "Iteration 190, loss = 19223864375.20300674\n",
      "Iteration 191, loss = 19223857995.02039337\n",
      "Iteration 192, loss = 19223851554.81294632\n",
      "Iteration 193, loss = 19223845241.30467987\n",
      "Iteration 194, loss = 19223838870.12572861\n",
      "Iteration 195, loss = 19223832505.71611786\n",
      "Iteration 196, loss = 19223826150.16611862\n",
      "Iteration 197, loss = 19223819775.94767380\n",
      "Iteration 198, loss = 19223813454.20022583\n",
      "Iteration 199, loss = 19223807123.37115097\n",
      "Iteration 200, loss = 19223800801.68942642\n",
      "Iteration 201, loss = 19223794461.63917542\n",
      "Iteration 202, loss = 19223788152.92274857\n",
      "Iteration 203, loss = 19223781846.29364395\n",
      "Iteration 204, loss = 19223775496.31777573\n",
      "Iteration 205, loss = 19223769228.49456024\n",
      "Iteration 206, loss = 19223762970.44374084\n",
      "Iteration 207, loss = 19223756615.03110886\n",
      "Iteration 208, loss = 19223750368.31781006\n",
      "Iteration 209, loss = 19223744047.30759430\n",
      "Iteration 210, loss = 19223737753.11685944\n",
      "Iteration 211, loss = 19223731505.93470383\n",
      "Iteration 212, loss = 19223725228.40113068\n",
      "Iteration 213, loss = 19223718972.67446136\n",
      "Iteration 214, loss = 19223712714.27123642\n",
      "Iteration 215, loss = 19223706439.34988785\n",
      "Iteration 216, loss = 19223700220.99146271\n",
      "Iteration 217, loss = 19223694026.93896103\n",
      "Iteration 218, loss = 19223687707.57646179\n",
      "Iteration 219, loss = 19223681448.33082962\n",
      "Iteration 220, loss = 19223675232.27699280\n",
      "Iteration 221, loss = 19223668976.99206543\n",
      "Iteration 222, loss = 19223662751.42388916\n",
      "Iteration 223, loss = 19223656515.63742065\n",
      "Iteration 224, loss = 19223650328.94345093\n",
      "Iteration 225, loss = 19223644035.06780243\n",
      "Iteration 226, loss = 19223637871.85872650\n",
      "Iteration 227, loss = 19223631623.48721313\n",
      "Iteration 228, loss = 19223625435.66056442\n",
      "Iteration 229, loss = 19223619206.89574051\n",
      "Iteration 230, loss = 19223613044.07941055\n",
      "Iteration 231, loss = 19223606814.61752319\n",
      "Iteration 232, loss = 19223600621.04491043\n",
      "Iteration 233, loss = 19223594389.52809525\n",
      "Iteration 234, loss = 19223588198.18502426\n",
      "Iteration 235, loss = 19223582016.14535141\n",
      "Iteration 236, loss = 19223575808.84831619\n",
      "Iteration 237, loss = 19223569612.04606628\n",
      "Iteration 238, loss = 19223563404.27388763\n",
      "Iteration 239, loss = 19223557196.27188873\n",
      "Iteration 240, loss = 19223551066.16701889\n",
      "Iteration 241, loss = 19223544836.56974411\n",
      "Iteration 242, loss = 19223538610.71044540\n",
      "Iteration 243, loss = 19223532470.66403580\n",
      "Iteration 244, loss = 19223526279.73580170\n",
      "Iteration 245, loss = 19223520126.43646240\n",
      "Iteration 246, loss = 19223513976.18918228\n",
      "Iteration 247, loss = 19223507742.06696320\n",
      "Iteration 248, loss = 19223501578.78726959\n",
      "Iteration 249, loss = 19223495426.59431839\n",
      "Iteration 250, loss = 19223489281.53835678\n",
      "Iteration 251, loss = 19223483115.89307022\n",
      "Iteration 252, loss = 19223476987.89105988\n",
      "Iteration 253, loss = 19223470799.41922379\n",
      "Iteration 254, loss = 19223464703.89441299\n",
      "Iteration 255, loss = 19223458502.17821884\n",
      "Iteration 256, loss = 19223452409.94617844\n",
      "Iteration 257, loss = 19223446270.08661652\n",
      "Iteration 258, loss = 19223440081.64979553\n",
      "Iteration 259, loss = 19223433941.01656342\n",
      "Iteration 260, loss = 19223427818.89508820\n",
      "Iteration 261, loss = 19223421650.11963654\n",
      "Iteration 262, loss = 19223415547.00112534\n",
      "Iteration 263, loss = 19223409389.38398361\n",
      "Iteration 264, loss = 19223403269.05260849\n",
      "Iteration 265, loss = 19223397108.72829056\n",
      "Iteration 266, loss = 19223391010.25027847\n",
      "Iteration 267, loss = 19223384868.01289749\n",
      "Iteration 268, loss = 19223378753.56853867\n",
      "Iteration 269, loss = 19223372554.73248291\n",
      "Iteration 270, loss = 19223366460.29674149\n",
      "Iteration 271, loss = 19223360317.13393402\n",
      "Iteration 272, loss = 19223354259.81449127\n",
      "Iteration 273, loss = 19223348091.04865646\n",
      "Iteration 274, loss = 19223341978.50714874\n",
      "Iteration 275, loss = 19223335845.02549362\n",
      "Iteration 276, loss = 19223329738.57596588\n",
      "Iteration 277, loss = 19223323625.83333588\n",
      "Iteration 278, loss = 19223317514.29706192\n",
      "Iteration 279, loss = 19223311394.36019516\n",
      "Iteration 280, loss = 19223305257.61574173\n",
      "Iteration 281, loss = 19223299174.49562454\n",
      "Iteration 282, loss = 19223293052.96008301\n",
      "Iteration 283, loss = 19223286972.55180359\n",
      "Iteration 284, loss = 19223280892.82888794\n",
      "Iteration 285, loss = 19223274750.39714050\n",
      "Iteration 286, loss = 19223268650.65117264\n",
      "Iteration 287, loss = 19223262579.73896027\n",
      "Iteration 288, loss = 19223256506.29010391\n",
      "Iteration 289, loss = 19223250355.58070374\n",
      "Iteration 290, loss = 19223244300.72404480\n",
      "Iteration 291, loss = 19223238196.97830200\n",
      "Iteration 292, loss = 19223232102.52134323\n",
      "Iteration 293, loss = 19223225993.03131104\n",
      "Iteration 294, loss = 19223219921.65736008\n",
      "Iteration 295, loss = 19223213843.41974258\n",
      "Iteration 296, loss = 19223207724.26957703\n",
      "Iteration 297, loss = 19223201627.87796402\n",
      "Iteration 298, loss = 19223195551.63223267\n",
      "Iteration 299, loss = 19223189441.60853958\n",
      "Iteration 300, loss = 19223183398.20274353\n",
      "Iteration 301, loss = 19223177286.39313507\n",
      "Iteration 302, loss = 19223171194.27916718\n",
      "Iteration 303, loss = 19223165101.80151749\n",
      "Iteration 304, loss = 19223159029.54231644\n",
      "Iteration 305, loss = 19223152975.05510712\n",
      "Iteration 306, loss = 19223146862.19786835\n",
      "Iteration 307, loss = 19223140789.73208237\n",
      "Iteration 308, loss = 19223134756.79927826\n",
      "Iteration 309, loss = 19223128669.75304413\n",
      "Iteration 310, loss = 19223122605.71928406\n",
      "Iteration 311, loss = 19223116495.60823441\n",
      "Iteration 312, loss = 19223110434.97269821\n",
      "Iteration 313, loss = 19223104349.68554688\n",
      "Iteration 314, loss = 19223098334.78933716\n",
      "Iteration 315, loss = 19223092261.19151688\n",
      "Iteration 316, loss = 19223086153.48749924\n",
      "Iteration 317, loss = 19223080105.19242859\n",
      "Iteration 318, loss = 19223074042.77166748\n",
      "Iteration 319, loss = 19223067975.45960236\n",
      "Iteration 320, loss = 19223061892.18859482\n",
      "Iteration 321, loss = 19223055866.07820511\n",
      "Iteration 322, loss = 19223049811.11166000\n",
      "Iteration 323, loss = 19223043733.74921036\n",
      "Iteration 324, loss = 19223037664.07339859\n",
      "Iteration 325, loss = 19223031651.57738113\n",
      "Iteration 326, loss = 19223025562.23944473\n",
      "Iteration 327, loss = 19223019515.40132141\n",
      "Iteration 328, loss = 19223013454.27937698\n",
      "Iteration 329, loss = 19223007420.05171204\n",
      "Iteration 330, loss = 19223001339.30240250\n",
      "Iteration 331, loss = 19222995288.67672348\n",
      "Iteration 332, loss = 19222989243.77803802\n",
      "Iteration 333, loss = 19222983184.01054764\n",
      "Iteration 334, loss = 19222977136.30926132\n",
      "Iteration 335, loss = 19222971074.51525116\n",
      "Iteration 336, loss = 19222965020.31931686\n",
      "Iteration 337, loss = 19222958949.43317032\n",
      "Iteration 338, loss = 19222952907.25188065\n",
      "Iteration 339, loss = 19222946895.88682175\n",
      "Iteration 340, loss = 19222940816.07135010\n",
      "Iteration 341, loss = 19222934775.92943954\n",
      "Iteration 342, loss = 19222928688.25772476\n",
      "Iteration 343, loss = 19222922679.49219894\n",
      "Iteration 344, loss = 19222916632.04536057\n",
      "Iteration 345, loss = 19222910573.57688522\n",
      "Iteration 346, loss = 19222904542.18408585\n",
      "Iteration 347, loss = 19222898465.21242523\n",
      "Iteration 348, loss = 19222892425.22663879\n",
      "Iteration 349, loss = 19222886414.78102875\n",
      "Iteration 350, loss = 19222880351.90396881\n",
      "Iteration 351, loss = 19222874274.76468277\n",
      "Iteration 352, loss = 19222868236.92631531\n",
      "Iteration 353, loss = 19222862201.40281296\n",
      "Iteration 354, loss = 19222856169.56241608\n",
      "Iteration 355, loss = 19222850103.69449615\n",
      "Iteration 356, loss = 19222844085.73799515\n",
      "Iteration 357, loss = 19222838054.78502655\n",
      "Iteration 358, loss = 19222832008.09548950\n",
      "Iteration 359, loss = 19222825994.15785217\n",
      "Iteration 360, loss = 19222819945.69490814\n",
      "Iteration 361, loss = 19222813898.08421326\n",
      "Iteration 362, loss = 19222807893.68081665\n",
      "Iteration 363, loss = 19222801869.42835617\n",
      "Iteration 364, loss = 19222795853.87210846\n",
      "Iteration 365, loss = 19222789779.46435547\n",
      "Iteration 366, loss = 19222783800.88077927\n",
      "Iteration 367, loss = 19222777754.96033096\n",
      "Iteration 368, loss = 19222771721.95368958\n",
      "Iteration 369, loss = 19222765701.97849655\n",
      "Iteration 370, loss = 19222759699.56753922\n",
      "Iteration 371, loss = 19222753650.13797379\n",
      "Iteration 372, loss = 19222747667.48579788\n",
      "Iteration 373, loss = 19222741570.07501221\n",
      "Iteration 374, loss = 19222735628.44173431\n",
      "Iteration 375, loss = 19222729548.46875763\n",
      "Iteration 376, loss = 19222723531.87677383\n",
      "Iteration 377, loss = 19222717517.92105865\n",
      "Iteration 378, loss = 19222711497.58295441\n",
      "Iteration 379, loss = 19222705478.48989487\n",
      "Iteration 380, loss = 19222699440.30547333\n",
      "Iteration 381, loss = 19222693444.70220947\n",
      "Iteration 382, loss = 19222687411.94422531\n",
      "Iteration 383, loss = 19222681436.03467941\n",
      "Iteration 384, loss = 19222675377.26294708\n",
      "Iteration 385, loss = 19222669346.96217728\n",
      "Iteration 386, loss = 19222663391.67959213\n",
      "Iteration 387, loss = 19222657326.24565887\n",
      "Iteration 388, loss = 19222651312.94380951\n",
      "Iteration 389, loss = 19222645319.95533371\n",
      "Iteration 390, loss = 19222639297.97355270\n",
      "Iteration 391, loss = 19222633259.35618210\n",
      "Iteration 392, loss = 19222627268.97991180\n",
      "Iteration 393, loss = 19222621265.12471008\n",
      "Iteration 394, loss = 19222615210.66714096\n",
      "Iteration 395, loss = 19222609220.72019958\n",
      "Iteration 396, loss = 19222603208.18811417\n",
      "Iteration 397, loss = 19222597196.43132782\n",
      "Iteration 398, loss = 19222591168.97563553\n",
      "Iteration 399, loss = 19222585228.60318756\n",
      "Iteration 400, loss = 19222579168.08436203\n",
      "Iteration 401, loss = 19222573162.94183731\n",
      "Iteration 402, loss = 19222567177.75381088\n",
      "Iteration 403, loss = 19222561158.86721802\n",
      "Iteration 404, loss = 19222555155.30163193\n",
      "Iteration 405, loss = 19222549146.53843689\n",
      "Iteration 406, loss = 19222543156.51897430\n",
      "Iteration 407, loss = 19222537135.28518677\n",
      "Iteration 408, loss = 19222531153.03773117\n",
      "Iteration 409, loss = 19222525151.89862442\n",
      "Iteration 410, loss = 19222519161.07887268\n",
      "Iteration 411, loss = 19222513147.74835205\n",
      "Iteration 412, loss = 19222507186.75172806\n",
      "Iteration 413, loss = 19222501130.35485077\n",
      "Iteration 414, loss = 19222495146.27104568\n",
      "Iteration 415, loss = 19222489163.25526047\n",
      "Iteration 416, loss = 19222483129.88610077\n",
      "Iteration 417, loss = 19222477127.94809723\n",
      "Iteration 418, loss = 19222471128.62382507\n",
      "Iteration 419, loss = 19222465106.53807449\n",
      "Iteration 420, loss = 19222459149.15499878\n",
      "Iteration 421, loss = 19222453161.30384827\n",
      "Iteration 422, loss = 19222447115.35664749\n",
      "Iteration 423, loss = 19222441131.51264191\n",
      "Iteration 424, loss = 19222435131.97654343\n",
      "Iteration 425, loss = 19222429156.29357147\n",
      "Iteration 426, loss = 19222423175.08518219\n",
      "Iteration 427, loss = 19222417107.51349640\n",
      "Iteration 428, loss = 19222411181.52542877\n",
      "Iteration 429, loss = 19222405167.02542877\n",
      "Iteration 430, loss = 19222399146.52764893\n",
      "Iteration 431, loss = 19222393188.44891739\n",
      "Iteration 432, loss = 19222387151.79575729\n",
      "Iteration 433, loss = 19222381170.28093719\n",
      "Iteration 434, loss = 19222375190.31031799\n",
      "Iteration 435, loss = 19222369191.94081116\n",
      "Iteration 436, loss = 19222363182.17089462\n",
      "Iteration 437, loss = 19222357193.96564865\n",
      "Iteration 438, loss = 19222351214.08472824\n",
      "Iteration 439, loss = 19222345206.58265686\n",
      "Iteration 440, loss = 19222339223.09347916\n",
      "Iteration 441, loss = 19222333223.53063965\n",
      "Iteration 442, loss = 19222327239.13541794\n",
      "Iteration 443, loss = 19222321200.31827164\n",
      "Iteration 444, loss = 19222315238.51314163\n",
      "Iteration 445, loss = 19222309284.93049240\n",
      "Iteration 446, loss = 19222303265.95413971\n",
      "Iteration 447, loss = 19222297268.56776047\n",
      "Iteration 448, loss = 19222291282.40567398\n",
      "Iteration 449, loss = 19222285308.29713058\n",
      "Iteration 450, loss = 19222279301.54055405\n",
      "Iteration 451, loss = 19222273281.89486694\n",
      "Iteration 452, loss = 19222267337.31142426\n",
      "Iteration 453, loss = 19222261367.76798630\n",
      "Iteration 454, loss = 19222255338.75548553\n",
      "Iteration 455, loss = 19222249428.49747086\n",
      "Iteration 456, loss = 19222243403.62958145\n",
      "Iteration 457, loss = 19222237438.49176025\n",
      "Iteration 458, loss = 19222231400.17248535\n",
      "Iteration 459, loss = 19222225431.72344208\n",
      "Iteration 460, loss = 19222219483.55807114\n",
      "Iteration 461, loss = 19222213451.97847748\n",
      "Iteration 462, loss = 19222207483.62992859\n",
      "Iteration 463, loss = 19222201482.97132492\n",
      "Iteration 464, loss = 19222195511.87445068\n",
      "Iteration 465, loss = 19222189552.62114334\n",
      "Iteration 466, loss = 19222183518.91939163\n",
      "Iteration 467, loss = 19222177537.20737839\n",
      "Iteration 468, loss = 19222171569.12501526\n",
      "Iteration 469, loss = 19222165606.51906204\n",
      "Iteration 470, loss = 19222159585.77418518\n",
      "Iteration 471, loss = 19222153626.86386108\n",
      "Iteration 472, loss = 19222147638.71916962\n",
      "Iteration 473, loss = 19222141601.55051422\n",
      "Iteration 474, loss = 19222135643.62520599\n",
      "Iteration 475, loss = 19222129664.76003265\n",
      "Iteration 476, loss = 19222123702.60685349\n",
      "Iteration 477, loss = 19222117674.09547424\n",
      "Iteration 478, loss = 19222111707.03837204\n",
      "Iteration 479, loss = 19222105712.52429199\n",
      "Iteration 480, loss = 19222099729.78980637\n",
      "Iteration 481, loss = 19222093765.60951614\n",
      "Iteration 482, loss = 19222087780.38060760\n",
      "Iteration 483, loss = 19222081833.78282928\n",
      "Iteration 484, loss = 19222075824.42404938\n",
      "Iteration 485, loss = 19222069838.51939774\n",
      "Iteration 486, loss = 19222063872.03616714\n",
      "Iteration 487, loss = 19222057941.18650055\n",
      "Iteration 488, loss = 19222051939.27993393\n",
      "Iteration 489, loss = 19222045959.03377914\n",
      "Iteration 490, loss = 19222039933.73311996\n",
      "Iteration 491, loss = 19222033998.42964935\n",
      "Iteration 492, loss = 19222027990.72935486\n",
      "Iteration 493, loss = 19222022060.57635117\n",
      "Iteration 494, loss = 19222016050.79761505\n",
      "Iteration 495, loss = 19222010063.55411530\n",
      "Iteration 496, loss = 19222004050.58698273\n",
      "Iteration 497, loss = 19221998102.82444382\n",
      "Iteration 498, loss = 19221992120.49621964\n",
      "Iteration 499, loss = 19221986158.82117844\n",
      "Iteration 500, loss = 19221980145.02260590\n",
      "Iteration 1, loss = 19204029680.31111526\n",
      "Iteration 2, loss = 19204009444.54949570\n",
      "Iteration 3, loss = 19203989127.45738220\n",
      "Iteration 4, loss = 19203968287.06892776\n",
      "Iteration 5, loss = 19203946508.47930908\n",
      "Iteration 6, loss = 19203924513.42490768\n",
      "Iteration 7, loss = 19203902475.44897461\n",
      "Iteration 8, loss = 19203879182.40083694\n",
      "Iteration 9, loss = 19203853934.89663315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 19203828477.48357773\n",
      "Iteration 11, loss = 19203800842.14887238\n",
      "Iteration 12, loss = 19203771670.58734894\n",
      "Iteration 13, loss = 19203744002.56703568\n",
      "Iteration 14, loss = 19203719901.91313934\n",
      "Iteration 15, loss = 19203695671.52560425\n",
      "Iteration 16, loss = 19203669486.85149002\n",
      "Iteration 17, loss = 19203645387.25576019\n",
      "Iteration 18, loss = 19203622486.32218933\n",
      "Iteration 19, loss = 19203599418.40811539\n",
      "Iteration 20, loss = 19203577350.12702560\n",
      "Iteration 21, loss = 19203555393.84100723\n",
      "Iteration 22, loss = 19203531654.96068954\n",
      "Iteration 23, loss = 19203507829.42569733\n",
      "Iteration 24, loss = 19203486411.23812103\n",
      "Iteration 25, loss = 19203466307.90058517\n",
      "Iteration 26, loss = 19203447813.76525116\n",
      "Iteration 27, loss = 19203430497.01518250\n",
      "Iteration 28, loss = 19203414231.60929489\n",
      "Iteration 29, loss = 19203399818.07426453\n",
      "Iteration 30, loss = 19203386378.50759506\n",
      "Iteration 31, loss = 19203373941.54582214\n",
      "Iteration 32, loss = 19203362386.03619003\n",
      "Iteration 33, loss = 19203351192.07061386\n",
      "Iteration 34, loss = 19203340451.32374954\n",
      "Iteration 35, loss = 19203330211.88047028\n",
      "Iteration 36, loss = 19203319966.64859772\n",
      "Iteration 37, loss = 19203309773.06616974\n",
      "Iteration 38, loss = 19203299805.82681656\n",
      "Iteration 39, loss = 19203290290.22467804\n",
      "Iteration 40, loss = 19203280709.36608887\n",
      "Iteration 41, loss = 19203271393.35821152\n",
      "Iteration 42, loss = 19203262296.77056122\n",
      "Iteration 43, loss = 19203253222.15725327\n",
      "Iteration 44, loss = 19203244233.98844910\n",
      "Iteration 45, loss = 19203235384.93734741\n",
      "Iteration 46, loss = 19203226530.65399551\n",
      "Iteration 47, loss = 19203217720.46850967\n",
      "Iteration 48, loss = 19203208882.28878403\n",
      "Iteration 49, loss = 19203199946.95617676\n",
      "Iteration 50, loss = 19203190948.09218979\n",
      "Iteration 51, loss = 19203182179.97724152\n",
      "Iteration 52, loss = 19203173438.91686630\n",
      "Iteration 53, loss = 19203164674.22529984\n",
      "Iteration 54, loss = 19203155982.80405426\n",
      "Iteration 55, loss = 19203147197.31388474\n",
      "Iteration 56, loss = 19203138377.64135361\n",
      "Iteration 57, loss = 19203129823.89402390\n",
      "Iteration 58, loss = 19203121353.87307358\n",
      "Iteration 59, loss = 19203112831.95458603\n",
      "Iteration 60, loss = 19203104396.91597748\n",
      "Iteration 61, loss = 19203095917.63283920\n",
      "Iteration 62, loss = 19203087420.79462814\n",
      "Iteration 63, loss = 19203078829.16960144\n",
      "Iteration 64, loss = 19203070172.80174255\n",
      "Iteration 65, loss = 19203061445.95180130\n",
      "Iteration 66, loss = 19203052639.75085449\n",
      "Iteration 67, loss = 19203043739.77634811\n",
      "Iteration 68, loss = 19203034720.97676468\n",
      "Iteration 69, loss = 19203025760.03007889\n",
      "Iteration 70, loss = 19203016740.97716522\n",
      "Iteration 71, loss = 19203007803.39330673\n",
      "Iteration 72, loss = 19202999028.46297836\n",
      "Iteration 73, loss = 19202990220.50814438\n",
      "Iteration 74, loss = 19202981520.13579559\n",
      "Iteration 75, loss = 19202972964.55826569\n",
      "Iteration 76, loss = 19202964395.28359604\n",
      "Iteration 77, loss = 19202955866.09466171\n",
      "Iteration 78, loss = 19202947459.28606796\n",
      "Iteration 79, loss = 19202938982.95255661\n",
      "Iteration 80, loss = 19202930667.19369888\n",
      "Iteration 81, loss = 19202922266.76039505\n",
      "Iteration 82, loss = 19202913961.54122162\n",
      "Iteration 83, loss = 19202905585.95080566\n",
      "Iteration 84, loss = 19202897358.87314224\n",
      "Iteration 85, loss = 19202889132.17023087\n",
      "Iteration 86, loss = 19202880883.26688385\n",
      "Iteration 87, loss = 19202872668.83563614\n",
      "Iteration 88, loss = 19202864524.63331985\n",
      "Iteration 89, loss = 19202856403.56134033\n",
      "Iteration 90, loss = 19202848256.45256805\n",
      "Iteration 91, loss = 19202840193.44206619\n",
      "Iteration 92, loss = 19202832114.72307968\n",
      "Iteration 93, loss = 19202824088.76871490\n",
      "Iteration 94, loss = 19202816046.12034988\n",
      "Iteration 95, loss = 19202808001.41658020\n",
      "Iteration 96, loss = 19202799969.35793304\n",
      "Iteration 97, loss = 19202791989.00914383\n",
      "Iteration 98, loss = 19202784031.54663467\n",
      "Iteration 99, loss = 19202776011.85914993\n",
      "Iteration 100, loss = 19202768011.80976105\n",
      "Iteration 101, loss = 19202760009.07848740\n",
      "Iteration 102, loss = 19202751985.87957764\n",
      "Iteration 103, loss = 19202743961.10047913\n",
      "Iteration 104, loss = 19202736059.94433212\n",
      "Iteration 105, loss = 19202728048.94803238\n",
      "Iteration 106, loss = 19202720063.50051117\n",
      "Iteration 107, loss = 19202712053.67153931\n",
      "Iteration 108, loss = 19202704103.74194336\n",
      "Iteration 109, loss = 19202696062.87361908\n",
      "Iteration 110, loss = 19202688138.28784943\n",
      "Iteration 111, loss = 19202680212.40522003\n",
      "Iteration 112, loss = 19202672379.09934235\n",
      "Iteration 113, loss = 19202664472.15887070\n",
      "Iteration 114, loss = 19202656635.43365860\n",
      "Iteration 115, loss = 19202648764.42082214\n",
      "Iteration 116, loss = 19202640983.63218307\n",
      "Iteration 117, loss = 19202633087.37279892\n",
      "Iteration 118, loss = 19202625316.84566879\n",
      "Iteration 119, loss = 19202617464.17985916\n",
      "Iteration 120, loss = 19202609643.22714996\n",
      "Iteration 121, loss = 19202601765.50974274\n",
      "Iteration 122, loss = 19202593884.74316406\n",
      "Iteration 123, loss = 19202586052.28292465\n",
      "Iteration 124, loss = 19202578257.15033722\n",
      "Iteration 125, loss = 19202570507.32899857\n",
      "Iteration 126, loss = 19202562710.36666870\n",
      "Iteration 127, loss = 19202554912.84766769\n",
      "Iteration 128, loss = 19202547096.14733505\n",
      "Iteration 129, loss = 19202539348.31403351\n",
      "Iteration 130, loss = 19202531515.35906219\n",
      "Iteration 131, loss = 19202523812.09945297\n",
      "Iteration 132, loss = 19202516052.82982254\n",
      "Iteration 133, loss = 19202508310.17642212\n",
      "Iteration 134, loss = 19202500589.86255646\n",
      "Iteration 135, loss = 19202492862.37554932\n",
      "Iteration 136, loss = 19202485151.12049484\n",
      "Iteration 137, loss = 19202477408.18830490\n",
      "Iteration 138, loss = 19202469736.68991852\n",
      "Iteration 139, loss = 19202462006.64282608\n",
      "Iteration 140, loss = 19202454362.43534470\n",
      "Iteration 141, loss = 19202446679.43570328\n",
      "Iteration 142, loss = 19202439073.81257629\n",
      "Iteration 143, loss = 19202431427.03784561\n",
      "Iteration 144, loss = 19202423837.56199265\n",
      "Iteration 145, loss = 19202416218.93854523\n",
      "Iteration 146, loss = 19202408620.32479095\n",
      "Iteration 147, loss = 19202401021.57720947\n",
      "Iteration 148, loss = 19202393318.95029068\n",
      "Iteration 149, loss = 19202385716.17726898\n",
      "Iteration 150, loss = 19202378071.91669846\n",
      "Iteration 151, loss = 19202370398.81934738\n",
      "Iteration 152, loss = 19202362816.58240128\n",
      "Iteration 153, loss = 19202355089.43174362\n",
      "Iteration 154, loss = 19202347539.47760010\n",
      "Iteration 155, loss = 19202339852.49560547\n",
      "Iteration 156, loss = 19202332222.77122116\n",
      "Iteration 157, loss = 19202324646.48756409\n",
      "Iteration 158, loss = 19202317015.78417587\n",
      "Iteration 159, loss = 19202309389.90415955\n",
      "Iteration 160, loss = 19202301714.99901581\n",
      "Iteration 161, loss = 19202294074.29493332\n",
      "Iteration 162, loss = 19202286414.77002335\n",
      "Iteration 163, loss = 19202278787.22114182\n",
      "Iteration 164, loss = 19202271179.41689682\n",
      "Iteration 165, loss = 19202263592.55967712\n",
      "Iteration 166, loss = 19202255986.97696304\n",
      "Iteration 167, loss = 19202248466.90014648\n",
      "Iteration 168, loss = 19202240884.37971115\n",
      "Iteration 169, loss = 19202233357.24375916\n",
      "Iteration 170, loss = 19202225840.39073181\n",
      "Iteration 171, loss = 19202218317.03185272\n",
      "Iteration 172, loss = 19202210691.02722931\n",
      "Iteration 173, loss = 19202203105.56943130\n",
      "Iteration 174, loss = 19202195571.11794281\n",
      "Iteration 175, loss = 19202187959.14500046\n",
      "Iteration 176, loss = 19202180375.79222107\n",
      "Iteration 177, loss = 19202172815.47235489\n",
      "Iteration 178, loss = 19202165238.59555817\n",
      "Iteration 179, loss = 19202157623.65076447\n",
      "Iteration 180, loss = 19202150076.31470108\n",
      "Iteration 181, loss = 19202142496.78264618\n",
      "Iteration 182, loss = 19202134893.14218903\n",
      "Iteration 183, loss = 19202127343.99934387\n",
      "Iteration 184, loss = 19202119780.65130615\n",
      "Iteration 185, loss = 19202112244.51799774\n",
      "Iteration 186, loss = 19202104695.08272171\n",
      "Iteration 187, loss = 19202097155.50181580\n",
      "Iteration 188, loss = 19202089625.27686691\n",
      "Iteration 189, loss = 19202082096.84787369\n",
      "Iteration 190, loss = 19202074633.90179825\n",
      "Iteration 191, loss = 19202067096.47478104\n",
      "Iteration 192, loss = 19202059556.61669922\n",
      "Iteration 193, loss = 19202052039.65971756\n",
      "Iteration 194, loss = 19202044505.84371948\n",
      "Iteration 195, loss = 19202036979.41653061\n",
      "Iteration 196, loss = 19202029494.45090103\n",
      "Iteration 197, loss = 19202021922.48442841\n",
      "Iteration 198, loss = 19202014396.27658844\n",
      "Iteration 199, loss = 19202006919.09064484\n",
      "Iteration 200, loss = 19201999409.72538376\n",
      "Iteration 201, loss = 19201991880.09861755\n",
      "Iteration 202, loss = 19201984374.87442780\n",
      "Iteration 203, loss = 19201976855.57466125\n",
      "Iteration 204, loss = 19201969304.65649033\n",
      "Iteration 205, loss = 19201961778.64827728\n",
      "Iteration 206, loss = 19201954306.80669403\n",
      "Iteration 207, loss = 19201946825.14779282\n",
      "Iteration 208, loss = 19201939286.15588760\n",
      "Iteration 209, loss = 19201931804.36147690\n",
      "Iteration 210, loss = 19201924331.23978806\n",
      "Iteration 211, loss = 19201916795.71811295\n",
      "Iteration 212, loss = 19201909297.84032440\n",
      "Iteration 213, loss = 19201901840.59098434\n",
      "Iteration 214, loss = 19201894327.64727783\n",
      "Iteration 215, loss = 19201886855.93029404\n",
      "Iteration 216, loss = 19201879400.16608429\n",
      "Iteration 217, loss = 19201871866.52579880\n",
      "Iteration 218, loss = 19201864362.05024338\n",
      "Iteration 219, loss = 19201856843.55331802\n",
      "Iteration 220, loss = 19201849351.47044373\n",
      "Iteration 221, loss = 19201841781.15938187\n",
      "Iteration 222, loss = 19201834354.62921906\n",
      "Iteration 223, loss = 19201826832.80694962\n",
      "Iteration 224, loss = 19201819378.58028030\n",
      "Iteration 225, loss = 19201811944.03499222\n",
      "Iteration 226, loss = 19201804397.88137817\n",
      "Iteration 227, loss = 19201796936.51625824\n",
      "Iteration 228, loss = 19201789431.89237213\n",
      "Iteration 229, loss = 19201781895.28138351\n",
      "Iteration 230, loss = 19201774443.75622940\n",
      "Iteration 231, loss = 19201766976.33967590\n",
      "Iteration 232, loss = 19201759478.68464279\n",
      "Iteration 233, loss = 19201752037.89677429\n",
      "Iteration 234, loss = 19201744651.06811523\n",
      "Iteration 235, loss = 19201737237.79336166\n",
      "Iteration 236, loss = 19201729784.01743698\n",
      "Iteration 237, loss = 19201722323.62665176\n",
      "Iteration 238, loss = 19201714910.04154205\n",
      "Iteration 239, loss = 19201707507.11191177\n",
      "Iteration 240, loss = 19201700061.29112244\n",
      "Iteration 241, loss = 19201692605.56083679\n",
      "Iteration 242, loss = 19201685156.17321014\n",
      "Iteration 243, loss = 19201677701.54457092\n",
      "Iteration 244, loss = 19201670206.42472076\n",
      "Iteration 245, loss = 19201662711.08851624\n",
      "Iteration 246, loss = 19201655268.11167145\n",
      "Iteration 247, loss = 19201647806.73187256\n",
      "Iteration 248, loss = 19201640324.83708572\n",
      "Iteration 249, loss = 19201632849.86064148\n",
      "Iteration 250, loss = 19201625416.90025330\n",
      "Iteration 251, loss = 19201617916.87499237\n",
      "Iteration 252, loss = 19201610484.44211197\n",
      "Iteration 253, loss = 19201603085.93151093\n",
      "Iteration 254, loss = 19201595630.92965317\n",
      "Iteration 255, loss = 19201588198.78676224\n",
      "Iteration 256, loss = 19201580806.98629761\n",
      "Iteration 257, loss = 19201573363.81626129\n",
      "Iteration 258, loss = 19201565838.71801758\n",
      "Iteration 259, loss = 19201558397.98386765\n",
      "Iteration 260, loss = 19201550864.45000458\n",
      "Iteration 261, loss = 19201543400.89720917\n",
      "Iteration 262, loss = 19201535909.20132446\n",
      "Iteration 263, loss = 19201528491.05269241\n",
      "Iteration 264, loss = 19201521004.19961548\n",
      "Iteration 265, loss = 19201513524.32584000\n",
      "Iteration 266, loss = 19201506121.08402634\n",
      "Iteration 267, loss = 19201498629.46599960\n",
      "Iteration 268, loss = 19201491269.27736664\n",
      "Iteration 269, loss = 19201483800.23130798\n",
      "Iteration 270, loss = 19201476322.32769394\n",
      "Iteration 271, loss = 19201468934.44606781\n",
      "Iteration 272, loss = 19201461457.28651428\n",
      "Iteration 273, loss = 19201454054.03960419\n",
      "Iteration 274, loss = 19201446577.03596497\n",
      "Iteration 275, loss = 19201439164.90739822\n",
      "Iteration 276, loss = 19201431743.34803009\n",
      "Iteration 277, loss = 19201424276.53430176\n",
      "Iteration 278, loss = 19201416844.66452026\n",
      "Iteration 279, loss = 19201409447.30332947\n",
      "Iteration 280, loss = 19201401979.05602264\n",
      "Iteration 281, loss = 19201394595.20383453\n",
      "Iteration 282, loss = 19201387103.01879501\n",
      "Iteration 283, loss = 19201379720.46213913\n",
      "Iteration 284, loss = 19201372219.36702347\n",
      "Iteration 285, loss = 19201364823.88605881\n",
      "Iteration 286, loss = 19201357320.68357086\n",
      "Iteration 287, loss = 19201349903.82955551\n",
      "Iteration 288, loss = 19201342445.10631561\n",
      "Iteration 289, loss = 19201335037.06423187\n",
      "Iteration 290, loss = 19201327581.26449203\n",
      "Iteration 291, loss = 19201320175.22334290\n",
      "Iteration 292, loss = 19201312805.34506989\n",
      "Iteration 293, loss = 19201305359.14282608\n",
      "Iteration 294, loss = 19201297891.89658737\n",
      "Iteration 295, loss = 19201290490.54840088\n",
      "Iteration 296, loss = 19201283004.49271011\n",
      "Iteration 297, loss = 19201275540.93528366\n",
      "Iteration 298, loss = 19201268028.06916809\n",
      "Iteration 299, loss = 19201260535.54998779\n",
      "Iteration 300, loss = 19201253014.74577332\n",
      "Iteration 301, loss = 19201245511.50479889\n",
      "Iteration 302, loss = 19201237932.10388184\n",
      "Iteration 303, loss = 19201230165.28776550\n",
      "Iteration 304, loss = 19201222048.29803848\n",
      "Iteration 305, loss = 19201213585.69966507\n",
      "Iteration 306, loss = 19201204597.09604263\n",
      "Iteration 307, loss = 19201195326.20002365\n",
      "Iteration 308, loss = 19201185995.16862106\n",
      "Iteration 309, loss = 19201176877.16184616\n",
      "Iteration 310, loss = 19201167909.96866608\n",
      "Iteration 311, loss = 19201159128.96854782\n",
      "Iteration 312, loss = 19201150526.75771713\n",
      "Iteration 313, loss = 19201141997.58372116\n",
      "Iteration 314, loss = 19201133529.15481186\n",
      "Iteration 315, loss = 19201125276.69961166\n",
      "Iteration 316, loss = 19201116919.42789841\n",
      "Iteration 317, loss = 19201108673.44842911\n",
      "Iteration 318, loss = 19201100457.54616165\n",
      "Iteration 319, loss = 19201092266.60641861\n",
      "Iteration 320, loss = 19201084106.92773819\n",
      "Iteration 321, loss = 19201075972.71215057\n",
      "Iteration 322, loss = 19201067872.90086746\n",
      "Iteration 323, loss = 19201059792.75302124\n",
      "Iteration 324, loss = 19201051698.85271835\n",
      "Iteration 325, loss = 19201043577.65944290\n",
      "Iteration 326, loss = 19201035565.62570953\n",
      "Iteration 327, loss = 19201027426.63582230\n",
      "Iteration 328, loss = 19201019423.49676514\n",
      "Iteration 329, loss = 19201011428.53997040\n",
      "Iteration 330, loss = 19201003518.02509689\n",
      "Iteration 331, loss = 19200995511.69281006\n",
      "Iteration 332, loss = 19200987585.98538971\n",
      "Iteration 333, loss = 19200979584.92023468\n",
      "Iteration 334, loss = 19200971716.24674988\n",
      "Iteration 335, loss = 19200963771.28786087\n",
      "Iteration 336, loss = 19200955865.59999466\n",
      "Iteration 337, loss = 19200947989.32414627\n",
      "Iteration 338, loss = 19200940096.87882614\n",
      "Iteration 339, loss = 19200932260.80899048\n",
      "Iteration 340, loss = 19200924344.64385986\n",
      "Iteration 341, loss = 19200916501.23235703\n",
      "Iteration 342, loss = 19200908599.35345078\n",
      "Iteration 343, loss = 19200900708.48699570\n",
      "Iteration 344, loss = 19200892841.86320114\n",
      "Iteration 345, loss = 19200884998.77169037\n",
      "Iteration 346, loss = 19200877139.20599365\n",
      "Iteration 347, loss = 19200869348.98303986\n",
      "Iteration 348, loss = 19200861429.92938995\n",
      "Iteration 349, loss = 19200853686.76237488\n",
      "Iteration 350, loss = 19200845860.93209457\n",
      "Iteration 351, loss = 19200838020.61339569\n",
      "Iteration 352, loss = 19200830186.67408371\n",
      "Iteration 353, loss = 19200822439.48287582\n",
      "Iteration 354, loss = 19200814557.15322876\n",
      "Iteration 355, loss = 19200806758.87086487\n",
      "Iteration 356, loss = 19200798910.07189178\n",
      "Iteration 357, loss = 19200791106.01739502\n",
      "Iteration 358, loss = 19200783328.60116959\n",
      "Iteration 359, loss = 19200775438.28927994\n",
      "Iteration 360, loss = 19200767667.97437668\n",
      "Iteration 361, loss = 19200759846.63859177\n",
      "Iteration 362, loss = 19200752039.37493515\n",
      "Iteration 363, loss = 19200744214.81142426\n",
      "Iteration 364, loss = 19200736426.42335129\n",
      "Iteration 365, loss = 19200728602.35809326\n",
      "Iteration 366, loss = 19200720892.78067398\n",
      "Iteration 367, loss = 19200713152.16975021\n",
      "Iteration 368, loss = 19200705388.19877625\n",
      "Iteration 369, loss = 19200697674.80367661\n",
      "Iteration 370, loss = 19200689941.04423141\n",
      "Iteration 371, loss = 19200682178.91688919\n",
      "Iteration 372, loss = 19200674418.70724869\n",
      "Iteration 373, loss = 19200666643.70041656\n",
      "Iteration 374, loss = 19200658924.01585007\n",
      "Iteration 375, loss = 19200651130.67453003\n",
      "Iteration 376, loss = 19200643431.56354141\n",
      "Iteration 377, loss = 19200635718.53708649\n",
      "Iteration 378, loss = 19200627914.45699692\n",
      "Iteration 379, loss = 19200620207.36648941\n",
      "Iteration 380, loss = 19200612409.83557510\n",
      "Iteration 381, loss = 19200604647.74726486\n",
      "Iteration 382, loss = 19200596923.65101624\n",
      "Iteration 383, loss = 19200589206.89322281\n",
      "Iteration 384, loss = 19200581436.79084015\n",
      "Iteration 385, loss = 19200573736.76178741\n",
      "Iteration 386, loss = 19200565969.32553864\n",
      "Iteration 387, loss = 19200558279.35809708\n",
      "Iteration 388, loss = 19200550598.23767471\n",
      "Iteration 389, loss = 19200542854.67257309\n",
      "Iteration 390, loss = 19200535170.73620987\n",
      "Iteration 391, loss = 19200527524.18152237\n",
      "Iteration 392, loss = 19200519793.09983444\n",
      "Iteration 393, loss = 19200512058.44869995\n",
      "Iteration 394, loss = 19200504373.92324829\n",
      "Iteration 395, loss = 19200496626.71370697\n",
      "Iteration 396, loss = 19200488933.54149628\n",
      "Iteration 397, loss = 19200481244.30947113\n",
      "Iteration 398, loss = 19200473545.22734451\n",
      "Iteration 399, loss = 19200465930.62351608\n",
      "Iteration 400, loss = 19200458194.31461716\n",
      "Iteration 401, loss = 19200450481.00248337\n",
      "Iteration 402, loss = 19200442779.96941376\n",
      "Iteration 403, loss = 19200435081.28905487\n",
      "Iteration 404, loss = 19200427396.42881393\n",
      "Iteration 405, loss = 19200419698.78264999\n",
      "Iteration 406, loss = 19200412001.11951447\n",
      "Iteration 407, loss = 19200404309.17471695\n",
      "Iteration 408, loss = 19200396618.88357162\n",
      "Iteration 409, loss = 19200388836.05215073\n",
      "Iteration 410, loss = 19200381174.53966141\n",
      "Iteration 411, loss = 19200373430.63442230\n",
      "Iteration 412, loss = 19200365637.59699249\n",
      "Iteration 413, loss = 19200357947.57198715\n",
      "Iteration 414, loss = 19200350173.98915100\n",
      "Iteration 415, loss = 19200342384.59846115\n",
      "Iteration 416, loss = 19200334604.73353577\n",
      "Iteration 417, loss = 19200326775.99169159\n",
      "Iteration 418, loss = 19200318833.30610275\n",
      "Iteration 419, loss = 19200310830.58818436\n",
      "Iteration 420, loss = 19200302519.68311310\n",
      "Iteration 421, loss = 19200293617.37287521\n",
      "Iteration 422, loss = 19200284345.27386475\n",
      "Iteration 423, loss = 19200274600.47297287\n",
      "Iteration 424, loss = 19200264929.36676407\n",
      "Iteration 425, loss = 19200255390.62843323\n",
      "Iteration 426, loss = 19200246191.40856552\n",
      "Iteration 427, loss = 19200237195.53398132\n",
      "Iteration 428, loss = 19200228223.85343933\n",
      "Iteration 429, loss = 19200219391.20171738\n",
      "Iteration 430, loss = 19200210641.43490601\n",
      "Iteration 431, loss = 19200201931.90335083\n",
      "Iteration 432, loss = 19200193328.54894638\n",
      "Iteration 433, loss = 19200184714.34283829\n",
      "Iteration 434, loss = 19200176214.59128189\n",
      "Iteration 435, loss = 19200167704.23257828\n",
      "Iteration 436, loss = 19200159235.15725327\n",
      "Iteration 437, loss = 19200150829.00865936\n",
      "Iteration 438, loss = 19200142411.95958710\n",
      "Iteration 439, loss = 19200134154.54074097\n",
      "Iteration 440, loss = 19200125775.95912170\n",
      "Iteration 441, loss = 19200117506.48468399\n",
      "Iteration 442, loss = 19200109172.06311798\n",
      "Iteration 443, loss = 19200100964.03693771\n",
      "Iteration 444, loss = 19200092742.67512131\n",
      "Iteration 445, loss = 19200084490.82498932\n",
      "Iteration 446, loss = 19200076271.44817734\n",
      "Iteration 447, loss = 19200068061.07226562\n",
      "Iteration 448, loss = 19200059833.87181854\n",
      "Iteration 449, loss = 19200051620.60379791\n",
      "Iteration 450, loss = 19200043444.31000900\n",
      "Iteration 451, loss = 19200035170.89378738\n",
      "Iteration 452, loss = 19200026998.32727051\n",
      "Iteration 453, loss = 19200018815.83496475\n",
      "Iteration 454, loss = 19200010570.50839233\n",
      "Iteration 455, loss = 19200002397.45585251\n",
      "Iteration 456, loss = 19199994258.11172104\n",
      "Iteration 457, loss = 19199986152.69092560\n",
      "Iteration 458, loss = 19199978096.70561600\n",
      "Iteration 459, loss = 19199969956.06748199\n",
      "Iteration 460, loss = 19199961839.47248077\n",
      "Iteration 461, loss = 19199953692.44100571\n",
      "Iteration 462, loss = 19199945578.00789642\n",
      "Iteration 463, loss = 19199937524.35916138\n",
      "Iteration 464, loss = 19199929468.60583878\n",
      "Iteration 465, loss = 19199921413.67429352\n",
      "Iteration 466, loss = 19199913333.88241577\n",
      "Iteration 467, loss = 19199905314.06503296\n",
      "Iteration 468, loss = 19199897258.80324936\n",
      "Iteration 469, loss = 19199889290.09775543\n",
      "Iteration 470, loss = 19199881296.34941864\n",
      "Iteration 471, loss = 19199873237.18997574\n",
      "Iteration 472, loss = 19199865253.77070999\n",
      "Iteration 473, loss = 19199857235.54508209\n",
      "Iteration 474, loss = 19199849166.85066605\n",
      "Iteration 475, loss = 19199841177.88579559\n",
      "Iteration 476, loss = 19199833069.68236923\n",
      "Iteration 477, loss = 19199825047.46208572\n",
      "Iteration 478, loss = 19199817031.29029083\n",
      "Iteration 479, loss = 19199808976.45925140\n",
      "Iteration 480, loss = 19199800984.60452271\n",
      "Iteration 481, loss = 19199792994.54899216\n",
      "Iteration 482, loss = 19199785003.53870010\n",
      "Iteration 483, loss = 19199777062.13755798\n",
      "Iteration 484, loss = 19199769070.48699570\n",
      "Iteration 485, loss = 19199761088.71935654\n",
      "Iteration 486, loss = 19199753081.57117462\n",
      "Iteration 487, loss = 19199745011.80704117\n",
      "Iteration 488, loss = 19199736941.97018433\n",
      "Iteration 489, loss = 19199728852.22980881\n",
      "Iteration 490, loss = 19199720905.54645538\n",
      "Iteration 491, loss = 19199712814.79267120\n",
      "Iteration 492, loss = 19199704801.56334305\n",
      "Iteration 493, loss = 19199696820.69109726\n",
      "Iteration 494, loss = 19199688849.71673203\n",
      "Iteration 495, loss = 19199680933.44832230\n",
      "Iteration 496, loss = 19199672934.03952789\n",
      "Iteration 497, loss = 19199665020.81486893\n",
      "Iteration 498, loss = 19199657053.83403778\n",
      "Iteration 499, loss = 19199649152.51959610\n",
      "Iteration 500, loss = 19199641081.50382614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19193757307.37649536\n",
      "Iteration 2, loss = 19193738322.93878937\n",
      "Iteration 3, loss = 19193719556.88265991\n",
      "Iteration 4, loss = 19193700086.43210220\n",
      "Iteration 5, loss = 19193679323.98862076\n",
      "Iteration 6, loss = 19193658020.81785583\n",
      "Iteration 7, loss = 19193636064.00794601\n",
      "Iteration 8, loss = 19193614060.71626282\n",
      "Iteration 9, loss = 19193592797.89460754\n",
      "Iteration 10, loss = 19193571521.01971436\n",
      "Iteration 11, loss = 19193550145.06711960\n",
      "Iteration 12, loss = 19193528670.30910110\n",
      "Iteration 13, loss = 19193505908.89410019\n",
      "Iteration 14, loss = 19193482667.21595764\n",
      "Iteration 15, loss = 19193461156.90984344\n",
      "Iteration 16, loss = 19193439917.37041473\n",
      "Iteration 17, loss = 19193418740.84685516\n",
      "Iteration 18, loss = 19193398546.21846008\n",
      "Iteration 19, loss = 19193379790.93571472\n",
      "Iteration 20, loss = 19193362762.95495605\n",
      "Iteration 21, loss = 19193345361.33510971\n",
      "Iteration 22, loss = 19193326994.96060562\n",
      "Iteration 23, loss = 19193311054.58944321\n",
      "Iteration 24, loss = 19193297140.68815231\n",
      "Iteration 25, loss = 19193284462.48083878\n",
      "Iteration 26, loss = 19193272462.40773392\n",
      "Iteration 27, loss = 19193261080.62870026\n",
      "Iteration 28, loss = 19193250169.03535843\n",
      "Iteration 29, loss = 19193238691.90391922\n",
      "Iteration 30, loss = 19193227359.76808929\n",
      "Iteration 31, loss = 19193216146.82399368\n",
      "Iteration 32, loss = 19193205284.90929413\n",
      "Iteration 33, loss = 19193195453.97960281\n",
      "Iteration 34, loss = 19193186192.34778976\n",
      "Iteration 35, loss = 19193176910.15440369\n",
      "Iteration 36, loss = 19193167633.38546371\n",
      "Iteration 37, loss = 19193158812.07948303\n",
      "Iteration 38, loss = 19193150575.46712494\n",
      "Iteration 39, loss = 19193142591.19967270\n",
      "Iteration 40, loss = 19193134806.42992020\n",
      "Iteration 41, loss = 19193126914.82969666\n",
      "Iteration 42, loss = 19193119130.16857147\n",
      "Iteration 43, loss = 19193111363.95880508\n",
      "Iteration 44, loss = 19193103875.27980042\n",
      "Iteration 45, loss = 19193096531.49348450\n",
      "Iteration 46, loss = 19193089125.29301453\n",
      "Iteration 47, loss = 19193081736.58858109\n",
      "Iteration 48, loss = 19193074452.00440216\n",
      "Iteration 49, loss = 19193067166.71480179\n",
      "Iteration 50, loss = 19193059859.06666565\n",
      "Iteration 51, loss = 19193052492.00125885\n",
      "Iteration 52, loss = 19193045091.00828552\n",
      "Iteration 53, loss = 19193037686.98696518\n",
      "Iteration 54, loss = 19193030126.71427155\n",
      "Iteration 55, loss = 19193022524.82876587\n",
      "Iteration 56, loss = 19193014902.70434952\n",
      "Iteration 57, loss = 19193007222.46805954\n",
      "Iteration 58, loss = 19192999627.08961105\n",
      "Iteration 59, loss = 19192992105.28392410\n",
      "Iteration 60, loss = 19192984571.47314072\n",
      "Iteration 61, loss = 19192977115.05728912\n",
      "Iteration 62, loss = 19192969697.26400757\n",
      "Iteration 63, loss = 19192962343.39335632\n",
      "Iteration 64, loss = 19192955024.55081558\n",
      "Iteration 65, loss = 19192947745.57285690\n",
      "Iteration 66, loss = 19192940538.81661987\n",
      "Iteration 67, loss = 19192933296.12027359\n",
      "Iteration 68, loss = 19192926216.93373108\n",
      "Iteration 69, loss = 19192919067.90566635\n",
      "Iteration 70, loss = 19192911977.54229736\n",
      "Iteration 71, loss = 19192904942.81095123\n",
      "Iteration 72, loss = 19192897943.41187668\n",
      "Iteration 73, loss = 19192890886.89757156\n",
      "Iteration 74, loss = 19192883949.41086578\n",
      "Iteration 75, loss = 19192877017.39365005\n",
      "Iteration 76, loss = 19192870095.48363495\n",
      "Iteration 77, loss = 19192863199.04338455\n",
      "Iteration 78, loss = 19192856400.73538208\n",
      "Iteration 79, loss = 19192849497.72707748\n",
      "Iteration 80, loss = 19192842636.83233261\n",
      "Iteration 81, loss = 19192835864.59535599\n",
      "Iteration 82, loss = 19192829089.04148483\n",
      "Iteration 83, loss = 19192822280.16495132\n",
      "Iteration 84, loss = 19192815508.08164978\n",
      "Iteration 85, loss = 19192808837.30662537\n",
      "Iteration 86, loss = 19192802060.03607941\n",
      "Iteration 87, loss = 19192795378.60125732\n",
      "Iteration 88, loss = 19192788657.20224762\n",
      "Iteration 89, loss = 19192781958.51480865\n",
      "Iteration 90, loss = 19192775319.07917023\n",
      "Iteration 91, loss = 19192768643.36192322\n",
      "Iteration 92, loss = 19192761973.61776352\n",
      "Iteration 93, loss = 19192755355.02686691\n",
      "Iteration 94, loss = 19192748723.49315643\n",
      "Iteration 95, loss = 19192742075.63142014\n",
      "Iteration 96, loss = 19192735496.45724869\n",
      "Iteration 97, loss = 19192728907.86631012\n",
      "Iteration 98, loss = 19192722299.92660141\n",
      "Iteration 99, loss = 19192715772.00125122\n",
      "Iteration 100, loss = 19192709182.78732300\n",
      "Iteration 101, loss = 19192702624.54925919\n",
      "Iteration 102, loss = 19192696061.70310211\n",
      "Iteration 103, loss = 19192689533.89545059\n",
      "Iteration 104, loss = 19192682986.78127670\n",
      "Iteration 105, loss = 19192676486.08859253\n",
      "Iteration 106, loss = 19192669981.24327850\n",
      "Iteration 107, loss = 19192663453.12495804\n",
      "Iteration 108, loss = 19192656952.00635529\n",
      "Iteration 109, loss = 19192650476.41252518\n",
      "Iteration 110, loss = 19192643971.11418152\n",
      "Iteration 111, loss = 19192637517.81107712\n",
      "Iteration 112, loss = 19192631015.28148270\n",
      "Iteration 113, loss = 19192624559.52731705\n",
      "Iteration 114, loss = 19192618064.83249283\n",
      "Iteration 115, loss = 19192611681.18300247\n",
      "Iteration 116, loss = 19192605169.57217407\n",
      "Iteration 117, loss = 19192598769.09341049\n",
      "Iteration 118, loss = 19192592346.28494644\n",
      "Iteration 119, loss = 19192585894.15465927\n",
      "Iteration 120, loss = 19192579470.22589874\n",
      "Iteration 121, loss = 19192573047.28882599\n",
      "Iteration 122, loss = 19192566619.51025009\n",
      "Iteration 123, loss = 19192560208.34408951\n",
      "Iteration 124, loss = 19192553817.77809906\n",
      "Iteration 125, loss = 19192547436.91160965\n",
      "Iteration 126, loss = 19192541027.58614731\n",
      "Iteration 127, loss = 19192534662.12775421\n",
      "Iteration 128, loss = 19192528211.37337875\n",
      "Iteration 129, loss = 19192521926.06236649\n",
      "Iteration 130, loss = 19192515485.08453751\n",
      "Iteration 131, loss = 19192509113.31824493\n",
      "Iteration 132, loss = 19192502712.17583847\n",
      "Iteration 133, loss = 19192496365.70072937\n",
      "Iteration 134, loss = 19192489938.57827759\n",
      "Iteration 135, loss = 19192483596.17307281\n",
      "Iteration 136, loss = 19192477268.31657410\n",
      "Iteration 137, loss = 19192470878.73437119\n",
      "Iteration 138, loss = 19192464571.61172867\n",
      "Iteration 139, loss = 19192458161.59250259\n",
      "Iteration 140, loss = 19192451827.50619507\n",
      "Iteration 141, loss = 19192445459.18670273\n",
      "Iteration 142, loss = 19192439096.30166245\n",
      "Iteration 143, loss = 19192432810.81768036\n",
      "Iteration 144, loss = 19192426463.49044037\n",
      "Iteration 145, loss = 19192420160.96496582\n",
      "Iteration 146, loss = 19192413820.06355667\n",
      "Iteration 147, loss = 19192407508.36790848\n",
      "Iteration 148, loss = 19192401175.21451187\n",
      "Iteration 149, loss = 19192394828.99051666\n",
      "Iteration 150, loss = 19192388541.22755814\n",
      "Iteration 151, loss = 19192382237.78152084\n",
      "Iteration 152, loss = 19192375920.16139984\n",
      "Iteration 153, loss = 19192369613.83340836\n",
      "Iteration 154, loss = 19192363341.66051865\n",
      "Iteration 155, loss = 19192357031.25106049\n",
      "Iteration 156, loss = 19192350732.27051163\n",
      "Iteration 157, loss = 19192344454.65181351\n",
      "Iteration 158, loss = 19192338121.89279556\n",
      "Iteration 159, loss = 19192331895.91753769\n",
      "Iteration 160, loss = 19192325558.87654495\n",
      "Iteration 161, loss = 19192319304.34720993\n",
      "Iteration 162, loss = 19192313032.40631866\n",
      "Iteration 163, loss = 19192306727.04451752\n",
      "Iteration 164, loss = 19192300453.86928177\n",
      "Iteration 165, loss = 19192294204.23628998\n",
      "Iteration 166, loss = 19192287947.77007675\n",
      "Iteration 167, loss = 19192281642.91068268\n",
      "Iteration 168, loss = 19192275370.13843155\n",
      "Iteration 169, loss = 19192269124.10290146\n",
      "Iteration 170, loss = 19192262845.99197388\n",
      "Iteration 171, loss = 19192256566.19409561\n",
      "Iteration 172, loss = 19192250262.33576584\n",
      "Iteration 173, loss = 19192244003.61800385\n",
      "Iteration 174, loss = 19192237704.63652420\n",
      "Iteration 175, loss = 19192231422.35047531\n",
      "Iteration 176, loss = 19192225073.40522003\n",
      "Iteration 177, loss = 19192218691.52415848\n",
      "Iteration 178, loss = 19192212257.52215958\n",
      "Iteration 179, loss = 19192205755.43426895\n",
      "Iteration 180, loss = 19192199070.07697678\n",
      "Iteration 181, loss = 19192192233.67985916\n",
      "Iteration 182, loss = 19192185170.81550980\n",
      "Iteration 183, loss = 19192177901.76902390\n",
      "Iteration 184, loss = 19192170526.45178604\n",
      "Iteration 185, loss = 19192163094.70383453\n",
      "Iteration 186, loss = 19192155692.41017151\n",
      "Iteration 187, loss = 19192148353.70968628\n",
      "Iteration 188, loss = 19192141135.07012177\n",
      "Iteration 189, loss = 19192133978.75923538\n",
      "Iteration 190, loss = 19192126809.30852890\n",
      "Iteration 191, loss = 19192119805.62559891\n",
      "Iteration 192, loss = 19192112707.76238251\n",
      "Iteration 193, loss = 19192105769.57832718\n",
      "Iteration 194, loss = 19192098812.73982620\n",
      "Iteration 195, loss = 19192091968.64169693\n",
      "Iteration 196, loss = 19192085045.19672012\n",
      "Iteration 197, loss = 19192078182.78628159\n",
      "Iteration 198, loss = 19192071299.34155273\n",
      "Iteration 199, loss = 19192064518.38396454\n",
      "Iteration 200, loss = 19192057722.46968460\n",
      "Iteration 201, loss = 19192050965.08274841\n",
      "Iteration 202, loss = 19192044114.02963638\n",
      "Iteration 203, loss = 19192037401.18360901\n",
      "Iteration 204, loss = 19192030615.48390579\n",
      "Iteration 205, loss = 19192023894.35451889\n",
      "Iteration 206, loss = 19192017196.80080032\n",
      "Iteration 207, loss = 19192010515.50500870\n",
      "Iteration 208, loss = 19192003824.74633026\n",
      "Iteration 209, loss = 19191997115.83600235\n",
      "Iteration 210, loss = 19191990467.57561111\n",
      "Iteration 211, loss = 19191983782.70903778\n",
      "Iteration 212, loss = 19191977125.52711105\n",
      "Iteration 213, loss = 19191970517.55087280\n",
      "Iteration 214, loss = 19191963860.74016571\n",
      "Iteration 215, loss = 19191957262.19124985\n",
      "Iteration 216, loss = 19191950580.10868454\n",
      "Iteration 217, loss = 19191943990.13121414\n",
      "Iteration 218, loss = 19191937422.28545380\n",
      "Iteration 219, loss = 19191930781.35251236\n",
      "Iteration 220, loss = 19191924139.87935257\n",
      "Iteration 221, loss = 19191917630.72220993\n",
      "Iteration 222, loss = 19191910980.03336716\n",
      "Iteration 223, loss = 19191904444.88834381\n",
      "Iteration 224, loss = 19191897893.41150284\n",
      "Iteration 225, loss = 19191891286.92149353\n",
      "Iteration 226, loss = 19191884714.30042648\n",
      "Iteration 227, loss = 19191878111.35515594\n",
      "Iteration 228, loss = 19191871604.69070435\n",
      "Iteration 229, loss = 19191865048.01741791\n",
      "Iteration 230, loss = 19191858512.83533096\n",
      "Iteration 231, loss = 19191851979.05543518\n",
      "Iteration 232, loss = 19191845427.83831024\n",
      "Iteration 233, loss = 19191838895.51676941\n",
      "Iteration 234, loss = 19191832397.01299667\n",
      "Iteration 235, loss = 19191825853.64837646\n",
      "Iteration 236, loss = 19191819316.02672958\n",
      "Iteration 237, loss = 19191812804.74273300\n",
      "Iteration 238, loss = 19191806324.18015671\n",
      "Iteration 239, loss = 19191799786.58961487\n",
      "Iteration 240, loss = 19191793234.18218231\n",
      "Iteration 241, loss = 19191786764.29880142\n",
      "Iteration 242, loss = 19191780271.86839294\n",
      "Iteration 243, loss = 19191773767.70153809\n",
      "Iteration 244, loss = 19191767279.58738708\n",
      "Iteration 245, loss = 19191760775.85118103\n",
      "Iteration 246, loss = 19191754258.23460770\n",
      "Iteration 247, loss = 19191747809.25483322\n",
      "Iteration 248, loss = 19191741299.63244629\n",
      "Iteration 249, loss = 19191734869.00593948\n",
      "Iteration 250, loss = 19191728309.71096802\n",
      "Iteration 251, loss = 19191721877.46272278\n",
      "Iteration 252, loss = 19191715422.24527359\n",
      "Iteration 253, loss = 19191708945.18404007\n",
      "Iteration 254, loss = 19191702467.06023407\n",
      "Iteration 255, loss = 19191696001.34586716\n",
      "Iteration 256, loss = 19191689499.07556915\n",
      "Iteration 257, loss = 19191683062.41985703\n",
      "Iteration 258, loss = 19191676574.22237015\n",
      "Iteration 259, loss = 19191670125.28504562\n",
      "Iteration 260, loss = 19191663698.65895844\n",
      "Iteration 261, loss = 19191657248.90578842\n",
      "Iteration 262, loss = 19191650777.64790344\n",
      "Iteration 263, loss = 19191644325.17439651\n",
      "Iteration 264, loss = 19191637848.91648102\n",
      "Iteration 265, loss = 19191631451.84027100\n",
      "Iteration 266, loss = 19191624957.28720474\n",
      "Iteration 267, loss = 19191618545.43864822\n",
      "Iteration 268, loss = 19191612127.96399307\n",
      "Iteration 269, loss = 19191605652.96529770\n",
      "Iteration 270, loss = 19191599227.27888870\n",
      "Iteration 271, loss = 19191592770.11072159\n",
      "Iteration 272, loss = 19191586370.32097244\n",
      "Iteration 273, loss = 19191579931.69028473\n",
      "Iteration 274, loss = 19191573481.67499924\n",
      "Iteration 275, loss = 19191567049.78716660\n",
      "Iteration 276, loss = 19191560632.55292892\n",
      "Iteration 277, loss = 19191554175.99012375\n",
      "Iteration 278, loss = 19191547791.42837143\n",
      "Iteration 279, loss = 19191541363.38514328\n",
      "Iteration 280, loss = 19191534951.75736618\n",
      "Iteration 281, loss = 19191528508.95699310\n",
      "Iteration 282, loss = 19191522115.15239334\n",
      "Iteration 283, loss = 19191515649.91630173\n",
      "Iteration 284, loss = 19191509281.75885391\n",
      "Iteration 285, loss = 19191502873.48907471\n",
      "Iteration 286, loss = 19191496434.42762375\n",
      "Iteration 287, loss = 19191490035.89882278\n",
      "Iteration 288, loss = 19191483626.89313507\n",
      "Iteration 289, loss = 19191477252.87258530\n",
      "Iteration 290, loss = 19191470819.27297592\n",
      "Iteration 291, loss = 19191464428.92602921\n",
      "Iteration 292, loss = 19191457996.00647736\n",
      "Iteration 293, loss = 19191451620.31870270\n",
      "Iteration 294, loss = 19191445230.57037735\n",
      "Iteration 295, loss = 19191438819.49922943\n",
      "Iteration 296, loss = 19191432410.29949188\n",
      "Iteration 297, loss = 19191426013.12923050\n",
      "Iteration 298, loss = 19191419628.22579575\n",
      "Iteration 299, loss = 19191413250.43606186\n",
      "Iteration 300, loss = 19191406884.13968277\n",
      "Iteration 301, loss = 19191400424.81511688\n",
      "Iteration 302, loss = 19191394063.21350479\n",
      "Iteration 303, loss = 19191387692.78733826\n",
      "Iteration 304, loss = 19191381295.75360870\n",
      "Iteration 305, loss = 19191374879.49940872\n",
      "Iteration 306, loss = 19191368500.81669998\n",
      "Iteration 307, loss = 19191362118.59864807\n",
      "Iteration 308, loss = 19191355762.64485168\n",
      "Iteration 309, loss = 19191349345.43086243\n",
      "Iteration 310, loss = 19191342985.64876175\n",
      "Iteration 311, loss = 19191336632.73931503\n",
      "Iteration 312, loss = 19191330189.01289749\n",
      "Iteration 313, loss = 19191323828.93278885\n",
      "Iteration 314, loss = 19191317465.85448074\n",
      "Iteration 315, loss = 19191311086.52117157\n",
      "Iteration 316, loss = 19191304735.53714371\n",
      "Iteration 317, loss = 19191298310.61069107\n",
      "Iteration 318, loss = 19191291949.86385727\n",
      "Iteration 319, loss = 19191285633.99420547\n",
      "Iteration 320, loss = 19191279250.03307724\n",
      "Iteration 321, loss = 19191272838.49605560\n",
      "Iteration 322, loss = 19191266479.97824478\n",
      "Iteration 323, loss = 19191260100.12105560\n",
      "Iteration 324, loss = 19191253735.31120300\n",
      "Iteration 325, loss = 19191247363.71131516\n",
      "Iteration 326, loss = 19191241011.69291306\n",
      "Iteration 327, loss = 19191234643.93368912\n",
      "Iteration 328, loss = 19191228265.52371216\n",
      "Iteration 329, loss = 19191221885.67562103\n",
      "Iteration 330, loss = 19191215544.66557693\n",
      "Iteration 331, loss = 19191209223.80050278\n",
      "Iteration 332, loss = 19191202798.55880737\n",
      "Iteration 333, loss = 19191196413.85382080\n",
      "Iteration 334, loss = 19191190091.49171448\n",
      "Iteration 335, loss = 19191183746.73347473\n",
      "Iteration 336, loss = 19191177372.06087875\n",
      "Iteration 337, loss = 19191171040.27863312\n",
      "Iteration 338, loss = 19191164650.72374725\n",
      "Iteration 339, loss = 19191158312.65040588\n",
      "Iteration 340, loss = 19191151942.63684082\n",
      "Iteration 341, loss = 19191145580.27896500\n",
      "Iteration 342, loss = 19191139247.78986740\n",
      "Iteration 343, loss = 19191132902.77909470\n",
      "Iteration 344, loss = 19191126510.75338364\n",
      "Iteration 345, loss = 19191120239.04501724\n",
      "Iteration 346, loss = 19191113846.16432190\n",
      "Iteration 347, loss = 19191107476.19475555\n",
      "Iteration 348, loss = 19191101149.37943268\n",
      "Iteration 349, loss = 19191094767.28891754\n",
      "Iteration 350, loss = 19191088454.16517639\n",
      "Iteration 351, loss = 19191082066.03437042\n",
      "Iteration 352, loss = 19191075758.86705399\n",
      "Iteration 353, loss = 19191069383.82271194\n",
      "Iteration 354, loss = 19191063048.09247589\n",
      "Iteration 355, loss = 19191056708.49024963\n",
      "Iteration 356, loss = 19191050376.18580246\n",
      "Iteration 357, loss = 19191044037.64587021\n",
      "Iteration 358, loss = 19191037661.31060410\n",
      "Iteration 359, loss = 19191031314.16244507\n",
      "Iteration 360, loss = 19191024959.92457199\n",
      "Iteration 361, loss = 19191018660.08446503\n",
      "Iteration 362, loss = 19191012324.83757019\n",
      "Iteration 363, loss = 19191005959.98886108\n",
      "Iteration 364, loss = 19190999630.46337128\n",
      "Iteration 365, loss = 19190993299.95628357\n",
      "Iteration 366, loss = 19190986964.74745941\n",
      "Iteration 367, loss = 19190980574.14373016\n",
      "Iteration 368, loss = 19190974294.98834991\n",
      "Iteration 369, loss = 19190967927.77945709\n",
      "Iteration 370, loss = 19190961629.38772583\n",
      "Iteration 371, loss = 19190955254.03577805\n",
      "Iteration 372, loss = 19190948946.21389389\n",
      "Iteration 373, loss = 19190942625.98801422\n",
      "Iteration 374, loss = 19190936260.21267700\n",
      "Iteration 375, loss = 19190929923.22145844\n",
      "Iteration 376, loss = 19190923602.09428024\n",
      "Iteration 377, loss = 19190917297.42021561\n",
      "Iteration 378, loss = 19190910970.37947083\n",
      "Iteration 379, loss = 19190904620.53233719\n",
      "Iteration 380, loss = 19190898277.28590775\n",
      "Iteration 381, loss = 19190891951.19444656\n",
      "Iteration 382, loss = 19190885595.67874146\n",
      "Iteration 383, loss = 19190879263.78940964\n",
      "Iteration 384, loss = 19190872986.71022797\n",
      "Iteration 385, loss = 19190866617.56273270\n",
      "Iteration 386, loss = 19190860305.33292770\n",
      "Iteration 387, loss = 19190853992.46063614\n",
      "Iteration 388, loss = 19190847625.26288986\n",
      "Iteration 389, loss = 19190841325.98392105\n",
      "Iteration 390, loss = 19190835012.10598373\n",
      "Iteration 391, loss = 19190828659.12168121\n",
      "Iteration 392, loss = 19190822372.00466919\n",
      "Iteration 393, loss = 19190816048.29032516\n",
      "Iteration 394, loss = 19190809721.24306870\n",
      "Iteration 395, loss = 19190803352.55679321\n",
      "Iteration 396, loss = 19190797052.80083466\n",
      "Iteration 397, loss = 19190790715.60797882\n",
      "Iteration 398, loss = 19190784416.33543777\n",
      "Iteration 399, loss = 19190778172.69150543\n",
      "Iteration 400, loss = 19190771724.37129974\n",
      "Iteration 401, loss = 19190765433.54736710\n",
      "Iteration 402, loss = 19190759149.66244888\n",
      "Iteration 403, loss = 19190752791.37808990\n",
      "Iteration 404, loss = 19190746480.40863419\n",
      "Iteration 405, loss = 19190740195.17461014\n",
      "Iteration 406, loss = 19190733830.15684509\n",
      "Iteration 407, loss = 19190727521.61264420\n",
      "Iteration 408, loss = 19190721251.83288193\n",
      "Iteration 409, loss = 19190714865.52043915\n",
      "Iteration 410, loss = 19190708614.34815216\n",
      "Iteration 411, loss = 19190702283.30840683\n",
      "Iteration 412, loss = 19190695936.20016098\n",
      "Iteration 413, loss = 19190689617.20589447\n",
      "Iteration 414, loss = 19190683312.27447128\n",
      "Iteration 415, loss = 19190677015.29254913\n",
      "Iteration 416, loss = 19190670702.59918976\n",
      "Iteration 417, loss = 19190664375.86874771\n",
      "Iteration 418, loss = 19190658120.53384018\n",
      "Iteration 419, loss = 19190651724.38349152\n",
      "Iteration 420, loss = 19190645448.25864792\n",
      "Iteration 421, loss = 19190639134.47957611\n",
      "Iteration 422, loss = 19190632834.19903946\n",
      "Iteration 423, loss = 19190626540.02630615\n",
      "Iteration 424, loss = 19190620230.79373169\n",
      "Iteration 425, loss = 19190613879.78941345\n",
      "Iteration 426, loss = 19190607549.28262711\n",
      "Iteration 427, loss = 19190601285.47645569\n",
      "Iteration 428, loss = 19190594946.21586227\n",
      "Iteration 429, loss = 19190588643.46885681\n",
      "Iteration 430, loss = 19190582339.53318024\n",
      "Iteration 431, loss = 19190576026.10552597\n",
      "Iteration 432, loss = 19190569687.69847870\n",
      "Iteration 433, loss = 19190563414.62973022\n",
      "Iteration 434, loss = 19190557090.56025314\n",
      "Iteration 435, loss = 19190550751.81330490\n",
      "Iteration 436, loss = 19190544451.77856827\n",
      "Iteration 437, loss = 19190538144.23923492\n",
      "Iteration 438, loss = 19190531835.79890060\n",
      "Iteration 439, loss = 19190525506.69020081\n",
      "Iteration 440, loss = 19190519170.53283310\n",
      "Iteration 441, loss = 19190512898.40363312\n",
      "Iteration 442, loss = 19190506550.59118652\n",
      "Iteration 443, loss = 19190500149.55430984\n",
      "Iteration 444, loss = 19190493801.19449997\n",
      "Iteration 445, loss = 19190487425.69168854\n",
      "Iteration 446, loss = 19190480966.77650833\n",
      "Iteration 447, loss = 19190474327.94082260\n",
      "Iteration 448, loss = 19190467480.07075882\n",
      "Iteration 449, loss = 19190460112.72184372\n",
      "Iteration 450, loss = 19190452249.57490540\n",
      "Iteration 451, loss = 19190443982.11032486\n",
      "Iteration 452, loss = 19190435780.24390411\n",
      "Iteration 453, loss = 19190427710.51010513\n",
      "Iteration 454, loss = 19190419905.67240143\n",
      "Iteration 455, loss = 19190412287.40519333\n",
      "Iteration 456, loss = 19190404697.32564163\n",
      "Iteration 457, loss = 19190397258.22354889\n",
      "Iteration 458, loss = 19190389952.55952835\n",
      "Iteration 459, loss = 19190382685.81854630\n",
      "Iteration 460, loss = 19190375427.31254578\n",
      "Iteration 461, loss = 19190368244.69614792\n",
      "Iteration 462, loss = 19190361072.56730270\n",
      "Iteration 463, loss = 19190353964.87316513\n",
      "Iteration 464, loss = 19190346894.07348251\n",
      "Iteration 465, loss = 19190339876.30177307\n",
      "Iteration 466, loss = 19190332811.37612152\n",
      "Iteration 467, loss = 19190325803.28969955\n",
      "Iteration 468, loss = 19190318785.68167496\n",
      "Iteration 469, loss = 19190311855.92045212\n",
      "Iteration 470, loss = 19190304906.65126419\n",
      "Iteration 471, loss = 19190297981.63274384\n",
      "Iteration 472, loss = 19190291092.81248093\n",
      "Iteration 473, loss = 19190284137.33350372\n",
      "Iteration 474, loss = 19190277245.86965179\n",
      "Iteration 475, loss = 19190270388.66933060\n",
      "Iteration 476, loss = 19190263498.49695587\n",
      "Iteration 477, loss = 19190256683.41666794\n",
      "Iteration 478, loss = 19190249806.78425980\n",
      "Iteration 479, loss = 19190242957.68193436\n",
      "Iteration 480, loss = 19190236168.17471313\n",
      "Iteration 481, loss = 19190229279.90085602\n",
      "Iteration 482, loss = 19190222526.69481659\n",
      "Iteration 483, loss = 19190215726.91855621\n",
      "Iteration 484, loss = 19190208891.81566238\n",
      "Iteration 485, loss = 19190202123.37357330\n",
      "Iteration 486, loss = 19190195356.71923828\n",
      "Iteration 487, loss = 19190188542.82221222\n",
      "Iteration 488, loss = 19190181799.09335709\n",
      "Iteration 489, loss = 19190175058.89082336\n",
      "Iteration 490, loss = 19190168278.77562332\n",
      "Iteration 491, loss = 19190161497.47859192\n",
      "Iteration 492, loss = 19190154755.27932358\n",
      "Iteration 493, loss = 19190148039.46261597\n",
      "Iteration 494, loss = 19190141304.04233932\n",
      "Iteration 495, loss = 19190134525.47385406\n",
      "Iteration 496, loss = 19190127807.63173676\n",
      "Iteration 497, loss = 19190121110.39992142\n",
      "Iteration 498, loss = 19190114404.70603180\n",
      "Iteration 499, loss = 19190107683.41029358\n",
      "Iteration 500, loss = 19190100932.60095596\n",
      "Iteration 1, loss = 17987588228.79123688\n",
      "Iteration 2, loss = 17987584623.68030930\n",
      "Iteration 3, loss = 17987581013.81898499\n",
      "Iteration 4, loss = 17987577399.32012558\n",
      "Iteration 5, loss = 17987573780.22563171\n",
      "Iteration 6, loss = 17987570156.51373672\n",
      "Iteration 7, loss = 17987566528.14055634\n",
      "Iteration 8, loss = 17987562895.04373932\n",
      "Iteration 9, loss = 17987559257.12754059\n",
      "Iteration 10, loss = 17987555614.26151657\n",
      "Iteration 11, loss = 17987551966.29366302\n",
      "Iteration 12, loss = 17987548313.06497955\n",
      "Iteration 13, loss = 17987544654.41593552\n",
      "Iteration 14, loss = 17987540990.18437195\n",
      "Iteration 15, loss = 17987537320.19978714\n",
      "Iteration 16, loss = 17987533644.27897644\n",
      "Iteration 17, loss = 17987529962.22539902\n",
      "Iteration 18, loss = 17987526273.83246231\n",
      "Iteration 19, loss = 17987522578.88957214\n",
      "Iteration 20, loss = 17987518877.18880463\n",
      "Iteration 21, loss = 17987515168.53060532\n",
      "Iteration 22, loss = 17987511452.72734070\n",
      "Iteration 23, loss = 17987507729.60480881\n",
      "Iteration 24, loss = 17987503999.00225067\n",
      "Iteration 25, loss = 17987500260.77172852\n",
      "Iteration 26, loss = 17987496514.77744675\n",
      "Iteration 27, loss = 17987492760.89543533\n",
      "Iteration 28, loss = 17987488999.01357269\n",
      "Iteration 29, loss = 17987485229.03198624\n",
      "Iteration 30, loss = 17987481450.86352539\n",
      "Iteration 31, loss = 17987477664.43429565\n",
      "Iteration 32, loss = 17987473869.68393326\n",
      "Iteration 33, loss = 17987470066.56568527\n",
      "Iteration 34, loss = 17987466255.04616547\n",
      "Iteration 35, loss = 17987462435.10498428\n",
      "Iteration 36, loss = 17987458606.73416138\n",
      "Iteration 37, loss = 17987454769.93761826\n",
      "Iteration 38, loss = 17987450924.73069763\n",
      "Iteration 39, loss = 17987447071.13992691\n",
      "Iteration 40, loss = 17987443209.20299149\n",
      "Iteration 41, loss = 17987439338.96902084\n",
      "Iteration 42, loss = 17987435460.49908066\n",
      "Iteration 43, loss = 17987431573.86694336\n",
      "Iteration 44, loss = 17987427679.16004944\n",
      "Iteration 45, loss = 17987423776.48050690\n",
      "Iteration 46, loss = 17987419865.94622803\n",
      "Iteration 47, loss = 17987415947.69193268\n",
      "Iteration 48, loss = 17987412021.87016678\n",
      "Iteration 49, loss = 17987408088.65208817\n",
      "Iteration 50, loss = 17987404148.22811127\n",
      "Iteration 51, loss = 17987400200.80836105\n",
      "Iteration 52, loss = 17987396246.62294388\n",
      "Iteration 53, loss = 17987392285.92195892\n",
      "Iteration 54, loss = 17987388318.97545242\n",
      "Iteration 55, loss = 17987384346.07314682\n",
      "Iteration 56, loss = 17987380367.52404022\n",
      "Iteration 57, loss = 17987376383.65599823\n",
      "Iteration 58, loss = 17987372394.81515503\n",
      "Iteration 59, loss = 17987368401.36540604\n",
      "Iteration 60, loss = 17987364403.68771744\n",
      "Iteration 61, loss = 17987360402.17955017\n",
      "Iteration 62, loss = 17987356397.25418854\n",
      "Iteration 63, loss = 17987352389.34009552\n",
      "Iteration 64, loss = 17987348378.88024521\n",
      "Iteration 65, loss = 17987344366.33150101\n",
      "Iteration 66, loss = 17987340352.16392136\n",
      "Iteration 67, loss = 17987336336.86012650\n",
      "Iteration 68, loss = 17987332320.91458511\n",
      "Iteration 69, loss = 17987328304.83297348\n",
      "Iteration 70, loss = 17987324289.13146210\n",
      "Iteration 71, loss = 17987320274.33600998\n",
      "Iteration 72, loss = 17987316260.98164749\n",
      "Iteration 73, loss = 17987312249.61178207\n",
      "Iteration 74, loss = 17987308240.77744675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75, loss = 17987304235.03658295\n",
      "Iteration 76, loss = 17987300232.95326614\n",
      "Iteration 77, loss = 17987296235.09698105\n",
      "Iteration 78, loss = 17987292242.04181290\n",
      "Iteration 79, loss = 17987288254.36571884\n",
      "Iteration 80, loss = 17987284272.64971542\n",
      "Iteration 81, loss = 17987280297.47715378\n",
      "Iteration 82, loss = 17987276329.43288803\n",
      "Iteration 83, loss = 17987272369.10253525\n",
      "Iteration 84, loss = 17987268417.07166290\n",
      "Iteration 85, loss = 17987264473.92504883\n",
      "Iteration 86, loss = 17987260540.24585724\n",
      "Iteration 87, loss = 17987256616.61485672\n",
      "Iteration 88, loss = 17987252703.60964966\n",
      "Iteration 89, loss = 17987248801.80386353\n",
      "Iteration 90, loss = 17987244911.76637268\n",
      "Iteration 91, loss = 17987241034.06053162\n",
      "Iteration 92, loss = 17987237169.24336624\n",
      "Iteration 93, loss = 17987233317.86484909\n",
      "Iteration 94, loss = 17987229480.46708298\n",
      "Iteration 95, loss = 17987225657.58361816\n",
      "Iteration 96, loss = 17987221849.73864746\n",
      "Iteration 97, loss = 17987218057.44639587\n",
      "Iteration 98, loss = 17987214281.21035767\n",
      "Iteration 99, loss = 17987210521.52266312\n",
      "Iteration 100, loss = 17987206778.86347198\n",
      "Iteration 101, loss = 17987203053.70038223\n",
      "Iteration 102, loss = 17987199346.48786163\n",
      "Iteration 103, loss = 17987195657.66674805\n",
      "Iteration 104, loss = 17987191987.66378403\n",
      "Iteration 105, loss = 17987188336.89116669\n",
      "Iteration 106, loss = 17987184705.74616623\n",
      "Iteration 107, loss = 17987181094.61076736\n",
      "Iteration 108, loss = 17987177503.85134506\n",
      "Iteration 109, loss = 17987173933.81838608\n",
      "Iteration 110, loss = 17987170384.84623718\n",
      "Iteration 111, loss = 17987166857.25284576\n",
      "Iteration 112, loss = 17987163351.33957672\n",
      "Iteration 113, loss = 17987159867.39101028\n",
      "Iteration 114, loss = 17987156405.67479706\n",
      "Iteration 115, loss = 17987152966.44147491\n",
      "Iteration 116, loss = 17987149549.92435074\n",
      "Iteration 117, loss = 17987146156.33942032\n",
      "Iteration 118, loss = 17987142785.88521957\n",
      "Iteration 119, loss = 17987139438.74281693\n",
      "Iteration 120, loss = 17987136115.07567215\n",
      "Iteration 121, loss = 17987132815.02964020\n",
      "Iteration 122, loss = 17987129538.73291016\n",
      "Iteration 123, loss = 17987126286.29597855\n",
      "Iteration 124, loss = 17987123057.81164932\n",
      "Iteration 125, loss = 17987119853.35501480\n",
      "Iteration 126, loss = 17987116672.98348236\n",
      "Iteration 127, loss = 17987113516.73679733\n",
      "Iteration 128, loss = 17987110384.63711929\n",
      "Iteration 129, loss = 17987107276.68907166\n",
      "Iteration 130, loss = 17987104192.87987900\n",
      "Iteration 131, loss = 17987101133.17949677\n",
      "Iteration 132, loss = 17987098097.54077148\n",
      "Iteration 133, loss = 17987095085.89965057\n",
      "Iteration 134, loss = 17987092098.17544937\n",
      "Iteration 135, loss = 17987089134.27111816\n",
      "Iteration 136, loss = 17987086194.07358170\n",
      "Iteration 137, loss = 17987083277.45409393\n",
      "Iteration 138, loss = 17987080384.26869583\n",
      "Iteration 139, loss = 17987077514.35865784\n",
      "Iteration 140, loss = 17987074667.55097961\n",
      "Iteration 141, loss = 17987071843.65895081\n",
      "Iteration 142, loss = 17987069042.48274612\n",
      "Iteration 143, loss = 17987066263.81003952\n",
      "Iteration 144, loss = 17987063507.41665649\n",
      "Iteration 145, loss = 17987060773.06726074\n",
      "Iteration 146, loss = 17987058060.51603699\n",
      "Iteration 147, loss = 17987055369.50744629\n",
      "Iteration 148, loss = 17987052699.77692413\n",
      "Iteration 149, loss = 17987050051.05167007\n",
      "Iteration 150, loss = 17987047423.05138397\n",
      "Iteration 151, loss = 17987044815.48903656\n",
      "Iteration 152, loss = 17987042228.07164764\n",
      "Iteration 153, loss = 17987039660.50102234\n",
      "Iteration 154, loss = 17987037112.47455215\n",
      "Iteration 155, loss = 17987034583.68594360\n",
      "Iteration 156, loss = 17987032073.82595825\n",
      "Iteration 157, loss = 17987029582.58315277\n",
      "Iteration 158, loss = 17987027109.64460373\n",
      "Iteration 159, loss = 17987024654.69656754\n",
      "Iteration 160, loss = 17987022217.42519760\n",
      "Iteration 161, loss = 17987019797.51718521\n",
      "Iteration 162, loss = 17987017394.66038513\n",
      "Iteration 163, loss = 17987015008.54443741\n",
      "Iteration 164, loss = 17987012638.86137009\n",
      "Iteration 165, loss = 17987010285.30609131\n",
      "Iteration 166, loss = 17987007947.57699585\n",
      "Iteration 167, loss = 17987005625.37640762\n",
      "Iteration 168, loss = 17987003318.41106796\n",
      "Iteration 169, loss = 17987001026.39256668\n",
      "Iteration 170, loss = 17986998749.03771973\n",
      "Iteration 171, loss = 17986996486.06894302\n",
      "Iteration 172, loss = 17986994237.21458435\n",
      "Iteration 173, loss = 17986992002.20918274\n",
      "Iteration 174, loss = 17986989780.79372787\n",
      "Iteration 175, loss = 17986987572.71583557\n",
      "Iteration 176, loss = 17986985377.72993851\n",
      "Iteration 177, loss = 17986983195.59735870\n",
      "Iteration 178, loss = 17986981026.08641052\n",
      "Iteration 179, loss = 17986978868.97240448\n",
      "Iteration 180, loss = 17986976724.03763199\n",
      "Iteration 181, loss = 17986974591.07130432\n",
      "Iteration 182, loss = 17986972469.86945343\n",
      "Iteration 183, loss = 17986970360.23477554\n",
      "Iteration 184, loss = 17986968261.97645950\n",
      "Iteration 185, loss = 17986966174.90996170\n",
      "Iteration 186, loss = 17986964098.85676575\n",
      "Iteration 187, loss = 17986962033.64408493\n",
      "Iteration 188, loss = 17986959979.10459137\n",
      "Iteration 189, loss = 17986957935.07605743\n",
      "Iteration 190, loss = 17986955901.40103531\n",
      "Iteration 191, loss = 17986953877.92651749\n",
      "Iteration 192, loss = 17986951864.50356293\n",
      "Iteration 193, loss = 17986949860.98694611\n",
      "Iteration 194, loss = 17986947867.23481750\n",
      "Iteration 195, loss = 17986945883.10833740\n",
      "Iteration 196, loss = 17986943908.47136688\n",
      "Iteration 197, loss = 17986941943.19012833\n",
      "Iteration 198, loss = 17986939987.13293839\n",
      "Iteration 199, loss = 17986938040.16988754\n",
      "Iteration 200, loss = 17986936102.17263031\n",
      "Iteration 201, loss = 17986934173.01415634\n",
      "Iteration 202, loss = 17986932252.56855392\n",
      "Iteration 203, loss = 17986930340.71088791\n",
      "Iteration 204, loss = 17986928437.31701660\n",
      "Iteration 205, loss = 17986926542.26349640\n",
      "Iteration 206, loss = 17986924655.42746735\n",
      "Iteration 207, loss = 17986922776.68663406\n",
      "Iteration 208, loss = 17986920905.91915512\n",
      "Iteration 209, loss = 17986919043.00367355\n",
      "Iteration 210, loss = 17986917187.81929398\n",
      "Iteration 211, loss = 17986915340.24560547\n",
      "Iteration 212, loss = 17986913500.16270447\n",
      "Iteration 213, loss = 17986911667.45125198\n",
      "Iteration 214, loss = 17986909841.99251556\n",
      "Iteration 215, loss = 17986908023.66845703\n",
      "Iteration 216, loss = 17986906212.36178589\n",
      "Iteration 217, loss = 17986904407.95604706\n",
      "Iteration 218, loss = 17986902610.33571625\n",
      "Iteration 219, loss = 17986900819.38629532\n",
      "Iteration 220, loss = 17986899034.99436569\n",
      "Iteration 221, loss = 17986897257.04772186\n",
      "Iteration 222, loss = 17986895485.43543625\n",
      "Iteration 223, loss = 17986893720.04796219\n",
      "Iteration 224, loss = 17986891960.77723694\n",
      "Iteration 225, loss = 17986890207.51671982\n",
      "Iteration 226, loss = 17986888460.16152573\n",
      "Iteration 227, loss = 17986886718.60848618\n",
      "Iteration 228, loss = 17986884982.75621033\n",
      "Iteration 229, loss = 17986883252.50518036\n",
      "Iteration 230, loss = 17986881527.75779343\n",
      "Iteration 231, loss = 17986879808.41844559\n",
      "Iteration 232, loss = 17986878094.39356613\n",
      "Iteration 233, loss = 17986876385.59168243\n",
      "Iteration 234, loss = 17986874681.92343521\n",
      "Iteration 235, loss = 17986872983.30164337\n",
      "Iteration 236, loss = 17986871289.64130783\n",
      "Iteration 237, loss = 17986869600.85964966\n",
      "Iteration 238, loss = 17986867916.87608719\n",
      "Iteration 239, loss = 17986866237.61225891\n",
      "Iteration 240, loss = 17986864562.99200439\n",
      "Iteration 241, loss = 17986862892.94132996\n",
      "Iteration 242, loss = 17986861227.38839722\n",
      "Iteration 243, loss = 17986859566.26344681\n",
      "Iteration 244, loss = 17986857909.49877930\n",
      "Iteration 245, loss = 17986856257.02864838\n",
      "Iteration 246, loss = 17986854608.78922272\n",
      "Iteration 247, loss = 17986852964.71844482\n",
      "Iteration 248, loss = 17986851324.75599289\n",
      "Iteration 249, loss = 17986849688.84313583\n",
      "Iteration 250, loss = 17986848056.92264938\n",
      "Iteration 251, loss = 17986846428.93868637\n",
      "Iteration 252, loss = 17986844804.83667374\n",
      "Iteration 253, loss = 17986843184.56317902\n",
      "Iteration 254, loss = 17986841568.06580734\n",
      "Iteration 255, loss = 17986839955.29309845\n",
      "Iteration 256, loss = 17986838346.19442368\n",
      "Iteration 257, loss = 17986836740.71986389\n",
      "Iteration 258, loss = 17986835138.82014847\n",
      "Iteration 259, loss = 17986833540.44656372\n",
      "Iteration 260, loss = 17986831945.55088806\n",
      "Iteration 261, loss = 17986830354.08533096\n",
      "Iteration 262, loss = 17986828766.00248718\n",
      "Iteration 263, loss = 17986827181.25531769\n",
      "Iteration 264, loss = 17986825599.79708862\n",
      "Iteration 265, loss = 17986824021.58140182\n",
      "Iteration 266, loss = 17986822446.56214142\n",
      "Iteration 267, loss = 17986820874.69352722\n",
      "Iteration 268, loss = 17986819305.93007660\n",
      "Iteration 269, loss = 17986817740.22663116\n",
      "Iteration 270, loss = 17986816177.53839493\n",
      "Iteration 271, loss = 17986814617.82094193\n",
      "Iteration 272, loss = 17986813061.03025055\n",
      "Iteration 273, loss = 17986811507.12269592\n",
      "Iteration 274, loss = 17986809956.05510712\n",
      "Iteration 275, loss = 17986808407.78480911\n",
      "Iteration 276, loss = 17986806862.26959229\n",
      "Iteration 277, loss = 17986805319.46778488\n",
      "Iteration 278, loss = 17986803779.33825302\n",
      "Iteration 279, loss = 17986802241.84042358\n",
      "Iteration 280, loss = 17986800706.93430328\n",
      "Iteration 281, loss = 17986799174.58050156\n",
      "Iteration 282, loss = 17986797644.74021149\n",
      "Iteration 283, loss = 17986796117.37525940\n",
      "Iteration 284, loss = 17986794592.44809341\n",
      "Iteration 285, loss = 17986793069.92177963\n",
      "Iteration 286, loss = 17986791549.76002884\n",
      "Iteration 287, loss = 17986790031.92716980\n",
      "Iteration 288, loss = 17986788516.38818359\n",
      "Iteration 289, loss = 17986787003.10867310\n",
      "Iteration 290, loss = 17986785492.05486298\n",
      "Iteration 291, loss = 17986783983.19361115\n",
      "Iteration 292, loss = 17986782476.49238586\n",
      "Iteration 293, loss = 17986780971.91927338\n",
      "Iteration 294, loss = 17986779469.44297409\n",
      "Iteration 295, loss = 17986777969.03277588\n",
      "Iteration 296, loss = 17986776470.65855789\n",
      "Iteration 297, loss = 17986774974.29080200\n",
      "Iteration 298, loss = 17986773479.90055466\n",
      "Iteration 299, loss = 17986771987.45944214\n",
      "Iteration 300, loss = 17986770496.93964767\n",
      "Iteration 301, loss = 17986769008.31393051\n",
      "Iteration 302, loss = 17986767521.55559540\n",
      "Iteration 303, loss = 17986766036.63850021\n",
      "Iteration 304, loss = 17986764553.53705978\n",
      "Iteration 305, loss = 17986763072.22622299\n",
      "Iteration 306, loss = 17986761592.68149948\n",
      "Iteration 307, loss = 17986760114.87893295\n",
      "Iteration 308, loss = 17986758638.79510498\n",
      "Iteration 309, loss = 17986757164.40717316\n",
      "Iteration 310, loss = 17986755691.69282532\n",
      "Iteration 311, loss = 17986754220.63031387\n",
      "Iteration 312, loss = 17986752751.19844818\n",
      "Iteration 313, loss = 17986751283.37662506\n",
      "Iteration 314, loss = 17986749817.14479446\n",
      "Iteration 315, loss = 17986748352.48351669\n",
      "Iteration 316, loss = 17986746889.37393188\n",
      "Iteration 317, loss = 17986745427.79779816\n",
      "Iteration 318, loss = 17986743967.73746872\n",
      "Iteration 319, loss = 17986742509.17595291\n",
      "Iteration 320, loss = 17986741052.09687805\n",
      "Iteration 321, loss = 17986739596.48451233\n",
      "Iteration 322, loss = 17986738142.32378387\n",
      "Iteration 323, loss = 17986736689.60026550\n",
      "Iteration 324, loss = 17986735238.30020142\n",
      "Iteration 325, loss = 17986733788.41046143\n",
      "Iteration 326, loss = 17986732339.91859436\n",
      "Iteration 327, loss = 17986730892.81276703\n",
      "Iteration 328, loss = 17986729447.08179092\n",
      "Iteration 329, loss = 17986728002.71506882\n",
      "Iteration 330, loss = 17986726559.70261002\n",
      "Iteration 331, loss = 17986725118.03495026\n",
      "Iteration 332, loss = 17986723677.70316315\n",
      "Iteration 333, loss = 17986722238.69881058\n",
      "Iteration 334, loss = 17986720801.01387787\n",
      "Iteration 335, loss = 17986719364.64074326\n",
      "Iteration 336, loss = 17986717929.57212448\n",
      "Iteration 337, loss = 17986716495.80101776\n",
      "Iteration 338, loss = 17986715063.32064438\n",
      "Iteration 339, loss = 17986713632.12439346\n",
      "Iteration 340, loss = 17986712202.20575714\n",
      "Iteration 341, loss = 17986710773.55828857\n",
      "Iteration 342, loss = 17986709346.17551422\n",
      "Iteration 343, loss = 17986707920.05092239\n",
      "Iteration 344, loss = 17986706495.17788315\n",
      "Iteration 345, loss = 17986705071.54962921\n",
      "Iteration 346, loss = 17986703649.15919113\n",
      "Iteration 347, loss = 17986702227.99939346\n",
      "Iteration 348, loss = 17986700808.06281662\n",
      "Iteration 349, loss = 17986699389.34176254\n",
      "Iteration 350, loss = 17986697971.82828522\n",
      "Iteration 351, loss = 17986696555.51413345\n",
      "Iteration 352, loss = 17986695140.39078522\n",
      "Iteration 353, loss = 17986693726.44944382\n",
      "Iteration 354, loss = 17986692313.68104172\n",
      "Iteration 355, loss = 17986690902.07627106\n",
      "Iteration 356, loss = 17986689491.62557220\n",
      "Iteration 357, loss = 17986688082.31918716\n",
      "Iteration 358, loss = 17986686674.14716339\n",
      "Iteration 359, loss = 17986685267.09937668\n",
      "Iteration 360, loss = 17986683861.16556168\n",
      "Iteration 361, loss = 17986682456.33534622\n",
      "Iteration 362, loss = 17986681052.59824753\n",
      "Iteration 363, loss = 17986679649.94371796\n",
      "Iteration 364, loss = 17986678248.36116409\n",
      "Iteration 365, loss = 17986676847.83998489\n",
      "Iteration 366, loss = 17986675448.36954498\n",
      "Iteration 367, loss = 17986674049.93925476\n",
      "Iteration 368, loss = 17986672652.53852081\n",
      "Iteration 369, loss = 17986671256.15682983\n",
      "Iteration 370, loss = 17986669860.78370667\n",
      "Iteration 371, loss = 17986668466.40876007\n",
      "Iteration 372, loss = 17986667073.02168274\n",
      "Iteration 373, loss = 17986665680.61222458\n",
      "Iteration 374, loss = 17986664289.17029953\n",
      "Iteration 375, loss = 17986662898.68586349\n",
      "Iteration 376, loss = 17986661509.14902115\n",
      "Iteration 377, loss = 17986660120.54997253\n",
      "Iteration 378, loss = 17986658732.87904358\n",
      "Iteration 379, loss = 17986657346.12667847\n",
      "Iteration 380, loss = 17986655960.28342819\n",
      "Iteration 381, loss = 17986654575.33999634\n",
      "Iteration 382, loss = 17986653191.28716278\n",
      "Iteration 383, loss = 17986651808.11585236\n",
      "Iteration 384, loss = 17986650425.81710815\n",
      "Iteration 385, loss = 17986649044.38208771\n",
      "Iteration 386, loss = 17986647663.80206680\n",
      "Iteration 387, loss = 17986646284.06842804\n",
      "Iteration 388, loss = 17986644905.17267609\n",
      "Iteration 389, loss = 17986643527.10640717\n",
      "Iteration 390, loss = 17986642149.86133194\n",
      "Iteration 391, loss = 17986640773.42927551\n",
      "Iteration 392, loss = 17986639397.80213928\n",
      "Iteration 393, loss = 17986638022.97195053\n",
      "Iteration 394, loss = 17986636648.93081284\n",
      "Iteration 395, loss = 17986635275.67092133\n",
      "Iteration 396, loss = 17986633903.18457794\n",
      "Iteration 397, loss = 17986632531.46415329\n",
      "Iteration 398, loss = 17986631160.50210571\n",
      "Iteration 399, loss = 17986629790.29100037\n",
      "Iteration 400, loss = 17986628420.82344818\n",
      "Iteration 401, loss = 17986627052.09215164\n",
      "Iteration 402, loss = 17986625684.08988953\n",
      "Iteration 403, loss = 17986624316.80951691\n",
      "Iteration 404, loss = 17986622950.24395370\n",
      "Iteration 405, loss = 17986621584.38619995\n",
      "Iteration 406, loss = 17986620219.22930908\n",
      "Iteration 407, loss = 17986618854.76641846\n",
      "Iteration 408, loss = 17986617490.99072266\n",
      "Iteration 409, loss = 17986616127.89549255\n",
      "Iteration 410, loss = 17986614765.47405243\n",
      "Iteration 411, loss = 17986613403.71979904\n",
      "Iteration 412, loss = 17986612042.62619781\n",
      "Iteration 413, loss = 17986610682.18678284\n",
      "Iteration 414, loss = 17986609322.39515686\n",
      "Iteration 415, loss = 17986607963.24498367\n",
      "Iteration 416, loss = 17986606604.73001099\n",
      "Iteration 417, loss = 17986605246.84406662\n",
      "Iteration 418, loss = 17986603889.58104324\n",
      "Iteration 419, loss = 17986602532.93494415\n",
      "Iteration 420, loss = 17986601176.89983368\n",
      "Iteration 421, loss = 17986599821.46989441\n",
      "Iteration 422, loss = 17986598466.63940430\n",
      "Iteration 423, loss = 17986597112.40274429\n",
      "Iteration 424, loss = 17986595758.75444412\n",
      "Iteration 425, loss = 17986594405.68913651\n",
      "Iteration 426, loss = 17986593053.20160294\n",
      "Iteration 427, loss = 17986591701.28678513\n",
      "Iteration 428, loss = 17986590349.93978882\n",
      "Iteration 429, loss = 17986588999.15589905\n",
      "Iteration 430, loss = 17986587648.93059921\n",
      "Iteration 431, loss = 17986586299.25957870\n",
      "Iteration 432, loss = 17986584950.13876343\n",
      "Iteration 433, loss = 17986583601.56430435\n",
      "Iteration 434, loss = 17986582253.53262711\n",
      "Iteration 435, loss = 17986580906.04041290\n",
      "Iteration 436, loss = 17986579559.08463669\n",
      "Iteration 437, loss = 17986578212.66257477\n",
      "Iteration 438, loss = 17986576866.77178955\n",
      "Iteration 439, loss = 17986575521.41018677\n",
      "Iteration 440, loss = 17986574176.57596970\n",
      "Iteration 441, loss = 17986572832.26765823\n",
      "Iteration 442, loss = 17986571488.48410416\n",
      "Iteration 443, loss = 17986570145.22444916\n",
      "Iteration 444, loss = 17986568802.48814011\n",
      "Iteration 445, loss = 17986567460.27489471\n",
      "Iteration 446, loss = 17986566118.58467865\n",
      "Iteration 447, loss = 17986564777.41770172\n",
      "Iteration 448, loss = 17986563436.77434540\n",
      "Iteration 449, loss = 17986562096.65517044\n",
      "Iteration 450, loss = 17986560757.06082916\n",
      "Iteration 451, loss = 17986559417.99208832\n",
      "Iteration 452, loss = 17986558079.44970703\n",
      "Iteration 453, loss = 17986556741.43447113\n",
      "Iteration 454, loss = 17986555403.94707108\n",
      "Iteration 455, loss = 17986554066.98812866\n",
      "Iteration 456, loss = 17986552730.55811310\n",
      "Iteration 457, loss = 17986551394.65731812\n",
      "Iteration 458, loss = 17986550059.28581619\n",
      "Iteration 459, loss = 17986548724.44344711\n",
      "Iteration 460, loss = 17986547390.12978363\n",
      "Iteration 461, loss = 17986546056.34410477\n",
      "Iteration 462, loss = 17986544723.08541489\n",
      "Iteration 463, loss = 17986543390.35239792\n",
      "Iteration 464, loss = 17986542058.14342117\n",
      "Iteration 465, loss = 17986540726.45656204\n",
      "Iteration 466, loss = 17986539395.28960800\n",
      "Iteration 467, loss = 17986538064.64004517\n",
      "Iteration 468, loss = 17986536734.50509262\n",
      "Iteration 469, loss = 17986535404.88172531\n",
      "Iteration 470, loss = 17986534075.76667404\n",
      "Iteration 471, loss = 17986532747.15647507\n",
      "Iteration 472, loss = 17986531419.04745865\n",
      "Iteration 473, loss = 17986530091.43579865\n",
      "Iteration 474, loss = 17986528764.31751633\n",
      "Iteration 475, loss = 17986527437.68852234\n",
      "Iteration 476, loss = 17986526111.54463196\n",
      "Iteration 477, loss = 17986524785.88156509\n",
      "Iteration 478, loss = 17986523460.69499969\n",
      "Iteration 479, loss = 17986522135.98056030\n",
      "Iteration 480, loss = 17986520811.73386002\n",
      "Iteration 481, loss = 17986519487.95050049\n",
      "Iteration 482, loss = 17986518164.62608337\n",
      "Iteration 483, loss = 17986516841.75624084\n",
      "Iteration 484, loss = 17986515519.33662796\n",
      "Iteration 485, loss = 17986514197.36294937\n",
      "Iteration 486, loss = 17986512875.83096313\n",
      "Iteration 487, loss = 17986511554.73648453\n",
      "Iteration 488, loss = 17986510234.07539749\n",
      "Iteration 489, loss = 17986508913.84366608\n",
      "Iteration 490, loss = 17986507594.03734207\n",
      "Iteration 491, loss = 17986506274.65255356\n",
      "Iteration 492, loss = 17986504955.68554306\n",
      "Iteration 493, loss = 17986503637.13264465\n",
      "Iteration 494, loss = 17986502318.99028778\n",
      "Iteration 495, loss = 17986501001.25502777\n",
      "Iteration 496, loss = 17986499683.92353439\n",
      "Iteration 497, loss = 17986498366.99259186\n",
      "Iteration 498, loss = 17986497050.45911407\n",
      "Iteration 499, loss = 17986495734.32013321\n",
      "Iteration 500, loss = 17986494418.57281494\n",
      "Iteration 1, loss = 19463665967.65018082\n",
      "Iteration 2, loss = 19463658180.56414032\n",
      "Iteration 3, loss = 19463650429.75268555\n",
      "Iteration 4, loss = 19463642658.73178101\n",
      "Iteration 5, loss = 19463634861.94784164\n",
      "Iteration 6, loss = 19463627111.75849915\n",
      "Iteration 7, loss = 19463619241.37162018\n",
      "Iteration 8, loss = 19463611556.24116516\n",
      "Iteration 9, loss = 19463603670.30320358\n",
      "Iteration 10, loss = 19463595958.41507339\n",
      "Iteration 11, loss = 19463588044.39512634\n",
      "Iteration 12, loss = 19463580300.07561111\n",
      "Iteration 13, loss = 19463572453.79661560\n",
      "Iteration 14, loss = 19463564610.45266342\n",
      "Iteration 15, loss = 19463556790.32361984\n",
      "Iteration 16, loss = 19463548878.27225494\n",
      "Iteration 17, loss = 19463541046.84724426\n",
      "Iteration 18, loss = 19463533148.79385757\n",
      "Iteration 19, loss = 19463525244.69653320\n",
      "Iteration 20, loss = 19463517292.15136337\n",
      "Iteration 21, loss = 19463509329.87477112\n",
      "Iteration 22, loss = 19463501324.31506348\n",
      "Iteration 23, loss = 19463493253.30229187\n",
      "Iteration 24, loss = 19463485219.92127228\n",
      "Iteration 25, loss = 19463477023.14108658\n",
      "Iteration 26, loss = 19463468960.71483231\n",
      "Iteration 27, loss = 19463460762.17309189\n",
      "Iteration 28, loss = 19463452500.29678345\n",
      "Iteration 29, loss = 19463444244.66680527\n",
      "Iteration 30, loss = 19463435939.48747253\n",
      "Iteration 31, loss = 19463427449.57191467\n",
      "Iteration 32, loss = 19463419035.89767075\n",
      "Iteration 33, loss = 19463410518.27553177\n",
      "Iteration 34, loss = 19463401803.19447327\n",
      "Iteration 35, loss = 19463393288.81688690\n",
      "Iteration 36, loss = 19463384650.19757462\n",
      "Iteration 37, loss = 19463375963.56561279\n",
      "Iteration 38, loss = 19463367336.41093445\n",
      "Iteration 39, loss = 19463358647.41237259\n",
      "Iteration 40, loss = 19463350083.56428528\n",
      "Iteration 41, loss = 19463341504.86143875\n",
      "Iteration 42, loss = 19463332763.73956299\n",
      "Iteration 43, loss = 19463324315.18103790\n",
      "Iteration 44, loss = 19463315696.25948334\n",
      "Iteration 45, loss = 19463307090.01723099\n",
      "Iteration 46, loss = 19463298602.84315491\n",
      "Iteration 47, loss = 19463290013.76412201\n",
      "Iteration 48, loss = 19463281498.70236588\n",
      "Iteration 49, loss = 19463273035.12269592\n",
      "Iteration 50, loss = 19463264557.16575623\n",
      "Iteration 51, loss = 19463256251.71634674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 19463247921.16941452\n",
      "Iteration 53, loss = 19463239788.50819016\n",
      "Iteration 54, loss = 19463231620.77262497\n",
      "Iteration 55, loss = 19463223641.32960129\n",
      "Iteration 56, loss = 19463215748.69249725\n",
      "Iteration 57, loss = 19463207948.25492096\n",
      "Iteration 58, loss = 19463200254.25730515\n",
      "Iteration 59, loss = 19463192645.57788849\n",
      "Iteration 60, loss = 19463185188.85775375\n",
      "Iteration 61, loss = 19463177814.78945923\n",
      "Iteration 62, loss = 19463170556.77132416\n",
      "Iteration 63, loss = 19463163277.72528076\n",
      "Iteration 64, loss = 19463156134.71446991\n",
      "Iteration 65, loss = 19463149237.45204544\n",
      "Iteration 66, loss = 19463142282.82299042\n",
      "Iteration 67, loss = 19463135490.73851013\n",
      "Iteration 68, loss = 19463128845.37780380\n",
      "Iteration 69, loss = 19463122249.36001587\n",
      "Iteration 70, loss = 19463115904.70162582\n",
      "Iteration 71, loss = 19463109574.14678955\n",
      "Iteration 72, loss = 19463103392.98412323\n",
      "Iteration 73, loss = 19463097288.23736572\n",
      "Iteration 74, loss = 19463091343.64222336\n",
      "Iteration 75, loss = 19463085546.19317627\n",
      "Iteration 76, loss = 19463079715.91968536\n",
      "Iteration 77, loss = 19463074169.71047974\n",
      "Iteration 78, loss = 19463068558.66246033\n",
      "Iteration 79, loss = 19463063087.74135590\n",
      "Iteration 80, loss = 19463057727.00096130\n",
      "Iteration 81, loss = 19463052441.50160599\n",
      "Iteration 82, loss = 19463047233.61499786\n",
      "Iteration 83, loss = 19463042108.80508041\n",
      "Iteration 84, loss = 19463037117.60304642\n",
      "Iteration 85, loss = 19463032188.25952911\n",
      "Iteration 86, loss = 19463027396.69443130\n",
      "Iteration 87, loss = 19463022707.72016525\n",
      "Iteration 88, loss = 19463018039.65224075\n",
      "Iteration 89, loss = 19463013482.41343689\n",
      "Iteration 90, loss = 19463009030.34618378\n",
      "Iteration 91, loss = 19463004643.55839920\n",
      "Iteration 92, loss = 19463000309.53928757\n",
      "Iteration 93, loss = 19462996079.46432495\n",
      "Iteration 94, loss = 19462991959.85875320\n",
      "Iteration 95, loss = 19462987868.59368134\n",
      "Iteration 96, loss = 19462983864.93835449\n",
      "Iteration 97, loss = 19462979963.68121338\n",
      "Iteration 98, loss = 19462976053.03155899\n",
      "Iteration 99, loss = 19462972254.48078156\n",
      "Iteration 100, loss = 19462968476.12982178\n",
      "Iteration 101, loss = 19462964765.52957916\n",
      "Iteration 102, loss = 19462961072.53815842\n",
      "Iteration 103, loss = 19462957468.86143875\n",
      "Iteration 104, loss = 19462953868.65924835\n",
      "Iteration 105, loss = 19462950335.44268417\n",
      "Iteration 106, loss = 19462946826.26806259\n",
      "Iteration 107, loss = 19462943402.96200180\n",
      "Iteration 108, loss = 19462939981.50836563\n",
      "Iteration 109, loss = 19462936627.79089737\n",
      "Iteration 110, loss = 19462933290.95299911\n",
      "Iteration 111, loss = 19462930049.34748459\n",
      "Iteration 112, loss = 19462926804.56148529\n",
      "Iteration 113, loss = 19462923610.53506851\n",
      "Iteration 114, loss = 19462920460.15800476\n",
      "Iteration 115, loss = 19462917335.12931824\n",
      "Iteration 116, loss = 19462914242.03575516\n",
      "Iteration 117, loss = 19462911164.63374710\n",
      "Iteration 118, loss = 19462908130.05018616\n",
      "Iteration 119, loss = 19462905094.61036682\n",
      "Iteration 120, loss = 19462902120.63735580\n",
      "Iteration 121, loss = 19462899109.84702682\n",
      "Iteration 122, loss = 19462896147.63525772\n",
      "Iteration 123, loss = 19462893201.61322403\n",
      "Iteration 124, loss = 19462890279.22039413\n",
      "Iteration 125, loss = 19462887347.92795181\n",
      "Iteration 126, loss = 19462884426.21073532\n",
      "Iteration 127, loss = 19462881539.93302536\n",
      "Iteration 128, loss = 19462878641.69771957\n",
      "Iteration 129, loss = 19462875679.49639130\n",
      "Iteration 130, loss = 19462872819.70886230\n",
      "Iteration 131, loss = 19462869905.33764648\n",
      "Iteration 132, loss = 19462867018.98339462\n",
      "Iteration 133, loss = 19462864142.01652908\n",
      "Iteration 134, loss = 19462861305.91870499\n",
      "Iteration 135, loss = 19462858462.57306290\n",
      "Iteration 136, loss = 19462855638.42636490\n",
      "Iteration 137, loss = 19462852822.94789886\n",
      "Iteration 138, loss = 19462850051.88253021\n",
      "Iteration 139, loss = 19462847236.02550888\n",
      "Iteration 140, loss = 19462844442.27011490\n",
      "Iteration 141, loss = 19462841668.11883926\n",
      "Iteration 142, loss = 19462838854.63794708\n",
      "Iteration 143, loss = 19462836095.98082352\n",
      "Iteration 144, loss = 19462833296.92753983\n",
      "Iteration 145, loss = 19462830510.05411911\n",
      "Iteration 146, loss = 19462827722.04833221\n",
      "Iteration 147, loss = 19462824938.08199310\n",
      "Iteration 148, loss = 19462822150.32654953\n",
      "Iteration 149, loss = 19462819374.36264038\n",
      "Iteration 150, loss = 19462816557.01673889\n",
      "Iteration 151, loss = 19462813773.85695648\n",
      "Iteration 152, loss = 19462810990.45763397\n",
      "Iteration 153, loss = 19462808233.20436478\n",
      "Iteration 154, loss = 19462805445.71187592\n",
      "Iteration 155, loss = 19462802698.77813339\n",
      "Iteration 156, loss = 19462799941.46475601\n",
      "Iteration 157, loss = 19462797200.29864883\n",
      "Iteration 158, loss = 19462794453.28359604\n",
      "Iteration 159, loss = 19462791712.03567123\n",
      "Iteration 160, loss = 19462788968.34686661\n",
      "Iteration 161, loss = 19462786208.96620178\n",
      "Iteration 162, loss = 19462783496.66813660\n",
      "Iteration 163, loss = 19462780745.32255554\n",
      "Iteration 164, loss = 19462778008.81758118\n",
      "Iteration 165, loss = 19462775261.72815704\n",
      "Iteration 166, loss = 19462772517.47707367\n",
      "Iteration 167, loss = 19462769759.13279343\n",
      "Iteration 168, loss = 19462767030.56407166\n",
      "Iteration 169, loss = 19462764289.00310516\n",
      "Iteration 170, loss = 19462761566.07227707\n",
      "Iteration 171, loss = 19462758861.98289108\n",
      "Iteration 172, loss = 19462756162.00556946\n",
      "Iteration 173, loss = 19462753446.34555817\n",
      "Iteration 174, loss = 19462750745.79019547\n",
      "Iteration 175, loss = 19462748066.88591766\n",
      "Iteration 176, loss = 19462745351.09518433\n",
      "Iteration 177, loss = 19462742629.72631454\n",
      "Iteration 178, loss = 19462739923.97661209\n",
      "Iteration 179, loss = 19462737230.32409286\n",
      "Iteration 180, loss = 19462734545.21781921\n",
      "Iteration 181, loss = 19462731904.57343674\n",
      "Iteration 182, loss = 19462729221.42724991\n",
      "Iteration 183, loss = 19462726602.77685928\n",
      "Iteration 184, loss = 19462723944.58377075\n",
      "Iteration 185, loss = 19462721330.61935043\n",
      "Iteration 186, loss = 19462718701.51204300\n",
      "Iteration 187, loss = 19462716078.67851257\n",
      "Iteration 188, loss = 19462713429.13354111\n",
      "Iteration 189, loss = 19462710842.24822235\n",
      "Iteration 190, loss = 19462708227.12543869\n",
      "Iteration 191, loss = 19462705631.27533340\n",
      "Iteration 192, loss = 19462703032.15093613\n",
      "Iteration 193, loss = 19462700469.13538742\n",
      "Iteration 194, loss = 19462697931.60641861\n",
      "Iteration 195, loss = 19462695393.90374756\n",
      "Iteration 196, loss = 19462692859.02451706\n",
      "Iteration 197, loss = 19462690350.18740463\n",
      "Iteration 198, loss = 19462687821.91682434\n",
      "Iteration 199, loss = 19462685293.77326965\n",
      "Iteration 200, loss = 19462682772.80669785\n",
      "Iteration 201, loss = 19462680245.21042633\n",
      "Iteration 202, loss = 19462677718.14095688\n",
      "Iteration 203, loss = 19462675190.80355072\n",
      "Iteration 204, loss = 19462672676.51115799\n",
      "Iteration 205, loss = 19462670150.89887238\n",
      "Iteration 206, loss = 19462667653.54209518\n",
      "Iteration 207, loss = 19462665129.99995804\n",
      "Iteration 208, loss = 19462662622.73496628\n",
      "Iteration 209, loss = 19462660134.72458267\n",
      "Iteration 210, loss = 19462657612.83533096\n",
      "Iteration 211, loss = 19462655117.56024170\n",
      "Iteration 212, loss = 19462652612.52880096\n",
      "Iteration 213, loss = 19462650108.25293732\n",
      "Iteration 214, loss = 19462647614.81930923\n",
      "Iteration 215, loss = 19462645156.14975357\n",
      "Iteration 216, loss = 19462642691.53591156\n",
      "Iteration 217, loss = 19462640232.40657425\n",
      "Iteration 218, loss = 19462637776.23318481\n",
      "Iteration 219, loss = 19462635314.90277863\n",
      "Iteration 220, loss = 19462632864.14282227\n",
      "Iteration 221, loss = 19462630368.81674194\n",
      "Iteration 222, loss = 19462627897.95194244\n",
      "Iteration 223, loss = 19462625431.29859161\n",
      "Iteration 224, loss = 19462622937.56299210\n",
      "Iteration 225, loss = 19462620472.76903915\n",
      "Iteration 226, loss = 19462617992.81314087\n",
      "Iteration 227, loss = 19462615535.15504837\n",
      "Iteration 228, loss = 19462613079.22893143\n",
      "Iteration 229, loss = 19462610636.91793823\n",
      "Iteration 230, loss = 19462608193.09095764\n",
      "Iteration 231, loss = 19462605786.95537186\n",
      "Iteration 232, loss = 19462603333.87452316\n",
      "Iteration 233, loss = 19462600903.48387146\n",
      "Iteration 234, loss = 19462598449.30739594\n",
      "Iteration 235, loss = 19462596018.05512619\n",
      "Iteration 236, loss = 19462593584.88771439\n",
      "Iteration 237, loss = 19462591096.89959335\n",
      "Iteration 238, loss = 19462588693.04013443\n",
      "Iteration 239, loss = 19462586248.98871231\n",
      "Iteration 240, loss = 19462583792.74710464\n",
      "Iteration 241, loss = 19462581382.57341003\n",
      "Iteration 242, loss = 19462578959.29777145\n",
      "Iteration 243, loss = 19462576542.63590622\n",
      "Iteration 244, loss = 19462574123.29598236\n",
      "Iteration 245, loss = 19462571744.79457092\n",
      "Iteration 246, loss = 19462569352.61801910\n",
      "Iteration 247, loss = 19462566974.24594879\n",
      "Iteration 248, loss = 19462564570.66007233\n",
      "Iteration 249, loss = 19462562197.03652573\n",
      "Iteration 250, loss = 19462559804.63178635\n",
      "Iteration 251, loss = 19462557399.42021179\n",
      "Iteration 252, loss = 19462555006.18988037\n",
      "Iteration 253, loss = 19462552607.98586273\n",
      "Iteration 254, loss = 19462550230.48168182\n",
      "Iteration 255, loss = 19462547824.63204193\n",
      "Iteration 256, loss = 19462545463.77502441\n",
      "Iteration 257, loss = 19462543085.26102448\n",
      "Iteration 258, loss = 19462540729.08378983\n",
      "Iteration 259, loss = 19462538354.79353714\n",
      "Iteration 260, loss = 19462535977.17408752\n",
      "Iteration 261, loss = 19462533617.64886093\n",
      "Iteration 262, loss = 19462531229.30120468\n",
      "Iteration 263, loss = 19462528894.95930481\n",
      "Iteration 264, loss = 19462526516.15073776\n",
      "Iteration 265, loss = 19462524171.70256042\n",
      "Iteration 266, loss = 19462521814.38116074\n",
      "Iteration 267, loss = 19462519481.87294769\n",
      "Iteration 268, loss = 19462517142.53619385\n",
      "Iteration 269, loss = 19462514803.38962173\n",
      "Iteration 270, loss = 19462512466.48305130\n",
      "Iteration 271, loss = 19462510123.60276413\n",
      "Iteration 272, loss = 19462507761.00084686\n",
      "Iteration 273, loss = 19462505433.74083710\n",
      "Iteration 274, loss = 19462503070.72098541\n",
      "Iteration 275, loss = 19462500728.64007950\n",
      "Iteration 276, loss = 19462498382.96297836\n",
      "Iteration 277, loss = 19462496048.21403885\n",
      "Iteration 278, loss = 19462493715.07579803\n",
      "Iteration 279, loss = 19462491404.76945496\n",
      "Iteration 280, loss = 19462489085.19732666\n",
      "Iteration 281, loss = 19462486779.10267639\n",
      "Iteration 282, loss = 19462484468.73691177\n",
      "Iteration 283, loss = 19462482156.83701706\n",
      "Iteration 284, loss = 19462479848.69424820\n",
      "Iteration 285, loss = 19462477541.70708847\n",
      "Iteration 286, loss = 19462475232.17319489\n",
      "Iteration 287, loss = 19462472905.30573654\n",
      "Iteration 288, loss = 19462470590.47286987\n",
      "Iteration 289, loss = 19462468265.89350128\n",
      "Iteration 290, loss = 19462465959.31127930\n",
      "Iteration 291, loss = 19462463638.39328384\n",
      "Iteration 292, loss = 19462461282.95145035\n",
      "Iteration 293, loss = 19462458990.62835312\n",
      "Iteration 294, loss = 19462456653.93867874\n",
      "Iteration 295, loss = 19462454328.33309937\n",
      "Iteration 296, loss = 19462452033.61610031\n",
      "Iteration 297, loss = 19462449711.85953522\n",
      "Iteration 298, loss = 19462447403.30936432\n",
      "Iteration 299, loss = 19462445114.36590576\n",
      "Iteration 300, loss = 19462442806.32030106\n",
      "Iteration 301, loss = 19462440489.18018341\n",
      "Iteration 302, loss = 19462438187.57369995\n",
      "Iteration 303, loss = 19462435859.23195648\n",
      "Iteration 304, loss = 19462433556.28258514\n",
      "Iteration 305, loss = 19462431249.44363785\n",
      "Iteration 306, loss = 19462428952.68505096\n",
      "Iteration 307, loss = 19462426654.07874680\n",
      "Iteration 308, loss = 19462424371.13025284\n",
      "Iteration 309, loss = 19462422071.90812683\n",
      "Iteration 310, loss = 19462419781.10567856\n",
      "Iteration 311, loss = 19462417467.85792923\n",
      "Iteration 312, loss = 19462415161.94709778\n",
      "Iteration 313, loss = 19462412872.38785934\n",
      "Iteration 314, loss = 19462410560.34778595\n",
      "Iteration 315, loss = 19462408256.00099182\n",
      "Iteration 316, loss = 19462405982.72723770\n",
      "Iteration 317, loss = 19462403708.40241623\n",
      "Iteration 318, loss = 19462401437.73877335\n",
      "Iteration 319, loss = 19462399175.94800568\n",
      "Iteration 320, loss = 19462396888.10337830\n",
      "Iteration 321, loss = 19462394652.70542145\n",
      "Iteration 322, loss = 19462392382.26676559\n",
      "Iteration 323, loss = 19462390115.04482651\n",
      "Iteration 324, loss = 19462387836.05885315\n",
      "Iteration 325, loss = 19462385577.30583572\n",
      "Iteration 326, loss = 19462383296.71965408\n",
      "Iteration 327, loss = 19462381042.64606094\n",
      "Iteration 328, loss = 19462378746.16250610\n",
      "Iteration 329, loss = 19462376472.90507507\n",
      "Iteration 330, loss = 19462374188.82603455\n",
      "Iteration 331, loss = 19462371917.22455978\n",
      "Iteration 332, loss = 19462369624.16328430\n",
      "Iteration 333, loss = 19462367362.88520813\n",
      "Iteration 334, loss = 19462365062.18841934\n",
      "Iteration 335, loss = 19462362795.43880463\n",
      "Iteration 336, loss = 19462360517.53849411\n",
      "Iteration 337, loss = 19462358225.61176682\n",
      "Iteration 338, loss = 19462355943.90398407\n",
      "Iteration 339, loss = 19462353633.61302185\n",
      "Iteration 340, loss = 19462351351.78553391\n",
      "Iteration 341, loss = 19462349043.85977554\n",
      "Iteration 342, loss = 19462346765.10652924\n",
      "Iteration 343, loss = 19462344517.14399719\n",
      "Iteration 344, loss = 19462342219.85091782\n",
      "Iteration 345, loss = 19462339973.84904099\n",
      "Iteration 346, loss = 19462337727.59839249\n",
      "Iteration 347, loss = 19462335477.34129333\n",
      "Iteration 348, loss = 19462333225.73420715\n",
      "Iteration 349, loss = 19462330974.76599121\n",
      "Iteration 350, loss = 19462328708.86237335\n",
      "Iteration 351, loss = 19462326457.03468323\n",
      "Iteration 352, loss = 19462324203.67607498\n",
      "Iteration 353, loss = 19462321943.00708389\n",
      "Iteration 354, loss = 19462319663.06557465\n",
      "Iteration 355, loss = 19462317410.41852188\n",
      "Iteration 356, loss = 19462315154.53944397\n",
      "Iteration 357, loss = 19462312869.57043076\n",
      "Iteration 358, loss = 19462310591.43865204\n",
      "Iteration 359, loss = 19462308333.75555038\n",
      "Iteration 360, loss = 19462306054.47003555\n",
      "Iteration 361, loss = 19462303796.00556564\n",
      "Iteration 362, loss = 19462301512.22661591\n",
      "Iteration 363, loss = 19462299240.60911179\n",
      "Iteration 364, loss = 19462296981.32086182\n",
      "Iteration 365, loss = 19462294693.49415588\n",
      "Iteration 366, loss = 19462292421.02847672\n",
      "Iteration 367, loss = 19462290134.95917892\n",
      "Iteration 368, loss = 19462287857.83121872\n",
      "Iteration 369, loss = 19462285591.47495270\n",
      "Iteration 370, loss = 19462283300.31654358\n",
      "Iteration 371, loss = 19462281038.60286713\n",
      "Iteration 372, loss = 19462278769.10057449\n",
      "Iteration 373, loss = 19462276487.13026810\n",
      "Iteration 374, loss = 19462274210.24878311\n",
      "Iteration 375, loss = 19462271936.20206833\n",
      "Iteration 376, loss = 19462269630.51141739\n",
      "Iteration 377, loss = 19462267352.45217514\n",
      "Iteration 378, loss = 19462265045.82567978\n",
      "Iteration 379, loss = 19462262762.12236404\n",
      "Iteration 380, loss = 19462260433.56161499\n",
      "Iteration 381, loss = 19462258161.33699036\n",
      "Iteration 382, loss = 19462255827.71231461\n",
      "Iteration 383, loss = 19462253508.31544495\n",
      "Iteration 384, loss = 19462251168.25860214\n",
      "Iteration 385, loss = 19462248856.71973038\n",
      "Iteration 386, loss = 19462246466.91150284\n",
      "Iteration 387, loss = 19462244115.43484116\n",
      "Iteration 388, loss = 19462241713.71294785\n",
      "Iteration 389, loss = 19462239299.43466949\n",
      "Iteration 390, loss = 19462236850.23530960\n",
      "Iteration 391, loss = 19462234411.70965958\n",
      "Iteration 392, loss = 19462231897.47342682\n",
      "Iteration 393, loss = 19462229415.38982391\n",
      "Iteration 394, loss = 19462226914.91982651\n",
      "Iteration 395, loss = 19462224343.98080063\n",
      "Iteration 396, loss = 19462221827.92848969\n",
      "Iteration 397, loss = 19462219222.10087204\n",
      "Iteration 398, loss = 19462216656.49227142\n",
      "Iteration 399, loss = 19462214044.69446564\n",
      "Iteration 400, loss = 19462211440.62093353\n",
      "Iteration 401, loss = 19462208834.34463501\n",
      "Iteration 402, loss = 19462206223.98595810\n",
      "Iteration 403, loss = 19462203637.33858490\n",
      "Iteration 404, loss = 19462201015.32915878\n",
      "Iteration 405, loss = 19462198439.69709015\n",
      "Iteration 406, loss = 19462195795.99032593\n",
      "Iteration 407, loss = 19462193211.63498688\n",
      "Iteration 408, loss = 19462190575.43692017\n",
      "Iteration 409, loss = 19462187963.40468979\n",
      "Iteration 410, loss = 19462185345.00970459\n",
      "Iteration 411, loss = 19462182732.89516830\n",
      "Iteration 412, loss = 19462180126.97215271\n",
      "Iteration 413, loss = 19462177554.10913849\n",
      "Iteration 414, loss = 19462174968.88227081\n",
      "Iteration 415, loss = 19462172413.54652405\n",
      "Iteration 416, loss = 19462169866.91548157\n",
      "Iteration 417, loss = 19462167331.00381851\n",
      "Iteration 418, loss = 19462164797.21279526\n",
      "Iteration 419, loss = 19462162256.11522293\n",
      "Iteration 420, loss = 19462159760.70602417\n",
      "Iteration 421, loss = 19462157213.14295578\n",
      "Iteration 422, loss = 19462154707.85913467\n",
      "Iteration 423, loss = 19462152220.97393799\n",
      "Iteration 424, loss = 19462149702.58739471\n",
      "Iteration 425, loss = 19462147234.37027359\n",
      "Iteration 426, loss = 19462144728.71831894\n",
      "Iteration 427, loss = 19462142284.10879135\n",
      "Iteration 428, loss = 19462139802.82910538\n",
      "Iteration 429, loss = 19462137349.85245132\n",
      "Iteration 430, loss = 19462134869.87253952\n",
      "Iteration 431, loss = 19462132430.78942108\n",
      "Iteration 432, loss = 19462129968.08737183\n",
      "Iteration 433, loss = 19462127541.70678329\n",
      "Iteration 434, loss = 19462125100.81950760\n",
      "Iteration 435, loss = 19462122648.72594070\n",
      "Iteration 436, loss = 19462120225.72171402\n",
      "Iteration 437, loss = 19462117778.53720474\n",
      "Iteration 438, loss = 19462115343.64110947\n",
      "Iteration 439, loss = 19462112902.40531921\n",
      "Iteration 440, loss = 19462110484.47980118\n",
      "Iteration 441, loss = 19462108035.43349457\n",
      "Iteration 442, loss = 19462105609.76511383\n",
      "Iteration 443, loss = 19462103197.58661270\n",
      "Iteration 444, loss = 19462100775.47340775\n",
      "Iteration 445, loss = 19462098403.29444504\n",
      "Iteration 446, loss = 19462095996.84617233\n",
      "Iteration 447, loss = 19462093650.65590668\n",
      "Iteration 448, loss = 19462091236.67669678\n",
      "Iteration 449, loss = 19462088869.05249786\n",
      "Iteration 450, loss = 19462086497.35057831\n",
      "Iteration 451, loss = 19462084094.12140656\n",
      "Iteration 452, loss = 19462081703.97377396\n",
      "Iteration 453, loss = 19462079310.24108505\n",
      "Iteration 454, loss = 19462076935.02110672\n",
      "Iteration 455, loss = 19462074540.82965469\n",
      "Iteration 456, loss = 19462072149.75327682\n",
      "Iteration 457, loss = 19462069764.32815933\n",
      "Iteration 458, loss = 19462067356.26927185\n",
      "Iteration 459, loss = 19462064976.15205765\n",
      "Iteration 460, loss = 19462062601.55688095\n",
      "Iteration 461, loss = 19462060202.91173935\n",
      "Iteration 462, loss = 19462057839.70689774\n",
      "Iteration 463, loss = 19462055455.48830795\n",
      "Iteration 464, loss = 19462053088.37789917\n",
      "Iteration 465, loss = 19462050724.60342789\n",
      "Iteration 466, loss = 19462048391.86952591\n",
      "Iteration 467, loss = 19462046030.32888031\n",
      "Iteration 468, loss = 19462043724.10057831\n",
      "Iteration 469, loss = 19462041388.14962006\n",
      "Iteration 470, loss = 19462039045.49478912\n",
      "Iteration 471, loss = 19462036715.45486069\n",
      "Iteration 472, loss = 19462034360.86206818\n",
      "Iteration 473, loss = 19462031990.12388611\n",
      "Iteration 474, loss = 19462029603.20891571\n",
      "Iteration 475, loss = 19462027220.00589371\n",
      "Iteration 476, loss = 19462024807.08092499\n",
      "Iteration 477, loss = 19462022399.83461761\n",
      "Iteration 478, loss = 19462019957.52557373\n",
      "Iteration 479, loss = 19462017503.69313431\n",
      "Iteration 480, loss = 19462015028.10541916\n",
      "Iteration 481, loss = 19462012494.61903000\n",
      "Iteration 482, loss = 19462009982.65923309\n",
      "Iteration 483, loss = 19462007412.36219025\n",
      "Iteration 484, loss = 19462004790.16838837\n",
      "Iteration 485, loss = 19462002196.81983948\n",
      "Iteration 486, loss = 19461999518.90816879\n",
      "Iteration 487, loss = 19461996861.82349777\n",
      "Iteration 488, loss = 19461994171.65117264\n",
      "Iteration 489, loss = 19461991453.40233994\n",
      "Iteration 490, loss = 19461988736.07579041\n",
      "Iteration 491, loss = 19461985987.95280075\n",
      "Iteration 492, loss = 19461983227.23192596\n",
      "Iteration 493, loss = 19461980465.98348236\n",
      "Iteration 494, loss = 19461977723.08263016\n",
      "Iteration 495, loss = 19461974958.47355270\n",
      "Iteration 496, loss = 19461972193.01026154\n",
      "Iteration 497, loss = 19461969460.64884567\n",
      "Iteration 498, loss = 19461966741.91046524\n",
      "Iteration 499, loss = 19461964032.07400513\n",
      "Iteration 500, loss = 19461961317.88108826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 20102386861.32903290\n",
      "Iteration 2, loss = 20102379649.98853302\n",
      "Iteration 3, loss = 20102372397.12948608\n",
      "Iteration 4, loss = 20102365131.01738358\n",
      "Iteration 5, loss = 20102358006.95439148\n",
      "Iteration 6, loss = 20102350684.55485916\n",
      "Iteration 7, loss = 20102343446.23471451\n",
      "Iteration 8, loss = 20102336209.00228119\n",
      "Iteration 9, loss = 20102329037.34987259\n",
      "Iteration 10, loss = 20102321736.46797562\n",
      "Iteration 11, loss = 20102314492.00960541\n",
      "Iteration 12, loss = 20102307295.71492004\n",
      "Iteration 13, loss = 20102299971.27399063\n",
      "Iteration 14, loss = 20102292635.39064026\n",
      "Iteration 15, loss = 20102285344.39586639\n",
      "Iteration 16, loss = 20102277996.52862167\n",
      "Iteration 17, loss = 20102270605.52593613\n",
      "Iteration 18, loss = 20102263128.75922012\n",
      "Iteration 19, loss = 20102255586.33079910\n",
      "Iteration 20, loss = 20102248047.89888763\n",
      "Iteration 21, loss = 20102240428.35975266\n",
      "Iteration 22, loss = 20102232719.34679031\n",
      "Iteration 23, loss = 20102224879.72462082\n",
      "Iteration 24, loss = 20102216931.64396667\n",
      "Iteration 25, loss = 20102208919.46494675\n",
      "Iteration 26, loss = 20102200859.50394440\n",
      "Iteration 27, loss = 20102192572.30519104\n",
      "Iteration 28, loss = 20102184138.15397644\n",
      "Iteration 29, loss = 20102175680.26004410\n",
      "Iteration 30, loss = 20102167047.76354980\n",
      "Iteration 31, loss = 20102158286.45335388\n",
      "Iteration 32, loss = 20102149513.51757812\n",
      "Iteration 33, loss = 20102140518.18662262\n",
      "Iteration 34, loss = 20102131476.30153275\n",
      "Iteration 35, loss = 20102122452.89476395\n",
      "Iteration 36, loss = 20102113255.82214737\n",
      "Iteration 37, loss = 20102104107.55904388\n",
      "Iteration 38, loss = 20102094940.24185181\n",
      "Iteration 39, loss = 20102085616.57045746\n",
      "Iteration 40, loss = 20102076350.88814545\n",
      "Iteration 41, loss = 20102067037.59277725\n",
      "Iteration 42, loss = 20102057711.66753769\n",
      "Iteration 43, loss = 20102048400.85905457\n",
      "Iteration 44, loss = 20102039058.45888901\n",
      "Iteration 45, loss = 20102029555.92628479\n",
      "Iteration 46, loss = 20102020189.82747269\n",
      "Iteration 47, loss = 20102010768.13725281\n",
      "Iteration 48, loss = 20102001317.11000824\n",
      "Iteration 49, loss = 20101991847.89293671\n",
      "Iteration 50, loss = 20101982534.93242645\n",
      "Iteration 51, loss = 20101973123.30157852\n",
      "Iteration 52, loss = 20101963918.46044540\n",
      "Iteration 53, loss = 20101954762.05848694\n",
      "Iteration 54, loss = 20101945842.57747269\n",
      "Iteration 55, loss = 20101936741.44033432\n",
      "Iteration 56, loss = 20101928105.89675140\n",
      "Iteration 57, loss = 20101919552.79180908\n",
      "Iteration 58, loss = 20101911062.77987289\n",
      "Iteration 59, loss = 20101902892.29082489\n",
      "Iteration 60, loss = 20101894893.43350983\n",
      "Iteration 61, loss = 20101887032.16997147\n",
      "Iteration 62, loss = 20101879401.79574203\n",
      "Iteration 63, loss = 20101871881.21342087\n",
      "Iteration 64, loss = 20101864638.14220810\n",
      "Iteration 65, loss = 20101857422.97520447\n",
      "Iteration 66, loss = 20101850425.39263916\n",
      "Iteration 67, loss = 20101843552.50860596\n",
      "Iteration 68, loss = 20101836778.69810486\n",
      "Iteration 69, loss = 20101830182.52019501\n",
      "Iteration 70, loss = 20101823745.67417526\n",
      "Iteration 71, loss = 20101817475.28165817\n",
      "Iteration 72, loss = 20101811353.83059692\n",
      "Iteration 73, loss = 20101805353.97118759\n",
      "Iteration 74, loss = 20101799541.91360092\n",
      "Iteration 75, loss = 20101793858.86733246\n",
      "Iteration 76, loss = 20101788253.65179062\n",
      "Iteration 77, loss = 20101782806.68266296\n",
      "Iteration 78, loss = 20101777455.51910782\n",
      "Iteration 79, loss = 20101772139.58351135\n",
      "Iteration 80, loss = 20101766964.43820190\n",
      "Iteration 81, loss = 20101761823.89190292\n",
      "Iteration 82, loss = 20101756815.76726913\n",
      "Iteration 83, loss = 20101751831.28699112\n",
      "Iteration 84, loss = 20101746966.87344742\n",
      "Iteration 85, loss = 20101742086.02318573\n",
      "Iteration 86, loss = 20101737363.61401367\n",
      "Iteration 87, loss = 20101732662.92443466\n",
      "Iteration 88, loss = 20101728026.56726074\n",
      "Iteration 89, loss = 20101723425.50906372\n",
      "Iteration 90, loss = 20101718929.46195221\n",
      "Iteration 91, loss = 20101714444.44275284\n",
      "Iteration 92, loss = 20101710082.83241653\n",
      "Iteration 93, loss = 20101705839.78767395\n",
      "Iteration 94, loss = 20101701713.29428101\n",
      "Iteration 95, loss = 20101697616.06175613\n",
      "Iteration 96, loss = 20101693686.13952255\n",
      "Iteration 97, loss = 20101689762.47718048\n",
      "Iteration 98, loss = 20101685968.92846680\n",
      "Iteration 99, loss = 20101682229.66133881\n",
      "Iteration 100, loss = 20101678545.83740997\n",
      "Iteration 101, loss = 20101674932.92146301\n",
      "Iteration 102, loss = 20101671354.60961151\n",
      "Iteration 103, loss = 20101667838.79179001\n",
      "Iteration 104, loss = 20101664324.83750153\n",
      "Iteration 105, loss = 20101660847.82878494\n",
      "Iteration 106, loss = 20101657378.80051422\n",
      "Iteration 107, loss = 20101653956.19692993\n",
      "Iteration 108, loss = 20101650496.02669144\n",
      "Iteration 109, loss = 20101647087.00003433\n",
      "Iteration 110, loss = 20101643670.88989639\n",
      "Iteration 111, loss = 20101640335.95315170\n",
      "Iteration 112, loss = 20101636927.27280045\n",
      "Iteration 113, loss = 20101633625.58200455\n",
      "Iteration 114, loss = 20101630276.89978409\n",
      "Iteration 115, loss = 20101627001.09269714\n",
      "Iteration 116, loss = 20101623783.87488937\n",
      "Iteration 117, loss = 20101620557.33744812\n",
      "Iteration 118, loss = 20101617428.63742065\n",
      "Iteration 119, loss = 20101614279.03272247\n",
      "Iteration 120, loss = 20101611199.82862473\n",
      "Iteration 121, loss = 20101608113.64757919\n",
      "Iteration 122, loss = 20101605032.43625641\n",
      "Iteration 123, loss = 20101602014.42624283\n",
      "Iteration 124, loss = 20101598943.99704361\n",
      "Iteration 125, loss = 20101595987.67610931\n",
      "Iteration 126, loss = 20101592978.26890945\n",
      "Iteration 127, loss = 20101590004.59444046\n",
      "Iteration 128, loss = 20101587030.77582550\n",
      "Iteration 129, loss = 20101584052.10272217\n",
      "Iteration 130, loss = 20101581110.46713638\n",
      "Iteration 131, loss = 20101578160.86793900\n",
      "Iteration 132, loss = 20101575209.06740952\n",
      "Iteration 133, loss = 20101572281.77922821\n",
      "Iteration 134, loss = 20101569366.76020050\n",
      "Iteration 135, loss = 20101566455.87285233\n",
      "Iteration 136, loss = 20101563546.49720001\n",
      "Iteration 137, loss = 20101560610.73999023\n",
      "Iteration 138, loss = 20101557741.64396667\n",
      "Iteration 139, loss = 20101554818.17038345\n",
      "Iteration 140, loss = 20101551942.73857880\n",
      "Iteration 141, loss = 20101549024.02385712\n",
      "Iteration 142, loss = 20101546182.50447845\n",
      "Iteration 143, loss = 20101543294.65858841\n",
      "Iteration 144, loss = 20101540437.29994965\n",
      "Iteration 145, loss = 20101537558.51800156\n",
      "Iteration 146, loss = 20101534692.10039520\n",
      "Iteration 147, loss = 20101531821.09748459\n",
      "Iteration 148, loss = 20101528985.51860046\n",
      "Iteration 149, loss = 20101526111.52795410\n",
      "Iteration 150, loss = 20101523279.43371201\n",
      "Iteration 151, loss = 20101520456.56680679\n",
      "Iteration 152, loss = 20101517580.97164154\n",
      "Iteration 153, loss = 20101514762.84667969\n",
      "Iteration 154, loss = 20101511943.77959824\n",
      "Iteration 155, loss = 20101509132.46837997\n",
      "Iteration 156, loss = 20101506297.09385681\n",
      "Iteration 157, loss = 20101503490.01340103\n",
      "Iteration 158, loss = 20101500704.18840790\n",
      "Iteration 159, loss = 20101497918.06286240\n",
      "Iteration 160, loss = 20101495146.88270187\n",
      "Iteration 161, loss = 20101492341.54951096\n",
      "Iteration 162, loss = 20101489589.12017441\n",
      "Iteration 163, loss = 20101486824.18987656\n",
      "Iteration 164, loss = 20101484081.53508377\n",
      "Iteration 165, loss = 20101481319.66785049\n",
      "Iteration 166, loss = 20101478560.25348282\n",
      "Iteration 167, loss = 20101475825.56868362\n",
      "Iteration 168, loss = 20101473116.28752899\n",
      "Iteration 169, loss = 20101470381.10253143\n",
      "Iteration 170, loss = 20101467660.60039139\n",
      "Iteration 171, loss = 20101464928.29351044\n",
      "Iteration 172, loss = 20101462233.61033630\n",
      "Iteration 173, loss = 20101459508.11033630\n",
      "Iteration 174, loss = 20101456811.09350967\n",
      "Iteration 175, loss = 20101454125.47596359\n",
      "Iteration 176, loss = 20101451442.85637283\n",
      "Iteration 177, loss = 20101448757.47260284\n",
      "Iteration 178, loss = 20101446065.02204132\n",
      "Iteration 179, loss = 20101443381.18442154\n",
      "Iteration 180, loss = 20101440679.02295685\n",
      "Iteration 181, loss = 20101438028.51256561\n",
      "Iteration 182, loss = 20101435351.47498703\n",
      "Iteration 183, loss = 20101432667.54348755\n",
      "Iteration 184, loss = 20101429983.38432693\n",
      "Iteration 185, loss = 20101427316.01800919\n",
      "Iteration 186, loss = 20101424642.65930939\n",
      "Iteration 187, loss = 20101421968.88788605\n",
      "Iteration 188, loss = 20101419290.24304581\n",
      "Iteration 189, loss = 20101416662.48777390\n",
      "Iteration 190, loss = 20101413982.62841415\n",
      "Iteration 191, loss = 20101411391.66920090\n",
      "Iteration 192, loss = 20101408727.95636749\n",
      "Iteration 193, loss = 20101406092.88904572\n",
      "Iteration 194, loss = 20101403493.45015717\n",
      "Iteration 195, loss = 20101400848.69359970\n",
      "Iteration 196, loss = 20101398267.81425858\n",
      "Iteration 197, loss = 20101395639.43311310\n",
      "Iteration 198, loss = 20101393033.79892731\n",
      "Iteration 199, loss = 20101390435.94057846\n",
      "Iteration 200, loss = 20101387848.64167023\n",
      "Iteration 201, loss = 20101385246.33355331\n",
      "Iteration 202, loss = 20101382678.30973816\n",
      "Iteration 203, loss = 20101380074.72499847\n",
      "Iteration 204, loss = 20101377494.80162430\n",
      "Iteration 205, loss = 20101374949.04690933\n",
      "Iteration 206, loss = 20101372343.09404755\n",
      "Iteration 207, loss = 20101369759.74737167\n",
      "Iteration 208, loss = 20101367218.91920090\n",
      "Iteration 209, loss = 20101364658.99481201\n",
      "Iteration 210, loss = 20101362108.95846176\n",
      "Iteration 211, loss = 20101359533.25356674\n",
      "Iteration 212, loss = 20101356961.77104187\n",
      "Iteration 213, loss = 20101354435.73141861\n",
      "Iteration 214, loss = 20101351887.46697235\n",
      "Iteration 215, loss = 20101349342.30003357\n",
      "Iteration 216, loss = 20101346796.85435104\n",
      "Iteration 217, loss = 20101344255.63502884\n",
      "Iteration 218, loss = 20101341729.11106110\n",
      "Iteration 219, loss = 20101339163.43982697\n",
      "Iteration 220, loss = 20101336664.05001450\n",
      "Iteration 221, loss = 20101334106.72751236\n",
      "Iteration 222, loss = 20101331596.71365356\n",
      "Iteration 223, loss = 20101329080.28334427\n",
      "Iteration 224, loss = 20101326542.62826920\n",
      "Iteration 225, loss = 20101324038.89601135\n",
      "Iteration 226, loss = 20101321507.71343613\n",
      "Iteration 227, loss = 20101319015.91329193\n",
      "Iteration 228, loss = 20101316485.83807373\n",
      "Iteration 229, loss = 20101313961.82550812\n",
      "Iteration 230, loss = 20101311452.04293060\n",
      "Iteration 231, loss = 20101308959.17277527\n",
      "Iteration 232, loss = 20101306437.14173889\n",
      "Iteration 233, loss = 20101303971.62487030\n",
      "Iteration 234, loss = 20101301456.15320206\n",
      "Iteration 235, loss = 20101298967.52477264\n",
      "Iteration 236, loss = 20101296464.11490631\n",
      "Iteration 237, loss = 20101293942.64416122\n",
      "Iteration 238, loss = 20101291461.62686539\n",
      "Iteration 239, loss = 20101288972.90201950\n",
      "Iteration 240, loss = 20101286466.08573532\n",
      "Iteration 241, loss = 20101283998.14302063\n",
      "Iteration 242, loss = 20101281508.12712479\n",
      "Iteration 243, loss = 20101278976.59891891\n",
      "Iteration 244, loss = 20101276510.65727234\n",
      "Iteration 245, loss = 20101274006.62653351\n",
      "Iteration 246, loss = 20101271516.86529541\n",
      "Iteration 247, loss = 20101268999.38082123\n",
      "Iteration 248, loss = 20101266526.78536987\n",
      "Iteration 249, loss = 20101264078.29751587\n",
      "Iteration 250, loss = 20101261557.93471146\n",
      "Iteration 251, loss = 20101259099.58219528\n",
      "Iteration 252, loss = 20101256601.40095520\n",
      "Iteration 253, loss = 20101254119.74251556\n",
      "Iteration 254, loss = 20101251666.82149506\n",
      "Iteration 255, loss = 20101249201.23165894\n",
      "Iteration 256, loss = 20101246721.72499847\n",
      "Iteration 257, loss = 20101244295.01541138\n",
      "Iteration 258, loss = 20101241798.24564743\n",
      "Iteration 259, loss = 20101239330.37070847\n",
      "Iteration 260, loss = 20101236885.73984528\n",
      "Iteration 261, loss = 20101234445.64995575\n",
      "Iteration 262, loss = 20101231977.03318787\n",
      "Iteration 263, loss = 20101229542.12318039\n",
      "Iteration 264, loss = 20101227071.14288330\n",
      "Iteration 265, loss = 20101224640.77681351\n",
      "Iteration 266, loss = 20101222179.65039062\n",
      "Iteration 267, loss = 20101219741.97736740\n",
      "Iteration 268, loss = 20101217306.84433365\n",
      "Iteration 269, loss = 20101214850.23553085\n",
      "Iteration 270, loss = 20101212428.02187729\n",
      "Iteration 271, loss = 20101209969.73834991\n",
      "Iteration 272, loss = 20101207535.93592453\n",
      "Iteration 273, loss = 20101205102.53742981\n",
      "Iteration 274, loss = 20101202675.59017181\n",
      "Iteration 275, loss = 20101200198.76771545\n",
      "Iteration 276, loss = 20101197799.67773438\n",
      "Iteration 277, loss = 20101195343.85567474\n",
      "Iteration 278, loss = 20101192925.58567429\n",
      "Iteration 279, loss = 20101190497.87421417\n",
      "Iteration 280, loss = 20101188085.71144104\n",
      "Iteration 281, loss = 20101185637.66827011\n",
      "Iteration 282, loss = 20101183213.54278183\n",
      "Iteration 283, loss = 20101180783.78536987\n",
      "Iteration 284, loss = 20101178373.44110870\n",
      "Iteration 285, loss = 20101175936.73256302\n",
      "Iteration 286, loss = 20101173514.01996231\n",
      "Iteration 287, loss = 20101171087.31858063\n",
      "Iteration 288, loss = 20101168677.25777817\n",
      "Iteration 289, loss = 20101166272.54776382\n",
      "Iteration 290, loss = 20101163846.63607025\n",
      "Iteration 291, loss = 20101161425.58731461\n",
      "Iteration 292, loss = 20101159037.43310165\n",
      "Iteration 293, loss = 20101156594.92855072\n",
      "Iteration 294, loss = 20101154215.92390060\n",
      "Iteration 295, loss = 20101151765.36688995\n",
      "Iteration 296, loss = 20101149352.38527679\n",
      "Iteration 297, loss = 20101146977.80067062\n",
      "Iteration 298, loss = 20101144556.80009460\n",
      "Iteration 299, loss = 20101142148.87081146\n",
      "Iteration 300, loss = 20101139755.97806549\n",
      "Iteration 301, loss = 20101137347.02968597\n",
      "Iteration 302, loss = 20101134941.99060822\n",
      "Iteration 303, loss = 20101132525.95148087\n",
      "Iteration 304, loss = 20101130154.38939667\n",
      "Iteration 305, loss = 20101127735.67919159\n",
      "Iteration 306, loss = 20101125326.15247345\n",
      "Iteration 307, loss = 20101122931.96862030\n",
      "Iteration 308, loss = 20101120563.88399506\n",
      "Iteration 309, loss = 20101118157.48775101\n",
      "Iteration 310, loss = 20101115745.85665894\n",
      "Iteration 311, loss = 20101113352.18250656\n",
      "Iteration 312, loss = 20101110979.15054703\n",
      "Iteration 313, loss = 20101108569.29697418\n",
      "Iteration 314, loss = 20101106181.57598877\n",
      "Iteration 315, loss = 20101103794.46058655\n",
      "Iteration 316, loss = 20101101414.92367554\n",
      "Iteration 317, loss = 20101099007.88479233\n",
      "Iteration 318, loss = 20101096597.17264557\n",
      "Iteration 319, loss = 20101094210.96011353\n",
      "Iteration 320, loss = 20101091816.63473129\n",
      "Iteration 321, loss = 20101089421.57352448\n",
      "Iteration 322, loss = 20101087040.63530731\n",
      "Iteration 323, loss = 20101084628.47552490\n",
      "Iteration 324, loss = 20101082250.06262970\n",
      "Iteration 325, loss = 20101079845.55873871\n",
      "Iteration 326, loss = 20101077486.18416977\n",
      "Iteration 327, loss = 20101075070.81824112\n",
      "Iteration 328, loss = 20101072714.58997726\n",
      "Iteration 329, loss = 20101070346.24298096\n",
      "Iteration 330, loss = 20101067935.62659454\n",
      "Iteration 331, loss = 20101065557.25521851\n",
      "Iteration 332, loss = 20101063178.58378983\n",
      "Iteration 333, loss = 20101060807.84202576\n",
      "Iteration 334, loss = 20101058441.40082169\n",
      "Iteration 335, loss = 20101056035.26596451\n",
      "Iteration 336, loss = 20101053689.91741180\n",
      "Iteration 337, loss = 20101051291.31661224\n",
      "Iteration 338, loss = 20101048931.27661896\n",
      "Iteration 339, loss = 20101046560.26112747\n",
      "Iteration 340, loss = 20101044186.77540588\n",
      "Iteration 341, loss = 20101041800.18601990\n",
      "Iteration 342, loss = 20101039429.91621399\n",
      "Iteration 343, loss = 20101037077.48473358\n",
      "Iteration 344, loss = 20101034676.91213226\n",
      "Iteration 345, loss = 20101032324.48993301\n",
      "Iteration 346, loss = 20101029947.52266312\n",
      "Iteration 347, loss = 20101027587.72761154\n",
      "Iteration 348, loss = 20101025217.23936844\n",
      "Iteration 349, loss = 20101022874.25522995\n",
      "Iteration 350, loss = 20101020503.42155075\n",
      "Iteration 351, loss = 20101018131.65008545\n",
      "Iteration 352, loss = 20101015769.88456345\n",
      "Iteration 353, loss = 20101013390.71718597\n",
      "Iteration 354, loss = 20101011028.00301361\n",
      "Iteration 355, loss = 20101008694.19623947\n",
      "Iteration 356, loss = 20101006331.77722549\n",
      "Iteration 357, loss = 20101003990.75913239\n",
      "Iteration 358, loss = 20101001616.07112122\n",
      "Iteration 359, loss = 20100999242.77151871\n",
      "Iteration 360, loss = 20100996897.84952545\n",
      "Iteration 361, loss = 20100994553.71998215\n",
      "Iteration 362, loss = 20100992164.38321304\n",
      "Iteration 363, loss = 20100989836.06032181\n",
      "Iteration 364, loss = 20100987476.33102798\n",
      "Iteration 365, loss = 20100985119.09885788\n",
      "Iteration 366, loss = 20100982755.40282822\n",
      "Iteration 367, loss = 20100980423.15710831\n",
      "Iteration 368, loss = 20100978064.58544540\n",
      "Iteration 369, loss = 20100975699.31296921\n",
      "Iteration 370, loss = 20100973334.83674240\n",
      "Iteration 371, loss = 20100971000.96801376\n",
      "Iteration 372, loss = 20100968650.51921463\n",
      "Iteration 373, loss = 20100966299.85478973\n",
      "Iteration 374, loss = 20100963931.95403290\n",
      "Iteration 375, loss = 20100961573.28221130\n",
      "Iteration 376, loss = 20100959252.80986023\n",
      "Iteration 377, loss = 20100956888.39964294\n",
      "Iteration 378, loss = 20100954548.70135117\n",
      "Iteration 379, loss = 20100952215.67058182\n",
      "Iteration 380, loss = 20100949847.53772736\n",
      "Iteration 381, loss = 20100947507.00311279\n",
      "Iteration 382, loss = 20100945174.90473938\n",
      "Iteration 383, loss = 20100942833.35766220\n",
      "Iteration 384, loss = 20100940473.15116501\n",
      "Iteration 385, loss = 20100938130.10745621\n",
      "Iteration 386, loss = 20100935784.50011063\n",
      "Iteration 387, loss = 20100933491.06623459\n",
      "Iteration 388, loss = 20100931120.11838150\n",
      "Iteration 389, loss = 20100928794.49114227\n",
      "Iteration 390, loss = 20100926444.60822678\n",
      "Iteration 391, loss = 20100924089.99404907\n",
      "Iteration 392, loss = 20100921763.68889999\n",
      "Iteration 393, loss = 20100919406.18343735\n",
      "Iteration 394, loss = 20100917107.79755020\n",
      "Iteration 395, loss = 20100914749.90591812\n",
      "Iteration 396, loss = 20100912428.17246246\n",
      "Iteration 397, loss = 20100910087.11236191\n",
      "Iteration 398, loss = 20100907739.69081879\n",
      "Iteration 399, loss = 20100905389.21450806\n",
      "Iteration 400, loss = 20100903074.72364807\n",
      "Iteration 401, loss = 20100900729.89245224\n",
      "Iteration 402, loss = 20100898403.55062485\n",
      "Iteration 403, loss = 20100896047.52863312\n",
      "Iteration 404, loss = 20100893732.63894272\n",
      "Iteration 405, loss = 20100891392.77330399\n",
      "Iteration 406, loss = 20100889071.55561447\n",
      "Iteration 407, loss = 20100886720.58222580\n",
      "Iteration 408, loss = 20100884392.31343079\n",
      "Iteration 409, loss = 20100882053.32333374\n",
      "Iteration 410, loss = 20100879720.36265945\n",
      "Iteration 411, loss = 20100877387.73690033\n",
      "Iteration 412, loss = 20100875067.95911789\n",
      "Iteration 413, loss = 20100872748.40081024\n",
      "Iteration 414, loss = 20100870402.79664230\n",
      "Iteration 415, loss = 20100868078.27159119\n",
      "Iteration 416, loss = 20100865745.31956100\n",
      "Iteration 417, loss = 20100863415.14449692\n",
      "Iteration 418, loss = 20100861079.73299026\n",
      "Iteration 419, loss = 20100858763.29048157\n",
      "Iteration 420, loss = 20100856449.76618576\n",
      "Iteration 421, loss = 20100854122.56116104\n",
      "Iteration 422, loss = 20100851771.24024963\n",
      "Iteration 423, loss = 20100849457.61581802\n",
      "Iteration 424, loss = 20100847148.76239395\n",
      "Iteration 425, loss = 20100844815.57374954\n",
      "Iteration 426, loss = 20100842482.14763260\n",
      "Iteration 427, loss = 20100840187.59738541\n",
      "Iteration 428, loss = 20100837849.55579758\n",
      "Iteration 429, loss = 20100835521.28776169\n",
      "Iteration 430, loss = 20100833215.89188004\n",
      "Iteration 431, loss = 20100830896.46438217\n",
      "Iteration 432, loss = 20100828551.97782516\n",
      "Iteration 433, loss = 20100826245.72572327\n",
      "Iteration 434, loss = 20100823932.01064301\n",
      "Iteration 435, loss = 20100821598.95534897\n",
      "Iteration 436, loss = 20100819302.02473831\n",
      "Iteration 437, loss = 20100816982.57595825\n",
      "Iteration 438, loss = 20100814659.20544052\n",
      "Iteration 439, loss = 20100812333.75868988\n",
      "Iteration 440, loss = 20100810027.64007568\n",
      "Iteration 441, loss = 20100807723.81491852\n",
      "Iteration 442, loss = 20100805386.62353897\n",
      "Iteration 443, loss = 20100803074.38008881\n",
      "Iteration 444, loss = 20100800768.40516663\n",
      "Iteration 445, loss = 20100798450.07552338\n",
      "Iteration 446, loss = 20100796139.07707596\n",
      "Iteration 447, loss = 20100793824.75448227\n",
      "Iteration 448, loss = 20100791524.72469330\n",
      "Iteration 449, loss = 20100789190.76737213\n",
      "Iteration 450, loss = 20100786860.80992126\n",
      "Iteration 451, loss = 20100784580.25648117\n",
      "Iteration 452, loss = 20100782265.23349380\n",
      "Iteration 453, loss = 20100779957.04769135\n",
      "Iteration 454, loss = 20100777635.96867371\n",
      "Iteration 455, loss = 20100775300.43370438\n",
      "Iteration 456, loss = 20100773012.10466766\n",
      "Iteration 457, loss = 20100770722.96241760\n",
      "Iteration 458, loss = 20100768378.95966721\n",
      "Iteration 459, loss = 20100766052.66423035\n",
      "Iteration 460, loss = 20100763758.52078247\n",
      "Iteration 461, loss = 20100761467.49349976\n",
      "Iteration 462, loss = 20100759140.87951279\n",
      "Iteration 463, loss = 20100756818.10012436\n",
      "Iteration 464, loss = 20100754519.61423111\n",
      "Iteration 465, loss = 20100752225.30504608\n",
      "Iteration 466, loss = 20100749899.65407944\n",
      "Iteration 467, loss = 20100747585.58952713\n",
      "Iteration 468, loss = 20100745278.50485611\n",
      "Iteration 469, loss = 20100742982.59975052\n",
      "Iteration 470, loss = 20100740665.56855011\n",
      "Iteration 471, loss = 20100738351.90443802\n",
      "Iteration 472, loss = 20100736052.93231964\n",
      "Iteration 473, loss = 20100733762.65467453\n",
      "Iteration 474, loss = 20100731435.14392471\n",
      "Iteration 475, loss = 20100729110.95541382\n",
      "Iteration 476, loss = 20100726824.31136322\n",
      "Iteration 477, loss = 20100724526.93266678\n",
      "Iteration 478, loss = 20100722236.46862030\n",
      "Iteration 479, loss = 20100719911.07506180\n",
      "Iteration 480, loss = 20100717608.37562561\n",
      "Iteration 481, loss = 20100715321.66836929\n",
      "Iteration 482, loss = 20100713008.01726151\n",
      "Iteration 483, loss = 20100710684.44893646\n",
      "Iteration 484, loss = 20100708412.77830505\n",
      "Iteration 485, loss = 20100706090.62025452\n",
      "Iteration 486, loss = 20100703779.68771744\n",
      "Iteration 487, loss = 20100701519.30582428\n",
      "Iteration 488, loss = 20100699201.64593887\n",
      "Iteration 489, loss = 20100696892.10644150\n",
      "Iteration 490, loss = 20100694593.57748413\n",
      "Iteration 491, loss = 20100692303.68767929\n",
      "Iteration 492, loss = 20100689985.41312408\n",
      "Iteration 493, loss = 20100687671.62537003\n",
      "Iteration 494, loss = 20100685389.71456146\n",
      "Iteration 495, loss = 20100683096.13055801\n",
      "Iteration 496, loss = 20100680777.17193604\n",
      "Iteration 497, loss = 20100678456.21918106\n",
      "Iteration 498, loss = 20100676185.13393021\n",
      "Iteration 499, loss = 20100673873.38267899\n",
      "Iteration 500, loss = 20100671568.70558167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19475706743.85572052\n",
      "Iteration 2, loss = 19475696369.38847351\n",
      "Iteration 3, loss = 19475685973.90608215\n",
      "Iteration 4, loss = 19475675615.21582794\n",
      "Iteration 5, loss = 19475665382.87048340\n",
      "Iteration 6, loss = 19475655231.34703827\n",
      "Iteration 7, loss = 19475644929.39596939\n",
      "Iteration 8, loss = 19475634896.50655746\n",
      "Iteration 9, loss = 19475624783.19981003\n",
      "Iteration 10, loss = 19475614637.13531113\n",
      "Iteration 11, loss = 19475604603.72130203\n",
      "Iteration 12, loss = 19475594396.57902908\n",
      "Iteration 13, loss = 19475584401.89714432\n",
      "Iteration 14, loss = 19475574209.93306732\n",
      "Iteration 15, loss = 19475564157.22604370\n",
      "Iteration 16, loss = 19475554095.08956909\n",
      "Iteration 17, loss = 19475543933.34563065\n",
      "Iteration 18, loss = 19475533739.46698761\n",
      "Iteration 19, loss = 19475523423.01473236\n",
      "Iteration 20, loss = 19475513059.39336014\n",
      "Iteration 21, loss = 19475502550.58423996\n",
      "Iteration 22, loss = 19475491750.63503265\n",
      "Iteration 23, loss = 19475480691.41619492\n",
      "Iteration 24, loss = 19475469500.49915695\n",
      "Iteration 25, loss = 19475457841.40828705\n",
      "Iteration 26, loss = 19475445953.46429062\n",
      "Iteration 27, loss = 19475433743.98726654\n",
      "Iteration 28, loss = 19475421412.82186508\n",
      "Iteration 29, loss = 19475408822.29357147\n",
      "Iteration 30, loss = 19475396292.45296860\n",
      "Iteration 31, loss = 19475383894.96974564\n",
      "Iteration 32, loss = 19475371654.44128036\n",
      "Iteration 33, loss = 19475359800.88262558\n",
      "Iteration 34, loss = 19475348075.63474655\n",
      "Iteration 35, loss = 19475336808.55270767\n",
      "Iteration 36, loss = 19475325694.11607742\n",
      "Iteration 37, loss = 19475314865.73331833\n",
      "Iteration 38, loss = 19475304006.13936234\n",
      "Iteration 39, loss = 19475293295.80541992\n",
      "Iteration 40, loss = 19475282636.70801544\n",
      "Iteration 41, loss = 19475272113.21730423\n",
      "Iteration 42, loss = 19475261767.86432266\n",
      "Iteration 43, loss = 19475251490.91736603\n",
      "Iteration 44, loss = 19475241422.77832413\n",
      "Iteration 45, loss = 19475231240.22434235\n",
      "Iteration 46, loss = 19475221323.44863510\n",
      "Iteration 47, loss = 19475211519.09565735\n",
      "Iteration 48, loss = 19475202018.96192932\n",
      "Iteration 49, loss = 19475192832.74487305\n",
      "Iteration 50, loss = 19475183988.64538574\n",
      "Iteration 51, loss = 19475175447.90150452\n",
      "Iteration 52, loss = 19475167052.28028870\n",
      "Iteration 53, loss = 19475158998.50844955\n",
      "Iteration 54, loss = 19475151129.02276993\n",
      "Iteration 55, loss = 19475143517.49077225\n",
      "Iteration 56, loss = 19475136038.71546936\n",
      "Iteration 57, loss = 19475128856.38228989\n",
      "Iteration 58, loss = 19475121853.38386917\n",
      "Iteration 59, loss = 19475115062.13623047\n",
      "Iteration 60, loss = 19475108479.99494553\n",
      "Iteration 61, loss = 19475102020.13427734\n",
      "Iteration 62, loss = 19475095648.00457382\n",
      "Iteration 63, loss = 19475089409.61961365\n",
      "Iteration 64, loss = 19475083268.01462555\n",
      "Iteration 65, loss = 19475077207.44095612\n",
      "Iteration 66, loss = 19475071272.82588577\n",
      "Iteration 67, loss = 19475065520.58888626\n",
      "Iteration 68, loss = 19475059886.13966751\n",
      "Iteration 69, loss = 19475054423.76375580\n",
      "Iteration 70, loss = 19475049105.09201050\n",
      "Iteration 71, loss = 19475043952.48933411\n",
      "Iteration 72, loss = 19475038947.02000809\n",
      "Iteration 73, loss = 19475034074.32731628\n",
      "Iteration 74, loss = 19475029327.05289841\n",
      "Iteration 75, loss = 19475024732.19429016\n",
      "Iteration 76, loss = 19475020132.97476578\n",
      "Iteration 77, loss = 19475015681.24460220\n",
      "Iteration 78, loss = 19475011196.05938339\n",
      "Iteration 79, loss = 19475006786.85543823\n",
      "Iteration 80, loss = 19475002399.80003738\n",
      "Iteration 81, loss = 19474998057.25585938\n",
      "Iteration 82, loss = 19474993759.84279633\n",
      "Iteration 83, loss = 19474989436.13935852\n",
      "Iteration 84, loss = 19474985219.84571457\n",
      "Iteration 85, loss = 19474980971.87643814\n",
      "Iteration 86, loss = 19474976754.45491409\n",
      "Iteration 87, loss = 19474972570.17309189\n",
      "Iteration 88, loss = 19474968372.90784836\n",
      "Iteration 89, loss = 19474964192.36636353\n",
      "Iteration 90, loss = 19474960028.38422012\n",
      "Iteration 91, loss = 19474955910.98450470\n",
      "Iteration 92, loss = 19474951796.05929565\n",
      "Iteration 93, loss = 19474947665.32600403\n",
      "Iteration 94, loss = 19474943549.26696777\n",
      "Iteration 95, loss = 19474939432.27573776\n",
      "Iteration 96, loss = 19474935291.40616989\n",
      "Iteration 97, loss = 19474931208.97479630\n",
      "Iteration 98, loss = 19474927056.49757385\n",
      "Iteration 99, loss = 19474922904.10997009\n",
      "Iteration 100, loss = 19474918774.28021622\n",
      "Iteration 101, loss = 19474914570.91619110\n",
      "Iteration 102, loss = 19474910451.74414444\n",
      "Iteration 103, loss = 19474906294.45919800\n",
      "Iteration 104, loss = 19474902180.33432770\n",
      "Iteration 105, loss = 19474898033.93299484\n",
      "Iteration 106, loss = 19474893902.66664886\n",
      "Iteration 107, loss = 19474889739.18292999\n",
      "Iteration 108, loss = 19474885580.10811234\n",
      "Iteration 109, loss = 19474881407.86878586\n",
      "Iteration 110, loss = 19474877218.62214279\n",
      "Iteration 111, loss = 19474873004.84370041\n",
      "Iteration 112, loss = 19474868746.00714493\n",
      "Iteration 113, loss = 19474864493.07319641\n",
      "Iteration 114, loss = 19474860205.77728653\n",
      "Iteration 115, loss = 19474855931.57375717\n",
      "Iteration 116, loss = 19474851659.58058548\n",
      "Iteration 117, loss = 19474847393.40112686\n",
      "Iteration 118, loss = 19474843130.75392532\n",
      "Iteration 119, loss = 19474838905.05603790\n",
      "Iteration 120, loss = 19474834725.55751419\n",
      "Iteration 121, loss = 19474830522.22884750\n",
      "Iteration 122, loss = 19474826364.20332718\n",
      "Iteration 123, loss = 19474822161.06039810\n",
      "Iteration 124, loss = 19474818050.84051895\n",
      "Iteration 125, loss = 19474813862.24599457\n",
      "Iteration 126, loss = 19474809760.55018997\n",
      "Iteration 127, loss = 19474805662.76096344\n",
      "Iteration 128, loss = 19474801548.07887650\n",
      "Iteration 129, loss = 19474797463.74154282\n",
      "Iteration 130, loss = 19474793367.16175842\n",
      "Iteration 131, loss = 19474789326.88098145\n",
      "Iteration 132, loss = 19474785235.35762024\n",
      "Iteration 133, loss = 19474781192.50815964\n",
      "Iteration 134, loss = 19474777142.86257553\n",
      "Iteration 135, loss = 19474773094.88329697\n",
      "Iteration 136, loss = 19474769106.10150528\n",
      "Iteration 137, loss = 19474765082.07752991\n",
      "Iteration 138, loss = 19474761043.39989853\n",
      "Iteration 139, loss = 19474757085.11624527\n",
      "Iteration 140, loss = 19474753085.56555176\n",
      "Iteration 141, loss = 19474749137.36258316\n",
      "Iteration 142, loss = 19474745175.15486145\n",
      "Iteration 143, loss = 19474741249.40024567\n",
      "Iteration 144, loss = 19474737280.65645981\n",
      "Iteration 145, loss = 19474733370.21129227\n",
      "Iteration 146, loss = 19474729463.59837723\n",
      "Iteration 147, loss = 19474725543.63896179\n",
      "Iteration 148, loss = 19474721667.42859650\n",
      "Iteration 149, loss = 19474717731.19105530\n",
      "Iteration 150, loss = 19474713861.24148560\n",
      "Iteration 151, loss = 19474709945.25374603\n",
      "Iteration 152, loss = 19474706039.97637177\n",
      "Iteration 153, loss = 19474702111.07361221\n",
      "Iteration 154, loss = 19474698244.12617493\n",
      "Iteration 155, loss = 19474694315.63128281\n",
      "Iteration 156, loss = 19474690463.94566727\n",
      "Iteration 157, loss = 19474686568.16800308\n",
      "Iteration 158, loss = 19474682710.82626724\n",
      "Iteration 159, loss = 19474678850.01865005\n",
      "Iteration 160, loss = 19474675001.67321777\n",
      "Iteration 161, loss = 19474671174.92144012\n",
      "Iteration 162, loss = 19474667317.31719971\n",
      "Iteration 163, loss = 19474663501.64143372\n",
      "Iteration 164, loss = 19474659646.31436539\n",
      "Iteration 165, loss = 19474655888.39870834\n",
      "Iteration 166, loss = 19474652035.38210297\n",
      "Iteration 167, loss = 19474648232.17950821\n",
      "Iteration 168, loss = 19474644412.51407623\n",
      "Iteration 169, loss = 19474640624.55995560\n",
      "Iteration 170, loss = 19474636851.50821686\n",
      "Iteration 171, loss = 19474633044.98418808\n",
      "Iteration 172, loss = 19474629293.86568451\n",
      "Iteration 173, loss = 19474625561.83079147\n",
      "Iteration 174, loss = 19474621796.89059830\n",
      "Iteration 175, loss = 19474618031.17749786\n",
      "Iteration 176, loss = 19474614309.58697891\n",
      "Iteration 177, loss = 19474610549.82308578\n",
      "Iteration 178, loss = 19474606835.34531403\n",
      "Iteration 179, loss = 19474603075.55681992\n",
      "Iteration 180, loss = 19474599350.37857819\n",
      "Iteration 181, loss = 19474595633.19368744\n",
      "Iteration 182, loss = 19474591859.45321655\n",
      "Iteration 183, loss = 19474588172.29933167\n",
      "Iteration 184, loss = 19474584431.61530304\n",
      "Iteration 185, loss = 19474580701.70936584\n",
      "Iteration 186, loss = 19474576997.96312714\n",
      "Iteration 187, loss = 19474573271.60842896\n",
      "Iteration 188, loss = 19474569554.65916061\n",
      "Iteration 189, loss = 19474565815.32887650\n",
      "Iteration 190, loss = 19474562091.64544678\n",
      "Iteration 191, loss = 19474558404.06131363\n",
      "Iteration 192, loss = 19474554698.77720261\n",
      "Iteration 193, loss = 19474551002.78134537\n",
      "Iteration 194, loss = 19474547301.62946701\n",
      "Iteration 195, loss = 19474543656.81500626\n",
      "Iteration 196, loss = 19474539903.54668808\n",
      "Iteration 197, loss = 19474536239.00421906\n",
      "Iteration 198, loss = 19474532548.81071091\n",
      "Iteration 199, loss = 19474528888.44930267\n",
      "Iteration 200, loss = 19474525155.02822113\n",
      "Iteration 201, loss = 19474521433.64506912\n",
      "Iteration 202, loss = 19474517792.79171753\n",
      "Iteration 203, loss = 19474514051.02653503\n",
      "Iteration 204, loss = 19474510366.56202316\n",
      "Iteration 205, loss = 19474506677.06143951\n",
      "Iteration 206, loss = 19474503040.53787613\n",
      "Iteration 207, loss = 19474499371.92384338\n",
      "Iteration 208, loss = 19474495692.11130905\n",
      "Iteration 209, loss = 19474492070.74202347\n",
      "Iteration 210, loss = 19474488396.24864197\n",
      "Iteration 211, loss = 19474484691.98492050\n",
      "Iteration 212, loss = 19474481064.35259247\n",
      "Iteration 213, loss = 19474477417.70157623\n",
      "Iteration 214, loss = 19474473765.49811554\n",
      "Iteration 215, loss = 19474470090.69070435\n",
      "Iteration 216, loss = 19474466447.12804794\n",
      "Iteration 217, loss = 19474462797.97859573\n",
      "Iteration 218, loss = 19474459170.07415009\n",
      "Iteration 219, loss = 19474455499.74018860\n",
      "Iteration 220, loss = 19474451880.96484375\n",
      "Iteration 221, loss = 19474448250.46504593\n",
      "Iteration 222, loss = 19474444604.21791840\n",
      "Iteration 223, loss = 19474440972.43086243\n",
      "Iteration 224, loss = 19474437353.70782089\n",
      "Iteration 225, loss = 19474433756.98469162\n",
      "Iteration 226, loss = 19474430105.30795670\n",
      "Iteration 227, loss = 19474426479.19548416\n",
      "Iteration 228, loss = 19474422878.30416107\n",
      "Iteration 229, loss = 19474419244.59720612\n",
      "Iteration 230, loss = 19474415635.80213547\n",
      "Iteration 231, loss = 19474412034.29606628\n",
      "Iteration 232, loss = 19474408386.68511581\n",
      "Iteration 233, loss = 19474404789.63553238\n",
      "Iteration 234, loss = 19474401173.86729813\n",
      "Iteration 235, loss = 19474397553.84092331\n",
      "Iteration 236, loss = 19474393932.35964966\n",
      "Iteration 237, loss = 19474390320.04786301\n",
      "Iteration 238, loss = 19474386691.73566437\n",
      "Iteration 239, loss = 19474383091.33890533\n",
      "Iteration 240, loss = 19474379463.23190308\n",
      "Iteration 241, loss = 19474375880.31272125\n",
      "Iteration 242, loss = 19474372254.87779236\n",
      "Iteration 243, loss = 19474368649.31471252\n",
      "Iteration 244, loss = 19474365070.96047974\n",
      "Iteration 245, loss = 19474361468.96226883\n",
      "Iteration 246, loss = 19474357863.92341995\n",
      "Iteration 247, loss = 19474354280.63645172\n",
      "Iteration 248, loss = 19474350690.38710022\n",
      "Iteration 249, loss = 19474347097.24173737\n",
      "Iteration 250, loss = 19474343506.18439102\n",
      "Iteration 251, loss = 19474339935.57671738\n",
      "Iteration 252, loss = 19474336294.51758957\n",
      "Iteration 253, loss = 19474332727.98481750\n",
      "Iteration 254, loss = 19474329149.41315842\n",
      "Iteration 255, loss = 19474325559.05889893\n",
      "Iteration 256, loss = 19474321928.92707062\n",
      "Iteration 257, loss = 19474318339.16831589\n",
      "Iteration 258, loss = 19474314758.45861816\n",
      "Iteration 259, loss = 19474311123.28075790\n",
      "Iteration 260, loss = 19474307557.00355148\n",
      "Iteration 261, loss = 19474303982.82244492\n",
      "Iteration 262, loss = 19474300332.98549271\n",
      "Iteration 263, loss = 19474296775.95171738\n",
      "Iteration 264, loss = 19474293167.61811066\n",
      "Iteration 265, loss = 19474289615.12978745\n",
      "Iteration 266, loss = 19474286019.33685684\n",
      "Iteration 267, loss = 19474282434.49967957\n",
      "Iteration 268, loss = 19474278877.18719101\n",
      "Iteration 269, loss = 19474275301.58085251\n",
      "Iteration 270, loss = 19474271685.94330215\n",
      "Iteration 271, loss = 19474268107.28984451\n",
      "Iteration 272, loss = 19474264461.54092407\n",
      "Iteration 273, loss = 19474260857.39994812\n",
      "Iteration 274, loss = 19474257220.74287796\n",
      "Iteration 275, loss = 19474253586.06082535\n",
      "Iteration 276, loss = 19474249919.57880783\n",
      "Iteration 277, loss = 19474246237.16339874\n",
      "Iteration 278, loss = 19474242502.59663010\n",
      "Iteration 279, loss = 19474238789.76418686\n",
      "Iteration 280, loss = 19474234983.12045670\n",
      "Iteration 281, loss = 19474231175.34206009\n",
      "Iteration 282, loss = 19474227305.69396973\n",
      "Iteration 283, loss = 19474223461.11375809\n",
      "Iteration 284, loss = 19474219481.53769302\n",
      "Iteration 285, loss = 19474215531.89714813\n",
      "Iteration 286, loss = 19474211507.52175903\n",
      "Iteration 287, loss = 19474207494.22200775\n",
      "Iteration 288, loss = 19474203452.22955322\n",
      "Iteration 289, loss = 19474199406.42714310\n",
      "Iteration 290, loss = 19474195365.04742813\n",
      "Iteration 291, loss = 19474191314.83373260\n",
      "Iteration 292, loss = 19474187275.04593277\n",
      "Iteration 293, loss = 19474183250.40117264\n",
      "Iteration 294, loss = 19474179257.96766281\n",
      "Iteration 295, loss = 19474175241.01778412\n",
      "Iteration 296, loss = 19474171294.28922653\n",
      "Iteration 297, loss = 19474167263.47933197\n",
      "Iteration 298, loss = 19474163343.58984756\n",
      "Iteration 299, loss = 19474159366.16947174\n",
      "Iteration 300, loss = 19474155403.65594864\n",
      "Iteration 301, loss = 19474151438.22404861\n",
      "Iteration 302, loss = 19474147494.04569244\n",
      "Iteration 303, loss = 19474143597.76129913\n",
      "Iteration 304, loss = 19474139659.56703186\n",
      "Iteration 305, loss = 19474135775.68549347\n",
      "Iteration 306, loss = 19474131882.80580139\n",
      "Iteration 307, loss = 19474128012.88846588\n",
      "Iteration 308, loss = 19474124134.38395691\n",
      "Iteration 309, loss = 19474120288.73432159\n",
      "Iteration 310, loss = 19474116409.78263092\n",
      "Iteration 311, loss = 19474112544.59937286\n",
      "Iteration 312, loss = 19474108675.10533905\n",
      "Iteration 313, loss = 19474104847.15697479\n",
      "Iteration 314, loss = 19474100996.62911606\n",
      "Iteration 315, loss = 19474097186.90942001\n",
      "Iteration 316, loss = 19474093347.18836975\n",
      "Iteration 317, loss = 19474089500.56357193\n",
      "Iteration 318, loss = 19474085711.15895081\n",
      "Iteration 319, loss = 19474081888.35442352\n",
      "Iteration 320, loss = 19474078089.34535980\n",
      "Iteration 321, loss = 19474074272.17251205\n",
      "Iteration 322, loss = 19474070496.21219254\n",
      "Iteration 323, loss = 19474066708.74023819\n",
      "Iteration 324, loss = 19474062933.33580780\n",
      "Iteration 325, loss = 19474059140.97688293\n",
      "Iteration 326, loss = 19474055355.32800674\n",
      "Iteration 327, loss = 19474051601.57903671\n",
      "Iteration 328, loss = 19474047835.04851532\n",
      "Iteration 329, loss = 19474044079.08772278\n",
      "Iteration 330, loss = 19474040349.89730072\n",
      "Iteration 331, loss = 19474036586.26479721\n",
      "Iteration 332, loss = 19474032831.89146042\n",
      "Iteration 333, loss = 19474029124.45076752\n",
      "Iteration 334, loss = 19474025345.52910614\n",
      "Iteration 335, loss = 19474021593.31836319\n",
      "Iteration 336, loss = 19474017869.68198013\n",
      "Iteration 337, loss = 19474014135.43024063\n",
      "Iteration 338, loss = 19474010385.82587051\n",
      "Iteration 339, loss = 19474006651.11191940\n",
      "Iteration 340, loss = 19474002908.00586700\n",
      "Iteration 341, loss = 19473999197.26184082\n",
      "Iteration 342, loss = 19473995501.51002502\n",
      "Iteration 343, loss = 19473991764.27825928\n",
      "Iteration 344, loss = 19473988066.41436386\n",
      "Iteration 345, loss = 19473984329.52425003\n",
      "Iteration 346, loss = 19473980612.65777588\n",
      "Iteration 347, loss = 19473976900.09003067\n",
      "Iteration 348, loss = 19473973158.59832001\n",
      "Iteration 349, loss = 19473969453.77888107\n",
      "Iteration 350, loss = 19473965745.08729172\n",
      "Iteration 351, loss = 19473962044.82735825\n",
      "Iteration 352, loss = 19473958328.05121231\n",
      "Iteration 353, loss = 19473954655.02716446\n",
      "Iteration 354, loss = 19473950951.12774277\n",
      "Iteration 355, loss = 19473947250.95248795\n",
      "Iteration 356, loss = 19473943571.02396393\n",
      "Iteration 357, loss = 19473939876.46240616\n",
      "Iteration 358, loss = 19473936196.26274490\n",
      "Iteration 359, loss = 19473932506.86887741\n",
      "Iteration 360, loss = 19473928817.38123703\n",
      "Iteration 361, loss = 19473925151.19189453\n",
      "Iteration 362, loss = 19473921474.52631760\n",
      "Iteration 363, loss = 19473917771.38892746\n",
      "Iteration 364, loss = 19473914072.99959183\n",
      "Iteration 365, loss = 19473910370.79854202\n",
      "Iteration 366, loss = 19473906722.60131073\n",
      "Iteration 367, loss = 19473903041.17353821\n",
      "Iteration 368, loss = 19473899367.72104263\n",
      "Iteration 369, loss = 19473895651.43105698\n",
      "Iteration 370, loss = 19473891962.28303146\n",
      "Iteration 371, loss = 19473888311.48054886\n",
      "Iteration 372, loss = 19473884623.36256790\n",
      "Iteration 373, loss = 19473880913.51919556\n",
      "Iteration 374, loss = 19473877265.54035950\n",
      "Iteration 375, loss = 19473873549.71187592\n",
      "Iteration 376, loss = 19473869856.67963028\n",
      "Iteration 377, loss = 19473866222.00754929\n",
      "Iteration 378, loss = 19473862538.71107864\n",
      "Iteration 379, loss = 19473858875.46669769\n",
      "Iteration 380, loss = 19473855206.34938049\n",
      "Iteration 381, loss = 19473851536.90268707\n",
      "Iteration 382, loss = 19473847865.66125107\n",
      "Iteration 383, loss = 19473844208.51554108\n",
      "Iteration 384, loss = 19473840564.07352066\n",
      "Iteration 385, loss = 19473836884.77464294\n",
      "Iteration 386, loss = 19473833224.35746765\n",
      "Iteration 387, loss = 19473829583.30606842\n",
      "Iteration 388, loss = 19473825893.12992859\n",
      "Iteration 389, loss = 19473822260.64426804\n",
      "Iteration 390, loss = 19473818606.09326172\n",
      "Iteration 391, loss = 19473814954.85800171\n",
      "Iteration 392, loss = 19473811314.27374268\n",
      "Iteration 393, loss = 19473807633.79745102\n",
      "Iteration 394, loss = 19473804028.01450348\n",
      "Iteration 395, loss = 19473800382.42772293\n",
      "Iteration 396, loss = 19473796711.80070114\n",
      "Iteration 397, loss = 19473793064.19192886\n",
      "Iteration 398, loss = 19473789435.70811462\n",
      "Iteration 399, loss = 19473785790.98263550\n",
      "Iteration 400, loss = 19473782136.02608490\n",
      "Iteration 401, loss = 19473778497.98754120\n",
      "Iteration 402, loss = 19473774879.01345825\n",
      "Iteration 403, loss = 19473771212.48765564\n",
      "Iteration 404, loss = 19473767591.15082550\n",
      "Iteration 405, loss = 19473763940.82142258\n",
      "Iteration 406, loss = 19473760334.34821320\n",
      "Iteration 407, loss = 19473756686.21389771\n",
      "Iteration 408, loss = 19473753072.00646591\n",
      "Iteration 409, loss = 19473749461.70652390\n",
      "Iteration 410, loss = 19473745834.58145905\n",
      "Iteration 411, loss = 19473742232.10606766\n",
      "Iteration 412, loss = 19473738599.50197983\n",
      "Iteration 413, loss = 19473734981.88982773\n",
      "Iteration 414, loss = 19473731351.40526199\n",
      "Iteration 415, loss = 19473727703.27092743\n",
      "Iteration 416, loss = 19473724072.22377014\n",
      "Iteration 417, loss = 19473720446.06302261\n",
      "Iteration 418, loss = 19473716816.67284012\n",
      "Iteration 419, loss = 19473713172.11716080\n",
      "Iteration 420, loss = 19473709524.11102295\n",
      "Iteration 421, loss = 19473705903.42633820\n",
      "Iteration 422, loss = 19473702247.98183823\n",
      "Iteration 423, loss = 19473698595.91706467\n",
      "Iteration 424, loss = 19473694979.02704239\n",
      "Iteration 425, loss = 19473691299.89617538\n",
      "Iteration 426, loss = 19473687705.63489532\n",
      "Iteration 427, loss = 19473684064.88632584\n",
      "Iteration 428, loss = 19473680431.30311966\n",
      "Iteration 429, loss = 19473676800.82134247\n",
      "Iteration 430, loss = 19473673178.03351974\n",
      "Iteration 431, loss = 19473669545.96649551\n",
      "Iteration 432, loss = 19473665928.66693115\n",
      "Iteration 433, loss = 19473662283.69689178\n",
      "Iteration 434, loss = 19473658681.21343613\n",
      "Iteration 435, loss = 19473655066.70713043\n",
      "Iteration 436, loss = 19473651447.40114212\n",
      "Iteration 437, loss = 19473647880.39385605\n",
      "Iteration 438, loss = 19473644236.72917175\n",
      "Iteration 439, loss = 19473640649.20257950\n",
      "Iteration 440, loss = 19473637079.64899063\n",
      "Iteration 441, loss = 19473633435.05146408\n",
      "Iteration 442, loss = 19473629852.04525757\n",
      "Iteration 443, loss = 19473626230.63121033\n",
      "Iteration 444, loss = 19473622614.03576660\n",
      "Iteration 445, loss = 19473618985.02891541\n",
      "Iteration 446, loss = 19473615398.87806702\n",
      "Iteration 447, loss = 19473611816.24253845\n",
      "Iteration 448, loss = 19473608197.03349686\n",
      "Iteration 449, loss = 19473604581.11514282\n",
      "Iteration 450, loss = 19473600991.17060471\n",
      "Iteration 451, loss = 19473597399.09196854\n",
      "Iteration 452, loss = 19473593791.05260086\n",
      "Iteration 453, loss = 19473590177.49671173\n",
      "Iteration 454, loss = 19473586571.33818436\n",
      "Iteration 455, loss = 19473582928.71355820\n",
      "Iteration 456, loss = 19473579347.62232208\n",
      "Iteration 457, loss = 19473575716.11565781\n",
      "Iteration 458, loss = 19473572088.08903122\n",
      "Iteration 459, loss = 19473568479.16038513\n",
      "Iteration 460, loss = 19473564868.66960907\n",
      "Iteration 461, loss = 19473561254.18294907\n",
      "Iteration 462, loss = 19473557665.01884842\n",
      "Iteration 463, loss = 19473554086.72109985\n",
      "Iteration 464, loss = 19473550496.53039932\n",
      "Iteration 465, loss = 19473546911.21945572\n",
      "Iteration 466, loss = 19473543334.65622330\n",
      "Iteration 467, loss = 19473539723.74870300\n",
      "Iteration 468, loss = 19473536162.73392868\n",
      "Iteration 469, loss = 19473532616.02344131\n",
      "Iteration 470, loss = 19473528998.83534622\n",
      "Iteration 471, loss = 19473525442.15287018\n",
      "Iteration 472, loss = 19473521864.90932465\n",
      "Iteration 473, loss = 19473518279.14421463\n",
      "Iteration 474, loss = 19473514703.20374680\n",
      "Iteration 475, loss = 19473511105.75894547\n",
      "Iteration 476, loss = 19473507525.36558914\n",
      "Iteration 477, loss = 19473503946.60037994\n",
      "Iteration 478, loss = 19473500347.05208969\n",
      "Iteration 479, loss = 19473496777.49603653\n",
      "Iteration 480, loss = 19473493204.87803650\n",
      "Iteration 481, loss = 19473489603.40975189\n",
      "Iteration 482, loss = 19473486034.33518600\n",
      "Iteration 483, loss = 19473482420.28996658\n",
      "Iteration 484, loss = 19473478831.50517273\n",
      "Iteration 485, loss = 19473475230.18776703\n",
      "Iteration 486, loss = 19473471660.27810669\n",
      "Iteration 487, loss = 19473468056.19035339\n",
      "Iteration 488, loss = 19473464486.33118820\n",
      "Iteration 489, loss = 19473460924.48578262\n",
      "Iteration 490, loss = 19473457349.75768280\n",
      "Iteration 491, loss = 19473453750.73811722\n",
      "Iteration 492, loss = 19473450200.83015442\n",
      "Iteration 493, loss = 19473446623.67942047\n",
      "Iteration 494, loss = 19473443061.87844849\n",
      "Iteration 495, loss = 19473439449.23029327\n",
      "Iteration 496, loss = 19473435880.86215591\n",
      "Iteration 497, loss = 19473432349.85420609\n",
      "Iteration 498, loss = 19473428742.20343781\n",
      "Iteration 499, loss = 19473425180.77254486\n",
      "Iteration 500, loss = 19473421607.10512543\n",
      "Iteration 1, loss = 19652868488.14599228\n",
      "Iteration 2, loss = 19652858082.70090485\n",
      "Iteration 3, loss = 19652847756.87211609\n",
      "Iteration 4, loss = 19652837363.74806595\n",
      "Iteration 5, loss = 19652826690.59812164\n",
      "Iteration 6, loss = 19652816231.76464462\n",
      "Iteration 7, loss = 19652805499.63628769\n",
      "Iteration 8, loss = 19652794715.57675171\n",
      "Iteration 9, loss = 19652783808.65917206\n",
      "Iteration 10, loss = 19652772999.25342560\n",
      "Iteration 11, loss = 19652761977.21979523\n",
      "Iteration 12, loss = 19652751072.36740112\n",
      "Iteration 13, loss = 19652740096.78674316\n",
      "Iteration 14, loss = 19652729185.63740540\n",
      "Iteration 15, loss = 19652718426.35997009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 19652707709.49091721\n",
      "Iteration 17, loss = 19652697072.23963165\n",
      "Iteration 18, loss = 19652686568.00749207\n",
      "Iteration 19, loss = 19652676059.60143661\n",
      "Iteration 20, loss = 19652665644.84743881\n",
      "Iteration 21, loss = 19652655193.42883301\n",
      "Iteration 22, loss = 19652644825.52213669\n",
      "Iteration 23, loss = 19652634354.37172318\n",
      "Iteration 24, loss = 19652623929.52309418\n",
      "Iteration 25, loss = 19652613495.07267380\n",
      "Iteration 26, loss = 19652603010.59132385\n",
      "Iteration 27, loss = 19652592670.99920654\n",
      "Iteration 28, loss = 19652582063.44359589\n",
      "Iteration 29, loss = 19652571438.43697357\n",
      "Iteration 30, loss = 19652560722.42948151\n",
      "Iteration 31, loss = 19652549845.16870880\n",
      "Iteration 32, loss = 19652538891.31699371\n",
      "Iteration 33, loss = 19652527906.76652527\n",
      "Iteration 34, loss = 19652517025.24430847\n",
      "Iteration 35, loss = 19652506178.31415176\n",
      "Iteration 36, loss = 19652495425.72109985\n",
      "Iteration 37, loss = 19652484660.49523544\n",
      "Iteration 38, loss = 19652473917.26577759\n",
      "Iteration 39, loss = 19652463303.29477310\n",
      "Iteration 40, loss = 19652452626.23770523\n",
      "Iteration 41, loss = 19652441994.45047379\n",
      "Iteration 42, loss = 19652431492.23573685\n",
      "Iteration 43, loss = 19652421119.11237717\n",
      "Iteration 44, loss = 19652410844.53672791\n",
      "Iteration 45, loss = 19652400728.80244827\n",
      "Iteration 46, loss = 19652390613.47242737\n",
      "Iteration 47, loss = 19652380583.24545288\n",
      "Iteration 48, loss = 19652370646.43430710\n",
      "Iteration 49, loss = 19652360827.56700134\n",
      "Iteration 50, loss = 19652351082.40646744\n",
      "Iteration 51, loss = 19652341410.22814178\n",
      "Iteration 52, loss = 19652331868.59102249\n",
      "Iteration 53, loss = 19652322201.21984863\n",
      "Iteration 54, loss = 19652312864.38167953\n",
      "Iteration 55, loss = 19652303423.67279816\n",
      "Iteration 56, loss = 19652294104.76291656\n",
      "Iteration 57, loss = 19652284745.72126007\n",
      "Iteration 58, loss = 19652275108.31930161\n",
      "Iteration 59, loss = 19652265815.22645187\n",
      "Iteration 60, loss = 19652256502.36348343\n",
      "Iteration 61, loss = 19652247606.75050354\n",
      "Iteration 62, loss = 19652239004.48971939\n",
      "Iteration 63, loss = 19652230712.60274124\n",
      "Iteration 64, loss = 19652222828.43305588\n",
      "Iteration 65, loss = 19652215432.99055099\n",
      "Iteration 66, loss = 19652208318.15211487\n",
      "Iteration 67, loss = 19652201545.13678360\n",
      "Iteration 68, loss = 19652194866.21788788\n",
      "Iteration 69, loss = 19652188312.75930405\n",
      "Iteration 70, loss = 19652181934.54195404\n",
      "Iteration 71, loss = 19652175638.82142639\n",
      "Iteration 72, loss = 19652169453.44909286\n",
      "Iteration 73, loss = 19652163262.85542297\n",
      "Iteration 74, loss = 19652157094.33539581\n",
      "Iteration 75, loss = 19652151030.36288834\n",
      "Iteration 76, loss = 19652144977.64809036\n",
      "Iteration 77, loss = 19652138962.21814728\n",
      "Iteration 78, loss = 19652132961.29064560\n",
      "Iteration 79, loss = 19652126947.63768005\n",
      "Iteration 80, loss = 19652121036.04629135\n",
      "Iteration 81, loss = 19652115076.19605637\n",
      "Iteration 82, loss = 19652109156.85887146\n",
      "Iteration 83, loss = 19652103356.27924347\n",
      "Iteration 84, loss = 19652097576.19688416\n",
      "Iteration 85, loss = 19652091875.65240479\n",
      "Iteration 86, loss = 19652086264.09424591\n",
      "Iteration 87, loss = 19652080598.28590775\n",
      "Iteration 88, loss = 19652075017.39388275\n",
      "Iteration 89, loss = 19652069460.50698471\n",
      "Iteration 90, loss = 19652063953.35542679\n",
      "Iteration 91, loss = 19652058410.55726242\n",
      "Iteration 92, loss = 19652052946.21988678\n",
      "Iteration 93, loss = 19652047464.70891571\n",
      "Iteration 94, loss = 19652042000.75946808\n",
      "Iteration 95, loss = 19652036479.10530472\n",
      "Iteration 96, loss = 19652031050.25548172\n",
      "Iteration 97, loss = 19652025526.78148651\n",
      "Iteration 98, loss = 19652020100.98624420\n",
      "Iteration 99, loss = 19652014628.99038696\n",
      "Iteration 100, loss = 19652009269.03829956\n",
      "Iteration 101, loss = 19652003905.86575699\n",
      "Iteration 102, loss = 19651998561.82822037\n",
      "Iteration 103, loss = 19651993257.63581848\n",
      "Iteration 104, loss = 19651987913.08675766\n",
      "Iteration 105, loss = 19651982601.68507767\n",
      "Iteration 106, loss = 19651977201.23836517\n",
      "Iteration 107, loss = 19651971906.04674149\n",
      "Iteration 108, loss = 19651966507.81940460\n",
      "Iteration 109, loss = 19651961069.04253769\n",
      "Iteration 110, loss = 19651955715.43318176\n",
      "Iteration 111, loss = 19651950380.58407211\n",
      "Iteration 112, loss = 19651945039.44760513\n",
      "Iteration 113, loss = 19651939759.25445557\n",
      "Iteration 114, loss = 19651934477.71840668\n",
      "Iteration 115, loss = 19651929155.45797348\n",
      "Iteration 116, loss = 19651923859.91429901\n",
      "Iteration 117, loss = 19651918522.89785004\n",
      "Iteration 118, loss = 19651913256.26754761\n",
      "Iteration 119, loss = 19651908041.82669449\n",
      "Iteration 120, loss = 19651902722.31653976\n",
      "Iteration 121, loss = 19651897460.28194809\n",
      "Iteration 122, loss = 19651892191.59408951\n",
      "Iteration 123, loss = 19651886965.09275055\n",
      "Iteration 124, loss = 19651881803.69555283\n",
      "Iteration 125, loss = 19651876520.37605667\n",
      "Iteration 126, loss = 19651871340.44759750\n",
      "Iteration 127, loss = 19651866110.46480179\n",
      "Iteration 128, loss = 19651860937.10795593\n",
      "Iteration 129, loss = 19651855807.08083344\n",
      "Iteration 130, loss = 19651850620.65041733\n",
      "Iteration 131, loss = 19651845438.09814453\n",
      "Iteration 132, loss = 19651840332.40952682\n",
      "Iteration 133, loss = 19651835173.57661057\n",
      "Iteration 134, loss = 19651830029.00782776\n",
      "Iteration 135, loss = 19651824947.78484344\n",
      "Iteration 136, loss = 19651819814.59331512\n",
      "Iteration 137, loss = 19651814740.05345917\n",
      "Iteration 138, loss = 19651809639.74485779\n",
      "Iteration 139, loss = 19651804480.73055649\n",
      "Iteration 140, loss = 19651799359.87815094\n",
      "Iteration 141, loss = 19651794273.80493164\n",
      "Iteration 142, loss = 19651789159.32773590\n",
      "Iteration 143, loss = 19651784118.52615738\n",
      "Iteration 144, loss = 19651779080.12069702\n",
      "Iteration 145, loss = 19651774062.04956818\n",
      "Iteration 146, loss = 19651768983.06178284\n",
      "Iteration 147, loss = 19651764019.63204193\n",
      "Iteration 148, loss = 19651759018.49838638\n",
      "Iteration 149, loss = 19651754042.14892578\n",
      "Iteration 150, loss = 19651749063.49477768\n",
      "Iteration 151, loss = 19651744075.45130157\n",
      "Iteration 152, loss = 19651739054.74974823\n",
      "Iteration 153, loss = 19651734145.53925705\n",
      "Iteration 154, loss = 19651729183.76512527\n",
      "Iteration 155, loss = 19651724239.98038864\n",
      "Iteration 156, loss = 19651719286.83209991\n",
      "Iteration 157, loss = 19651714320.50599289\n",
      "Iteration 158, loss = 19651709466.16191864\n",
      "Iteration 159, loss = 19651704520.34637070\n",
      "Iteration 160, loss = 19651699566.55060196\n",
      "Iteration 161, loss = 19651694715.03456879\n",
      "Iteration 162, loss = 19651689817.80747604\n",
      "Iteration 163, loss = 19651684863.80757523\n",
      "Iteration 164, loss = 19651680001.18303299\n",
      "Iteration 165, loss = 19651675145.09693527\n",
      "Iteration 166, loss = 19651670257.56425095\n",
      "Iteration 167, loss = 19651665363.91083145\n",
      "Iteration 168, loss = 19651660496.16212845\n",
      "Iteration 169, loss = 19651655608.03445053\n",
      "Iteration 170, loss = 19651650775.65887833\n",
      "Iteration 171, loss = 19651645917.91509628\n",
      "Iteration 172, loss = 19651641086.88179016\n",
      "Iteration 173, loss = 19651636221.32046127\n",
      "Iteration 174, loss = 19651631384.97263336\n",
      "Iteration 175, loss = 19651626511.44830322\n",
      "Iteration 176, loss = 19651621696.77038956\n",
      "Iteration 177, loss = 19651616813.30253601\n",
      "Iteration 178, loss = 19651611988.88454056\n",
      "Iteration 179, loss = 19651607119.74092484\n",
      "Iteration 180, loss = 19651602259.15032196\n",
      "Iteration 181, loss = 19651597476.15120697\n",
      "Iteration 182, loss = 19651592642.98174286\n",
      "Iteration 183, loss = 19651587880.47914886\n",
      "Iteration 184, loss = 19651583046.71973038\n",
      "Iteration 185, loss = 19651578234.79747391\n",
      "Iteration 186, loss = 19651573416.66556931\n",
      "Iteration 187, loss = 19651568688.24027634\n",
      "Iteration 188, loss = 19651563908.50142670\n",
      "Iteration 189, loss = 19651559102.80995941\n",
      "Iteration 190, loss = 19651554328.42777634\n",
      "Iteration 191, loss = 19651549526.77159882\n",
      "Iteration 192, loss = 19651544715.87676620\n",
      "Iteration 193, loss = 19651539967.41647720\n",
      "Iteration 194, loss = 19651535225.78551102\n",
      "Iteration 195, loss = 19651530404.19235611\n",
      "Iteration 196, loss = 19651525608.61256409\n",
      "Iteration 197, loss = 19651520871.11354446\n",
      "Iteration 198, loss = 19651516095.45960236\n",
      "Iteration 199, loss = 19651511340.80932236\n",
      "Iteration 200, loss = 19651506590.56372070\n",
      "Iteration 201, loss = 19651501815.19262314\n",
      "Iteration 202, loss = 19651497148.31921768\n",
      "Iteration 203, loss = 19651492403.91781998\n",
      "Iteration 204, loss = 19651487628.26425934\n",
      "Iteration 205, loss = 19651482925.00740433\n",
      "Iteration 206, loss = 19651478193.27152634\n",
      "Iteration 207, loss = 19651473469.97331619\n",
      "Iteration 208, loss = 19651468715.67567062\n",
      "Iteration 209, loss = 19651464008.79043579\n",
      "Iteration 210, loss = 19651459323.18426514\n",
      "Iteration 211, loss = 19651454604.56962967\n",
      "Iteration 212, loss = 19651449909.19004059\n",
      "Iteration 213, loss = 19651445172.18241501\n",
      "Iteration 214, loss = 19651440426.39604568\n",
      "Iteration 215, loss = 19651435806.64059067\n",
      "Iteration 216, loss = 19651431069.46340179\n",
      "Iteration 217, loss = 19651426378.71235657\n",
      "Iteration 218, loss = 19651421668.22026062\n",
      "Iteration 219, loss = 19651416986.56399536\n",
      "Iteration 220, loss = 19651412289.80592346\n",
      "Iteration 221, loss = 19651407605.80142593\n",
      "Iteration 222, loss = 19651402910.23986435\n",
      "Iteration 223, loss = 19651398310.18520355\n",
      "Iteration 224, loss = 19651393555.99337387\n",
      "Iteration 225, loss = 19651388869.86204910\n",
      "Iteration 226, loss = 19651384206.91011810\n",
      "Iteration 227, loss = 19651379557.37966919\n",
      "Iteration 228, loss = 19651374885.99925232\n",
      "Iteration 229, loss = 19651370204.10436630\n",
      "Iteration 230, loss = 19651365534.42533493\n",
      "Iteration 231, loss = 19651360865.00901794\n",
      "Iteration 232, loss = 19651356178.88189697\n",
      "Iteration 233, loss = 19651351564.75349045\n",
      "Iteration 234, loss = 19651346899.84700775\n",
      "Iteration 235, loss = 19651342239.24829483\n",
      "Iteration 236, loss = 19651337596.43019485\n",
      "Iteration 237, loss = 19651332939.17707825\n",
      "Iteration 238, loss = 19651328308.01072693\n",
      "Iteration 239, loss = 19651323628.95336151\n",
      "Iteration 240, loss = 19651318980.68870544\n",
      "Iteration 241, loss = 19651314309.90404510\n",
      "Iteration 242, loss = 19651309698.33656693\n",
      "Iteration 243, loss = 19651305073.48665619\n",
      "Iteration 244, loss = 19651300408.04226685\n",
      "Iteration 245, loss = 19651295806.81406021\n",
      "Iteration 246, loss = 19651291099.28176880\n",
      "Iteration 247, loss = 19651286511.66353989\n",
      "Iteration 248, loss = 19651281925.15760422\n",
      "Iteration 249, loss = 19651277256.64155579\n",
      "Iteration 250, loss = 19651272641.52896118\n",
      "Iteration 251, loss = 19651267979.64564896\n",
      "Iteration 252, loss = 19651263341.20319748\n",
      "Iteration 253, loss = 19651258719.57952881\n",
      "Iteration 254, loss = 19651254126.51210022\n",
      "Iteration 255, loss = 19651249512.77291489\n",
      "Iteration 256, loss = 19651244870.62128830\n",
      "Iteration 257, loss = 19651240246.93585587\n",
      "Iteration 258, loss = 19651235664.98635101\n",
      "Iteration 259, loss = 19651230974.16497803\n",
      "Iteration 260, loss = 19651226393.38084793\n",
      "Iteration 261, loss = 19651221734.39704895\n",
      "Iteration 262, loss = 19651217133.32059860\n",
      "Iteration 263, loss = 19651212537.68884277\n",
      "Iteration 264, loss = 19651207882.60832596\n",
      "Iteration 265, loss = 19651203266.49263000\n",
      "Iteration 266, loss = 19651198727.29267502\n",
      "Iteration 267, loss = 19651194075.52304459\n",
      "Iteration 268, loss = 19651189452.62257004\n",
      "Iteration 269, loss = 19651184863.72087097\n",
      "Iteration 270, loss = 19651180277.93629074\n",
      "Iteration 271, loss = 19651175685.14172363\n",
      "Iteration 272, loss = 19651171075.19204330\n",
      "Iteration 273, loss = 19651166458.84312057\n",
      "Iteration 274, loss = 19651161934.33066177\n",
      "Iteration 275, loss = 19651157269.42430115\n",
      "Iteration 276, loss = 19651152706.08914948\n",
      "Iteration 277, loss = 19651148096.67423630\n",
      "Iteration 278, loss = 19651143518.12419510\n",
      "Iteration 279, loss = 19651138937.54065323\n",
      "Iteration 280, loss = 19651134319.82556152\n",
      "Iteration 281, loss = 19651129803.21767807\n",
      "Iteration 282, loss = 19651125194.73085022\n",
      "Iteration 283, loss = 19651120631.30784607\n",
      "Iteration 284, loss = 19651115998.01388931\n",
      "Iteration 285, loss = 19651111443.35783005\n",
      "Iteration 286, loss = 19651106848.57850647\n",
      "Iteration 287, loss = 19651102283.26372528\n",
      "Iteration 288, loss = 19651097703.91795349\n",
      "Iteration 289, loss = 19651093116.70637131\n",
      "Iteration 290, loss = 19651088551.01964951\n",
      "Iteration 291, loss = 19651083926.05253220\n",
      "Iteration 292, loss = 19651079465.93293762\n",
      "Iteration 293, loss = 19651074871.76136398\n",
      "Iteration 294, loss = 19651070313.85046387\n",
      "Iteration 295, loss = 19651065685.59183502\n",
      "Iteration 296, loss = 19651061133.63833237\n",
      "Iteration 297, loss = 19651056544.06375122\n",
      "Iteration 298, loss = 19651051995.92886734\n",
      "Iteration 299, loss = 19651047435.40497971\n",
      "Iteration 300, loss = 19651042902.02883530\n",
      "Iteration 301, loss = 19651038351.85529709\n",
      "Iteration 302, loss = 19651033790.24445724\n",
      "Iteration 303, loss = 19651029167.19453049\n",
      "Iteration 304, loss = 19651024669.16386795\n",
      "Iteration 305, loss = 19651020076.87747574\n",
      "Iteration 306, loss = 19651015529.78407288\n",
      "Iteration 307, loss = 19651010953.80125809\n",
      "Iteration 308, loss = 19651006442.04851532\n",
      "Iteration 309, loss = 19651001854.26235580\n",
      "Iteration 310, loss = 19650997337.57916641\n",
      "Iteration 311, loss = 19650992742.70171356\n",
      "Iteration 312, loss = 19650988259.05171585\n",
      "Iteration 313, loss = 19650983671.58586884\n",
      "Iteration 314, loss = 19650979107.23578644\n",
      "Iteration 315, loss = 19650974561.02474976\n",
      "Iteration 316, loss = 19650970032.92537308\n",
      "Iteration 317, loss = 19650965508.90790939\n",
      "Iteration 318, loss = 19650960943.48601913\n",
      "Iteration 319, loss = 19650956376.07587814\n",
      "Iteration 320, loss = 19650951819.31042480\n",
      "Iteration 321, loss = 19650947291.31975174\n",
      "Iteration 322, loss = 19650942744.93558121\n",
      "Iteration 323, loss = 19650938245.34862137\n",
      "Iteration 324, loss = 19650933659.36307907\n",
      "Iteration 325, loss = 19650929111.62731552\n",
      "Iteration 326, loss = 19650924618.17972946\n",
      "Iteration 327, loss = 19650920052.96952820\n",
      "Iteration 328, loss = 19650915490.08448029\n",
      "Iteration 329, loss = 19650910984.36915207\n",
      "Iteration 330, loss = 19650906476.59193420\n",
      "Iteration 331, loss = 19650901900.56465912\n",
      "Iteration 332, loss = 19650897335.60956955\n",
      "Iteration 333, loss = 19650892815.08888245\n",
      "Iteration 334, loss = 19650888302.47884750\n",
      "Iteration 335, loss = 19650883759.56093597\n",
      "Iteration 336, loss = 19650879229.15386963\n",
      "Iteration 337, loss = 19650874690.31267548\n",
      "Iteration 338, loss = 19650870185.30804825\n",
      "Iteration 339, loss = 19650865600.70718002\n",
      "Iteration 340, loss = 19650861071.12278748\n",
      "Iteration 341, loss = 19650856534.19602966\n",
      "Iteration 342, loss = 19650852041.83063889\n",
      "Iteration 343, loss = 19650847506.77257156\n",
      "Iteration 344, loss = 19650842929.05145264\n",
      "Iteration 345, loss = 19650838428.24162674\n",
      "Iteration 346, loss = 19650833905.50551224\n",
      "Iteration 347, loss = 19650829332.67444611\n",
      "Iteration 348, loss = 19650824798.02082062\n",
      "Iteration 349, loss = 19650820266.45235062\n",
      "Iteration 350, loss = 19650815721.42827606\n",
      "Iteration 351, loss = 19650811188.78042221\n",
      "Iteration 352, loss = 19650806611.98489761\n",
      "Iteration 353, loss = 19650802084.33200836\n",
      "Iteration 354, loss = 19650797517.07199860\n",
      "Iteration 355, loss = 19650792918.80216599\n",
      "Iteration 356, loss = 19650788355.29024124\n",
      "Iteration 357, loss = 19650783764.10652161\n",
      "Iteration 358, loss = 19650779137.56475830\n",
      "Iteration 359, loss = 19650774511.30451202\n",
      "Iteration 360, loss = 19650769879.70610046\n",
      "Iteration 361, loss = 19650765185.00896835\n",
      "Iteration 362, loss = 19650760472.07775116\n",
      "Iteration 363, loss = 19650755706.19614792\n",
      "Iteration 364, loss = 19650750876.12624359\n",
      "Iteration 365, loss = 19650746000.18685150\n",
      "Iteration 366, loss = 19650741131.29927444\n",
      "Iteration 367, loss = 19650736170.00218201\n",
      "Iteration 368, loss = 19650731204.34577560\n",
      "Iteration 369, loss = 19650726172.53700638\n",
      "Iteration 370, loss = 19650721143.08856201\n",
      "Iteration 371, loss = 19650716061.92036438\n",
      "Iteration 372, loss = 19650710971.37608337\n",
      "Iteration 373, loss = 19650705914.73832703\n",
      "Iteration 374, loss = 19650700871.79101562\n",
      "Iteration 375, loss = 19650695829.05463409\n",
      "Iteration 376, loss = 19650690814.78447723\n",
      "Iteration 377, loss = 19650685787.05348206\n",
      "Iteration 378, loss = 19650680786.36978531\n",
      "Iteration 379, loss = 19650675812.69407272\n",
      "Iteration 380, loss = 19650670878.50315857\n",
      "Iteration 381, loss = 19650665897.32222366\n",
      "Iteration 382, loss = 19650660946.07464600\n",
      "Iteration 383, loss = 19650656074.50542068\n",
      "Iteration 384, loss = 19650651103.42663193\n",
      "Iteration 385, loss = 19650646246.44301224\n",
      "Iteration 386, loss = 19650641329.34803391\n",
      "Iteration 387, loss = 19650636464.60048294\n",
      "Iteration 388, loss = 19650631555.67955017\n",
      "Iteration 389, loss = 19650626694.70356750\n",
      "Iteration 390, loss = 19650621905.34267044\n",
      "Iteration 391, loss = 19650617008.84776306\n",
      "Iteration 392, loss = 19650612176.04267502\n",
      "Iteration 393, loss = 19650607305.58182144\n",
      "Iteration 394, loss = 19650602516.21975327\n",
      "Iteration 395, loss = 19650597686.33961487\n",
      "Iteration 396, loss = 19650592875.02633286\n",
      "Iteration 397, loss = 19650588058.40054321\n",
      "Iteration 398, loss = 19650583240.41292953\n",
      "Iteration 399, loss = 19650578442.36576843\n",
      "Iteration 400, loss = 19650573668.59956360\n",
      "Iteration 401, loss = 19650568871.45024490\n",
      "Iteration 402, loss = 19650564095.57113647\n",
      "Iteration 403, loss = 19650559309.35168076\n",
      "Iteration 404, loss = 19650554506.23325348\n",
      "Iteration 405, loss = 19650549731.34992218\n",
      "Iteration 406, loss = 19650544960.05455017\n",
      "Iteration 407, loss = 19650540195.27160263\n",
      "Iteration 408, loss = 19650535442.95088577\n",
      "Iteration 409, loss = 19650530697.08264923\n",
      "Iteration 410, loss = 19650525980.56864929\n",
      "Iteration 411, loss = 19650521215.38918304\n",
      "Iteration 412, loss = 19650516468.28028107\n",
      "Iteration 413, loss = 19650511689.41152573\n",
      "Iteration 414, loss = 19650506956.78577042\n",
      "Iteration 415, loss = 19650502218.40131378\n",
      "Iteration 416, loss = 19650497465.77190781\n",
      "Iteration 417, loss = 19650492712.70037842\n",
      "Iteration 418, loss = 19650488001.20285034\n",
      "Iteration 419, loss = 19650483272.86138153\n",
      "Iteration 420, loss = 19650478580.23789215\n",
      "Iteration 421, loss = 19650473846.00231934\n",
      "Iteration 422, loss = 19650469131.42761993\n",
      "Iteration 423, loss = 19650464396.42278671\n",
      "Iteration 424, loss = 19650459655.28316116\n",
      "Iteration 425, loss = 19650454944.10043335\n",
      "Iteration 426, loss = 19650450278.94691467\n",
      "Iteration 427, loss = 19650445550.51507187\n",
      "Iteration 428, loss = 19650440810.84060287\n",
      "Iteration 429, loss = 19650436153.05121231\n",
      "Iteration 430, loss = 19650431425.30170441\n",
      "Iteration 431, loss = 19650426725.76074600\n",
      "Iteration 432, loss = 19650422037.97253418\n",
      "Iteration 433, loss = 19650417338.92723083\n",
      "Iteration 434, loss = 19650412652.32091904\n",
      "Iteration 435, loss = 19650407972.87182236\n",
      "Iteration 436, loss = 19650403246.62466049\n",
      "Iteration 437, loss = 19650398559.87950134\n",
      "Iteration 438, loss = 19650393803.63658524\n",
      "Iteration 439, loss = 19650389220.03984451\n",
      "Iteration 440, loss = 19650384500.94764328\n",
      "Iteration 441, loss = 19650379797.72502518\n",
      "Iteration 442, loss = 19650375164.64921570\n",
      "Iteration 443, loss = 19650370413.80764008\n",
      "Iteration 444, loss = 19650365822.84305954\n",
      "Iteration 445, loss = 19650361133.02432632\n",
      "Iteration 446, loss = 19650356376.22774887\n",
      "Iteration 447, loss = 19650351768.38341522\n",
      "Iteration 448, loss = 19650347081.48346710\n",
      "Iteration 449, loss = 19650342413.14347839\n",
      "Iteration 450, loss = 19650337760.35673141\n",
      "Iteration 451, loss = 19650333055.89073944\n",
      "Iteration 452, loss = 19650328415.81259537\n",
      "Iteration 453, loss = 19650323736.67613602\n",
      "Iteration 454, loss = 19650319063.07458496\n",
      "Iteration 455, loss = 19650314452.76564407\n",
      "Iteration 456, loss = 19650309755.18907928\n",
      "Iteration 457, loss = 19650305095.27339554\n",
      "Iteration 458, loss = 19650300429.88323593\n",
      "Iteration 459, loss = 19650295777.21073914\n",
      "Iteration 460, loss = 19650291122.15500641\n",
      "Iteration 461, loss = 19650286425.69530487\n",
      "Iteration 462, loss = 19650281820.52238083\n",
      "Iteration 463, loss = 19650277174.81718445\n",
      "Iteration 464, loss = 19650272543.33082199\n",
      "Iteration 465, loss = 19650267847.01963043\n",
      "Iteration 466, loss = 19650263219.83705902\n",
      "Iteration 467, loss = 19650258530.11603928\n",
      "Iteration 468, loss = 19650253941.44353104\n",
      "Iteration 469, loss = 19650249261.98002243\n",
      "Iteration 470, loss = 19650244635.61176300\n",
      "Iteration 471, loss = 19650240001.55926895\n",
      "Iteration 472, loss = 19650235358.73696899\n",
      "Iteration 473, loss = 19650230701.50733566\n",
      "Iteration 474, loss = 19650226063.97351837\n",
      "Iteration 475, loss = 19650221402.67707062\n",
      "Iteration 476, loss = 19650216827.61504745\n",
      "Iteration 477, loss = 19650212154.43021774\n",
      "Iteration 478, loss = 19650207545.04487228\n",
      "Iteration 479, loss = 19650202879.45365524\n",
      "Iteration 480, loss = 19650198220.23549271\n",
      "Iteration 481, loss = 19650193586.88694000\n",
      "Iteration 482, loss = 19650188951.25245667\n",
      "Iteration 483, loss = 19650184365.78063583\n",
      "Iteration 484, loss = 19650179753.93265152\n",
      "Iteration 485, loss = 19650175148.14925385\n",
      "Iteration 486, loss = 19650170415.27529526\n",
      "Iteration 487, loss = 19650165825.04963303\n",
      "Iteration 488, loss = 19650161236.19995117\n",
      "Iteration 489, loss = 19650156555.66355896\n",
      "Iteration 490, loss = 19650151955.69134140\n",
      "Iteration 491, loss = 19650147323.82302475\n",
      "Iteration 492, loss = 19650142712.12114334\n",
      "Iteration 493, loss = 19650138087.38151169\n",
      "Iteration 494, loss = 19650133463.44534683\n",
      "Iteration 495, loss = 19650128868.44095993\n",
      "Iteration 496, loss = 19650124213.70738602\n",
      "Iteration 497, loss = 19650119606.34745407\n",
      "Iteration 498, loss = 19650114957.81553650\n",
      "Iteration 499, loss = 19650110325.61387634\n",
      "Iteration 500, loss = 19650105772.45816803\n",
      "Iteration 1, loss = 20145452376.90696335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 20145438446.82794189\n",
      "Iteration 3, loss = 20145424254.32437134\n",
      "Iteration 4, loss = 20145410204.63375473\n",
      "Iteration 5, loss = 20145395898.71700287\n",
      "Iteration 6, loss = 20145381613.71991730\n",
      "Iteration 7, loss = 20145367424.05930328\n",
      "Iteration 8, loss = 20145353182.85380936\n",
      "Iteration 9, loss = 20145339009.54901886\n",
      "Iteration 10, loss = 20145325150.58625412\n",
      "Iteration 11, loss = 20145310948.99443054\n",
      "Iteration 12, loss = 20145296987.00841904\n",
      "Iteration 13, loss = 20145282645.54600143\n",
      "Iteration 14, loss = 20145268548.42444992\n",
      "Iteration 15, loss = 20145254121.53261566\n",
      "Iteration 16, loss = 20145239769.66078568\n",
      "Iteration 17, loss = 20145225126.62280273\n",
      "Iteration 18, loss = 20145210190.93489075\n",
      "Iteration 19, loss = 20145194521.36791992\n",
      "Iteration 20, loss = 20145178606.73860168\n",
      "Iteration 21, loss = 20145162803.71733093\n",
      "Iteration 22, loss = 20145146856.33858109\n",
      "Iteration 23, loss = 20145131004.52502060\n",
      "Iteration 24, loss = 20145115406.14133835\n",
      "Iteration 25, loss = 20145100321.15439224\n",
      "Iteration 26, loss = 20145085720.89749527\n",
      "Iteration 27, loss = 20145071893.04953766\n",
      "Iteration 28, loss = 20145058638.45194244\n",
      "Iteration 29, loss = 20145045680.34384918\n",
      "Iteration 30, loss = 20145033507.82482529\n",
      "Iteration 31, loss = 20145021946.60457230\n",
      "Iteration 32, loss = 20145010689.14684677\n",
      "Iteration 33, loss = 20144999638.17276382\n",
      "Iteration 34, loss = 20144988605.45049286\n",
      "Iteration 35, loss = 20144977654.43870163\n",
      "Iteration 36, loss = 20144966896.63262177\n",
      "Iteration 37, loss = 20144956034.69718933\n",
      "Iteration 38, loss = 20144945377.01262665\n",
      "Iteration 39, loss = 20144935150.44582367\n",
      "Iteration 40, loss = 20144925021.47511292\n",
      "Iteration 41, loss = 20144915372.22595978\n",
      "Iteration 42, loss = 20144906451.63670349\n",
      "Iteration 43, loss = 20144897823.52413177\n",
      "Iteration 44, loss = 20144889147.51415634\n",
      "Iteration 45, loss = 20144880400.30901718\n",
      "Iteration 46, loss = 20144871871.69244385\n",
      "Iteration 47, loss = 20144863762.08899689\n",
      "Iteration 48, loss = 20144855851.29821014\n",
      "Iteration 49, loss = 20144848288.51550293\n",
      "Iteration 50, loss = 20144841290.24140167\n",
      "Iteration 51, loss = 20144834724.06014633\n",
      "Iteration 52, loss = 20144828455.01502609\n",
      "Iteration 53, loss = 20144822436.17432404\n",
      "Iteration 54, loss = 20144816522.46158218\n",
      "Iteration 55, loss = 20144810671.83157349\n",
      "Iteration 56, loss = 20144804907.31341934\n",
      "Iteration 57, loss = 20144799102.45216751\n",
      "Iteration 58, loss = 20144793331.22183990\n",
      "Iteration 59, loss = 20144787575.14120102\n",
      "Iteration 60, loss = 20144781821.55589676\n",
      "Iteration 61, loss = 20144776027.95956039\n",
      "Iteration 62, loss = 20144770365.30215073\n",
      "Iteration 63, loss = 20144764807.51142502\n",
      "Iteration 64, loss = 20144759410.04982758\n",
      "Iteration 65, loss = 20144754057.75761795\n",
      "Iteration 66, loss = 20144748702.93158340\n",
      "Iteration 67, loss = 20144743434.94267654\n",
      "Iteration 68, loss = 20144738262.24308777\n",
      "Iteration 69, loss = 20144733151.08417511\n",
      "Iteration 70, loss = 20144728090.59282684\n",
      "Iteration 71, loss = 20144723037.10238647\n",
      "Iteration 72, loss = 20144717908.98571014\n",
      "Iteration 73, loss = 20144712880.20555878\n",
      "Iteration 74, loss = 20144707813.51015472\n",
      "Iteration 75, loss = 20144702806.10902786\n",
      "Iteration 76, loss = 20144697856.03854752\n",
      "Iteration 77, loss = 20144692985.92600632\n",
      "Iteration 78, loss = 20144688161.20492935\n",
      "Iteration 79, loss = 20144683339.57711029\n",
      "Iteration 80, loss = 20144678547.71849442\n",
      "Iteration 81, loss = 20144673753.74955368\n",
      "Iteration 82, loss = 20144668912.65322495\n",
      "Iteration 83, loss = 20144664173.44695282\n",
      "Iteration 84, loss = 20144659447.22542953\n",
      "Iteration 85, loss = 20144654722.92792892\n",
      "Iteration 86, loss = 20144650084.76770782\n",
      "Iteration 87, loss = 20144645456.49009705\n",
      "Iteration 88, loss = 20144640794.28826904\n",
      "Iteration 89, loss = 20144636185.37574387\n",
      "Iteration 90, loss = 20144631566.83691025\n",
      "Iteration 91, loss = 20144626989.12408066\n",
      "Iteration 92, loss = 20144622421.38283539\n",
      "Iteration 93, loss = 20144617878.72763062\n",
      "Iteration 94, loss = 20144613325.23742294\n",
      "Iteration 95, loss = 20144608819.14397430\n",
      "Iteration 96, loss = 20144604245.65073013\n",
      "Iteration 97, loss = 20144599756.67457199\n",
      "Iteration 98, loss = 20144595251.92020035\n",
      "Iteration 99, loss = 20144590764.98765182\n",
      "Iteration 100, loss = 20144586256.95313644\n",
      "Iteration 101, loss = 20144581797.37440872\n",
      "Iteration 102, loss = 20144577322.77682877\n",
      "Iteration 103, loss = 20144572876.08712387\n",
      "Iteration 104, loss = 20144568442.63344193\n",
      "Iteration 105, loss = 20144563971.78425598\n",
      "Iteration 106, loss = 20144559515.89076614\n",
      "Iteration 107, loss = 20144555087.84237671\n",
      "Iteration 108, loss = 20144550700.05903625\n",
      "Iteration 109, loss = 20144546283.90974426\n",
      "Iteration 110, loss = 20144541833.61991119\n",
      "Iteration 111, loss = 20144537431.38738632\n",
      "Iteration 112, loss = 20144533036.65538406\n",
      "Iteration 113, loss = 20144528669.15769958\n",
      "Iteration 114, loss = 20144524294.41895676\n",
      "Iteration 115, loss = 20144519894.18719482\n",
      "Iteration 116, loss = 20144515505.16001892\n",
      "Iteration 117, loss = 20144511128.39677811\n",
      "Iteration 118, loss = 20144506749.13029480\n",
      "Iteration 119, loss = 20144502387.74876785\n",
      "Iteration 120, loss = 20144498055.68401718\n",
      "Iteration 121, loss = 20144493684.87424088\n",
      "Iteration 122, loss = 20144489361.52787018\n",
      "Iteration 123, loss = 20144484958.84991074\n",
      "Iteration 124, loss = 20144480619.91331100\n",
      "Iteration 125, loss = 20144476293.64507675\n",
      "Iteration 126, loss = 20144471974.25719833\n",
      "Iteration 127, loss = 20144467674.10848618\n",
      "Iteration 128, loss = 20144463346.92327118\n",
      "Iteration 129, loss = 20144459044.61358643\n",
      "Iteration 130, loss = 20144454710.72496796\n",
      "Iteration 131, loss = 20144450395.22619247\n",
      "Iteration 132, loss = 20144446075.63007355\n",
      "Iteration 133, loss = 20144441753.55858231\n",
      "Iteration 134, loss = 20144437475.68964005\n",
      "Iteration 135, loss = 20144433198.42596436\n",
      "Iteration 136, loss = 20144428897.07661438\n",
      "Iteration 137, loss = 20144424617.24034119\n",
      "Iteration 138, loss = 20144420309.71822357\n",
      "Iteration 139, loss = 20144416094.52716064\n",
      "Iteration 140, loss = 20144411795.68590546\n",
      "Iteration 141, loss = 20144407541.08844376\n",
      "Iteration 142, loss = 20144403293.55004120\n",
      "Iteration 143, loss = 20144399017.77678680\n",
      "Iteration 144, loss = 20144394783.24508667\n",
      "Iteration 145, loss = 20144390507.55281448\n",
      "Iteration 146, loss = 20144386271.34314346\n",
      "Iteration 147, loss = 20144382038.81005478\n",
      "Iteration 148, loss = 20144377784.92152023\n",
      "Iteration 149, loss = 20144373542.45098114\n",
      "Iteration 150, loss = 20144369308.93858337\n",
      "Iteration 151, loss = 20144365125.65832138\n",
      "Iteration 152, loss = 20144360851.67702866\n",
      "Iteration 153, loss = 20144356648.02029037\n",
      "Iteration 154, loss = 20144352430.24676514\n",
      "Iteration 155, loss = 20144348235.76530075\n",
      "Iteration 156, loss = 20144343994.34425354\n",
      "Iteration 157, loss = 20144339784.64807510\n",
      "Iteration 158, loss = 20144335576.86306000\n",
      "Iteration 159, loss = 20144331357.80236053\n",
      "Iteration 160, loss = 20144327132.76189804\n",
      "Iteration 161, loss = 20144322958.46438217\n",
      "Iteration 162, loss = 20144318742.08334732\n",
      "Iteration 163, loss = 20144314536.79788589\n",
      "Iteration 164, loss = 20144310326.63855362\n",
      "Iteration 165, loss = 20144306122.20183182\n",
      "Iteration 166, loss = 20144301931.02849197\n",
      "Iteration 167, loss = 20144297740.89870071\n",
      "Iteration 168, loss = 20144293546.51767731\n",
      "Iteration 169, loss = 20144289390.29641342\n",
      "Iteration 170, loss = 20144285181.21099472\n",
      "Iteration 171, loss = 20144280984.27585220\n",
      "Iteration 172, loss = 20144276857.55490875\n",
      "Iteration 173, loss = 20144272667.81318283\n",
      "Iteration 174, loss = 20144268483.70114517\n",
      "Iteration 175, loss = 20144264343.40331268\n",
      "Iteration 176, loss = 20144260182.96887207\n",
      "Iteration 177, loss = 20144256001.64131927\n",
      "Iteration 178, loss = 20144251814.04756165\n",
      "Iteration 179, loss = 20144247619.28487015\n",
      "Iteration 180, loss = 20144243478.31794739\n",
      "Iteration 181, loss = 20144239312.50932312\n",
      "Iteration 182, loss = 20144235167.84066010\n",
      "Iteration 183, loss = 20144230985.90602875\n",
      "Iteration 184, loss = 20144226840.79048538\n",
      "Iteration 185, loss = 20144222688.17269897\n",
      "Iteration 186, loss = 20144218559.63751221\n",
      "Iteration 187, loss = 20144214428.82194138\n",
      "Iteration 188, loss = 20144210274.02275085\n",
      "Iteration 189, loss = 20144206155.95087433\n",
      "Iteration 190, loss = 20144202020.82781982\n",
      "Iteration 191, loss = 20144197835.05588913\n",
      "Iteration 192, loss = 20144193726.00099182\n",
      "Iteration 193, loss = 20144189583.95622635\n",
      "Iteration 194, loss = 20144185422.66616058\n",
      "Iteration 195, loss = 20144181285.89706421\n",
      "Iteration 196, loss = 20144177155.62983322\n",
      "Iteration 197, loss = 20144173001.60227203\n",
      "Iteration 198, loss = 20144168890.45483017\n",
      "Iteration 199, loss = 20144164718.58028412\n",
      "Iteration 200, loss = 20144160581.03746033\n",
      "Iteration 201, loss = 20144156486.23664474\n",
      "Iteration 202, loss = 20144152350.93191528\n",
      "Iteration 203, loss = 20144148225.31241226\n",
      "Iteration 204, loss = 20144144102.00972748\n",
      "Iteration 205, loss = 20144139990.93576813\n",
      "Iteration 206, loss = 20144135852.85173416\n",
      "Iteration 207, loss = 20144131726.68964767\n",
      "Iteration 208, loss = 20144127604.90121078\n",
      "Iteration 209, loss = 20144123496.16710663\n",
      "Iteration 210, loss = 20144119374.21696472\n",
      "Iteration 211, loss = 20144115221.08704376\n",
      "Iteration 212, loss = 20144111107.87227249\n",
      "Iteration 213, loss = 20144106982.36187363\n",
      "Iteration 214, loss = 20144102864.58109283\n",
      "Iteration 215, loss = 20144098760.35596085\n",
      "Iteration 216, loss = 20144094610.35076904\n",
      "Iteration 217, loss = 20144090543.41633606\n",
      "Iteration 218, loss = 20144086396.69622421\n",
      "Iteration 219, loss = 20144082325.39094543\n",
      "Iteration 220, loss = 20144078200.18394470\n",
      "Iteration 221, loss = 20144074088.47814560\n",
      "Iteration 222, loss = 20144070004.69866180\n",
      "Iteration 223, loss = 20144065863.25409698\n",
      "Iteration 224, loss = 20144061783.63443756\n",
      "Iteration 225, loss = 20144057668.21295547\n",
      "Iteration 226, loss = 20144053607.17540741\n",
      "Iteration 227, loss = 20144049475.45417404\n",
      "Iteration 228, loss = 20144045422.77450180\n",
      "Iteration 229, loss = 20144041333.72246552\n",
      "Iteration 230, loss = 20144037230.67070770\n",
      "Iteration 231, loss = 20144033148.90429306\n",
      "Iteration 232, loss = 20144029047.91600037\n",
      "Iteration 233, loss = 20144024979.57742310\n",
      "Iteration 234, loss = 20144020880.68085098\n",
      "Iteration 235, loss = 20144016797.03574371\n",
      "Iteration 236, loss = 20144012734.22990417\n",
      "Iteration 237, loss = 20144008661.13062286\n",
      "Iteration 238, loss = 20144004530.50426865\n",
      "Iteration 239, loss = 20144000492.25330734\n",
      "Iteration 240, loss = 20143996378.77496719\n",
      "Iteration 241, loss = 20143992305.70051956\n",
      "Iteration 242, loss = 20143988243.56326675\n",
      "Iteration 243, loss = 20143984144.31012344\n",
      "Iteration 244, loss = 20143980049.38429642\n",
      "Iteration 245, loss = 20143976044.57086182\n",
      "Iteration 246, loss = 20143971921.60286331\n",
      "Iteration 247, loss = 20143967849.94029999\n",
      "Iteration 248, loss = 20143963782.28925705\n",
      "Iteration 249, loss = 20143959747.10837173\n",
      "Iteration 250, loss = 20143955639.56704330\n",
      "Iteration 251, loss = 20143951586.15522766\n",
      "Iteration 252, loss = 20143947489.09673691\n",
      "Iteration 253, loss = 20143943443.68489075\n",
      "Iteration 254, loss = 20143939352.79496002\n",
      "Iteration 255, loss = 20143935280.10047531\n",
      "Iteration 256, loss = 20143931213.64632034\n",
      "Iteration 257, loss = 20143927156.99554825\n",
      "Iteration 258, loss = 20143923087.34643555\n",
      "Iteration 259, loss = 20143919005.27124023\n",
      "Iteration 260, loss = 20143914913.72908020\n",
      "Iteration 261, loss = 20143910865.20086670\n",
      "Iteration 262, loss = 20143906772.07269287\n",
      "Iteration 263, loss = 20143902685.13211060\n",
      "Iteration 264, loss = 20143898623.27827072\n",
      "Iteration 265, loss = 20143894535.87519073\n",
      "Iteration 266, loss = 20143890445.88103867\n",
      "Iteration 267, loss = 20143886374.06071472\n",
      "Iteration 268, loss = 20143882293.11972809\n",
      "Iteration 269, loss = 20143878231.32713699\n",
      "Iteration 270, loss = 20143874150.78584671\n",
      "Iteration 271, loss = 20143870077.45772934\n",
      "Iteration 272, loss = 20143866037.50521088\n",
      "Iteration 273, loss = 20143861939.32843018\n",
      "Iteration 274, loss = 20143857908.23997498\n",
      "Iteration 275, loss = 20143853823.55358505\n",
      "Iteration 276, loss = 20143849752.04267120\n",
      "Iteration 277, loss = 20143845698.95904922\n",
      "Iteration 278, loss = 20143841673.09098053\n",
      "Iteration 279, loss = 20143837589.20157242\n",
      "Iteration 280, loss = 20143833579.12071609\n",
      "Iteration 281, loss = 20143829472.20900726\n",
      "Iteration 282, loss = 20143825462.12489319\n",
      "Iteration 283, loss = 20143821377.04153442\n",
      "Iteration 284, loss = 20143817318.22385025\n",
      "Iteration 285, loss = 20143813257.65183640\n",
      "Iteration 286, loss = 20143809187.69076920\n",
      "Iteration 287, loss = 20143805129.78991699\n",
      "Iteration 288, loss = 20143801073.38069916\n",
      "Iteration 289, loss = 20143796989.01899338\n",
      "Iteration 290, loss = 20143792925.42747116\n",
      "Iteration 291, loss = 20143788820.37786102\n",
      "Iteration 292, loss = 20143784828.17747879\n",
      "Iteration 293, loss = 20143780720.49781036\n",
      "Iteration 294, loss = 20143776628.84818649\n",
      "Iteration 295, loss = 20143772597.28373718\n",
      "Iteration 296, loss = 20143768574.50987625\n",
      "Iteration 297, loss = 20143764481.45475006\n",
      "Iteration 298, loss = 20143760420.33638763\n",
      "Iteration 299, loss = 20143756370.65550613\n",
      "Iteration 300, loss = 20143752356.36753464\n",
      "Iteration 301, loss = 20143748302.73750687\n",
      "Iteration 302, loss = 20143744262.90597153\n",
      "Iteration 303, loss = 20143740207.26161194\n",
      "Iteration 304, loss = 20143736181.49822617\n",
      "Iteration 305, loss = 20143732094.16346359\n",
      "Iteration 306, loss = 20143728075.41512299\n",
      "Iteration 307, loss = 20143724014.85926437\n",
      "Iteration 308, loss = 20143719964.26162338\n",
      "Iteration 309, loss = 20143715900.92295837\n",
      "Iteration 310, loss = 20143711863.46182251\n",
      "Iteration 311, loss = 20143707746.52535248\n",
      "Iteration 312, loss = 20143703717.44279480\n",
      "Iteration 313, loss = 20143699664.65473175\n",
      "Iteration 314, loss = 20143695622.54998016\n",
      "Iteration 315, loss = 20143691575.10934067\n",
      "Iteration 316, loss = 20143687519.85071564\n",
      "Iteration 317, loss = 20143683500.91188049\n",
      "Iteration 318, loss = 20143679464.73029327\n",
      "Iteration 319, loss = 20143675427.09849548\n",
      "Iteration 320, loss = 20143671424.92840195\n",
      "Iteration 321, loss = 20143667373.63365173\n",
      "Iteration 322, loss = 20143663353.07958221\n",
      "Iteration 323, loss = 20143659333.36874008\n",
      "Iteration 324, loss = 20143655243.43767166\n",
      "Iteration 325, loss = 20143651249.50058365\n",
      "Iteration 326, loss = 20143647205.64918137\n",
      "Iteration 327, loss = 20143643195.09466934\n",
      "Iteration 328, loss = 20143639113.50676346\n",
      "Iteration 329, loss = 20143635121.08442688\n",
      "Iteration 330, loss = 20143631056.17494583\n",
      "Iteration 331, loss = 20143627025.10116196\n",
      "Iteration 332, loss = 20143622986.67421341\n",
      "Iteration 333, loss = 20143618950.12388992\n",
      "Iteration 334, loss = 20143614945.14243698\n",
      "Iteration 335, loss = 20143610915.90123367\n",
      "Iteration 336, loss = 20143606901.18295670\n",
      "Iteration 337, loss = 20143602858.65867615\n",
      "Iteration 338, loss = 20143598842.96494293\n",
      "Iteration 339, loss = 20143594828.57516861\n",
      "Iteration 340, loss = 20143590787.72200012\n",
      "Iteration 341, loss = 20143586758.24338913\n",
      "Iteration 342, loss = 20143582755.11772919\n",
      "Iteration 343, loss = 20143578693.09146881\n",
      "Iteration 344, loss = 20143574672.90313721\n",
      "Iteration 345, loss = 20143570644.74848938\n",
      "Iteration 346, loss = 20143566609.61969757\n",
      "Iteration 347, loss = 20143562619.03034210\n",
      "Iteration 348, loss = 20143558570.69741058\n",
      "Iteration 349, loss = 20143554546.81478119\n",
      "Iteration 350, loss = 20143550496.92065430\n",
      "Iteration 351, loss = 20143546491.05582428\n",
      "Iteration 352, loss = 20143542492.83972168\n",
      "Iteration 353, loss = 20143538479.61945724\n",
      "Iteration 354, loss = 20143534464.83714294\n",
      "Iteration 355, loss = 20143530488.70807648\n",
      "Iteration 356, loss = 20143526447.18673325\n",
      "Iteration 357, loss = 20143522457.56249619\n",
      "Iteration 358, loss = 20143518422.61612320\n",
      "Iteration 359, loss = 20143514416.68800354\n",
      "Iteration 360, loss = 20143510388.63506317\n",
      "Iteration 361, loss = 20143506369.03176498\n",
      "Iteration 362, loss = 20143502344.42646790\n",
      "Iteration 363, loss = 20143498335.22352219\n",
      "Iteration 364, loss = 20143494322.30410385\n",
      "Iteration 365, loss = 20143490316.75874329\n",
      "Iteration 366, loss = 20143486289.43591309\n",
      "Iteration 367, loss = 20143482299.60854721\n",
      "Iteration 368, loss = 20143478255.32538605\n",
      "Iteration 369, loss = 20143474255.57032394\n",
      "Iteration 370, loss = 20143470273.53123474\n",
      "Iteration 371, loss = 20143466271.45300674\n",
      "Iteration 372, loss = 20143462238.63435745\n",
      "Iteration 373, loss = 20143458226.40318298\n",
      "Iteration 374, loss = 20143454206.42620087\n",
      "Iteration 375, loss = 20143450215.32913208\n",
      "Iteration 376, loss = 20143446184.65435410\n",
      "Iteration 377, loss = 20143442154.76231766\n",
      "Iteration 378, loss = 20143438105.44610214\n",
      "Iteration 379, loss = 20143434078.63893509\n",
      "Iteration 380, loss = 20143430093.93078232\n",
      "Iteration 381, loss = 20143426017.26398849\n",
      "Iteration 382, loss = 20143422019.04478073\n",
      "Iteration 383, loss = 20143418029.24316025\n",
      "Iteration 384, loss = 20143413981.07531357\n",
      "Iteration 385, loss = 20143409954.63472748\n",
      "Iteration 386, loss = 20143405956.29721832\n",
      "Iteration 387, loss = 20143401941.10875702\n",
      "Iteration 388, loss = 20143397892.39922333\n",
      "Iteration 389, loss = 20143393898.99475861\n",
      "Iteration 390, loss = 20143389877.06844711\n",
      "Iteration 391, loss = 20143385856.89193344\n",
      "Iteration 392, loss = 20143381815.88137436\n",
      "Iteration 393, loss = 20143377803.62779999\n",
      "Iteration 394, loss = 20143373804.64318085\n",
      "Iteration 395, loss = 20143369765.94670486\n",
      "Iteration 396, loss = 20143365782.44243240\n",
      "Iteration 397, loss = 20143361735.84733200\n",
      "Iteration 398, loss = 20143357716.78862762\n",
      "Iteration 399, loss = 20143353705.04622650\n",
      "Iteration 400, loss = 20143349712.33007431\n",
      "Iteration 401, loss = 20143345687.01583862\n",
      "Iteration 402, loss = 20143341666.07431412\n",
      "Iteration 403, loss = 20143337669.23338699\n",
      "Iteration 404, loss = 20143333659.59236145\n",
      "Iteration 405, loss = 20143329611.07390213\n",
      "Iteration 406, loss = 20143325606.73824310\n",
      "Iteration 407, loss = 20143321649.16198730\n",
      "Iteration 408, loss = 20143317632.15451431\n",
      "Iteration 409, loss = 20143313620.11560059\n",
      "Iteration 410, loss = 20143309634.76229858\n",
      "Iteration 411, loss = 20143305620.89402008\n",
      "Iteration 412, loss = 20143301635.72749329\n",
      "Iteration 413, loss = 20143297647.58576584\n",
      "Iteration 414, loss = 20143293634.49209595\n",
      "Iteration 415, loss = 20143289637.25847244\n",
      "Iteration 416, loss = 20143285636.97827148\n",
      "Iteration 417, loss = 20143281658.29545975\n",
      "Iteration 418, loss = 20143277642.99794769\n",
      "Iteration 419, loss = 20143273631.72387314\n",
      "Iteration 420, loss = 20143269651.38428497\n",
      "Iteration 421, loss = 20143265632.31987762\n",
      "Iteration 422, loss = 20143261638.18595886\n",
      "Iteration 423, loss = 20143257628.02325821\n",
      "Iteration 424, loss = 20143253637.68841553\n",
      "Iteration 425, loss = 20143249617.39894104\n",
      "Iteration 426, loss = 20143245649.50217438\n",
      "Iteration 427, loss = 20143241633.18534470\n",
      "Iteration 428, loss = 20143237650.25707626\n",
      "Iteration 429, loss = 20143233649.13195419\n",
      "Iteration 430, loss = 20143229642.29943466\n",
      "Iteration 431, loss = 20143225633.94011307\n",
      "Iteration 432, loss = 20143221667.42176056\n",
      "Iteration 433, loss = 20143217639.23091125\n",
      "Iteration 434, loss = 20143213654.01903152\n",
      "Iteration 435, loss = 20143209654.50905609\n",
      "Iteration 436, loss = 20143205662.23917007\n",
      "Iteration 437, loss = 20143201636.50355530\n",
      "Iteration 438, loss = 20143197667.90571594\n",
      "Iteration 439, loss = 20143193672.24563599\n",
      "Iteration 440, loss = 20143189679.76106644\n",
      "Iteration 441, loss = 20143185666.14785004\n",
      "Iteration 442, loss = 20143181670.87053680\n",
      "Iteration 443, loss = 20143177709.39935684\n",
      "Iteration 444, loss = 20143173691.80996323\n",
      "Iteration 445, loss = 20143169662.42894745\n",
      "Iteration 446, loss = 20143165699.59318924\n",
      "Iteration 447, loss = 20143161698.28957748\n",
      "Iteration 448, loss = 20143157675.76842499\n",
      "Iteration 449, loss = 20143153662.02257156\n",
      "Iteration 450, loss = 20143149684.22247314\n",
      "Iteration 451, loss = 20143145630.04567719\n",
      "Iteration 452, loss = 20143141630.27039719\n",
      "Iteration 453, loss = 20143137623.42940140\n",
      "Iteration 454, loss = 20143133624.70465469\n",
      "Iteration 455, loss = 20143129559.51454544\n",
      "Iteration 456, loss = 20143125555.32928467\n",
      "Iteration 457, loss = 20143121527.59025192\n",
      "Iteration 458, loss = 20143117528.12229919\n",
      "Iteration 459, loss = 20143113496.83815384\n",
      "Iteration 460, loss = 20143109413.28593063\n",
      "Iteration 461, loss = 20143105331.67111588\n",
      "Iteration 462, loss = 20143101210.75245285\n",
      "Iteration 463, loss = 20143097075.60412598\n",
      "Iteration 464, loss = 20143092908.30028534\n",
      "Iteration 465, loss = 20143088632.36403656\n",
      "Iteration 466, loss = 20143084270.12360001\n",
      "Iteration 467, loss = 20143079744.26257706\n",
      "Iteration 468, loss = 20143075047.73175430\n",
      "Iteration 469, loss = 20143070105.38319778\n",
      "Iteration 470, loss = 20143065112.96027374\n",
      "Iteration 471, loss = 20143059948.79840851\n",
      "Iteration 472, loss = 20143054797.43091965\n",
      "Iteration 473, loss = 20143049631.41482544\n",
      "Iteration 474, loss = 20143044614.94369125\n",
      "Iteration 475, loss = 20143039608.56125259\n",
      "Iteration 476, loss = 20143034701.29516602\n",
      "Iteration 477, loss = 20143029822.72827148\n",
      "Iteration 478, loss = 20143025063.43262100\n",
      "Iteration 479, loss = 20143020267.76728439\n",
      "Iteration 480, loss = 20143015585.11940384\n",
      "Iteration 481, loss = 20143010853.35822678\n",
      "Iteration 482, loss = 20143006233.27914810\n",
      "Iteration 483, loss = 20143001599.00070953\n",
      "Iteration 484, loss = 20142997003.83536148\n",
      "Iteration 485, loss = 20142992412.36367035\n",
      "Iteration 486, loss = 20142987870.24270248\n",
      "Iteration 487, loss = 20142983314.44515228\n",
      "Iteration 488, loss = 20142978770.42836761\n",
      "Iteration 489, loss = 20142974276.65154648\n",
      "Iteration 490, loss = 20142969769.96831894\n",
      "Iteration 491, loss = 20142965273.12580109\n",
      "Iteration 492, loss = 20142960768.80008698\n",
      "Iteration 493, loss = 20142956313.83807755\n",
      "Iteration 494, loss = 20142951873.70590591\n",
      "Iteration 495, loss = 20142947401.37757492\n",
      "Iteration 496, loss = 20142942977.25606537\n",
      "Iteration 497, loss = 20142938539.88398361\n",
      "Iteration 498, loss = 20142934145.19153976\n",
      "Iteration 499, loss = 20142929738.80622101\n",
      "Iteration 500, loss = 20142925315.39273834\n",
      "Iteration 1, loss = 20308860777.20553589\n",
      "Iteration 2, loss = 20308844362.53430557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 20308827927.24627686\n",
      "Iteration 4, loss = 20308811536.29046249\n",
      "Iteration 5, loss = 20308794937.14836502\n",
      "Iteration 6, loss = 20308778361.96673965\n",
      "Iteration 7, loss = 20308761545.78665161\n",
      "Iteration 8, loss = 20308744237.00302505\n",
      "Iteration 9, loss = 20308726615.56316376\n",
      "Iteration 10, loss = 20308709063.77365112\n",
      "Iteration 11, loss = 20308691650.67272949\n",
      "Iteration 12, loss = 20308674558.08393097\n",
      "Iteration 13, loss = 20308656866.44031906\n",
      "Iteration 14, loss = 20308637913.22274399\n",
      "Iteration 15, loss = 20308617201.91785812\n",
      "Iteration 16, loss = 20308596354.37069321\n",
      "Iteration 17, loss = 20308576485.45679474\n",
      "Iteration 18, loss = 20308558338.84709549\n",
      "Iteration 19, loss = 20308541733.97274780\n",
      "Iteration 20, loss = 20308525775.72699356\n",
      "Iteration 21, loss = 20308509627.25214767\n",
      "Iteration 22, loss = 20308493418.82447433\n",
      "Iteration 23, loss = 20308477542.21455383\n",
      "Iteration 24, loss = 20308462889.39514923\n",
      "Iteration 25, loss = 20308449351.27023315\n",
      "Iteration 26, loss = 20308436209.46118546\n",
      "Iteration 27, loss = 20308423813.01395798\n",
      "Iteration 28, loss = 20308411475.64770508\n",
      "Iteration 29, loss = 20308399232.92790222\n",
      "Iteration 30, loss = 20308387358.49037933\n",
      "Iteration 31, loss = 20308375475.70503235\n",
      "Iteration 32, loss = 20308364124.88770294\n",
      "Iteration 33, loss = 20308353413.98046494\n",
      "Iteration 34, loss = 20308343196.18708038\n",
      "Iteration 35, loss = 20308333586.53868484\n",
      "Iteration 36, loss = 20308324535.48383713\n",
      "Iteration 37, loss = 20308315617.25999832\n",
      "Iteration 38, loss = 20308306891.68504333\n",
      "Iteration 39, loss = 20308298374.94449234\n",
      "Iteration 40, loss = 20308289983.30509949\n",
      "Iteration 41, loss = 20308281956.55928802\n",
      "Iteration 42, loss = 20308274479.12121201\n",
      "Iteration 43, loss = 20308267246.65162277\n",
      "Iteration 44, loss = 20308260080.74974442\n",
      "Iteration 45, loss = 20308252968.19112778\n",
      "Iteration 46, loss = 20308245915.21055222\n",
      "Iteration 47, loss = 20308238950.32757187\n",
      "Iteration 48, loss = 20308232025.71156311\n",
      "Iteration 49, loss = 20308225081.21049118\n",
      "Iteration 50, loss = 20308218175.18768692\n",
      "Iteration 51, loss = 20308211224.21278381\n",
      "Iteration 52, loss = 20308204278.73375702\n",
      "Iteration 53, loss = 20308197444.19557190\n",
      "Iteration 54, loss = 20308190618.03980637\n",
      "Iteration 55, loss = 20308183844.63765335\n",
      "Iteration 56, loss = 20308177199.21165085\n",
      "Iteration 57, loss = 20308170609.93496704\n",
      "Iteration 58, loss = 20308163949.17333221\n",
      "Iteration 59, loss = 20308157326.06497955\n",
      "Iteration 60, loss = 20308150751.70438004\n",
      "Iteration 61, loss = 20308144268.17899704\n",
      "Iteration 62, loss = 20308137620.85164261\n",
      "Iteration 63, loss = 20308130968.03023529\n",
      "Iteration 64, loss = 20308124416.89550018\n",
      "Iteration 65, loss = 20308117781.02754593\n",
      "Iteration 66, loss = 20308111144.15282059\n",
      "Iteration 67, loss = 20308104531.95248795\n",
      "Iteration 68, loss = 20308097882.22932053\n",
      "Iteration 69, loss = 20308091242.03118896\n",
      "Iteration 70, loss = 20308084637.10079193\n",
      "Iteration 71, loss = 20308077949.25723267\n",
      "Iteration 72, loss = 20308071306.08762360\n",
      "Iteration 73, loss = 20308064588.06173325\n",
      "Iteration 74, loss = 20308057908.34939575\n",
      "Iteration 75, loss = 20308051190.60314560\n",
      "Iteration 76, loss = 20308044436.60789108\n",
      "Iteration 77, loss = 20308037800.48448181\n",
      "Iteration 78, loss = 20308031122.50628281\n",
      "Iteration 79, loss = 20308024448.55828857\n",
      "Iteration 80, loss = 20308017707.15235901\n",
      "Iteration 81, loss = 20308010941.00442123\n",
      "Iteration 82, loss = 20308004177.67374039\n",
      "Iteration 83, loss = 20307997449.01996994\n",
      "Iteration 84, loss = 20307990749.67411423\n",
      "Iteration 85, loss = 20307984099.30953598\n",
      "Iteration 86, loss = 20307977327.75996017\n",
      "Iteration 87, loss = 20307970591.41708755\n",
      "Iteration 88, loss = 20307963928.86954880\n",
      "Iteration 89, loss = 20307957275.50680923\n",
      "Iteration 90, loss = 20307950605.20657349\n",
      "Iteration 91, loss = 20307944069.78083801\n",
      "Iteration 92, loss = 20307937464.93067551\n",
      "Iteration 93, loss = 20307931031.77539444\n",
      "Iteration 94, loss = 20307924543.23961258\n",
      "Iteration 95, loss = 20307918132.81346893\n",
      "Iteration 96, loss = 20307911762.08997345\n",
      "Iteration 97, loss = 20307905375.30771255\n",
      "Iteration 98, loss = 20307898987.69688034\n",
      "Iteration 99, loss = 20307892511.22880554\n",
      "Iteration 100, loss = 20307886093.02970505\n",
      "Iteration 101, loss = 20307879683.59477615\n",
      "Iteration 102, loss = 20307873251.06211472\n",
      "Iteration 103, loss = 20307866819.10201263\n",
      "Iteration 104, loss = 20307860432.80856323\n",
      "Iteration 105, loss = 20307854124.15419006\n",
      "Iteration 106, loss = 20307847808.33419800\n",
      "Iteration 107, loss = 20307841460.91305542\n",
      "Iteration 108, loss = 20307835156.12240982\n",
      "Iteration 109, loss = 20307828854.84486389\n",
      "Iteration 110, loss = 20307822538.09498596\n",
      "Iteration 111, loss = 20307816226.60487747\n",
      "Iteration 112, loss = 20307809891.82159042\n",
      "Iteration 113, loss = 20307803561.10418701\n",
      "Iteration 114, loss = 20307797359.77516937\n",
      "Iteration 115, loss = 20307791043.95363998\n",
      "Iteration 116, loss = 20307784799.56279755\n",
      "Iteration 117, loss = 20307778566.60710526\n",
      "Iteration 118, loss = 20307772329.71444321\n",
      "Iteration 119, loss = 20307766140.53015900\n",
      "Iteration 120, loss = 20307759908.84259415\n",
      "Iteration 121, loss = 20307753754.00617599\n",
      "Iteration 122, loss = 20307747542.89244080\n",
      "Iteration 123, loss = 20307741317.38399124\n",
      "Iteration 124, loss = 20307735165.28504562\n",
      "Iteration 125, loss = 20307729098.22201157\n",
      "Iteration 126, loss = 20307722975.79430008\n",
      "Iteration 127, loss = 20307716805.07095718\n",
      "Iteration 128, loss = 20307710638.68049240\n",
      "Iteration 129, loss = 20307704362.79046249\n",
      "Iteration 130, loss = 20307698090.23443222\n",
      "Iteration 131, loss = 20307691803.18280792\n",
      "Iteration 132, loss = 20307685545.07055664\n",
      "Iteration 133, loss = 20307679290.97922897\n",
      "Iteration 134, loss = 20307673094.11319351\n",
      "Iteration 135, loss = 20307666770.04512405\n",
      "Iteration 136, loss = 20307660306.72648239\n",
      "Iteration 137, loss = 20307653767.49122620\n",
      "Iteration 138, loss = 20307647012.92852020\n",
      "Iteration 139, loss = 20307640120.62686539\n",
      "Iteration 140, loss = 20307633086.71012878\n",
      "Iteration 141, loss = 20307626026.30904007\n",
      "Iteration 142, loss = 20307618806.02431870\n",
      "Iteration 143, loss = 20307611485.05953598\n",
      "Iteration 144, loss = 20307604150.30586243\n",
      "Iteration 145, loss = 20307596742.26701736\n",
      "Iteration 146, loss = 20307589371.74514008\n",
      "Iteration 147, loss = 20307581986.98246384\n",
      "Iteration 148, loss = 20307574620.81358719\n",
      "Iteration 149, loss = 20307567208.03607178\n",
      "Iteration 150, loss = 20307559842.84206772\n",
      "Iteration 151, loss = 20307552461.53611755\n",
      "Iteration 152, loss = 20307545140.12070465\n",
      "Iteration 153, loss = 20307537928.76478195\n",
      "Iteration 154, loss = 20307530864.75984573\n",
      "Iteration 155, loss = 20307523811.52904510\n",
      "Iteration 156, loss = 20307516790.43753433\n",
      "Iteration 157, loss = 20307509738.29219437\n",
      "Iteration 158, loss = 20307502744.03596497\n",
      "Iteration 159, loss = 20307495749.28413010\n",
      "Iteration 160, loss = 20307488793.19646072\n",
      "Iteration 161, loss = 20307481942.00008011\n",
      "Iteration 162, loss = 20307475025.78062057\n",
      "Iteration 163, loss = 20307468163.56312943\n",
      "Iteration 164, loss = 20307461332.27243042\n",
      "Iteration 165, loss = 20307454543.93132782\n",
      "Iteration 166, loss = 20307447827.94305038\n",
      "Iteration 167, loss = 20307441188.71490479\n",
      "Iteration 168, loss = 20307434433.81783295\n",
      "Iteration 169, loss = 20307427710.60680771\n",
      "Iteration 170, loss = 20307420913.45384598\n",
      "Iteration 171, loss = 20307414151.17905045\n",
      "Iteration 172, loss = 20307407421.76374054\n",
      "Iteration 173, loss = 20307400691.36023712\n",
      "Iteration 174, loss = 20307394004.53697205\n",
      "Iteration 175, loss = 20307387326.61126709\n",
      "Iteration 176, loss = 20307380645.70924377\n",
      "Iteration 177, loss = 20307373967.17423630\n",
      "Iteration 178, loss = 20307367255.14710236\n",
      "Iteration 179, loss = 20307360568.33721542\n",
      "Iteration 180, loss = 20307353813.97313309\n",
      "Iteration 181, loss = 20307347106.51179886\n",
      "Iteration 182, loss = 20307340388.39785004\n",
      "Iteration 183, loss = 20307333781.61613464\n",
      "Iteration 184, loss = 20307327186.84911728\n",
      "Iteration 185, loss = 20307320712.79613495\n",
      "Iteration 186, loss = 20307314160.69731140\n",
      "Iteration 187, loss = 20307307575.34455872\n",
      "Iteration 188, loss = 20307301056.54923248\n",
      "Iteration 189, loss = 20307294513.45015717\n",
      "Iteration 190, loss = 20307287957.37913132\n",
      "Iteration 191, loss = 20307281496.78103638\n",
      "Iteration 192, loss = 20307275044.18870544\n",
      "Iteration 193, loss = 20307268529.55874634\n",
      "Iteration 194, loss = 20307262079.68521118\n",
      "Iteration 195, loss = 20307255544.64755630\n",
      "Iteration 196, loss = 20307249076.15082550\n",
      "Iteration 197, loss = 20307242495.06086349\n",
      "Iteration 198, loss = 20307235862.73133469\n",
      "Iteration 199, loss = 20307229200.17338943\n",
      "Iteration 200, loss = 20307222396.14466858\n",
      "Iteration 201, loss = 20307215385.91712570\n",
      "Iteration 202, loss = 20307208244.35334015\n",
      "Iteration 203, loss = 20307200949.51192856\n",
      "Iteration 204, loss = 20307193523.37039185\n",
      "Iteration 205, loss = 20307186010.47844315\n",
      "Iteration 206, loss = 20307178523.33870316\n",
      "Iteration 207, loss = 20307171123.17335892\n",
      "Iteration 208, loss = 20307163687.70032883\n",
      "Iteration 209, loss = 20307156364.35337830\n",
      "Iteration 210, loss = 20307149145.42285156\n",
      "Iteration 211, loss = 20307141903.15338516\n",
      "Iteration 212, loss = 20307134762.94012833\n",
      "Iteration 213, loss = 20307127690.27245331\n",
      "Iteration 214, loss = 20307120560.88469696\n",
      "Iteration 215, loss = 20307113560.34123230\n",
      "Iteration 216, loss = 20307106549.49654007\n",
      "Iteration 217, loss = 20307099485.49325180\n",
      "Iteration 218, loss = 20307092471.48198700\n",
      "Iteration 219, loss = 20307085352.71974564\n",
      "Iteration 220, loss = 20307078304.41947174\n",
      "Iteration 221, loss = 20307071319.14476776\n",
      "Iteration 222, loss = 20307064451.08444977\n",
      "Iteration 223, loss = 20307057572.31821442\n",
      "Iteration 224, loss = 20307050655.74516296\n",
      "Iteration 225, loss = 20307043746.28367233\n",
      "Iteration 226, loss = 20307036852.95112228\n",
      "Iteration 227, loss = 20307029851.83846283\n",
      "Iteration 228, loss = 20307022823.83890533\n",
      "Iteration 229, loss = 20307015804.36075974\n",
      "Iteration 230, loss = 20307008742.71699524\n",
      "Iteration 231, loss = 20307001745.80521774\n",
      "Iteration 232, loss = 20306994597.35160828\n",
      "Iteration 233, loss = 20306987333.22071457\n",
      "Iteration 234, loss = 20306980101.75184631\n",
      "Iteration 235, loss = 20306972709.62788010\n",
      "Iteration 236, loss = 20306965225.56581497\n",
      "Iteration 237, loss = 20306957585.56763077\n",
      "Iteration 238, loss = 20306950032.95093918\n",
      "Iteration 239, loss = 20306942551.39576340\n",
      "Iteration 240, loss = 20306934981.25492859\n",
      "Iteration 241, loss = 20306927421.00481033\n",
      "Iteration 242, loss = 20306919978.31969070\n",
      "Iteration 243, loss = 20306912487.59222794\n",
      "Iteration 244, loss = 20306905052.59968948\n",
      "Iteration 245, loss = 20306897635.59422684\n",
      "Iteration 246, loss = 20306890366.60957336\n",
      "Iteration 247, loss = 20306883078.25463104\n",
      "Iteration 248, loss = 20306875664.99024963\n",
      "Iteration 249, loss = 20306868343.22011185\n",
      "Iteration 250, loss = 20306861007.28131104\n",
      "Iteration 251, loss = 20306853736.16674805\n",
      "Iteration 252, loss = 20306846642.16796112\n",
      "Iteration 253, loss = 20306839422.15038681\n",
      "Iteration 254, loss = 20306832268.34381104\n",
      "Iteration 255, loss = 20306825139.01725006\n",
      "Iteration 256, loss = 20306818040.02669907\n",
      "Iteration 257, loss = 20306810920.32480621\n",
      "Iteration 258, loss = 20306803924.22386169\n",
      "Iteration 259, loss = 20306796880.03382492\n",
      "Iteration 260, loss = 20306789845.86865234\n",
      "Iteration 261, loss = 20306782855.56607056\n",
      "Iteration 262, loss = 20306775829.58886337\n",
      "Iteration 263, loss = 20306768913.30570984\n",
      "Iteration 264, loss = 20306761927.17965698\n",
      "Iteration 265, loss = 20306755030.97686768\n",
      "Iteration 266, loss = 20306748063.38688660\n",
      "Iteration 267, loss = 20306741096.21085739\n",
      "Iteration 268, loss = 20306734108.86054993\n",
      "Iteration 269, loss = 20306727222.79954147\n",
      "Iteration 270, loss = 20306720189.33911514\n",
      "Iteration 271, loss = 20306713249.44942856\n",
      "Iteration 272, loss = 20306706382.35278702\n",
      "Iteration 273, loss = 20306699477.35478592\n",
      "Iteration 274, loss = 20306692636.00053787\n",
      "Iteration 275, loss = 20306685717.09797287\n",
      "Iteration 276, loss = 20306678878.99749756\n",
      "Iteration 277, loss = 20306672069.57435608\n",
      "Iteration 278, loss = 20306665218.59461975\n",
      "Iteration 279, loss = 20306658305.78000641\n",
      "Iteration 280, loss = 20306651494.26123047\n",
      "Iteration 281, loss = 20306644562.27926254\n",
      "Iteration 282, loss = 20306637551.53799057\n",
      "Iteration 283, loss = 20306630655.55642700\n",
      "Iteration 284, loss = 20306623804.42357254\n",
      "Iteration 285, loss = 20306616942.08748627\n",
      "Iteration 286, loss = 20306610115.73963165\n",
      "Iteration 287, loss = 20306603307.50396347\n",
      "Iteration 288, loss = 20306596462.91441727\n",
      "Iteration 289, loss = 20306589634.89076996\n",
      "Iteration 290, loss = 20306582866.36027527\n",
      "Iteration 291, loss = 20306576074.18999863\n",
      "Iteration 292, loss = 20306569388.00310135\n",
      "Iteration 293, loss = 20306562639.87881470\n",
      "Iteration 294, loss = 20306555874.53807449\n",
      "Iteration 295, loss = 20306549089.97756577\n",
      "Iteration 296, loss = 20306542317.71224213\n",
      "Iteration 297, loss = 20306535499.13425827\n",
      "Iteration 298, loss = 20306528776.12833405\n",
      "Iteration 299, loss = 20306521951.02424240\n",
      "Iteration 300, loss = 20306515128.78123093\n",
      "Iteration 301, loss = 20306508285.71837234\n",
      "Iteration 302, loss = 20306501427.97480774\n",
      "Iteration 303, loss = 20306494612.08406830\n",
      "Iteration 304, loss = 20306487720.69931030\n",
      "Iteration 305, loss = 20306480927.24654007\n",
      "Iteration 306, loss = 20306474050.30133438\n",
      "Iteration 307, loss = 20306467271.83199310\n",
      "Iteration 308, loss = 20306460477.25750351\n",
      "Iteration 309, loss = 20306453778.03961182\n",
      "Iteration 310, loss = 20306447018.00850296\n",
      "Iteration 311, loss = 20306440351.05281448\n",
      "Iteration 312, loss = 20306433730.16553497\n",
      "Iteration 313, loss = 20306427161.37611389\n",
      "Iteration 314, loss = 20306420494.85155869\n",
      "Iteration 315, loss = 20306413813.03863144\n",
      "Iteration 316, loss = 20306407126.59399796\n",
      "Iteration 317, loss = 20306400353.77753448\n",
      "Iteration 318, loss = 20306393499.64876175\n",
      "Iteration 319, loss = 20306386745.27445221\n",
      "Iteration 320, loss = 20306379952.69030762\n",
      "Iteration 321, loss = 20306373123.99085617\n",
      "Iteration 322, loss = 20306366314.87805939\n",
      "Iteration 323, loss = 20306359534.92205811\n",
      "Iteration 324, loss = 20306352677.31232834\n",
      "Iteration 325, loss = 20306345777.70842361\n",
      "Iteration 326, loss = 20306338939.03789902\n",
      "Iteration 327, loss = 20306332184.85170364\n",
      "Iteration 328, loss = 20306325454.06338501\n",
      "Iteration 329, loss = 20306318725.62862778\n",
      "Iteration 330, loss = 20306312043.09395981\n",
      "Iteration 331, loss = 20306305425.99163818\n",
      "Iteration 332, loss = 20306298810.43540573\n",
      "Iteration 333, loss = 20306292183.91261673\n",
      "Iteration 334, loss = 20306285504.41199493\n",
      "Iteration 335, loss = 20306278780.11416626\n",
      "Iteration 336, loss = 20306272065.09704971\n",
      "Iteration 337, loss = 20306265400.13581467\n",
      "Iteration 338, loss = 20306258693.98925018\n",
      "Iteration 339, loss = 20306252052.40546036\n",
      "Iteration 340, loss = 20306245396.67160416\n",
      "Iteration 341, loss = 20306238665.27613831\n",
      "Iteration 342, loss = 20306231871.77727127\n",
      "Iteration 343, loss = 20306225180.51886749\n",
      "Iteration 344, loss = 20306218511.74409103\n",
      "Iteration 345, loss = 20306211830.36330414\n",
      "Iteration 346, loss = 20306205153.14913940\n",
      "Iteration 347, loss = 20306198496.72348022\n",
      "Iteration 348, loss = 20306191884.74699402\n",
      "Iteration 349, loss = 20306185206.76914597\n",
      "Iteration 350, loss = 20306178553.38429260\n",
      "Iteration 351, loss = 20306172004.14217758\n",
      "Iteration 352, loss = 20306165355.45682144\n",
      "Iteration 353, loss = 20306158724.45152283\n",
      "Iteration 354, loss = 20306152081.75947571\n",
      "Iteration 355, loss = 20306145465.65834045\n",
      "Iteration 356, loss = 20306138833.93485260\n",
      "Iteration 357, loss = 20306132251.05596542\n",
      "Iteration 358, loss = 20306125616.77874374\n",
      "Iteration 359, loss = 20306119034.99264908\n",
      "Iteration 360, loss = 20306112388.00956726\n",
      "Iteration 361, loss = 20306105749.24007034\n",
      "Iteration 362, loss = 20306099107.96383667\n",
      "Iteration 363, loss = 20306092481.47517776\n",
      "Iteration 364, loss = 20306085841.27190781\n",
      "Iteration 365, loss = 20306079212.15533829\n",
      "Iteration 366, loss = 20306072623.24236298\n",
      "Iteration 367, loss = 20306065988.74170303\n",
      "Iteration 368, loss = 20306059384.25043106\n",
      "Iteration 369, loss = 20306052730.79600143\n",
      "Iteration 370, loss = 20306046116.91396332\n",
      "Iteration 371, loss = 20306039408.94307709\n",
      "Iteration 372, loss = 20306032771.19532394\n",
      "Iteration 373, loss = 20306026151.40444183\n",
      "Iteration 374, loss = 20306019611.91947556\n",
      "Iteration 375, loss = 20306013010.63227844\n",
      "Iteration 376, loss = 20306006419.66207123\n",
      "Iteration 377, loss = 20305999849.80415726\n",
      "Iteration 378, loss = 20305993163.83140564\n",
      "Iteration 379, loss = 20305986434.64532089\n",
      "Iteration 380, loss = 20305979752.93550110\n",
      "Iteration 381, loss = 20305973132.09468842\n",
      "Iteration 382, loss = 20305966504.52901077\n",
      "Iteration 383, loss = 20305959918.40068817\n",
      "Iteration 384, loss = 20305953283.74515533\n",
      "Iteration 385, loss = 20305946584.31519699\n",
      "Iteration 386, loss = 20305939978.27293396\n",
      "Iteration 387, loss = 20305933299.68505096\n",
      "Iteration 388, loss = 20305926659.61231613\n",
      "Iteration 389, loss = 20305920035.41771317\n",
      "Iteration 390, loss = 20305913491.80794907\n",
      "Iteration 391, loss = 20305906856.39038086\n",
      "Iteration 392, loss = 20305900224.34568787\n",
      "Iteration 393, loss = 20305893572.02900696\n",
      "Iteration 394, loss = 20305886987.85266113\n",
      "Iteration 395, loss = 20305880352.51933289\n",
      "Iteration 396, loss = 20305873655.34955597\n",
      "Iteration 397, loss = 20305866971.41231537\n",
      "Iteration 398, loss = 20305860318.65629196\n",
      "Iteration 399, loss = 20305853666.84534073\n",
      "Iteration 400, loss = 20305847057.30296707\n",
      "Iteration 401, loss = 20305840458.95409393\n",
      "Iteration 402, loss = 20305833961.84238815\n",
      "Iteration 403, loss = 20305827406.08662033\n",
      "Iteration 404, loss = 20305820907.66476440\n",
      "Iteration 405, loss = 20305814329.57443237\n",
      "Iteration 406, loss = 20305807801.71964264\n",
      "Iteration 407, loss = 20305801135.08218002\n",
      "Iteration 408, loss = 20305794522.58214569\n",
      "Iteration 409, loss = 20305787861.81879044\n",
      "Iteration 410, loss = 20305781250.23258972\n",
      "Iteration 411, loss = 20305774660.12369537\n",
      "Iteration 412, loss = 20305768095.91201019\n",
      "Iteration 413, loss = 20305761555.45162964\n",
      "Iteration 414, loss = 20305755013.10361481\n",
      "Iteration 415, loss = 20305748498.48885345\n",
      "Iteration 416, loss = 20305742003.84480667\n",
      "Iteration 417, loss = 20305735490.04472733\n",
      "Iteration 418, loss = 20305728966.33689880\n",
      "Iteration 419, loss = 20305722423.64720535\n",
      "Iteration 420, loss = 20305715864.08279037\n",
      "Iteration 421, loss = 20305709317.95725250\n",
      "Iteration 422, loss = 20305702730.94534302\n",
      "Iteration 423, loss = 20305696117.26099777\n",
      "Iteration 424, loss = 20305689538.89144135\n",
      "Iteration 425, loss = 20305682909.12732697\n",
      "Iteration 426, loss = 20305676339.17525482\n",
      "Iteration 427, loss = 20305669678.45316696\n",
      "Iteration 428, loss = 20305663062.36870575\n",
      "Iteration 429, loss = 20305656427.37030792\n",
      "Iteration 430, loss = 20305649858.04664230\n",
      "Iteration 431, loss = 20305643328.57580185\n",
      "Iteration 432, loss = 20305636782.33991623\n",
      "Iteration 433, loss = 20305630260.31687164\n",
      "Iteration 434, loss = 20305623800.97087097\n",
      "Iteration 435, loss = 20305617208.28987122\n",
      "Iteration 436, loss = 20305610626.36624908\n",
      "Iteration 437, loss = 20305604068.18197632\n",
      "Iteration 438, loss = 20305597437.26639175\n",
      "Iteration 439, loss = 20305590790.41891861\n",
      "Iteration 440, loss = 20305584136.37184525\n",
      "Iteration 441, loss = 20305577543.13660431\n",
      "Iteration 442, loss = 20305570963.20412064\n",
      "Iteration 443, loss = 20305564446.15334702\n",
      "Iteration 444, loss = 20305557973.55359268\n",
      "Iteration 445, loss = 20305551424.55451965\n",
      "Iteration 446, loss = 20305544934.16540909\n",
      "Iteration 447, loss = 20305538385.92014313\n",
      "Iteration 448, loss = 20305531841.21232605\n",
      "Iteration 449, loss = 20305525344.89695358\n",
      "Iteration 450, loss = 20305518762.48230362\n",
      "Iteration 451, loss = 20305512219.53861618\n",
      "Iteration 452, loss = 20305505659.39910507\n",
      "Iteration 453, loss = 20305499162.30420303\n",
      "Iteration 454, loss = 20305492666.75964355\n",
      "Iteration 455, loss = 20305486242.29879761\n",
      "Iteration 456, loss = 20305479805.60568237\n",
      "Iteration 457, loss = 20305473379.83060455\n",
      "Iteration 458, loss = 20305466996.72569656\n",
      "Iteration 459, loss = 20305460562.01498795\n",
      "Iteration 460, loss = 20305454104.30314255\n",
      "Iteration 461, loss = 20305447592.62048340\n",
      "Iteration 462, loss = 20305440983.14425659\n",
      "Iteration 463, loss = 20305434389.62783432\n",
      "Iteration 464, loss = 20305427795.50985336\n",
      "Iteration 465, loss = 20305421138.54695892\n",
      "Iteration 466, loss = 20305414499.44092941\n",
      "Iteration 467, loss = 20305407926.27178574\n",
      "Iteration 468, loss = 20305401288.63699341\n",
      "Iteration 469, loss = 20305394687.25017166\n",
      "Iteration 470, loss = 20305388180.62694550\n",
      "Iteration 471, loss = 20305381681.04108047\n",
      "Iteration 472, loss = 20305375227.33795166\n",
      "Iteration 473, loss = 20305368729.19445801\n",
      "Iteration 474, loss = 20305362256.61902618\n",
      "Iteration 475, loss = 20305355740.63978195\n",
      "Iteration 476, loss = 20305349231.85242844\n",
      "Iteration 477, loss = 20305342687.98873520\n",
      "Iteration 478, loss = 20305336088.35850525\n",
      "Iteration 479, loss = 20305329452.07863235\n",
      "Iteration 480, loss = 20305322881.48262024\n",
      "Iteration 481, loss = 20305316346.84664917\n",
      "Iteration 482, loss = 20305309803.82056427\n",
      "Iteration 483, loss = 20305303343.38957214\n",
      "Iteration 484, loss = 20305296831.01946640\n",
      "Iteration 485, loss = 20305290313.37088776\n",
      "Iteration 486, loss = 20305283878.56406021\n",
      "Iteration 487, loss = 20305277354.29913330\n",
      "Iteration 488, loss = 20305270822.86193085\n",
      "Iteration 489, loss = 20305264378.31453323\n",
      "Iteration 490, loss = 20305257897.62709427\n",
      "Iteration 491, loss = 20305251446.09330368\n",
      "Iteration 492, loss = 20305244953.00207901\n",
      "Iteration 493, loss = 20305238465.18368530\n",
      "Iteration 494, loss = 20305231986.39597702\n",
      "Iteration 495, loss = 20305225525.58422470\n",
      "Iteration 496, loss = 20305219132.36820221\n",
      "Iteration 497, loss = 20305212626.43172073\n",
      "Iteration 498, loss = 20305206157.01752472\n",
      "Iteration 499, loss = 20305199671.35424805\n",
      "Iteration 500, loss = 20305193298.10081482\n",
      "Iteration 1, loss = 20261225705.24296188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 20261207521.53529358\n",
      "Iteration 3, loss = 20261188936.54261398\n",
      "Iteration 4, loss = 20261169736.70622635\n",
      "Iteration 5, loss = 20261150485.63843536\n",
      "Iteration 6, loss = 20261130203.19825745\n",
      "Iteration 7, loss = 20261109443.11462784\n",
      "Iteration 8, loss = 20261088615.09953308\n",
      "Iteration 9, loss = 20261067011.92186737\n",
      "Iteration 10, loss = 20261045253.49397659\n",
      "Iteration 11, loss = 20261022314.69832611\n",
      "Iteration 12, loss = 20260998491.46257019\n",
      "Iteration 13, loss = 20260974237.16919708\n",
      "Iteration 14, loss = 20260950969.95749664\n",
      "Iteration 15, loss = 20260929185.18481445\n",
      "Iteration 16, loss = 20260906854.47687149\n",
      "Iteration 17, loss = 20260882809.32830048\n",
      "Iteration 18, loss = 20260858000.97673035\n",
      "Iteration 19, loss = 20260832771.96105957\n",
      "Iteration 20, loss = 20260806667.86646652\n",
      "Iteration 21, loss = 20260779935.49831009\n",
      "Iteration 22, loss = 20260753161.77500534\n",
      "Iteration 23, loss = 20260727731.85704422\n",
      "Iteration 24, loss = 20260705338.26982117\n",
      "Iteration 25, loss = 20260685794.03341293\n",
      "Iteration 26, loss = 20260667883.25898361\n",
      "Iteration 27, loss = 20260650999.14287949\n",
      "Iteration 28, loss = 20260634047.33266449\n",
      "Iteration 29, loss = 20260617508.94377518\n",
      "Iteration 30, loss = 20260600260.89715195\n",
      "Iteration 31, loss = 20260583434.51702118\n",
      "Iteration 32, loss = 20260567779.60807037\n",
      "Iteration 33, loss = 20260552179.60751724\n",
      "Iteration 34, loss = 20260537820.09849930\n",
      "Iteration 35, loss = 20260524964.50661469\n",
      "Iteration 36, loss = 20260512823.07694244\n",
      "Iteration 37, loss = 20260501553.63068771\n",
      "Iteration 38, loss = 20260490598.23243713\n",
      "Iteration 39, loss = 20260479908.63204193\n",
      "Iteration 40, loss = 20260469973.08619690\n",
      "Iteration 41, loss = 20260460637.95070267\n",
      "Iteration 42, loss = 20260451574.70269394\n",
      "Iteration 43, loss = 20260442502.14127350\n",
      "Iteration 44, loss = 20260433604.27133942\n",
      "Iteration 45, loss = 20260424648.34881592\n",
      "Iteration 46, loss = 20260415432.89948273\n",
      "Iteration 47, loss = 20260406404.78585815\n",
      "Iteration 48, loss = 20260397786.59436035\n",
      "Iteration 49, loss = 20260389204.23426056\n",
      "Iteration 50, loss = 20260380856.07242966\n",
      "Iteration 51, loss = 20260372532.78729630\n",
      "Iteration 52, loss = 20260364312.30474091\n",
      "Iteration 53, loss = 20260356116.48035431\n",
      "Iteration 54, loss = 20260347958.19050598\n",
      "Iteration 55, loss = 20260339936.23372269\n",
      "Iteration 56, loss = 20260332013.91012955\n",
      "Iteration 57, loss = 20260324098.69588470\n",
      "Iteration 58, loss = 20260316204.98461914\n",
      "Iteration 59, loss = 20260308381.33627319\n",
      "Iteration 60, loss = 20260300558.40862274\n",
      "Iteration 61, loss = 20260292701.15364838\n",
      "Iteration 62, loss = 20260284742.19559097\n",
      "Iteration 63, loss = 20260276855.98927689\n",
      "Iteration 64, loss = 20260268999.16026688\n",
      "Iteration 65, loss = 20260261426.76439285\n",
      "Iteration 66, loss = 20260253893.03353500\n",
      "Iteration 67, loss = 20260246335.46563721\n",
      "Iteration 68, loss = 20260238913.68841171\n",
      "Iteration 69, loss = 20260231531.25712967\n",
      "Iteration 70, loss = 20260224095.50349045\n",
      "Iteration 71, loss = 20260216786.08494568\n",
      "Iteration 72, loss = 20260209481.50964355\n",
      "Iteration 73, loss = 20260202201.49222183\n",
      "Iteration 74, loss = 20260194882.75970840\n",
      "Iteration 75, loss = 20260187621.88810730\n",
      "Iteration 76, loss = 20260180375.53011322\n",
      "Iteration 77, loss = 20260173015.75051880\n",
      "Iteration 78, loss = 20260165800.89162064\n",
      "Iteration 79, loss = 20260158629.18599701\n",
      "Iteration 80, loss = 20260151452.63793945\n",
      "Iteration 81, loss = 20260144309.21574020\n",
      "Iteration 82, loss = 20260137220.09317398\n",
      "Iteration 83, loss = 20260130147.34685898\n",
      "Iteration 84, loss = 20260123060.50662613\n",
      "Iteration 85, loss = 20260116072.13023758\n",
      "Iteration 86, loss = 20260109025.97776413\n",
      "Iteration 87, loss = 20260101909.33885193\n",
      "Iteration 88, loss = 20260094987.91529083\n",
      "Iteration 89, loss = 20260088020.15955353\n",
      "Iteration 90, loss = 20260081050.49453735\n",
      "Iteration 91, loss = 20260074050.01316833\n",
      "Iteration 92, loss = 20260067158.61933136\n",
      "Iteration 93, loss = 20260060202.79684448\n",
      "Iteration 94, loss = 20260053271.02841187\n",
      "Iteration 95, loss = 20260046320.98178482\n",
      "Iteration 96, loss = 20260039411.73301697\n",
      "Iteration 97, loss = 20260032492.19639206\n",
      "Iteration 98, loss = 20260025599.47862625\n",
      "Iteration 99, loss = 20260018702.00425339\n",
      "Iteration 100, loss = 20260011809.52780533\n",
      "Iteration 101, loss = 20260004991.38096237\n",
      "Iteration 102, loss = 20259998152.12581635\n",
      "Iteration 103, loss = 20259991353.66549301\n",
      "Iteration 104, loss = 20259984529.43502426\n",
      "Iteration 105, loss = 20259977658.89564514\n",
      "Iteration 106, loss = 20259970923.75550461\n",
      "Iteration 107, loss = 20259964100.00714874\n",
      "Iteration 108, loss = 20259957343.27759171\n",
      "Iteration 109, loss = 20259950565.17600250\n",
      "Iteration 110, loss = 20259943799.03482819\n",
      "Iteration 111, loss = 20259937091.82423782\n",
      "Iteration 112, loss = 20259930303.00982285\n",
      "Iteration 113, loss = 20259923594.59590530\n",
      "Iteration 114, loss = 20259916838.69707489\n",
      "Iteration 115, loss = 20259910143.63356400\n",
      "Iteration 116, loss = 20259903422.36613464\n",
      "Iteration 117, loss = 20259896773.32603836\n",
      "Iteration 118, loss = 20259890024.72076416\n",
      "Iteration 119, loss = 20259883342.65060425\n",
      "Iteration 120, loss = 20259876649.74341583\n",
      "Iteration 121, loss = 20259869976.38986969\n",
      "Iteration 122, loss = 20259863302.01329041\n",
      "Iteration 123, loss = 20259856637.53675461\n",
      "Iteration 124, loss = 20259849978.27717972\n",
      "Iteration 125, loss = 20259843338.81657028\n",
      "Iteration 126, loss = 20259836657.65077972\n",
      "Iteration 127, loss = 20259830038.47464371\n",
      "Iteration 128, loss = 20259823396.54558563\n",
      "Iteration 129, loss = 20259816775.18421936\n",
      "Iteration 130, loss = 20259810144.33382034\n",
      "Iteration 131, loss = 20259803477.44604111\n",
      "Iteration 132, loss = 20259796871.61760330\n",
      "Iteration 133, loss = 20259790237.47597885\n",
      "Iteration 134, loss = 20259783563.32020187\n",
      "Iteration 135, loss = 20259776972.87860107\n",
      "Iteration 136, loss = 20259770371.52575302\n",
      "Iteration 137, loss = 20259763735.59884262\n",
      "Iteration 138, loss = 20259757188.55823517\n",
      "Iteration 139, loss = 20259750564.89305878\n",
      "Iteration 140, loss = 20259743976.51367188\n",
      "Iteration 141, loss = 20259737452.93106461\n",
      "Iteration 142, loss = 20259730798.63065720\n",
      "Iteration 143, loss = 20259724282.64320374\n",
      "Iteration 144, loss = 20259717703.88330078\n",
      "Iteration 145, loss = 20259711063.97873688\n",
      "Iteration 146, loss = 20259704543.00203323\n",
      "Iteration 147, loss = 20259697941.37946701\n",
      "Iteration 148, loss = 20259691302.45783615\n",
      "Iteration 149, loss = 20259684734.40571213\n",
      "Iteration 150, loss = 20259678134.55887985\n",
      "Iteration 151, loss = 20259671619.42039490\n",
      "Iteration 152, loss = 20259665052.80556870\n",
      "Iteration 153, loss = 20259658437.04291916\n",
      "Iteration 154, loss = 20259651949.42345428\n",
      "Iteration 155, loss = 20259645392.22336578\n",
      "Iteration 156, loss = 20259638786.13966370\n",
      "Iteration 157, loss = 20259632294.71144867\n",
      "Iteration 158, loss = 20259625720.30004501\n",
      "Iteration 159, loss = 20259619239.67414093\n",
      "Iteration 160, loss = 20259612675.87485886\n",
      "Iteration 161, loss = 20259606151.48563004\n",
      "Iteration 162, loss = 20259599641.37078094\n",
      "Iteration 163, loss = 20259593155.41149139\n",
      "Iteration 164, loss = 20259586600.47520447\n",
      "Iteration 165, loss = 20259580152.48932266\n",
      "Iteration 166, loss = 20259573581.31308365\n",
      "Iteration 167, loss = 20259567103.89414597\n",
      "Iteration 168, loss = 20259560580.65299225\n",
      "Iteration 169, loss = 20259554076.39982605\n",
      "Iteration 170, loss = 20259547573.60638428\n",
      "Iteration 171, loss = 20259541080.02607346\n",
      "Iteration 172, loss = 20259534583.89327621\n",
      "Iteration 173, loss = 20259528105.49438858\n",
      "Iteration 174, loss = 20259521578.25288010\n",
      "Iteration 175, loss = 20259515076.34354019\n",
      "Iteration 176, loss = 20259508600.62847137\n",
      "Iteration 177, loss = 20259502145.43401337\n",
      "Iteration 178, loss = 20259495659.12884903\n",
      "Iteration 179, loss = 20259489198.21102524\n",
      "Iteration 180, loss = 20259482738.76261902\n",
      "Iteration 181, loss = 20259476279.07696152\n",
      "Iteration 182, loss = 20259469817.39253616\n",
      "Iteration 183, loss = 20259463337.52473068\n",
      "Iteration 184, loss = 20259456906.63750839\n",
      "Iteration 185, loss = 20259450442.88180923\n",
      "Iteration 186, loss = 20259443996.98171234\n",
      "Iteration 187, loss = 20259437568.99348831\n",
      "Iteration 188, loss = 20259431090.55610657\n",
      "Iteration 189, loss = 20259424585.35593796\n",
      "Iteration 190, loss = 20259418184.94561005\n",
      "Iteration 191, loss = 20259411725.30607605\n",
      "Iteration 192, loss = 20259405314.20246124\n",
      "Iteration 193, loss = 20259398881.79689026\n",
      "Iteration 194, loss = 20259392426.25699615\n",
      "Iteration 195, loss = 20259385997.16631699\n",
      "Iteration 196, loss = 20259379545.05511856\n",
      "Iteration 197, loss = 20259373081.99521255\n",
      "Iteration 198, loss = 20259366700.75691605\n",
      "Iteration 199, loss = 20259360248.36604691\n",
      "Iteration 200, loss = 20259353777.94142532\n",
      "Iteration 201, loss = 20259347355.21002960\n",
      "Iteration 202, loss = 20259340932.15721130\n",
      "Iteration 203, loss = 20259334491.92380905\n",
      "Iteration 204, loss = 20259328045.25772095\n",
      "Iteration 205, loss = 20259321653.17309189\n",
      "Iteration 206, loss = 20259315231.29045486\n",
      "Iteration 207, loss = 20259308774.30540848\n",
      "Iteration 208, loss = 20259302376.77283478\n",
      "Iteration 209, loss = 20259295965.28040695\n",
      "Iteration 210, loss = 20259289548.08528900\n",
      "Iteration 211, loss = 20259283071.48317337\n",
      "Iteration 212, loss = 20259276701.35409164\n",
      "Iteration 213, loss = 20259270306.58742905\n",
      "Iteration 214, loss = 20259263893.31183624\n",
      "Iteration 215, loss = 20259257483.01252365\n",
      "Iteration 216, loss = 20259250990.91040421\n",
      "Iteration 217, loss = 20259244688.27642822\n",
      "Iteration 218, loss = 20259238268.88102722\n",
      "Iteration 219, loss = 20259231875.80631256\n",
      "Iteration 220, loss = 20259225467.44337845\n",
      "Iteration 221, loss = 20259219099.17397690\n",
      "Iteration 222, loss = 20259212648.63851547\n",
      "Iteration 223, loss = 20259206260.15932083\n",
      "Iteration 224, loss = 20259199854.08226395\n",
      "Iteration 225, loss = 20259193482.63971710\n",
      "Iteration 226, loss = 20259187107.50508881\n",
      "Iteration 227, loss = 20259180699.98277283\n",
      "Iteration 228, loss = 20259174350.03944016\n",
      "Iteration 229, loss = 20259167944.07326889\n",
      "Iteration 230, loss = 20259161547.53210449\n",
      "Iteration 231, loss = 20259155199.48438644\n",
      "Iteration 232, loss = 20259148787.57810974\n",
      "Iteration 233, loss = 20259142419.62374115\n",
      "Iteration 234, loss = 20259136034.89898682\n",
      "Iteration 235, loss = 20259129654.74298096\n",
      "Iteration 236, loss = 20259123291.92025375\n",
      "Iteration 237, loss = 20259116885.51055145\n",
      "Iteration 238, loss = 20259110488.07452393\n",
      "Iteration 239, loss = 20259104115.28138351\n",
      "Iteration 240, loss = 20259097741.04175568\n",
      "Iteration 241, loss = 20259091325.60033417\n",
      "Iteration 242, loss = 20259084919.48281097\n",
      "Iteration 243, loss = 20259078566.18020630\n",
      "Iteration 244, loss = 20259072197.04695129\n",
      "Iteration 245, loss = 20259065828.50725555\n",
      "Iteration 246, loss = 20259059424.57284164\n",
      "Iteration 247, loss = 20259053067.29607010\n",
      "Iteration 248, loss = 20259046674.86233521\n",
      "Iteration 249, loss = 20259040330.33434296\n",
      "Iteration 250, loss = 20259033957.80316925\n",
      "Iteration 251, loss = 20259027565.38835526\n",
      "Iteration 252, loss = 20259021203.52824402\n",
      "Iteration 253, loss = 20259014806.57038879\n",
      "Iteration 254, loss = 20259008476.72760010\n",
      "Iteration 255, loss = 20259002083.81586075\n",
      "Iteration 256, loss = 20258995760.10213089\n",
      "Iteration 257, loss = 20258989380.56958008\n",
      "Iteration 258, loss = 20258982978.14626312\n",
      "Iteration 259, loss = 20258976689.31781006\n",
      "Iteration 260, loss = 20258970288.50669098\n",
      "Iteration 261, loss = 20258963915.46918869\n",
      "Iteration 262, loss = 20258957582.97537231\n",
      "Iteration 263, loss = 20258951192.32970428\n",
      "Iteration 264, loss = 20258944869.92004395\n",
      "Iteration 265, loss = 20258938525.14156342\n",
      "Iteration 266, loss = 20258932192.75957870\n",
      "Iteration 267, loss = 20258925822.84730911\n",
      "Iteration 268, loss = 20258919460.45856094\n",
      "Iteration 269, loss = 20258913134.84748077\n",
      "Iteration 270, loss = 20258906786.39596558\n",
      "Iteration 271, loss = 20258900374.56948471\n",
      "Iteration 272, loss = 20258894085.93948364\n",
      "Iteration 273, loss = 20258887740.68286514\n",
      "Iteration 274, loss = 20258881367.27590179\n",
      "Iteration 275, loss = 20258875085.98459625\n",
      "Iteration 276, loss = 20258868685.32791519\n",
      "Iteration 277, loss = 20258862292.67470932\n",
      "Iteration 278, loss = 20258856034.81069565\n",
      "Iteration 279, loss = 20258849671.95817566\n",
      "Iteration 280, loss = 20258843334.77320862\n",
      "Iteration 281, loss = 20258836965.22106171\n",
      "Iteration 282, loss = 20258830672.07473755\n",
      "Iteration 283, loss = 20258824286.46477509\n",
      "Iteration 284, loss = 20258817944.51727295\n",
      "Iteration 285, loss = 20258811666.07696915\n",
      "Iteration 286, loss = 20258805234.63372040\n",
      "Iteration 287, loss = 20258798955.80271530\n",
      "Iteration 288, loss = 20258792649.71695709\n",
      "Iteration 289, loss = 20258786268.14361954\n",
      "Iteration 290, loss = 20258779908.99758148\n",
      "Iteration 291, loss = 20258773598.03749084\n",
      "Iteration 292, loss = 20258767306.22266769\n",
      "Iteration 293, loss = 20258760959.38299561\n",
      "Iteration 294, loss = 20258754600.44680023\n",
      "Iteration 295, loss = 20258748346.02665710\n",
      "Iteration 296, loss = 20258741980.58900070\n",
      "Iteration 297, loss = 20258735658.13813782\n",
      "Iteration 298, loss = 20258729301.58019638\n",
      "Iteration 299, loss = 20258723021.98107910\n",
      "Iteration 300, loss = 20258716696.33570099\n",
      "Iteration 301, loss = 20258710388.06038666\n",
      "Iteration 302, loss = 20258704085.23381042\n",
      "Iteration 303, loss = 20258697737.39380264\n",
      "Iteration 304, loss = 20258691465.65446091\n",
      "Iteration 305, loss = 20258685104.61969757\n",
      "Iteration 306, loss = 20258678811.45811081\n",
      "Iteration 307, loss = 20258672510.55400848\n",
      "Iteration 308, loss = 20258666177.06617355\n",
      "Iteration 309, loss = 20258659845.78414154\n",
      "Iteration 310, loss = 20258653517.19332504\n",
      "Iteration 311, loss = 20258647210.69520950\n",
      "Iteration 312, loss = 20258640922.75222397\n",
      "Iteration 313, loss = 20258634571.40453339\n",
      "Iteration 314, loss = 20258628279.97093201\n",
      "Iteration 315, loss = 20258621935.70098495\n",
      "Iteration 316, loss = 20258615676.47006989\n",
      "Iteration 317, loss = 20258609294.67103577\n",
      "Iteration 318, loss = 20258603016.98427582\n",
      "Iteration 319, loss = 20258596697.27983093\n",
      "Iteration 320, loss = 20258590396.11462784\n",
      "Iteration 321, loss = 20258584070.77737045\n",
      "Iteration 322, loss = 20258577771.89556122\n",
      "Iteration 323, loss = 20258571414.30902481\n",
      "Iteration 324, loss = 20258565143.98389053\n",
      "Iteration 325, loss = 20258558837.42475128\n",
      "Iteration 326, loss = 20258552546.16766357\n",
      "Iteration 327, loss = 20258546258.26635361\n",
      "Iteration 328, loss = 20258539916.09756088\n",
      "Iteration 329, loss = 20258533609.13635254\n",
      "Iteration 330, loss = 20258527339.43595505\n",
      "Iteration 331, loss = 20258521032.17607498\n",
      "Iteration 332, loss = 20258514701.65131760\n",
      "Iteration 333, loss = 20258508403.31454086\n",
      "Iteration 334, loss = 20258502148.30237961\n",
      "Iteration 335, loss = 20258495809.57553864\n",
      "Iteration 336, loss = 20258489524.38494492\n",
      "Iteration 337, loss = 20258483233.37788391\n",
      "Iteration 338, loss = 20258476908.33296204\n",
      "Iteration 339, loss = 20258470574.70314026\n",
      "Iteration 340, loss = 20258464320.37023544\n",
      "Iteration 341, loss = 20258457977.06645966\n",
      "Iteration 342, loss = 20258451668.80568695\n",
      "Iteration 343, loss = 20258445437.45014572\n",
      "Iteration 344, loss = 20258439094.02551270\n",
      "Iteration 345, loss = 20258432783.37322617\n",
      "Iteration 346, loss = 20258426490.39367676\n",
      "Iteration 347, loss = 20258420250.09734726\n",
      "Iteration 348, loss = 20258413878.76768112\n",
      "Iteration 349, loss = 20258407586.73724747\n",
      "Iteration 350, loss = 20258401285.75375748\n",
      "Iteration 351, loss = 20258394999.36064911\n",
      "Iteration 352, loss = 20258388725.10727310\n",
      "Iteration 353, loss = 20258382438.45618820\n",
      "Iteration 354, loss = 20258376078.81774139\n",
      "Iteration 355, loss = 20258369803.61128998\n",
      "Iteration 356, loss = 20258363538.63251114\n",
      "Iteration 357, loss = 20258357218.76826096\n",
      "Iteration 358, loss = 20258350906.41009521\n",
      "Iteration 359, loss = 20258344614.66164017\n",
      "Iteration 360, loss = 20258338345.77256393\n",
      "Iteration 361, loss = 20258332075.94315338\n",
      "Iteration 362, loss = 20258325784.75479889\n",
      "Iteration 363, loss = 20258319459.92143250\n",
      "Iteration 364, loss = 20258313147.34992218\n",
      "Iteration 365, loss = 20258306891.39269638\n",
      "Iteration 366, loss = 20258300542.00991440\n",
      "Iteration 367, loss = 20258294323.06809235\n",
      "Iteration 368, loss = 20258287982.24149704\n",
      "Iteration 369, loss = 20258281705.76757812\n",
      "Iteration 370, loss = 20258275381.01097488\n",
      "Iteration 371, loss = 20258269121.52601624\n",
      "Iteration 372, loss = 20258262807.18451691\n",
      "Iteration 373, loss = 20258256498.09072113\n",
      "Iteration 374, loss = 20258250210.76470947\n",
      "Iteration 375, loss = 20258243913.66434097\n",
      "Iteration 376, loss = 20258237640.20962906\n",
      "Iteration 377, loss = 20258231323.36583710\n",
      "Iteration 378, loss = 20258225041.95511627\n",
      "Iteration 379, loss = 20258218755.86921692\n",
      "Iteration 380, loss = 20258212476.81723785\n",
      "Iteration 381, loss = 20258206181.58238602\n",
      "Iteration 382, loss = 20258199942.30596542\n",
      "Iteration 383, loss = 20258193671.31538391\n",
      "Iteration 384, loss = 20258187419.07034683\n",
      "Iteration 385, loss = 20258181057.99718094\n",
      "Iteration 386, loss = 20258174820.54109955\n",
      "Iteration 387, loss = 20258168534.90175247\n",
      "Iteration 388, loss = 20258162240.97577667\n",
      "Iteration 389, loss = 20258155967.60231781\n",
      "Iteration 390, loss = 20258149663.64740753\n",
      "Iteration 391, loss = 20258143397.22123337\n",
      "Iteration 392, loss = 20258137087.02825165\n",
      "Iteration 393, loss = 20258130826.20236206\n",
      "Iteration 394, loss = 20258124538.45955658\n",
      "Iteration 395, loss = 20258118308.68730164\n",
      "Iteration 396, loss = 20258111964.45672607\n",
      "Iteration 397, loss = 20258105668.45137787\n",
      "Iteration 398, loss = 20258099423.86446762\n",
      "Iteration 399, loss = 20258093136.43788910\n",
      "Iteration 400, loss = 20258086806.01397705\n",
      "Iteration 401, loss = 20258080574.85282516\n",
      "Iteration 402, loss = 20258074273.91634369\n",
      "Iteration 403, loss = 20258067962.92285538\n",
      "Iteration 404, loss = 20258061711.89566040\n",
      "Iteration 405, loss = 20258055441.54598999\n",
      "Iteration 406, loss = 20258049141.56591034\n",
      "Iteration 407, loss = 20258042886.74496460\n",
      "Iteration 408, loss = 20258036609.15732193\n",
      "Iteration 409, loss = 20258030329.23227310\n",
      "Iteration 410, loss = 20258024073.13205719\n",
      "Iteration 411, loss = 20258017731.44691086\n",
      "Iteration 412, loss = 20258011479.88681030\n",
      "Iteration 413, loss = 20258005184.28468323\n",
      "Iteration 414, loss = 20257998945.39101791\n",
      "Iteration 415, loss = 20257992629.40206909\n",
      "Iteration 416, loss = 20257986360.40115356\n",
      "Iteration 417, loss = 20257980124.25070953\n",
      "Iteration 418, loss = 20257973806.61537933\n",
      "Iteration 419, loss = 20257967556.04629135\n",
      "Iteration 420, loss = 20257961239.08017731\n",
      "Iteration 421, loss = 20257955001.13780594\n",
      "Iteration 422, loss = 20257948783.80087280\n",
      "Iteration 423, loss = 20257942484.96354675\n",
      "Iteration 424, loss = 20257936228.89426422\n",
      "Iteration 425, loss = 20257929972.90125656\n",
      "Iteration 426, loss = 20257923701.95911407\n",
      "Iteration 427, loss = 20257917427.05560303\n",
      "Iteration 428, loss = 20257911135.29652786\n",
      "Iteration 429, loss = 20257904905.78767776\n",
      "Iteration 430, loss = 20257898649.97030258\n",
      "Iteration 431, loss = 20257892327.21722794\n",
      "Iteration 432, loss = 20257886089.97541428\n",
      "Iteration 433, loss = 20257879802.96271896\n",
      "Iteration 434, loss = 20257873514.71603012\n",
      "Iteration 435, loss = 20257867279.01080322\n",
      "Iteration 436, loss = 20257860959.09310150\n",
      "Iteration 437, loss = 20257854722.24040604\n",
      "Iteration 438, loss = 20257848423.55104828\n",
      "Iteration 439, loss = 20257842133.39361191\n",
      "Iteration 440, loss = 20257835832.64270020\n",
      "Iteration 441, loss = 20257829611.62866974\n",
      "Iteration 442, loss = 20257823330.54583740\n",
      "Iteration 443, loss = 20257817024.47481918\n",
      "Iteration 444, loss = 20257810788.45074463\n",
      "Iteration 445, loss = 20257804489.80315399\n",
      "Iteration 446, loss = 20257798248.33764648\n",
      "Iteration 447, loss = 20257791948.23505402\n",
      "Iteration 448, loss = 20257785682.34330368\n",
      "Iteration 449, loss = 20257779414.19330597\n",
      "Iteration 450, loss = 20257773149.74039841\n",
      "Iteration 451, loss = 20257766884.13483810\n",
      "Iteration 452, loss = 20257760601.65009689\n",
      "Iteration 453, loss = 20257754352.58415985\n",
      "Iteration 454, loss = 20257748102.24153519\n",
      "Iteration 455, loss = 20257741877.37223053\n",
      "Iteration 456, loss = 20257735571.26507187\n",
      "Iteration 457, loss = 20257729278.35507202\n",
      "Iteration 458, loss = 20257723073.26583099\n",
      "Iteration 459, loss = 20257716782.22005463\n",
      "Iteration 460, loss = 20257710541.37414169\n",
      "Iteration 461, loss = 20257704279.88020325\n",
      "Iteration 462, loss = 20257697998.93718338\n",
      "Iteration 463, loss = 20257691781.99047089\n",
      "Iteration 464, loss = 20257685475.30138016\n",
      "Iteration 465, loss = 20257679223.91313171\n",
      "Iteration 466, loss = 20257672959.67937088\n",
      "Iteration 467, loss = 20257666639.02680969\n",
      "Iteration 468, loss = 20257660442.18275833\n",
      "Iteration 469, loss = 20257654137.46024704\n",
      "Iteration 470, loss = 20257647872.43134308\n",
      "Iteration 471, loss = 20257641633.81998444\n",
      "Iteration 472, loss = 20257635387.08896637\n",
      "Iteration 473, loss = 20257629066.55283737\n",
      "Iteration 474, loss = 20257622850.58989716\n",
      "Iteration 475, loss = 20257616580.56698227\n",
      "Iteration 476, loss = 20257610347.16641235\n",
      "Iteration 477, loss = 20257603998.78327942\n",
      "Iteration 478, loss = 20257597761.82239532\n",
      "Iteration 479, loss = 20257591551.39259720\n",
      "Iteration 480, loss = 20257585247.30411911\n",
      "Iteration 481, loss = 20257578993.00785446\n",
      "Iteration 482, loss = 20257572750.55345535\n",
      "Iteration 483, loss = 20257566447.17241287\n",
      "Iteration 484, loss = 20257560173.03792572\n",
      "Iteration 485, loss = 20257553983.64853668\n",
      "Iteration 486, loss = 20257547664.90529251\n",
      "Iteration 487, loss = 20257541407.51183319\n",
      "Iteration 488, loss = 20257535140.63674545\n",
      "Iteration 489, loss = 20257528907.60445786\n",
      "Iteration 490, loss = 20257522629.94413757\n",
      "Iteration 491, loss = 20257516388.98987198\n",
      "Iteration 492, loss = 20257510122.92869568\n",
      "Iteration 493, loss = 20257503882.43861008\n",
      "Iteration 494, loss = 20257497641.63835144\n",
      "Iteration 495, loss = 20257491345.74452972\n",
      "Iteration 496, loss = 20257485096.34391403\n",
      "Iteration 497, loss = 20257478816.40977859\n",
      "Iteration 498, loss = 20257472591.93387985\n",
      "Iteration 499, loss = 20257466324.84668732\n",
      "Iteration 500, loss = 20257460073.37521362\n",
      "Iteration 1, loss = 20124820252.57970428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 20124801261.65341187\n",
      "Iteration 3, loss = 20124782614.74423981\n",
      "Iteration 4, loss = 20124763903.89614868\n",
      "Iteration 5, loss = 20124745579.92532349\n",
      "Iteration 6, loss = 20124727633.99100113\n",
      "Iteration 7, loss = 20124709645.42734146\n",
      "Iteration 8, loss = 20124691026.39286423\n",
      "Iteration 9, loss = 20124671041.55853271\n",
      "Iteration 10, loss = 20124650613.17705536\n",
      "Iteration 11, loss = 20124629704.36975098\n",
      "Iteration 12, loss = 20124607904.23369980\n",
      "Iteration 13, loss = 20124585425.34808731\n",
      "Iteration 14, loss = 20124561932.80129623\n",
      "Iteration 15, loss = 20124540610.30579376\n",
      "Iteration 16, loss = 20124520188.64765167\n",
      "Iteration 17, loss = 20124500306.85522842\n",
      "Iteration 18, loss = 20124479607.89912796\n",
      "Iteration 19, loss = 20124456868.90742874\n",
      "Iteration 20, loss = 20124432956.02197647\n",
      "Iteration 21, loss = 20124408665.57860565\n",
      "Iteration 22, loss = 20124387871.62003326\n",
      "Iteration 23, loss = 20124370492.65604019\n",
      "Iteration 24, loss = 20124353482.00704575\n",
      "Iteration 25, loss = 20124338017.44682693\n",
      "Iteration 26, loss = 20124323735.22632217\n",
      "Iteration 27, loss = 20124309064.16586304\n",
      "Iteration 28, loss = 20124294016.86806107\n",
      "Iteration 29, loss = 20124279359.35345840\n",
      "Iteration 30, loss = 20124265342.09020233\n",
      "Iteration 31, loss = 20124251367.71468353\n",
      "Iteration 32, loss = 20124238298.82972336\n",
      "Iteration 33, loss = 20124225883.22015381\n",
      "Iteration 34, loss = 20124214818.95084763\n",
      "Iteration 35, loss = 20124204384.58263779\n",
      "Iteration 36, loss = 20124194340.79299545\n",
      "Iteration 37, loss = 20124184563.79964447\n",
      "Iteration 38, loss = 20124175360.57239532\n",
      "Iteration 39, loss = 20124166510.74145889\n",
      "Iteration 40, loss = 20124157801.83232498\n",
      "Iteration 41, loss = 20124149247.13651657\n",
      "Iteration 42, loss = 20124140755.35818100\n",
      "Iteration 43, loss = 20124132647.70616150\n",
      "Iteration 44, loss = 20124124691.09300613\n",
      "Iteration 45, loss = 20124116758.65494156\n",
      "Iteration 46, loss = 20124109075.70756912\n",
      "Iteration 47, loss = 20124101526.27477646\n",
      "Iteration 48, loss = 20124094080.13109589\n",
      "Iteration 49, loss = 20124086731.41442490\n",
      "Iteration 50, loss = 20124079458.80608368\n",
      "Iteration 51, loss = 20124072271.90547180\n",
      "Iteration 52, loss = 20124065117.53097153\n",
      "Iteration 53, loss = 20124057964.64981461\n",
      "Iteration 54, loss = 20124051001.52853394\n",
      "Iteration 55, loss = 20124043935.53079224\n",
      "Iteration 56, loss = 20124036918.76754379\n",
      "Iteration 57, loss = 20124029914.88774872\n",
      "Iteration 58, loss = 20124022991.55041885\n",
      "Iteration 59, loss = 20124016026.93470001\n",
      "Iteration 60, loss = 20124009138.26467514\n",
      "Iteration 61, loss = 20124002143.43141556\n",
      "Iteration 62, loss = 20123995165.00134277\n",
      "Iteration 63, loss = 20123988211.57082748\n",
      "Iteration 64, loss = 20123981168.01789474\n",
      "Iteration 65, loss = 20123974034.55615616\n",
      "Iteration 66, loss = 20123966772.53662491\n",
      "Iteration 67, loss = 20123959553.99861145\n",
      "Iteration 68, loss = 20123952317.96220779\n",
      "Iteration 69, loss = 20123945077.98046875\n",
      "Iteration 70, loss = 20123937898.25176239\n",
      "Iteration 71, loss = 20123930798.26387024\n",
      "Iteration 72, loss = 20123923686.94828796\n",
      "Iteration 73, loss = 20123916707.31629181\n",
      "Iteration 74, loss = 20123909672.83576202\n",
      "Iteration 75, loss = 20123902638.54384232\n",
      "Iteration 76, loss = 20123895750.33483505\n",
      "Iteration 77, loss = 20123888759.86098480\n",
      "Iteration 78, loss = 20123881855.49716187\n",
      "Iteration 79, loss = 20123874953.52288437\n",
      "Iteration 80, loss = 20123868106.25795746\n",
      "Iteration 81, loss = 20123861317.93373108\n",
      "Iteration 82, loss = 20123854534.68283081\n",
      "Iteration 83, loss = 20123847814.59309387\n",
      "Iteration 84, loss = 20123841048.48516464\n",
      "Iteration 85, loss = 20123834358.38848114\n",
      "Iteration 86, loss = 20123827671.69786835\n",
      "Iteration 87, loss = 20123820996.40718842\n",
      "Iteration 88, loss = 20123814286.49820709\n",
      "Iteration 89, loss = 20123807623.07345581\n",
      "Iteration 90, loss = 20123800887.31462860\n",
      "Iteration 91, loss = 20123794195.59878159\n",
      "Iteration 92, loss = 20123787525.23719406\n",
      "Iteration 93, loss = 20123780902.67779160\n",
      "Iteration 94, loss = 20123774286.01874161\n",
      "Iteration 95, loss = 20123767732.03436661\n",
      "Iteration 96, loss = 20123761129.77198410\n",
      "Iteration 97, loss = 20123754602.60523987\n",
      "Iteration 98, loss = 20123748050.25047302\n",
      "Iteration 99, loss = 20123741584.15733337\n",
      "Iteration 100, loss = 20123735058.49890900\n",
      "Iteration 101, loss = 20123728581.99329376\n",
      "Iteration 102, loss = 20123722143.93294144\n",
      "Iteration 103, loss = 20123715640.94984818\n",
      "Iteration 104, loss = 20123709134.91621017\n",
      "Iteration 105, loss = 20123702683.50949097\n",
      "Iteration 106, loss = 20123696247.15503693\n",
      "Iteration 107, loss = 20123689803.57166290\n",
      "Iteration 108, loss = 20123683372.05796051\n",
      "Iteration 109, loss = 20123676977.48405075\n",
      "Iteration 110, loss = 20123670541.90067673\n",
      "Iteration 111, loss = 20123664090.21221161\n",
      "Iteration 112, loss = 20123657719.46339798\n",
      "Iteration 113, loss = 20123651331.97535324\n",
      "Iteration 114, loss = 20123644988.40143204\n",
      "Iteration 115, loss = 20123638612.52307510\n",
      "Iteration 116, loss = 20123632269.49039459\n",
      "Iteration 117, loss = 20123625926.30111694\n",
      "Iteration 118, loss = 20123619560.45066452\n",
      "Iteration 119, loss = 20123613168.18110657\n",
      "Iteration 120, loss = 20123606828.97002792\n",
      "Iteration 121, loss = 20123600508.10998917\n",
      "Iteration 122, loss = 20123594171.43320847\n",
      "Iteration 123, loss = 20123587871.64039993\n",
      "Iteration 124, loss = 20123581571.10818863\n",
      "Iteration 125, loss = 20123575253.94612503\n",
      "Iteration 126, loss = 20123568964.93764877\n",
      "Iteration 127, loss = 20123562730.79161453\n",
      "Iteration 128, loss = 20123556459.31757736\n",
      "Iteration 129, loss = 20123550212.34265518\n",
      "Iteration 130, loss = 20123543971.66062927\n",
      "Iteration 131, loss = 20123537649.84600449\n",
      "Iteration 132, loss = 20123531385.13068390\n",
      "Iteration 133, loss = 20123525102.92744446\n",
      "Iteration 134, loss = 20123518847.50323486\n",
      "Iteration 135, loss = 20123512584.00560379\n",
      "Iteration 136, loss = 20123506326.00103760\n",
      "Iteration 137, loss = 20123500110.16831970\n",
      "Iteration 138, loss = 20123493872.78558350\n",
      "Iteration 139, loss = 20123487616.96264648\n",
      "Iteration 140, loss = 20123481358.75590134\n",
      "Iteration 141, loss = 20123475161.75133896\n",
      "Iteration 142, loss = 20123468917.60930252\n",
      "Iteration 143, loss = 20123462710.45566559\n",
      "Iteration 144, loss = 20123456453.31633377\n",
      "Iteration 145, loss = 20123450213.48961258\n",
      "Iteration 146, loss = 20123444001.26178360\n",
      "Iteration 147, loss = 20123437741.95474625\n",
      "Iteration 148, loss = 20123431524.55207443\n",
      "Iteration 149, loss = 20123425340.31261444\n",
      "Iteration 150, loss = 20123419121.74049759\n",
      "Iteration 151, loss = 20123412912.50383377\n",
      "Iteration 152, loss = 20123406691.72688293\n",
      "Iteration 153, loss = 20123400495.43062973\n",
      "Iteration 154, loss = 20123394290.90982437\n",
      "Iteration 155, loss = 20123388065.04185104\n",
      "Iteration 156, loss = 20123381855.90658188\n",
      "Iteration 157, loss = 20123375675.68113327\n",
      "Iteration 158, loss = 20123369510.77414703\n",
      "Iteration 159, loss = 20123363355.84946442\n",
      "Iteration 160, loss = 20123357160.46336365\n",
      "Iteration 161, loss = 20123351053.32978058\n",
      "Iteration 162, loss = 20123344880.70820999\n",
      "Iteration 163, loss = 20123338726.76012421\n",
      "Iteration 164, loss = 20123332556.29987335\n",
      "Iteration 165, loss = 20123326381.79035187\n",
      "Iteration 166, loss = 20123320260.22614670\n",
      "Iteration 167, loss = 20123314055.41853333\n",
      "Iteration 168, loss = 20123307955.56197739\n",
      "Iteration 169, loss = 20123301824.22666168\n",
      "Iteration 170, loss = 20123295680.08905029\n",
      "Iteration 171, loss = 20123289521.86617279\n",
      "Iteration 172, loss = 20123283356.08567047\n",
      "Iteration 173, loss = 20123277253.98569107\n",
      "Iteration 174, loss = 20123271112.00373840\n",
      "Iteration 175, loss = 20123264998.86803818\n",
      "Iteration 176, loss = 20123258846.58566284\n",
      "Iteration 177, loss = 20123252709.43533707\n",
      "Iteration 178, loss = 20123246618.07517624\n",
      "Iteration 179, loss = 20123240517.68687057\n",
      "Iteration 180, loss = 20123234430.74964905\n",
      "Iteration 181, loss = 20123228285.55625916\n",
      "Iteration 182, loss = 20123222153.41679382\n",
      "Iteration 183, loss = 20123216007.65702820\n",
      "Iteration 184, loss = 20123209943.49707413\n",
      "Iteration 185, loss = 20123203825.72004318\n",
      "Iteration 186, loss = 20123197676.26883316\n",
      "Iteration 187, loss = 20123191581.88073730\n",
      "Iteration 188, loss = 20123185518.47717667\n",
      "Iteration 189, loss = 20123179404.28329086\n",
      "Iteration 190, loss = 20123173306.54246521\n",
      "Iteration 191, loss = 20123167232.82376862\n",
      "Iteration 192, loss = 20123161140.30467987\n",
      "Iteration 193, loss = 20123154997.08751678\n",
      "Iteration 194, loss = 20123148863.00620270\n",
      "Iteration 195, loss = 20123142750.52149200\n",
      "Iteration 196, loss = 20123136602.34587097\n",
      "Iteration 197, loss = 20123130452.77115631\n",
      "Iteration 198, loss = 20123124381.92216492\n",
      "Iteration 199, loss = 20123118291.60812378\n",
      "Iteration 200, loss = 20123112144.27215195\n",
      "Iteration 201, loss = 20123106107.72610474\n",
      "Iteration 202, loss = 20123100041.71268845\n",
      "Iteration 203, loss = 20123094000.01580811\n",
      "Iteration 204, loss = 20123087883.90713120\n",
      "Iteration 205, loss = 20123081834.27427292\n",
      "Iteration 206, loss = 20123075772.04950714\n",
      "Iteration 207, loss = 20123069670.26343536\n",
      "Iteration 208, loss = 20123063631.25001144\n",
      "Iteration 209, loss = 20123057502.82583237\n",
      "Iteration 210, loss = 20123051432.41466904\n",
      "Iteration 211, loss = 20123045304.22198105\n",
      "Iteration 212, loss = 20123039223.19580841\n",
      "Iteration 213, loss = 20123033123.27581787\n",
      "Iteration 214, loss = 20123027013.12800598\n",
      "Iteration 215, loss = 20123020954.58418655\n",
      "Iteration 216, loss = 20123014838.08785629\n",
      "Iteration 217, loss = 20123008837.68079376\n",
      "Iteration 218, loss = 20123002755.37470627\n",
      "Iteration 219, loss = 20122996712.88836670\n",
      "Iteration 220, loss = 20122990667.00584030\n",
      "Iteration 221, loss = 20122984558.01528168\n",
      "Iteration 222, loss = 20122978532.09078979\n",
      "Iteration 223, loss = 20122972455.35082245\n",
      "Iteration 224, loss = 20122966426.29906082\n",
      "Iteration 225, loss = 20122960318.44390106\n",
      "Iteration 226, loss = 20122954217.40496445\n",
      "Iteration 227, loss = 20122948222.32092285\n",
      "Iteration 228, loss = 20122942187.13081741\n",
      "Iteration 229, loss = 20122936093.55712891\n",
      "Iteration 230, loss = 20122930086.81054306\n",
      "Iteration 231, loss = 20122924031.58457565\n",
      "Iteration 232, loss = 20122917951.27436829\n",
      "Iteration 233, loss = 20122911888.52146530\n",
      "Iteration 234, loss = 20122905813.56637955\n",
      "Iteration 235, loss = 20122899783.83419037\n",
      "Iteration 236, loss = 20122893737.74314499\n",
      "Iteration 237, loss = 20122887719.62944412\n",
      "Iteration 238, loss = 20122881652.52885818\n",
      "Iteration 239, loss = 20122875593.85853577\n",
      "Iteration 240, loss = 20122869559.77845001\n",
      "Iteration 241, loss = 20122863524.62099838\n",
      "Iteration 242, loss = 20122857488.17281723\n",
      "Iteration 243, loss = 20122851413.76271439\n",
      "Iteration 244, loss = 20122845367.38192368\n",
      "Iteration 245, loss = 20122839336.89478683\n",
      "Iteration 246, loss = 20122833280.30336761\n",
      "Iteration 247, loss = 20122827302.55496597\n",
      "Iteration 248, loss = 20122821265.91966629\n",
      "Iteration 249, loss = 20122815239.53158569\n",
      "Iteration 250, loss = 20122809203.82447433\n",
      "Iteration 251, loss = 20122803190.98509598\n",
      "Iteration 252, loss = 20122797180.55438232\n",
      "Iteration 253, loss = 20122791114.01240540\n",
      "Iteration 254, loss = 20122785068.23910522\n",
      "Iteration 255, loss = 20122779042.56589890\n",
      "Iteration 256, loss = 20122772988.98239517\n",
      "Iteration 257, loss = 20122766919.06937027\n",
      "Iteration 258, loss = 20122760886.05633163\n",
      "Iteration 259, loss = 20122754868.21797562\n",
      "Iteration 260, loss = 20122748798.72316360\n",
      "Iteration 261, loss = 20122742809.13886261\n",
      "Iteration 262, loss = 20122736732.94902420\n",
      "Iteration 263, loss = 20122730714.34030914\n",
      "Iteration 264, loss = 20122724646.18095398\n",
      "Iteration 265, loss = 20122718601.86685181\n",
      "Iteration 266, loss = 20122712559.92633438\n",
      "Iteration 267, loss = 20122706547.48266602\n",
      "Iteration 268, loss = 20122700488.03559875\n",
      "Iteration 269, loss = 20122694446.71892929\n",
      "Iteration 270, loss = 20122688391.25168991\n",
      "Iteration 271, loss = 20122682385.78171539\n",
      "Iteration 272, loss = 20122676343.75628281\n",
      "Iteration 273, loss = 20122670267.76509857\n",
      "Iteration 274, loss = 20122664207.80260849\n",
      "Iteration 275, loss = 20122658171.87971497\n",
      "Iteration 276, loss = 20122652120.55567169\n",
      "Iteration 277, loss = 20122646078.09851074\n",
      "Iteration 278, loss = 20122640001.50297546\n",
      "Iteration 279, loss = 20122633972.77940369\n",
      "Iteration 280, loss = 20122627934.10758591\n",
      "Iteration 281, loss = 20122621953.23845673\n",
      "Iteration 282, loss = 20122615924.29804230\n",
      "Iteration 283, loss = 20122609912.18975449\n",
      "Iteration 284, loss = 20122603913.40564728\n",
      "Iteration 285, loss = 20122597936.22616196\n",
      "Iteration 286, loss = 20122591907.73488235\n",
      "Iteration 287, loss = 20122585911.35433960\n",
      "Iteration 288, loss = 20122579888.69927216\n",
      "Iteration 289, loss = 20122573833.34273911\n",
      "Iteration 290, loss = 20122567809.95312119\n",
      "Iteration 291, loss = 20122561796.73750305\n",
      "Iteration 292, loss = 20122555788.90513992\n",
      "Iteration 293, loss = 20122549800.23041916\n",
      "Iteration 294, loss = 20122543772.38357925\n",
      "Iteration 295, loss = 20122537762.35349655\n",
      "Iteration 296, loss = 20122531769.32762146\n",
      "Iteration 297, loss = 20122525752.50837708\n",
      "Iteration 298, loss = 20122519766.46176529\n",
      "Iteration 299, loss = 20122513758.91299057\n",
      "Iteration 300, loss = 20122507766.72279739\n",
      "Iteration 301, loss = 20122501812.78842926\n",
      "Iteration 302, loss = 20122495836.03768921\n",
      "Iteration 303, loss = 20122489852.85231781\n",
      "Iteration 304, loss = 20122483876.29955292\n",
      "Iteration 305, loss = 20122477876.14492035\n",
      "Iteration 306, loss = 20122471907.32785416\n",
      "Iteration 307, loss = 20122465878.00199509\n",
      "Iteration 308, loss = 20122459914.95188904\n",
      "Iteration 309, loss = 20122453905.72313309\n",
      "Iteration 310, loss = 20122447940.96597672\n",
      "Iteration 311, loss = 20122441921.73523712\n",
      "Iteration 312, loss = 20122435928.78080368\n",
      "Iteration 313, loss = 20122429965.05905533\n",
      "Iteration 314, loss = 20122423949.42531586\n",
      "Iteration 315, loss = 20122417982.14372635\n",
      "Iteration 316, loss = 20122412029.19472122\n",
      "Iteration 317, loss = 20122406046.12483597\n",
      "Iteration 318, loss = 20122400053.20555115\n",
      "Iteration 319, loss = 20122394060.33610153\n",
      "Iteration 320, loss = 20122388114.75351334\n",
      "Iteration 321, loss = 20122382135.50297546\n",
      "Iteration 322, loss = 20122376214.37251282\n",
      "Iteration 323, loss = 20122370177.82110214\n",
      "Iteration 324, loss = 20122364229.33583069\n",
      "Iteration 325, loss = 20122358222.18043900\n",
      "Iteration 326, loss = 20122352233.17104340\n",
      "Iteration 327, loss = 20122346250.87359619\n",
      "Iteration 328, loss = 20122340280.04870987\n",
      "Iteration 329, loss = 20122334264.87437820\n",
      "Iteration 330, loss = 20122328258.98308945\n",
      "Iteration 331, loss = 20122322207.05353546\n",
      "Iteration 332, loss = 20122316265.24463654\n",
      "Iteration 333, loss = 20122310236.59833908\n",
      "Iteration 334, loss = 20122304254.55815887\n",
      "Iteration 335, loss = 20122298259.27098846\n",
      "Iteration 336, loss = 20122292233.41828156\n",
      "Iteration 337, loss = 20122286228.67604446\n",
      "Iteration 338, loss = 20122280222.91085052\n",
      "Iteration 339, loss = 20122274238.15184021\n",
      "Iteration 340, loss = 20122268253.09274673\n",
      "Iteration 341, loss = 20122262292.30227280\n",
      "Iteration 342, loss = 20122256268.64840317\n",
      "Iteration 343, loss = 20122250362.42116928\n",
      "Iteration 344, loss = 20122244328.04983139\n",
      "Iteration 345, loss = 20122238392.69526291\n",
      "Iteration 346, loss = 20122232335.14446259\n",
      "Iteration 347, loss = 20122226365.88863373\n",
      "Iteration 348, loss = 20122220406.50764465\n",
      "Iteration 349, loss = 20122214423.70853424\n",
      "Iteration 350, loss = 20122208483.56226349\n",
      "Iteration 351, loss = 20122202473.16055298\n",
      "Iteration 352, loss = 20122196527.18384552\n",
      "Iteration 353, loss = 20122190587.40853119\n",
      "Iteration 354, loss = 20122184598.70833969\n",
      "Iteration 355, loss = 20122178663.88040543\n",
      "Iteration 356, loss = 20122172706.09110641\n",
      "Iteration 357, loss = 20122166776.43083954\n",
      "Iteration 358, loss = 20122160811.97659302\n",
      "Iteration 359, loss = 20122154883.86972046\n",
      "Iteration 360, loss = 20122148940.80059433\n",
      "Iteration 361, loss = 20122142975.82253647\n",
      "Iteration 362, loss = 20122136971.64362717\n",
      "Iteration 363, loss = 20122130992.51563644\n",
      "Iteration 364, loss = 20122125021.62985992\n",
      "Iteration 365, loss = 20122119038.96218872\n",
      "Iteration 366, loss = 20122113076.79527283\n",
      "Iteration 367, loss = 20122107134.53126526\n",
      "Iteration 368, loss = 20122101176.42505264\n",
      "Iteration 369, loss = 20122095189.96324539\n",
      "Iteration 370, loss = 20122089246.35013199\n",
      "Iteration 371, loss = 20122083240.86639023\n",
      "Iteration 372, loss = 20122077250.15274048\n",
      "Iteration 373, loss = 20122071315.75991821\n",
      "Iteration 374, loss = 20122065367.88238144\n",
      "Iteration 375, loss = 20122059353.70672989\n",
      "Iteration 376, loss = 20122053335.55443954\n",
      "Iteration 377, loss = 20122047391.15478516\n",
      "Iteration 378, loss = 20122041388.08784103\n",
      "Iteration 379, loss = 20122035406.20609665\n",
      "Iteration 380, loss = 20122029431.17954254\n",
      "Iteration 381, loss = 20122023489.59954071\n",
      "Iteration 382, loss = 20122017506.11343002\n",
      "Iteration 383, loss = 20122011533.05294037\n",
      "Iteration 384, loss = 20122005603.57087708\n",
      "Iteration 385, loss = 20121999640.74962997\n",
      "Iteration 386, loss = 20121993644.61149597\n",
      "Iteration 387, loss = 20121987709.17688751\n",
      "Iteration 388, loss = 20121981757.75190353\n",
      "Iteration 389, loss = 20121975758.50613022\n",
      "Iteration 390, loss = 20121969768.20547867\n",
      "Iteration 391, loss = 20121963837.03677368\n",
      "Iteration 392, loss = 20121957883.70103073\n",
      "Iteration 393, loss = 20121951867.55001068\n",
      "Iteration 394, loss = 20121945889.27923965\n",
      "Iteration 395, loss = 20121939895.16711426\n",
      "Iteration 396, loss = 20121933916.51440430\n",
      "Iteration 397, loss = 20121927998.57982635\n",
      "Iteration 398, loss = 20121921997.27571487\n",
      "Iteration 399, loss = 20121916042.63654327\n",
      "Iteration 400, loss = 20121910128.26572418\n",
      "Iteration 401, loss = 20121904173.10169220\n",
      "Iteration 402, loss = 20121898197.70502853\n",
      "Iteration 403, loss = 20121892274.01148987\n",
      "Iteration 404, loss = 20121886314.26952744\n",
      "Iteration 405, loss = 20121880352.71237564\n",
      "Iteration 406, loss = 20121874392.15837860\n",
      "Iteration 407, loss = 20121868445.21436691\n",
      "Iteration 408, loss = 20121862458.91146469\n",
      "Iteration 409, loss = 20121856478.68079376\n",
      "Iteration 410, loss = 20121850549.53186035\n",
      "Iteration 411, loss = 20121844540.31842041\n",
      "Iteration 412, loss = 20121838550.28668594\n",
      "Iteration 413, loss = 20121832586.13794708\n",
      "Iteration 414, loss = 20121826558.59340668\n",
      "Iteration 415, loss = 20121820578.04857635\n",
      "Iteration 416, loss = 20121814567.16188049\n",
      "Iteration 417, loss = 20121808651.13822174\n",
      "Iteration 418, loss = 20121802679.09524918\n",
      "Iteration 419, loss = 20121796705.31268692\n",
      "Iteration 420, loss = 20121790745.37924194\n",
      "Iteration 421, loss = 20121784788.40299988\n",
      "Iteration 422, loss = 20121778820.93589020\n",
      "Iteration 423, loss = 20121772855.91717148\n",
      "Iteration 424, loss = 20121766942.10183716\n",
      "Iteration 425, loss = 20121760960.03857803\n",
      "Iteration 426, loss = 20121755007.51015472\n",
      "Iteration 427, loss = 20121749116.01252747\n",
      "Iteration 428, loss = 20121743139.40455246\n",
      "Iteration 429, loss = 20121737239.01861191\n",
      "Iteration 430, loss = 20121731310.06840515\n",
      "Iteration 431, loss = 20121725355.86857986\n",
      "Iteration 432, loss = 20121719429.38869858\n",
      "Iteration 433, loss = 20121713482.14356613\n",
      "Iteration 434, loss = 20121707527.82357025\n",
      "Iteration 435, loss = 20121701615.18591690\n",
      "Iteration 436, loss = 20121695631.92527390\n",
      "Iteration 437, loss = 20121689640.28349304\n",
      "Iteration 438, loss = 20121683706.16687012\n",
      "Iteration 439, loss = 20121677777.92185211\n",
      "Iteration 440, loss = 20121671800.46495438\n",
      "Iteration 441, loss = 20121665844.66616440\n",
      "Iteration 442, loss = 20121659849.83425140\n",
      "Iteration 443, loss = 20121653921.38561630\n",
      "Iteration 444, loss = 20121647968.61535263\n",
      "Iteration 445, loss = 20121642007.25620270\n",
      "Iteration 446, loss = 20121636124.98861313\n",
      "Iteration 447, loss = 20121630165.12240982\n",
      "Iteration 448, loss = 20121624264.70687485\n",
      "Iteration 449, loss = 20121618276.13134384\n",
      "Iteration 450, loss = 20121612297.08361435\n",
      "Iteration 451, loss = 20121606325.39208984\n",
      "Iteration 452, loss = 20121600369.13864517\n",
      "Iteration 453, loss = 20121594417.70145035\n",
      "Iteration 454, loss = 20121588459.33760071\n",
      "Iteration 455, loss = 20121582524.97858810\n",
      "Iteration 456, loss = 20121576581.36369705\n",
      "Iteration 457, loss = 20121570629.18626022\n",
      "Iteration 458, loss = 20121564688.96184158\n",
      "Iteration 459, loss = 20121558782.94577026\n",
      "Iteration 460, loss = 20121552796.33843994\n",
      "Iteration 461, loss = 20121546858.86024857\n",
      "Iteration 462, loss = 20121540872.01350403\n",
      "Iteration 463, loss = 20121534895.32531738\n",
      "Iteration 464, loss = 20121528873.45901108\n",
      "Iteration 465, loss = 20121522916.92020798\n",
      "Iteration 466, loss = 20121516936.77688980\n",
      "Iteration 467, loss = 20121510937.91528702\n",
      "Iteration 468, loss = 20121504987.87033463\n",
      "Iteration 469, loss = 20121499024.47137833\n",
      "Iteration 470, loss = 20121493129.19228363\n",
      "Iteration 471, loss = 20121487131.05393982\n",
      "Iteration 472, loss = 20121481177.76906967\n",
      "Iteration 473, loss = 20121475208.30680084\n",
      "Iteration 474, loss = 20121469179.51279068\n",
      "Iteration 475, loss = 20121463203.15930557\n",
      "Iteration 476, loss = 20121457243.14892960\n",
      "Iteration 477, loss = 20121451273.00940704\n",
      "Iteration 478, loss = 20121445380.79923248\n",
      "Iteration 479, loss = 20121439457.75603104\n",
      "Iteration 480, loss = 20121433517.91770172\n",
      "Iteration 481, loss = 20121427590.38378906\n",
      "Iteration 482, loss = 20121421570.16143417\n",
      "Iteration 483, loss = 20121415632.08716965\n",
      "Iteration 484, loss = 20121409674.12520981\n",
      "Iteration 485, loss = 20121403732.37913513\n",
      "Iteration 486, loss = 20121397775.61524582\n",
      "Iteration 487, loss = 20121391878.61791611\n",
      "Iteration 488, loss = 20121385881.95975113\n",
      "Iteration 489, loss = 20121379942.69445038\n",
      "Iteration 490, loss = 20121374029.11647034\n",
      "Iteration 491, loss = 20121368034.40053558\n",
      "Iteration 492, loss = 20121362125.90805817\n",
      "Iteration 493, loss = 20121356191.54144669\n",
      "Iteration 494, loss = 20121350301.50857544\n",
      "Iteration 495, loss = 20121344282.85475922\n",
      "Iteration 496, loss = 20121338360.10163879\n",
      "Iteration 497, loss = 20121332374.87723160\n",
      "Iteration 498, loss = 20121326425.87719727\n",
      "Iteration 499, loss = 20121320468.92807388\n",
      "Iteration 500, loss = 20121314484.87177658\n",
      "Iteration 1, loss = 20022050551.36706161\n",
      "Iteration 2, loss = 20022028377.00746155\n",
      "Iteration 3, loss = 20022006259.60122681\n",
      "Iteration 4, loss = 20021984272.60755539\n",
      "Iteration 5, loss = 20021962720.75655365\n",
      "Iteration 6, loss = 20021941423.74531174\n",
      "Iteration 7, loss = 20021920189.92928696\n",
      "Iteration 8, loss = 20021899149.57176971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 20021876958.42035675\n",
      "Iteration 10, loss = 20021853843.79602051\n",
      "Iteration 11, loss = 20021829260.14928055\n",
      "Iteration 12, loss = 20021803007.64844131\n",
      "Iteration 13, loss = 20021778502.24049377\n",
      "Iteration 14, loss = 20021756892.41076660\n",
      "Iteration 15, loss = 20021735715.34738922\n",
      "Iteration 16, loss = 20021714556.54375839\n",
      "Iteration 17, loss = 20021692718.83805084\n",
      "Iteration 18, loss = 20021670729.15039062\n",
      "Iteration 19, loss = 20021651587.08097458\n",
      "Iteration 20, loss = 20021632221.56045151\n",
      "Iteration 21, loss = 20021612677.45235443\n",
      "Iteration 22, loss = 20021594341.65090942\n",
      "Iteration 23, loss = 20021577128.44912338\n",
      "Iteration 24, loss = 20021561101.87249756\n",
      "Iteration 25, loss = 20021544332.27632523\n",
      "Iteration 26, loss = 20021528020.45962906\n",
      "Iteration 27, loss = 20021513076.53462601\n",
      "Iteration 28, loss = 20021499367.42408752\n",
      "Iteration 29, loss = 20021486578.14314270\n",
      "Iteration 30, loss = 20021473297.47060776\n",
      "Iteration 31, loss = 20021459842.12537003\n",
      "Iteration 32, loss = 20021447023.46558762\n",
      "Iteration 33, loss = 20021435275.55418777\n",
      "Iteration 34, loss = 20021424426.08258057\n",
      "Iteration 35, loss = 20021413912.94333267\n",
      "Iteration 36, loss = 20021404057.13700485\n",
      "Iteration 37, loss = 20021394542.80765152\n",
      "Iteration 38, loss = 20021385100.68133163\n",
      "Iteration 39, loss = 20021375892.44445801\n",
      "Iteration 40, loss = 20021366679.46110916\n",
      "Iteration 41, loss = 20021357442.71330261\n",
      "Iteration 42, loss = 20021347888.90525436\n",
      "Iteration 43, loss = 20021338584.24448013\n",
      "Iteration 44, loss = 20021329410.50529861\n",
      "Iteration 45, loss = 20021320621.14931870\n",
      "Iteration 46, loss = 20021311718.86806870\n",
      "Iteration 47, loss = 20021302852.58334732\n",
      "Iteration 48, loss = 20021293908.14414978\n",
      "Iteration 49, loss = 20021285144.46656799\n",
      "Iteration 50, loss = 20021276486.54798508\n",
      "Iteration 51, loss = 20021267906.16464996\n",
      "Iteration 52, loss = 20021259267.16269302\n",
      "Iteration 53, loss = 20021250678.58361816\n",
      "Iteration 54, loss = 20021242090.26668167\n",
      "Iteration 55, loss = 20021233583.19027710\n",
      "Iteration 56, loss = 20021225075.10719299\n",
      "Iteration 57, loss = 20021216553.62148666\n",
      "Iteration 58, loss = 20021208281.20301056\n",
      "Iteration 59, loss = 20021199832.25014496\n",
      "Iteration 60, loss = 20021191611.84451294\n",
      "Iteration 61, loss = 20021183290.92219925\n",
      "Iteration 62, loss = 20021175060.86334991\n",
      "Iteration 63, loss = 20021166951.68178940\n",
      "Iteration 64, loss = 20021158785.14148331\n",
      "Iteration 65, loss = 20021150674.87341690\n",
      "Iteration 66, loss = 20021142607.13033295\n",
      "Iteration 67, loss = 20021134549.35477066\n",
      "Iteration 68, loss = 20021126588.70275879\n",
      "Iteration 69, loss = 20021118591.88278580\n",
      "Iteration 70, loss = 20021110619.30178452\n",
      "Iteration 71, loss = 20021102696.18122864\n",
      "Iteration 72, loss = 20021094785.46287537\n",
      "Iteration 73, loss = 20021086777.84812927\n",
      "Iteration 74, loss = 20021078886.47134018\n",
      "Iteration 75, loss = 20021071080.30381393\n",
      "Iteration 76, loss = 20021063191.26668549\n",
      "Iteration 77, loss = 20021055374.70128250\n",
      "Iteration 78, loss = 20021047609.64563751\n",
      "Iteration 79, loss = 20021039850.17734909\n",
      "Iteration 80, loss = 20021032055.87907410\n",
      "Iteration 81, loss = 20021024314.77787399\n",
      "Iteration 82, loss = 20021016566.48571777\n",
      "Iteration 83, loss = 20021008879.35187531\n",
      "Iteration 84, loss = 20021001180.81187439\n",
      "Iteration 85, loss = 20020993520.84029388\n",
      "Iteration 86, loss = 20020985830.48002243\n",
      "Iteration 87, loss = 20020978171.35986710\n",
      "Iteration 88, loss = 20020970570.86944580\n",
      "Iteration 89, loss = 20020962906.75856018\n",
      "Iteration 90, loss = 20020955325.71710205\n",
      "Iteration 91, loss = 20020947698.93869019\n",
      "Iteration 92, loss = 20020940068.77359009\n",
      "Iteration 93, loss = 20020932512.87639618\n",
      "Iteration 94, loss = 20020924903.54546356\n",
      "Iteration 95, loss = 20020917358.97921371\n",
      "Iteration 96, loss = 20020909809.76910782\n",
      "Iteration 97, loss = 20020902292.53854752\n",
      "Iteration 98, loss = 20020894757.04023361\n",
      "Iteration 99, loss = 20020887195.88469315\n",
      "Iteration 100, loss = 20020879718.57084274\n",
      "Iteration 101, loss = 20020872203.88248825\n",
      "Iteration 102, loss = 20020864679.46151352\n",
      "Iteration 103, loss = 20020857137.49140167\n",
      "Iteration 104, loss = 20020849688.56826401\n",
      "Iteration 105, loss = 20020842192.86935425\n",
      "Iteration 106, loss = 20020834725.92160797\n",
      "Iteration 107, loss = 20020827254.67783737\n",
      "Iteration 108, loss = 20020819759.00574112\n",
      "Iteration 109, loss = 20020812344.28197479\n",
      "Iteration 110, loss = 20020804894.75022888\n",
      "Iteration 111, loss = 20020797438.19570160\n",
      "Iteration 112, loss = 20020789970.44897079\n",
      "Iteration 113, loss = 20020782648.36746979\n",
      "Iteration 114, loss = 20020775133.54415131\n",
      "Iteration 115, loss = 20020767745.76218033\n",
      "Iteration 116, loss = 20020760287.19672775\n",
      "Iteration 117, loss = 20020752915.36552811\n",
      "Iteration 118, loss = 20020745533.31541824\n",
      "Iteration 119, loss = 20020738101.75785065\n",
      "Iteration 120, loss = 20020730728.15505600\n",
      "Iteration 121, loss = 20020723308.91473007\n",
      "Iteration 122, loss = 20020715938.83567047\n",
      "Iteration 123, loss = 20020708542.27737045\n",
      "Iteration 124, loss = 20020701182.80524826\n",
      "Iteration 125, loss = 20020693846.45832062\n",
      "Iteration 126, loss = 20020686447.90575790\n",
      "Iteration 127, loss = 20020679108.99764633\n",
      "Iteration 128, loss = 20020671748.85670471\n",
      "Iteration 129, loss = 20020664393.57873535\n",
      "Iteration 130, loss = 20020657050.02747726\n",
      "Iteration 131, loss = 20020649654.10649872\n",
      "Iteration 132, loss = 20020642378.60697556\n",
      "Iteration 133, loss = 20020635026.43924332\n",
      "Iteration 134, loss = 20020627689.13633728\n",
      "Iteration 135, loss = 20020620397.73720932\n",
      "Iteration 136, loss = 20020613039.09095001\n",
      "Iteration 137, loss = 20020605713.10328674\n",
      "Iteration 138, loss = 20020598417.71522141\n",
      "Iteration 139, loss = 20020591107.81497955\n",
      "Iteration 140, loss = 20020583801.95755386\n",
      "Iteration 141, loss = 20020576488.97199249\n",
      "Iteration 142, loss = 20020569212.22397995\n",
      "Iteration 143, loss = 20020561880.03939819\n",
      "Iteration 144, loss = 20020554636.54127502\n",
      "Iteration 145, loss = 20020547319.87901306\n",
      "Iteration 146, loss = 20020540011.46802139\n",
      "Iteration 147, loss = 20020532776.21592331\n",
      "Iteration 148, loss = 20020525455.99149704\n",
      "Iteration 149, loss = 20020518182.90160751\n",
      "Iteration 150, loss = 20020510919.71485901\n",
      "Iteration 151, loss = 20020503628.05422592\n",
      "Iteration 152, loss = 20020496418.47752762\n",
      "Iteration 153, loss = 20020489129.36534500\n",
      "Iteration 154, loss = 20020481836.30816650\n",
      "Iteration 155, loss = 20020474566.31614685\n",
      "Iteration 156, loss = 20020467334.60147858\n",
      "Iteration 157, loss = 20020460067.40731049\n",
      "Iteration 158, loss = 20020452821.43559647\n",
      "Iteration 159, loss = 20020445553.63493347\n",
      "Iteration 160, loss = 20020438295.41188049\n",
      "Iteration 161, loss = 20020431027.99863052\n",
      "Iteration 162, loss = 20020423833.57836151\n",
      "Iteration 163, loss = 20020416541.74443436\n",
      "Iteration 164, loss = 20020409353.29047394\n",
      "Iteration 165, loss = 20020402067.22105789\n",
      "Iteration 166, loss = 20020394841.54613495\n",
      "Iteration 167, loss = 20020387609.70153427\n",
      "Iteration 168, loss = 20020380346.29235840\n",
      "Iteration 169, loss = 20020373128.02220535\n",
      "Iteration 170, loss = 20020365887.52334595\n",
      "Iteration 171, loss = 20020358673.08169556\n",
      "Iteration 172, loss = 20020351495.43331146\n",
      "Iteration 173, loss = 20020344244.63154221\n",
      "Iteration 174, loss = 20020336996.47918701\n",
      "Iteration 175, loss = 20020329777.49195862\n",
      "Iteration 176, loss = 20020322589.02151871\n",
      "Iteration 177, loss = 20020315386.39758682\n",
      "Iteration 178, loss = 20020308210.82694626\n",
      "Iteration 179, loss = 20020300970.46845245\n",
      "Iteration 180, loss = 20020293776.12738800\n",
      "Iteration 181, loss = 20020286517.24666214\n",
      "Iteration 182, loss = 20020279361.18059921\n",
      "Iteration 183, loss = 20020272083.45696259\n",
      "Iteration 184, loss = 20020264953.93558121\n",
      "Iteration 185, loss = 20020257743.13456726\n",
      "Iteration 186, loss = 20020250507.98712540\n",
      "Iteration 187, loss = 20020243332.97068024\n",
      "Iteration 188, loss = 20020236158.05032349\n",
      "Iteration 189, loss = 20020228958.57484818\n",
      "Iteration 190, loss = 20020221711.06904984\n",
      "Iteration 191, loss = 20020214545.42414093\n",
      "Iteration 192, loss = 20020207352.31321716\n",
      "Iteration 193, loss = 20020200148.47107697\n",
      "Iteration 194, loss = 20020192925.42982864\n",
      "Iteration 195, loss = 20020185757.88110733\n",
      "Iteration 196, loss = 20020178541.83189392\n",
      "Iteration 197, loss = 20020171308.46776962\n",
      "Iteration 198, loss = 20020164027.52586365\n",
      "Iteration 199, loss = 20020156789.06284332\n",
      "Iteration 200, loss = 20020149470.68058395\n",
      "Iteration 201, loss = 20020141990.39625168\n",
      "Iteration 202, loss = 20020134355.69894791\n",
      "Iteration 203, loss = 20020126485.00381088\n",
      "Iteration 204, loss = 20020118097.92141342\n",
      "Iteration 205, loss = 20020109433.61558151\n",
      "Iteration 206, loss = 20020100520.89142227\n",
      "Iteration 207, loss = 20020091788.81698608\n",
      "Iteration 208, loss = 20020083089.29970551\n",
      "Iteration 209, loss = 20020074597.13214493\n",
      "Iteration 210, loss = 20020066254.71758652\n",
      "Iteration 211, loss = 20020057981.15447235\n",
      "Iteration 212, loss = 20020049838.19077682\n",
      "Iteration 213, loss = 20020041718.48061752\n",
      "Iteration 214, loss = 20020033638.94642258\n",
      "Iteration 215, loss = 20020025705.87967300\n",
      "Iteration 216, loss = 20020017691.44250488\n",
      "Iteration 217, loss = 20020009811.23810577\n",
      "Iteration 218, loss = 20020001880.95758438\n",
      "Iteration 219, loss = 20019994054.53957367\n",
      "Iteration 220, loss = 20019986252.23808670\n",
      "Iteration 221, loss = 20019978460.88368988\n",
      "Iteration 222, loss = 20019970648.44807053\n",
      "Iteration 223, loss = 20019962885.29674911\n",
      "Iteration 224, loss = 20019955127.80571747\n",
      "Iteration 225, loss = 20019947359.80704880\n",
      "Iteration 226, loss = 20019939702.48237991\n",
      "Iteration 227, loss = 20019931993.61702347\n",
      "Iteration 228, loss = 20019924268.74455643\n",
      "Iteration 229, loss = 20019916628.31937790\n",
      "Iteration 230, loss = 20019908939.33382034\n",
      "Iteration 231, loss = 20019901275.53567123\n",
      "Iteration 232, loss = 20019893688.81431198\n",
      "Iteration 233, loss = 20019886037.17319107\n",
      "Iteration 234, loss = 20019878392.09278870\n",
      "Iteration 235, loss = 20019870792.34750366\n",
      "Iteration 236, loss = 20019863103.67892075\n",
      "Iteration 237, loss = 20019855596.15071106\n",
      "Iteration 238, loss = 20019847973.36189651\n",
      "Iteration 239, loss = 20019840398.15251923\n",
      "Iteration 240, loss = 20019832806.97094345\n",
      "Iteration 241, loss = 20019825259.62363052\n",
      "Iteration 242, loss = 20019817655.52178574\n",
      "Iteration 243, loss = 20019810110.94398880\n",
      "Iteration 244, loss = 20019802546.13440323\n",
      "Iteration 245, loss = 20019795030.39337158\n",
      "Iteration 246, loss = 20019787419.43324280\n",
      "Iteration 247, loss = 20019779946.27151108\n",
      "Iteration 248, loss = 20019772378.32121277\n",
      "Iteration 249, loss = 20019764836.59387589\n",
      "Iteration 250, loss = 20019757304.88999557\n",
      "Iteration 251, loss = 20019749837.75693130\n",
      "Iteration 252, loss = 20019742259.27130508\n",
      "Iteration 253, loss = 20019734765.48607635\n",
      "Iteration 254, loss = 20019727268.59446335\n",
      "Iteration 255, loss = 20019719737.65512466\n",
      "Iteration 256, loss = 20019712203.28774643\n",
      "Iteration 257, loss = 20019704764.77763748\n",
      "Iteration 258, loss = 20019697260.04150009\n",
      "Iteration 259, loss = 20019689748.68659210\n",
      "Iteration 260, loss = 20019682291.26466751\n",
      "Iteration 261, loss = 20019674785.37791443\n",
      "Iteration 262, loss = 20019667319.66316986\n",
      "Iteration 263, loss = 20019659803.43140411\n",
      "Iteration 264, loss = 20019652321.51630402\n",
      "Iteration 265, loss = 20019644853.81837845\n",
      "Iteration 266, loss = 20019637417.04466629\n",
      "Iteration 267, loss = 20019629941.20916367\n",
      "Iteration 268, loss = 20019622459.00515366\n",
      "Iteration 269, loss = 20019614983.16386414\n",
      "Iteration 270, loss = 20019607545.83881760\n",
      "Iteration 271, loss = 20019600099.30429459\n",
      "Iteration 272, loss = 20019592627.33229828\n",
      "Iteration 273, loss = 20019585164.40459061\n",
      "Iteration 274, loss = 20019577727.19855881\n",
      "Iteration 275, loss = 20019570345.23571014\n",
      "Iteration 276, loss = 20019562794.60977554\n",
      "Iteration 277, loss = 20019555384.35136032\n",
      "Iteration 278, loss = 20019547966.58705521\n",
      "Iteration 279, loss = 20019540523.85556412\n",
      "Iteration 280, loss = 20019533124.70143127\n",
      "Iteration 281, loss = 20019525661.03134537\n",
      "Iteration 282, loss = 20019518254.73010254\n",
      "Iteration 283, loss = 20019510817.22505569\n",
      "Iteration 284, loss = 20019503390.36451721\n",
      "Iteration 285, loss = 20019495960.96159363\n",
      "Iteration 286, loss = 20019488594.38059998\n",
      "Iteration 287, loss = 20019481147.88060379\n",
      "Iteration 288, loss = 20019473695.03365326\n",
      "Iteration 289, loss = 20019466270.08447647\n",
      "Iteration 290, loss = 20019458919.39041901\n",
      "Iteration 291, loss = 20019451525.60046005\n",
      "Iteration 292, loss = 20019444078.54801559\n",
      "Iteration 293, loss = 20019436640.67371750\n",
      "Iteration 294, loss = 20019429284.03188705\n",
      "Iteration 295, loss = 20019421823.08618546\n",
      "Iteration 296, loss = 20019414436.82106400\n",
      "Iteration 297, loss = 20019407069.99998474\n",
      "Iteration 298, loss = 20019399627.41042328\n",
      "Iteration 299, loss = 20019392255.60658646\n",
      "Iteration 300, loss = 20019384826.43308258\n",
      "Iteration 301, loss = 20019377447.12426758\n",
      "Iteration 302, loss = 20019370045.49982452\n",
      "Iteration 303, loss = 20019362646.21304703\n",
      "Iteration 304, loss = 20019355232.50264359\n",
      "Iteration 305, loss = 20019347903.91043091\n",
      "Iteration 306, loss = 20019340471.25828171\n",
      "Iteration 307, loss = 20019333055.62625885\n",
      "Iteration 308, loss = 20019325730.62071991\n",
      "Iteration 309, loss = 20019318282.46469498\n",
      "Iteration 310, loss = 20019310917.38233948\n",
      "Iteration 311, loss = 20019303544.51042175\n",
      "Iteration 312, loss = 20019296165.92715073\n",
      "Iteration 313, loss = 20019288780.17183685\n",
      "Iteration 314, loss = 20019281404.44801331\n",
      "Iteration 315, loss = 20019274006.36751175\n",
      "Iteration 316, loss = 20019266647.72117996\n",
      "Iteration 317, loss = 20019259262.61712646\n",
      "Iteration 318, loss = 20019251915.09035492\n",
      "Iteration 319, loss = 20019244545.81272507\n",
      "Iteration 320, loss = 20019237159.90446472\n",
      "Iteration 321, loss = 20019229730.19617844\n",
      "Iteration 322, loss = 20019222426.27493668\n",
      "Iteration 323, loss = 20019215082.29432678\n",
      "Iteration 324, loss = 20019207692.37980270\n",
      "Iteration 325, loss = 20019200300.68128204\n",
      "Iteration 326, loss = 20019192939.40183640\n",
      "Iteration 327, loss = 20019185612.76750565\n",
      "Iteration 328, loss = 20019178271.54954529\n",
      "Iteration 329, loss = 20019170860.64643860\n",
      "Iteration 330, loss = 20019163543.24819946\n",
      "Iteration 331, loss = 20019156153.97628021\n",
      "Iteration 332, loss = 20019148769.09056473\n",
      "Iteration 333, loss = 20019141427.63884735\n",
      "Iteration 334, loss = 20019134081.40523148\n",
      "Iteration 335, loss = 20019126677.23128891\n",
      "Iteration 336, loss = 20019119361.61548996\n",
      "Iteration 337, loss = 20019111996.33336258\n",
      "Iteration 338, loss = 20019104675.41321945\n",
      "Iteration 339, loss = 20019097302.36756897\n",
      "Iteration 340, loss = 20019089960.68797302\n",
      "Iteration 341, loss = 20019082610.72698593\n",
      "Iteration 342, loss = 20019075233.71436310\n",
      "Iteration 343, loss = 20019067847.27883148\n",
      "Iteration 344, loss = 20019060550.89649963\n",
      "Iteration 345, loss = 20019053195.95260239\n",
      "Iteration 346, loss = 20019045794.01517868\n",
      "Iteration 347, loss = 20019038429.73424530\n",
      "Iteration 348, loss = 20019031109.41566086\n",
      "Iteration 349, loss = 20019023720.70046616\n",
      "Iteration 350, loss = 20019016295.10036850\n",
      "Iteration 351, loss = 20019008815.22185516\n",
      "Iteration 352, loss = 20019001351.95038223\n",
      "Iteration 353, loss = 20018993745.33423615\n",
      "Iteration 354, loss = 20018985994.91266251\n",
      "Iteration 355, loss = 20018977870.14038086\n",
      "Iteration 356, loss = 20018969365.34175110\n",
      "Iteration 357, loss = 20018960475.43987274\n",
      "Iteration 358, loss = 20018951469.40883636\n",
      "Iteration 359, loss = 20018942441.54175186\n",
      "Iteration 360, loss = 20018933658.51264572\n",
      "Iteration 361, loss = 20018924948.03596497\n",
      "Iteration 362, loss = 20018916382.54194641\n",
      "Iteration 363, loss = 20018907953.63722610\n",
      "Iteration 364, loss = 20018899555.56680298\n",
      "Iteration 365, loss = 20018891202.57162476\n",
      "Iteration 366, loss = 20018882953.13970947\n",
      "Iteration 367, loss = 20018874767.52675247\n",
      "Iteration 368, loss = 20018866560.14634323\n",
      "Iteration 369, loss = 20018858460.75406647\n",
      "Iteration 370, loss = 20018850328.35799408\n",
      "Iteration 371, loss = 20018842290.18617630\n",
      "Iteration 372, loss = 20018834221.35860062\n",
      "Iteration 373, loss = 20018826165.51515198\n",
      "Iteration 374, loss = 20018818185.92991638\n",
      "Iteration 375, loss = 20018810229.77038193\n",
      "Iteration 376, loss = 20018802253.90871048\n",
      "Iteration 377, loss = 20018794259.77022552\n",
      "Iteration 378, loss = 20018786347.15773010\n",
      "Iteration 379, loss = 20018778390.01083755\n",
      "Iteration 380, loss = 20018770507.15493393\n",
      "Iteration 381, loss = 20018762658.29149246\n",
      "Iteration 382, loss = 20018754719.34746170\n",
      "Iteration 383, loss = 20018746862.13002396\n",
      "Iteration 384, loss = 20018738980.19965363\n",
      "Iteration 385, loss = 20018731152.78706741\n",
      "Iteration 386, loss = 20018723271.62369919\n",
      "Iteration 387, loss = 20018715454.68957901\n",
      "Iteration 388, loss = 20018707630.54975891\n",
      "Iteration 389, loss = 20018699793.97957611\n",
      "Iteration 390, loss = 20018692025.75842285\n",
      "Iteration 391, loss = 20018684195.45643616\n",
      "Iteration 392, loss = 20018676427.05713654\n",
      "Iteration 393, loss = 20018668586.34764099\n",
      "Iteration 394, loss = 20018660842.90272522\n",
      "Iteration 395, loss = 20018653016.95876312\n",
      "Iteration 396, loss = 20018645325.86167145\n",
      "Iteration 397, loss = 20018637493.03849411\n",
      "Iteration 398, loss = 20018629754.03031158\n",
      "Iteration 399, loss = 20018622026.44997787\n",
      "Iteration 400, loss = 20018614231.58058167\n",
      "Iteration 401, loss = 20018606505.47653198\n",
      "Iteration 402, loss = 20018598804.50933075\n",
      "Iteration 403, loss = 20018591027.91650391\n",
      "Iteration 404, loss = 20018583311.40359879\n",
      "Iteration 405, loss = 20018575555.11394882\n",
      "Iteration 406, loss = 20018567844.38611603\n",
      "Iteration 407, loss = 20018560120.48341370\n",
      "Iteration 408, loss = 20018552420.70765686\n",
      "Iteration 409, loss = 20018544705.29311752\n",
      "Iteration 410, loss = 20018537024.11145020\n",
      "Iteration 411, loss = 20018529295.63383484\n",
      "Iteration 412, loss = 20018521627.81185150\n",
      "Iteration 413, loss = 20018513924.22637939\n",
      "Iteration 414, loss = 20018506200.66865158\n",
      "Iteration 415, loss = 20018498573.03389740\n",
      "Iteration 416, loss = 20018490835.07561493\n",
      "Iteration 417, loss = 20018483195.40559006\n",
      "Iteration 418, loss = 20018475483.76702499\n",
      "Iteration 419, loss = 20018467856.65390015\n",
      "Iteration 420, loss = 20018460135.62253189\n",
      "Iteration 421, loss = 20018452495.10015106\n",
      "Iteration 422, loss = 20018444841.59537506\n",
      "Iteration 423, loss = 20018437149.24838638\n",
      "Iteration 424, loss = 20018429483.21623993\n",
      "Iteration 425, loss = 20018421827.02256775\n",
      "Iteration 426, loss = 20018414180.00304031\n",
      "Iteration 427, loss = 20018406508.76096725\n",
      "Iteration 428, loss = 20018398880.18832779\n",
      "Iteration 429, loss = 20018391173.14576721\n",
      "Iteration 430, loss = 20018383593.47213364\n",
      "Iteration 431, loss = 20018375920.87458801\n",
      "Iteration 432, loss = 20018368247.43140030\n",
      "Iteration 433, loss = 20018360628.16059113\n",
      "Iteration 434, loss = 20018352972.35448456\n",
      "Iteration 435, loss = 20018345375.84719467\n",
      "Iteration 436, loss = 20018337731.11925888\n",
      "Iteration 437, loss = 20018330106.28463364\n",
      "Iteration 438, loss = 20018322394.37477493\n",
      "Iteration 439, loss = 20018314820.68961334\n",
      "Iteration 440, loss = 20018307217.55110931\n",
      "Iteration 441, loss = 20018299580.11207962\n",
      "Iteration 442, loss = 20018291923.13858032\n",
      "Iteration 443, loss = 20018284328.77841949\n",
      "Iteration 444, loss = 20018276657.20582581\n",
      "Iteration 445, loss = 20018269084.66519165\n",
      "Iteration 446, loss = 20018261467.39169693\n",
      "Iteration 447, loss = 20018253815.46928406\n",
      "Iteration 448, loss = 20018246223.32600021\n",
      "Iteration 449, loss = 20018238650.67795563\n",
      "Iteration 450, loss = 20018230942.44072723\n",
      "Iteration 451, loss = 20018223368.54652786\n",
      "Iteration 452, loss = 20018215748.27784729\n",
      "Iteration 453, loss = 20018208146.28005219\n",
      "Iteration 454, loss = 20018200557.60591125\n",
      "Iteration 455, loss = 20018192953.72513580\n",
      "Iteration 456, loss = 20018185293.85068512\n",
      "Iteration 457, loss = 20018177755.83399200\n",
      "Iteration 458, loss = 20018170129.33180237\n",
      "Iteration 459, loss = 20018162525.26331711\n",
      "Iteration 460, loss = 20018154910.97494888\n",
      "Iteration 461, loss = 20018147343.90023804\n",
      "Iteration 462, loss = 20018139688.26657104\n",
      "Iteration 463, loss = 20018132114.96118927\n",
      "Iteration 464, loss = 20018124573.50236511\n",
      "Iteration 465, loss = 20018116936.00914764\n",
      "Iteration 466, loss = 20018109353.35386276\n",
      "Iteration 467, loss = 20018101770.97057724\n",
      "Iteration 468, loss = 20018094177.01556396\n",
      "Iteration 469, loss = 20018086613.79607391\n",
      "Iteration 470, loss = 20018079014.16127396\n",
      "Iteration 471, loss = 20018071374.71639633\n",
      "Iteration 472, loss = 20018063827.71780014\n",
      "Iteration 473, loss = 20018056258.82743073\n",
      "Iteration 474, loss = 20018048647.47475815\n",
      "Iteration 475, loss = 20018041075.58551025\n",
      "Iteration 476, loss = 20018033535.96231461\n",
      "Iteration 477, loss = 20018025901.94090271\n",
      "Iteration 478, loss = 20018018345.39075089\n",
      "Iteration 479, loss = 20018010794.68938065\n",
      "Iteration 480, loss = 20018003165.49945068\n",
      "Iteration 481, loss = 20017995616.51721954\n",
      "Iteration 482, loss = 20017988060.05263901\n",
      "Iteration 483, loss = 20017980471.95177841\n",
      "Iteration 484, loss = 20017972856.89551544\n",
      "Iteration 485, loss = 20017965345.73829651\n",
      "Iteration 486, loss = 20017957733.71888733\n",
      "Iteration 487, loss = 20017950216.65048981\n",
      "Iteration 488, loss = 20017942615.92472076\n",
      "Iteration 489, loss = 20017935041.87066650\n",
      "Iteration 490, loss = 20017927534.65655899\n",
      "Iteration 491, loss = 20017919922.86012650\n",
      "Iteration 492, loss = 20017912354.75921631\n",
      "Iteration 493, loss = 20017904787.74653625\n",
      "Iteration 494, loss = 20017897228.66259003\n",
      "Iteration 495, loss = 20017889695.78675461\n",
      "Iteration 496, loss = 20017882137.57161713\n",
      "Iteration 497, loss = 20017874624.27275848\n",
      "Iteration 498, loss = 20017866977.94928741\n",
      "Iteration 499, loss = 20017859458.86676788\n",
      "Iteration 500, loss = 20017851961.66936493\n",
      "Iteration 1, loss = 17987651209.31570435\n",
      "Iteration 2, loss = 17987647584.80769730\n",
      "Iteration 3, loss = 17987643960.02605057\n",
      "Iteration 4, loss = 17987640335.05905533\n",
      "Iteration 5, loss = 17987636710.00203705\n",
      "Iteration 6, loss = 17987633084.84444046\n",
      "Iteration 7, loss = 17987629459.42509842\n",
      "Iteration 8, loss = 17987625833.48442459\n",
      "Iteration 9, loss = 17987622206.75073624\n",
      "Iteration 10, loss = 17987618578.97800827\n",
      "Iteration 11, loss = 17987614949.94501114\n",
      "Iteration 12, loss = 17987611319.44139481\n",
      "Iteration 13, loss = 17987607687.25175858\n",
      "Iteration 14, loss = 17987604053.14395142\n",
      "Iteration 15, loss = 17987600416.86408234\n",
      "Iteration 16, loss = 17987596778.13855743\n",
      "Iteration 17, loss = 17987593136.68161011\n",
      "Iteration 18, loss = 17987589492.20529938\n",
      "Iteration 19, loss = 17987585844.42850494\n",
      "Iteration 20, loss = 17987582193.08242035\n",
      "Iteration 21, loss = 17987578537.91205215\n",
      "Iteration 22, loss = 17987574878.67454529\n",
      "Iteration 23, loss = 17987571215.13577652\n",
      "Iteration 24, loss = 17987567547.06676483\n",
      "Iteration 25, loss = 17987563874.24079514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 17987560196.43173218\n",
      "Iteration 27, loss = 17987556513.41358566\n",
      "Iteration 28, loss = 17987552824.96107101\n",
      "Iteration 29, loss = 17987549130.85098648\n",
      "Iteration 30, loss = 17987545430.86392975\n",
      "Iteration 31, loss = 17987541724.78612518\n",
      "Iteration 32, loss = 17987538012.41109848\n",
      "Iteration 33, loss = 17987534293.54112244\n",
      "Iteration 34, loss = 17987530567.98825836\n",
      "Iteration 35, loss = 17987526835.57522202\n",
      "Iteration 36, loss = 17987523096.13603592\n",
      "Iteration 37, loss = 17987519349.51663589\n",
      "Iteration 38, loss = 17987515595.57551956\n",
      "Iteration 39, loss = 17987511834.18454742\n",
      "Iteration 40, loss = 17987508065.22985458\n",
      "Iteration 41, loss = 17987504288.61299896\n",
      "Iteration 42, loss = 17987500504.25217438\n",
      "Iteration 43, loss = 17987496712.08362579\n",
      "Iteration 44, loss = 17987492912.06300354\n",
      "Iteration 45, loss = 17987489104.16679764\n",
      "Iteration 46, loss = 17987485288.39366913\n",
      "Iteration 47, loss = 17987481464.76569366\n",
      "Iteration 48, loss = 17987477633.32949829\n",
      "Iteration 49, loss = 17987473794.15720749\n",
      "Iteration 50, loss = 17987469947.34724045\n",
      "Iteration 51, loss = 17987466093.02494431\n",
      "Iteration 52, loss = 17987462231.34299469\n",
      "Iteration 53, loss = 17987458362.48170090\n",
      "Iteration 54, loss = 17987454486.64904785\n",
      "Iteration 55, loss = 17987450604.08069229\n",
      "Iteration 56, loss = 17987446715.03975296\n",
      "Iteration 57, loss = 17987442819.81647873\n",
      "Iteration 58, loss = 17987438918.72782516\n",
      "Iteration 59, loss = 17987435012.11689758\n",
      "Iteration 60, loss = 17987431100.35234451\n",
      "Iteration 61, loss = 17987427183.82760239\n",
      "Iteration 62, loss = 17987423262.96013260\n",
      "Iteration 63, loss = 17987419338.19050217\n",
      "Iteration 64, loss = 17987415409.98148727\n",
      "Iteration 65, loss = 17987411478.81704712\n",
      "Iteration 66, loss = 17987407545.20127487\n",
      "Iteration 67, loss = 17987403609.65723801\n",
      "Iteration 68, loss = 17987399672.72587204\n",
      "Iteration 69, loss = 17987395734.96466064\n",
      "Iteration 70, loss = 17987391796.94640350\n",
      "Iteration 71, loss = 17987387859.25784302\n",
      "Iteration 72, loss = 17987383922.49828339\n",
      "Iteration 73, loss = 17987379987.27817917\n",
      "Iteration 74, loss = 17987376054.21763992\n",
      "Iteration 75, loss = 17987372123.94501114\n",
      "Iteration 76, loss = 17987368197.09533691\n",
      "Iteration 77, loss = 17987364274.30888748\n",
      "Iteration 78, loss = 17987360356.22962570\n",
      "Iteration 79, loss = 17987356443.50375748\n",
      "Iteration 80, loss = 17987352536.77821350\n",
      "Iteration 81, loss = 17987348636.69923401\n",
      "Iteration 82, loss = 17987344743.91093445\n",
      "Iteration 83, loss = 17987340859.05392838\n",
      "Iteration 84, loss = 17987336982.76402283\n",
      "Iteration 85, loss = 17987333115.67090225\n",
      "Iteration 86, loss = 17987329258.39698792\n",
      "Iteration 87, loss = 17987325411.55624390\n",
      "Iteration 88, loss = 17987321575.75316238\n",
      "Iteration 89, loss = 17987317751.58174515\n",
      "Iteration 90, loss = 17987313939.62466049\n",
      "Iteration 91, loss = 17987310140.45235825\n",
      "Iteration 92, loss = 17987306354.62241364\n",
      "Iteration 93, loss = 17987302582.67884064\n",
      "Iteration 94, loss = 17987298825.15155411\n",
      "Iteration 95, loss = 17987295082.55592728\n",
      "Iteration 96, loss = 17987291355.39238358\n",
      "Iteration 97, loss = 17987287644.14610291\n",
      "Iteration 98, loss = 17987283949.28683472\n",
      "Iteration 99, loss = 17987280271.26876068\n",
      "Iteration 100, loss = 17987276610.53041458\n",
      "Iteration 101, loss = 17987272967.49469757\n",
      "Iteration 102, loss = 17987269342.56893921\n",
      "Iteration 103, loss = 17987265736.14497375\n",
      "Iteration 104, loss = 17987262148.59936523\n",
      "Iteration 105, loss = 17987258580.29347229\n",
      "Iteration 106, loss = 17987255031.57376862\n",
      "Iteration 107, loss = 17987251502.77194977\n",
      "Iteration 108, loss = 17987247994.20523834\n",
      "Iteration 109, loss = 17987244506.17658997\n",
      "Iteration 110, loss = 17987241038.97492599\n",
      "Iteration 111, loss = 17987237592.87535477\n",
      "Iteration 112, loss = 17987234168.13936234\n",
      "Iteration 113, loss = 17987230765.01506805\n",
      "Iteration 114, loss = 17987227383.73733521\n",
      "Iteration 115, loss = 17987224024.52796555\n",
      "Iteration 116, loss = 17987220687.59584427\n",
      "Iteration 117, loss = 17987217373.13703918\n",
      "Iteration 118, loss = 17987214081.33491516\n",
      "Iteration 119, loss = 17987210812.36019516\n",
      "Iteration 120, loss = 17987207566.37106705\n",
      "Iteration 121, loss = 17987204343.51318359\n",
      "Iteration 122, loss = 17987201143.91975784\n",
      "Iteration 123, loss = 17987197967.71158218\n",
      "Iteration 124, loss = 17987194814.99707413\n",
      "Iteration 125, loss = 17987191685.87231445\n",
      "Iteration 126, loss = 17987188580.42105103\n",
      "Iteration 127, loss = 17987185498.71479797\n",
      "Iteration 128, loss = 17987182440.81282425\n",
      "Iteration 129, loss = 17987179406.76223755\n",
      "Iteration 130, loss = 17987176396.59798431\n",
      "Iteration 131, loss = 17987173410.34292603\n",
      "Iteration 132, loss = 17987170448.00790024\n",
      "Iteration 133, loss = 17987167509.59172440\n",
      "Iteration 134, loss = 17987164595.08128738\n",
      "Iteration 135, loss = 17987161704.45154953\n",
      "Iteration 136, loss = 17987158837.66558838\n",
      "Iteration 137, loss = 17987155994.67462540\n",
      "Iteration 138, loss = 17987153175.41803741\n",
      "Iteration 139, loss = 17987150379.82332230\n",
      "Iteration 140, loss = 17987147607.80615234\n",
      "Iteration 141, loss = 17987144859.27034760\n",
      "Iteration 142, loss = 17987142134.10784531\n",
      "Iteration 143, loss = 17987139432.19870758\n",
      "Iteration 144, loss = 17987136753.41115952\n",
      "Iteration 145, loss = 17987134097.60156250\n",
      "Iteration 146, loss = 17987131464.61452866\n",
      "Iteration 147, loss = 17987128854.28291702\n",
      "Iteration 148, loss = 17987126266.42803955\n",
      "Iteration 149, loss = 17987123700.85976028\n",
      "Iteration 150, loss = 17987121157.37673950\n",
      "Iteration 151, loss = 17987118635.76667404\n",
      "Iteration 152, loss = 17987116135.80662537\n",
      "Iteration 153, loss = 17987113657.26338196\n",
      "Iteration 154, loss = 17987111199.89393997\n",
      "Iteration 155, loss = 17987108763.44595337\n",
      "Iteration 156, loss = 17987106347.65827560\n",
      "Iteration 157, loss = 17987103952.26156616\n",
      "Iteration 158, loss = 17987101576.97887039\n",
      "Iteration 159, loss = 17987099221.52629471\n",
      "Iteration 160, loss = 17987096885.61360550\n",
      "Iteration 161, loss = 17987094568.94490814\n",
      "Iteration 162, loss = 17987092271.21937561\n",
      "Iteration 163, loss = 17987089992.13183212\n",
      "Iteration 164, loss = 17987087731.37348938\n",
      "Iteration 165, loss = 17987085488.63252640\n",
      "Iteration 166, loss = 17987083263.59480286\n",
      "Iteration 167, loss = 17987081055.94439697\n",
      "Iteration 168, loss = 17987078865.36424255\n",
      "Iteration 169, loss = 17987076691.53666306\n",
      "Iteration 170, loss = 17987074534.14390182\n",
      "Iteration 171, loss = 17987072392.86861801\n",
      "Iteration 172, loss = 17987070267.39436722\n",
      "Iteration 173, loss = 17987068157.40602112\n",
      "Iteration 174, loss = 17987066062.59017563\n",
      "Iteration 175, loss = 17987063982.63550186\n",
      "Iteration 176, loss = 17987061917.23314667\n",
      "Iteration 177, loss = 17987059866.07699966\n",
      "Iteration 178, loss = 17987057828.86400223\n",
      "Iteration 179, loss = 17987055805.29444504\n",
      "Iteration 180, loss = 17987053795.07218170\n",
      "Iteration 181, loss = 17987051797.90489578\n",
      "Iteration 182, loss = 17987049813.50429916\n",
      "Iteration 183, loss = 17987047841.58637238\n",
      "Iteration 184, loss = 17987045881.87150955\n",
      "Iteration 185, loss = 17987043934.08475876\n",
      "Iteration 186, loss = 17987041997.95595551\n",
      "Iteration 187, loss = 17987040073.21994019\n",
      "Iteration 188, loss = 17987038159.61666107\n",
      "Iteration 189, loss = 17987036256.89138794\n",
      "Iteration 190, loss = 17987034364.79483414\n",
      "Iteration 191, loss = 17987032483.08328247\n",
      "Iteration 192, loss = 17987030611.51874924\n",
      "Iteration 193, loss = 17987028749.86908340\n",
      "Iteration 194, loss = 17987026897.90803146\n",
      "Iteration 195, loss = 17987025055.41540909\n",
      "Iteration 196, loss = 17987023222.17708206\n",
      "Iteration 197, loss = 17987021397.98507309\n",
      "Iteration 198, loss = 17987019582.63755798\n",
      "Iteration 199, loss = 17987017775.93886948\n",
      "Iteration 200, loss = 17987015977.69949722\n",
      "Iteration 201, loss = 17987014187.73601532\n",
      "Iteration 202, loss = 17987012405.87102509\n",
      "Iteration 203, loss = 17987010631.93306732\n",
      "Iteration 204, loss = 17987008865.75650787\n",
      "Iteration 205, loss = 17987007107.18138123\n",
      "Iteration 206, loss = 17987005356.05327606\n",
      "Iteration 207, loss = 17987003612.22315216\n",
      "Iteration 208, loss = 17987001875.54715729\n",
      "Iteration 209, loss = 17987000145.88645554\n",
      "Iteration 210, loss = 17986998423.10701370\n",
      "Iteration 211, loss = 17986996707.07941055\n",
      "Iteration 212, loss = 17986994997.67865372\n",
      "Iteration 213, loss = 17986993294.78395081\n",
      "Iteration 214, loss = 17986991598.27851868\n",
      "Iteration 215, loss = 17986989908.04940033\n",
      "Iteration 216, loss = 17986988223.98723602\n",
      "Iteration 217, loss = 17986986545.98611832\n",
      "Iteration 218, loss = 17986984873.94338226\n",
      "Iteration 219, loss = 17986983207.75942230\n",
      "Iteration 220, loss = 17986981547.33753204\n",
      "Iteration 221, loss = 17986979892.58374023\n",
      "Iteration 222, loss = 17986978243.40664291\n",
      "Iteration 223, loss = 17986976599.71725082\n",
      "Iteration 224, loss = 17986974961.42884064\n",
      "Iteration 225, loss = 17986973328.45681000\n",
      "Iteration 226, loss = 17986971700.71855164\n",
      "Iteration 227, loss = 17986970078.13328552\n",
      "Iteration 228, loss = 17986968460.62196732\n",
      "Iteration 229, loss = 17986966848.10712433\n",
      "Iteration 230, loss = 17986965240.51276016\n",
      "Iteration 231, loss = 17986963637.76419449\n",
      "Iteration 232, loss = 17986962039.78797531\n",
      "Iteration 233, loss = 17986960446.51169968\n",
      "Iteration 234, loss = 17986958857.86394501\n",
      "Iteration 235, loss = 17986957273.77408600\n",
      "Iteration 236, loss = 17986955694.17218781\n",
      "Iteration 237, loss = 17986954118.98884964\n",
      "Iteration 238, loss = 17986952548.15507126\n",
      "Iteration 239, loss = 17986950981.60210037\n",
      "Iteration 240, loss = 17986949419.26127243\n",
      "Iteration 241, loss = 17986947861.06386566\n",
      "Iteration 242, loss = 17986946306.94095993\n",
      "Iteration 243, loss = 17986944756.82326126\n",
      "Iteration 244, loss = 17986943210.64096451\n",
      "Iteration 245, loss = 17986941668.32363510\n",
      "Iteration 246, loss = 17986940129.80002594\n",
      "Iteration 247, loss = 17986938594.99803925\n",
      "Iteration 248, loss = 17986937063.84458542\n",
      "Iteration 249, loss = 17986935536.26552582\n",
      "Iteration 250, loss = 17986934012.18562317\n",
      "Iteration 251, loss = 17986932491.52851486\n",
      "Iteration 252, loss = 17986930974.21673203\n",
      "Iteration 253, loss = 17986929460.17171478\n",
      "Iteration 254, loss = 17986927949.31389618\n",
      "Iteration 255, loss = 17986926441.56278229\n",
      "Iteration 256, loss = 17986924936.83707809\n",
      "Iteration 257, loss = 17986923435.05481339\n",
      "Iteration 258, loss = 17986921936.13356781\n",
      "Iteration 259, loss = 17986920439.99058533\n",
      "Iteration 260, loss = 17986918946.54306030\n",
      "Iteration 261, loss = 17986917455.70832443\n",
      "Iteration 262, loss = 17986915967.40409851\n",
      "Iteration 263, loss = 17986914481.54875946\n",
      "Iteration 264, loss = 17986912998.06159210\n",
      "Iteration 265, loss = 17986911516.86307907\n",
      "Iteration 266, loss = 17986910037.87514496\n",
      "Iteration 267, loss = 17986908561.02144623\n",
      "Iteration 268, loss = 17986907086.22765350\n",
      "Iteration 269, loss = 17986905613.42171478\n",
      "Iteration 270, loss = 17986904142.53410339\n",
      "Iteration 271, loss = 17986902673.49808502\n",
      "Iteration 272, loss = 17986901206.24996567\n",
      "Iteration 273, loss = 17986899740.72930145\n",
      "Iteration 274, loss = 17986898276.87912369\n",
      "Iteration 275, loss = 17986896814.64614868\n",
      "Iteration 276, loss = 17986895353.98091125\n",
      "Iteration 277, loss = 17986893894.83796692\n",
      "Iteration 278, loss = 17986892437.17601013\n",
      "Iteration 279, loss = 17986890980.95794296\n",
      "Iteration 280, loss = 17986889526.15100861\n",
      "Iteration 281, loss = 17986888072.72677994\n",
      "Iteration 282, loss = 17986886620.66122818\n",
      "Iteration 283, loss = 17986885169.93468094\n",
      "Iteration 284, loss = 17986883720.53178406\n",
      "Iteration 285, loss = 17986882272.44142151\n",
      "Iteration 286, loss = 17986880825.65661240\n",
      "Iteration 287, loss = 17986879380.17436600\n",
      "Iteration 288, loss = 17986877935.99551010\n",
      "Iteration 289, loss = 17986876493.12449265\n",
      "Iteration 290, loss = 17986875051.56916809\n",
      "Iteration 291, loss = 17986873611.34054947\n",
      "Iteration 292, loss = 17986872172.45252991\n",
      "Iteration 293, loss = 17986870734.92162704\n",
      "Iteration 294, loss = 17986869298.76665497\n",
      "Iteration 295, loss = 17986867864.00844193\n",
      "Iteration 296, loss = 17986866430.66952515\n",
      "Iteration 297, loss = 17986864998.77382660\n",
      "Iteration 298, loss = 17986863568.34634018\n",
      "Iteration 299, loss = 17986862139.41286469\n",
      "Iteration 300, loss = 17986860711.99966431\n",
      "Iteration 301, loss = 17986859286.13322830\n",
      "Iteration 302, loss = 17986857861.83998108\n",
      "Iteration 303, loss = 17986856439.14604568\n",
      "Iteration 304, loss = 17986855018.07700348\n",
      "Iteration 305, loss = 17986853598.65770721\n",
      "Iteration 306, loss = 17986852180.91207123\n",
      "Iteration 307, loss = 17986850764.86291122\n",
      "Iteration 308, loss = 17986849350.53181839\n",
      "Iteration 309, loss = 17986847937.93901443\n",
      "Iteration 310, loss = 17986846527.10326767\n",
      "Iteration 311, loss = 17986845118.04180145\n",
      "Iteration 312, loss = 17986843710.77025604\n",
      "Iteration 313, loss = 17986842305.30261993\n",
      "Iteration 314, loss = 17986840901.65122604\n",
      "Iteration 315, loss = 17986839499.82673264\n",
      "Iteration 316, loss = 17986838099.83815002\n",
      "Iteration 317, loss = 17986836701.69282532\n",
      "Iteration 318, loss = 17986835305.39650345\n",
      "Iteration 319, loss = 17986833910.95335388\n",
      "Iteration 320, loss = 17986832518.36600494\n",
      "Iteration 321, loss = 17986831127.63563919\n",
      "Iteration 322, loss = 17986829738.76200485\n",
      "Iteration 323, loss = 17986828351.74351883\n",
      "Iteration 324, loss = 17986826966.57730484\n",
      "Iteration 325, loss = 17986825583.25928497\n",
      "Iteration 326, loss = 17986824201.78424072\n",
      "Iteration 327, loss = 17986822822.14590454\n",
      "Iteration 328, loss = 17986821444.33698273\n",
      "Iteration 329, loss = 17986820068.34928513\n",
      "Iteration 330, loss = 17986818694.17375183\n",
      "Iteration 331, loss = 17986817321.80055237\n",
      "Iteration 332, loss = 17986815951.21911240\n",
      "Iteration 333, loss = 17986814582.41822052\n",
      "Iteration 334, loss = 17986813215.38606262\n",
      "Iteration 335, loss = 17986811850.11029434\n",
      "Iteration 336, loss = 17986810486.57807159\n",
      "Iteration 337, loss = 17986809124.77613831\n",
      "Iteration 338, loss = 17986807764.69085312\n",
      "Iteration 339, loss = 17986806406.30823898\n",
      "Iteration 340, loss = 17986805049.61401749\n",
      "Iteration 341, loss = 17986803694.59369278\n",
      "Iteration 342, loss = 17986802341.23252487\n",
      "Iteration 343, loss = 17986800989.51562119\n",
      "Iteration 344, loss = 17986799639.42794037\n",
      "Iteration 345, loss = 17986798290.95432281\n",
      "Iteration 346, loss = 17986796944.07954025\n",
      "Iteration 347, loss = 17986795598.78829575\n",
      "Iteration 348, loss = 17986794255.06526947\n",
      "Iteration 349, loss = 17986792912.89511871\n",
      "Iteration 350, loss = 17986791572.26253128\n",
      "Iteration 351, loss = 17986790233.15217972\n",
      "Iteration 352, loss = 17986788895.54883194\n",
      "Iteration 353, loss = 17986787559.43728256\n",
      "Iteration 354, loss = 17986786224.80240250\n",
      "Iteration 355, loss = 17986784891.62915802\n",
      "Iteration 356, loss = 17986783559.90260315\n",
      "Iteration 357, loss = 17986782229.60789490\n",
      "Iteration 358, loss = 17986780900.73031616\n",
      "Iteration 359, loss = 17986779573.25526047\n",
      "Iteration 360, loss = 17986778247.16825867\n",
      "Iteration 361, loss = 17986776922.45496368\n",
      "Iteration 362, loss = 17986775599.10117722\n",
      "Iteration 363, loss = 17986774277.09284973\n",
      "Iteration 364, loss = 17986772956.41607285\n",
      "Iteration 365, loss = 17986771637.05709457\n",
      "Iteration 366, loss = 17986770319.00231552\n",
      "Iteration 367, loss = 17986769002.23829269\n",
      "Iteration 368, loss = 17986767686.75173569\n",
      "Iteration 369, loss = 17986766372.52952576\n",
      "Iteration 370, loss = 17986765059.55869293\n",
      "Iteration 371, loss = 17986763747.82644272\n",
      "Iteration 372, loss = 17986762437.32012558\n",
      "Iteration 373, loss = 17986761128.02726364\n",
      "Iteration 374, loss = 17986759819.93553162\n",
      "Iteration 375, loss = 17986758513.03278351\n",
      "Iteration 376, loss = 17986757207.30701828\n",
      "Iteration 377, loss = 17986755902.74639130\n",
      "Iteration 378, loss = 17986754599.33921814\n",
      "Iteration 379, loss = 17986753297.07399368\n",
      "Iteration 380, loss = 17986751995.93932724\n",
      "Iteration 381, loss = 17986750695.92403412\n",
      "Iteration 382, loss = 17986749397.01702881\n",
      "Iteration 383, loss = 17986748099.20741272\n",
      "Iteration 384, loss = 17986746802.48443222\n",
      "Iteration 385, loss = 17986745506.83746338\n",
      "Iteration 386, loss = 17986744212.25605392\n",
      "Iteration 387, loss = 17986742918.72988129\n",
      "Iteration 388, loss = 17986741626.24876404\n",
      "Iteration 389, loss = 17986740334.80266953\n",
      "Iteration 390, loss = 17986739044.38169861\n",
      "Iteration 391, loss = 17986737754.97608948\n",
      "Iteration 392, loss = 17986736466.57621765\n",
      "Iteration 393, loss = 17986735179.17258453\n",
      "Iteration 394, loss = 17986733892.75583649\n",
      "Iteration 395, loss = 17986732607.31673050\n",
      "Iteration 396, loss = 17986731322.84616470\n",
      "Iteration 397, loss = 17986730039.33515930\n",
      "Iteration 398, loss = 17986728756.77485657\n",
      "Iteration 399, loss = 17986727475.15651321\n",
      "Iteration 400, loss = 17986726194.47151566\n",
      "Iteration 401, loss = 17986724914.71136856\n",
      "Iteration 402, loss = 17986723635.86767960\n",
      "Iteration 403, loss = 17986722357.93218231\n",
      "Iteration 404, loss = 17986721080.89671707\n",
      "Iteration 405, loss = 17986719804.75323486\n",
      "Iteration 406, loss = 17986718529.49378586\n",
      "Iteration 407, loss = 17986717255.11054993\n",
      "Iteration 408, loss = 17986715981.59578705\n",
      "Iteration 409, loss = 17986714708.94187164\n",
      "Iteration 410, loss = 17986713437.14128494\n",
      "Iteration 411, loss = 17986712166.18658829\n",
      "Iteration 412, loss = 17986710896.07046127\n",
      "Iteration 413, loss = 17986709626.78567123\n",
      "Iteration 414, loss = 17986708358.32508087\n",
      "Iteration 415, loss = 17986707090.68164825\n",
      "Iteration 416, loss = 17986705823.84841537\n",
      "Iteration 417, loss = 17986704557.81851578\n",
      "Iteration 418, loss = 17986703292.58518600\n",
      "Iteration 419, loss = 17986702028.14172745\n",
      "Iteration 420, loss = 17986700764.48154449\n",
      "Iteration 421, loss = 17986699501.59811783\n",
      "Iteration 422, loss = 17986698239.48501968\n",
      "Iteration 423, loss = 17986696978.13588715\n",
      "Iteration 424, loss = 17986695717.54444504\n",
      "Iteration 425, loss = 17986694457.70451736\n",
      "Iteration 426, loss = 17986693198.60996246\n",
      "Iteration 427, loss = 17986691940.25475693\n",
      "Iteration 428, loss = 17986690682.63293457\n",
      "Iteration 429, loss = 17986689425.73859024\n",
      "Iteration 430, loss = 17986688169.56591034\n",
      "Iteration 431, loss = 17986686914.10914612\n",
      "Iteration 432, loss = 17986685659.36262894\n",
      "Iteration 433, loss = 17986684405.32073212\n",
      "Iteration 434, loss = 17986683151.97793198\n",
      "Iteration 435, loss = 17986681899.32874680\n",
      "Iteration 436, loss = 17986680647.36775589\n",
      "Iteration 437, loss = 17986679396.08963776\n",
      "Iteration 438, loss = 17986678145.48910141\n",
      "Iteration 439, loss = 17986676895.56091690\n",
      "Iteration 440, loss = 17986675646.29994965\n",
      "Iteration 441, loss = 17986674397.70108795\n",
      "Iteration 442, loss = 17986673149.75930786\n",
      "Iteration 443, loss = 17986671902.46962357\n",
      "Iteration 444, loss = 17986670655.82711792\n",
      "Iteration 445, loss = 17986669409.82693100\n",
      "Iteration 446, loss = 17986668164.46426010\n",
      "Iteration 447, loss = 17986666919.73434830\n",
      "Iteration 448, loss = 17986665675.63250351\n",
      "Iteration 449, loss = 17986664432.15407562\n",
      "Iteration 450, loss = 17986663189.29448318\n",
      "Iteration 451, loss = 17986661947.04918671\n",
      "Iteration 452, loss = 17986660705.41369247\n",
      "Iteration 453, loss = 17986659464.38357162\n",
      "Iteration 454, loss = 17986658223.95442581\n",
      "Iteration 455, loss = 17986656984.12192917\n",
      "Iteration 456, loss = 17986655744.88178253\n",
      "Iteration 457, loss = 17986654506.22974014\n",
      "Iteration 458, loss = 17986653268.16161728\n",
      "Iteration 459, loss = 17986652030.67325592\n",
      "Iteration 460, loss = 17986650793.76055145\n",
      "Iteration 461, loss = 17986649557.41944122\n",
      "Iteration 462, loss = 17986648321.64590836\n",
      "Iteration 463, loss = 17986647086.43598175\n",
      "Iteration 464, loss = 17986645851.78573990\n",
      "Iteration 465, loss = 17986644617.69126892\n",
      "Iteration 466, loss = 17986643384.14874649\n",
      "Iteration 467, loss = 17986642151.15435791\n",
      "Iteration 468, loss = 17986640918.70432663\n",
      "Iteration 469, loss = 17986639686.79493713\n",
      "Iteration 470, loss = 17986638455.42249298\n",
      "Iteration 471, loss = 17986637224.58336258\n",
      "Iteration 472, loss = 17986635994.27391815\n",
      "Iteration 473, loss = 17986634764.49058914\n",
      "Iteration 474, loss = 17986633535.22984695\n",
      "Iteration 475, loss = 17986632306.48818588\n",
      "Iteration 476, loss = 17986631078.26213074\n",
      "Iteration 477, loss = 17986629850.54827118\n",
      "Iteration 478, loss = 17986628623.34320831\n",
      "Iteration 479, loss = 17986627396.64357376\n",
      "Iteration 480, loss = 17986626170.44604874\n",
      "Iteration 481, loss = 17986624944.74733734\n",
      "Iteration 482, loss = 17986623719.54418182\n",
      "Iteration 483, loss = 17986622494.83335495\n",
      "Iteration 484, loss = 17986621270.61166763\n",
      "Iteration 485, loss = 17986620046.87594604\n",
      "Iteration 486, loss = 17986618823.62307358\n",
      "Iteration 487, loss = 17986617600.84993362\n",
      "Iteration 488, loss = 17986616378.55346298\n",
      "Iteration 489, loss = 17986615156.73062515\n",
      "Iteration 490, loss = 17986613935.37839890\n",
      "Iteration 491, loss = 17986612714.49382019\n",
      "Iteration 492, loss = 17986611494.07392120\n",
      "Iteration 493, loss = 17986610274.11578751\n",
      "Iteration 494, loss = 17986609054.61650848\n",
      "Iteration 495, loss = 17986607835.57323074\n",
      "Iteration 496, loss = 17986606616.98310089\n",
      "Iteration 497, loss = 17986605398.84331894\n",
      "Iteration 498, loss = 17986604181.15108490\n",
      "Iteration 499, loss = 17986602963.90364075\n",
      "Iteration 500, loss = 17986601747.09825897\n",
      "Iteration 1, loss = 19463694624.68973923\n",
      "Iteration 2, loss = 19463687838.13995361\n",
      "Iteration 3, loss = 19463680960.81228256\n",
      "Iteration 4, loss = 19463674191.94551086\n",
      "Iteration 5, loss = 19463667348.82381058\n",
      "Iteration 6, loss = 19463660528.98139954\n",
      "Iteration 7, loss = 19463653828.04925537\n",
      "Iteration 8, loss = 19463646982.83080292\n",
      "Iteration 9, loss = 19463640248.77721024\n",
      "Iteration 10, loss = 19463633491.24094772\n",
      "Iteration 11, loss = 19463626768.42050171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 19463620038.70831299\n",
      "Iteration 13, loss = 19463613298.84341049\n",
      "Iteration 14, loss = 19463606637.50078964\n",
      "Iteration 15, loss = 19463599774.48113251\n",
      "Iteration 16, loss = 19463593104.27946472\n",
      "Iteration 17, loss = 19463586348.02093124\n",
      "Iteration 18, loss = 19463579544.06342316\n",
      "Iteration 19, loss = 19463572737.78018951\n",
      "Iteration 20, loss = 19463565916.80078506\n",
      "Iteration 21, loss = 19463559063.46316910\n",
      "Iteration 22, loss = 19463552267.59698105\n",
      "Iteration 23, loss = 19463545361.95047760\n",
      "Iteration 24, loss = 19463538476.08586121\n",
      "Iteration 25, loss = 19463531626.18757248\n",
      "Iteration 26, loss = 19463524578.22591782\n",
      "Iteration 27, loss = 19463517720.50315857\n",
      "Iteration 28, loss = 19463510647.40191650\n",
      "Iteration 29, loss = 19463503626.29137421\n",
      "Iteration 30, loss = 19463496590.96866608\n",
      "Iteration 31, loss = 19463489413.01557159\n",
      "Iteration 32, loss = 19463482262.06526566\n",
      "Iteration 33, loss = 19463475103.11403275\n",
      "Iteration 34, loss = 19463467925.87881470\n",
      "Iteration 35, loss = 19463460676.55905151\n",
      "Iteration 36, loss = 19463453427.65811920\n",
      "Iteration 37, loss = 19463446119.85246277\n",
      "Iteration 38, loss = 19463438810.82628250\n",
      "Iteration 39, loss = 19463431484.96815491\n",
      "Iteration 40, loss = 19463424092.66919327\n",
      "Iteration 41, loss = 19463416679.51963043\n",
      "Iteration 42, loss = 19463409377.18851471\n",
      "Iteration 43, loss = 19463402013.93167496\n",
      "Iteration 44, loss = 19463394681.26829147\n",
      "Iteration 45, loss = 19463387376.24312592\n",
      "Iteration 46, loss = 19463380168.77810287\n",
      "Iteration 47, loss = 19463372982.54868317\n",
      "Iteration 48, loss = 19463365890.35930252\n",
      "Iteration 49, loss = 19463358873.73182678\n",
      "Iteration 50, loss = 19463351835.30331039\n",
      "Iteration 51, loss = 19463345044.73406601\n",
      "Iteration 52, loss = 19463338245.82466125\n",
      "Iteration 53, loss = 19463331562.05783081\n",
      "Iteration 54, loss = 19463324920.06510925\n",
      "Iteration 55, loss = 19463318450.05584335\n",
      "Iteration 56, loss = 19463311996.83294678\n",
      "Iteration 57, loss = 19463305699.18323517\n",
      "Iteration 58, loss = 19463299430.46544647\n",
      "Iteration 59, loss = 19463293260.90369797\n",
      "Iteration 60, loss = 19463287165.05489731\n",
      "Iteration 61, loss = 19463281235.85512543\n",
      "Iteration 62, loss = 19463275338.98075867\n",
      "Iteration 63, loss = 19463269474.46165466\n",
      "Iteration 64, loss = 19463263750.32999802\n",
      "Iteration 65, loss = 19463258094.72173691\n",
      "Iteration 66, loss = 19463252469.59546280\n",
      "Iteration 67, loss = 19463246991.94330215\n",
      "Iteration 68, loss = 19463241478.83762360\n",
      "Iteration 69, loss = 19463236101.25040817\n",
      "Iteration 70, loss = 19463230797.59621811\n",
      "Iteration 71, loss = 19463225480.14896393\n",
      "Iteration 72, loss = 19463220344.09554672\n",
      "Iteration 73, loss = 19463215223.36745834\n",
      "Iteration 74, loss = 19463210156.39356232\n",
      "Iteration 75, loss = 19463205179.33112717\n",
      "Iteration 76, loss = 19463200284.91183853\n",
      "Iteration 77, loss = 19463195413.04650879\n",
      "Iteration 78, loss = 19463190678.30042648\n",
      "Iteration 79, loss = 19463185902.19105148\n",
      "Iteration 80, loss = 19463181239.90620422\n",
      "Iteration 81, loss = 19463176645.46540070\n",
      "Iteration 82, loss = 19463172114.53729248\n",
      "Iteration 83, loss = 19463167656.73741531\n",
      "Iteration 84, loss = 19463163228.03006363\n",
      "Iteration 85, loss = 19463158952.40844345\n",
      "Iteration 86, loss = 19463154667.90825272\n",
      "Iteration 87, loss = 19463150514.39983749\n",
      "Iteration 88, loss = 19463146441.84695053\n",
      "Iteration 89, loss = 19463142433.26142120\n",
      "Iteration 90, loss = 19463138472.77756119\n",
      "Iteration 91, loss = 19463134626.58170319\n",
      "Iteration 92, loss = 19463130814.73865128\n",
      "Iteration 93, loss = 19463127073.32643127\n",
      "Iteration 94, loss = 19463123420.36392593\n",
      "Iteration 95, loss = 19463119822.51359177\n",
      "Iteration 96, loss = 19463116283.27251053\n",
      "Iteration 97, loss = 19463112804.66527557\n",
      "Iteration 98, loss = 19463109389.66696548\n",
      "Iteration 99, loss = 19463105999.84537125\n",
      "Iteration 100, loss = 19463102676.39823914\n",
      "Iteration 101, loss = 19463099374.50118637\n",
      "Iteration 102, loss = 19463096129.22985840\n",
      "Iteration 103, loss = 19463092932.78501892\n",
      "Iteration 104, loss = 19463089750.04486084\n",
      "Iteration 105, loss = 19463086615.32833481\n",
      "Iteration 106, loss = 19463083504.26823044\n",
      "Iteration 107, loss = 19463080471.61460114\n",
      "Iteration 108, loss = 19463077473.61150742\n",
      "Iteration 109, loss = 19463074475.43465042\n",
      "Iteration 110, loss = 19463071544.52820206\n",
      "Iteration 111, loss = 19463068633.78032684\n",
      "Iteration 112, loss = 19463065737.92900848\n",
      "Iteration 113, loss = 19463062906.74117661\n",
      "Iteration 114, loss = 19463060048.90838242\n",
      "Iteration 115, loss = 19463057230.97819138\n",
      "Iteration 116, loss = 19463054434.39754105\n",
      "Iteration 117, loss = 19463051658.93539047\n",
      "Iteration 118, loss = 19463048888.60273743\n",
      "Iteration 119, loss = 19463046144.18169022\n",
      "Iteration 120, loss = 19463043396.49069214\n",
      "Iteration 121, loss = 19463040670.01134491\n",
      "Iteration 122, loss = 19463037940.49582672\n",
      "Iteration 123, loss = 19463035214.18542099\n",
      "Iteration 124, loss = 19463032517.41993332\n",
      "Iteration 125, loss = 19463029782.95032120\n",
      "Iteration 126, loss = 19463027080.94565964\n",
      "Iteration 127, loss = 19463024364.51721191\n",
      "Iteration 128, loss = 19463021704.70090485\n",
      "Iteration 129, loss = 19463019047.53393173\n",
      "Iteration 130, loss = 19463016381.95224762\n",
      "Iteration 131, loss = 19463013778.54959869\n",
      "Iteration 132, loss = 19463011132.19737625\n",
      "Iteration 133, loss = 19463008522.93278503\n",
      "Iteration 134, loss = 19463005903.02969742\n",
      "Iteration 135, loss = 19463003281.89128876\n",
      "Iteration 136, loss = 19463000660.62128067\n",
      "Iteration 137, loss = 19462998038.54584885\n",
      "Iteration 138, loss = 19462995416.97240448\n",
      "Iteration 139, loss = 19462992794.79212952\n",
      "Iteration 140, loss = 19462990176.94358063\n",
      "Iteration 141, loss = 19462987556.87062836\n",
      "Iteration 142, loss = 19462984933.86219406\n",
      "Iteration 143, loss = 19462982321.90326691\n",
      "Iteration 144, loss = 19462979707.90509796\n",
      "Iteration 145, loss = 19462977068.15682983\n",
      "Iteration 146, loss = 19462974469.88784790\n",
      "Iteration 147, loss = 19462971844.23080444\n",
      "Iteration 148, loss = 19462969201.32216263\n",
      "Iteration 149, loss = 19462966567.64253616\n",
      "Iteration 150, loss = 19462963908.78552628\n",
      "Iteration 151, loss = 19462961290.15290070\n",
      "Iteration 152, loss = 19462958619.75105667\n",
      "Iteration 153, loss = 19462955972.15527725\n",
      "Iteration 154, loss = 19462953337.82682037\n",
      "Iteration 155, loss = 19462950669.01180267\n",
      "Iteration 156, loss = 19462947992.89223480\n",
      "Iteration 157, loss = 19462945352.56024170\n",
      "Iteration 158, loss = 19462942671.35781097\n",
      "Iteration 159, loss = 19462940000.42524338\n",
      "Iteration 160, loss = 19462937307.21259308\n",
      "Iteration 161, loss = 19462934656.05568314\n",
      "Iteration 162, loss = 19462931953.86296844\n",
      "Iteration 163, loss = 19462929252.37905121\n",
      "Iteration 164, loss = 19462926555.89895630\n",
      "Iteration 165, loss = 19462923828.89249802\n",
      "Iteration 166, loss = 19462921106.54615021\n",
      "Iteration 167, loss = 19462918392.19219589\n",
      "Iteration 168, loss = 19462915660.27806091\n",
      "Iteration 169, loss = 19462912928.54899597\n",
      "Iteration 170, loss = 19462910204.07761002\n",
      "Iteration 171, loss = 19462907498.67158508\n",
      "Iteration 172, loss = 19462904757.86899948\n",
      "Iteration 173, loss = 19462902027.87035370\n",
      "Iteration 174, loss = 19462899326.89376450\n",
      "Iteration 175, loss = 19462896587.76794434\n",
      "Iteration 176, loss = 19462893850.51531982\n",
      "Iteration 177, loss = 19462891132.13271332\n",
      "Iteration 178, loss = 19462888366.90031815\n",
      "Iteration 179, loss = 19462885625.76986694\n",
      "Iteration 180, loss = 19462882873.34510422\n",
      "Iteration 181, loss = 19462880136.75146103\n",
      "Iteration 182, loss = 19462877381.13530350\n",
      "Iteration 183, loss = 19462874661.40313339\n",
      "Iteration 184, loss = 19462871933.37280655\n",
      "Iteration 185, loss = 19462869235.27336502\n",
      "Iteration 186, loss = 19462866536.51501846\n",
      "Iteration 187, loss = 19462863856.74663162\n",
      "Iteration 188, loss = 19462861211.45898819\n",
      "Iteration 189, loss = 19462858509.55532074\n",
      "Iteration 190, loss = 19462855874.56087494\n",
      "Iteration 191, loss = 19462853215.16474533\n",
      "Iteration 192, loss = 19462850566.76636124\n",
      "Iteration 193, loss = 19462847920.23144913\n",
      "Iteration 194, loss = 19462845307.06284714\n",
      "Iteration 195, loss = 19462842699.84782791\n",
      "Iteration 196, loss = 19462840100.86209869\n",
      "Iteration 197, loss = 19462837506.95586014\n",
      "Iteration 198, loss = 19462834925.45117188\n",
      "Iteration 199, loss = 19462832352.35666275\n",
      "Iteration 200, loss = 19462829763.23898697\n",
      "Iteration 201, loss = 19462827211.95324707\n",
      "Iteration 202, loss = 19462824619.36414719\n",
      "Iteration 203, loss = 19462822071.20248795\n",
      "Iteration 204, loss = 19462819504.31890869\n",
      "Iteration 205, loss = 19462816951.30068207\n",
      "Iteration 206, loss = 19462814412.47271347\n",
      "Iteration 207, loss = 19462811856.87368774\n",
      "Iteration 208, loss = 19462809332.51349258\n",
      "Iteration 209, loss = 19462806800.49632263\n",
      "Iteration 210, loss = 19462804263.77503586\n",
      "Iteration 211, loss = 19462801778.26659012\n",
      "Iteration 212, loss = 19462799239.78948593\n",
      "Iteration 213, loss = 19462796758.26861572\n",
      "Iteration 214, loss = 19462794242.04935074\n",
      "Iteration 215, loss = 19462791690.85131073\n",
      "Iteration 216, loss = 19462789193.08896637\n",
      "Iteration 217, loss = 19462786680.77996063\n",
      "Iteration 218, loss = 19462784133.79716110\n",
      "Iteration 219, loss = 19462781636.89765549\n",
      "Iteration 220, loss = 19462779115.07122040\n",
      "Iteration 221, loss = 19462776634.22605896\n",
      "Iteration 222, loss = 19462774139.06268311\n",
      "Iteration 223, loss = 19462771665.27922821\n",
      "Iteration 224, loss = 19462769201.22142410\n",
      "Iteration 225, loss = 19462766740.14179993\n",
      "Iteration 226, loss = 19462764314.55498123\n",
      "Iteration 227, loss = 19462761883.09848022\n",
      "Iteration 228, loss = 19462759451.41464615\n",
      "Iteration 229, loss = 19462757021.69399261\n",
      "Iteration 230, loss = 19462754597.26772308\n",
      "Iteration 231, loss = 19462752188.79836655\n",
      "Iteration 232, loss = 19462749756.17634201\n",
      "Iteration 233, loss = 19462747320.92696381\n",
      "Iteration 234, loss = 19462744912.21833038\n",
      "Iteration 235, loss = 19462742476.14660645\n",
      "Iteration 236, loss = 19462740052.66875839\n",
      "Iteration 237, loss = 19462737626.21528625\n",
      "Iteration 238, loss = 19462735195.63399887\n",
      "Iteration 239, loss = 19462732752.05205917\n",
      "Iteration 240, loss = 19462730341.74697495\n",
      "Iteration 241, loss = 19462727904.04355240\n",
      "Iteration 242, loss = 19462725489.33456421\n",
      "Iteration 243, loss = 19462723095.10634995\n",
      "Iteration 244, loss = 19462720675.15006638\n",
      "Iteration 245, loss = 19462718285.53012085\n",
      "Iteration 246, loss = 19462715900.16865540\n",
      "Iteration 247, loss = 19462713491.40130997\n",
      "Iteration 248, loss = 19462711089.65392303\n",
      "Iteration 249, loss = 19462708708.34482956\n",
      "Iteration 250, loss = 19462706317.98546219\n",
      "Iteration 251, loss = 19462703933.79929733\n",
      "Iteration 252, loss = 19462701543.77957916\n",
      "Iteration 253, loss = 19462699164.07048798\n",
      "Iteration 254, loss = 19462696795.99580383\n",
      "Iteration 255, loss = 19462694408.58940506\n",
      "Iteration 256, loss = 19462692037.31550980\n",
      "Iteration 257, loss = 19462689661.42838669\n",
      "Iteration 258, loss = 19462687293.86218262\n",
      "Iteration 259, loss = 19462684920.08625793\n",
      "Iteration 260, loss = 19462682542.11284637\n",
      "Iteration 261, loss = 19462680167.56017303\n",
      "Iteration 262, loss = 19462677800.48891068\n",
      "Iteration 263, loss = 19462675426.95596313\n",
      "Iteration 264, loss = 19462673045.97438049\n",
      "Iteration 265, loss = 19462670703.15850449\n",
      "Iteration 266, loss = 19462668334.49663544\n",
      "Iteration 267, loss = 19462665963.84683990\n",
      "Iteration 268, loss = 19462663592.69495773\n",
      "Iteration 269, loss = 19462661216.10809708\n",
      "Iteration 270, loss = 19462658861.53126144\n",
      "Iteration 271, loss = 19462656480.06680679\n",
      "Iteration 272, loss = 19462654084.59181213\n",
      "Iteration 273, loss = 19462651730.84561539\n",
      "Iteration 274, loss = 19462649346.07141876\n",
      "Iteration 275, loss = 19462647007.49687195\n",
      "Iteration 276, loss = 19462644621.88726807\n",
      "Iteration 277, loss = 19462642284.06311035\n",
      "Iteration 278, loss = 19462639932.37897491\n",
      "Iteration 279, loss = 19462637603.11506271\n",
      "Iteration 280, loss = 19462635254.92098236\n",
      "Iteration 281, loss = 19462632960.63045883\n",
      "Iteration 282, loss = 19462630628.73467255\n",
      "Iteration 283, loss = 19462628330.72107697\n",
      "Iteration 284, loss = 19462626033.67601013\n",
      "Iteration 285, loss = 19462623742.81676102\n",
      "Iteration 286, loss = 19462621446.28142929\n",
      "Iteration 287, loss = 19462619161.81396866\n",
      "Iteration 288, loss = 19462616861.11564636\n",
      "Iteration 289, loss = 19462614546.78911209\n",
      "Iteration 290, loss = 19462612252.40142059\n",
      "Iteration 291, loss = 19462609927.35147476\n",
      "Iteration 292, loss = 19462607597.11517715\n",
      "Iteration 293, loss = 19462605281.56904602\n",
      "Iteration 294, loss = 19462602965.13924789\n",
      "Iteration 295, loss = 19462600649.69787598\n",
      "Iteration 296, loss = 19462598326.27188492\n",
      "Iteration 297, loss = 19462596008.53697586\n",
      "Iteration 298, loss = 19462593676.25135040\n",
      "Iteration 299, loss = 19462591335.08948517\n",
      "Iteration 300, loss = 19462588960.89504242\n",
      "Iteration 301, loss = 19462586640.51672363\n",
      "Iteration 302, loss = 19462584268.04724884\n",
      "Iteration 303, loss = 19462581910.92201996\n",
      "Iteration 304, loss = 19462579577.12228775\n",
      "Iteration 305, loss = 19462577224.81889725\n",
      "Iteration 306, loss = 19462574885.01546097\n",
      "Iteration 307, loss = 19462572550.39326477\n",
      "Iteration 308, loss = 19462570186.62436295\n",
      "Iteration 309, loss = 19462567841.07606125\n",
      "Iteration 310, loss = 19462565502.97668076\n",
      "Iteration 311, loss = 19462563154.15933228\n",
      "Iteration 312, loss = 19462560807.66823196\n",
      "Iteration 313, loss = 19462558468.64935303\n",
      "Iteration 314, loss = 19462556175.13307953\n",
      "Iteration 315, loss = 19462553844.66827011\n",
      "Iteration 316, loss = 19462551551.67696762\n",
      "Iteration 317, loss = 19462549255.87490845\n",
      "Iteration 318, loss = 19462546927.24409485\n",
      "Iteration 319, loss = 19462544637.70759583\n",
      "Iteration 320, loss = 19462542345.66467285\n",
      "Iteration 321, loss = 19462540007.97800827\n",
      "Iteration 322, loss = 19462537712.93759918\n",
      "Iteration 323, loss = 19462535413.78181839\n",
      "Iteration 324, loss = 19462533099.16850662\n",
      "Iteration 325, loss = 19462530849.05892944\n",
      "Iteration 326, loss = 19462528522.88522339\n",
      "Iteration 327, loss = 19462526226.85769272\n",
      "Iteration 328, loss = 19462523940.87290573\n",
      "Iteration 329, loss = 19462521659.37439728\n",
      "Iteration 330, loss = 19462519359.17252731\n",
      "Iteration 331, loss = 19462517079.36639023\n",
      "Iteration 332, loss = 19462514781.87304688\n",
      "Iteration 333, loss = 19462512490.87423706\n",
      "Iteration 334, loss = 19462510204.38706589\n",
      "Iteration 335, loss = 19462507917.95103073\n",
      "Iteration 336, loss = 19462505633.84127808\n",
      "Iteration 337, loss = 19462503335.86991501\n",
      "Iteration 338, loss = 19462501060.15198517\n",
      "Iteration 339, loss = 19462498779.04203415\n",
      "Iteration 340, loss = 19462496506.58925247\n",
      "Iteration 341, loss = 19462494206.43527985\n",
      "Iteration 342, loss = 19462491914.03215790\n",
      "Iteration 343, loss = 19462489641.00216293\n",
      "Iteration 344, loss = 19462487335.08685303\n",
      "Iteration 345, loss = 19462485042.48430634\n",
      "Iteration 346, loss = 19462482758.20101929\n",
      "Iteration 347, loss = 19462480453.37870407\n",
      "Iteration 348, loss = 19462478182.47353363\n",
      "Iteration 349, loss = 19462475903.40374374\n",
      "Iteration 350, loss = 19462473630.25885010\n",
      "Iteration 351, loss = 19462471338.11922836\n",
      "Iteration 352, loss = 19462469067.32770538\n",
      "Iteration 353, loss = 19462466795.84548950\n",
      "Iteration 354, loss = 19462464514.87011337\n",
      "Iteration 355, loss = 19462462234.45870209\n",
      "Iteration 356, loss = 19462459975.25159454\n",
      "Iteration 357, loss = 19462457720.50047684\n",
      "Iteration 358, loss = 19462455474.16662216\n",
      "Iteration 359, loss = 19462453247.83871460\n",
      "Iteration 360, loss = 19462451014.19000244\n",
      "Iteration 361, loss = 19462448762.16513443\n",
      "Iteration 362, loss = 19462446503.26818848\n",
      "Iteration 363, loss = 19462444247.94976807\n",
      "Iteration 364, loss = 19462441974.68985367\n",
      "Iteration 365, loss = 19462439678.81251526\n",
      "Iteration 366, loss = 19462437407.18177032\n",
      "Iteration 367, loss = 19462435114.54016113\n",
      "Iteration 368, loss = 19462432841.26347351\n",
      "Iteration 369, loss = 19462430578.64313126\n",
      "Iteration 370, loss = 19462428304.59239960\n",
      "Iteration 371, loss = 19462426038.97377396\n",
      "Iteration 372, loss = 19462423776.83419037\n",
      "Iteration 373, loss = 19462421513.05244064\n",
      "Iteration 374, loss = 19462419257.94039154\n",
      "Iteration 375, loss = 19462417000.49741364\n",
      "Iteration 376, loss = 19462414771.09910202\n",
      "Iteration 377, loss = 19462412528.82701874\n",
      "Iteration 378, loss = 19462410276.50064087\n",
      "Iteration 379, loss = 19462408038.64165878\n",
      "Iteration 380, loss = 19462405804.78534317\n",
      "Iteration 381, loss = 19462403563.58699417\n",
      "Iteration 382, loss = 19462401310.30057144\n",
      "Iteration 383, loss = 19462399062.78761292\n",
      "Iteration 384, loss = 19462396842.98389816\n",
      "Iteration 385, loss = 19462394593.51609039\n",
      "Iteration 386, loss = 19462392362.40057373\n",
      "Iteration 387, loss = 19462390129.73598862\n",
      "Iteration 388, loss = 19462387893.85095596\n",
      "Iteration 389, loss = 19462385648.10954285\n",
      "Iteration 390, loss = 19462383408.13930511\n",
      "Iteration 391, loss = 19462381169.77915955\n",
      "Iteration 392, loss = 19462378928.78166199\n",
      "Iteration 393, loss = 19462376720.69712448\n",
      "Iteration 394, loss = 19462374489.68696213\n",
      "Iteration 395, loss = 19462372247.21026230\n",
      "Iteration 396, loss = 19462370024.28892517\n",
      "Iteration 397, loss = 19462367790.44422531\n",
      "Iteration 398, loss = 19462365535.29138565\n",
      "Iteration 399, loss = 19462363287.12974930\n",
      "Iteration 400, loss = 19462361012.64562988\n",
      "Iteration 401, loss = 19462358759.22114563\n",
      "Iteration 402, loss = 19462356487.28472137\n",
      "Iteration 403, loss = 19462354239.33602142\n",
      "Iteration 404, loss = 19462351976.67146301\n",
      "Iteration 405, loss = 19462349710.98165131\n",
      "Iteration 406, loss = 19462347450.74542236\n",
      "Iteration 407, loss = 19462345188.57752991\n",
      "Iteration 408, loss = 19462342923.51928329\n",
      "Iteration 409, loss = 19462340702.16414642\n",
      "Iteration 410, loss = 19462338444.56118011\n",
      "Iteration 411, loss = 19462336200.85195160\n",
      "Iteration 412, loss = 19462333974.78538132\n",
      "Iteration 413, loss = 19462331742.03675079\n",
      "Iteration 414, loss = 19462329509.33937073\n",
      "Iteration 415, loss = 19462327249.44320297\n",
      "Iteration 416, loss = 19462325027.58985901\n",
      "Iteration 417, loss = 19462322782.56363297\n",
      "Iteration 418, loss = 19462320545.40487289\n",
      "Iteration 419, loss = 19462318319.79084015\n",
      "Iteration 420, loss = 19462316115.86959457\n",
      "Iteration 421, loss = 19462313892.99599838\n",
      "Iteration 422, loss = 19462311685.39155960\n",
      "Iteration 423, loss = 19462309457.75111389\n",
      "Iteration 424, loss = 19462307222.94416809\n",
      "Iteration 425, loss = 19462304999.35591888\n",
      "Iteration 426, loss = 19462302759.28753662\n",
      "Iteration 427, loss = 19462300512.64215851\n",
      "Iteration 428, loss = 19462298268.51088715\n",
      "Iteration 429, loss = 19462296017.19664383\n",
      "Iteration 430, loss = 19462293781.48320389\n",
      "Iteration 431, loss = 19462291532.81989288\n",
      "Iteration 432, loss = 19462289288.64715576\n",
      "Iteration 433, loss = 19462287024.08910751\n",
      "Iteration 434, loss = 19462284804.67235184\n",
      "Iteration 435, loss = 19462282561.34867859\n",
      "Iteration 436, loss = 19462280320.07210922\n",
      "Iteration 437, loss = 19462278076.86100388\n",
      "Iteration 438, loss = 19462275832.63249969\n",
      "Iteration 439, loss = 19462273597.16614914\n",
      "Iteration 440, loss = 19462271333.67731857\n",
      "Iteration 441, loss = 19462269101.12427521\n",
      "Iteration 442, loss = 19462266844.19627380\n",
      "Iteration 443, loss = 19462264585.82860184\n",
      "Iteration 444, loss = 19462262366.07614136\n",
      "Iteration 445, loss = 19462260097.65404129\n",
      "Iteration 446, loss = 19462257850.43111420\n",
      "Iteration 447, loss = 19462255614.61383438\n",
      "Iteration 448, loss = 19462253353.67774200\n",
      "Iteration 449, loss = 19462251134.40671158\n",
      "Iteration 450, loss = 19462248861.97418976\n",
      "Iteration 451, loss = 19462246643.48004150\n",
      "Iteration 452, loss = 19462244419.69630051\n",
      "Iteration 453, loss = 19462242167.35195160\n",
      "Iteration 454, loss = 19462239960.96207428\n",
      "Iteration 455, loss = 19462237735.57562637\n",
      "Iteration 456, loss = 19462235516.65198898\n",
      "Iteration 457, loss = 19462233318.65628052\n",
      "Iteration 458, loss = 19462231114.47254562\n",
      "Iteration 459, loss = 19462228901.18631744\n",
      "Iteration 460, loss = 19462226713.97808456\n",
      "Iteration 461, loss = 19462224498.52673721\n",
      "Iteration 462, loss = 19462222314.39785004\n",
      "Iteration 463, loss = 19462220103.12987137\n",
      "Iteration 464, loss = 19462217915.81264496\n",
      "Iteration 465, loss = 19462215711.93063354\n",
      "Iteration 466, loss = 19462213518.78672028\n",
      "Iteration 467, loss = 19462211318.35800934\n",
      "Iteration 468, loss = 19462209108.80801773\n",
      "Iteration 469, loss = 19462206944.39062881\n",
      "Iteration 470, loss = 19462204724.41519165\n",
      "Iteration 471, loss = 19462202530.46651840\n",
      "Iteration 472, loss = 19462200355.51733780\n",
      "Iteration 473, loss = 19462198117.59172821\n",
      "Iteration 474, loss = 19462195943.30420303\n",
      "Iteration 475, loss = 19462193714.61890411\n",
      "Iteration 476, loss = 19462191469.83525467\n",
      "Iteration 477, loss = 19462189251.28033066\n",
      "Iteration 478, loss = 19462186988.70765686\n",
      "Iteration 479, loss = 19462184762.29973602\n",
      "Iteration 480, loss = 19462182529.11545181\n",
      "Iteration 481, loss = 19462180263.26850510\n",
      "Iteration 482, loss = 19462178021.38431549\n",
      "Iteration 483, loss = 19462175774.22349167\n",
      "Iteration 484, loss = 19462173540.70174026\n",
      "Iteration 485, loss = 19462171312.36722565\n",
      "Iteration 486, loss = 19462169048.27778625\n",
      "Iteration 487, loss = 19462166818.30736923\n",
      "Iteration 488, loss = 19462164598.00741959\n",
      "Iteration 489, loss = 19462162354.84413528\n",
      "Iteration 490, loss = 19462160151.52487946\n",
      "Iteration 491, loss = 19462157926.56594849\n",
      "Iteration 492, loss = 19462155744.39466476\n",
      "Iteration 493, loss = 19462153535.91840744\n",
      "Iteration 494, loss = 19462151328.14997482\n",
      "Iteration 495, loss = 19462149152.88946915\n",
      "Iteration 496, loss = 19462146924.43300629\n",
      "Iteration 497, loss = 19462144723.95901871\n",
      "Iteration 498, loss = 19462142535.67810822\n",
      "Iteration 499, loss = 19462140318.12955093\n",
      "Iteration 500, loss = 19462138113.78345871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 20102316882.88219833\n",
      "Iteration 2, loss = 20102309627.83189011\n",
      "Iteration 3, loss = 20102302325.13599777\n",
      "Iteration 4, loss = 20102295023.16283798\n",
      "Iteration 5, loss = 20102287789.41988754\n",
      "Iteration 6, loss = 20102280316.65635300\n",
      "Iteration 7, loss = 20102273064.80903625\n",
      "Iteration 8, loss = 20102265590.69439316\n",
      "Iteration 9, loss = 20102258131.12022781\n",
      "Iteration 10, loss = 20102250631.67361069\n",
      "Iteration 11, loss = 20102243097.78353500\n",
      "Iteration 12, loss = 20102235701.67868423\n",
      "Iteration 13, loss = 20102227962.13633347\n",
      "Iteration 14, loss = 20102220355.47976685\n",
      "Iteration 15, loss = 20102212754.64527512\n",
      "Iteration 16, loss = 20102204974.48881149\n",
      "Iteration 17, loss = 20102197327.31322098\n",
      "Iteration 18, loss = 20102189436.27462006\n",
      "Iteration 19, loss = 20102181689.72835541\n",
      "Iteration 20, loss = 20102173718.04703903\n",
      "Iteration 21, loss = 20102165850.41138840\n",
      "Iteration 22, loss = 20102157910.35950089\n",
      "Iteration 23, loss = 20102149958.81850052\n",
      "Iteration 24, loss = 20102141955.21749878\n",
      "Iteration 25, loss = 20102133788.69872284\n",
      "Iteration 26, loss = 20102125781.66598511\n",
      "Iteration 27, loss = 20102117582.95962524\n",
      "Iteration 28, loss = 20102109320.20233917\n",
      "Iteration 29, loss = 20102101049.46392441\n",
      "Iteration 30, loss = 20102092780.59785843\n",
      "Iteration 31, loss = 20102084342.88285828\n",
      "Iteration 32, loss = 20102075959.42496490\n",
      "Iteration 33, loss = 20102067422.69344330\n",
      "Iteration 34, loss = 20102058840.43951035\n",
      "Iteration 35, loss = 20102050151.91067505\n",
      "Iteration 36, loss = 20102041388.28734970\n",
      "Iteration 37, loss = 20102032585.72814178\n",
      "Iteration 38, loss = 20102023639.87249374\n",
      "Iteration 39, loss = 20102014779.71037292\n",
      "Iteration 40, loss = 20102005684.48295212\n",
      "Iteration 41, loss = 20101996741.57040787\n",
      "Iteration 42, loss = 20101987569.21205902\n",
      "Iteration 43, loss = 20101978591.52661896\n",
      "Iteration 44, loss = 20101969553.14574814\n",
      "Iteration 45, loss = 20101960624.70002365\n",
      "Iteration 46, loss = 20101951807.28228760\n",
      "Iteration 47, loss = 20101943133.61718750\n",
      "Iteration 48, loss = 20101934479.70824814\n",
      "Iteration 49, loss = 20101926004.31333160\n",
      "Iteration 50, loss = 20101917538.59810257\n",
      "Iteration 51, loss = 20101909362.60748291\n",
      "Iteration 52, loss = 20101901084.87490082\n",
      "Iteration 53, loss = 20101892963.01683426\n",
      "Iteration 54, loss = 20101884955.70241928\n",
      "Iteration 55, loss = 20101877025.08939362\n",
      "Iteration 56, loss = 20101869238.75326920\n",
      "Iteration 57, loss = 20101861483.55057907\n",
      "Iteration 58, loss = 20101853824.00449371\n",
      "Iteration 59, loss = 20101846334.55637741\n",
      "Iteration 60, loss = 20101838929.98836517\n",
      "Iteration 61, loss = 20101831703.23820496\n",
      "Iteration 62, loss = 20101824553.43405533\n",
      "Iteration 63, loss = 20101817567.41947174\n",
      "Iteration 64, loss = 20101810691.56333160\n",
      "Iteration 65, loss = 20101803944.82993698\n",
      "Iteration 66, loss = 20101797309.73458099\n",
      "Iteration 67, loss = 20101790744.92668152\n",
      "Iteration 68, loss = 20101784285.16750336\n",
      "Iteration 69, loss = 20101777794.07766342\n",
      "Iteration 70, loss = 20101771427.71931839\n",
      "Iteration 71, loss = 20101765081.33737946\n",
      "Iteration 72, loss = 20101758838.10939026\n",
      "Iteration 73, loss = 20101752533.60680389\n",
      "Iteration 74, loss = 20101746169.62923431\n",
      "Iteration 75, loss = 20101740023.54137802\n",
      "Iteration 76, loss = 20101733639.75551224\n",
      "Iteration 77, loss = 20101727346.63919830\n",
      "Iteration 78, loss = 20101721046.77468872\n",
      "Iteration 79, loss = 20101714948.57675934\n",
      "Iteration 80, loss = 20101708676.32638550\n",
      "Iteration 81, loss = 20101702672.86424255\n",
      "Iteration 82, loss = 20101696694.97354126\n",
      "Iteration 83, loss = 20101690903.65308762\n",
      "Iteration 84, loss = 20101685246.93029022\n",
      "Iteration 85, loss = 20101679792.45859528\n",
      "Iteration 86, loss = 20101674514.30516052\n",
      "Iteration 87, loss = 20101669370.92084122\n",
      "Iteration 88, loss = 20101664385.87824249\n",
      "Iteration 89, loss = 20101659536.79104996\n",
      "Iteration 90, loss = 20101654852.10871506\n",
      "Iteration 91, loss = 20101650291.00796127\n",
      "Iteration 92, loss = 20101645889.19741058\n",
      "Iteration 93, loss = 20101641558.10839844\n",
      "Iteration 94, loss = 20101637386.55700302\n",
      "Iteration 95, loss = 20101633295.35332870\n",
      "Iteration 96, loss = 20101629298.72804260\n",
      "Iteration 97, loss = 20101625348.77223587\n",
      "Iteration 98, loss = 20101621490.21255875\n",
      "Iteration 99, loss = 20101617676.84067154\n",
      "Iteration 100, loss = 20101613984.77447510\n",
      "Iteration 101, loss = 20101610297.11277390\n",
      "Iteration 102, loss = 20101606708.90739822\n",
      "Iteration 103, loss = 20101603097.00119019\n",
      "Iteration 104, loss = 20101599559.07062149\n",
      "Iteration 105, loss = 20101596062.53976440\n",
      "Iteration 106, loss = 20101592582.87233353\n",
      "Iteration 107, loss = 20101589139.07362747\n",
      "Iteration 108, loss = 20101585722.96287918\n",
      "Iteration 109, loss = 20101582341.82731628\n",
      "Iteration 110, loss = 20101578945.08789444\n",
      "Iteration 111, loss = 20101575609.54309845\n",
      "Iteration 112, loss = 20101572274.97561264\n",
      "Iteration 113, loss = 20101568966.62254333\n",
      "Iteration 114, loss = 20101565674.19416809\n",
      "Iteration 115, loss = 20101562434.30538940\n",
      "Iteration 116, loss = 20101559198.73569489\n",
      "Iteration 117, loss = 20101555973.12606812\n",
      "Iteration 118, loss = 20101552812.06332016\n",
      "Iteration 119, loss = 20101549627.06379318\n",
      "Iteration 120, loss = 20101546457.31361008\n",
      "Iteration 121, loss = 20101543373.37739944\n",
      "Iteration 122, loss = 20101540218.37487030\n",
      "Iteration 123, loss = 20101537060.73156738\n",
      "Iteration 124, loss = 20101534003.49028015\n",
      "Iteration 125, loss = 20101530897.98999405\n",
      "Iteration 126, loss = 20101527819.07305145\n",
      "Iteration 127, loss = 20101524755.00462723\n",
      "Iteration 128, loss = 20101521716.40444946\n",
      "Iteration 129, loss = 20101518672.87958145\n",
      "Iteration 130, loss = 20101515649.85927582\n",
      "Iteration 131, loss = 20101512620.96666718\n",
      "Iteration 132, loss = 20101509620.28807068\n",
      "Iteration 133, loss = 20101506587.06047821\n",
      "Iteration 134, loss = 20101503623.61933136\n",
      "Iteration 135, loss = 20101500619.69163132\n",
      "Iteration 136, loss = 20101497643.28246689\n",
      "Iteration 137, loss = 20101494698.02352524\n",
      "Iteration 138, loss = 20101491712.86619568\n",
      "Iteration 139, loss = 20101488758.17195129\n",
      "Iteration 140, loss = 20101485810.33205795\n",
      "Iteration 141, loss = 20101482883.43875504\n",
      "Iteration 142, loss = 20101479939.58292389\n",
      "Iteration 143, loss = 20101477000.28824615\n",
      "Iteration 144, loss = 20101474060.21314621\n",
      "Iteration 145, loss = 20101471140.64847946\n",
      "Iteration 146, loss = 20101468218.03514481\n",
      "Iteration 147, loss = 20101465297.58292389\n",
      "Iteration 148, loss = 20101462375.24202728\n",
      "Iteration 149, loss = 20101459486.19571304\n",
      "Iteration 150, loss = 20101456565.91179657\n",
      "Iteration 151, loss = 20101453670.62030792\n",
      "Iteration 152, loss = 20101450811.56501770\n",
      "Iteration 153, loss = 20101447904.51851654\n",
      "Iteration 154, loss = 20101445041.99642944\n",
      "Iteration 155, loss = 20101442178.60301208\n",
      "Iteration 156, loss = 20101439290.81374359\n",
      "Iteration 157, loss = 20101436458.11671448\n",
      "Iteration 158, loss = 20101433567.68453598\n",
      "Iteration 159, loss = 20101430737.40000153\n",
      "Iteration 160, loss = 20101427885.64957047\n",
      "Iteration 161, loss = 20101425057.08157349\n",
      "Iteration 162, loss = 20101422219.07791138\n",
      "Iteration 163, loss = 20101419412.93263245\n",
      "Iteration 164, loss = 20101416575.86507797\n",
      "Iteration 165, loss = 20101413794.56293106\n",
      "Iteration 166, loss = 20101410988.91781998\n",
      "Iteration 167, loss = 20101408182.73828125\n",
      "Iteration 168, loss = 20101405408.88438797\n",
      "Iteration 169, loss = 20101402638.61243439\n",
      "Iteration 170, loss = 20101399826.39755630\n",
      "Iteration 171, loss = 20101397089.03594971\n",
      "Iteration 172, loss = 20101394320.90535736\n",
      "Iteration 173, loss = 20101391534.97389984\n",
      "Iteration 174, loss = 20101388764.59837723\n",
      "Iteration 175, loss = 20101386022.34789658\n",
      "Iteration 176, loss = 20101383267.32947159\n",
      "Iteration 177, loss = 20101380519.17900848\n",
      "Iteration 178, loss = 20101377795.80312347\n",
      "Iteration 179, loss = 20101375032.53075027\n",
      "Iteration 180, loss = 20101372309.96211624\n",
      "Iteration 181, loss = 20101369598.36429596\n",
      "Iteration 182, loss = 20101366846.58592606\n",
      "Iteration 183, loss = 20101364124.18127823\n",
      "Iteration 184, loss = 20101361390.18103790\n",
      "Iteration 185, loss = 20101358677.83017731\n",
      "Iteration 186, loss = 20101355952.99069214\n",
      "Iteration 187, loss = 20101353232.17782974\n",
      "Iteration 188, loss = 20101350537.89943695\n",
      "Iteration 189, loss = 20101347825.24339294\n",
      "Iteration 190, loss = 20101345135.96464157\n",
      "Iteration 191, loss = 20101342438.70957565\n",
      "Iteration 192, loss = 20101339722.30794144\n",
      "Iteration 193, loss = 20101337012.29129028\n",
      "Iteration 194, loss = 20101334346.13517761\n",
      "Iteration 195, loss = 20101331653.50713348\n",
      "Iteration 196, loss = 20101328941.69764328\n",
      "Iteration 197, loss = 20101326275.48019409\n",
      "Iteration 198, loss = 20101323602.99569321\n",
      "Iteration 199, loss = 20101320921.07150650\n",
      "Iteration 200, loss = 20101318233.31336975\n",
      "Iteration 201, loss = 20101315566.61392975\n",
      "Iteration 202, loss = 20101312905.25531387\n",
      "Iteration 203, loss = 20101310243.20545197\n",
      "Iteration 204, loss = 20101307587.09440994\n",
      "Iteration 205, loss = 20101304903.50641251\n",
      "Iteration 206, loss = 20101302256.02127075\n",
      "Iteration 207, loss = 20101299599.27412796\n",
      "Iteration 208, loss = 20101296950.42956924\n",
      "Iteration 209, loss = 20101294310.26462555\n",
      "Iteration 210, loss = 20101291663.47113419\n",
      "Iteration 211, loss = 20101288999.38972473\n",
      "Iteration 212, loss = 20101286346.61793900\n",
      "Iteration 213, loss = 20101283723.92852020\n",
      "Iteration 214, loss = 20101281053.07175827\n",
      "Iteration 215, loss = 20101278441.08778381\n",
      "Iteration 216, loss = 20101275790.74084091\n",
      "Iteration 217, loss = 20101273158.01247787\n",
      "Iteration 218, loss = 20101270540.17054749\n",
      "Iteration 219, loss = 20101267879.62658310\n",
      "Iteration 220, loss = 20101265253.28791809\n",
      "Iteration 221, loss = 20101262635.53538513\n",
      "Iteration 222, loss = 20101260016.39104462\n",
      "Iteration 223, loss = 20101257371.33850861\n",
      "Iteration 224, loss = 20101254766.40336227\n",
      "Iteration 225, loss = 20101252130.96491241\n",
      "Iteration 226, loss = 20101249519.61925125\n",
      "Iteration 227, loss = 20101246904.12812424\n",
      "Iteration 228, loss = 20101244269.38164139\n",
      "Iteration 229, loss = 20101241656.79905319\n",
      "Iteration 230, loss = 20101239050.24521637\n",
      "Iteration 231, loss = 20101236440.56511307\n",
      "Iteration 232, loss = 20101233850.89380646\n",
      "Iteration 233, loss = 20101231228.24021912\n",
      "Iteration 234, loss = 20101228640.19054031\n",
      "Iteration 235, loss = 20101226011.86414719\n",
      "Iteration 236, loss = 20101223442.44197464\n",
      "Iteration 237, loss = 20101220794.04132843\n",
      "Iteration 238, loss = 20101218218.45351028\n",
      "Iteration 239, loss = 20101215598.42256546\n",
      "Iteration 240, loss = 20101213020.53608322\n",
      "Iteration 241, loss = 20101210414.22027206\n",
      "Iteration 242, loss = 20101207830.87414169\n",
      "Iteration 243, loss = 20101205199.92779160\n",
      "Iteration 244, loss = 20101202607.27270126\n",
      "Iteration 245, loss = 20101200032.39822388\n",
      "Iteration 246, loss = 20101197435.69660187\n",
      "Iteration 247, loss = 20101194862.44133759\n",
      "Iteration 248, loss = 20101192240.32791901\n",
      "Iteration 249, loss = 20101189671.29930115\n",
      "Iteration 250, loss = 20101187066.91529465\n",
      "Iteration 251, loss = 20101184494.19294357\n",
      "Iteration 252, loss = 20101181898.86921310\n",
      "Iteration 253, loss = 20101179320.74774170\n",
      "Iteration 254, loss = 20101176762.79420853\n",
      "Iteration 255, loss = 20101174154.84555817\n",
      "Iteration 256, loss = 20101171564.48323059\n",
      "Iteration 257, loss = 20101169003.80437469\n",
      "Iteration 258, loss = 20101166441.45685959\n",
      "Iteration 259, loss = 20101163858.82244492\n",
      "Iteration 260, loss = 20101161290.53274918\n",
      "Iteration 261, loss = 20101158725.13968277\n",
      "Iteration 262, loss = 20101156131.53390503\n",
      "Iteration 263, loss = 20101153565.20987320\n",
      "Iteration 264, loss = 20101151027.06292343\n",
      "Iteration 265, loss = 20101148443.57133102\n",
      "Iteration 266, loss = 20101145907.70140076\n",
      "Iteration 267, loss = 20101143334.31515121\n",
      "Iteration 268, loss = 20101140775.28803635\n",
      "Iteration 269, loss = 20101138197.26140594\n",
      "Iteration 270, loss = 20101135639.88093185\n",
      "Iteration 271, loss = 20101133090.38852692\n",
      "Iteration 272, loss = 20101130533.66747665\n",
      "Iteration 273, loss = 20101127966.10280228\n",
      "Iteration 274, loss = 20101125422.68717575\n",
      "Iteration 275, loss = 20101122871.98019409\n",
      "Iteration 276, loss = 20101120348.60014343\n",
      "Iteration 277, loss = 20101117768.97946548\n",
      "Iteration 278, loss = 20101115232.67857742\n",
      "Iteration 279, loss = 20101112639.59275055\n",
      "Iteration 280, loss = 20101110126.81701279\n",
      "Iteration 281, loss = 20101107560.79863739\n",
      "Iteration 282, loss = 20101105029.68236542\n",
      "Iteration 283, loss = 20101102465.39710999\n",
      "Iteration 284, loss = 20101099932.44588089\n",
      "Iteration 285, loss = 20101097403.45941925\n",
      "Iteration 286, loss = 20101094844.86455536\n",
      "Iteration 287, loss = 20101092274.39320755\n",
      "Iteration 288, loss = 20101089748.85007858\n",
      "Iteration 289, loss = 20101087204.80511093\n",
      "Iteration 290, loss = 20101084652.31059265\n",
      "Iteration 291, loss = 20101082135.29692078\n",
      "Iteration 292, loss = 20101079566.06608963\n",
      "Iteration 293, loss = 20101077027.38015366\n",
      "Iteration 294, loss = 20101074503.61113358\n",
      "Iteration 295, loss = 20101071964.41157532\n",
      "Iteration 296, loss = 20101069431.33522797\n",
      "Iteration 297, loss = 20101066880.12164688\n",
      "Iteration 298, loss = 20101064344.65372467\n",
      "Iteration 299, loss = 20101061814.07506180\n",
      "Iteration 300, loss = 20101059278.90863800\n",
      "Iteration 301, loss = 20101056728.54901123\n",
      "Iteration 302, loss = 20101054218.37868118\n",
      "Iteration 303, loss = 20101051687.23280716\n",
      "Iteration 304, loss = 20101049152.50020599\n",
      "Iteration 305, loss = 20101046624.67630768\n",
      "Iteration 306, loss = 20101044097.21697617\n",
      "Iteration 307, loss = 20101041578.23307037\n",
      "Iteration 308, loss = 20101039018.79188156\n",
      "Iteration 309, loss = 20101036523.24258041\n",
      "Iteration 310, loss = 20101034005.77244568\n",
      "Iteration 311, loss = 20101031472.21382141\n",
      "Iteration 312, loss = 20101028950.44170761\n",
      "Iteration 313, loss = 20101026419.72360611\n",
      "Iteration 314, loss = 20101023899.33777237\n",
      "Iteration 315, loss = 20101021383.92449570\n",
      "Iteration 316, loss = 20101018862.96575546\n",
      "Iteration 317, loss = 20101016346.63829803\n",
      "Iteration 318, loss = 20101013834.84428024\n",
      "Iteration 319, loss = 20101011325.90991592\n",
      "Iteration 320, loss = 20101008810.70335007\n",
      "Iteration 321, loss = 20101006273.84120941\n",
      "Iteration 322, loss = 20101003767.50408554\n",
      "Iteration 323, loss = 20101001261.24010468\n",
      "Iteration 324, loss = 20100998722.71497345\n",
      "Iteration 325, loss = 20100996231.35956573\n",
      "Iteration 326, loss = 20100993695.16500854\n",
      "Iteration 327, loss = 20100991196.64250565\n",
      "Iteration 328, loss = 20100988684.31044006\n",
      "Iteration 329, loss = 20100986207.63309860\n",
      "Iteration 330, loss = 20100983681.63012695\n",
      "Iteration 331, loss = 20100981157.22698212\n",
      "Iteration 332, loss = 20100978639.90101242\n",
      "Iteration 333, loss = 20100976133.50616455\n",
      "Iteration 334, loss = 20100973640.34337234\n",
      "Iteration 335, loss = 20100971137.35884094\n",
      "Iteration 336, loss = 20100968618.10577774\n",
      "Iteration 337, loss = 20100966133.25197983\n",
      "Iteration 338, loss = 20100963615.56079865\n",
      "Iteration 339, loss = 20100961088.20732880\n",
      "Iteration 340, loss = 20100958607.66620636\n",
      "Iteration 341, loss = 20100956092.38665771\n",
      "Iteration 342, loss = 20100953604.86375427\n",
      "Iteration 343, loss = 20100951114.14810181\n",
      "Iteration 344, loss = 20100948604.83345795\n",
      "Iteration 345, loss = 20100946102.05227280\n",
      "Iteration 346, loss = 20100943597.66182327\n",
      "Iteration 347, loss = 20100941087.28918076\n",
      "Iteration 348, loss = 20100938590.38664246\n",
      "Iteration 349, loss = 20100936107.84377670\n",
      "Iteration 350, loss = 20100933588.09343338\n",
      "Iteration 351, loss = 20100931102.46098709\n",
      "Iteration 352, loss = 20100928618.29650116\n",
      "Iteration 353, loss = 20100926107.41065598\n",
      "Iteration 354, loss = 20100923607.58588028\n",
      "Iteration 355, loss = 20100921105.08015823\n",
      "Iteration 356, loss = 20100918629.91100311\n",
      "Iteration 357, loss = 20100916129.82801819\n",
      "Iteration 358, loss = 20100913628.03699112\n",
      "Iteration 359, loss = 20100911152.58901978\n",
      "Iteration 360, loss = 20100908647.62014389\n",
      "Iteration 361, loss = 20100906140.86172867\n",
      "Iteration 362, loss = 20100903681.80989456\n",
      "Iteration 363, loss = 20100901159.52167511\n",
      "Iteration 364, loss = 20100898683.17689896\n",
      "Iteration 365, loss = 20100896192.44213867\n",
      "Iteration 366, loss = 20100893718.83963776\n",
      "Iteration 367, loss = 20100891206.68249512\n",
      "Iteration 368, loss = 20100888762.69236755\n",
      "Iteration 369, loss = 20100886247.30398560\n",
      "Iteration 370, loss = 20100883756.68460464\n",
      "Iteration 371, loss = 20100881269.47895050\n",
      "Iteration 372, loss = 20100878803.82783890\n",
      "Iteration 373, loss = 20100876314.92926788\n",
      "Iteration 374, loss = 20100873840.24787903\n",
      "Iteration 375, loss = 20100871345.83694077\n",
      "Iteration 376, loss = 20100868897.84550858\n",
      "Iteration 377, loss = 20100866368.18043518\n",
      "Iteration 378, loss = 20100863935.94756317\n",
      "Iteration 379, loss = 20100861430.74496841\n",
      "Iteration 380, loss = 20100858945.39161301\n",
      "Iteration 381, loss = 20100856471.05926895\n",
      "Iteration 382, loss = 20100853992.35088348\n",
      "Iteration 383, loss = 20100851531.04016113\n",
      "Iteration 384, loss = 20100849067.80791855\n",
      "Iteration 385, loss = 20100846573.37497711\n",
      "Iteration 386, loss = 20100844082.65312958\n",
      "Iteration 387, loss = 20100841583.45868683\n",
      "Iteration 388, loss = 20100839122.84799957\n",
      "Iteration 389, loss = 20100836654.25534439\n",
      "Iteration 390, loss = 20100834197.32536316\n",
      "Iteration 391, loss = 20100831692.04933167\n",
      "Iteration 392, loss = 20100829222.36253738\n",
      "Iteration 393, loss = 20100826743.62843704\n",
      "Iteration 394, loss = 20100824252.15850449\n",
      "Iteration 395, loss = 20100821786.76408768\n",
      "Iteration 396, loss = 20100819305.70281982\n",
      "Iteration 397, loss = 20100816850.63317871\n",
      "Iteration 398, loss = 20100814348.11848831\n",
      "Iteration 399, loss = 20100811882.37981415\n",
      "Iteration 400, loss = 20100809408.52062988\n",
      "Iteration 401, loss = 20100806923.88489914\n",
      "Iteration 402, loss = 20100804452.41532898\n",
      "Iteration 403, loss = 20100801971.48281097\n",
      "Iteration 404, loss = 20100799520.85699081\n",
      "Iteration 405, loss = 20100797028.29119873\n",
      "Iteration 406, loss = 20100794556.38198853\n",
      "Iteration 407, loss = 20100792098.84369278\n",
      "Iteration 408, loss = 20100789622.94229507\n",
      "Iteration 409, loss = 20100787146.76049042\n",
      "Iteration 410, loss = 20100784668.17161179\n",
      "Iteration 411, loss = 20100782196.19951630\n",
      "Iteration 412, loss = 20100779745.16253281\n",
      "Iteration 413, loss = 20100777256.72781754\n",
      "Iteration 414, loss = 20100774822.48238373\n",
      "Iteration 415, loss = 20100772317.94068527\n",
      "Iteration 416, loss = 20100769877.11503220\n",
      "Iteration 417, loss = 20100767399.85027313\n",
      "Iteration 418, loss = 20100764927.68537521\n",
      "Iteration 419, loss = 20100762459.28795242\n",
      "Iteration 420, loss = 20100760000.19892120\n",
      "Iteration 421, loss = 20100757584.04365540\n",
      "Iteration 422, loss = 20100755046.33777237\n",
      "Iteration 423, loss = 20100752618.13522339\n",
      "Iteration 424, loss = 20100750148.10174561\n",
      "Iteration 425, loss = 20100747682.00989532\n",
      "Iteration 426, loss = 20100745219.01180267\n",
      "Iteration 427, loss = 20100742769.08618164\n",
      "Iteration 428, loss = 20100740287.26426697\n",
      "Iteration 429, loss = 20100737836.63824081\n",
      "Iteration 430, loss = 20100735369.02437973\n",
      "Iteration 431, loss = 20100732910.49584198\n",
      "Iteration 432, loss = 20100730450.76114273\n",
      "Iteration 433, loss = 20100727976.92338562\n",
      "Iteration 434, loss = 20100725524.00795364\n",
      "Iteration 435, loss = 20100723052.84218216\n",
      "Iteration 436, loss = 20100720592.56309891\n",
      "Iteration 437, loss = 20100718123.07674789\n",
      "Iteration 438, loss = 20100715696.55921173\n",
      "Iteration 439, loss = 20100713214.41395187\n",
      "Iteration 440, loss = 20100710742.33510590\n",
      "Iteration 441, loss = 20100708286.86214828\n",
      "Iteration 442, loss = 20100705852.44142532\n",
      "Iteration 443, loss = 20100703367.15239334\n",
      "Iteration 444, loss = 20100700922.78907776\n",
      "Iteration 445, loss = 20100698470.22575760\n",
      "Iteration 446, loss = 20100695983.89162827\n",
      "Iteration 447, loss = 20100693539.40509033\n",
      "Iteration 448, loss = 20100691067.35369492\n",
      "Iteration 449, loss = 20100688638.14767838\n",
      "Iteration 450, loss = 20100686147.73257828\n",
      "Iteration 451, loss = 20100683668.88249588\n",
      "Iteration 452, loss = 20100681245.55283356\n",
      "Iteration 453, loss = 20100678771.51486206\n",
      "Iteration 454, loss = 20100676302.26103973\n",
      "Iteration 455, loss = 20100673838.64402008\n",
      "Iteration 456, loss = 20100671381.76978302\n",
      "Iteration 457, loss = 20100668942.33145523\n",
      "Iteration 458, loss = 20100666465.27374268\n",
      "Iteration 459, loss = 20100664016.60750580\n",
      "Iteration 460, loss = 20100661553.59260178\n",
      "Iteration 461, loss = 20100659086.93820953\n",
      "Iteration 462, loss = 20100656656.99241257\n",
      "Iteration 463, loss = 20100654188.75686264\n",
      "Iteration 464, loss = 20100651744.45813370\n",
      "Iteration 465, loss = 20100649292.60316849\n",
      "Iteration 466, loss = 20100646847.02599716\n",
      "Iteration 467, loss = 20100644399.57378387\n",
      "Iteration 468, loss = 20100641944.48154831\n",
      "Iteration 469, loss = 20100639481.64202118\n",
      "Iteration 470, loss = 20100637041.75094604\n",
      "Iteration 471, loss = 20100634576.50268555\n",
      "Iteration 472, loss = 20100632160.22852707\n",
      "Iteration 473, loss = 20100629692.85353470\n",
      "Iteration 474, loss = 20100627257.52281570\n",
      "Iteration 475, loss = 20100624808.72388840\n",
      "Iteration 476, loss = 20100622360.18825531\n",
      "Iteration 477, loss = 20100619917.62258530\n",
      "Iteration 478, loss = 20100617471.97924805\n",
      "Iteration 479, loss = 20100615004.17484283\n",
      "Iteration 480, loss = 20100612570.29073715\n",
      "Iteration 481, loss = 20100610120.81259918\n",
      "Iteration 482, loss = 20100607674.47854614\n",
      "Iteration 483, loss = 20100605256.46310043\n",
      "Iteration 484, loss = 20100602783.01117706\n",
      "Iteration 485, loss = 20100600356.85087204\n",
      "Iteration 486, loss = 20100597893.85670853\n",
      "Iteration 487, loss = 20100595447.52210999\n",
      "Iteration 488, loss = 20100592997.08204651\n",
      "Iteration 489, loss = 20100590573.29794693\n",
      "Iteration 490, loss = 20100588131.27655029\n",
      "Iteration 491, loss = 20100585690.39044189\n",
      "Iteration 492, loss = 20100583212.23637772\n",
      "Iteration 493, loss = 20100580791.37194061\n",
      "Iteration 494, loss = 20100578356.98464966\n",
      "Iteration 495, loss = 20100575914.02749252\n",
      "Iteration 496, loss = 20100573449.31886292\n",
      "Iteration 497, loss = 20100571010.15707397\n",
      "Iteration 498, loss = 20100568580.89500427\n",
      "Iteration 499, loss = 20100566150.11278152\n",
      "Iteration 500, loss = 20100563692.11964417\n",
      "Iteration 1, loss = 19475885627.76936722\n",
      "Iteration 2, loss = 19475874841.84301376\n",
      "Iteration 3, loss = 19475864319.23434830\n",
      "Iteration 4, loss = 19475853596.51615906\n",
      "Iteration 5, loss = 19475843158.44013596\n",
      "Iteration 6, loss = 19475832642.55759430\n",
      "Iteration 7, loss = 19475822182.66139984\n",
      "Iteration 8, loss = 19475811850.45554352\n",
      "Iteration 9, loss = 19475801396.67902756\n",
      "Iteration 10, loss = 19475791082.87538910\n",
      "Iteration 11, loss = 19475780819.37920761\n",
      "Iteration 12, loss = 19475770441.17812729\n",
      "Iteration 13, loss = 19475760064.50368500\n",
      "Iteration 14, loss = 19475749867.41230011\n",
      "Iteration 15, loss = 19475739471.02326584\n",
      "Iteration 16, loss = 19475729030.77859879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 19475718645.95698547\n",
      "Iteration 18, loss = 19475708027.08145523\n",
      "Iteration 19, loss = 19475697264.75980377\n",
      "Iteration 20, loss = 19475686371.36233902\n",
      "Iteration 21, loss = 19475675326.93980408\n",
      "Iteration 22, loss = 19475664086.17928696\n",
      "Iteration 23, loss = 19475652609.88645935\n",
      "Iteration 24, loss = 19475641137.38741684\n",
      "Iteration 25, loss = 19475629652.72873688\n",
      "Iteration 26, loss = 19475618149.17939377\n",
      "Iteration 27, loss = 19475606797.22212601\n",
      "Iteration 28, loss = 19475595553.23514175\n",
      "Iteration 29, loss = 19475584374.79142380\n",
      "Iteration 30, loss = 19475573269.26049423\n",
      "Iteration 31, loss = 19475562055.80574417\n",
      "Iteration 32, loss = 19475551003.93588257\n",
      "Iteration 33, loss = 19475539723.59430695\n",
      "Iteration 34, loss = 19475528185.73314285\n",
      "Iteration 35, loss = 19475516512.35840607\n",
      "Iteration 36, loss = 19475504627.25023270\n",
      "Iteration 37, loss = 19475492373.95657349\n",
      "Iteration 38, loss = 19475480072.13780212\n",
      "Iteration 39, loss = 19475467758.03667068\n",
      "Iteration 40, loss = 19475455343.53113174\n",
      "Iteration 41, loss = 19475442749.16218567\n",
      "Iteration 42, loss = 19475430250.09504700\n",
      "Iteration 43, loss = 19475417833.66186142\n",
      "Iteration 44, loss = 19475405594.08327866\n",
      "Iteration 45, loss = 19475393463.63158417\n",
      "Iteration 46, loss = 19475381554.74451447\n",
      "Iteration 47, loss = 19475369773.91991043\n",
      "Iteration 48, loss = 19475358326.63659668\n",
      "Iteration 49, loss = 19475347510.63533401\n",
      "Iteration 50, loss = 19475336839.06050873\n",
      "Iteration 51, loss = 19475326939.61157990\n",
      "Iteration 52, loss = 19475317305.82001114\n",
      "Iteration 53, loss = 19475308023.87766266\n",
      "Iteration 54, loss = 19475299046.93453217\n",
      "Iteration 55, loss = 19475290296.90415573\n",
      "Iteration 56, loss = 19475281854.68004608\n",
      "Iteration 57, loss = 19475273747.72352982\n",
      "Iteration 58, loss = 19475265873.12855530\n",
      "Iteration 59, loss = 19475258447.41248703\n",
      "Iteration 60, loss = 19475251369.52897644\n",
      "Iteration 61, loss = 19475244592.60580063\n",
      "Iteration 62, loss = 19475238139.62544632\n",
      "Iteration 63, loss = 19475231878.17876434\n",
      "Iteration 64, loss = 19475225762.14365005\n",
      "Iteration 65, loss = 19475219827.74701309\n",
      "Iteration 66, loss = 19475214158.52461243\n",
      "Iteration 67, loss = 19475208692.89157867\n",
      "Iteration 68, loss = 19475203536.28632736\n",
      "Iteration 69, loss = 19475198477.46769714\n",
      "Iteration 70, loss = 19475193525.45546722\n",
      "Iteration 71, loss = 19475188669.19581985\n",
      "Iteration 72, loss = 19475183976.41732025\n",
      "Iteration 73, loss = 19475179309.56600189\n",
      "Iteration 74, loss = 19475174765.41291809\n",
      "Iteration 75, loss = 19475170348.20981598\n",
      "Iteration 76, loss = 19475165957.95059967\n",
      "Iteration 77, loss = 19475161591.52368927\n",
      "Iteration 78, loss = 19475157279.63976288\n",
      "Iteration 79, loss = 19475153028.52155685\n",
      "Iteration 80, loss = 19475148746.07490158\n",
      "Iteration 81, loss = 19475144522.63642502\n",
      "Iteration 82, loss = 19475140348.40692520\n",
      "Iteration 83, loss = 19475136191.61571503\n",
      "Iteration 84, loss = 19475132093.91419983\n",
      "Iteration 85, loss = 19475128016.83874130\n",
      "Iteration 86, loss = 19475123950.18683243\n",
      "Iteration 87, loss = 19475119913.63333893\n",
      "Iteration 88, loss = 19475115875.25437164\n",
      "Iteration 89, loss = 19475111892.05725479\n",
      "Iteration 90, loss = 19475107966.39806366\n",
      "Iteration 91, loss = 19475104069.44523239\n",
      "Iteration 92, loss = 19475100230.13645172\n",
      "Iteration 93, loss = 19475096407.84178162\n",
      "Iteration 94, loss = 19475092580.87251663\n",
      "Iteration 95, loss = 19475088789.63898468\n",
      "Iteration 96, loss = 19475085048.59492874\n",
      "Iteration 97, loss = 19475081255.28153992\n",
      "Iteration 98, loss = 19475077547.86814499\n",
      "Iteration 99, loss = 19475073782.03126907\n",
      "Iteration 100, loss = 19475070084.93916702\n",
      "Iteration 101, loss = 19475066396.40013504\n",
      "Iteration 102, loss = 19475062734.57008743\n",
      "Iteration 103, loss = 19475059065.90264893\n",
      "Iteration 104, loss = 19475055460.44433212\n",
      "Iteration 105, loss = 19475051834.19396591\n",
      "Iteration 106, loss = 19475048262.55513382\n",
      "Iteration 107, loss = 19475044644.11426163\n",
      "Iteration 108, loss = 19475041100.18000793\n",
      "Iteration 109, loss = 19475037537.29243088\n",
      "Iteration 110, loss = 19475033981.14608002\n",
      "Iteration 111, loss = 19475030455.52419281\n",
      "Iteration 112, loss = 19475026928.94466019\n",
      "Iteration 113, loss = 19475023389.69385147\n",
      "Iteration 114, loss = 19475019873.34781265\n",
      "Iteration 115, loss = 19475016373.88865280\n",
      "Iteration 116, loss = 19475012871.62958908\n",
      "Iteration 117, loss = 19475009391.30148697\n",
      "Iteration 118, loss = 19475005929.82795334\n",
      "Iteration 119, loss = 19475002466.04599762\n",
      "Iteration 120, loss = 19474999010.79367828\n",
      "Iteration 121, loss = 19474995575.16945267\n",
      "Iteration 122, loss = 19474992153.21158981\n",
      "Iteration 123, loss = 19474988731.05706406\n",
      "Iteration 124, loss = 19474985281.84505081\n",
      "Iteration 125, loss = 19474981904.20343399\n",
      "Iteration 126, loss = 19474978462.02334976\n",
      "Iteration 127, loss = 19474975046.25430679\n",
      "Iteration 128, loss = 19474971648.38428497\n",
      "Iteration 129, loss = 19474968218.73987198\n",
      "Iteration 130, loss = 19474964816.68562698\n",
      "Iteration 131, loss = 19474961410.91453171\n",
      "Iteration 132, loss = 19474958046.30969238\n",
      "Iteration 133, loss = 19474954636.01133347\n",
      "Iteration 134, loss = 19474951315.12605286\n",
      "Iteration 135, loss = 19474947937.15131760\n",
      "Iteration 136, loss = 19474944583.01371384\n",
      "Iteration 137, loss = 19474941198.35277557\n",
      "Iteration 138, loss = 19474937893.65208054\n",
      "Iteration 139, loss = 19474934535.14686584\n",
      "Iteration 140, loss = 19474931188.56277466\n",
      "Iteration 141, loss = 19474927860.66278458\n",
      "Iteration 142, loss = 19474924532.26177597\n",
      "Iteration 143, loss = 19474921204.47416687\n",
      "Iteration 144, loss = 19474917874.55598450\n",
      "Iteration 145, loss = 19474914552.06344986\n",
      "Iteration 146, loss = 19474911223.38721085\n",
      "Iteration 147, loss = 19474907929.21804810\n",
      "Iteration 148, loss = 19474904600.65607452\n",
      "Iteration 149, loss = 19474901279.47516632\n",
      "Iteration 150, loss = 19474897991.85466003\n",
      "Iteration 151, loss = 19474894672.73726654\n",
      "Iteration 152, loss = 19474891373.78125763\n",
      "Iteration 153, loss = 19474888088.10937119\n",
      "Iteration 154, loss = 19474884781.46547699\n",
      "Iteration 155, loss = 19474881508.31423569\n",
      "Iteration 156, loss = 19474878220.46150970\n",
      "Iteration 157, loss = 19474874950.81214142\n",
      "Iteration 158, loss = 19474871670.25647354\n",
      "Iteration 159, loss = 19474868356.38319016\n",
      "Iteration 160, loss = 19474865100.17090607\n",
      "Iteration 161, loss = 19474861840.42292023\n",
      "Iteration 162, loss = 19474858556.58836365\n",
      "Iteration 163, loss = 19474855279.27277374\n",
      "Iteration 164, loss = 19474852001.36687851\n",
      "Iteration 165, loss = 19474848743.75310135\n",
      "Iteration 166, loss = 19474845486.58359909\n",
      "Iteration 167, loss = 19474842214.26159286\n",
      "Iteration 168, loss = 19474838975.90855789\n",
      "Iteration 169, loss = 19474835763.01688766\n",
      "Iteration 170, loss = 19474832528.31270981\n",
      "Iteration 171, loss = 19474829254.93880463\n",
      "Iteration 172, loss = 19474826066.60810852\n",
      "Iteration 173, loss = 19474822836.47385406\n",
      "Iteration 174, loss = 19474819600.18963242\n",
      "Iteration 175, loss = 19474816390.82094574\n",
      "Iteration 176, loss = 19474813154.21987915\n",
      "Iteration 177, loss = 19474809932.81353378\n",
      "Iteration 178, loss = 19474806719.85620880\n",
      "Iteration 179, loss = 19474803520.42993164\n",
      "Iteration 180, loss = 19474800309.86239624\n",
      "Iteration 181, loss = 19474797085.35803986\n",
      "Iteration 182, loss = 19474793895.02414703\n",
      "Iteration 183, loss = 19474790685.03908157\n",
      "Iteration 184, loss = 19474787456.47420502\n",
      "Iteration 185, loss = 19474784217.60974121\n",
      "Iteration 186, loss = 19474780994.10720062\n",
      "Iteration 187, loss = 19474777792.96393204\n",
      "Iteration 188, loss = 19474774558.92124176\n",
      "Iteration 189, loss = 19474771359.89296722\n",
      "Iteration 190, loss = 19474768166.47370529\n",
      "Iteration 191, loss = 19474764944.11066818\n",
      "Iteration 192, loss = 19474761744.29218292\n",
      "Iteration 193, loss = 19474758531.47513199\n",
      "Iteration 194, loss = 19474755353.15684509\n",
      "Iteration 195, loss = 19474752125.26652527\n",
      "Iteration 196, loss = 19474748961.18523788\n",
      "Iteration 197, loss = 19474745751.72360229\n",
      "Iteration 198, loss = 19474742555.44006348\n",
      "Iteration 199, loss = 19474739363.84690857\n",
      "Iteration 200, loss = 19474736183.36937714\n",
      "Iteration 201, loss = 19474733007.02962875\n",
      "Iteration 202, loss = 19474729826.62125397\n",
      "Iteration 203, loss = 19474726669.45944977\n",
      "Iteration 204, loss = 19474723473.71849060\n",
      "Iteration 205, loss = 19474720325.84680176\n",
      "Iteration 206, loss = 19474717145.52062225\n",
      "Iteration 207, loss = 19474713972.75695801\n",
      "Iteration 208, loss = 19474710801.38843918\n",
      "Iteration 209, loss = 19474707624.48306656\n",
      "Iteration 210, loss = 19474704452.94880295\n",
      "Iteration 211, loss = 19474701260.10243225\n",
      "Iteration 212, loss = 19474698094.75603867\n",
      "Iteration 213, loss = 19474694957.42421341\n",
      "Iteration 214, loss = 19474691771.29481888\n",
      "Iteration 215, loss = 19474688617.53559113\n",
      "Iteration 216, loss = 19474685457.63732910\n",
      "Iteration 217, loss = 19474682311.95096588\n",
      "Iteration 218, loss = 19474679146.14420319\n",
      "Iteration 219, loss = 19474676008.39348221\n",
      "Iteration 220, loss = 19474672884.19031525\n",
      "Iteration 221, loss = 19474669750.24431610\n",
      "Iteration 222, loss = 19474666588.82072067\n",
      "Iteration 223, loss = 19474663500.17670441\n",
      "Iteration 224, loss = 19474660349.42721558\n",
      "Iteration 225, loss = 19474657230.43130493\n",
      "Iteration 226, loss = 19474654099.38958359\n",
      "Iteration 227, loss = 19474650968.90654373\n",
      "Iteration 228, loss = 19474647848.91115952\n",
      "Iteration 229, loss = 19474644708.53168869\n",
      "Iteration 230, loss = 19474641564.16159439\n",
      "Iteration 231, loss = 19474638448.72207642\n",
      "Iteration 232, loss = 19474635311.82882309\n",
      "Iteration 233, loss = 19474632206.09386444\n",
      "Iteration 234, loss = 19474629082.27483368\n",
      "Iteration 235, loss = 19474625956.56319427\n",
      "Iteration 236, loss = 19474622833.20934296\n",
      "Iteration 237, loss = 19474619713.51949310\n",
      "Iteration 238, loss = 19474616600.91865158\n",
      "Iteration 239, loss = 19474613479.16792679\n",
      "Iteration 240, loss = 19474610369.81280136\n",
      "Iteration 241, loss = 19474607235.45122528\n",
      "Iteration 242, loss = 19474604126.73337555\n",
      "Iteration 243, loss = 19474601015.66751099\n",
      "Iteration 244, loss = 19474597909.62573624\n",
      "Iteration 245, loss = 19474594798.49662399\n",
      "Iteration 246, loss = 19474591676.72942734\n",
      "Iteration 247, loss = 19474588567.68295670\n",
      "Iteration 248, loss = 19474585446.79566193\n",
      "Iteration 249, loss = 19474582356.35536575\n",
      "Iteration 250, loss = 19474579216.13169861\n",
      "Iteration 251, loss = 19474576135.82651138\n",
      "Iteration 252, loss = 19474573027.99014664\n",
      "Iteration 253, loss = 19474569924.58287430\n",
      "Iteration 254, loss = 19474566807.66844559\n",
      "Iteration 255, loss = 19474563716.01870346\n",
      "Iteration 256, loss = 19474560629.19696045\n",
      "Iteration 257, loss = 19474557535.32655334\n",
      "Iteration 258, loss = 19474554447.55184174\n",
      "Iteration 259, loss = 19474551343.99971390\n",
      "Iteration 260, loss = 19474548256.09812164\n",
      "Iteration 261, loss = 19474545153.83206177\n",
      "Iteration 262, loss = 19474542083.54701996\n",
      "Iteration 263, loss = 19474538985.40124130\n",
      "Iteration 264, loss = 19474535887.32531738\n",
      "Iteration 265, loss = 19474532824.35753250\n",
      "Iteration 266, loss = 19474529704.67225266\n",
      "Iteration 267, loss = 19474526608.59628677\n",
      "Iteration 268, loss = 19474523488.16493225\n",
      "Iteration 269, loss = 19474520449.04947281\n",
      "Iteration 270, loss = 19474517323.57041550\n",
      "Iteration 271, loss = 19474514252.00952148\n",
      "Iteration 272, loss = 19474511182.05103302\n",
      "Iteration 273, loss = 19474508111.07863998\n",
      "Iteration 274, loss = 19474505035.11827469\n",
      "Iteration 275, loss = 19474501969.56243896\n",
      "Iteration 276, loss = 19474498905.63251495\n",
      "Iteration 277, loss = 19474495813.26093674\n",
      "Iteration 278, loss = 19474492733.72892380\n",
      "Iteration 279, loss = 19474489678.78355026\n",
      "Iteration 280, loss = 19474486604.48658371\n",
      "Iteration 281, loss = 19474483530.42721939\n",
      "Iteration 282, loss = 19474480417.32881165\n",
      "Iteration 283, loss = 19474477371.51791763\n",
      "Iteration 284, loss = 19474474268.62187958\n",
      "Iteration 285, loss = 19474471194.66854858\n",
      "Iteration 286, loss = 19474468112.76739120\n",
      "Iteration 287, loss = 19474465014.97364426\n",
      "Iteration 288, loss = 19474461914.45610428\n",
      "Iteration 289, loss = 19474458860.30645370\n",
      "Iteration 290, loss = 19474455777.62147903\n",
      "Iteration 291, loss = 19474452684.15032196\n",
      "Iteration 292, loss = 19474449627.21210098\n",
      "Iteration 293, loss = 19474446552.72837830\n",
      "Iteration 294, loss = 19474443499.90282059\n",
      "Iteration 295, loss = 19474440412.24996948\n",
      "Iteration 296, loss = 19474437357.96721268\n",
      "Iteration 297, loss = 19474434302.08034897\n",
      "Iteration 298, loss = 19474431238.21466064\n",
      "Iteration 299, loss = 19474428164.61001205\n",
      "Iteration 300, loss = 19474425096.75670242\n",
      "Iteration 301, loss = 19474422047.96480942\n",
      "Iteration 302, loss = 19474418988.17108154\n",
      "Iteration 303, loss = 19474415942.08167267\n",
      "Iteration 304, loss = 19474412858.74985886\n",
      "Iteration 305, loss = 19474409815.23853683\n",
      "Iteration 306, loss = 19474406738.02547073\n",
      "Iteration 307, loss = 19474403717.81711197\n",
      "Iteration 308, loss = 19474400612.99682236\n",
      "Iteration 309, loss = 19474397563.88486099\n",
      "Iteration 310, loss = 19474394531.66656113\n",
      "Iteration 311, loss = 19474391459.50627518\n",
      "Iteration 312, loss = 19474388435.91566086\n",
      "Iteration 313, loss = 19474385364.99594498\n",
      "Iteration 314, loss = 19474382354.32287979\n",
      "Iteration 315, loss = 19474379299.67802429\n",
      "Iteration 316, loss = 19474376267.75845337\n",
      "Iteration 317, loss = 19474373226.52049255\n",
      "Iteration 318, loss = 19474370208.07815170\n",
      "Iteration 319, loss = 19474367163.92131424\n",
      "Iteration 320, loss = 19474364131.31162643\n",
      "Iteration 321, loss = 19474361106.05726624\n",
      "Iteration 322, loss = 19474358036.23896027\n",
      "Iteration 323, loss = 19474355017.09950638\n",
      "Iteration 324, loss = 19474351968.19089508\n",
      "Iteration 325, loss = 19474348902.02661514\n",
      "Iteration 326, loss = 19474345846.33030701\n",
      "Iteration 327, loss = 19474342806.61773682\n",
      "Iteration 328, loss = 19474339742.60503387\n",
      "Iteration 329, loss = 19474336697.09456253\n",
      "Iteration 330, loss = 19474333652.55769730\n",
      "Iteration 331, loss = 19474330570.48460770\n",
      "Iteration 332, loss = 19474327535.85176468\n",
      "Iteration 333, loss = 19474324493.13379288\n",
      "Iteration 334, loss = 19474321441.70701599\n",
      "Iteration 335, loss = 19474318400.20945358\n",
      "Iteration 336, loss = 19474315332.20483017\n",
      "Iteration 337, loss = 19474312268.87222672\n",
      "Iteration 338, loss = 19474309240.99502182\n",
      "Iteration 339, loss = 19474306215.72486496\n",
      "Iteration 340, loss = 19474303150.84220123\n",
      "Iteration 341, loss = 19474300139.15971756\n",
      "Iteration 342, loss = 19474297091.07094574\n",
      "Iteration 343, loss = 19474294038.35762787\n",
      "Iteration 344, loss = 19474291035.18397903\n",
      "Iteration 345, loss = 19474287996.05662918\n",
      "Iteration 346, loss = 19474284977.36353302\n",
      "Iteration 347, loss = 19474281952.97550964\n",
      "Iteration 348, loss = 19474278903.53553391\n",
      "Iteration 349, loss = 19474275861.08594131\n",
      "Iteration 350, loss = 19474272836.34560013\n",
      "Iteration 351, loss = 19474269794.05267334\n",
      "Iteration 352, loss = 19474266741.00769424\n",
      "Iteration 353, loss = 19474263705.28433990\n",
      "Iteration 354, loss = 19474260657.30664444\n",
      "Iteration 355, loss = 19474257647.71747971\n",
      "Iteration 356, loss = 19474254604.85438538\n",
      "Iteration 357, loss = 19474251559.37052536\n",
      "Iteration 358, loss = 19474248533.69660950\n",
      "Iteration 359, loss = 19474245468.64213943\n",
      "Iteration 360, loss = 19474242426.86999893\n",
      "Iteration 361, loss = 19474239387.82610321\n",
      "Iteration 362, loss = 19474236376.35173416\n",
      "Iteration 363, loss = 19474233339.21305466\n",
      "Iteration 364, loss = 19474230270.78672791\n",
      "Iteration 365, loss = 19474227262.32730103\n",
      "Iteration 366, loss = 19474224216.44960403\n",
      "Iteration 367, loss = 19474221211.53348923\n",
      "Iteration 368, loss = 19474218185.76924515\n",
      "Iteration 369, loss = 19474215170.08567429\n",
      "Iteration 370, loss = 19474212125.96192932\n",
      "Iteration 371, loss = 19474209107.83967972\n",
      "Iteration 372, loss = 19474206066.22677994\n",
      "Iteration 373, loss = 19474203047.67309570\n",
      "Iteration 374, loss = 19474199991.21528625\n",
      "Iteration 375, loss = 19474196958.95418549\n",
      "Iteration 376, loss = 19474193925.56954193\n",
      "Iteration 377, loss = 19474190903.54126358\n",
      "Iteration 378, loss = 19474187900.62608719\n",
      "Iteration 379, loss = 19474184874.75727844\n",
      "Iteration 380, loss = 19474181864.08641052\n",
      "Iteration 381, loss = 19474178846.47170639\n",
      "Iteration 382, loss = 19474175854.20111847\n",
      "Iteration 383, loss = 19474172809.25000000\n",
      "Iteration 384, loss = 19474169823.53291321\n",
      "Iteration 385, loss = 19474166805.75537109\n",
      "Iteration 386, loss = 19474163806.18832779\n",
      "Iteration 387, loss = 19474160808.17249298\n",
      "Iteration 388, loss = 19474157806.92885971\n",
      "Iteration 389, loss = 19474154796.44905853\n",
      "Iteration 390, loss = 19474151821.40776443\n",
      "Iteration 391, loss = 19474148833.54835892\n",
      "Iteration 392, loss = 19474145821.82002258\n",
      "Iteration 393, loss = 19474142835.68964386\n",
      "Iteration 394, loss = 19474139834.82699966\n",
      "Iteration 395, loss = 19474136832.01652908\n",
      "Iteration 396, loss = 19474133823.52779007\n",
      "Iteration 397, loss = 19474130844.57639694\n",
      "Iteration 398, loss = 19474127806.15826035\n",
      "Iteration 399, loss = 19474124811.18020248\n",
      "Iteration 400, loss = 19474121790.37564468\n",
      "Iteration 401, loss = 19474118779.67734146\n",
      "Iteration 402, loss = 19474115765.67503738\n",
      "Iteration 403, loss = 19474112698.14286804\n",
      "Iteration 404, loss = 19474109680.03058624\n",
      "Iteration 405, loss = 19474106623.64688492\n",
      "Iteration 406, loss = 19474103592.74873352\n",
      "Iteration 407, loss = 19474100549.51778412\n",
      "Iteration 408, loss = 19474097486.99777603\n",
      "Iteration 409, loss = 19474094459.81684113\n",
      "Iteration 410, loss = 19474091417.24834824\n",
      "Iteration 411, loss = 19474088376.45742035\n",
      "Iteration 412, loss = 19474085356.73152542\n",
      "Iteration 413, loss = 19474082324.95550919\n",
      "Iteration 414, loss = 19474079299.94203186\n",
      "Iteration 415, loss = 19474076293.19724655\n",
      "Iteration 416, loss = 19474073265.74264908\n",
      "Iteration 417, loss = 19474070277.18507385\n",
      "Iteration 418, loss = 19474067256.53863525\n",
      "Iteration 419, loss = 19474064268.23915100\n",
      "Iteration 420, loss = 19474061254.81597519\n",
      "Iteration 421, loss = 19474058219.57769775\n",
      "Iteration 422, loss = 19474055220.40467072\n",
      "Iteration 423, loss = 19474052224.57798386\n",
      "Iteration 424, loss = 19474049211.90555573\n",
      "Iteration 425, loss = 19474046214.37715149\n",
      "Iteration 426, loss = 19474043207.07726669\n",
      "Iteration 427, loss = 19474040202.84433365\n",
      "Iteration 428, loss = 19474037189.13645935\n",
      "Iteration 429, loss = 19474034186.90837097\n",
      "Iteration 430, loss = 19474031178.61137390\n",
      "Iteration 431, loss = 19474028179.81627274\n",
      "Iteration 432, loss = 19474025171.54133606\n",
      "Iteration 433, loss = 19474022164.22715378\n",
      "Iteration 434, loss = 19474019173.78527451\n",
      "Iteration 435, loss = 19474016178.79799271\n",
      "Iteration 436, loss = 19474013154.93406296\n",
      "Iteration 437, loss = 19474010172.55483246\n",
      "Iteration 438, loss = 19474007182.23492050\n",
      "Iteration 439, loss = 19474004149.01629257\n",
      "Iteration 440, loss = 19474001158.31327820\n",
      "Iteration 441, loss = 19473998164.80461502\n",
      "Iteration 442, loss = 19473995164.04235077\n",
      "Iteration 443, loss = 19473992170.47941589\n",
      "Iteration 444, loss = 19473989158.38859940\n",
      "Iteration 445, loss = 19473986163.96961975\n",
      "Iteration 446, loss = 19473983189.37541962\n",
      "Iteration 447, loss = 19473980193.80691910\n",
      "Iteration 448, loss = 19473977190.29107666\n",
      "Iteration 449, loss = 19473974196.60202789\n",
      "Iteration 450, loss = 19473971223.25333405\n",
      "Iteration 451, loss = 19473968198.28331757\n",
      "Iteration 452, loss = 19473965225.87770081\n",
      "Iteration 453, loss = 19473962211.15769196\n",
      "Iteration 454, loss = 19473959235.14810562\n",
      "Iteration 455, loss = 19473956239.05602646\n",
      "Iteration 456, loss = 19473953231.29949570\n",
      "Iteration 457, loss = 19473950233.10982132\n",
      "Iteration 458, loss = 19473947249.39388657\n",
      "Iteration 459, loss = 19473944247.32070923\n",
      "Iteration 460, loss = 19473941241.00921249\n",
      "Iteration 461, loss = 19473938236.42294312\n",
      "Iteration 462, loss = 19473935226.63447952\n",
      "Iteration 463, loss = 19473932256.43665695\n",
      "Iteration 464, loss = 19473929244.44765091\n",
      "Iteration 465, loss = 19473926246.34816360\n",
      "Iteration 466, loss = 19473923225.70035553\n",
      "Iteration 467, loss = 19473920262.82326889\n",
      "Iteration 468, loss = 19473917254.46451569\n",
      "Iteration 469, loss = 19473914258.36075974\n",
      "Iteration 470, loss = 19473911276.52922058\n",
      "Iteration 471, loss = 19473908318.68259811\n",
      "Iteration 472, loss = 19473905283.45173645\n",
      "Iteration 473, loss = 19473902320.36280060\n",
      "Iteration 474, loss = 19473899349.28451920\n",
      "Iteration 475, loss = 19473896353.37008667\n",
      "Iteration 476, loss = 19473893370.80794525\n",
      "Iteration 477, loss = 19473890405.91554642\n",
      "Iteration 478, loss = 19473887388.27765656\n",
      "Iteration 479, loss = 19473884411.91304779\n",
      "Iteration 480, loss = 19473881425.36676407\n",
      "Iteration 481, loss = 19473878459.60473633\n",
      "Iteration 482, loss = 19473875440.36819458\n",
      "Iteration 483, loss = 19473872460.99787521\n",
      "Iteration 484, loss = 19473869484.31524658\n",
      "Iteration 485, loss = 19473866490.65975571\n",
      "Iteration 486, loss = 19473863524.18333817\n",
      "Iteration 487, loss = 19473860538.92291641\n",
      "Iteration 488, loss = 19473857535.51708603\n",
      "Iteration 489, loss = 19473854542.97523117\n",
      "Iteration 490, loss = 19473851567.49422073\n",
      "Iteration 491, loss = 19473848561.76646042\n",
      "Iteration 492, loss = 19473845569.73127365\n",
      "Iteration 493, loss = 19473842583.07558823\n",
      "Iteration 494, loss = 19473839566.62084579\n",
      "Iteration 495, loss = 19473836578.45105743\n",
      "Iteration 496, loss = 19473833612.46080780\n",
      "Iteration 497, loss = 19473830623.57664108\n",
      "Iteration 498, loss = 19473827631.81167221\n",
      "Iteration 499, loss = 19473824656.66552734\n",
      "Iteration 500, loss = 19473821674.28874207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19652973288.30361557\n",
      "Iteration 2, loss = 19652962533.66426086\n",
      "Iteration 3, loss = 19652951781.42639542\n",
      "Iteration 4, loss = 19652941046.14810181\n",
      "Iteration 5, loss = 19652930526.22039032\n",
      "Iteration 6, loss = 19652919803.58722687\n",
      "Iteration 7, loss = 19652909337.03389359\n",
      "Iteration 8, loss = 19652898776.15492630\n",
      "Iteration 9, loss = 19652888211.03920364\n",
      "Iteration 10, loss = 19652877502.18181610\n",
      "Iteration 11, loss = 19652866733.92049026\n",
      "Iteration 12, loss = 19652855796.74808884\n",
      "Iteration 13, loss = 19652844829.97496796\n",
      "Iteration 14, loss = 19652833608.98189163\n",
      "Iteration 15, loss = 19652822209.42651749\n",
      "Iteration 16, loss = 19652810649.54262543\n",
      "Iteration 17, loss = 19652798871.51387405\n",
      "Iteration 18, loss = 19652786878.64858627\n",
      "Iteration 19, loss = 19652774477.29748154\n",
      "Iteration 20, loss = 19652761920.65774155\n",
      "Iteration 21, loss = 19652748689.06692886\n",
      "Iteration 22, loss = 19652735483.13711166\n",
      "Iteration 23, loss = 19652721879.70537949\n",
      "Iteration 24, loss = 19652708351.56472397\n",
      "Iteration 25, loss = 19652694714.02648926\n",
      "Iteration 26, loss = 19652681402.46505356\n",
      "Iteration 27, loss = 19652668194.54093552\n",
      "Iteration 28, loss = 19652655339.56070328\n",
      "Iteration 29, loss = 19652642538.53246689\n",
      "Iteration 30, loss = 19652629954.65298080\n",
      "Iteration 31, loss = 19652617250.05320740\n",
      "Iteration 32, loss = 19652604761.77757263\n",
      "Iteration 33, loss = 19652592647.93814850\n",
      "Iteration 34, loss = 19652581116.75090790\n",
      "Iteration 35, loss = 19652569924.42974091\n",
      "Iteration 36, loss = 19652559095.53346252\n",
      "Iteration 37, loss = 19652548603.88993835\n",
      "Iteration 38, loss = 19652538325.04450607\n",
      "Iteration 39, loss = 19652528151.85887146\n",
      "Iteration 40, loss = 19652518229.13013458\n",
      "Iteration 41, loss = 19652508419.92811966\n",
      "Iteration 42, loss = 19652498955.15462112\n",
      "Iteration 43, loss = 19652489660.71459198\n",
      "Iteration 44, loss = 19652480774.76086426\n",
      "Iteration 45, loss = 19652472107.81793213\n",
      "Iteration 46, loss = 19652463686.53195572\n",
      "Iteration 47, loss = 19652455339.37148666\n",
      "Iteration 48, loss = 19652446930.40808105\n",
      "Iteration 49, loss = 19652438254.38445663\n",
      "Iteration 50, loss = 19652429507.22090912\n",
      "Iteration 51, loss = 19652420779.92533493\n",
      "Iteration 52, loss = 19652412352.53078842\n",
      "Iteration 53, loss = 19652404495.29433060\n",
      "Iteration 54, loss = 19652397015.80831146\n",
      "Iteration 55, loss = 19652389708.73142242\n",
      "Iteration 56, loss = 19652382524.45592117\n",
      "Iteration 57, loss = 19652375385.56999207\n",
      "Iteration 58, loss = 19652368251.51115417\n",
      "Iteration 59, loss = 19652361183.93917465\n",
      "Iteration 60, loss = 19652354324.67977524\n",
      "Iteration 61, loss = 19652347729.93815613\n",
      "Iteration 62, loss = 19652341553.07047272\n",
      "Iteration 63, loss = 19652335695.28264618\n",
      "Iteration 64, loss = 19652330019.38298416\n",
      "Iteration 65, loss = 19652324613.89189529\n",
      "Iteration 66, loss = 19652319331.89913559\n",
      "Iteration 67, loss = 19652314238.41798019\n",
      "Iteration 68, loss = 19652309312.69855118\n",
      "Iteration 69, loss = 19652304372.31699753\n",
      "Iteration 70, loss = 19652299539.62647247\n",
      "Iteration 71, loss = 19652294690.06810379\n",
      "Iteration 72, loss = 19652289769.01552963\n",
      "Iteration 73, loss = 19652284796.90016937\n",
      "Iteration 74, loss = 19652279924.49724579\n",
      "Iteration 75, loss = 19652275081.78466797\n",
      "Iteration 76, loss = 19652270416.69497681\n",
      "Iteration 77, loss = 19652265853.72729111\n",
      "Iteration 78, loss = 19652261393.09481049\n",
      "Iteration 79, loss = 19652256937.27074814\n",
      "Iteration 80, loss = 19652252545.99409866\n",
      "Iteration 81, loss = 19652248173.42123032\n",
      "Iteration 82, loss = 19652243890.53763962\n",
      "Iteration 83, loss = 19652239531.72450256\n",
      "Iteration 84, loss = 19652235276.95383072\n",
      "Iteration 85, loss = 19652231038.25020981\n",
      "Iteration 86, loss = 19652226786.59465027\n",
      "Iteration 87, loss = 19652222606.05202866\n",
      "Iteration 88, loss = 19652218448.77050400\n",
      "Iteration 89, loss = 19652214213.94852829\n",
      "Iteration 90, loss = 19652210070.04905701\n",
      "Iteration 91, loss = 19652205917.74314499\n",
      "Iteration 92, loss = 19652201768.99880219\n",
      "Iteration 93, loss = 19652197657.50792313\n",
      "Iteration 94, loss = 19652193522.85034561\n",
      "Iteration 95, loss = 19652189369.94711304\n",
      "Iteration 96, loss = 19652185159.52497101\n",
      "Iteration 97, loss = 19652180967.77869415\n",
      "Iteration 98, loss = 19652176749.10808563\n",
      "Iteration 99, loss = 19652172515.03065491\n",
      "Iteration 100, loss = 19652168336.05245209\n",
      "Iteration 101, loss = 19652164167.38087463\n",
      "Iteration 102, loss = 19652159940.86165237\n",
      "Iteration 103, loss = 19652155804.72979355\n",
      "Iteration 104, loss = 19652151592.18558884\n",
      "Iteration 105, loss = 19652147414.28517151\n",
      "Iteration 106, loss = 19652143189.66851425\n",
      "Iteration 107, loss = 19652139018.06399918\n",
      "Iteration 108, loss = 19652134917.19321823\n",
      "Iteration 109, loss = 19652130770.20521545\n",
      "Iteration 110, loss = 19652126656.82585144\n",
      "Iteration 111, loss = 19652122551.63483429\n",
      "Iteration 112, loss = 19652118479.28378296\n",
      "Iteration 113, loss = 19652114392.29585648\n",
      "Iteration 114, loss = 19652110313.94526672\n",
      "Iteration 115, loss = 19652106238.41444397\n",
      "Iteration 116, loss = 19652102143.60933304\n",
      "Iteration 117, loss = 19652098083.36297989\n",
      "Iteration 118, loss = 19652094111.15513611\n",
      "Iteration 119, loss = 19652090075.15713120\n",
      "Iteration 120, loss = 19652086146.28376389\n",
      "Iteration 121, loss = 19652082173.86339951\n",
      "Iteration 122, loss = 19652078247.89531326\n",
      "Iteration 123, loss = 19652074343.88673019\n",
      "Iteration 124, loss = 19652070428.44553375\n",
      "Iteration 125, loss = 19652066507.86448669\n",
      "Iteration 126, loss = 19652062634.83112335\n",
      "Iteration 127, loss = 19652058771.70039749\n",
      "Iteration 128, loss = 19652054888.02290726\n",
      "Iteration 129, loss = 19652051050.33649826\n",
      "Iteration 130, loss = 19652047196.41673279\n",
      "Iteration 131, loss = 19652043376.56034851\n",
      "Iteration 132, loss = 19652039582.81191254\n",
      "Iteration 133, loss = 19652035671.05562210\n",
      "Iteration 134, loss = 19652031898.07143784\n",
      "Iteration 135, loss = 19652028149.56021881\n",
      "Iteration 136, loss = 19652024280.01808167\n",
      "Iteration 137, loss = 19652020526.51382065\n",
      "Iteration 138, loss = 19652016732.45560837\n",
      "Iteration 139, loss = 19652012963.91922760\n",
      "Iteration 140, loss = 19652009159.55665588\n",
      "Iteration 141, loss = 19652005403.41982269\n",
      "Iteration 142, loss = 19652001620.98826981\n",
      "Iteration 143, loss = 19651997868.35546875\n",
      "Iteration 144, loss = 19651994121.47377777\n",
      "Iteration 145, loss = 19651990315.71629333\n",
      "Iteration 146, loss = 19651986574.40894318\n",
      "Iteration 147, loss = 19651982767.13857651\n",
      "Iteration 148, loss = 19651978966.97911453\n",
      "Iteration 149, loss = 19651975185.56348801\n",
      "Iteration 150, loss = 19651971381.24942780\n",
      "Iteration 151, loss = 19651967630.32135773\n",
      "Iteration 152, loss = 19651963824.79889297\n",
      "Iteration 153, loss = 19651960050.49931717\n",
      "Iteration 154, loss = 19651956270.60944366\n",
      "Iteration 155, loss = 19651952429.76969910\n",
      "Iteration 156, loss = 19651948588.36806870\n",
      "Iteration 157, loss = 19651944758.95888519\n",
      "Iteration 158, loss = 19651940910.69440842\n",
      "Iteration 159, loss = 19651937022.88192749\n",
      "Iteration 160, loss = 19651933082.26892090\n",
      "Iteration 161, loss = 19651929202.69996262\n",
      "Iteration 162, loss = 19651925240.53577805\n",
      "Iteration 163, loss = 19651921276.99282455\n",
      "Iteration 164, loss = 19651917321.27871704\n",
      "Iteration 165, loss = 19651913346.32020950\n",
      "Iteration 166, loss = 19651909364.18402863\n",
      "Iteration 167, loss = 19651905361.75086975\n",
      "Iteration 168, loss = 19651901346.67517853\n",
      "Iteration 169, loss = 19651897405.90447617\n",
      "Iteration 170, loss = 19651893417.69875336\n",
      "Iteration 171, loss = 19651889448.97060394\n",
      "Iteration 172, loss = 19651885464.87396622\n",
      "Iteration 173, loss = 19651881528.22108841\n",
      "Iteration 174, loss = 19651877603.90620041\n",
      "Iteration 175, loss = 19651873630.26439667\n",
      "Iteration 176, loss = 19651869730.10268784\n",
      "Iteration 177, loss = 19651865786.62294769\n",
      "Iteration 178, loss = 19651861921.37124252\n",
      "Iteration 179, loss = 19651858029.22486496\n",
      "Iteration 180, loss = 19651854135.30338287\n",
      "Iteration 181, loss = 19651850227.69261169\n",
      "Iteration 182, loss = 19651846383.02468872\n",
      "Iteration 183, loss = 19651842530.46534729\n",
      "Iteration 184, loss = 19651838660.12227631\n",
      "Iteration 185, loss = 19651834815.87752914\n",
      "Iteration 186, loss = 19651830969.10682297\n",
      "Iteration 187, loss = 19651827152.51577759\n",
      "Iteration 188, loss = 19651823333.04064941\n",
      "Iteration 189, loss = 19651819521.03708267\n",
      "Iteration 190, loss = 19651815665.13674545\n",
      "Iteration 191, loss = 19651811876.69436264\n",
      "Iteration 192, loss = 19651808093.78224564\n",
      "Iteration 193, loss = 19651804300.31245804\n",
      "Iteration 194, loss = 19651800501.63344574\n",
      "Iteration 195, loss = 19651796739.20127106\n",
      "Iteration 196, loss = 19651792929.95011520\n",
      "Iteration 197, loss = 19651789178.65726471\n",
      "Iteration 198, loss = 19651785385.72882462\n",
      "Iteration 199, loss = 19651781619.30996704\n",
      "Iteration 200, loss = 19651777886.55920410\n",
      "Iteration 201, loss = 19651774107.56629181\n",
      "Iteration 202, loss = 19651770356.07453156\n",
      "Iteration 203, loss = 19651766629.29914093\n",
      "Iteration 204, loss = 19651762878.64023590\n",
      "Iteration 205, loss = 19651759152.41427231\n",
      "Iteration 206, loss = 19651755384.14786530\n",
      "Iteration 207, loss = 19651751665.58878708\n",
      "Iteration 208, loss = 19651747953.08504868\n",
      "Iteration 209, loss = 19651744200.11489868\n",
      "Iteration 210, loss = 19651740520.93894958\n",
      "Iteration 211, loss = 19651736791.08405685\n",
      "Iteration 212, loss = 19651733079.31211090\n",
      "Iteration 213, loss = 19651729364.94990158\n",
      "Iteration 214, loss = 19651725639.46284103\n",
      "Iteration 215, loss = 19651721955.20339966\n",
      "Iteration 216, loss = 19651718245.70242310\n",
      "Iteration 217, loss = 19651714563.47452927\n",
      "Iteration 218, loss = 19651710857.20636368\n",
      "Iteration 219, loss = 19651707154.36248779\n",
      "Iteration 220, loss = 19651703491.14851761\n",
      "Iteration 221, loss = 19651699796.15338898\n",
      "Iteration 222, loss = 19651696102.11918259\n",
      "Iteration 223, loss = 19651692425.94785309\n",
      "Iteration 224, loss = 19651688750.37380219\n",
      "Iteration 225, loss = 19651685098.64109802\n",
      "Iteration 226, loss = 19651681390.01753235\n",
      "Iteration 227, loss = 19651677750.91399384\n",
      "Iteration 228, loss = 19651674021.04517746\n",
      "Iteration 229, loss = 19651670409.55593109\n",
      "Iteration 230, loss = 19651666752.09077835\n",
      "Iteration 231, loss = 19651663059.65691757\n",
      "Iteration 232, loss = 19651659421.64311218\n",
      "Iteration 233, loss = 19651655762.36483383\n",
      "Iteration 234, loss = 19651652118.16635895\n",
      "Iteration 235, loss = 19651648437.26507187\n",
      "Iteration 236, loss = 19651644798.17934036\n",
      "Iteration 237, loss = 19651641145.84685898\n",
      "Iteration 238, loss = 19651637462.94126892\n",
      "Iteration 239, loss = 19651633832.25101852\n",
      "Iteration 240, loss = 19651630207.68964386\n",
      "Iteration 241, loss = 19651626518.17291641\n",
      "Iteration 242, loss = 19651622871.31448746\n",
      "Iteration 243, loss = 19651619197.11899185\n",
      "Iteration 244, loss = 19651615551.13206100\n",
      "Iteration 245, loss = 19651611927.90142059\n",
      "Iteration 246, loss = 19651608294.26142120\n",
      "Iteration 247, loss = 19651604655.84813690\n",
      "Iteration 248, loss = 19651601018.29251862\n",
      "Iteration 249, loss = 19651597389.71895218\n",
      "Iteration 250, loss = 19651593754.75962448\n",
      "Iteration 251, loss = 19651590129.73533630\n",
      "Iteration 252, loss = 19651586471.38490295\n",
      "Iteration 253, loss = 19651582845.19314575\n",
      "Iteration 254, loss = 19651579240.32306290\n",
      "Iteration 255, loss = 19651575595.06304550\n",
      "Iteration 256, loss = 19651571993.87833405\n",
      "Iteration 257, loss = 19651568354.05253220\n",
      "Iteration 258, loss = 19651564725.13545609\n",
      "Iteration 259, loss = 19651561109.36555481\n",
      "Iteration 260, loss = 19651557452.49087143\n",
      "Iteration 261, loss = 19651553872.57749176\n",
      "Iteration 262, loss = 19651550193.95930481\n",
      "Iteration 263, loss = 19651546547.10095596\n",
      "Iteration 264, loss = 19651542874.59529877\n",
      "Iteration 265, loss = 19651539288.03538895\n",
      "Iteration 266, loss = 19651535612.14796066\n",
      "Iteration 267, loss = 19651531907.72541046\n",
      "Iteration 268, loss = 19651528228.84310150\n",
      "Iteration 269, loss = 19651524500.74719620\n",
      "Iteration 270, loss = 19651520809.22272491\n",
      "Iteration 271, loss = 19651517015.23084641\n",
      "Iteration 272, loss = 19651513245.63443756\n",
      "Iteration 273, loss = 19651509462.95919800\n",
      "Iteration 274, loss = 19651505595.86259460\n",
      "Iteration 275, loss = 19651501715.50105286\n",
      "Iteration 276, loss = 19651497741.44513702\n",
      "Iteration 277, loss = 19651493803.67047882\n",
      "Iteration 278, loss = 19651489746.00459290\n",
      "Iteration 279, loss = 19651485743.43462753\n",
      "Iteration 280, loss = 19651481635.21263504\n",
      "Iteration 281, loss = 19651477527.26146317\n",
      "Iteration 282, loss = 19651473467.26905823\n",
      "Iteration 283, loss = 19651469346.58514404\n",
      "Iteration 284, loss = 19651465243.51960373\n",
      "Iteration 285, loss = 19651461151.31084442\n",
      "Iteration 286, loss = 19651457060.43582153\n",
      "Iteration 287, loss = 19651452991.01047516\n",
      "Iteration 288, loss = 19651448989.42728424\n",
      "Iteration 289, loss = 19651444953.84759140\n",
      "Iteration 290, loss = 19651440927.71639252\n",
      "Iteration 291, loss = 19651436930.02957535\n",
      "Iteration 292, loss = 19651432934.86016846\n",
      "Iteration 293, loss = 19651428962.67292786\n",
      "Iteration 294, loss = 19651425017.90316391\n",
      "Iteration 295, loss = 19651421055.46209335\n",
      "Iteration 296, loss = 19651417101.94120789\n",
      "Iteration 297, loss = 19651413172.38832474\n",
      "Iteration 298, loss = 19651409261.44152832\n",
      "Iteration 299, loss = 19651405339.08266830\n",
      "Iteration 300, loss = 19651401424.32796478\n",
      "Iteration 301, loss = 19651397557.54505539\n",
      "Iteration 302, loss = 19651393663.44601822\n",
      "Iteration 303, loss = 19651389734.65698624\n",
      "Iteration 304, loss = 19651385886.95294571\n",
      "Iteration 305, loss = 19651382027.05043411\n",
      "Iteration 306, loss = 19651378127.35592270\n",
      "Iteration 307, loss = 19651374286.90050888\n",
      "Iteration 308, loss = 19651370419.13659286\n",
      "Iteration 309, loss = 19651366532.24703217\n",
      "Iteration 310, loss = 19651362762.36000061\n",
      "Iteration 311, loss = 19651358876.87619019\n",
      "Iteration 312, loss = 19651355002.81313705\n",
      "Iteration 313, loss = 19651351170.72285843\n",
      "Iteration 314, loss = 19651347307.46200562\n",
      "Iteration 315, loss = 19651343507.85465240\n",
      "Iteration 316, loss = 19651339634.93000793\n",
      "Iteration 317, loss = 19651335777.93909454\n",
      "Iteration 318, loss = 19651331911.96426010\n",
      "Iteration 319, loss = 19651328080.57365417\n",
      "Iteration 320, loss = 19651324180.49607849\n",
      "Iteration 321, loss = 19651320311.80586624\n",
      "Iteration 322, loss = 19651316417.76306915\n",
      "Iteration 323, loss = 19651312461.78085709\n",
      "Iteration 324, loss = 19651308498.45792007\n",
      "Iteration 325, loss = 19651304507.39196014\n",
      "Iteration 326, loss = 19651300468.12659454\n",
      "Iteration 327, loss = 19651296374.89117813\n",
      "Iteration 328, loss = 19651292220.51942825\n",
      "Iteration 329, loss = 19651288040.61625290\n",
      "Iteration 330, loss = 19651283765.95824051\n",
      "Iteration 331, loss = 19651279486.94384766\n",
      "Iteration 332, loss = 19651275105.97187424\n",
      "Iteration 333, loss = 19651270741.05873108\n",
      "Iteration 334, loss = 19651266407.03297043\n",
      "Iteration 335, loss = 19651262098.89649200\n",
      "Iteration 336, loss = 19651257762.03381348\n",
      "Iteration 337, loss = 19651253442.34444427\n",
      "Iteration 338, loss = 19651249156.84967804\n",
      "Iteration 339, loss = 19651244893.50131226\n",
      "Iteration 340, loss = 19651240637.70706177\n",
      "Iteration 341, loss = 19651236408.74795914\n",
      "Iteration 342, loss = 19651232207.92285156\n",
      "Iteration 343, loss = 19651227957.37516403\n",
      "Iteration 344, loss = 19651223805.69368744\n",
      "Iteration 345, loss = 19651219686.51856613\n",
      "Iteration 346, loss = 19651215549.54453278\n",
      "Iteration 347, loss = 19651211360.74872971\n",
      "Iteration 348, loss = 19651207240.38084030\n",
      "Iteration 349, loss = 19651203190.47419357\n",
      "Iteration 350, loss = 19651199071.45013046\n",
      "Iteration 351, loss = 19651194930.40062714\n",
      "Iteration 352, loss = 19651190870.74979019\n",
      "Iteration 353, loss = 19651186791.03984451\n",
      "Iteration 354, loss = 19651182763.13484573\n",
      "Iteration 355, loss = 19651178698.54639435\n",
      "Iteration 356, loss = 19651174653.42989349\n",
      "Iteration 357, loss = 19651170605.15879059\n",
      "Iteration 358, loss = 19651166615.11360168\n",
      "Iteration 359, loss = 19651162567.16939163\n",
      "Iteration 360, loss = 19651158552.47392273\n",
      "Iteration 361, loss = 19651154547.57658768\n",
      "Iteration 362, loss = 19651150552.32147980\n",
      "Iteration 363, loss = 19651146544.50450897\n",
      "Iteration 364, loss = 19651142561.19182205\n",
      "Iteration 365, loss = 19651138553.01527786\n",
      "Iteration 366, loss = 19651134577.44812393\n",
      "Iteration 367, loss = 19651130616.75764465\n",
      "Iteration 368, loss = 19651126604.75284576\n",
      "Iteration 369, loss = 19651122669.07537460\n",
      "Iteration 370, loss = 19651118690.91753387\n",
      "Iteration 371, loss = 19651114745.13574219\n",
      "Iteration 372, loss = 19651110807.80855560\n",
      "Iteration 373, loss = 19651106864.72711945\n",
      "Iteration 374, loss = 19651102916.41026306\n",
      "Iteration 375, loss = 19651098969.92761230\n",
      "Iteration 376, loss = 19651095026.76679611\n",
      "Iteration 377, loss = 19651091088.72766113\n",
      "Iteration 378, loss = 19651087149.14542007\n",
      "Iteration 379, loss = 19651083224.41117477\n",
      "Iteration 380, loss = 19651079314.07692719\n",
      "Iteration 381, loss = 19651075399.42945099\n",
      "Iteration 382, loss = 19651071454.56404114\n",
      "Iteration 383, loss = 19651067544.44869995\n",
      "Iteration 384, loss = 19651063651.61010742\n",
      "Iteration 385, loss = 19651059796.59020615\n",
      "Iteration 386, loss = 19651055826.79322433\n",
      "Iteration 387, loss = 19651051937.04285431\n",
      "Iteration 388, loss = 19651048022.44984818\n",
      "Iteration 389, loss = 19651044119.27515411\n",
      "Iteration 390, loss = 19651040247.45190430\n",
      "Iteration 391, loss = 19651036368.57153702\n",
      "Iteration 392, loss = 19651032465.52133179\n",
      "Iteration 393, loss = 19651028607.94709015\n",
      "Iteration 394, loss = 19651024692.07211304\n",
      "Iteration 395, loss = 19651020839.49670792\n",
      "Iteration 396, loss = 19651016948.17531204\n",
      "Iteration 397, loss = 19651013072.52556992\n",
      "Iteration 398, loss = 19651009190.17468643\n",
      "Iteration 399, loss = 19651005341.62862778\n",
      "Iteration 400, loss = 19651001447.69342422\n",
      "Iteration 401, loss = 19650997612.40065384\n",
      "Iteration 402, loss = 19650993708.29478073\n",
      "Iteration 403, loss = 19650989860.73859024\n",
      "Iteration 404, loss = 19650986001.85649109\n",
      "Iteration 405, loss = 19650982148.37287140\n",
      "Iteration 406, loss = 19650978284.34363174\n",
      "Iteration 407, loss = 19650974440.84335327\n",
      "Iteration 408, loss = 19650970564.63123322\n",
      "Iteration 409, loss = 19650966731.88803482\n",
      "Iteration 410, loss = 19650962862.20602036\n",
      "Iteration 411, loss = 19650959072.04498291\n",
      "Iteration 412, loss = 19650955157.09430313\n",
      "Iteration 413, loss = 19650951378.85323334\n",
      "Iteration 414, loss = 19650947523.25941467\n",
      "Iteration 415, loss = 19650943668.92854309\n",
      "Iteration 416, loss = 19650939835.38933563\n",
      "Iteration 417, loss = 19650936006.94251633\n",
      "Iteration 418, loss = 19650932192.26772308\n",
      "Iteration 419, loss = 19650928322.90896225\n",
      "Iteration 420, loss = 19650924501.72621536\n",
      "Iteration 421, loss = 19650920689.37704849\n",
      "Iteration 422, loss = 19650916841.26126480\n",
      "Iteration 423, loss = 19650913029.70359802\n",
      "Iteration 424, loss = 19650909193.44682312\n",
      "Iteration 425, loss = 19650905394.95976257\n",
      "Iteration 426, loss = 19650901585.64838791\n",
      "Iteration 427, loss = 19650897729.21103287\n",
      "Iteration 428, loss = 19650893933.90482330\n",
      "Iteration 429, loss = 19650890112.24992752\n",
      "Iteration 430, loss = 19650886331.58877563\n",
      "Iteration 431, loss = 19650882486.55224609\n",
      "Iteration 432, loss = 19650878665.92400360\n",
      "Iteration 433, loss = 19650874858.37415314\n",
      "Iteration 434, loss = 19650871044.81266403\n",
      "Iteration 435, loss = 19650867242.48524475\n",
      "Iteration 436, loss = 19650863459.16445160\n",
      "Iteration 437, loss = 19650859653.99964905\n",
      "Iteration 438, loss = 19650855822.07884598\n",
      "Iteration 439, loss = 19650852043.17773438\n",
      "Iteration 440, loss = 19650848243.03879166\n",
      "Iteration 441, loss = 19650844457.03533936\n",
      "Iteration 442, loss = 19650840607.60132217\n",
      "Iteration 443, loss = 19650836848.14598465\n",
      "Iteration 444, loss = 19650833057.05823517\n",
      "Iteration 445, loss = 19650829241.08821106\n",
      "Iteration 446, loss = 19650825462.85325623\n",
      "Iteration 447, loss = 19650821655.39354706\n",
      "Iteration 448, loss = 19650817864.89628601\n",
      "Iteration 449, loss = 19650814099.94706345\n",
      "Iteration 450, loss = 19650810309.35463333\n",
      "Iteration 451, loss = 19650806497.12708282\n",
      "Iteration 452, loss = 19650802692.27405930\n",
      "Iteration 453, loss = 19650798944.85740662\n",
      "Iteration 454, loss = 19650795125.85578156\n",
      "Iteration 455, loss = 19650791337.80113220\n",
      "Iteration 456, loss = 19650787585.49874878\n",
      "Iteration 457, loss = 19650783785.58971405\n",
      "Iteration 458, loss = 19650780038.75737381\n",
      "Iteration 459, loss = 19650776243.97446823\n",
      "Iteration 460, loss = 19650772440.15485382\n",
      "Iteration 461, loss = 19650768655.17911148\n",
      "Iteration 462, loss = 19650764907.35206604\n",
      "Iteration 463, loss = 19650761090.59456635\n",
      "Iteration 464, loss = 19650757341.10561752\n",
      "Iteration 465, loss = 19650753570.56876373\n",
      "Iteration 466, loss = 19650749814.57145691\n",
      "Iteration 467, loss = 19650746002.68056488\n",
      "Iteration 468, loss = 19650742263.16348267\n",
      "Iteration 469, loss = 19650738489.92807770\n",
      "Iteration 470, loss = 19650734687.54884720\n",
      "Iteration 471, loss = 19650730931.69825745\n",
      "Iteration 472, loss = 19650727165.24540329\n",
      "Iteration 473, loss = 19650723399.03087616\n",
      "Iteration 474, loss = 19650719656.37477112\n",
      "Iteration 475, loss = 19650715889.57296753\n",
      "Iteration 476, loss = 19650712141.48954773\n",
      "Iteration 477, loss = 19650708359.40042496\n",
      "Iteration 478, loss = 19650704536.58413315\n",
      "Iteration 479, loss = 19650700803.70988846\n",
      "Iteration 480, loss = 19650697053.02598572\n",
      "Iteration 481, loss = 19650693335.58095932\n",
      "Iteration 482, loss = 19650689536.06384277\n",
      "Iteration 483, loss = 19650685782.68339157\n",
      "Iteration 484, loss = 19650682041.56734085\n",
      "Iteration 485, loss = 19650678273.08953857\n",
      "Iteration 486, loss = 19650674511.30860901\n",
      "Iteration 487, loss = 19650670765.04092026\n",
      "Iteration 488, loss = 19650667034.60505295\n",
      "Iteration 489, loss = 19650663240.46612930\n",
      "Iteration 490, loss = 19650659518.61423492\n",
      "Iteration 491, loss = 19650655762.57346344\n",
      "Iteration 492, loss = 19650652032.78836060\n",
      "Iteration 493, loss = 19650648240.45523071\n",
      "Iteration 494, loss = 19650644512.03034210\n",
      "Iteration 495, loss = 19650640764.83424759\n",
      "Iteration 496, loss = 19650637022.88898087\n",
      "Iteration 497, loss = 19650633278.25416946\n",
      "Iteration 498, loss = 19650629511.68576813\n",
      "Iteration 499, loss = 19650625778.51046753\n",
      "Iteration 500, loss = 19650622026.21952438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 20145346716.81595993\n",
      "Iteration 2, loss = 20145333488.13657761\n",
      "Iteration 3, loss = 20145320149.48078156\n",
      "Iteration 4, loss = 20145307098.33600616\n",
      "Iteration 5, loss = 20145293937.28061295\n",
      "Iteration 6, loss = 20145280879.47043991\n",
      "Iteration 7, loss = 20145267770.33609772\n",
      "Iteration 8, loss = 20145254326.35622787\n",
      "Iteration 9, loss = 20145240594.48646164\n",
      "Iteration 10, loss = 20145226458.38174820\n",
      "Iteration 11, loss = 20145211940.99954987\n",
      "Iteration 12, loss = 20145196764.58421707\n",
      "Iteration 13, loss = 20145180998.43826294\n",
      "Iteration 14, loss = 20145164900.60809708\n",
      "Iteration 15, loss = 20145148523.16245651\n",
      "Iteration 16, loss = 20145132202.29248047\n",
      "Iteration 17, loss = 20145115686.17764664\n",
      "Iteration 18, loss = 20145099790.88857651\n",
      "Iteration 19, loss = 20145083996.97134018\n",
      "Iteration 20, loss = 20145068432.55460358\n",
      "Iteration 21, loss = 20145053369.87159729\n",
      "Iteration 22, loss = 20145038899.61109543\n",
      "Iteration 23, loss = 20145024856.76203156\n",
      "Iteration 24, loss = 20145011396.69257355\n",
      "Iteration 25, loss = 20144998591.63256073\n",
      "Iteration 26, loss = 20144986129.59740829\n",
      "Iteration 27, loss = 20144973807.92708206\n",
      "Iteration 28, loss = 20144961469.59181213\n",
      "Iteration 29, loss = 20144949181.60089874\n",
      "Iteration 30, loss = 20144936979.89435959\n",
      "Iteration 31, loss = 20144925253.80767441\n",
      "Iteration 32, loss = 20144913755.88856506\n",
      "Iteration 33, loss = 20144902226.89854050\n",
      "Iteration 34, loss = 20144890364.60365677\n",
      "Iteration 35, loss = 20144878418.27570343\n",
      "Iteration 36, loss = 20144866603.82421112\n",
      "Iteration 37, loss = 20144855189.14233398\n",
      "Iteration 38, loss = 20144844703.73190308\n",
      "Iteration 39, loss = 20144834572.07471848\n",
      "Iteration 40, loss = 20144825023.00726700\n",
      "Iteration 41, loss = 20144815474.75278091\n",
      "Iteration 42, loss = 20144805850.60117340\n",
      "Iteration 43, loss = 20144796258.25353622\n",
      "Iteration 44, loss = 20144787068.88000488\n",
      "Iteration 45, loss = 20144778516.78264618\n",
      "Iteration 46, loss = 20144770515.35931396\n",
      "Iteration 47, loss = 20144762819.07271194\n",
      "Iteration 48, loss = 20144755188.69210815\n",
      "Iteration 49, loss = 20144747556.03958511\n",
      "Iteration 50, loss = 20144740303.48129654\n",
      "Iteration 51, loss = 20144733323.08979416\n",
      "Iteration 52, loss = 20144726442.48193741\n",
      "Iteration 53, loss = 20144719713.84506989\n",
      "Iteration 54, loss = 20144713060.69261551\n",
      "Iteration 55, loss = 20144706584.99865341\n",
      "Iteration 56, loss = 20144700215.58704376\n",
      "Iteration 57, loss = 20144693890.68186951\n",
      "Iteration 58, loss = 20144687670.28008270\n",
      "Iteration 59, loss = 20144681544.01821518\n",
      "Iteration 60, loss = 20144675375.02520752\n",
      "Iteration 61, loss = 20144669313.08867645\n",
      "Iteration 62, loss = 20144663298.86161804\n",
      "Iteration 63, loss = 20144657316.71046066\n",
      "Iteration 64, loss = 20144651427.09446716\n",
      "Iteration 65, loss = 20144645548.19955826\n",
      "Iteration 66, loss = 20144639743.11210251\n",
      "Iteration 67, loss = 20144633896.31772232\n",
      "Iteration 68, loss = 20144628184.84201050\n",
      "Iteration 69, loss = 20144622489.81664658\n",
      "Iteration 70, loss = 20144616754.84909058\n",
      "Iteration 71, loss = 20144611140.79352188\n",
      "Iteration 72, loss = 20144605529.66153717\n",
      "Iteration 73, loss = 20144599961.98456573\n",
      "Iteration 74, loss = 20144594370.02770996\n",
      "Iteration 75, loss = 20144588838.80983353\n",
      "Iteration 76, loss = 20144583321.81466675\n",
      "Iteration 77, loss = 20144577812.57785034\n",
      "Iteration 78, loss = 20144572368.12453079\n",
      "Iteration 79, loss = 20144566850.14086533\n",
      "Iteration 80, loss = 20144561447.38103104\n",
      "Iteration 81, loss = 20144555937.87629318\n",
      "Iteration 82, loss = 20144550499.29185867\n",
      "Iteration 83, loss = 20144545030.54050064\n",
      "Iteration 84, loss = 20144539701.27208710\n",
      "Iteration 85, loss = 20144534302.27473068\n",
      "Iteration 86, loss = 20144528916.99586487\n",
      "Iteration 87, loss = 20144523595.67326355\n",
      "Iteration 88, loss = 20144518253.34819794\n",
      "Iteration 89, loss = 20144512889.95399094\n",
      "Iteration 90, loss = 20144507640.72739029\n",
      "Iteration 91, loss = 20144502397.54762650\n",
      "Iteration 92, loss = 20144497106.55470276\n",
      "Iteration 93, loss = 20144491862.23209381\n",
      "Iteration 94, loss = 20144486587.42397690\n",
      "Iteration 95, loss = 20144481441.56407928\n",
      "Iteration 96, loss = 20144476213.15993500\n",
      "Iteration 97, loss = 20144471110.31223679\n",
      "Iteration 98, loss = 20144465892.33238983\n",
      "Iteration 99, loss = 20144460751.36505890\n",
      "Iteration 100, loss = 20144455583.10604477\n",
      "Iteration 101, loss = 20144450421.78384781\n",
      "Iteration 102, loss = 20144445250.36019135\n",
      "Iteration 103, loss = 20144440100.00724792\n",
      "Iteration 104, loss = 20144434958.86488724\n",
      "Iteration 105, loss = 20144429858.97762680\n",
      "Iteration 106, loss = 20144424773.65058517\n",
      "Iteration 107, loss = 20144419674.93020630\n",
      "Iteration 108, loss = 20144414630.66159821\n",
      "Iteration 109, loss = 20144409547.07997131\n",
      "Iteration 110, loss = 20144404529.82015991\n",
      "Iteration 111, loss = 20144399451.42895126\n",
      "Iteration 112, loss = 20144394423.65255356\n",
      "Iteration 113, loss = 20144389389.81187057\n",
      "Iteration 114, loss = 20144384403.44266891\n",
      "Iteration 115, loss = 20144379369.22165298\n",
      "Iteration 116, loss = 20144374338.32507706\n",
      "Iteration 117, loss = 20144369352.08266830\n",
      "Iteration 118, loss = 20144364319.68847275\n",
      "Iteration 119, loss = 20144359338.96916199\n",
      "Iteration 120, loss = 20144354296.33578873\n",
      "Iteration 121, loss = 20144349302.10981369\n",
      "Iteration 122, loss = 20144344371.02724838\n",
      "Iteration 123, loss = 20144339386.91776276\n",
      "Iteration 124, loss = 20144334408.75196075\n",
      "Iteration 125, loss = 20144329486.17477798\n",
      "Iteration 126, loss = 20144324550.49533844\n",
      "Iteration 127, loss = 20144319578.07003784\n",
      "Iteration 128, loss = 20144314661.33169556\n",
      "Iteration 129, loss = 20144309723.65929031\n",
      "Iteration 130, loss = 20144304801.16801071\n",
      "Iteration 131, loss = 20144299883.84647369\n",
      "Iteration 132, loss = 20144294954.85176468\n",
      "Iteration 133, loss = 20144290015.07688522\n",
      "Iteration 134, loss = 20144285150.66635513\n",
      "Iteration 135, loss = 20144280220.28047562\n",
      "Iteration 136, loss = 20144275317.72006226\n",
      "Iteration 137, loss = 20144270391.86585236\n",
      "Iteration 138, loss = 20144265518.25025177\n",
      "Iteration 139, loss = 20144260588.48496246\n",
      "Iteration 140, loss = 20144255739.03723907\n",
      "Iteration 141, loss = 20144250837.61140060\n",
      "Iteration 142, loss = 20144245924.10441589\n",
      "Iteration 143, loss = 20144241052.36891174\n",
      "Iteration 144, loss = 20144236177.52563477\n",
      "Iteration 145, loss = 20144231308.57566071\n",
      "Iteration 146, loss = 20144226426.95412445\n",
      "Iteration 147, loss = 20144221548.75371552\n",
      "Iteration 148, loss = 20144216733.23088074\n",
      "Iteration 149, loss = 20144211845.05649185\n",
      "Iteration 150, loss = 20144207034.84115982\n",
      "Iteration 151, loss = 20144202135.96566010\n",
      "Iteration 152, loss = 20144197357.43875122\n",
      "Iteration 153, loss = 20144192477.59664917\n",
      "Iteration 154, loss = 20144187629.79839325\n",
      "Iteration 155, loss = 20144182799.37707138\n",
      "Iteration 156, loss = 20144177978.23221970\n",
      "Iteration 157, loss = 20144173177.02568054\n",
      "Iteration 158, loss = 20144168325.59186554\n",
      "Iteration 159, loss = 20144163527.99372101\n",
      "Iteration 160, loss = 20144158768.98093033\n",
      "Iteration 161, loss = 20144153878.66559219\n",
      "Iteration 162, loss = 20144149051.04683685\n",
      "Iteration 163, loss = 20144144279.42076492\n",
      "Iteration 164, loss = 20144139435.98752213\n",
      "Iteration 165, loss = 20144134606.03860092\n",
      "Iteration 166, loss = 20144129819.63230896\n",
      "Iteration 167, loss = 20144125013.42755127\n",
      "Iteration 168, loss = 20144120215.59999084\n",
      "Iteration 169, loss = 20144115426.37907791\n",
      "Iteration 170, loss = 20144110610.32823563\n",
      "Iteration 171, loss = 20144105879.12201309\n",
      "Iteration 172, loss = 20144101067.82844162\n",
      "Iteration 173, loss = 20144096282.71997452\n",
      "Iteration 174, loss = 20144091511.24567413\n",
      "Iteration 175, loss = 20144086717.99119186\n",
      "Iteration 176, loss = 20144082011.18740463\n",
      "Iteration 177, loss = 20144077168.47113037\n",
      "Iteration 178, loss = 20144072411.49892426\n",
      "Iteration 179, loss = 20144067645.94206619\n",
      "Iteration 180, loss = 20144062851.02413940\n",
      "Iteration 181, loss = 20144058120.18027115\n",
      "Iteration 182, loss = 20144053313.10313416\n",
      "Iteration 183, loss = 20144048559.05160904\n",
      "Iteration 184, loss = 20144043784.79315948\n",
      "Iteration 185, loss = 20144039028.43676758\n",
      "Iteration 186, loss = 20144034278.25098038\n",
      "Iteration 187, loss = 20144029485.98298645\n",
      "Iteration 188, loss = 20144024746.20628738\n",
      "Iteration 189, loss = 20144019968.76181793\n",
      "Iteration 190, loss = 20144015229.83016968\n",
      "Iteration 191, loss = 20144010484.82718277\n",
      "Iteration 192, loss = 20144005715.48299026\n",
      "Iteration 193, loss = 20144000977.38880157\n",
      "Iteration 194, loss = 20143996225.18701553\n",
      "Iteration 195, loss = 20143991483.65002823\n",
      "Iteration 196, loss = 20143986723.44502640\n",
      "Iteration 197, loss = 20143982035.23514557\n",
      "Iteration 198, loss = 20143977243.62351990\n",
      "Iteration 199, loss = 20143972516.45661545\n",
      "Iteration 200, loss = 20143967766.82785416\n",
      "Iteration 201, loss = 20143963065.66035843\n",
      "Iteration 202, loss = 20143958301.66030502\n",
      "Iteration 203, loss = 20143953595.49679184\n",
      "Iteration 204, loss = 20143948838.33660507\n",
      "Iteration 205, loss = 20143944110.98106766\n",
      "Iteration 206, loss = 20143939398.03527832\n",
      "Iteration 207, loss = 20143934670.89993286\n",
      "Iteration 208, loss = 20143929948.45254898\n",
      "Iteration 209, loss = 20143925177.31573105\n",
      "Iteration 210, loss = 20143920483.20965576\n",
      "Iteration 211, loss = 20143915712.63232040\n",
      "Iteration 212, loss = 20143910958.64957047\n",
      "Iteration 213, loss = 20143906225.32279205\n",
      "Iteration 214, loss = 20143901532.56910324\n",
      "Iteration 215, loss = 20143896790.58682632\n",
      "Iteration 216, loss = 20143892043.05421448\n",
      "Iteration 217, loss = 20143887374.45466995\n",
      "Iteration 218, loss = 20143882638.07529449\n",
      "Iteration 219, loss = 20143877938.85070419\n",
      "Iteration 220, loss = 20143873238.24607849\n",
      "Iteration 221, loss = 20143868540.04509354\n",
      "Iteration 222, loss = 20143863836.37783432\n",
      "Iteration 223, loss = 20143859139.51559830\n",
      "Iteration 224, loss = 20143854436.66812897\n",
      "Iteration 225, loss = 20143849758.24074173\n",
      "Iteration 226, loss = 20143845005.42580032\n",
      "Iteration 227, loss = 20143840347.97906113\n",
      "Iteration 228, loss = 20143835618.31645203\n",
      "Iteration 229, loss = 20143830892.93351746\n",
      "Iteration 230, loss = 20143826247.45085526\n",
      "Iteration 231, loss = 20143821497.56366348\n",
      "Iteration 232, loss = 20143816777.21009064\n",
      "Iteration 233, loss = 20143812097.36899185\n",
      "Iteration 234, loss = 20143807419.11917496\n",
      "Iteration 235, loss = 20143802700.79478836\n",
      "Iteration 236, loss = 20143798033.27187347\n",
      "Iteration 237, loss = 20143793350.00348663\n",
      "Iteration 238, loss = 20143788644.60881805\n",
      "Iteration 239, loss = 20143783981.48828506\n",
      "Iteration 240, loss = 20143779244.62391281\n",
      "Iteration 241, loss = 20143774566.23124313\n",
      "Iteration 242, loss = 20143769883.18367004\n",
      "Iteration 243, loss = 20143765212.21557999\n",
      "Iteration 244, loss = 20143760524.13013458\n",
      "Iteration 245, loss = 20143755829.72275543\n",
      "Iteration 246, loss = 20143751140.54348755\n",
      "Iteration 247, loss = 20143746465.31405258\n",
      "Iteration 248, loss = 20143741808.12854385\n",
      "Iteration 249, loss = 20143737139.04478073\n",
      "Iteration 250, loss = 20143732448.44675446\n",
      "Iteration 251, loss = 20143727779.95685959\n",
      "Iteration 252, loss = 20143723069.67110062\n",
      "Iteration 253, loss = 20143718430.78250122\n",
      "Iteration 254, loss = 20143713753.37976456\n",
      "Iteration 255, loss = 20143709030.38286972\n",
      "Iteration 256, loss = 20143704370.10515213\n",
      "Iteration 257, loss = 20143699705.99617004\n",
      "Iteration 258, loss = 20143695045.38261795\n",
      "Iteration 259, loss = 20143690311.62084961\n",
      "Iteration 260, loss = 20143685678.33411407\n",
      "Iteration 261, loss = 20143680978.47954559\n",
      "Iteration 262, loss = 20143676327.74748611\n",
      "Iteration 263, loss = 20143671651.26159668\n",
      "Iteration 264, loss = 20143666982.90220261\n",
      "Iteration 265, loss = 20143662292.77310562\n",
      "Iteration 266, loss = 20143657692.61271667\n",
      "Iteration 267, loss = 20143653007.17836380\n",
      "Iteration 268, loss = 20143648351.01805496\n",
      "Iteration 269, loss = 20143643636.94651031\n",
      "Iteration 270, loss = 20143639001.84937668\n",
      "Iteration 271, loss = 20143634324.39671707\n",
      "Iteration 272, loss = 20143629690.11503601\n",
      "Iteration 273, loss = 20143625009.38709641\n",
      "Iteration 274, loss = 20143620351.29494858\n",
      "Iteration 275, loss = 20143615707.76625061\n",
      "Iteration 276, loss = 20143611074.39209366\n",
      "Iteration 277, loss = 20143606395.32284546\n",
      "Iteration 278, loss = 20143601805.74752045\n",
      "Iteration 279, loss = 20143597130.93835831\n",
      "Iteration 280, loss = 20143592484.52129745\n",
      "Iteration 281, loss = 20143587842.28783035\n",
      "Iteration 282, loss = 20143583194.84850693\n",
      "Iteration 283, loss = 20143578552.10680008\n",
      "Iteration 284, loss = 20143573910.41734314\n",
      "Iteration 285, loss = 20143569256.09536362\n",
      "Iteration 286, loss = 20143564625.81026077\n",
      "Iteration 287, loss = 20143560012.58757401\n",
      "Iteration 288, loss = 20143555366.70829773\n",
      "Iteration 289, loss = 20143550721.11659622\n",
      "Iteration 290, loss = 20143546093.77260971\n",
      "Iteration 291, loss = 20143541455.78522873\n",
      "Iteration 292, loss = 20143536821.57041931\n",
      "Iteration 293, loss = 20143532192.68821335\n",
      "Iteration 294, loss = 20143527517.03346634\n",
      "Iteration 295, loss = 20143522899.43650055\n",
      "Iteration 296, loss = 20143518228.79939270\n",
      "Iteration 297, loss = 20143513608.07229996\n",
      "Iteration 298, loss = 20143508958.47141647\n",
      "Iteration 299, loss = 20143504320.99211884\n",
      "Iteration 300, loss = 20143499678.91869354\n",
      "Iteration 301, loss = 20143495036.00266266\n",
      "Iteration 302, loss = 20143490412.73246765\n",
      "Iteration 303, loss = 20143485760.29487228\n",
      "Iteration 304, loss = 20143481115.92889023\n",
      "Iteration 305, loss = 20143476432.13262177\n",
      "Iteration 306, loss = 20143471814.29288101\n",
      "Iteration 307, loss = 20143467159.45098114\n",
      "Iteration 308, loss = 20143462533.00334167\n",
      "Iteration 309, loss = 20143457872.53017044\n",
      "Iteration 310, loss = 20143453213.29857254\n",
      "Iteration 311, loss = 20143448622.35102463\n",
      "Iteration 312, loss = 20143443962.49265671\n",
      "Iteration 313, loss = 20143439348.47914505\n",
      "Iteration 314, loss = 20143434703.47951889\n",
      "Iteration 315, loss = 20143430039.86428452\n",
      "Iteration 316, loss = 20143425406.05028534\n",
      "Iteration 317, loss = 20143420789.35045242\n",
      "Iteration 318, loss = 20143416131.03800201\n",
      "Iteration 319, loss = 20143411499.60118866\n",
      "Iteration 320, loss = 20143406869.85437393\n",
      "Iteration 321, loss = 20143402180.28979874\n",
      "Iteration 322, loss = 20143397592.40090561\n",
      "Iteration 323, loss = 20143392921.41873932\n",
      "Iteration 324, loss = 20143388309.21702194\n",
      "Iteration 325, loss = 20143383672.79886627\n",
      "Iteration 326, loss = 20143379034.56629944\n",
      "Iteration 327, loss = 20143374480.47243881\n",
      "Iteration 328, loss = 20143369781.64729691\n",
      "Iteration 329, loss = 20143365153.27489090\n",
      "Iteration 330, loss = 20143360533.43056870\n",
      "Iteration 331, loss = 20143355919.51010895\n",
      "Iteration 332, loss = 20143351303.25007629\n",
      "Iteration 333, loss = 20143346651.33723450\n",
      "Iteration 334, loss = 20143342052.70584869\n",
      "Iteration 335, loss = 20143337431.99776840\n",
      "Iteration 336, loss = 20143332837.15253830\n",
      "Iteration 337, loss = 20143328237.61790848\n",
      "Iteration 338, loss = 20143323589.01044464\n",
      "Iteration 339, loss = 20143318986.18434143\n",
      "Iteration 340, loss = 20143314394.24247360\n",
      "Iteration 341, loss = 20143309751.51219940\n",
      "Iteration 342, loss = 20143305156.54521179\n",
      "Iteration 343, loss = 20143300513.85102844\n",
      "Iteration 344, loss = 20143295931.43013000\n",
      "Iteration 345, loss = 20143291298.69103622\n",
      "Iteration 346, loss = 20143286664.24050140\n",
      "Iteration 347, loss = 20143282011.47310638\n",
      "Iteration 348, loss = 20143277446.49972534\n",
      "Iteration 349, loss = 20143272781.71157455\n",
      "Iteration 350, loss = 20143268176.06479263\n",
      "Iteration 351, loss = 20143263511.32818222\n",
      "Iteration 352, loss = 20143258929.62075424\n",
      "Iteration 353, loss = 20143254263.26761627\n",
      "Iteration 354, loss = 20143249634.50034714\n",
      "Iteration 355, loss = 20143245007.80801773\n",
      "Iteration 356, loss = 20143240403.09933472\n",
      "Iteration 357, loss = 20143235757.22174072\n",
      "Iteration 358, loss = 20143231134.77989197\n",
      "Iteration 359, loss = 20143226456.88632965\n",
      "Iteration 360, loss = 20143221790.72973633\n",
      "Iteration 361, loss = 20143217140.71647263\n",
      "Iteration 362, loss = 20143212437.80316925\n",
      "Iteration 363, loss = 20143207743.89185715\n",
      "Iteration 364, loss = 20143202978.74510193\n",
      "Iteration 365, loss = 20143198200.98514938\n",
      "Iteration 366, loss = 20143193394.15846252\n",
      "Iteration 367, loss = 20143188438.68230820\n",
      "Iteration 368, loss = 20143183404.68131256\n",
      "Iteration 369, loss = 20143178287.31952667\n",
      "Iteration 370, loss = 20143173023.89356232\n",
      "Iteration 371, loss = 20143167677.10120010\n",
      "Iteration 372, loss = 20143162257.19642258\n",
      "Iteration 373, loss = 20143156800.78575516\n",
      "Iteration 374, loss = 20143151316.78866959\n",
      "Iteration 375, loss = 20143145842.27319717\n",
      "Iteration 376, loss = 20143140434.51784134\n",
      "Iteration 377, loss = 20143135032.09554672\n",
      "Iteration 378, loss = 20143129731.97628403\n",
      "Iteration 379, loss = 20143124338.14661407\n",
      "Iteration 380, loss = 20143119097.03509521\n",
      "Iteration 381, loss = 20143113835.65508270\n",
      "Iteration 382, loss = 20143108622.00437927\n",
      "Iteration 383, loss = 20143103409.69209290\n",
      "Iteration 384, loss = 20143098212.83568192\n",
      "Iteration 385, loss = 20143093071.36000061\n",
      "Iteration 386, loss = 20143087874.13548660\n",
      "Iteration 387, loss = 20143082754.72074890\n",
      "Iteration 388, loss = 20143077633.51873779\n",
      "Iteration 389, loss = 20143072488.83795929\n",
      "Iteration 390, loss = 20143067400.07190704\n",
      "Iteration 391, loss = 20143062346.53244400\n",
      "Iteration 392, loss = 20143057230.56515503\n",
      "Iteration 393, loss = 20143052176.88827515\n",
      "Iteration 394, loss = 20143047123.28821182\n",
      "Iteration 395, loss = 20143042091.25141907\n",
      "Iteration 396, loss = 20143037016.86310196\n",
      "Iteration 397, loss = 20143032020.60297394\n",
      "Iteration 398, loss = 20143026971.99462509\n",
      "Iteration 399, loss = 20143021920.23042679\n",
      "Iteration 400, loss = 20143016974.37451172\n",
      "Iteration 401, loss = 20143011974.46784210\n",
      "Iteration 402, loss = 20143006981.58807755\n",
      "Iteration 403, loss = 20143001962.87878418\n",
      "Iteration 404, loss = 20142996998.70425034\n",
      "Iteration 405, loss = 20142992001.79467010\n",
      "Iteration 406, loss = 20142987035.69885635\n",
      "Iteration 407, loss = 20142982045.60728073\n",
      "Iteration 408, loss = 20142977078.02102661\n",
      "Iteration 409, loss = 20142972149.26761246\n",
      "Iteration 410, loss = 20142967182.18690109\n",
      "Iteration 411, loss = 20142962217.45752716\n",
      "Iteration 412, loss = 20142957266.85887146\n",
      "Iteration 413, loss = 20142952326.69838333\n",
      "Iteration 414, loss = 20142947358.30572510\n",
      "Iteration 415, loss = 20142942482.19192123\n",
      "Iteration 416, loss = 20142937519.10092926\n",
      "Iteration 417, loss = 20142932550.34170914\n",
      "Iteration 418, loss = 20142927629.62422180\n",
      "Iteration 419, loss = 20142922693.01715851\n",
      "Iteration 420, loss = 20142917778.68495178\n",
      "Iteration 421, loss = 20142912856.83649826\n",
      "Iteration 422, loss = 20142907915.92340851\n",
      "Iteration 423, loss = 20142903032.67079926\n",
      "Iteration 424, loss = 20142898086.39561844\n",
      "Iteration 425, loss = 20142893141.57633591\n",
      "Iteration 426, loss = 20142888229.52240753\n",
      "Iteration 427, loss = 20142883292.41887283\n",
      "Iteration 428, loss = 20142878345.55823517\n",
      "Iteration 429, loss = 20142873377.53619766\n",
      "Iteration 430, loss = 20142868337.49255753\n",
      "Iteration 431, loss = 20142863221.62637329\n",
      "Iteration 432, loss = 20142858033.23809814\n",
      "Iteration 433, loss = 20142852685.08607101\n",
      "Iteration 434, loss = 20142847022.79326248\n",
      "Iteration 435, loss = 20142841066.39626312\n",
      "Iteration 436, loss = 20142834921.74594879\n",
      "Iteration 437, loss = 20142828572.30666733\n",
      "Iteration 438, loss = 20142822227.17075348\n",
      "Iteration 439, loss = 20142816051.43027496\n",
      "Iteration 440, loss = 20142809979.65476227\n",
      "Iteration 441, loss = 20142803913.93239212\n",
      "Iteration 442, loss = 20142798107.34554672\n",
      "Iteration 443, loss = 20142792295.62578583\n",
      "Iteration 444, loss = 20142786620.42222214\n",
      "Iteration 445, loss = 20142781000.56750488\n",
      "Iteration 446, loss = 20142775366.48324966\n",
      "Iteration 447, loss = 20142769789.20465851\n",
      "Iteration 448, loss = 20142764308.46540070\n",
      "Iteration 449, loss = 20142758800.03266907\n",
      "Iteration 450, loss = 20142753365.91007996\n",
      "Iteration 451, loss = 20142747891.98044205\n",
      "Iteration 452, loss = 20142742498.76126862\n",
      "Iteration 453, loss = 20142737113.65857697\n",
      "Iteration 454, loss = 20142731744.71863174\n",
      "Iteration 455, loss = 20142726352.20209885\n",
      "Iteration 456, loss = 20142721033.56890106\n",
      "Iteration 457, loss = 20142715672.37312698\n",
      "Iteration 458, loss = 20142710380.15375900\n",
      "Iteration 459, loss = 20142705039.73953247\n",
      "Iteration 460, loss = 20142699771.16427231\n",
      "Iteration 461, loss = 20142694458.81277084\n",
      "Iteration 462, loss = 20142689218.62358093\n",
      "Iteration 463, loss = 20142683948.16539764\n",
      "Iteration 464, loss = 20142678637.79575348\n",
      "Iteration 465, loss = 20142673433.94607162\n",
      "Iteration 466, loss = 20142668245.72305679\n",
      "Iteration 467, loss = 20142662995.22937775\n",
      "Iteration 468, loss = 20142657707.29532242\n",
      "Iteration 469, loss = 20142652517.78758240\n",
      "Iteration 470, loss = 20142647340.05251694\n",
      "Iteration 471, loss = 20142642099.21738815\n",
      "Iteration 472, loss = 20142636909.33260345\n",
      "Iteration 473, loss = 20142631729.02468872\n",
      "Iteration 474, loss = 20142626492.18044281\n",
      "Iteration 475, loss = 20142621330.21490860\n",
      "Iteration 476, loss = 20142616187.16085815\n",
      "Iteration 477, loss = 20142611029.81193161\n",
      "Iteration 478, loss = 20142605838.90044785\n",
      "Iteration 479, loss = 20142600750.69936752\n",
      "Iteration 480, loss = 20142595569.45970154\n",
      "Iteration 481, loss = 20142590453.55586624\n",
      "Iteration 482, loss = 20142585263.72170639\n",
      "Iteration 483, loss = 20142580132.53739548\n",
      "Iteration 484, loss = 20142574979.64842224\n",
      "Iteration 485, loss = 20142569867.91628647\n",
      "Iteration 486, loss = 20142564671.54280472\n",
      "Iteration 487, loss = 20142559528.65053177\n",
      "Iteration 488, loss = 20142554412.94108200\n",
      "Iteration 489, loss = 20142549292.71659088\n",
      "Iteration 490, loss = 20142544123.78472519\n",
      "Iteration 491, loss = 20142539010.18014908\n",
      "Iteration 492, loss = 20142533910.15409470\n",
      "Iteration 493, loss = 20142528850.03283310\n",
      "Iteration 494, loss = 20142523709.75933456\n",
      "Iteration 495, loss = 20142518598.68307495\n",
      "Iteration 496, loss = 20142513494.67034149\n",
      "Iteration 497, loss = 20142508413.11159897\n",
      "Iteration 498, loss = 20142503333.36053467\n",
      "Iteration 499, loss = 20142498225.67036438\n",
      "Iteration 500, loss = 20142493149.41760635\n",
      "Iteration 1, loss = 20308736620.38319016\n",
      "Iteration 2, loss = 20308719688.17727661\n",
      "Iteration 3, loss = 20308702684.07358170\n",
      "Iteration 4, loss = 20308685590.91508865\n",
      "Iteration 5, loss = 20308668305.71546555\n",
      "Iteration 6, loss = 20308650955.26224518\n",
      "Iteration 7, loss = 20308633150.89948654\n",
      "Iteration 8, loss = 20308615312.43576050\n",
      "Iteration 9, loss = 20308597031.83753204\n",
      "Iteration 10, loss = 20308578435.59080124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 20308559116.95780182\n",
      "Iteration 12, loss = 20308539003.25586700\n",
      "Iteration 13, loss = 20308517674.07137299\n",
      "Iteration 14, loss = 20308495718.78747940\n",
      "Iteration 15, loss = 20308473229.83423615\n",
      "Iteration 16, loss = 20308450571.24460983\n",
      "Iteration 17, loss = 20308427094.92582321\n",
      "Iteration 18, loss = 20308402847.07174683\n",
      "Iteration 19, loss = 20308377101.64882660\n",
      "Iteration 20, loss = 20308349921.17713165\n",
      "Iteration 21, loss = 20308321744.49601364\n",
      "Iteration 22, loss = 20308293600.96383667\n",
      "Iteration 23, loss = 20308266154.25862122\n",
      "Iteration 24, loss = 20308241005.43648148\n",
      "Iteration 25, loss = 20308217660.84574509\n",
      "Iteration 26, loss = 20308196322.73452377\n",
      "Iteration 27, loss = 20308175256.18526840\n",
      "Iteration 28, loss = 20308154076.99606705\n",
      "Iteration 29, loss = 20308134400.27775574\n",
      "Iteration 30, loss = 20308115236.79786682\n",
      "Iteration 31, loss = 20308096568.77804947\n",
      "Iteration 32, loss = 20308079424.40569305\n",
      "Iteration 33, loss = 20308064505.65433884\n",
      "Iteration 34, loss = 20308050651.46005630\n",
      "Iteration 35, loss = 20308036551.20503616\n",
      "Iteration 36, loss = 20308022824.97177124\n",
      "Iteration 37, loss = 20308010246.73345566\n",
      "Iteration 38, loss = 20307998391.98623276\n",
      "Iteration 39, loss = 20307987149.26322174\n",
      "Iteration 40, loss = 20307976218.45261765\n",
      "Iteration 41, loss = 20307965934.56914902\n",
      "Iteration 42, loss = 20307956001.55083466\n",
      "Iteration 43, loss = 20307946358.24829865\n",
      "Iteration 44, loss = 20307936750.55896759\n",
      "Iteration 45, loss = 20307927510.70159531\n",
      "Iteration 46, loss = 20307918517.17951202\n",
      "Iteration 47, loss = 20307909683.10129166\n",
      "Iteration 48, loss = 20307901052.74551010\n",
      "Iteration 49, loss = 20307892434.67717743\n",
      "Iteration 50, loss = 20307883848.15520477\n",
      "Iteration 51, loss = 20307875204.24862289\n",
      "Iteration 52, loss = 20307866720.25709915\n",
      "Iteration 53, loss = 20307858475.68207550\n",
      "Iteration 54, loss = 20307850381.94407272\n",
      "Iteration 55, loss = 20307842277.81514740\n",
      "Iteration 56, loss = 20307834257.35422897\n",
      "Iteration 57, loss = 20307826331.23016357\n",
      "Iteration 58, loss = 20307818324.36927032\n",
      "Iteration 59, loss = 20307810336.10396957\n",
      "Iteration 60, loss = 20307802261.74892426\n",
      "Iteration 61, loss = 20307794252.30158997\n",
      "Iteration 62, loss = 20307786288.99638367\n",
      "Iteration 63, loss = 20307778353.26266861\n",
      "Iteration 64, loss = 20307770492.17520142\n",
      "Iteration 65, loss = 20307762712.41903687\n",
      "Iteration 66, loss = 20307754952.90804672\n",
      "Iteration 67, loss = 20307747156.40239716\n",
      "Iteration 68, loss = 20307739292.33039093\n",
      "Iteration 69, loss = 20307731470.99784088\n",
      "Iteration 70, loss = 20307723706.99632263\n",
      "Iteration 71, loss = 20307715980.84704971\n",
      "Iteration 72, loss = 20307708457.02911758\n",
      "Iteration 73, loss = 20307700919.46358109\n",
      "Iteration 74, loss = 20307693358.43745041\n",
      "Iteration 75, loss = 20307685920.35643768\n",
      "Iteration 76, loss = 20307678419.08323288\n",
      "Iteration 77, loss = 20307670980.42319107\n",
      "Iteration 78, loss = 20307663500.28983307\n",
      "Iteration 79, loss = 20307656036.13274384\n",
      "Iteration 80, loss = 20307648529.93636322\n",
      "Iteration 81, loss = 20307641031.30051041\n",
      "Iteration 82, loss = 20307633662.49910736\n",
      "Iteration 83, loss = 20307626276.35713959\n",
      "Iteration 84, loss = 20307618856.28509521\n",
      "Iteration 85, loss = 20307611496.55273438\n",
      "Iteration 86, loss = 20307604176.52716446\n",
      "Iteration 87, loss = 20307596955.27133560\n",
      "Iteration 88, loss = 20307589630.75671387\n",
      "Iteration 89, loss = 20307582305.09926224\n",
      "Iteration 90, loss = 20307574996.36777496\n",
      "Iteration 91, loss = 20307567786.44510651\n",
      "Iteration 92, loss = 20307560563.42758942\n",
      "Iteration 93, loss = 20307553365.36527634\n",
      "Iteration 94, loss = 20307546264.10823059\n",
      "Iteration 95, loss = 20307539084.06507111\n",
      "Iteration 96, loss = 20307531914.01516724\n",
      "Iteration 97, loss = 20307524727.72697830\n",
      "Iteration 98, loss = 20307517600.56139374\n",
      "Iteration 99, loss = 20307510423.61193848\n",
      "Iteration 100, loss = 20307503207.02016449\n",
      "Iteration 101, loss = 20307496008.34475327\n",
      "Iteration 102, loss = 20307488657.76339340\n",
      "Iteration 103, loss = 20307481361.52601624\n",
      "Iteration 104, loss = 20307474089.47253799\n",
      "Iteration 105, loss = 20307466910.51768112\n",
      "Iteration 106, loss = 20307459721.87174225\n",
      "Iteration 107, loss = 20307452533.80841064\n",
      "Iteration 108, loss = 20307445329.21065903\n",
      "Iteration 109, loss = 20307438173.00636292\n",
      "Iteration 110, loss = 20307431136.00098038\n",
      "Iteration 111, loss = 20307424124.82640457\n",
      "Iteration 112, loss = 20307417058.55023956\n",
      "Iteration 113, loss = 20307410004.67391205\n",
      "Iteration 114, loss = 20307402957.33608627\n",
      "Iteration 115, loss = 20307395886.68455887\n",
      "Iteration 116, loss = 20307388802.22611618\n",
      "Iteration 117, loss = 20307381745.60970306\n",
      "Iteration 118, loss = 20307374662.72627640\n",
      "Iteration 119, loss = 20307367517.48413086\n",
      "Iteration 120, loss = 20307360499.69196701\n",
      "Iteration 121, loss = 20307353378.17570877\n",
      "Iteration 122, loss = 20307346220.11602783\n",
      "Iteration 123, loss = 20307339077.13967896\n",
      "Iteration 124, loss = 20307331949.68886566\n",
      "Iteration 125, loss = 20307324902.42860794\n",
      "Iteration 126, loss = 20307317874.47055817\n",
      "Iteration 127, loss = 20307310754.95933151\n",
      "Iteration 128, loss = 20307303789.10964584\n",
      "Iteration 129, loss = 20307296747.04766464\n",
      "Iteration 130, loss = 20307289795.63441086\n",
      "Iteration 131, loss = 20307282721.97637558\n",
      "Iteration 132, loss = 20307275648.38688660\n",
      "Iteration 133, loss = 20307268454.20057297\n",
      "Iteration 134, loss = 20307261238.81937408\n",
      "Iteration 135, loss = 20307253858.30330658\n",
      "Iteration 136, loss = 20307246327.43589401\n",
      "Iteration 137, loss = 20307238746.94190979\n",
      "Iteration 138, loss = 20307231030.20259857\n",
      "Iteration 139, loss = 20307223340.78875351\n",
      "Iteration 140, loss = 20307215608.49475098\n",
      "Iteration 141, loss = 20307207782.53184509\n",
      "Iteration 142, loss = 20307199994.26214981\n",
      "Iteration 143, loss = 20307192306.62297821\n",
      "Iteration 144, loss = 20307184649.35968018\n",
      "Iteration 145, loss = 20307177125.48469925\n",
      "Iteration 146, loss = 20307169554.00712967\n",
      "Iteration 147, loss = 20307161943.61362076\n",
      "Iteration 148, loss = 20307154473.77013016\n",
      "Iteration 149, loss = 20307146945.59401321\n",
      "Iteration 150, loss = 20307139488.41080093\n",
      "Iteration 151, loss = 20307132075.35932541\n",
      "Iteration 152, loss = 20307124634.48854828\n",
      "Iteration 153, loss = 20307117296.46955872\n",
      "Iteration 154, loss = 20307109918.00014877\n",
      "Iteration 155, loss = 20307102603.34847260\n",
      "Iteration 156, loss = 20307095305.22700882\n",
      "Iteration 157, loss = 20307087916.51822281\n",
      "Iteration 158, loss = 20307080673.04333878\n",
      "Iteration 159, loss = 20307073255.85112000\n",
      "Iteration 160, loss = 20307065895.66327286\n",
      "Iteration 161, loss = 20307058571.14370728\n",
      "Iteration 162, loss = 20307051316.92233276\n",
      "Iteration 163, loss = 20307044008.28633881\n",
      "Iteration 164, loss = 20307036619.05527878\n",
      "Iteration 165, loss = 20307029324.64681625\n",
      "Iteration 166, loss = 20307022058.21644974\n",
      "Iteration 167, loss = 20307014809.39334869\n",
      "Iteration 168, loss = 20307007511.91614151\n",
      "Iteration 169, loss = 20307000226.91696167\n",
      "Iteration 170, loss = 20306992872.72776413\n",
      "Iteration 171, loss = 20306985715.66372299\n",
      "Iteration 172, loss = 20306978560.71783066\n",
      "Iteration 173, loss = 20306971480.09580612\n",
      "Iteration 174, loss = 20306964350.91192627\n",
      "Iteration 175, loss = 20306957130.60030365\n",
      "Iteration 176, loss = 20306950039.56933594\n",
      "Iteration 177, loss = 20306942822.84482574\n",
      "Iteration 178, loss = 20306935731.64059448\n",
      "Iteration 179, loss = 20306928572.14782715\n",
      "Iteration 180, loss = 20306921437.01093674\n",
      "Iteration 181, loss = 20306914286.70686340\n",
      "Iteration 182, loss = 20306907083.38636398\n",
      "Iteration 183, loss = 20306899925.01598740\n",
      "Iteration 184, loss = 20306892660.52379227\n",
      "Iteration 185, loss = 20306885446.44187546\n",
      "Iteration 186, loss = 20306878201.14597702\n",
      "Iteration 187, loss = 20306871037.21199036\n",
      "Iteration 188, loss = 20306863824.76642990\n",
      "Iteration 189, loss = 20306856692.96917725\n",
      "Iteration 190, loss = 20306849534.31433868\n",
      "Iteration 191, loss = 20306842376.40896225\n",
      "Iteration 192, loss = 20306835190.33866119\n",
      "Iteration 193, loss = 20306828009.86764526\n",
      "Iteration 194, loss = 20306820902.74411011\n",
      "Iteration 195, loss = 20306813673.68608856\n",
      "Iteration 196, loss = 20306806470.15686035\n",
      "Iteration 197, loss = 20306799301.85140991\n",
      "Iteration 198, loss = 20306792179.57424927\n",
      "Iteration 199, loss = 20306785101.05591202\n",
      "Iteration 200, loss = 20306778025.26681137\n",
      "Iteration 201, loss = 20306770895.65481567\n",
      "Iteration 202, loss = 20306763806.96437454\n",
      "Iteration 203, loss = 20306756780.84509659\n",
      "Iteration 204, loss = 20306749698.48347092\n",
      "Iteration 205, loss = 20306742574.47095490\n",
      "Iteration 206, loss = 20306735392.31082535\n",
      "Iteration 207, loss = 20306728176.33994293\n",
      "Iteration 208, loss = 20306720948.07186508\n",
      "Iteration 209, loss = 20306713770.16670227\n",
      "Iteration 210, loss = 20306706724.75095749\n",
      "Iteration 211, loss = 20306699743.89928436\n",
      "Iteration 212, loss = 20306692817.15491486\n",
      "Iteration 213, loss = 20306685833.31992340\n",
      "Iteration 214, loss = 20306678719.10807419\n",
      "Iteration 215, loss = 20306671586.75698471\n",
      "Iteration 216, loss = 20306664435.19995880\n",
      "Iteration 217, loss = 20306657219.74365234\n",
      "Iteration 218, loss = 20306650014.48775482\n",
      "Iteration 219, loss = 20306642885.01288986\n",
      "Iteration 220, loss = 20306635743.04119110\n",
      "Iteration 221, loss = 20306628693.89872360\n",
      "Iteration 222, loss = 20306621710.59098816\n",
      "Iteration 223, loss = 20306614724.82055283\n",
      "Iteration 224, loss = 20306607778.18926239\n",
      "Iteration 225, loss = 20306600785.80174255\n",
      "Iteration 226, loss = 20306593802.44224930\n",
      "Iteration 227, loss = 20306586766.09237289\n",
      "Iteration 228, loss = 20306579863.99698639\n",
      "Iteration 229, loss = 20306572803.86012650\n",
      "Iteration 230, loss = 20306565827.70270157\n",
      "Iteration 231, loss = 20306558925.49411011\n",
      "Iteration 232, loss = 20306551993.19668579\n",
      "Iteration 233, loss = 20306545001.38711548\n",
      "Iteration 234, loss = 20306538040.02671432\n",
      "Iteration 235, loss = 20306530971.82656479\n",
      "Iteration 236, loss = 20306523932.88176727\n",
      "Iteration 237, loss = 20306516995.24050522\n",
      "Iteration 238, loss = 20306510004.34118271\n",
      "Iteration 239, loss = 20306502973.07183456\n",
      "Iteration 240, loss = 20306496009.00733948\n",
      "Iteration 241, loss = 20306489024.70123291\n",
      "Iteration 242, loss = 20306482070.57778931\n",
      "Iteration 243, loss = 20306475105.31740189\n",
      "Iteration 244, loss = 20306468189.33795547\n",
      "Iteration 245, loss = 20306461209.02685928\n",
      "Iteration 246, loss = 20306454233.63071823\n",
      "Iteration 247, loss = 20306447258.14936447\n",
      "Iteration 248, loss = 20306440340.46315002\n",
      "Iteration 249, loss = 20306433412.78877258\n",
      "Iteration 250, loss = 20306426541.14146423\n",
      "Iteration 251, loss = 20306419700.02466202\n",
      "Iteration 252, loss = 20306412864.93565369\n",
      "Iteration 253, loss = 20306405825.13024902\n",
      "Iteration 254, loss = 20306398792.54448318\n",
      "Iteration 255, loss = 20306391752.19739914\n",
      "Iteration 256, loss = 20306384684.52848434\n",
      "Iteration 257, loss = 20306377557.77873993\n",
      "Iteration 258, loss = 20306370386.34820938\n",
      "Iteration 259, loss = 20306363141.53182983\n",
      "Iteration 260, loss = 20306355816.04047394\n",
      "Iteration 261, loss = 20306348361.11214828\n",
      "Iteration 262, loss = 20306340701.61268616\n",
      "Iteration 263, loss = 20306332959.56194687\n",
      "Iteration 264, loss = 20306324968.75384903\n",
      "Iteration 265, loss = 20306316810.80390549\n",
      "Iteration 266, loss = 20306308707.85592651\n",
      "Iteration 267, loss = 20306300653.93239594\n",
      "Iteration 268, loss = 20306292632.89100647\n",
      "Iteration 269, loss = 20306284712.48707962\n",
      "Iteration 270, loss = 20306276851.57009125\n",
      "Iteration 271, loss = 20306268888.44551849\n",
      "Iteration 272, loss = 20306261150.49815369\n",
      "Iteration 273, loss = 20306253358.14168930\n",
      "Iteration 274, loss = 20306245607.03839111\n",
      "Iteration 275, loss = 20306237900.68140030\n",
      "Iteration 276, loss = 20306230347.22309875\n",
      "Iteration 277, loss = 20306222679.49193954\n",
      "Iteration 278, loss = 20306215127.75528717\n",
      "Iteration 279, loss = 20306207622.71123505\n",
      "Iteration 280, loss = 20306200153.20035553\n",
      "Iteration 281, loss = 20306192758.32273102\n",
      "Iteration 282, loss = 20306185376.50844574\n",
      "Iteration 283, loss = 20306177956.15377426\n",
      "Iteration 284, loss = 20306170581.08698273\n",
      "Iteration 285, loss = 20306163109.79194260\n",
      "Iteration 286, loss = 20306155652.61470413\n",
      "Iteration 287, loss = 20306148092.73136139\n",
      "Iteration 288, loss = 20306140555.68648148\n",
      "Iteration 289, loss = 20306133061.58924484\n",
      "Iteration 290, loss = 20306125551.73452377\n",
      "Iteration 291, loss = 20306118050.53564835\n",
      "Iteration 292, loss = 20306110614.79335785\n",
      "Iteration 293, loss = 20306103160.78169250\n",
      "Iteration 294, loss = 20306095745.64188766\n",
      "Iteration 295, loss = 20306088383.13385010\n",
      "Iteration 296, loss = 20306081090.38295746\n",
      "Iteration 297, loss = 20306073815.88592911\n",
      "Iteration 298, loss = 20306066568.28205109\n",
      "Iteration 299, loss = 20306059289.94752502\n",
      "Iteration 300, loss = 20306051953.25236130\n",
      "Iteration 301, loss = 20306044681.42044449\n",
      "Iteration 302, loss = 20306037404.33544540\n",
      "Iteration 303, loss = 20306030146.80278397\n",
      "Iteration 304, loss = 20306022856.47757721\n",
      "Iteration 305, loss = 20306015443.78467941\n",
      "Iteration 306, loss = 20306008201.79817200\n",
      "Iteration 307, loss = 20306000895.52687836\n",
      "Iteration 308, loss = 20305993586.38327789\n",
      "Iteration 309, loss = 20305986342.38100433\n",
      "Iteration 310, loss = 20305979073.58097076\n",
      "Iteration 311, loss = 20305971822.44252396\n",
      "Iteration 312, loss = 20305964647.17633438\n",
      "Iteration 313, loss = 20305957420.43280411\n",
      "Iteration 314, loss = 20305950186.18243408\n",
      "Iteration 315, loss = 20305942907.23387909\n",
      "Iteration 316, loss = 20305935646.59465027\n",
      "Iteration 317, loss = 20305928349.43017578\n",
      "Iteration 318, loss = 20305921149.06770706\n",
      "Iteration 319, loss = 20305913968.23413849\n",
      "Iteration 320, loss = 20305906872.48593903\n",
      "Iteration 321, loss = 20305899773.58834076\n",
      "Iteration 322, loss = 20305892606.84115982\n",
      "Iteration 323, loss = 20305885360.69351196\n",
      "Iteration 324, loss = 20305878162.91820526\n",
      "Iteration 325, loss = 20305870918.16851807\n",
      "Iteration 326, loss = 20305863750.69779205\n",
      "Iteration 327, loss = 20305856579.51163101\n",
      "Iteration 328, loss = 20305849450.77621841\n",
      "Iteration 329, loss = 20305842353.92033386\n",
      "Iteration 330, loss = 20305835183.15810013\n",
      "Iteration 331, loss = 20305827990.60599136\n",
      "Iteration 332, loss = 20305820829.23003387\n",
      "Iteration 333, loss = 20305813608.19216156\n",
      "Iteration 334, loss = 20305806408.76864624\n",
      "Iteration 335, loss = 20305799147.45863342\n",
      "Iteration 336, loss = 20305791985.73730469\n",
      "Iteration 337, loss = 20305784832.95071411\n",
      "Iteration 338, loss = 20305777632.06473923\n",
      "Iteration 339, loss = 20305770490.48886490\n",
      "Iteration 340, loss = 20305763374.07929993\n",
      "Iteration 341, loss = 20305756184.08735657\n",
      "Iteration 342, loss = 20305748994.87403488\n",
      "Iteration 343, loss = 20305741787.32761383\n",
      "Iteration 344, loss = 20305734553.86948776\n",
      "Iteration 345, loss = 20305727269.87792969\n",
      "Iteration 346, loss = 20305719954.35712433\n",
      "Iteration 347, loss = 20305712633.05865860\n",
      "Iteration 348, loss = 20305705412.86127853\n",
      "Iteration 349, loss = 20305698163.00158691\n",
      "Iteration 350, loss = 20305690847.11802673\n",
      "Iteration 351, loss = 20305683539.97707748\n",
      "Iteration 352, loss = 20305676237.80437088\n",
      "Iteration 353, loss = 20305668941.45775604\n",
      "Iteration 354, loss = 20305661674.91485214\n",
      "Iteration 355, loss = 20305654445.69752502\n",
      "Iteration 356, loss = 20305647305.71080017\n",
      "Iteration 357, loss = 20305640101.11147690\n",
      "Iteration 358, loss = 20305632924.64826965\n",
      "Iteration 359, loss = 20305625743.60155106\n",
      "Iteration 360, loss = 20305618495.96204376\n",
      "Iteration 361, loss = 20305611313.31742477\n",
      "Iteration 362, loss = 20305604202.14846802\n",
      "Iteration 363, loss = 20305597050.54033279\n",
      "Iteration 364, loss = 20305589858.92191315\n",
      "Iteration 365, loss = 20305582664.12699509\n",
      "Iteration 366, loss = 20305575528.96493912\n",
      "Iteration 367, loss = 20305568439.42824936\n",
      "Iteration 368, loss = 20305561220.68380356\n",
      "Iteration 369, loss = 20305554143.79078674\n",
      "Iteration 370, loss = 20305546961.09237671\n",
      "Iteration 371, loss = 20305539692.53093338\n",
      "Iteration 372, loss = 20305532511.92257690\n",
      "Iteration 373, loss = 20305525285.92451096\n",
      "Iteration 374, loss = 20305518050.15015793\n",
      "Iteration 375, loss = 20305510830.07215500\n",
      "Iteration 376, loss = 20305503751.45525360\n",
      "Iteration 377, loss = 20305496658.23830414\n",
      "Iteration 378, loss = 20305489641.02209473\n",
      "Iteration 379, loss = 20305482510.93284607\n",
      "Iteration 380, loss = 20305475323.90198898\n",
      "Iteration 381, loss = 20305468091.30679703\n",
      "Iteration 382, loss = 20305460999.81864929\n",
      "Iteration 383, loss = 20305453851.18579483\n",
      "Iteration 384, loss = 20305446708.78357315\n",
      "Iteration 385, loss = 20305439559.80229568\n",
      "Iteration 386, loss = 20305432447.98878479\n",
      "Iteration 387, loss = 20305425368.32716370\n",
      "Iteration 388, loss = 20305418346.64863968\n",
      "Iteration 389, loss = 20305411268.06649399\n",
      "Iteration 390, loss = 20305404182.65378189\n",
      "Iteration 391, loss = 20305397087.99921417\n",
      "Iteration 392, loss = 20305389985.87025833\n",
      "Iteration 393, loss = 20305382922.09658813\n",
      "Iteration 394, loss = 20305375811.90970993\n",
      "Iteration 395, loss = 20305368673.50592804\n",
      "Iteration 396, loss = 20305361546.40615845\n",
      "Iteration 397, loss = 20305354426.94735718\n",
      "Iteration 398, loss = 20305347293.77888489\n",
      "Iteration 399, loss = 20305340121.58301544\n",
      "Iteration 400, loss = 20305332957.83375549\n",
      "Iteration 401, loss = 20305325715.06676102\n",
      "Iteration 402, loss = 20305318635.96598816\n",
      "Iteration 403, loss = 20305311495.40320969\n",
      "Iteration 404, loss = 20305304404.70672989\n",
      "Iteration 405, loss = 20305297215.77489090\n",
      "Iteration 406, loss = 20305289988.49488068\n",
      "Iteration 407, loss = 20305282805.57226562\n",
      "Iteration 408, loss = 20305275590.80333328\n",
      "Iteration 409, loss = 20305268429.42480469\n",
      "Iteration 410, loss = 20305261298.62264633\n",
      "Iteration 411, loss = 20305254171.48682022\n",
      "Iteration 412, loss = 20305246959.78100586\n",
      "Iteration 413, loss = 20305239872.40190125\n",
      "Iteration 414, loss = 20305232952.16099930\n",
      "Iteration 415, loss = 20305225908.38528824\n",
      "Iteration 416, loss = 20305218924.31352234\n",
      "Iteration 417, loss = 20305211974.93365479\n",
      "Iteration 418, loss = 20305205022.47694397\n",
      "Iteration 419, loss = 20305198014.87131882\n",
      "Iteration 420, loss = 20305190984.65815735\n",
      "Iteration 421, loss = 20305183957.28987122\n",
      "Iteration 422, loss = 20305176998.34936905\n",
      "Iteration 423, loss = 20305169919.56429672\n",
      "Iteration 424, loss = 20305162813.54564667\n",
      "Iteration 425, loss = 20305155608.17618942\n",
      "Iteration 426, loss = 20305148510.61479950\n",
      "Iteration 427, loss = 20305141345.21038437\n",
      "Iteration 428, loss = 20305134302.85919952\n",
      "Iteration 429, loss = 20305127276.48243332\n",
      "Iteration 430, loss = 20305120212.36237335\n",
      "Iteration 431, loss = 20305113157.00267792\n",
      "Iteration 432, loss = 20305106074.39042282\n",
      "Iteration 433, loss = 20305098993.71500397\n",
      "Iteration 434, loss = 20305092061.63808441\n",
      "Iteration 435, loss = 20305085107.69137573\n",
      "Iteration 436, loss = 20305078094.47580338\n",
      "Iteration 437, loss = 20305071027.34998322\n",
      "Iteration 438, loss = 20305063838.29861450\n",
      "Iteration 439, loss = 20305056677.91368866\n",
      "Iteration 440, loss = 20305049625.86147308\n",
      "Iteration 441, loss = 20305042601.80487823\n",
      "Iteration 442, loss = 20305035655.58385086\n",
      "Iteration 443, loss = 20305028662.14189148\n",
      "Iteration 444, loss = 20305021765.71291351\n",
      "Iteration 445, loss = 20305014717.11133194\n",
      "Iteration 446, loss = 20305007641.74315262\n",
      "Iteration 447, loss = 20305000444.93704605\n",
      "Iteration 448, loss = 20304993345.91073227\n",
      "Iteration 449, loss = 20304986202.52083969\n",
      "Iteration 450, loss = 20304979132.44131470\n",
      "Iteration 451, loss = 20304971999.73466873\n",
      "Iteration 452, loss = 20304964967.54146576\n",
      "Iteration 453, loss = 20304957923.03556824\n",
      "Iteration 454, loss = 20304950900.88834763\n",
      "Iteration 455, loss = 20304943902.71833801\n",
      "Iteration 456, loss = 20304936935.53046417\n",
      "Iteration 457, loss = 20304929984.54588318\n",
      "Iteration 458, loss = 20304923043.15019989\n",
      "Iteration 459, loss = 20304915974.83481979\n",
      "Iteration 460, loss = 20304908955.04792786\n",
      "Iteration 461, loss = 20304901917.79705048\n",
      "Iteration 462, loss = 20304894828.65600204\n",
      "Iteration 463, loss = 20304887796.36673737\n",
      "Iteration 464, loss = 20304880785.85103607\n",
      "Iteration 465, loss = 20304873791.67404175\n",
      "Iteration 466, loss = 20304866690.86911011\n",
      "Iteration 467, loss = 20304859588.31130600\n",
      "Iteration 468, loss = 20304852482.70439148\n",
      "Iteration 469, loss = 20304845485.18401337\n",
      "Iteration 470, loss = 20304838452.53114319\n",
      "Iteration 471, loss = 20304831405.94158936\n",
      "Iteration 472, loss = 20304824388.46902847\n",
      "Iteration 473, loss = 20304817396.19160080\n",
      "Iteration 474, loss = 20304810362.91461182\n",
      "Iteration 475, loss = 20304803377.71552658\n",
      "Iteration 476, loss = 20304796422.41575623\n",
      "Iteration 477, loss = 20304789413.69578171\n",
      "Iteration 478, loss = 20304782311.92480087\n",
      "Iteration 479, loss = 20304775278.50198746\n",
      "Iteration 480, loss = 20304768176.77789688\n",
      "Iteration 481, loss = 20304761070.45907211\n",
      "Iteration 482, loss = 20304753968.24615479\n",
      "Iteration 483, loss = 20304746806.76911926\n",
      "Iteration 484, loss = 20304739675.52529907\n",
      "Iteration 485, loss = 20304732455.75511169\n",
      "Iteration 486, loss = 20304725355.99879837\n",
      "Iteration 487, loss = 20304718235.72843552\n",
      "Iteration 488, loss = 20304711124.03647995\n",
      "Iteration 489, loss = 20304704100.80381393\n",
      "Iteration 490, loss = 20304696945.03812408\n",
      "Iteration 491, loss = 20304689898.75705338\n",
      "Iteration 492, loss = 20304682769.84754562\n",
      "Iteration 493, loss = 20304675764.17957687\n",
      "Iteration 494, loss = 20304668638.01848602\n",
      "Iteration 495, loss = 20304661659.02770233\n",
      "Iteration 496, loss = 20304654600.21517563\n",
      "Iteration 497, loss = 20304647529.02996063\n",
      "Iteration 498, loss = 20304640430.65337753\n",
      "Iteration 499, loss = 20304633401.93102264\n",
      "Iteration 500, loss = 20304626314.39749908\n",
      "Iteration 1, loss = 20004041374.67829895\n",
      "Iteration 2, loss = 20004024428.49465179\n",
      "Iteration 3, loss = 20004006937.65055847\n",
      "Iteration 4, loss = 20003989820.27991867\n",
      "Iteration 5, loss = 20003972322.53109360\n",
      "Iteration 6, loss = 20003954981.70691299\n",
      "Iteration 7, loss = 20003937805.28243637\n",
      "Iteration 8, loss = 20003920505.02526474\n",
      "Iteration 9, loss = 20003903683.92972183\n",
      "Iteration 10, loss = 20003887105.64050293\n",
      "Iteration 11, loss = 20003870764.22776794\n",
      "Iteration 12, loss = 20003854036.04427338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 20003837016.14161682\n",
      "Iteration 14, loss = 20003819577.97165298\n",
      "Iteration 15, loss = 20003800866.28668594\n",
      "Iteration 16, loss = 20003781224.05949783\n",
      "Iteration 17, loss = 20003760508.37895966\n",
      "Iteration 18, loss = 20003739485.30881500\n",
      "Iteration 19, loss = 20003717854.18330765\n",
      "Iteration 20, loss = 20003696236.10235596\n",
      "Iteration 21, loss = 20003675896.71770096\n",
      "Iteration 22, loss = 20003655523.93779755\n",
      "Iteration 23, loss = 20003633588.92999649\n",
      "Iteration 24, loss = 20003610642.31324387\n",
      "Iteration 25, loss = 20003589662.24374771\n",
      "Iteration 26, loss = 20003570114.13866806\n",
      "Iteration 27, loss = 20003550799.53368759\n",
      "Iteration 28, loss = 20003532287.53524017\n",
      "Iteration 29, loss = 20003514964.80472946\n",
      "Iteration 30, loss = 20003498957.12551117\n",
      "Iteration 31, loss = 20003485241.61846161\n",
      "Iteration 32, loss = 20003472810.96554947\n",
      "Iteration 33, loss = 20003460405.41657257\n",
      "Iteration 34, loss = 20003447699.58306885\n",
      "Iteration 35, loss = 20003436232.53524399\n",
      "Iteration 36, loss = 20003425569.82820129\n",
      "Iteration 37, loss = 20003415194.21150589\n",
      "Iteration 38, loss = 20003405405.51374817\n",
      "Iteration 39, loss = 20003396054.03329086\n",
      "Iteration 40, loss = 20003387031.78353119\n",
      "Iteration 41, loss = 20003378373.51867676\n",
      "Iteration 42, loss = 20003370026.16514969\n",
      "Iteration 43, loss = 20003362242.51375961\n",
      "Iteration 44, loss = 20003354626.93394089\n",
      "Iteration 45, loss = 20003347239.70793152\n",
      "Iteration 46, loss = 20003339904.06681442\n",
      "Iteration 47, loss = 20003332804.37097931\n",
      "Iteration 48, loss = 20003325721.66327667\n",
      "Iteration 49, loss = 20003318689.47098160\n",
      "Iteration 50, loss = 20003311735.27605057\n",
      "Iteration 51, loss = 20003304916.41637802\n",
      "Iteration 52, loss = 20003298120.72338486\n",
      "Iteration 53, loss = 20003291359.81770325\n",
      "Iteration 54, loss = 20003284502.96617508\n",
      "Iteration 55, loss = 20003277719.07654572\n",
      "Iteration 56, loss = 20003271021.13071060\n",
      "Iteration 57, loss = 20003264393.48087692\n",
      "Iteration 58, loss = 20003257863.20409393\n",
      "Iteration 59, loss = 20003251371.75142288\n",
      "Iteration 60, loss = 20003244859.77934265\n",
      "Iteration 61, loss = 20003238422.57790756\n",
      "Iteration 62, loss = 20003232020.95781708\n",
      "Iteration 63, loss = 20003225594.46779251\n",
      "Iteration 64, loss = 20003219174.66466141\n",
      "Iteration 65, loss = 20003212737.02249146\n",
      "Iteration 66, loss = 20003206174.00452423\n",
      "Iteration 67, loss = 20003199720.16756058\n",
      "Iteration 68, loss = 20003193231.55142212\n",
      "Iteration 69, loss = 20003186771.91829681\n",
      "Iteration 70, loss = 20003180184.04184341\n",
      "Iteration 71, loss = 20003173541.46715164\n",
      "Iteration 72, loss = 20003166880.62932205\n",
      "Iteration 73, loss = 20003160225.04013443\n",
      "Iteration 74, loss = 20003153467.79692459\n",
      "Iteration 75, loss = 20003146744.61495972\n",
      "Iteration 76, loss = 20003140057.33612823\n",
      "Iteration 77, loss = 20003133391.14067078\n",
      "Iteration 78, loss = 20003126777.24770737\n",
      "Iteration 79, loss = 20003120158.60694885\n",
      "Iteration 80, loss = 20003113571.81122971\n",
      "Iteration 81, loss = 20003106969.23798370\n",
      "Iteration 82, loss = 20003100359.74248123\n",
      "Iteration 83, loss = 20003093937.63000107\n",
      "Iteration 84, loss = 20003087485.91305542\n",
      "Iteration 85, loss = 20003081075.37386322\n",
      "Iteration 86, loss = 20003074691.46237564\n",
      "Iteration 87, loss = 20003068388.62671661\n",
      "Iteration 88, loss = 20003062097.76733398\n",
      "Iteration 89, loss = 20003055806.17769623\n",
      "Iteration 90, loss = 20003049513.46100998\n",
      "Iteration 91, loss = 20003043329.21258926\n",
      "Iteration 92, loss = 20003037085.27266312\n",
      "Iteration 93, loss = 20003030832.47687149\n",
      "Iteration 94, loss = 20003024613.72575760\n",
      "Iteration 95, loss = 20003018380.63662720\n",
      "Iteration 96, loss = 20003012184.63492966\n",
      "Iteration 97, loss = 20003006014.61831665\n",
      "Iteration 98, loss = 20002999898.87896347\n",
      "Iteration 99, loss = 20002993757.75225067\n",
      "Iteration 100, loss = 20002987607.92057800\n",
      "Iteration 101, loss = 20002981529.44943619\n",
      "Iteration 102, loss = 20002975416.52413940\n",
      "Iteration 103, loss = 20002969334.95343399\n",
      "Iteration 104, loss = 20002963259.39676285\n",
      "Iteration 105, loss = 20002957221.48214722\n",
      "Iteration 106, loss = 20002951133.50911713\n",
      "Iteration 107, loss = 20002945161.15570068\n",
      "Iteration 108, loss = 20002939093.56527328\n",
      "Iteration 109, loss = 20002933117.61594391\n",
      "Iteration 110, loss = 20002927069.36381149\n",
      "Iteration 111, loss = 20002921056.01717377\n",
      "Iteration 112, loss = 20002915083.01189804\n",
      "Iteration 113, loss = 20002909077.41525269\n",
      "Iteration 114, loss = 20002903095.25101089\n",
      "Iteration 115, loss = 20002897164.81665802\n",
      "Iteration 116, loss = 20002891182.01348877\n",
      "Iteration 117, loss = 20002885253.34989929\n",
      "Iteration 118, loss = 20002879249.94456482\n",
      "Iteration 119, loss = 20002873383.93667603\n",
      "Iteration 120, loss = 20002867428.04423523\n",
      "Iteration 121, loss = 20002861508.06997681\n",
      "Iteration 122, loss = 20002855579.64154053\n",
      "Iteration 123, loss = 20002849706.10174179\n",
      "Iteration 124, loss = 20002843812.55192566\n",
      "Iteration 125, loss = 20002837880.44738007\n",
      "Iteration 126, loss = 20002832000.84809875\n",
      "Iteration 127, loss = 20002826137.30510712\n",
      "Iteration 128, loss = 20002820255.42783356\n",
      "Iteration 129, loss = 20002814365.57379532\n",
      "Iteration 130, loss = 20002808519.49009323\n",
      "Iteration 131, loss = 20002802666.31569290\n",
      "Iteration 132, loss = 20002796763.92929077\n",
      "Iteration 133, loss = 20002790920.84236908\n",
      "Iteration 134, loss = 20002785104.80133438\n",
      "Iteration 135, loss = 20002779201.31600571\n",
      "Iteration 136, loss = 20002773394.00487137\n",
      "Iteration 137, loss = 20002767546.41488647\n",
      "Iteration 138, loss = 20002761700.97511673\n",
      "Iteration 139, loss = 20002755918.39028931\n",
      "Iteration 140, loss = 20002750054.97578049\n",
      "Iteration 141, loss = 20002744231.34391403\n",
      "Iteration 142, loss = 20002738414.99254990\n",
      "Iteration 143, loss = 20002732588.96769714\n",
      "Iteration 144, loss = 20002726773.92995071\n",
      "Iteration 145, loss = 20002720998.67084122\n",
      "Iteration 146, loss = 20002715181.28680038\n",
      "Iteration 147, loss = 20002709386.00773239\n",
      "Iteration 148, loss = 20002703585.74174500\n",
      "Iteration 149, loss = 20002697789.19768143\n",
      "Iteration 150, loss = 20002692001.41188049\n",
      "Iteration 151, loss = 20002686227.04346085\n",
      "Iteration 152, loss = 20002680452.62511826\n",
      "Iteration 153, loss = 20002674650.41991806\n",
      "Iteration 154, loss = 20002668937.20837784\n",
      "Iteration 155, loss = 20002663140.99123383\n",
      "Iteration 156, loss = 20002657389.58008194\n",
      "Iteration 157, loss = 20002651605.66139603\n",
      "Iteration 158, loss = 20002645856.91250992\n",
      "Iteration 159, loss = 20002640099.74382019\n",
      "Iteration 160, loss = 20002634361.53234863\n",
      "Iteration 161, loss = 20002628601.37659836\n",
      "Iteration 162, loss = 20002622885.20498276\n",
      "Iteration 163, loss = 20002617097.34563446\n",
      "Iteration 164, loss = 20002611388.80937195\n",
      "Iteration 165, loss = 20002605638.98260880\n",
      "Iteration 166, loss = 20002599844.76107407\n",
      "Iteration 167, loss = 20002594190.31765747\n",
      "Iteration 168, loss = 20002588401.91706848\n",
      "Iteration 169, loss = 20002582680.16261673\n",
      "Iteration 170, loss = 20002576892.15253448\n",
      "Iteration 171, loss = 20002571203.09508133\n",
      "Iteration 172, loss = 20002565434.69863892\n",
      "Iteration 173, loss = 20002559724.22445297\n",
      "Iteration 174, loss = 20002554007.51546860\n",
      "Iteration 175, loss = 20002548270.65914154\n",
      "Iteration 176, loss = 20002542560.97780228\n",
      "Iteration 177, loss = 20002536845.55910492\n",
      "Iteration 178, loss = 20002531123.65009308\n",
      "Iteration 179, loss = 20002525392.24232864\n",
      "Iteration 180, loss = 20002519716.88126755\n",
      "Iteration 181, loss = 20002514042.67863464\n",
      "Iteration 182, loss = 20002508288.22361755\n",
      "Iteration 183, loss = 20002502569.83776093\n",
      "Iteration 184, loss = 20002496909.62125397\n",
      "Iteration 185, loss = 20002491178.84228897\n",
      "Iteration 186, loss = 20002485481.65269470\n",
      "Iteration 187, loss = 20002479795.98093796\n",
      "Iteration 188, loss = 20002474109.24606705\n",
      "Iteration 189, loss = 20002468396.41073990\n",
      "Iteration 190, loss = 20002462717.01012802\n",
      "Iteration 191, loss = 20002457041.97780991\n",
      "Iteration 192, loss = 20002451332.92720795\n",
      "Iteration 193, loss = 20002445694.07442856\n",
      "Iteration 194, loss = 20002439990.81715775\n",
      "Iteration 195, loss = 20002434332.25070953\n",
      "Iteration 196, loss = 20002428658.48408890\n",
      "Iteration 197, loss = 20002422988.53280258\n",
      "Iteration 198, loss = 20002417340.85711670\n",
      "Iteration 199, loss = 20002411620.21329498\n",
      "Iteration 200, loss = 20002405964.15557098\n",
      "Iteration 201, loss = 20002400309.30406189\n",
      "Iteration 202, loss = 20002394658.17547607\n",
      "Iteration 203, loss = 20002388946.25841522\n",
      "Iteration 204, loss = 20002383274.94328308\n",
      "Iteration 205, loss = 20002377607.94843292\n",
      "Iteration 206, loss = 20002371935.97029495\n",
      "Iteration 207, loss = 20002366283.49966049\n",
      "Iteration 208, loss = 20002360614.43185806\n",
      "Iteration 209, loss = 20002354928.21695709\n",
      "Iteration 210, loss = 20002349282.98371506\n",
      "Iteration 211, loss = 20002343641.10721588\n",
      "Iteration 212, loss = 20002337984.99209213\n",
      "Iteration 213, loss = 20002332306.04504395\n",
      "Iteration 214, loss = 20002326688.31787491\n",
      "Iteration 215, loss = 20002321013.43942261\n",
      "Iteration 216, loss = 20002315353.67021942\n",
      "Iteration 217, loss = 20002309718.56647491\n",
      "Iteration 218, loss = 20002304086.98994064\n",
      "Iteration 219, loss = 20002298399.32945633\n",
      "Iteration 220, loss = 20002292752.15227509\n",
      "Iteration 221, loss = 20002287163.34543991\n",
      "Iteration 222, loss = 20002281484.04270172\n",
      "Iteration 223, loss = 20002275878.79726410\n",
      "Iteration 224, loss = 20002270242.40866852\n",
      "Iteration 225, loss = 20002264612.20368195\n",
      "Iteration 226, loss = 20002259005.76444244\n",
      "Iteration 227, loss = 20002253323.90520859\n",
      "Iteration 228, loss = 20002247724.83641815\n",
      "Iteration 229, loss = 20002242123.74392700\n",
      "Iteration 230, loss = 20002236503.92658615\n",
      "Iteration 231, loss = 20002230827.75122070\n",
      "Iteration 232, loss = 20002225251.57189178\n",
      "Iteration 233, loss = 20002219565.25083542\n",
      "Iteration 234, loss = 20002213980.19900894\n",
      "Iteration 235, loss = 20002208361.10023499\n",
      "Iteration 236, loss = 20002202725.98285294\n",
      "Iteration 237, loss = 20002197119.26340103\n",
      "Iteration 238, loss = 20002191511.55549622\n",
      "Iteration 239, loss = 20002185883.37982559\n",
      "Iteration 240, loss = 20002180219.11206818\n",
      "Iteration 241, loss = 20002174650.38021088\n",
      "Iteration 242, loss = 20002169025.41361237\n",
      "Iteration 243, loss = 20002163450.98032761\n",
      "Iteration 244, loss = 20002157815.58401871\n",
      "Iteration 245, loss = 20002152205.37470627\n",
      "Iteration 246, loss = 20002146597.60406494\n",
      "Iteration 247, loss = 20002140986.38055420\n",
      "Iteration 248, loss = 20002135353.01945496\n",
      "Iteration 249, loss = 20002129765.40555954\n",
      "Iteration 250, loss = 20002124163.48004532\n",
      "Iteration 251, loss = 20002118581.22110748\n",
      "Iteration 252, loss = 20002112949.34684753\n",
      "Iteration 253, loss = 20002107338.73767471\n",
      "Iteration 254, loss = 20002101766.66800690\n",
      "Iteration 255, loss = 20002096114.99948120\n",
      "Iteration 256, loss = 20002090562.48728180\n",
      "Iteration 257, loss = 20002084961.14988708\n",
      "Iteration 258, loss = 20002079314.62020874\n",
      "Iteration 259, loss = 20002073743.08898926\n",
      "Iteration 260, loss = 20002068118.45934677\n",
      "Iteration 261, loss = 20002062532.75656128\n",
      "Iteration 262, loss = 20002056940.61034012\n",
      "Iteration 263, loss = 20002051312.17598343\n",
      "Iteration 264, loss = 20002045738.23748398\n",
      "Iteration 265, loss = 20002040130.90255737\n",
      "Iteration 266, loss = 20002034562.13842773\n",
      "Iteration 267, loss = 20002028942.68888092\n",
      "Iteration 268, loss = 20002023367.99224091\n",
      "Iteration 269, loss = 20002017747.04541016\n",
      "Iteration 270, loss = 20002012189.68470764\n",
      "Iteration 271, loss = 20002006624.58370590\n",
      "Iteration 272, loss = 20002001032.08693314\n",
      "Iteration 273, loss = 20001995420.88602829\n",
      "Iteration 274, loss = 20001989860.55846405\n",
      "Iteration 275, loss = 20001984293.73943710\n",
      "Iteration 276, loss = 20001978675.66819382\n",
      "Iteration 277, loss = 20001973101.37260818\n",
      "Iteration 278, loss = 20001967538.61640167\n",
      "Iteration 279, loss = 20001961940.16262054\n",
      "Iteration 280, loss = 20001956362.21575165\n",
      "Iteration 281, loss = 20001950760.73677444\n",
      "Iteration 282, loss = 20001945229.97395325\n",
      "Iteration 283, loss = 20001939669.98341751\n",
      "Iteration 284, loss = 20001934062.59311676\n",
      "Iteration 285, loss = 20001928501.39961243\n",
      "Iteration 286, loss = 20001922959.35427856\n",
      "Iteration 287, loss = 20001917373.05058289\n",
      "Iteration 288, loss = 20001911788.96070099\n",
      "Iteration 289, loss = 20001906246.35366058\n",
      "Iteration 290, loss = 20001900674.67667389\n",
      "Iteration 291, loss = 20001895103.92016983\n",
      "Iteration 292, loss = 20001889520.46465683\n",
      "Iteration 293, loss = 20001883995.22744370\n",
      "Iteration 294, loss = 20001878468.40972137\n",
      "Iteration 295, loss = 20001872847.62236023\n",
      "Iteration 296, loss = 20001867274.16618347\n",
      "Iteration 297, loss = 20001861708.72475815\n",
      "Iteration 298, loss = 20001856201.90789795\n",
      "Iteration 299, loss = 20001850573.67568970\n",
      "Iteration 300, loss = 20001845043.73049927\n",
      "Iteration 301, loss = 20001839465.68492126\n",
      "Iteration 302, loss = 20001833911.62669373\n",
      "Iteration 303, loss = 20001828290.91625214\n",
      "Iteration 304, loss = 20001822766.10374451\n",
      "Iteration 305, loss = 20001817176.51832581\n",
      "Iteration 306, loss = 20001811613.12511826\n",
      "Iteration 307, loss = 20001806078.24168015\n",
      "Iteration 308, loss = 20001800495.40791321\n",
      "Iteration 309, loss = 20001794923.14888000\n",
      "Iteration 310, loss = 20001789342.25639343\n",
      "Iteration 311, loss = 20001783810.82670593\n",
      "Iteration 312, loss = 20001778188.43983078\n",
      "Iteration 313, loss = 20001772668.26725006\n",
      "Iteration 314, loss = 20001767101.72129822\n",
      "Iteration 315, loss = 20001761523.97078323\n",
      "Iteration 316, loss = 20001755957.27017593\n",
      "Iteration 317, loss = 20001750428.01643372\n",
      "Iteration 318, loss = 20001744799.03195953\n",
      "Iteration 319, loss = 20001739269.63397217\n",
      "Iteration 320, loss = 20001733730.09517670\n",
      "Iteration 321, loss = 20001728118.80986404\n",
      "Iteration 322, loss = 20001722592.08979797\n",
      "Iteration 323, loss = 20001717025.61611557\n",
      "Iteration 324, loss = 20001711459.59481812\n",
      "Iteration 325, loss = 20001705918.92950439\n",
      "Iteration 326, loss = 20001700328.19623184\n",
      "Iteration 327, loss = 20001694778.22008896\n",
      "Iteration 328, loss = 20001689220.83646393\n",
      "Iteration 329, loss = 20001683683.57170486\n",
      "Iteration 330, loss = 20001678099.75811768\n",
      "Iteration 331, loss = 20001672552.17538834\n",
      "Iteration 332, loss = 20001667002.20821381\n",
      "Iteration 333, loss = 20001661463.00822830\n",
      "Iteration 334, loss = 20001655883.33733368\n",
      "Iteration 335, loss = 20001650335.15813446\n",
      "Iteration 336, loss = 20001644795.61706924\n",
      "Iteration 337, loss = 20001639275.40204239\n",
      "Iteration 338, loss = 20001633708.86878967\n",
      "Iteration 339, loss = 20001628126.29534531\n",
      "Iteration 340, loss = 20001622586.46026993\n",
      "Iteration 341, loss = 20001617075.86596298\n",
      "Iteration 342, loss = 20001611526.63072968\n",
      "Iteration 343, loss = 20001605909.40365601\n",
      "Iteration 344, loss = 20001600381.03493881\n",
      "Iteration 345, loss = 20001594840.28611374\n",
      "Iteration 346, loss = 20001589299.46249390\n",
      "Iteration 347, loss = 20001583720.88573837\n",
      "Iteration 348, loss = 20001578191.11230087\n",
      "Iteration 349, loss = 20001572617.07078934\n",
      "Iteration 350, loss = 20001567102.78955078\n",
      "Iteration 351, loss = 20001561542.19588470\n",
      "Iteration 352, loss = 20001555973.68028259\n",
      "Iteration 353, loss = 20001550437.95777512\n",
      "Iteration 354, loss = 20001544901.56544495\n",
      "Iteration 355, loss = 20001539343.00984192\n",
      "Iteration 356, loss = 20001533771.01996613\n",
      "Iteration 357, loss = 20001528256.78570175\n",
      "Iteration 358, loss = 20001522691.07542038\n",
      "Iteration 359, loss = 20001517150.16162491\n",
      "Iteration 360, loss = 20001511608.48435974\n",
      "Iteration 361, loss = 20001506044.70545578\n",
      "Iteration 362, loss = 20001500519.52535248\n",
      "Iteration 363, loss = 20001494914.28888321\n",
      "Iteration 364, loss = 20001489445.93055344\n",
      "Iteration 365, loss = 20001483880.39258575\n",
      "Iteration 366, loss = 20001478340.48946381\n",
      "Iteration 367, loss = 20001472791.63889694\n",
      "Iteration 368, loss = 20001467260.13715363\n",
      "Iteration 369, loss = 20001461714.27729416\n",
      "Iteration 370, loss = 20001456174.32232285\n",
      "Iteration 371, loss = 20001450631.39997482\n",
      "Iteration 372, loss = 20001445115.12960052\n",
      "Iteration 373, loss = 20001439580.34716415\n",
      "Iteration 374, loss = 20001434012.13634109\n",
      "Iteration 375, loss = 20001428480.48123169\n",
      "Iteration 376, loss = 20001422945.89894104\n",
      "Iteration 377, loss = 20001417455.98884964\n",
      "Iteration 378, loss = 20001411875.62186050\n",
      "Iteration 379, loss = 20001406352.97109222\n",
      "Iteration 380, loss = 20001400784.05284500\n",
      "Iteration 381, loss = 20001395279.04999542\n",
      "Iteration 382, loss = 20001389723.75799942\n",
      "Iteration 383, loss = 20001384239.99758530\n",
      "Iteration 384, loss = 20001378683.34756851\n",
      "Iteration 385, loss = 20001373101.46902084\n",
      "Iteration 386, loss = 20001367611.57989502\n",
      "Iteration 387, loss = 20001362050.55525589\n",
      "Iteration 388, loss = 20001356554.74925995\n",
      "Iteration 389, loss = 20001351040.07413483\n",
      "Iteration 390, loss = 20001345476.68697357\n",
      "Iteration 391, loss = 20001339944.28689575\n",
      "Iteration 392, loss = 20001334416.32942581\n",
      "Iteration 393, loss = 20001328897.46513748\n",
      "Iteration 394, loss = 20001323384.89197159\n",
      "Iteration 395, loss = 20001317832.84690475\n",
      "Iteration 396, loss = 20001312353.78421783\n",
      "Iteration 397, loss = 20001306790.87821579\n",
      "Iteration 398, loss = 20001301246.21212387\n",
      "Iteration 399, loss = 20001295760.83847046\n",
      "Iteration 400, loss = 20001290230.04486084\n",
      "Iteration 401, loss = 20001284667.82011414\n",
      "Iteration 402, loss = 20001279163.86319733\n",
      "Iteration 403, loss = 20001273629.51215363\n",
      "Iteration 404, loss = 20001268080.80471420\n",
      "Iteration 405, loss = 20001262586.81461334\n",
      "Iteration 406, loss = 20001257082.71680450\n",
      "Iteration 407, loss = 20001251505.00643158\n",
      "Iteration 408, loss = 20001245996.87509155\n",
      "Iteration 409, loss = 20001240482.86824417\n",
      "Iteration 410, loss = 20001234968.86267090\n",
      "Iteration 411, loss = 20001229443.69667435\n",
      "Iteration 412, loss = 20001223930.82823944\n",
      "Iteration 413, loss = 20001218382.68583679\n",
      "Iteration 414, loss = 20001212840.45108795\n",
      "Iteration 415, loss = 20001207382.48706436\n",
      "Iteration 416, loss = 20001201825.27519226\n",
      "Iteration 417, loss = 20001196278.43060303\n",
      "Iteration 418, loss = 20001190785.26428986\n",
      "Iteration 419, loss = 20001185268.04725266\n",
      "Iteration 420, loss = 20001179752.70355988\n",
      "Iteration 421, loss = 20001174220.92399979\n",
      "Iteration 422, loss = 20001168698.59523392\n",
      "Iteration 423, loss = 20001163133.37276077\n",
      "Iteration 424, loss = 20001157643.23719406\n",
      "Iteration 425, loss = 20001152129.34231949\n",
      "Iteration 426, loss = 20001146579.75453949\n",
      "Iteration 427, loss = 20001141029.17887497\n",
      "Iteration 428, loss = 20001135565.29460144\n",
      "Iteration 429, loss = 20001130037.22131729\n",
      "Iteration 430, loss = 20001124492.31102371\n",
      "Iteration 431, loss = 20001118969.63534546\n",
      "Iteration 432, loss = 20001113439.50342178\n",
      "Iteration 433, loss = 20001107936.36419296\n",
      "Iteration 434, loss = 20001102393.49098969\n",
      "Iteration 435, loss = 20001096874.57905197\n",
      "Iteration 436, loss = 20001091361.84560776\n",
      "Iteration 437, loss = 20001085790.01238251\n",
      "Iteration 438, loss = 20001080299.28691864\n",
      "Iteration 439, loss = 20001074781.23050308\n",
      "Iteration 440, loss = 20001069259.65095901\n",
      "Iteration 441, loss = 20001063759.29178238\n",
      "Iteration 442, loss = 20001058204.38716507\n",
      "Iteration 443, loss = 20001052691.05463028\n",
      "Iteration 444, loss = 20001047200.95308304\n",
      "Iteration 445, loss = 20001041667.29496384\n",
      "Iteration 446, loss = 20001036161.70092773\n",
      "Iteration 447, loss = 20001030626.78160858\n",
      "Iteration 448, loss = 20001025084.58232498\n",
      "Iteration 449, loss = 20001019658.95880890\n",
      "Iteration 450, loss = 20001014078.63876724\n",
      "Iteration 451, loss = 20001008599.99315262\n",
      "Iteration 452, loss = 20001003073.26312256\n",
      "Iteration 453, loss = 20000997542.23593521\n",
      "Iteration 454, loss = 20000992048.50254440\n",
      "Iteration 455, loss = 20000986520.37789154\n",
      "Iteration 456, loss = 20000981017.48223114\n",
      "Iteration 457, loss = 20000975507.78712845\n",
      "Iteration 458, loss = 20000969996.41047668\n",
      "Iteration 459, loss = 20000964478.63462830\n",
      "Iteration 460, loss = 20000958953.71936798\n",
      "Iteration 461, loss = 20000953473.90421295\n",
      "Iteration 462, loss = 20000947912.71820450\n",
      "Iteration 463, loss = 20000942423.06660843\n",
      "Iteration 464, loss = 20000936913.33364487\n",
      "Iteration 465, loss = 20000931428.02642441\n",
      "Iteration 466, loss = 20000925877.03835297\n",
      "Iteration 467, loss = 20000920363.42604065\n",
      "Iteration 468, loss = 20000914873.51058960\n",
      "Iteration 469, loss = 20000909332.78876877\n",
      "Iteration 470, loss = 20000903860.34354401\n",
      "Iteration 471, loss = 20000898333.77049255\n",
      "Iteration 472, loss = 20000892792.35224915\n",
      "Iteration 473, loss = 20000887314.88878632\n",
      "Iteration 474, loss = 20000881809.35327911\n",
      "Iteration 475, loss = 20000876244.73465347\n",
      "Iteration 476, loss = 20000870741.75300980\n",
      "Iteration 477, loss = 20000865236.28739548\n",
      "Iteration 478, loss = 20000859744.25904465\n",
      "Iteration 479, loss = 20000854218.21603394\n",
      "Iteration 480, loss = 20000848700.48941422\n",
      "Iteration 481, loss = 20000843189.47320938\n",
      "Iteration 482, loss = 20000837665.81597137\n",
      "Iteration 483, loss = 20000832168.39444351\n",
      "Iteration 484, loss = 20000826674.19601822\n",
      "Iteration 485, loss = 20000821142.99395752\n",
      "Iteration 486, loss = 20000815595.28561401\n",
      "Iteration 487, loss = 20000810137.72368240\n",
      "Iteration 488, loss = 20000804600.65699387\n",
      "Iteration 489, loss = 20000799091.15120316\n",
      "Iteration 490, loss = 20000793565.45463181\n",
      "Iteration 491, loss = 20000788092.59041214\n",
      "Iteration 492, loss = 20000782576.93161392\n",
      "Iteration 493, loss = 20000777068.25540924\n",
      "Iteration 494, loss = 20000771499.15581131\n",
      "Iteration 495, loss = 20000766038.58110428\n",
      "Iteration 496, loss = 20000760541.18412781\n",
      "Iteration 497, loss = 20000754990.13983154\n",
      "Iteration 498, loss = 20000749484.40363693\n",
      "Iteration 499, loss = 20000743974.18910980\n",
      "Iteration 500, loss = 20000738451.68224716\n",
      "Iteration 1, loss = 19794269054.53698730\n",
      "Iteration 2, loss = 19794248489.43942261\n",
      "Iteration 3, loss = 19794227955.14358139\n",
      "Iteration 4, loss = 19794206997.08680725\n",
      "Iteration 5, loss = 19794185842.99354172\n",
      "Iteration 6, loss = 19794164209.29293060\n",
      "Iteration 7, loss = 19794142019.31029892\n",
      "Iteration 8, loss = 19794119554.04063416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 19794097055.45854568\n",
      "Iteration 10, loss = 19794075128.83007431\n",
      "Iteration 11, loss = 19794053012.93838120\n",
      "Iteration 12, loss = 19794030756.83156967\n",
      "Iteration 13, loss = 19794007480.32891083\n",
      "Iteration 14, loss = 19793983919.50409698\n",
      "Iteration 15, loss = 19793961456.92123413\n",
      "Iteration 16, loss = 19793940253.30944061\n",
      "Iteration 17, loss = 19793920546.84024429\n",
      "Iteration 18, loss = 19793902028.39733505\n",
      "Iteration 19, loss = 19793883880.07015610\n",
      "Iteration 20, loss = 19793865415.94224167\n",
      "Iteration 21, loss = 19793847358.15456009\n",
      "Iteration 22, loss = 19793828811.49166107\n",
      "Iteration 23, loss = 19793810128.37226868\n",
      "Iteration 24, loss = 19793791830.45661926\n",
      "Iteration 25, loss = 19793775092.88418579\n",
      "Iteration 26, loss = 19793759563.02262497\n",
      "Iteration 27, loss = 19793743896.38222885\n",
      "Iteration 28, loss = 19793728581.65680695\n",
      "Iteration 29, loss = 19793715164.53487778\n",
      "Iteration 30, loss = 19793702474.84241104\n",
      "Iteration 31, loss = 19793689973.36009216\n",
      "Iteration 32, loss = 19793676920.95882034\n",
      "Iteration 33, loss = 19793664246.75496292\n",
      "Iteration 34, loss = 19793652608.16432571\n",
      "Iteration 35, loss = 19793641952.23636246\n",
      "Iteration 36, loss = 19793632118.70263290\n",
      "Iteration 37, loss = 19793622991.27267456\n",
      "Iteration 38, loss = 19793614281.15381241\n",
      "Iteration 39, loss = 19793605839.78036880\n",
      "Iteration 40, loss = 19793597463.28285217\n",
      "Iteration 41, loss = 19793589279.96640396\n",
      "Iteration 42, loss = 19793581096.20804214\n",
      "Iteration 43, loss = 19793573040.14113235\n",
      "Iteration 44, loss = 19793565036.97503281\n",
      "Iteration 45, loss = 19793556981.92960358\n",
      "Iteration 46, loss = 19793548949.93030167\n",
      "Iteration 47, loss = 19793540937.64815140\n",
      "Iteration 48, loss = 19793532984.88420486\n",
      "Iteration 49, loss = 19793525065.47683334\n",
      "Iteration 50, loss = 19793517196.52363586\n",
      "Iteration 51, loss = 19793509321.08242416\n",
      "Iteration 52, loss = 19793501446.92364120\n",
      "Iteration 53, loss = 19793493722.75384903\n",
      "Iteration 54, loss = 19793485960.27692032\n",
      "Iteration 55, loss = 19793478324.06015396\n",
      "Iteration 56, loss = 19793470658.31211472\n",
      "Iteration 57, loss = 19793463020.21435547\n",
      "Iteration 58, loss = 19793455298.75095749\n",
      "Iteration 59, loss = 19793447693.02284241\n",
      "Iteration 60, loss = 19793440214.80804062\n",
      "Iteration 61, loss = 19793432757.75001144\n",
      "Iteration 62, loss = 19793425326.55194855\n",
      "Iteration 63, loss = 19793417952.69353867\n",
      "Iteration 64, loss = 19793410592.72681046\n",
      "Iteration 65, loss = 19793403270.94558334\n",
      "Iteration 66, loss = 19793395988.11643600\n",
      "Iteration 67, loss = 19793388714.38969040\n",
      "Iteration 68, loss = 19793381482.10996246\n",
      "Iteration 69, loss = 19793374270.32819366\n",
      "Iteration 70, loss = 19793367054.11695480\n",
      "Iteration 71, loss = 19793359859.90560150\n",
      "Iteration 72, loss = 19793352769.86532974\n",
      "Iteration 73, loss = 19793345616.71667099\n",
      "Iteration 74, loss = 19793338515.36103058\n",
      "Iteration 75, loss = 19793331417.38257599\n",
      "Iteration 76, loss = 19793324334.13152695\n",
      "Iteration 77, loss = 19793317206.83816910\n",
      "Iteration 78, loss = 19793310180.08076096\n",
      "Iteration 79, loss = 19793303176.74227524\n",
      "Iteration 80, loss = 19793296134.82730484\n",
      "Iteration 81, loss = 19793289089.53200912\n",
      "Iteration 82, loss = 19793282093.15408325\n",
      "Iteration 83, loss = 19793275127.92015839\n",
      "Iteration 84, loss = 19793268135.83055115\n",
      "Iteration 85, loss = 19793261160.20377350\n",
      "Iteration 86, loss = 19793254209.91645050\n",
      "Iteration 87, loss = 19793247294.35388947\n",
      "Iteration 88, loss = 19793240378.31385040\n",
      "Iteration 89, loss = 19793233434.88030624\n",
      "Iteration 90, loss = 19793226511.94511032\n",
      "Iteration 91, loss = 19793219609.04036713\n",
      "Iteration 92, loss = 19793212693.09682083\n",
      "Iteration 93, loss = 19793205890.99290085\n",
      "Iteration 94, loss = 19793199079.43540573\n",
      "Iteration 95, loss = 19793192238.83781815\n",
      "Iteration 96, loss = 19793185447.02454376\n",
      "Iteration 97, loss = 19793178607.69540787\n",
      "Iteration 98, loss = 19793171790.96300507\n",
      "Iteration 99, loss = 19793164958.11994934\n",
      "Iteration 100, loss = 19793158158.98300171\n",
      "Iteration 101, loss = 19793151326.21227646\n",
      "Iteration 102, loss = 19793144584.60265732\n",
      "Iteration 103, loss = 19793137766.72858810\n",
      "Iteration 104, loss = 19793131008.89531326\n",
      "Iteration 105, loss = 19793124279.04704285\n",
      "Iteration 106, loss = 19793117524.28786469\n",
      "Iteration 107, loss = 19793110816.15357590\n",
      "Iteration 108, loss = 19793104032.50511551\n",
      "Iteration 109, loss = 19793097286.43173981\n",
      "Iteration 110, loss = 19793090587.90962982\n",
      "Iteration 111, loss = 19793083856.37355042\n",
      "Iteration 112, loss = 19793077147.73536301\n",
      "Iteration 113, loss = 19793070444.99312210\n",
      "Iteration 114, loss = 19793063777.23312759\n",
      "Iteration 115, loss = 19793057056.23283768\n",
      "Iteration 116, loss = 19793050402.50309372\n",
      "Iteration 117, loss = 19793043764.63767624\n",
      "Iteration 118, loss = 19793037080.35364532\n",
      "Iteration 119, loss = 19793030375.78694916\n",
      "Iteration 120, loss = 19793023686.46091080\n",
      "Iteration 121, loss = 19793016993.71440506\n",
      "Iteration 122, loss = 19793010263.40179443\n",
      "Iteration 123, loss = 19793003609.03688431\n",
      "Iteration 124, loss = 19792996935.21736908\n",
      "Iteration 125, loss = 19792990173.25512695\n",
      "Iteration 126, loss = 19792983549.62393951\n",
      "Iteration 127, loss = 19792976850.05431747\n",
      "Iteration 128, loss = 19792970167.34058380\n",
      "Iteration 129, loss = 19792963512.88362503\n",
      "Iteration 130, loss = 19792956905.05208969\n",
      "Iteration 131, loss = 19792950249.53792953\n",
      "Iteration 132, loss = 19792943622.22774506\n",
      "Iteration 133, loss = 19792936980.67807388\n",
      "Iteration 134, loss = 19792930345.54835510\n",
      "Iteration 135, loss = 19792923719.24362564\n",
      "Iteration 136, loss = 19792917157.34541321\n",
      "Iteration 137, loss = 19792910500.24776459\n",
      "Iteration 138, loss = 19792903873.36993408\n",
      "Iteration 139, loss = 19792897271.11190796\n",
      "Iteration 140, loss = 19792890725.34704971\n",
      "Iteration 141, loss = 19792884130.27164841\n",
      "Iteration 142, loss = 19792877561.49340820\n",
      "Iteration 143, loss = 19792870999.44849777\n",
      "Iteration 144, loss = 19792864397.59285736\n",
      "Iteration 145, loss = 19792857896.65503311\n",
      "Iteration 146, loss = 19792851241.23017883\n",
      "Iteration 147, loss = 19792844636.46912003\n",
      "Iteration 148, loss = 19792838051.68273926\n",
      "Iteration 149, loss = 19792831510.53850174\n",
      "Iteration 150, loss = 19792824923.77342987\n",
      "Iteration 151, loss = 19792818336.15733337\n",
      "Iteration 152, loss = 19792811693.24534988\n",
      "Iteration 153, loss = 19792805187.82206345\n",
      "Iteration 154, loss = 19792798579.41383362\n",
      "Iteration 155, loss = 19792792043.17748642\n",
      "Iteration 156, loss = 19792785450.39886475\n",
      "Iteration 157, loss = 19792778871.87907028\n",
      "Iteration 158, loss = 19792772315.04959869\n",
      "Iteration 159, loss = 19792765728.75432968\n",
      "Iteration 160, loss = 19792759205.64832687\n",
      "Iteration 161, loss = 19792752721.05734253\n",
      "Iteration 162, loss = 19792746141.32421875\n",
      "Iteration 163, loss = 19792739640.28607178\n",
      "Iteration 164, loss = 19792733109.88216019\n",
      "Iteration 165, loss = 19792726613.36991501\n",
      "Iteration 166, loss = 19792720120.71399307\n",
      "Iteration 167, loss = 19792713590.99185944\n",
      "Iteration 168, loss = 19792707061.55695343\n",
      "Iteration 169, loss = 19792700527.15248108\n",
      "Iteration 170, loss = 19792694025.99289322\n",
      "Iteration 171, loss = 19792687476.96243286\n",
      "Iteration 172, loss = 19792681027.93553162\n",
      "Iteration 173, loss = 19792674483.74400711\n",
      "Iteration 174, loss = 19792667963.00094223\n",
      "Iteration 175, loss = 19792661425.76945114\n",
      "Iteration 176, loss = 19792654858.04534531\n",
      "Iteration 177, loss = 19792648350.65179825\n",
      "Iteration 178, loss = 19792641803.74830627\n",
      "Iteration 179, loss = 19792635307.67475510\n",
      "Iteration 180, loss = 19792628793.01445007\n",
      "Iteration 181, loss = 19792622324.31504822\n",
      "Iteration 182, loss = 19792615786.67442322\n",
      "Iteration 183, loss = 19792609238.59925461\n",
      "Iteration 184, loss = 19792602767.32646179\n",
      "Iteration 185, loss = 19792596242.01941299\n",
      "Iteration 186, loss = 19792589703.30795288\n",
      "Iteration 187, loss = 19792583198.39511871\n",
      "Iteration 188, loss = 19792576722.30179977\n",
      "Iteration 189, loss = 19792570221.25282288\n",
      "Iteration 190, loss = 19792563736.95743561\n",
      "Iteration 191, loss = 19792557220.13201523\n",
      "Iteration 192, loss = 19792550675.21896744\n",
      "Iteration 193, loss = 19792544179.90443420\n",
      "Iteration 194, loss = 19792537731.88018036\n",
      "Iteration 195, loss = 19792531170.02046204\n",
      "Iteration 196, loss = 19792524683.37276077\n",
      "Iteration 197, loss = 19792518190.95595169\n",
      "Iteration 198, loss = 19792511701.20824432\n",
      "Iteration 199, loss = 19792505236.67753220\n",
      "Iteration 200, loss = 19792498781.05744934\n",
      "Iteration 201, loss = 19792492303.09866333\n",
      "Iteration 202, loss = 19792485881.05552292\n",
      "Iteration 203, loss = 19792479374.61788559\n",
      "Iteration 204, loss = 19792472974.90548706\n",
      "Iteration 205, loss = 19792466532.89034271\n",
      "Iteration 206, loss = 19792460088.34241867\n",
      "Iteration 207, loss = 19792453643.90055847\n",
      "Iteration 208, loss = 19792447212.69827652\n",
      "Iteration 209, loss = 19792440699.16382217\n",
      "Iteration 210, loss = 19792434231.13746262\n",
      "Iteration 211, loss = 19792427789.21700287\n",
      "Iteration 212, loss = 19792421291.47819519\n",
      "Iteration 213, loss = 19792414828.99377823\n",
      "Iteration 214, loss = 19792408397.41379547\n",
      "Iteration 215, loss = 19792401982.45708084\n",
      "Iteration 216, loss = 19792395579.58462143\n",
      "Iteration 217, loss = 19792389128.81291199\n",
      "Iteration 218, loss = 19792382741.73994064\n",
      "Iteration 219, loss = 19792376268.41485977\n",
      "Iteration 220, loss = 19792369840.39030457\n",
      "Iteration 221, loss = 19792363444.62553406\n",
      "Iteration 222, loss = 19792356895.79118729\n",
      "Iteration 223, loss = 19792350531.37177277\n",
      "Iteration 224, loss = 19792344123.55269623\n",
      "Iteration 225, loss = 19792337694.33081055\n",
      "Iteration 226, loss = 19792331294.49148941\n",
      "Iteration 227, loss = 19792324908.23649597\n",
      "Iteration 228, loss = 19792318420.74040985\n",
      "Iteration 229, loss = 19792311972.21206665\n",
      "Iteration 230, loss = 19792305563.22282791\n",
      "Iteration 231, loss = 19792299074.18458939\n",
      "Iteration 232, loss = 19792292618.01284409\n",
      "Iteration 233, loss = 19792286125.26477814\n",
      "Iteration 234, loss = 19792279663.14387894\n",
      "Iteration 235, loss = 19792273216.04624176\n",
      "Iteration 236, loss = 19792266713.33904266\n",
      "Iteration 237, loss = 19792260261.30232239\n",
      "Iteration 238, loss = 19792253795.31851959\n",
      "Iteration 239, loss = 19792247377.02447510\n",
      "Iteration 240, loss = 19792240924.89855957\n",
      "Iteration 241, loss = 19792234482.74991608\n",
      "Iteration 242, loss = 19792227971.86699295\n",
      "Iteration 243, loss = 19792221493.53014374\n",
      "Iteration 244, loss = 19792215086.67454147\n",
      "Iteration 245, loss = 19792208626.70048141\n",
      "Iteration 246, loss = 19792202186.42813873\n",
      "Iteration 247, loss = 19792195782.61138153\n",
      "Iteration 248, loss = 19792189303.79151154\n",
      "Iteration 249, loss = 19792182901.03905487\n",
      "Iteration 250, loss = 19792176425.90731430\n",
      "Iteration 251, loss = 19792170012.76910400\n",
      "Iteration 252, loss = 19792163553.35148239\n",
      "Iteration 253, loss = 19792157185.23190689\n",
      "Iteration 254, loss = 19792150761.52018356\n",
      "Iteration 255, loss = 19792144389.83481216\n",
      "Iteration 256, loss = 19792137972.02684021\n",
      "Iteration 257, loss = 19792131633.39273834\n",
      "Iteration 258, loss = 19792125227.05204010\n",
      "Iteration 259, loss = 19792118849.21048355\n",
      "Iteration 260, loss = 19792112435.84321213\n",
      "Iteration 261, loss = 19792106041.22530365\n",
      "Iteration 262, loss = 19792099666.92073441\n",
      "Iteration 263, loss = 19792093175.88640213\n",
      "Iteration 264, loss = 19792086834.06773376\n",
      "Iteration 265, loss = 19792080331.93458176\n",
      "Iteration 266, loss = 19792073919.92787933\n",
      "Iteration 267, loss = 19792067448.32333374\n",
      "Iteration 268, loss = 19792061016.07343292\n",
      "Iteration 269, loss = 19792054585.44880295\n",
      "Iteration 270, loss = 19792048206.98060226\n",
      "Iteration 271, loss = 19792041706.42173004\n",
      "Iteration 272, loss = 19792035260.15402985\n",
      "Iteration 273, loss = 19792028796.67511368\n",
      "Iteration 274, loss = 19792022360.66888046\n",
      "Iteration 275, loss = 19792015947.71748352\n",
      "Iteration 276, loss = 19792009552.91727448\n",
      "Iteration 277, loss = 19792003146.80796432\n",
      "Iteration 278, loss = 19791996776.18640137\n",
      "Iteration 279, loss = 19791990359.63444138\n",
      "Iteration 280, loss = 19791983969.08247375\n",
      "Iteration 281, loss = 19791977562.25708771\n",
      "Iteration 282, loss = 19791971182.94723511\n",
      "Iteration 283, loss = 19791964767.42660904\n",
      "Iteration 284, loss = 19791958375.73083878\n",
      "Iteration 285, loss = 19791951955.80920410\n",
      "Iteration 286, loss = 19791945578.78969574\n",
      "Iteration 287, loss = 19791939101.84560394\n",
      "Iteration 288, loss = 19791932656.46181107\n",
      "Iteration 289, loss = 19791926260.66939163\n",
      "Iteration 290, loss = 19791919853.67515182\n",
      "Iteration 291, loss = 19791913421.85640717\n",
      "Iteration 292, loss = 19791906970.41826630\n",
      "Iteration 293, loss = 19791900570.69606018\n",
      "Iteration 294, loss = 19791894151.77220917\n",
      "Iteration 295, loss = 19791887667.37627411\n",
      "Iteration 296, loss = 19791881176.98075485\n",
      "Iteration 297, loss = 19791874665.81891251\n",
      "Iteration 298, loss = 19791868081.77824783\n",
      "Iteration 299, loss = 19791861367.32254410\n",
      "Iteration 300, loss = 19791854454.62101364\n",
      "Iteration 301, loss = 19791847271.55242920\n",
      "Iteration 302, loss = 19791839747.40910721\n",
      "Iteration 303, loss = 19791831943.58343124\n",
      "Iteration 304, loss = 19791824074.32143021\n",
      "Iteration 305, loss = 19791816269.87198639\n",
      "Iteration 306, loss = 19791808539.77369690\n",
      "Iteration 307, loss = 19791800863.54854584\n",
      "Iteration 308, loss = 19791793303.62784195\n",
      "Iteration 309, loss = 19791785847.58211136\n",
      "Iteration 310, loss = 19791778411.83692551\n",
      "Iteration 311, loss = 19791771047.13825989\n",
      "Iteration 312, loss = 19791763833.93221283\n",
      "Iteration 313, loss = 19791756600.25645065\n",
      "Iteration 314, loss = 19791749427.90167618\n",
      "Iteration 315, loss = 19791742220.29634094\n",
      "Iteration 316, loss = 19791735150.01662445\n",
      "Iteration 317, loss = 19791728015.33716965\n",
      "Iteration 318, loss = 19791720955.29710770\n",
      "Iteration 319, loss = 19791713869.56748962\n",
      "Iteration 320, loss = 19791706805.27876663\n",
      "Iteration 321, loss = 19791699795.31983566\n",
      "Iteration 322, loss = 19791692797.31823349\n",
      "Iteration 323, loss = 19791685842.66241074\n",
      "Iteration 324, loss = 19791678892.64104462\n",
      "Iteration 325, loss = 19791671906.52072906\n",
      "Iteration 326, loss = 19791664905.27146912\n",
      "Iteration 327, loss = 19791657969.85155869\n",
      "Iteration 328, loss = 19791650990.31595993\n",
      "Iteration 329, loss = 19791644043.09374619\n",
      "Iteration 330, loss = 19791637138.45409012\n",
      "Iteration 331, loss = 19791630211.30720139\n",
      "Iteration 332, loss = 19791623304.67837143\n",
      "Iteration 333, loss = 19791616475.76507950\n",
      "Iteration 334, loss = 19791609557.05953979\n",
      "Iteration 335, loss = 19791602695.71676636\n",
      "Iteration 336, loss = 19791595823.22821426\n",
      "Iteration 337, loss = 19791588942.27031326\n",
      "Iteration 338, loss = 19791582076.02352905\n",
      "Iteration 339, loss = 19791575269.94922256\n",
      "Iteration 340, loss = 19791568400.30795670\n",
      "Iteration 341, loss = 19791561572.72097397\n",
      "Iteration 342, loss = 19791554681.55462646\n",
      "Iteration 343, loss = 19791547883.20118332\n",
      "Iteration 344, loss = 19791540995.63703156\n",
      "Iteration 345, loss = 19791534142.67685318\n",
      "Iteration 346, loss = 19791527303.29838943\n",
      "Iteration 347, loss = 19791520534.11202240\n",
      "Iteration 348, loss = 19791513688.12250137\n",
      "Iteration 349, loss = 19791506900.75300980\n",
      "Iteration 350, loss = 19791500107.04279327\n",
      "Iteration 351, loss = 19791493325.50919342\n",
      "Iteration 352, loss = 19791486590.28739929\n",
      "Iteration 353, loss = 19791479846.44596863\n",
      "Iteration 354, loss = 19791473092.63882446\n",
      "Iteration 355, loss = 19791466358.48344803\n",
      "Iteration 356, loss = 19791459579.64172745\n",
      "Iteration 357, loss = 19791452828.10503387\n",
      "Iteration 358, loss = 19791446061.68038177\n",
      "Iteration 359, loss = 19791439279.08586502\n",
      "Iteration 360, loss = 19791432508.30952835\n",
      "Iteration 361, loss = 19791425731.02322006\n",
      "Iteration 362, loss = 19791419019.28521729\n",
      "Iteration 363, loss = 19791412262.63452530\n",
      "Iteration 364, loss = 19791405551.64189148\n",
      "Iteration 365, loss = 19791398845.13940048\n",
      "Iteration 366, loss = 19791392145.28058624\n",
      "Iteration 367, loss = 19791385435.59606552\n",
      "Iteration 368, loss = 19791378705.12831879\n",
      "Iteration 369, loss = 19791372015.84188461\n",
      "Iteration 370, loss = 19791365303.11747360\n",
      "Iteration 371, loss = 19791358601.43939972\n",
      "Iteration 372, loss = 19791351917.13485718\n",
      "Iteration 373, loss = 19791345243.21845245\n",
      "Iteration 374, loss = 19791338466.81971359\n",
      "Iteration 375, loss = 19791331803.50468826\n",
      "Iteration 376, loss = 19791325075.88997269\n",
      "Iteration 377, loss = 19791318418.82339478\n",
      "Iteration 378, loss = 19791311736.45717239\n",
      "Iteration 379, loss = 19791305019.63072586\n",
      "Iteration 380, loss = 19791298331.08359146\n",
      "Iteration 381, loss = 19791291669.28730392\n",
      "Iteration 382, loss = 19791284962.44372177\n",
      "Iteration 383, loss = 19791278276.02581406\n",
      "Iteration 384, loss = 19791271632.41226196\n",
      "Iteration 385, loss = 19791264911.43796158\n",
      "Iteration 386, loss = 19791258211.38677216\n",
      "Iteration 387, loss = 19791251494.74746704\n",
      "Iteration 388, loss = 19791244857.64047241\n",
      "Iteration 389, loss = 19791238176.59545135\n",
      "Iteration 390, loss = 19791231451.48766708\n",
      "Iteration 391, loss = 19791224797.37992477\n",
      "Iteration 392, loss = 19791218166.07915115\n",
      "Iteration 393, loss = 19791211485.96202087\n",
      "Iteration 394, loss = 19791204775.88878632\n",
      "Iteration 395, loss = 19791198129.19922638\n",
      "Iteration 396, loss = 19791191509.00589752\n",
      "Iteration 397, loss = 19791184870.91567993\n",
      "Iteration 398, loss = 19791178203.28748703\n",
      "Iteration 399, loss = 19791171516.48068237\n",
      "Iteration 400, loss = 19791164843.22055054\n",
      "Iteration 401, loss = 19791158129.61742401\n",
      "Iteration 402, loss = 19791151489.55874252\n",
      "Iteration 403, loss = 19791144800.89617920\n",
      "Iteration 404, loss = 19791138217.74720383\n",
      "Iteration 405, loss = 19791131566.69906616\n",
      "Iteration 406, loss = 19791124950.10839844\n",
      "Iteration 407, loss = 19791118235.03416824\n",
      "Iteration 408, loss = 19791111605.54536057\n",
      "Iteration 409, loss = 19791104933.09978104\n",
      "Iteration 410, loss = 19791098294.35632324\n",
      "Iteration 411, loss = 19791091658.21517944\n",
      "Iteration 412, loss = 19791084949.58385468\n",
      "Iteration 413, loss = 19791078362.82568359\n",
      "Iteration 414, loss = 19791071680.90172577\n",
      "Iteration 415, loss = 19791065015.82416916\n",
      "Iteration 416, loss = 19791058416.87006760\n",
      "Iteration 417, loss = 19791051779.40731049\n",
      "Iteration 418, loss = 19791045087.18509674\n",
      "Iteration 419, loss = 19791038414.92204285\n",
      "Iteration 420, loss = 19791031733.80609131\n",
      "Iteration 421, loss = 19791025041.28236389\n",
      "Iteration 422, loss = 19791018408.99660110\n",
      "Iteration 423, loss = 19791011747.11933136\n",
      "Iteration 424, loss = 19791005154.25907516\n",
      "Iteration 425, loss = 19790998493.97541809\n",
      "Iteration 426, loss = 19790991872.67263794\n",
      "Iteration 427, loss = 19790985198.73213196\n",
      "Iteration 428, loss = 19790978573.90269852\n",
      "Iteration 429, loss = 19790971938.89786530\n",
      "Iteration 430, loss = 19790965273.54623413\n",
      "Iteration 431, loss = 19790958582.39152145\n",
      "Iteration 432, loss = 19790951856.38762283\n",
      "Iteration 433, loss = 19790945145.06199265\n",
      "Iteration 434, loss = 19790938291.61786270\n",
      "Iteration 435, loss = 19790931372.68251801\n",
      "Iteration 436, loss = 19790924263.82872772\n",
      "Iteration 437, loss = 19790916758.61970520\n",
      "Iteration 438, loss = 19790908650.01919174\n",
      "Iteration 439, loss = 19790900073.59401321\n",
      "Iteration 440, loss = 19790891356.12651443\n",
      "Iteration 441, loss = 19790882694.46271896\n",
      "Iteration 442, loss = 19790874466.56333160\n",
      "Iteration 443, loss = 19790866352.74222183\n",
      "Iteration 444, loss = 19790858385.35309219\n",
      "Iteration 445, loss = 19790850521.24035645\n",
      "Iteration 446, loss = 19790842738.41640472\n",
      "Iteration 447, loss = 19790835110.71276093\n",
      "Iteration 448, loss = 19790827502.96112442\n",
      "Iteration 449, loss = 19790819924.16075897\n",
      "Iteration 450, loss = 19790812425.59047699\n",
      "Iteration 451, loss = 19790804965.91987991\n",
      "Iteration 452, loss = 19790797569.31127930\n",
      "Iteration 453, loss = 19790790230.75537872\n",
      "Iteration 454, loss = 19790782834.60470581\n",
      "Iteration 455, loss = 19790775532.41251373\n",
      "Iteration 456, loss = 19790768243.13213348\n",
      "Iteration 457, loss = 19790760954.23341751\n",
      "Iteration 458, loss = 19790753683.75335312\n",
      "Iteration 459, loss = 19790746483.73947906\n",
      "Iteration 460, loss = 19790739230.35081482\n",
      "Iteration 461, loss = 19790732061.88454056\n",
      "Iteration 462, loss = 19790724825.98603821\n",
      "Iteration 463, loss = 19790717677.12459183\n",
      "Iteration 464, loss = 19790710502.34301758\n",
      "Iteration 465, loss = 19790703340.77005005\n",
      "Iteration 466, loss = 19790696132.73833084\n",
      "Iteration 467, loss = 19790688958.21940231\n",
      "Iteration 468, loss = 19790681796.86638641\n",
      "Iteration 469, loss = 19790674654.71175003\n",
      "Iteration 470, loss = 19790667487.71268463\n",
      "Iteration 471, loss = 19790660398.75475693\n",
      "Iteration 472, loss = 19790653276.08160019\n",
      "Iteration 473, loss = 19790646138.29623413\n",
      "Iteration 474, loss = 19790638958.04121780\n",
      "Iteration 475, loss = 19790631823.72682190\n",
      "Iteration 476, loss = 19790624665.88676834\n",
      "Iteration 477, loss = 19790617522.25406647\n",
      "Iteration 478, loss = 19790610426.61331177\n",
      "Iteration 479, loss = 19790603260.29811859\n",
      "Iteration 480, loss = 19790596149.06546783\n",
      "Iteration 481, loss = 19790589061.53037262\n",
      "Iteration 482, loss = 19790582013.77946472\n",
      "Iteration 483, loss = 19790574877.12155914\n",
      "Iteration 484, loss = 19790567798.83560944\n",
      "Iteration 485, loss = 19790560814.09679413\n",
      "Iteration 486, loss = 19790553736.64760590\n",
      "Iteration 487, loss = 19790546716.32077789\n",
      "Iteration 488, loss = 19790539673.71396637\n",
      "Iteration 489, loss = 19790532641.83321762\n",
      "Iteration 490, loss = 19790525634.72568893\n",
      "Iteration 491, loss = 19790518674.18405151\n",
      "Iteration 492, loss = 19790511692.86813736\n",
      "Iteration 493, loss = 19790504753.79501343\n",
      "Iteration 494, loss = 19790497741.61823654\n",
      "Iteration 495, loss = 19790490786.17328262\n",
      "Iteration 496, loss = 19790483821.74189377\n",
      "Iteration 497, loss = 19790476871.84333038\n",
      "Iteration 498, loss = 19790469893.10756683\n",
      "Iteration 499, loss = 19790462888.42620850\n",
      "Iteration 500, loss = 19790455965.43339920\n",
      "Iteration 1, loss = 19409628253.90039062\n",
      "Iteration 2, loss = 19409606312.75159836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 19409584241.43854523\n",
      "Iteration 4, loss = 19409562986.30669785\n",
      "Iteration 5, loss = 19409542271.09715271\n",
      "Iteration 6, loss = 19409521656.24643326\n",
      "Iteration 7, loss = 19409501150.42877579\n",
      "Iteration 8, loss = 19409479677.08251190\n",
      "Iteration 9, loss = 19409456861.92442322\n",
      "Iteration 10, loss = 19409432014.06605148\n",
      "Iteration 11, loss = 19409406046.41534424\n",
      "Iteration 12, loss = 19409382559.40202332\n",
      "Iteration 13, loss = 19409360634.43935776\n",
      "Iteration 14, loss = 19409338900.80532455\n",
      "Iteration 15, loss = 19409316564.74433517\n",
      "Iteration 16, loss = 19409293435.05387115\n",
      "Iteration 17, loss = 19409268540.47135544\n",
      "Iteration 18, loss = 19409241053.35923386\n",
      "Iteration 19, loss = 19409215679.19063187\n",
      "Iteration 20, loss = 19409193384.48373795\n",
      "Iteration 21, loss = 19409171135.15404892\n",
      "Iteration 22, loss = 19409147683.09044647\n",
      "Iteration 23, loss = 19409125934.73857117\n",
      "Iteration 24, loss = 19409105896.23295212\n",
      "Iteration 25, loss = 19409086375.92275620\n",
      "Iteration 26, loss = 19409068129.24263382\n",
      "Iteration 27, loss = 19409049626.08780670\n",
      "Iteration 28, loss = 19409031416.95186234\n",
      "Iteration 29, loss = 19409014489.80237579\n",
      "Iteration 30, loss = 19409000862.62163162\n",
      "Iteration 31, loss = 19408988025.38831329\n",
      "Iteration 32, loss = 19408974828.31639481\n",
      "Iteration 33, loss = 19408961440.74759674\n",
      "Iteration 34, loss = 19408948511.97874069\n",
      "Iteration 35, loss = 19408936598.23520279\n",
      "Iteration 36, loss = 19408926082.59276581\n",
      "Iteration 37, loss = 19408916162.41888428\n",
      "Iteration 38, loss = 19408906299.05991364\n",
      "Iteration 39, loss = 19408896435.27206802\n",
      "Iteration 40, loss = 19408886990.15496445\n",
      "Iteration 41, loss = 19408877671.44502640\n",
      "Iteration 42, loss = 19408868614.25440979\n",
      "Iteration 43, loss = 19408859567.39448166\n",
      "Iteration 44, loss = 19408850497.96923828\n",
      "Iteration 45, loss = 19408841429.64083862\n",
      "Iteration 46, loss = 19408832616.83871841\n",
      "Iteration 47, loss = 19408823842.36510468\n",
      "Iteration 48, loss = 19408815259.99785233\n",
      "Iteration 49, loss = 19408806628.17020035\n",
      "Iteration 50, loss = 19408798082.80234146\n",
      "Iteration 51, loss = 19408789522.65208817\n",
      "Iteration 52, loss = 19408781070.32551956\n",
      "Iteration 53, loss = 19408772469.09532928\n",
      "Iteration 54, loss = 19408763975.91402054\n",
      "Iteration 55, loss = 19408755500.88622284\n",
      "Iteration 56, loss = 19408747015.34994888\n",
      "Iteration 57, loss = 19408738784.90234756\n",
      "Iteration 58, loss = 19408730501.25540161\n",
      "Iteration 59, loss = 19408722240.28830719\n",
      "Iteration 60, loss = 19408714067.22950745\n",
      "Iteration 61, loss = 19408705900.48598862\n",
      "Iteration 62, loss = 19408697801.63304138\n",
      "Iteration 63, loss = 19408689648.79687500\n",
      "Iteration 64, loss = 19408681556.45848083\n",
      "Iteration 65, loss = 19408673409.99085617\n",
      "Iteration 66, loss = 19408665394.10682678\n",
      "Iteration 67, loss = 19408657342.45530319\n",
      "Iteration 68, loss = 19408649317.53697205\n",
      "Iteration 69, loss = 19408641250.16841888\n",
      "Iteration 70, loss = 19408633240.09721375\n",
      "Iteration 71, loss = 19408625179.37528229\n",
      "Iteration 72, loss = 19408617275.58424377\n",
      "Iteration 73, loss = 19408609389.51391983\n",
      "Iteration 74, loss = 19408601442.01642227\n",
      "Iteration 75, loss = 19408593502.80574799\n",
      "Iteration 76, loss = 19408585608.22029495\n",
      "Iteration 77, loss = 19408577753.47463608\n",
      "Iteration 78, loss = 19408569917.82888412\n",
      "Iteration 79, loss = 19408562124.73507690\n",
      "Iteration 80, loss = 19408554341.59835434\n",
      "Iteration 81, loss = 19408546498.86815643\n",
      "Iteration 82, loss = 19408538802.56834412\n",
      "Iteration 83, loss = 19408531006.19501114\n",
      "Iteration 84, loss = 19408523315.73957062\n",
      "Iteration 85, loss = 19408515513.46181107\n",
      "Iteration 86, loss = 19408507829.07862473\n",
      "Iteration 87, loss = 19408500114.83175659\n",
      "Iteration 88, loss = 19408492444.11128616\n",
      "Iteration 89, loss = 19408484723.93210220\n",
      "Iteration 90, loss = 19408477071.26826477\n",
      "Iteration 91, loss = 19408469373.24366379\n",
      "Iteration 92, loss = 19408461769.00070190\n",
      "Iteration 93, loss = 19408454069.91276550\n",
      "Iteration 94, loss = 19408446433.13679504\n",
      "Iteration 95, loss = 19408438848.82650757\n",
      "Iteration 96, loss = 19408431222.79994965\n",
      "Iteration 97, loss = 19408423536.14014816\n",
      "Iteration 98, loss = 19408415967.50064850\n",
      "Iteration 99, loss = 19408408404.07424927\n",
      "Iteration 100, loss = 19408400792.72807693\n",
      "Iteration 101, loss = 19408393240.00007248\n",
      "Iteration 102, loss = 19408385609.94426346\n",
      "Iteration 103, loss = 19408377978.77032089\n",
      "Iteration 104, loss = 19408370450.95561218\n",
      "Iteration 105, loss = 19408362832.32752609\n",
      "Iteration 106, loss = 19408355223.03212738\n",
      "Iteration 107, loss = 19408347697.24904633\n",
      "Iteration 108, loss = 19408340158.22761154\n",
      "Iteration 109, loss = 19408332596.56644058\n",
      "Iteration 110, loss = 19408325062.89811707\n",
      "Iteration 111, loss = 19408317516.28092957\n",
      "Iteration 112, loss = 19408309984.84685898\n",
      "Iteration 113, loss = 19408302479.89648438\n",
      "Iteration 114, loss = 19408294949.49100876\n",
      "Iteration 115, loss = 19408287466.77000809\n",
      "Iteration 116, loss = 19408279899.00613022\n",
      "Iteration 117, loss = 19408272442.43458557\n",
      "Iteration 118, loss = 19408264934.25097275\n",
      "Iteration 119, loss = 19408257456.95708847\n",
      "Iteration 120, loss = 19408249915.55094147\n",
      "Iteration 121, loss = 19408242462.97394180\n",
      "Iteration 122, loss = 19408234990.39476013\n",
      "Iteration 123, loss = 19408227525.83487320\n",
      "Iteration 124, loss = 19408220085.09297180\n",
      "Iteration 125, loss = 19408212599.67205048\n",
      "Iteration 126, loss = 19408205116.91390991\n",
      "Iteration 127, loss = 19408197692.19002914\n",
      "Iteration 128, loss = 19408190209.50646210\n",
      "Iteration 129, loss = 19408182767.32226944\n",
      "Iteration 130, loss = 19408175318.63706589\n",
      "Iteration 131, loss = 19408167895.49210358\n",
      "Iteration 132, loss = 19408160441.86696243\n",
      "Iteration 133, loss = 19408153004.54935455\n",
      "Iteration 134, loss = 19408145550.61941528\n",
      "Iteration 135, loss = 19408138202.06045914\n",
      "Iteration 136, loss = 19408130710.41702271\n",
      "Iteration 137, loss = 19408123353.01441956\n",
      "Iteration 138, loss = 19408115936.17953491\n",
      "Iteration 139, loss = 19408108450.32800674\n",
      "Iteration 140, loss = 19408101062.05139542\n",
      "Iteration 141, loss = 19408093629.45403290\n",
      "Iteration 142, loss = 19408086265.66733170\n",
      "Iteration 143, loss = 19408078888.11917114\n",
      "Iteration 144, loss = 19408071441.40669250\n",
      "Iteration 145, loss = 19408064058.59614944\n",
      "Iteration 146, loss = 19408056703.79932022\n",
      "Iteration 147, loss = 19408049274.87091446\n",
      "Iteration 148, loss = 19408041864.83044052\n",
      "Iteration 149, loss = 19408034502.52075195\n",
      "Iteration 150, loss = 19408027123.75420380\n",
      "Iteration 151, loss = 19408019748.52947235\n",
      "Iteration 152, loss = 19408012379.34153748\n",
      "Iteration 153, loss = 19408004995.24849319\n",
      "Iteration 154, loss = 19407997649.24348450\n",
      "Iteration 155, loss = 19407990233.62213898\n",
      "Iteration 156, loss = 19407982916.76815796\n",
      "Iteration 157, loss = 19407975517.61272049\n",
      "Iteration 158, loss = 19407968154.41773987\n",
      "Iteration 159, loss = 19407960801.23033524\n",
      "Iteration 160, loss = 19407953429.90138245\n",
      "Iteration 161, loss = 19407946113.98045349\n",
      "Iteration 162, loss = 19407938764.96441650\n",
      "Iteration 163, loss = 19407931398.75948334\n",
      "Iteration 164, loss = 19407924002.60380173\n",
      "Iteration 165, loss = 19407916669.79187012\n",
      "Iteration 166, loss = 19407909355.65626907\n",
      "Iteration 167, loss = 19407902020.07597733\n",
      "Iteration 168, loss = 19407894655.10177994\n",
      "Iteration 169, loss = 19407887343.11313248\n",
      "Iteration 170, loss = 19407879978.23395920\n",
      "Iteration 171, loss = 19407872617.18182755\n",
      "Iteration 172, loss = 19407865379.36024475\n",
      "Iteration 173, loss = 19407857925.68948746\n",
      "Iteration 174, loss = 19407850628.79569244\n",
      "Iteration 175, loss = 19407843291.88135529\n",
      "Iteration 176, loss = 19407835977.95444107\n",
      "Iteration 177, loss = 19407828682.87458801\n",
      "Iteration 178, loss = 19407821307.69046402\n",
      "Iteration 179, loss = 19407814028.92950439\n",
      "Iteration 180, loss = 19407806628.23097610\n",
      "Iteration 181, loss = 19407799351.34079742\n",
      "Iteration 182, loss = 19407792014.83573532\n",
      "Iteration 183, loss = 19407784731.40105438\n",
      "Iteration 184, loss = 19407777358.62477112\n",
      "Iteration 185, loss = 19407770088.78840256\n",
      "Iteration 186, loss = 19407762747.56598663\n",
      "Iteration 187, loss = 19407755414.82525253\n",
      "Iteration 188, loss = 19407748131.44330597\n",
      "Iteration 189, loss = 19407740846.24053574\n",
      "Iteration 190, loss = 19407733516.59296036\n",
      "Iteration 191, loss = 19407726216.68149185\n",
      "Iteration 192, loss = 19407718934.72714233\n",
      "Iteration 193, loss = 19407711588.91658783\n",
      "Iteration 194, loss = 19407704323.32712173\n",
      "Iteration 195, loss = 19407696985.61280823\n",
      "Iteration 196, loss = 19407689731.48508072\n",
      "Iteration 197, loss = 19407682473.26073837\n",
      "Iteration 198, loss = 19407675114.45237732\n",
      "Iteration 199, loss = 19407667886.19690704\n",
      "Iteration 200, loss = 19407660533.60861206\n",
      "Iteration 201, loss = 19407653249.35718536\n",
      "Iteration 202, loss = 19407645995.61207581\n",
      "Iteration 203, loss = 19407638721.18338776\n",
      "Iteration 204, loss = 19407631424.52089691\n",
      "Iteration 205, loss = 19407624146.14379120\n",
      "Iteration 206, loss = 19407616811.51648331\n",
      "Iteration 207, loss = 19407609569.91788101\n",
      "Iteration 208, loss = 19407602285.40790176\n",
      "Iteration 209, loss = 19407595005.55562973\n",
      "Iteration 210, loss = 19407587745.36090088\n",
      "Iteration 211, loss = 19407580474.44191742\n",
      "Iteration 212, loss = 19407573175.15367889\n",
      "Iteration 213, loss = 19407565904.32029724\n",
      "Iteration 214, loss = 19407558629.61943054\n",
      "Iteration 215, loss = 19407551343.31916046\n",
      "Iteration 216, loss = 19407544125.73840332\n",
      "Iteration 217, loss = 19407536837.90369415\n",
      "Iteration 218, loss = 19407529526.64158630\n",
      "Iteration 219, loss = 19407522274.85105896\n",
      "Iteration 220, loss = 19407515057.75882721\n",
      "Iteration 221, loss = 19407507754.73311615\n",
      "Iteration 222, loss = 19407500447.02953339\n",
      "Iteration 223, loss = 19407493247.49716949\n",
      "Iteration 224, loss = 19407485943.57989883\n",
      "Iteration 225, loss = 19407478721.52379608\n",
      "Iteration 226, loss = 19407471439.74633408\n",
      "Iteration 227, loss = 19407464190.01876068\n",
      "Iteration 228, loss = 19407456871.55269241\n",
      "Iteration 229, loss = 19407449665.61806870\n",
      "Iteration 230, loss = 19407442373.17756653\n",
      "Iteration 231, loss = 19407435158.85831451\n",
      "Iteration 232, loss = 19407427852.93975830\n",
      "Iteration 233, loss = 19407420620.93729019\n",
      "Iteration 234, loss = 19407413358.38490295\n",
      "Iteration 235, loss = 19407406119.64617920\n",
      "Iteration 236, loss = 19407398818.01317978\n",
      "Iteration 237, loss = 19407391581.82426453\n",
      "Iteration 238, loss = 19407384327.29082108\n",
      "Iteration 239, loss = 19407377078.44461823\n",
      "Iteration 240, loss = 19407369842.08012009\n",
      "Iteration 241, loss = 19407362592.84263992\n",
      "Iteration 242, loss = 19407355395.87359238\n",
      "Iteration 243, loss = 19407348094.26676178\n",
      "Iteration 244, loss = 19407340848.13759613\n",
      "Iteration 245, loss = 19407333638.97933197\n",
      "Iteration 246, loss = 19407326345.85085297\n",
      "Iteration 247, loss = 19407319082.92346954\n",
      "Iteration 248, loss = 19407311901.47625351\n",
      "Iteration 249, loss = 19407304632.51441956\n",
      "Iteration 250, loss = 19407297379.85351944\n",
      "Iteration 251, loss = 19407290137.97534180\n",
      "Iteration 252, loss = 19407282923.82599640\n",
      "Iteration 253, loss = 19407275656.89936066\n",
      "Iteration 254, loss = 19407268434.66578674\n",
      "Iteration 255, loss = 19407261205.38879013\n",
      "Iteration 256, loss = 19407253966.92232895\n",
      "Iteration 257, loss = 19407246728.78810120\n",
      "Iteration 258, loss = 19407239454.14815140\n",
      "Iteration 259, loss = 19407232276.92720413\n",
      "Iteration 260, loss = 19407224994.80341721\n",
      "Iteration 261, loss = 19407217733.42321777\n",
      "Iteration 262, loss = 19407210570.94268799\n",
      "Iteration 263, loss = 19407203282.91659546\n",
      "Iteration 264, loss = 19407196063.86296844\n",
      "Iteration 265, loss = 19407188891.63837433\n",
      "Iteration 266, loss = 19407181599.68572235\n",
      "Iteration 267, loss = 19407174405.03611755\n",
      "Iteration 268, loss = 19407167141.74671555\n",
      "Iteration 269, loss = 19407159897.78872299\n",
      "Iteration 270, loss = 19407152720.45285797\n",
      "Iteration 271, loss = 19407145447.85396194\n",
      "Iteration 272, loss = 19407138196.71718216\n",
      "Iteration 273, loss = 19407131002.86448288\n",
      "Iteration 274, loss = 19407123741.22705460\n",
      "Iteration 275, loss = 19407116575.10293198\n",
      "Iteration 276, loss = 19407109326.19281006\n",
      "Iteration 277, loss = 19407102042.37671280\n",
      "Iteration 278, loss = 19407094891.12337112\n",
      "Iteration 279, loss = 19407087635.32910919\n",
      "Iteration 280, loss = 19407080376.64440155\n",
      "Iteration 281, loss = 19407073211.32314301\n",
      "Iteration 282, loss = 19407065942.90002823\n",
      "Iteration 283, loss = 19407058703.48343277\n",
      "Iteration 284, loss = 19407051488.32722092\n",
      "Iteration 285, loss = 19407044320.43813705\n",
      "Iteration 286, loss = 19407037054.96480560\n",
      "Iteration 287, loss = 19407029811.94474792\n",
      "Iteration 288, loss = 19407022621.44133759\n",
      "Iteration 289, loss = 19407015420.80427551\n",
      "Iteration 290, loss = 19407008191.31283569\n",
      "Iteration 291, loss = 19407000958.10331345\n",
      "Iteration 292, loss = 19406993754.64402771\n",
      "Iteration 293, loss = 19406986593.01215744\n",
      "Iteration 294, loss = 19406979284.31465912\n",
      "Iteration 295, loss = 19406972105.83640671\n",
      "Iteration 296, loss = 19406964914.34904099\n",
      "Iteration 297, loss = 19406957660.86266708\n",
      "Iteration 298, loss = 19406950500.73476791\n",
      "Iteration 299, loss = 19406943221.34998703\n",
      "Iteration 300, loss = 19406936048.46445084\n",
      "Iteration 301, loss = 19406928804.11743546\n",
      "Iteration 302, loss = 19406921645.36771774\n",
      "Iteration 303, loss = 19406914412.63715363\n",
      "Iteration 304, loss = 19406907167.33940125\n",
      "Iteration 305, loss = 19406900000.77628326\n",
      "Iteration 306, loss = 19406892768.88689423\n",
      "Iteration 307, loss = 19406885526.42732620\n",
      "Iteration 308, loss = 19406878350.21062851\n",
      "Iteration 309, loss = 19406871114.60416412\n",
      "Iteration 310, loss = 19406863955.72240448\n",
      "Iteration 311, loss = 19406856694.40377045\n",
      "Iteration 312, loss = 19406849514.11686707\n",
      "Iteration 313, loss = 19406842322.43624496\n",
      "Iteration 314, loss = 19406835075.80165482\n",
      "Iteration 315, loss = 19406827872.27865982\n",
      "Iteration 316, loss = 19406820694.60065460\n",
      "Iteration 317, loss = 19406813476.85869980\n",
      "Iteration 318, loss = 19406806202.49640656\n",
      "Iteration 319, loss = 19406799065.95862198\n",
      "Iteration 320, loss = 19406791781.91081619\n",
      "Iteration 321, loss = 19406784559.63504410\n",
      "Iteration 322, loss = 19406777314.18431091\n",
      "Iteration 323, loss = 19406769988.49816132\n",
      "Iteration 324, loss = 19406762704.32423401\n",
      "Iteration 325, loss = 19406755235.91584396\n",
      "Iteration 326, loss = 19406747638.45243454\n",
      "Iteration 327, loss = 19406739790.35129166\n",
      "Iteration 328, loss = 19406731522.32004929\n",
      "Iteration 329, loss = 19406722887.93850327\n",
      "Iteration 330, loss = 19406714091.53633499\n",
      "Iteration 331, loss = 19406705303.14945984\n",
      "Iteration 332, loss = 19406696689.58844376\n",
      "Iteration 333, loss = 19406688101.49839783\n",
      "Iteration 334, loss = 19406679712.73500443\n",
      "Iteration 335, loss = 19406671421.46519470\n",
      "Iteration 336, loss = 19406663231.55568314\n",
      "Iteration 337, loss = 19406655067.44195557\n",
      "Iteration 338, loss = 19406646919.98760605\n",
      "Iteration 339, loss = 19406638907.68820953\n",
      "Iteration 340, loss = 19406630850.06642914\n",
      "Iteration 341, loss = 19406622886.45116425\n",
      "Iteration 342, loss = 19406614921.56334305\n",
      "Iteration 343, loss = 19406606950.02938461\n",
      "Iteration 344, loss = 19406599095.05987167\n",
      "Iteration 345, loss = 19406591213.77177811\n",
      "Iteration 346, loss = 19406583320.37908936\n",
      "Iteration 347, loss = 19406575474.86851501\n",
      "Iteration 348, loss = 19406567673.78355408\n",
      "Iteration 349, loss = 19406559832.89936066\n",
      "Iteration 350, loss = 19406552091.75012207\n",
      "Iteration 351, loss = 19406544257.85509872\n",
      "Iteration 352, loss = 19406536462.37813568\n",
      "Iteration 353, loss = 19406528749.78285599\n",
      "Iteration 354, loss = 19406520981.93624115\n",
      "Iteration 355, loss = 19406513262.50899124\n",
      "Iteration 356, loss = 19406505516.86871719\n",
      "Iteration 357, loss = 19406497772.86978531\n",
      "Iteration 358, loss = 19406490065.37857437\n",
      "Iteration 359, loss = 19406482352.87677765\n",
      "Iteration 360, loss = 19406474698.41328049\n",
      "Iteration 361, loss = 19406466962.71116638\n",
      "Iteration 362, loss = 19406459320.09033966\n",
      "Iteration 363, loss = 19406451647.51576614\n",
      "Iteration 364, loss = 19406443942.65805435\n",
      "Iteration 365, loss = 19406436283.11400223\n",
      "Iteration 366, loss = 19406428645.19817734\n",
      "Iteration 367, loss = 19406421009.91843796\n",
      "Iteration 368, loss = 19406413394.27623367\n",
      "Iteration 369, loss = 19406405751.38130569\n",
      "Iteration 370, loss = 19406398075.77802658\n",
      "Iteration 371, loss = 19406390493.78281784\n",
      "Iteration 372, loss = 19406382891.44423676\n",
      "Iteration 373, loss = 19406375219.23614883\n",
      "Iteration 374, loss = 19406367622.93699265\n",
      "Iteration 375, loss = 19406360032.74811554\n",
      "Iteration 376, loss = 19406352419.14245605\n",
      "Iteration 377, loss = 19406344795.76041031\n",
      "Iteration 378, loss = 19406337197.11885834\n",
      "Iteration 379, loss = 19406329625.85947800\n",
      "Iteration 380, loss = 19406322054.24835968\n",
      "Iteration 381, loss = 19406314474.91709518\n",
      "Iteration 382, loss = 19406306852.44196320\n",
      "Iteration 383, loss = 19406299306.90847778\n",
      "Iteration 384, loss = 19406291729.60927963\n",
      "Iteration 385, loss = 19406284125.94163895\n",
      "Iteration 386, loss = 19406276552.89342117\n",
      "Iteration 387, loss = 19406268980.68352509\n",
      "Iteration 388, loss = 19406261383.03272247\n",
      "Iteration 389, loss = 19406253891.84074020\n",
      "Iteration 390, loss = 19406246306.70137787\n",
      "Iteration 391, loss = 19406238736.80527115\n",
      "Iteration 392, loss = 19406231196.67490005\n",
      "Iteration 393, loss = 19406223654.18592453\n",
      "Iteration 394, loss = 19406216124.42339706\n",
      "Iteration 395, loss = 19406208534.59691620\n",
      "Iteration 396, loss = 19406201013.57429504\n",
      "Iteration 397, loss = 19406193471.28632736\n",
      "Iteration 398, loss = 19406185934.85000229\n",
      "Iteration 399, loss = 19406178362.41258240\n",
      "Iteration 400, loss = 19406170855.02413940\n",
      "Iteration 401, loss = 19406163344.09772491\n",
      "Iteration 402, loss = 19406155776.91659927\n",
      "Iteration 403, loss = 19406148290.10520554\n",
      "Iteration 404, loss = 19406140741.83153534\n",
      "Iteration 405, loss = 19406133199.50518799\n",
      "Iteration 406, loss = 19406125667.67079544\n",
      "Iteration 407, loss = 19406118178.66743088\n",
      "Iteration 408, loss = 19406110657.67331314\n",
      "Iteration 409, loss = 19406103127.02181625\n",
      "Iteration 410, loss = 19406095647.87759781\n",
      "Iteration 411, loss = 19406088124.67537689\n",
      "Iteration 412, loss = 19406080558.36589813\n",
      "Iteration 413, loss = 19406073089.03051758\n",
      "Iteration 414, loss = 19406065586.85163879\n",
      "Iteration 415, loss = 19406058056.32549667\n",
      "Iteration 416, loss = 19406050575.48171997\n",
      "Iteration 417, loss = 19406043091.74328613\n",
      "Iteration 418, loss = 19406035532.43331909\n",
      "Iteration 419, loss = 19406028081.77797699\n",
      "Iteration 420, loss = 19406020559.65307236\n",
      "Iteration 421, loss = 19406013093.32711792\n",
      "Iteration 422, loss = 19406005558.42610168\n",
      "Iteration 423, loss = 19405998108.89145279\n",
      "Iteration 424, loss = 19405990644.79882812\n",
      "Iteration 425, loss = 19405983114.81810379\n",
      "Iteration 426, loss = 19405975632.51960754\n",
      "Iteration 427, loss = 19405968099.53371048\n",
      "Iteration 428, loss = 19405960640.18756866\n",
      "Iteration 429, loss = 19405953141.46621704\n",
      "Iteration 430, loss = 19405945645.11131287\n",
      "Iteration 431, loss = 19405938226.91533661\n",
      "Iteration 432, loss = 19405930733.68836594\n",
      "Iteration 433, loss = 19405923271.68453979\n",
      "Iteration 434, loss = 19405915755.78318787\n",
      "Iteration 435, loss = 19405908249.15956116\n",
      "Iteration 436, loss = 19405900805.18164825\n",
      "Iteration 437, loss = 19405893335.04918671\n",
      "Iteration 438, loss = 19405885808.61716080\n",
      "Iteration 439, loss = 19405878383.47773743\n",
      "Iteration 440, loss = 19405870898.65663528\n",
      "Iteration 441, loss = 19405863419.70236206\n",
      "Iteration 442, loss = 19405855967.59458542\n",
      "Iteration 443, loss = 19405848480.96456528\n",
      "Iteration 444, loss = 19405840984.90406799\n",
      "Iteration 445, loss = 19405833535.31903458\n",
      "Iteration 446, loss = 19405826075.69662476\n",
      "Iteration 447, loss = 19405818629.99744415\n",
      "Iteration 448, loss = 19405811185.61694717\n",
      "Iteration 449, loss = 19405803639.72601700\n",
      "Iteration 450, loss = 19405796240.39059067\n",
      "Iteration 451, loss = 19405788749.05748749\n",
      "Iteration 452, loss = 19405781331.80628586\n",
      "Iteration 453, loss = 19405773863.50574493\n",
      "Iteration 454, loss = 19405766413.94531631\n",
      "Iteration 455, loss = 19405758907.21097946\n",
      "Iteration 456, loss = 19405751449.58099747\n",
      "Iteration 457, loss = 19405744052.40333557\n",
      "Iteration 458, loss = 19405736560.85457993\n",
      "Iteration 459, loss = 19405729104.56342316\n",
      "Iteration 460, loss = 19405721673.06373596\n",
      "Iteration 461, loss = 19405714195.90998840\n",
      "Iteration 462, loss = 19405706760.95500946\n",
      "Iteration 463, loss = 19405699283.20447922\n",
      "Iteration 464, loss = 19405691884.76417542\n",
      "Iteration 465, loss = 19405684366.79737473\n",
      "Iteration 466, loss = 19405676950.46204758\n",
      "Iteration 467, loss = 19405669522.24902344\n",
      "Iteration 468, loss = 19405662089.81289291\n",
      "Iteration 469, loss = 19405654581.25088882\n",
      "Iteration 470, loss = 19405647153.62598419\n",
      "Iteration 471, loss = 19405639743.44627380\n",
      "Iteration 472, loss = 19405632263.59690094\n",
      "Iteration 473, loss = 19405624799.76499557\n",
      "Iteration 474, loss = 19405617417.82119370\n",
      "Iteration 475, loss = 19405609978.38596344\n",
      "Iteration 476, loss = 19405602475.45623016\n",
      "Iteration 477, loss = 19405595045.80258560\n",
      "Iteration 478, loss = 19405587630.24958038\n",
      "Iteration 479, loss = 19405580190.45598602\n",
      "Iteration 480, loss = 19405572765.48207474\n",
      "Iteration 481, loss = 19405565281.09762192\n",
      "Iteration 482, loss = 19405557929.49484634\n",
      "Iteration 483, loss = 19405550429.90213013\n",
      "Iteration 484, loss = 19405543018.83916092\n",
      "Iteration 485, loss = 19405535553.82550430\n",
      "Iteration 486, loss = 19405528133.91617966\n",
      "Iteration 487, loss = 19405520715.06754303\n",
      "Iteration 488, loss = 19405513273.56412125\n",
      "Iteration 489, loss = 19405505804.46710968\n",
      "Iteration 490, loss = 19405498399.39429474\n",
      "Iteration 491, loss = 19405490965.05854416\n",
      "Iteration 492, loss = 19405483495.99044037\n",
      "Iteration 493, loss = 19405476124.28691483\n",
      "Iteration 494, loss = 19405468655.81430435\n",
      "Iteration 495, loss = 19405461180.34354019\n",
      "Iteration 496, loss = 19405453825.10490417\n",
      "Iteration 497, loss = 19405446375.11524200\n",
      "Iteration 498, loss = 19405438923.34381104\n",
      "Iteration 499, loss = 19405431470.35545731\n",
      "Iteration 500, loss = 19405424082.67898941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear rangos de tamaños de entrenamiento\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calcular las curvas de aprendizaje\n",
    "train_sizes, train_scores, test_scores = learning_curve(model1, X, y, train_sizes=train_sizes, cv=5)\n",
    "\n",
    "# Calcular las medias y desviaciones estándar de los puntajes de entrenamiento y prueba\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Dibujar las curvas de aprendizaje\n",
    "plt.plot(train_sizes, train_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='#DDDDDD')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='#DDDDDD')\n",
    "\n",
    "# Crear la leyenda y los títulos\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
