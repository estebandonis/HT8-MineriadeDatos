{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as smd\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.diagnostic as diag\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.compose import make_column_selector as selector #Para seleccionar de forma automática las variables numéricas y categóricas\n",
    "from sklearn.preprocessing import OneHotEncoder #Para codificar las variables categóricas usando dummies\n",
    "from sklearn.preprocessing import StandardScaler #Para normalizar las variables numéricas\n",
    "from sklearn.compose import ColumnTransformer #Modifica las columnas usando los preprocesadores\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline #Planifica una secuencia de procesos\n",
    "from sklearn import set_config #Para mostrar graficamente el pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "set_config(display='diagram')\n",
    "#Metrics\n",
    "from sklearn.metrics import make_scorer, accuracy_score,precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_df = pd.read_csv('train.csv')\n",
    "\n",
    "houses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n",
      "['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n"
     ]
    }
   ],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(houses_df.drop(['Id', 'SalePrice'], axis=1))\n",
    "categorical_columns = categorical_columns_selector(houses_df)\n",
    "\n",
    "print(numerical_columns)\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the number of categories\n",
    "num_categories = 3\n",
    "\n",
    "# Define the category names\n",
    "category_names = ['Economica', 'Intermedia', 'Cara']\n",
    "\n",
    "# Create a new column in the DataFrame for the categories\n",
    "houses_df['Precio_Categoria'] = pd.qcut(houses_df['SalePrice'], q=num_categories, labels=category_names)\n",
    "\n",
    "# Verificar la distribución de las categorías\n",
    "plt.bar(houses_df['Precio_Categoria'].value_counts().index, houses_df['Precio_Categoria'].value_counts().values)\n",
    "plt.xlabel('Categoría de Precio')\n",
    "plt.ylabel('Número de Casas')\n",
    "plt.title('Distribución de Categorías de Precio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador_categorico = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "preprocesador_numerico = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador = ColumnTransformer([\n",
    "    ('Variables Categóricas',preprocesador_categorico, categorical_columns),\n",
    "    ('Variables Numéricas',preprocesador_numerico, numerical_columns)\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Categóricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Numéricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre></pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content \"><pre>MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = make_pipeline(preprocesador,MLPClassifier(activation=\"relu\",verbose=True,hidden_layer_sizes=(30, 20),max_iter=500))\n",
    "modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = houses_df.pop('Precio_Categoria')\n",
    "data = houses_df.drop(['Id', 'SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSSubClass         0\n",
       "MSZoning           0\n",
       "LotFrontage      259\n",
       "LotArea            0\n",
       "Street             0\n",
       "                ... \n",
       "MiscVal            0\n",
       "MoSold             0\n",
       "YrSold             0\n",
       "SaleType           0\n",
       "SaleCondition      0\n",
       "Length: 79, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target,test_size=0.3,train_size=0.7)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.07592107\n",
      "Iteration 2, loss = 0.91959851\n",
      "Iteration 3, loss = 0.82617140\n",
      "Iteration 4, loss = 0.75383625\n",
      "Iteration 5, loss = 0.69294218\n",
      "Iteration 6, loss = 0.63484581\n",
      "Iteration 7, loss = 0.58097041\n",
      "Iteration 8, loss = 0.53461889\n",
      "Iteration 9, loss = 0.49513334\n",
      "Iteration 10, loss = 0.45921181\n",
      "Iteration 11, loss = 0.42825233\n",
      "Iteration 12, loss = 0.40212846\n",
      "Iteration 13, loss = 0.38398277\n",
      "Iteration 14, loss = 0.36573000\n",
      "Iteration 15, loss = 0.35006971\n",
      "Iteration 16, loss = 0.33418153\n",
      "Iteration 17, loss = 0.32371158\n",
      "Iteration 18, loss = 0.31147181\n",
      "Iteration 19, loss = 0.30150542\n",
      "Iteration 20, loss = 0.29248074\n",
      "Iteration 21, loss = 0.28145576\n",
      "Iteration 22, loss = 0.27147835\n",
      "Iteration 23, loss = 0.26209246\n",
      "Iteration 24, loss = 0.25874452\n",
      "Iteration 25, loss = 0.25179937\n",
      "Iteration 26, loss = 0.24183317\n",
      "Iteration 27, loss = 0.23408144\n",
      "Iteration 28, loss = 0.22794328\n",
      "Iteration 29, loss = 0.21994170\n",
      "Iteration 30, loss = 0.21386674\n",
      "Iteration 31, loss = 0.20750539\n",
      "Iteration 32, loss = 0.20201340\n",
      "Iteration 33, loss = 0.20090215\n",
      "Iteration 34, loss = 0.19130803\n",
      "Iteration 35, loss = 0.18406162\n",
      "Iteration 36, loss = 0.18071147\n",
      "Iteration 37, loss = 0.17481781\n",
      "Iteration 38, loss = 0.16771262\n",
      "Iteration 39, loss = 0.16653320\n",
      "Iteration 40, loss = 0.16034194\n",
      "Iteration 41, loss = 0.16151698\n",
      "Iteration 42, loss = 0.14933841\n",
      "Iteration 43, loss = 0.14631298\n",
      "Iteration 44, loss = 0.14115963\n",
      "Iteration 45, loss = 0.13597929\n",
      "Iteration 46, loss = 0.13239114\n",
      "Iteration 47, loss = 0.12838812\n",
      "Iteration 48, loss = 0.12433181\n",
      "Iteration 49, loss = 0.12111175\n",
      "Iteration 50, loss = 0.11705036\n",
      "Iteration 51, loss = 0.11471254\n",
      "Iteration 52, loss = 0.11071100\n",
      "Iteration 53, loss = 0.10847086\n",
      "Iteration 54, loss = 0.10500693\n",
      "Iteration 55, loss = 0.10345864\n",
      "Iteration 56, loss = 0.09844274\n",
      "Iteration 57, loss = 0.09642553\n",
      "Iteration 58, loss = 0.09355275\n",
      "Iteration 59, loss = 0.09039702\n",
      "Iteration 60, loss = 0.08737995\n",
      "Iteration 61, loss = 0.08570273\n",
      "Iteration 62, loss = 0.08213305\n",
      "Iteration 63, loss = 0.08177474\n",
      "Iteration 64, loss = 0.07914409\n",
      "Iteration 65, loss = 0.07679464\n",
      "Iteration 66, loss = 0.07560082\n",
      "Iteration 67, loss = 0.07119156\n",
      "Iteration 68, loss = 0.07026198\n",
      "Iteration 69, loss = 0.06770664\n",
      "Iteration 70, loss = 0.06536174\n",
      "Iteration 71, loss = 0.06430038\n",
      "Iteration 72, loss = 0.06172423\n",
      "Iteration 73, loss = 0.06189124\n",
      "Iteration 74, loss = 0.05956399\n",
      "Iteration 75, loss = 0.05752169\n",
      "Iteration 76, loss = 0.05588618\n",
      "Iteration 77, loss = 0.05426613\n",
      "Iteration 78, loss = 0.05387973\n",
      "Iteration 79, loss = 0.05236837\n",
      "Iteration 80, loss = 0.05085940\n",
      "Iteration 81, loss = 0.04939705\n",
      "Iteration 82, loss = 0.04997329\n",
      "Iteration 83, loss = 0.04736740\n",
      "Iteration 84, loss = 0.04748086\n",
      "Iteration 85, loss = 0.04433151\n",
      "Iteration 86, loss = 0.04738515\n",
      "Iteration 87, loss = 0.04337633\n",
      "Iteration 88, loss = 0.04353594\n",
      "Iteration 89, loss = 0.04220975\n",
      "Iteration 90, loss = 0.04159095\n",
      "Iteration 91, loss = 0.03954391\n",
      "Iteration 92, loss = 0.03795566\n",
      "Iteration 93, loss = 0.03748313\n",
      "Iteration 94, loss = 0.03647816\n",
      "Iteration 95, loss = 0.03566057\n",
      "Iteration 96, loss = 0.03725025\n",
      "Iteration 97, loss = 0.03542966\n",
      "Iteration 98, loss = 0.03540910\n",
      "Iteration 99, loss = 0.03247120\n",
      "Iteration 100, loss = 0.03900872\n",
      "Iteration 101, loss = 0.03440744\n",
      "Iteration 102, loss = 0.03371869\n",
      "Iteration 103, loss = 0.02936743\n",
      "Iteration 104, loss = 0.03076175\n",
      "Iteration 105, loss = 0.02803886\n",
      "Iteration 106, loss = 0.02913324\n",
      "Iteration 107, loss = 0.02928321\n",
      "Iteration 108, loss = 0.02792165\n",
      "Iteration 109, loss = 0.02626245\n",
      "Iteration 110, loss = 0.02631152\n",
      "Iteration 111, loss = 0.02595932\n",
      "Iteration 112, loss = 0.02729134\n",
      "Iteration 113, loss = 0.02586524\n",
      "Iteration 114, loss = 0.02348251\n",
      "Iteration 115, loss = 0.02408060\n",
      "Iteration 116, loss = 0.02265707\n",
      "Iteration 117, loss = 0.02137036\n",
      "Iteration 118, loss = 0.02153884\n",
      "Iteration 119, loss = 0.02043767\n",
      "Iteration 120, loss = 0.02006139\n",
      "Iteration 121, loss = 0.01989789\n",
      "Iteration 122, loss = 0.02255812\n",
      "Iteration 123, loss = 0.02038772\n",
      "Iteration 124, loss = 0.01881458\n",
      "Iteration 125, loss = 0.01877477\n",
      "Iteration 126, loss = 0.01815749\n",
      "Iteration 127, loss = 0.01757094\n",
      "Iteration 128, loss = 0.01740823\n",
      "Iteration 129, loss = 0.01694436\n",
      "Iteration 130, loss = 0.01659439\n",
      "Iteration 131, loss = 0.01637484\n",
      "Iteration 132, loss = 0.01579302\n",
      "Iteration 133, loss = 0.01572786\n",
      "Iteration 134, loss = 0.01808376\n",
      "Iteration 135, loss = 0.01698607\n",
      "Iteration 136, loss = 0.01619181\n",
      "Iteration 137, loss = 0.01622463\n",
      "Iteration 138, loss = 0.01813739\n",
      "Iteration 139, loss = 0.02002750\n",
      "Iteration 140, loss = 0.01854368\n",
      "Iteration 141, loss = 0.01945499\n",
      "Iteration 142, loss = 0.01634626\n",
      "Iteration 143, loss = 0.01437377\n",
      "Iteration 144, loss = 0.01360975\n",
      "Iteration 145, loss = 0.01284383\n",
      "Iteration 146, loss = 0.01283506\n",
      "Iteration 147, loss = 0.01216569\n",
      "Iteration 148, loss = 0.01220479\n",
      "Iteration 149, loss = 0.01182765\n",
      "Iteration 150, loss = 0.01146481\n",
      "Iteration 151, loss = 0.01125237\n",
      "Iteration 152, loss = 0.01110188\n",
      "Iteration 153, loss = 0.01099286\n",
      "Iteration 154, loss = 0.01129202\n",
      "Iteration 155, loss = 0.01097397\n",
      "Iteration 156, loss = 0.01085141\n",
      "Iteration 157, loss = 0.01045908\n",
      "Iteration 158, loss = 0.01001911\n",
      "Iteration 159, loss = 0.00999108\n",
      "Iteration 160, loss = 0.00989518\n",
      "Iteration 161, loss = 0.01006453\n",
      "Iteration 162, loss = 0.01037897\n",
      "Iteration 163, loss = 0.00976345\n",
      "Iteration 164, loss = 0.00968614\n",
      "Iteration 165, loss = 0.00943716\n",
      "Iteration 166, loss = 0.00989873\n",
      "Iteration 167, loss = 0.00875244\n",
      "Iteration 168, loss = 0.00969541\n",
      "Iteration 169, loss = 0.00903106\n",
      "Iteration 170, loss = 0.00965048\n",
      "Iteration 171, loss = 0.00951737\n",
      "Iteration 172, loss = 0.00882677\n",
      "Iteration 173, loss = 0.00860896\n",
      "Iteration 174, loss = 0.00824478\n",
      "Iteration 175, loss = 0.00796995\n",
      "Iteration 176, loss = 0.00784091\n",
      "Iteration 177, loss = 0.00770377\n",
      "Iteration 178, loss = 0.00923592\n",
      "Iteration 179, loss = 0.00890406\n",
      "Iteration 180, loss = 0.00781597\n",
      "Iteration 181, loss = 0.00802458\n",
      "Iteration 182, loss = 0.00807813\n",
      "Iteration 183, loss = 0.00926079\n",
      "Iteration 184, loss = 0.00841495\n",
      "Iteration 185, loss = 0.00740600\n",
      "Iteration 186, loss = 0.00676228\n",
      "Iteration 187, loss = 0.00673009\n",
      "Iteration 188, loss = 0.00844077\n",
      "Iteration 189, loss = 0.00707743\n",
      "Iteration 190, loss = 0.00723115\n",
      "Iteration 191, loss = 0.00628729\n",
      "Iteration 192, loss = 0.00651763\n",
      "Iteration 193, loss = 0.00660633\n",
      "Iteration 194, loss = 0.00636777\n",
      "Iteration 195, loss = 0.00679567\n",
      "Iteration 196, loss = 0.00595530\n",
      "Iteration 197, loss = 0.00585679\n",
      "Iteration 198, loss = 0.00548806\n",
      "Iteration 199, loss = 0.00551140\n",
      "Iteration 200, loss = 0.00537936\n",
      "Iteration 201, loss = 0.00521166\n",
      "Iteration 202, loss = 0.00512144\n",
      "Iteration 203, loss = 0.00499222\n",
      "Iteration 204, loss = 0.00500792\n",
      "Iteration 205, loss = 0.00492669\n",
      "Iteration 206, loss = 0.00521572\n",
      "Iteration 207, loss = 0.00555048\n",
      "Iteration 208, loss = 0.00517534\n",
      "Iteration 209, loss = 0.00487922\n",
      "Iteration 210, loss = 0.00468286\n",
      "Iteration 211, loss = 0.00472738\n",
      "Iteration 212, loss = 0.00452565\n",
      "Iteration 213, loss = 0.00450669\n",
      "Iteration 214, loss = 0.00500280\n",
      "Iteration 215, loss = 0.00455290\n",
      "Iteration 216, loss = 0.00446194\n",
      "Iteration 217, loss = 0.00418318\n",
      "Iteration 218, loss = 0.00409504\n",
      "Iteration 219, loss = 0.00466335\n",
      "Iteration 220, loss = 0.00416949\n",
      "Iteration 221, loss = 0.00423305\n",
      "Iteration 222, loss = 0.00386912\n",
      "Iteration 223, loss = 0.00415728\n",
      "Iteration 224, loss = 0.00388132\n",
      "Iteration 225, loss = 0.00379067\n",
      "Iteration 226, loss = 0.00365789\n",
      "Iteration 227, loss = 0.00405387\n",
      "Iteration 228, loss = 0.00387552\n",
      "Iteration 229, loss = 0.00370138\n",
      "Iteration 230, loss = 0.00347725\n",
      "Iteration 231, loss = 0.00346198\n",
      "Iteration 232, loss = 0.00334070\n",
      "Iteration 233, loss = 0.00336170\n",
      "Iteration 234, loss = 0.00327053\n",
      "Iteration 235, loss = 0.00323015\n",
      "Iteration 236, loss = 0.00315645\n",
      "Iteration 237, loss = 0.00318192\n",
      "Iteration 238, loss = 0.00311165\n",
      "Iteration 239, loss = 0.00307752\n",
      "Iteration 240, loss = 0.00301164\n",
      "Iteration 241, loss = 0.00319600\n",
      "Iteration 242, loss = 0.00299085\n",
      "Iteration 243, loss = 0.00291422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(hidden_layer_sizes=(30, 20), max_iter=500,\n",
       "                               verbose=True))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit(data_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = modelo.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      " [[133   0  19]\n",
      " [  1 114  22]\n",
      " [ 16  24 109]]\n",
      "Accuracy:  0.8127853881278538\n",
      "Precision:  0.8127853881278538\n",
      "recall:  0.8127853881278538\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(target_test,pred)\n",
    "accuracy=accuracy_score(target_test,pred)\n",
    "precision =precision_score(target_test,pred,average='micro')\n",
    "recall =  recall_score(target_test,pred,average='micro')\n",
    "f1 = f1_score(target_test,pred,average='micro')\n",
    "print('Matriz de confusión\\n',cm)\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Categóricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Numéricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content \"><pre>MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(), max_iter=300,\n",
       "              verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(activation='identity', hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo1 = make_pipeline(preprocesador,MLPClassifier(activation=\"identity\",verbose=True,hidden_layer_sizes=(), max_iter=300))\n",
    "\n",
    "modelo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.44468612\n",
      "Iteration 2, loss = 1.27568249\n",
      "Iteration 3, loss = 1.15007751\n",
      "Iteration 4, loss = 1.05946692\n",
      "Iteration 5, loss = 0.98730446\n",
      "Iteration 6, loss = 0.92558438\n",
      "Iteration 7, loss = 0.87321855\n",
      "Iteration 8, loss = 0.82871392\n",
      "Iteration 9, loss = 0.79183320\n",
      "Iteration 10, loss = 0.75903137\n",
      "Iteration 11, loss = 0.73091701\n",
      "Iteration 12, loss = 0.70629276\n",
      "Iteration 13, loss = 0.68545090\n",
      "Iteration 14, loss = 0.66612097\n",
      "Iteration 15, loss = 0.64861899\n",
      "Iteration 16, loss = 0.63340282\n",
      "Iteration 17, loss = 0.61885134\n",
      "Iteration 18, loss = 0.60600259\n",
      "Iteration 19, loss = 0.59439808\n",
      "Iteration 20, loss = 0.58339576\n",
      "Iteration 21, loss = 0.57368033\n",
      "Iteration 22, loss = 0.56395954\n",
      "Iteration 23, loss = 0.55548142\n",
      "Iteration 24, loss = 0.54775117\n",
      "Iteration 25, loss = 0.54008023\n",
      "Iteration 26, loss = 0.53213305\n",
      "Iteration 27, loss = 0.52492515\n",
      "Iteration 28, loss = 0.51837825\n",
      "Iteration 29, loss = 0.51196482\n",
      "Iteration 30, loss = 0.50609696\n",
      "Iteration 31, loss = 0.50068349\n",
      "Iteration 32, loss = 0.49508488\n",
      "Iteration 33, loss = 0.48986040\n",
      "Iteration 34, loss = 0.48463613\n",
      "Iteration 35, loss = 0.47989865\n",
      "Iteration 36, loss = 0.47536303\n",
      "Iteration 37, loss = 0.47096873\n",
      "Iteration 38, loss = 0.46715115\n",
      "Iteration 39, loss = 0.46324927\n",
      "Iteration 40, loss = 0.45923156\n",
      "Iteration 41, loss = 0.45549339\n",
      "Iteration 42, loss = 0.45203587\n",
      "Iteration 43, loss = 0.44873788\n",
      "Iteration 44, loss = 0.44568434\n",
      "Iteration 45, loss = 0.44303819\n",
      "Iteration 46, loss = 0.43951690\n",
      "Iteration 47, loss = 0.43607449\n",
      "Iteration 48, loss = 0.43293827\n",
      "Iteration 49, loss = 0.43098234\n",
      "Iteration 50, loss = 0.42901083\n",
      "Iteration 51, loss = 0.42608140\n",
      "Iteration 52, loss = 0.42304384\n",
      "Iteration 53, loss = 0.42035165\n",
      "Iteration 54, loss = 0.41769085\n",
      "Iteration 55, loss = 0.41615513\n",
      "Iteration 56, loss = 0.41375562\n",
      "Iteration 57, loss = 0.41082930\n",
      "Iteration 58, loss = 0.40845140\n",
      "Iteration 59, loss = 0.40645584\n",
      "Iteration 60, loss = 0.40430976\n",
      "Iteration 61, loss = 0.40243824\n",
      "Iteration 62, loss = 0.40036458\n",
      "Iteration 63, loss = 0.39819421\n",
      "Iteration 64, loss = 0.39655243\n",
      "Iteration 65, loss = 0.39491344\n",
      "Iteration 66, loss = 0.39322150\n",
      "Iteration 67, loss = 0.39128055\n",
      "Iteration 68, loss = 0.38978052\n",
      "Iteration 69, loss = 0.38829290\n",
      "Iteration 70, loss = 0.38623740\n",
      "Iteration 71, loss = 0.38467968\n",
      "Iteration 72, loss = 0.38340887\n",
      "Iteration 73, loss = 0.38194031\n",
      "Iteration 74, loss = 0.37963360\n",
      "Iteration 75, loss = 0.37834161\n",
      "Iteration 76, loss = 0.37651739\n",
      "Iteration 77, loss = 0.37515646\n",
      "Iteration 78, loss = 0.37332005\n",
      "Iteration 79, loss = 0.37168902\n",
      "Iteration 80, loss = 0.37042429\n",
      "Iteration 81, loss = 0.36866373\n",
      "Iteration 82, loss = 0.36818304\n",
      "Iteration 83, loss = 0.36700870\n",
      "Iteration 84, loss = 0.36529274\n",
      "Iteration 85, loss = 0.36354775\n",
      "Iteration 86, loss = 0.36224060\n",
      "Iteration 87, loss = 0.36103407\n",
      "Iteration 88, loss = 0.36015367\n",
      "Iteration 89, loss = 0.35907220\n",
      "Iteration 90, loss = 0.35798787\n",
      "Iteration 91, loss = 0.35618575\n",
      "Iteration 92, loss = 0.35529808\n",
      "Iteration 93, loss = 0.35427020\n",
      "Iteration 94, loss = 0.35322175\n",
      "Iteration 95, loss = 0.35196284\n",
      "Iteration 96, loss = 0.35066577\n",
      "Iteration 97, loss = 0.34943706\n",
      "Iteration 98, loss = 0.34803892\n",
      "Iteration 99, loss = 0.34714703\n",
      "Iteration 100, loss = 0.34611129\n",
      "Iteration 101, loss = 0.34501517\n",
      "Iteration 102, loss = 0.34456913\n",
      "Iteration 103, loss = 0.34408670\n",
      "Iteration 104, loss = 0.34298348\n",
      "Iteration 105, loss = 0.34122194\n",
      "Iteration 106, loss = 0.33990730\n",
      "Iteration 107, loss = 0.33898661\n",
      "Iteration 108, loss = 0.33797774\n",
      "Iteration 109, loss = 0.33696758\n",
      "Iteration 110, loss = 0.33593708\n",
      "Iteration 111, loss = 0.33512812\n",
      "Iteration 112, loss = 0.33414404\n",
      "Iteration 113, loss = 0.33363988\n",
      "Iteration 114, loss = 0.33320561\n",
      "Iteration 115, loss = 0.33217368\n",
      "Iteration 116, loss = 0.33093781\n",
      "Iteration 117, loss = 0.32992187\n",
      "Iteration 118, loss = 0.32888736\n",
      "Iteration 119, loss = 0.32828170\n",
      "Iteration 120, loss = 0.32763906\n",
      "Iteration 121, loss = 0.32681309\n",
      "Iteration 122, loss = 0.32590529\n",
      "Iteration 123, loss = 0.32482440\n",
      "Iteration 124, loss = 0.32369872\n",
      "Iteration 125, loss = 0.32302924\n",
      "Iteration 126, loss = 0.32324654\n",
      "Iteration 127, loss = 0.32320317\n",
      "Iteration 128, loss = 0.32079666\n",
      "Iteration 129, loss = 0.31984987\n",
      "Iteration 130, loss = 0.31995982\n",
      "Iteration 131, loss = 0.32045502\n",
      "Iteration 132, loss = 0.31927773\n",
      "Iteration 133, loss = 0.31739410\n",
      "Iteration 134, loss = 0.31592788\n",
      "Iteration 135, loss = 0.31546482\n",
      "Iteration 136, loss = 0.31460485\n",
      "Iteration 137, loss = 0.31383600\n",
      "Iteration 138, loss = 0.31347733\n",
      "Iteration 139, loss = 0.31287046\n",
      "Iteration 140, loss = 0.31189939\n",
      "Iteration 141, loss = 0.31096927\n",
      "Iteration 142, loss = 0.31032537\n",
      "Iteration 143, loss = 0.30954534\n",
      "Iteration 144, loss = 0.30886317\n",
      "Iteration 145, loss = 0.30833538\n",
      "Iteration 146, loss = 0.30779255\n",
      "Iteration 147, loss = 0.30713489\n",
      "Iteration 148, loss = 0.30639326\n",
      "Iteration 149, loss = 0.30565118\n",
      "Iteration 150, loss = 0.30478990\n",
      "Iteration 151, loss = 0.30380595\n",
      "Iteration 152, loss = 0.30331419\n",
      "Iteration 153, loss = 0.30303684\n",
      "Iteration 154, loss = 0.30202122\n",
      "Iteration 155, loss = 0.30143716\n",
      "Iteration 156, loss = 0.30095982\n",
      "Iteration 157, loss = 0.30063275\n",
      "Iteration 158, loss = 0.29952636\n",
      "Iteration 159, loss = 0.29864290\n",
      "Iteration 160, loss = 0.29804221\n",
      "Iteration 161, loss = 0.29756987\n",
      "Iteration 162, loss = 0.29672547\n",
      "Iteration 163, loss = 0.29637108\n",
      "Iteration 164, loss = 0.29587291\n",
      "Iteration 165, loss = 0.29548323\n",
      "Iteration 166, loss = 0.29456076\n",
      "Iteration 167, loss = 0.29390105\n",
      "Iteration 168, loss = 0.29309242\n",
      "Iteration 169, loss = 0.29251414\n",
      "Iteration 170, loss = 0.29194047\n",
      "Iteration 171, loss = 0.29157765\n",
      "Iteration 172, loss = 0.29103648\n",
      "Iteration 173, loss = 0.29025422\n",
      "Iteration 174, loss = 0.28976830\n",
      "Iteration 175, loss = 0.28942319\n",
      "Iteration 176, loss = 0.28904931\n",
      "Iteration 177, loss = 0.28868443\n",
      "Iteration 178, loss = 0.28775384\n",
      "Iteration 179, loss = 0.28769554\n",
      "Iteration 180, loss = 0.28781096\n",
      "Iteration 181, loss = 0.28713713\n",
      "Iteration 182, loss = 0.28586971\n",
      "Iteration 183, loss = 0.28505606\n",
      "Iteration 184, loss = 0.28433875\n",
      "Iteration 185, loss = 0.28384260\n",
      "Iteration 186, loss = 0.28340494\n",
      "Iteration 187, loss = 0.28298833\n",
      "Iteration 188, loss = 0.28266512\n",
      "Iteration 189, loss = 0.28235318\n",
      "Iteration 190, loss = 0.28221947\n",
      "Iteration 191, loss = 0.28143733\n",
      "Iteration 192, loss = 0.28083615\n",
      "Iteration 193, loss = 0.28013787\n",
      "Iteration 194, loss = 0.27973217\n",
      "Iteration 195, loss = 0.27917398\n",
      "Iteration 196, loss = 0.27915860\n",
      "Iteration 197, loss = 0.27870784\n",
      "Iteration 198, loss = 0.27811233\n",
      "Iteration 199, loss = 0.27759668\n",
      "Iteration 200, loss = 0.27730975\n",
      "Iteration 201, loss = 0.27642574\n",
      "Iteration 202, loss = 0.27602516\n",
      "Iteration 203, loss = 0.27539763\n",
      "Iteration 204, loss = 0.27500181\n",
      "Iteration 205, loss = 0.27423311\n",
      "Iteration 206, loss = 0.27399241\n",
      "Iteration 207, loss = 0.27386490\n",
      "Iteration 208, loss = 0.27439598\n",
      "Iteration 209, loss = 0.27259627\n",
      "Iteration 210, loss = 0.27252969\n",
      "Iteration 211, loss = 0.27205338\n",
      "Iteration 212, loss = 0.27119779\n",
      "Iteration 213, loss = 0.27083023\n",
      "Iteration 214, loss = 0.27023763\n",
      "Iteration 215, loss = 0.26972601\n",
      "Iteration 216, loss = 0.26892642\n",
      "Iteration 217, loss = 0.26854751\n",
      "Iteration 218, loss = 0.26836984\n",
      "Iteration 219, loss = 0.26842278\n",
      "Iteration 220, loss = 0.26864622\n",
      "Iteration 221, loss = 0.26736673\n",
      "Iteration 222, loss = 0.26641523\n",
      "Iteration 223, loss = 0.26597293\n",
      "Iteration 224, loss = 0.26562611\n",
      "Iteration 225, loss = 0.26519931\n",
      "Iteration 226, loss = 0.26500203\n",
      "Iteration 227, loss = 0.26492207\n",
      "Iteration 228, loss = 0.26478348\n",
      "Iteration 229, loss = 0.26422149\n",
      "Iteration 230, loss = 0.26355981\n",
      "Iteration 231, loss = 0.26299614\n",
      "Iteration 232, loss = 0.26250822\n",
      "Iteration 233, loss = 0.26195279\n",
      "Iteration 234, loss = 0.26156513\n",
      "Iteration 235, loss = 0.26120068\n",
      "Iteration 236, loss = 0.26097842\n",
      "Iteration 237, loss = 0.26084253\n",
      "Iteration 238, loss = 0.26036894\n",
      "Iteration 239, loss = 0.26006165\n",
      "Iteration 240, loss = 0.25939015\n",
      "Iteration 241, loss = 0.25891411\n",
      "Iteration 242, loss = 0.25855399\n",
      "Iteration 243, loss = 0.25843077\n",
      "Iteration 244, loss = 0.25801397\n",
      "Iteration 245, loss = 0.25743076\n",
      "Iteration 246, loss = 0.25771898\n",
      "Iteration 247, loss = 0.25746815\n",
      "Iteration 248, loss = 0.25660153\n",
      "Iteration 249, loss = 0.25566404\n",
      "Iteration 250, loss = 0.25550633\n",
      "Iteration 251, loss = 0.25567680\n",
      "Iteration 252, loss = 0.25546182\n",
      "Iteration 253, loss = 0.25506484\n",
      "Iteration 254, loss = 0.25461996\n",
      "Iteration 255, loss = 0.25421735\n",
      "Iteration 256, loss = 0.25368499\n",
      "Iteration 257, loss = 0.25329026\n",
      "Iteration 258, loss = 0.25290759\n",
      "Iteration 259, loss = 0.25245565\n",
      "Iteration 260, loss = 0.25181601\n",
      "Iteration 261, loss = 0.25169697\n",
      "Iteration 262, loss = 0.25133183\n",
      "Iteration 263, loss = 0.25083879\n",
      "Iteration 264, loss = 0.25057339\n",
      "Iteration 265, loss = 0.25023303\n",
      "Iteration 266, loss = 0.24970168\n",
      "Iteration 267, loss = 0.24936159\n",
      "Iteration 268, loss = 0.24937912\n",
      "Iteration 269, loss = 0.24930526\n",
      "Iteration 270, loss = 0.24910168\n",
      "Iteration 271, loss = 0.24825346\n",
      "Iteration 272, loss = 0.24787475\n",
      "Iteration 273, loss = 0.24772850\n",
      "Iteration 274, loss = 0.24740106\n",
      "Iteration 275, loss = 0.24694510\n",
      "Iteration 276, loss = 0.24654267\n",
      "Iteration 277, loss = 0.24615418\n",
      "Iteration 278, loss = 0.24570303\n",
      "Iteration 279, loss = 0.24541823\n",
      "Iteration 280, loss = 0.24531733\n",
      "Iteration 281, loss = 0.24509158\n",
      "Iteration 282, loss = 0.24448906\n",
      "Iteration 283, loss = 0.24407504\n",
      "Iteration 284, loss = 0.24374887\n",
      "Iteration 285, loss = 0.24361953\n",
      "Iteration 286, loss = 0.24365433\n",
      "Iteration 287, loss = 0.24297971\n",
      "Iteration 288, loss = 0.24290356\n",
      "Iteration 289, loss = 0.24280911\n",
      "Iteration 290, loss = 0.24267470\n",
      "Iteration 291, loss = 0.24218285\n",
      "Iteration 292, loss = 0.24163200\n",
      "Iteration 293, loss = 0.24121951\n",
      "Iteration 294, loss = 0.24091268\n",
      "Iteration 295, loss = 0.24080986\n",
      "Iteration 296, loss = 0.24056770\n",
      "Iteration 297, loss = 0.24023776\n",
      "Iteration 298, loss = 0.23971044\n",
      "Iteration 299, loss = 0.23949681\n",
      "Iteration 300, loss = 0.23939529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(), max_iter=300,\n",
       "              verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   'Condition2', 'BldgType',\n",
       "                                                   'HouseStyle', 'RoofStyle',\n",
       "                                                   'RoofMatl', 'Exterior1st',\n",
       "                                                   'E...\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpclassifier',\n",
       "                 MLPClassifier(activation='identity', hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo1.fit(data_train,target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = modelo1.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      " [[131   0  21]\n",
      " [  1 115  21]\n",
      " [ 24  25 100]]\n",
      "Accuracy:  0.7899543378995434\n",
      "Precision:  0.7899543378995434\n",
      "recall:  0.7899543378995434\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(target_test,pred1)\n",
    "accuracy=accuracy_score(target_test,pred1)\n",
    "precision =precision_score(target_test,pred1,average='micro')\n",
    "recall =  recall_score(target_test,pred1,average='micro')\n",
    "f1 = f1_score(target_test,pred1,average='micro')\n",
    "print('Matriz de confusión\\n',cm)\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('recall: ',recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.86280695\n",
      "Iteration 2, loss = 0.43514965\n",
      "Iteration 3, loss = 0.38180355\n",
      "Iteration 4, loss = 0.34165969\n",
      "Iteration 5, loss = 0.30395871\n",
      "Iteration 6, loss = 0.27907170\n",
      "Iteration 1, loss = 1.03891020\n",
      "Iteration 7, loss = 0.27524708\n",
      "Iteration 2, loss = 0.46624539\n",
      "Iteration 8, loss = 0.27770323\n",
      "Iteration 3, loss = 0.43970928\n",
      "Iteration 9, loss = 0.24364976\n",
      "Iteration 4, loss = 0.36122107\n",
      "Iteration 10, loss = 0.21527758\n",
      "Iteration 5, loss = 0.32662617\n",
      "Iteration 11, loss = 0.20426254\n",
      "Iteration 6, loss = 0.28997952\n",
      "Iteration 12, loss = 0.18431477\n",
      "Iteration 7, loss = 0.27014314\n",
      "Iteration 13, loss = 0.17060465\n",
      "Iteration 8, loss = 0.24542109\n",
      "Iteration 14, loss = 0.18039237\n",
      "Iteration 9, loss = 0.22187332\n",
      "Iteration 15, loss = 0.14694004\n",
      "Iteration 10, loss = 0.20928104\n",
      "Iteration 16, loss = 0.15045169\n",
      "Iteration 11, loss = 0.19919773\n",
      "Iteration 17, loss = 0.13892919\n",
      "Iteration 12, loss = 0.18235603\n",
      "Iteration 18, loss = 0.13022818\n",
      "Iteration 13, loss = 0.18085481\n",
      "Iteration 19, loss = 0.11396502\n",
      "Iteration 14, loss = 0.18131041\n",
      "Iteration 20, loss = 0.11292012\n",
      "Iteration 15, loss = 0.17744831\n",
      "Iteration 21, loss = 0.12369061\n",
      "Iteration 16, loss = 0.19169723\n",
      "Iteration 22, loss = 0.10868363\n",
      "Iteration 17, loss = 0.16768448\n",
      "Iteration 23, loss = 0.10494827\n",
      "Iteration 18, loss = 0.19535893\n",
      "Iteration 24, loss = 0.14221036\n",
      "Iteration 19, loss = 0.18307178\n",
      "Iteration 25, loss = 0.12735597\n",
      "Iteration 20, loss = 0.15013044\n",
      "Iteration 26, loss = 0.14393042\n",
      "Iteration 21, loss = 0.15152200\n",
      "Iteration 27, loss = 0.12241020\n",
      "Iteration 22, loss = 0.14238042\n",
      "Iteration 28, loss = 0.11386767\n",
      "Iteration 23, loss = 0.16320808\n",
      "Iteration 29, loss = 0.11877002\n",
      "Iteration 24, loss = 0.14169955\n",
      "Iteration 30, loss = 0.10554122\n",
      "Iteration 25, loss = 0.12079469\n",
      "Iteration 31, loss = 0.08598499\n",
      "Iteration 26, loss = 0.11800988\n",
      "Iteration 32, loss = 0.08670317\n",
      "Iteration 27, loss = 0.11997665\n",
      "Iteration 33, loss = 0.08449345\n",
      "Iteration 28, loss = 0.11313245\n",
      "Iteration 34, loss = 0.11681038\n",
      "Iteration 29, loss = 0.10511754\n",
      "Iteration 35, loss = 0.08841855\n",
      "Iteration 30, loss = 0.10077149\n",
      "Iteration 36, loss = 0.08786809\n",
      "Iteration 31, loss = 0.09407697\n",
      "Iteration 37, loss = 0.07770121\n",
      "Iteration 32, loss = 0.09159998\n",
      "Iteration 38, loss = 0.06347502\n",
      "Iteration 33, loss = 0.10963336\n",
      "Iteration 39, loss = 0.08120758\n",
      "Iteration 34, loss = 0.13530378\n",
      "Iteration 40, loss = 0.07110426\n",
      "Iteration 35, loss = 0.11166924\n",
      "Iteration 41, loss = 0.08332761\n",
      "Iteration 36, loss = 0.13091571\n",
      "Iteration 42, loss = 0.07095989\n",
      "Iteration 37, loss = 0.12894681\n",
      "Iteration 43, loss = 0.06185419\n",
      "Iteration 38, loss = 0.14824462\n",
      "Iteration 44, loss = 0.04892542\n",
      "Iteration 39, loss = 0.12815743\n",
      "Iteration 45, loss = 0.06842937\n",
      "Iteration 40, loss = 0.09550278\n",
      "Iteration 46, loss = 0.09142179\n",
      "Iteration 41, loss = 0.08993330\n",
      "Iteration 47, loss = 0.09225155\n",
      "Iteration 42, loss = 0.09761744\n",
      "Iteration 48, loss = 0.15132521\n",
      "Iteration 43, loss = 0.09006306\n",
      "Iteration 49, loss = 0.11881761\n",
      "Iteration 44, loss = 0.08640783\n",
      "Iteration 50, loss = 0.12702300\n",
      "Iteration 45, loss = 0.07724181\n",
      "Iteration 51, loss = 0.07391791\n",
      "Iteration 46, loss = 0.07136897\n",
      "Iteration 52, loss = 0.11678950\n",
      "Iteration 47, loss = 0.07632042\n",
      "Iteration 53, loss = 0.08019146\n",
      "Iteration 48, loss = 0.06633001\n",
      "Iteration 54, loss = 0.09184425\n",
      "Iteration 49, loss = 0.06675011\n",
      "Iteration 55, loss = 0.07025821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.06677643\n",
      "Iteration 51, loss = 0.06437999\n",
      "Iteration 52, loss = 0.05557635\n",
      "Iteration 53, loss = 0.06838954\n",
      "Iteration 54, loss = 0.06963145\n",
      "Iteration 55, loss = 0.07239313\n",
      "Iteration 56, loss = 0.06818519\n",
      "Iteration 57, loss = 0.07401010\n",
      "Iteration 58, loss = 0.08228181\n",
      "Iteration 59, loss = 0.12134621\n",
      "Iteration 60, loss = 0.08910282\n",
      "Iteration 61, loss = 0.10074397\n",
      "Iteration 62, loss = 0.13179774\n",
      "Iteration 63, loss = 0.21429950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82606202\n",
      "Iteration 2, loss = 0.42207346\n",
      "Iteration 3, loss = 0.36406572\n",
      "Iteration 4, loss = 0.31791534\n",
      "Iteration 5, loss = 0.29013206\n",
      "Iteration 6, loss = 0.25985428\n",
      "Iteration 7, loss = 0.23949453\n",
      "Iteration 8, loss = 0.21625467\n",
      "Iteration 9, loss = 0.20793280\n",
      "Iteration 10, loss = 0.20251868\n",
      "Iteration 11, loss = 0.20228435\n",
      "Iteration 12, loss = 0.19473241\n",
      "Iteration 13, loss = 0.17369127\n",
      "Iteration 14, loss = 0.15905193\n",
      "Iteration 1, loss = 0.73657722\n",
      "Iteration 15, loss = 0.15257451\n",
      "Iteration 2, loss = 0.43772739\n",
      "Iteration 16, loss = 0.13389188\n",
      "Iteration 3, loss = 0.37825929\n",
      "Iteration 17, loss = 0.12574712\n",
      "Iteration 4, loss = 0.33913631\n",
      "Iteration 18, loss = 0.12920022\n",
      "Iteration 5, loss = 0.30883188\n",
      "Iteration 19, loss = 0.14330040\n",
      "Iteration 6, loss = 0.28591005\n",
      "Iteration 20, loss = 0.13751992\n",
      "Iteration 7, loss = 0.28293878\n",
      "Iteration 21, loss = 0.11962167\n",
      "Iteration 8, loss = 0.24128364\n",
      "Iteration 22, loss = 0.17696043\n",
      "Iteration 9, loss = 0.22048550\n",
      "Iteration 23, loss = 0.12904623\n",
      "Iteration 10, loss = 0.20394086\n",
      "Iteration 24, loss = 0.10358130\n",
      "Iteration 11, loss = 0.19597398\n",
      "Iteration 25, loss = 0.08811986\n",
      "Iteration 12, loss = 0.19450364\n",
      "Iteration 26, loss = 0.08804790\n",
      "Iteration 13, loss = 0.17197988\n",
      "Iteration 27, loss = 0.07657776\n",
      "Iteration 14, loss = 0.15877227\n",
      "Iteration 28, loss = 0.07117851\n",
      "Iteration 15, loss = 0.17062503\n",
      "Iteration 29, loss = 0.06944179\n",
      "Iteration 16, loss = 0.14884391\n",
      "Iteration 30, loss = 0.07794625\n",
      "Iteration 17, loss = 0.13859673\n",
      "Iteration 31, loss = 0.09502100\n",
      "Iteration 18, loss = 0.12760809\n",
      "Iteration 32, loss = 0.08687155\n",
      "Iteration 19, loss = 0.13281580\n",
      "Iteration 33, loss = 0.08267001\n",
      "Iteration 20, loss = 0.12648113\n",
      "Iteration 34, loss = 0.08355966\n",
      "Iteration 21, loss = 0.14941201\n",
      "Iteration 35, loss = 0.07541883\n",
      "Iteration 22, loss = 0.12202937\n",
      "Iteration 36, loss = 0.10296117\n",
      "Iteration 23, loss = 0.10540701\n",
      "Iteration 37, loss = 0.17449254\n",
      "Iteration 24, loss = 0.11386478\n",
      "Iteration 38, loss = 0.12096867\n",
      "Iteration 25, loss = 0.12499919\n",
      "Iteration 39, loss = 0.12871064\n",
      "Iteration 26, loss = 0.13292347\n",
      "Iteration 40, loss = 0.10837663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.12611449\n",
      "Iteration 28, loss = 0.11466122\n",
      "Iteration 29, loss = 0.13889301\n",
      "Iteration 30, loss = 0.13216625\n",
      "Iteration 31, loss = 0.15287554\n",
      "Iteration 32, loss = 0.15614766\n",
      "Iteration 33, loss = 0.11824258\n",
      "Iteration 34, loss = 0.10399718\n",
      "Iteration 35, loss = 0.10999329\n",
      "Iteration 36, loss = 0.09311983\n",
      "Iteration 37, loss = 0.08839608\n",
      "Iteration 38, loss = 0.08021317\n",
      "Iteration 39, loss = 0.09250975\n",
      "Iteration 1, loss = 1.00186667\n",
      "Iteration 40, loss = 0.08451364\n",
      "Iteration 2, loss = 0.45697330\n",
      "Iteration 41, loss = 0.08234241\n",
      "Iteration 3, loss = 0.37909895\n",
      "Iteration 42, loss = 0.11254840\n",
      "Iteration 4, loss = 0.33521863\n",
      "Iteration 43, loss = 0.11004261\n",
      "Iteration 5, loss = 0.28166932\n",
      "Iteration 44, loss = 0.10437888\n",
      "Iteration 6, loss = 0.26339991\n",
      "Iteration 7, loss = 0.24464625\n",
      "Iteration 45, loss = 0.10231087\n",
      "Iteration 8, loss = 0.22651196\n",
      "Iteration 46, loss = 0.09383107\n",
      "Iteration 9, loss = 0.19760090\n",
      "Iteration 47, loss = 0.08579724\n",
      "Iteration 10, loss = 0.18359516\n",
      "Iteration 48, loss = 0.08940118\n",
      "Iteration 11, loss = 0.17110113\n",
      "Iteration 49, loss = 0.08385005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.16190091\n",
      "Iteration 13, loss = 0.14877753\n",
      "Iteration 14, loss = 0.15225346\n",
      "Iteration 15, loss = 0.12744859\n",
      "Iteration 16, loss = 0.12101676\n",
      "Iteration 17, loss = 0.12023756\n",
      "Iteration 18, loss = 0.13811001\n",
      "Iteration 19, loss = 0.17432479\n",
      "Iteration 20, loss = 0.15466420\n",
      "Iteration 21, loss = 0.13891478\n",
      "Iteration 22, loss = 0.12684878\n",
      "Iteration 23, loss = 0.10331350\n",
      "Iteration 24, loss = 0.09900748\n",
      "Iteration 1, loss = 0.92140145\n",
      "Iteration 25, loss = 0.09571163\n",
      "Iteration 2, loss = 0.43734365\n",
      "Iteration 26, loss = 0.09015798\n",
      "Iteration 3, loss = 0.38338561\n",
      "Iteration 27, loss = 0.08503953\n",
      "Iteration 4, loss = 0.32832116\n",
      "Iteration 28, loss = 0.07232182\n",
      "Iteration 5, loss = 0.29836938\n",
      "Iteration 29, loss = 0.07018798\n",
      "Iteration 6, loss = 0.28226567\n",
      "Iteration 30, loss = 0.06693478Iteration 7, loss = 0.24332809\n",
      "\n",
      "Iteration 8, loss = 0.22185872\n",
      "Iteration 31, loss = 0.06439384\n",
      "Iteration 9, loss = 0.20937647\n",
      "Iteration 32, loss = 0.06117514\n",
      "Iteration 10, loss = 0.18936424\n",
      "Iteration 33, loss = 0.06006080\n",
      "Iteration 34, loss = 0.06710922\n",
      "Iteration 11, loss = 0.18335920\n",
      "Iteration 35, loss = 0.08095010\n",
      "Iteration 12, loss = 0.16334461\n",
      "Iteration 36, loss = 0.06076745\n",
      "Iteration 13, loss = 0.15661911\n",
      "Iteration 37, loss = 0.08590675\n",
      "Iteration 14, loss = 0.14919404\n",
      "Iteration 15, loss = 0.13620222\n",
      "Iteration 38, loss = 0.08705935\n",
      "Iteration 16, loss = 0.12340939\n",
      "Iteration 39, loss = 0.08311460\n",
      "Iteration 17, loss = 0.13551480\n",
      "Iteration 40, loss = 0.10272955\n",
      "Iteration 18, loss = 0.13349565\n",
      "Iteration 41, loss = 0.08681953\n",
      "Iteration 19, loss = 0.10916864\n",
      "Iteration 42, loss = 0.08098353\n",
      "Iteration 20, loss = 0.12471968\n",
      "Iteration 43, loss = 0.06329436\n",
      "Iteration 21, loss = 0.11922880\n",
      "Iteration 44, loss = 0.05369231\n",
      "Iteration 22, loss = 0.11724616\n",
      "Iteration 45, loss = 0.04707957\n",
      "Iteration 23, loss = 0.11229193\n",
      "Iteration 46, loss = 0.04839562\n",
      "Iteration 24, loss = 0.09596295\n",
      "Iteration 47, loss = 0.04862775\n",
      "Iteration 25, loss = 0.09766595\n",
      "Iteration 48, loss = 0.05631427\n",
      "Iteration 26, loss = 0.09399821\n",
      "Iteration 49, loss = 0.05963289\n",
      "Iteration 27, loss = 0.08758127\n",
      "Iteration 50, loss = 0.05563316\n",
      "Iteration 28, loss = 0.07974590\n",
      "Iteration 51, loss = 0.05513038\n",
      "Iteration 29, loss = 0.07737628\n",
      "Iteration 52, loss = 0.04688308\n",
      "Iteration 30, loss = 0.09209661\n",
      "Iteration 53, loss = 0.06369025\n",
      "Iteration 31, loss = 0.12298474\n",
      "Iteration 54, loss = 0.07342282\n",
      "Iteration 32, loss = 0.11680922\n",
      "Iteration 55, loss = 0.04963045\n",
      "Iteration 33, loss = 0.10641796\n",
      "Iteration 56, loss = 0.07352421\n",
      "Iteration 34, loss = 0.08653640\n",
      "Iteration 57, loss = 0.06734370\n",
      "Iteration 35, loss = 0.09754366\n",
      "Iteration 58, loss = 0.08512376\n",
      "Iteration 36, loss = 0.08860833\n",
      "Iteration 59, loss = 0.06824888\n",
      "Iteration 37, loss = 0.06706694\n",
      "Iteration 60, loss = 0.13113289\n",
      "Iteration 38, loss = 0.07760139\n",
      "Iteration 61, loss = 0.11993665\n",
      "Iteration 39, loss = 0.08061147\n",
      "Iteration 62, loss = 0.09057463\n",
      "Iteration 40, loss = 0.06000747\n",
      "Iteration 63, loss = 0.08612780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.06737772\n",
      "Iteration 42, loss = 0.06670428\n",
      "Iteration 43, loss = 0.06723131\n",
      "Iteration 44, loss = 0.06055013\n",
      "Iteration 45, loss = 0.08382947\n",
      "Iteration 46, loss = 0.08019947\n",
      "Iteration 47, loss = 0.06892248\n",
      "Iteration 48, loss = 0.06370634\n",
      "Iteration 49, loss = 0.04990105\n",
      "Iteration 50, loss = 0.04772060\n",
      "Iteration 51, loss = 0.07375447\n",
      "Iteration 52, loss = 0.08640950\n",
      "Iteration 53, loss = 0.06851475\n",
      "Iteration 1, loss = 1.24164506\n",
      "Iteration 54, loss = 0.06389984\n",
      "Iteration 2, loss = 0.58002328\n",
      "Iteration 55, loss = 0.04757714\n",
      "Iteration 3, loss = 0.43120374\n",
      "Iteration 56, loss = 0.05584469\n",
      "Iteration 4, loss = 0.42176499\n",
      "Iteration 57, loss = 0.03658198\n",
      "Iteration 5, loss = 0.35587402\n",
      "Iteration 58, loss = 0.03278722\n",
      "Iteration 6, loss = 0.31877992\n",
      "Iteration 59, loss = 0.03636888\n",
      "Iteration 7, loss = 0.29592373\n",
      "Iteration 60, loss = 0.04354115\n",
      "Iteration 8, loss = 0.27730969\n",
      "Iteration 61, loss = 0.04827650\n",
      "Iteration 9, loss = 0.26363372\n",
      "Iteration 62, loss = 0.03933254\n",
      "Iteration 10, loss = 0.24413896\n",
      "Iteration 63, loss = 0.03609715\n",
      "Iteration 11, loss = 0.23549697\n",
      "Iteration 64, loss = 0.06064241\n",
      "Iteration 12, loss = 0.22574859\n",
      "Iteration 65, loss = 0.03205517\n",
      "Iteration 13, loss = 0.21204648\n",
      "Iteration 66, loss = 0.03452717\n",
      "Iteration 14, loss = 0.20555671\n",
      "Iteration 67, loss = 0.04164674\n",
      "Iteration 15, loss = 0.17850194\n",
      "Iteration 68, loss = 0.03820343\n",
      "Iteration 16, loss = 0.16800923\n",
      "Iteration 69, loss = 0.04336938\n",
      "Iteration 17, loss = 0.15170358\n",
      "Iteration 70, loss = 0.06379219\n",
      "Iteration 18, loss = 0.14997785\n",
      "Iteration 71, loss = 0.08208191\n",
      "Iteration 19, loss = 0.13943564\n",
      "Iteration 72, loss = 0.14772504\n",
      "Iteration 20, loss = 0.13719358\n",
      "Iteration 73, loss = 0.14705015\n",
      "Iteration 21, loss = 0.13227471\n",
      "Iteration 74, loss = 0.10432769\n",
      "Iteration 22, loss = 0.13262048\n",
      "Iteration 75, loss = 0.08260116\n",
      "Iteration 23, loss = 0.12156476\n",
      "Iteration 76, loss = 0.06995487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.11849974\n",
      "Iteration 25, loss = 0.10792642\n",
      "Iteration 26, loss = 0.11769851\n",
      "Iteration 27, loss = 0.12732533\n",
      "Iteration 28, loss = 0.11253776\n",
      "Iteration 29, loss = 0.10349878\n",
      "Iteration 30, loss = 0.09591050\n",
      "Iteration 31, loss = 0.10203551\n",
      "Iteration 32, loss = 0.08275991\n",
      "Iteration 33, loss = 0.08748211\n",
      "Iteration 34, loss = 0.08940991\n",
      "Iteration 35, loss = 0.06969888\n",
      "Iteration 36, loss = 0.07910161\n",
      "Iteration 1, loss = 0.91237883\n",
      "Iteration 37, loss = 0.07805943\n",
      "Iteration 2, loss = 0.47810548\n",
      "Iteration 38, loss = 0.11401384\n",
      "Iteration 3, loss = 0.39129155\n",
      "Iteration 39, loss = 0.12865095\n",
      "Iteration 4, loss = 0.36556118\n",
      "Iteration 40, loss = 0.14565459\n",
      "Iteration 5, loss = 0.32364305\n",
      "Iteration 41, loss = 0.14221989\n",
      "Iteration 6, loss = 0.29153296\n",
      "Iteration 42, loss = 0.11614052\n",
      "Iteration 7, loss = 0.27452910\n",
      "Iteration 43, loss = 0.10184213\n",
      "Iteration 8, loss = 0.27519667\n",
      "Iteration 44, loss = 0.11311266\n",
      "Iteration 9, loss = 0.24722478\n",
      "Iteration 45, loss = 0.12984592\n",
      "Iteration 10, loss = 0.22660599\n",
      "Iteration 46, loss = 0.10156607\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.20027627\n",
      "Iteration 12, loss = 0.18140394\n",
      "Iteration 13, loss = 0.19229294\n",
      "Iteration 14, loss = 0.17643676\n",
      "Iteration 15, loss = 0.15454685\n",
      "Iteration 16, loss = 0.16308116\n",
      "Iteration 17, loss = 0.13341341\n",
      "Iteration 18, loss = 0.14588063\n",
      "Iteration 19, loss = 0.12329507\n",
      "Iteration 20, loss = 0.11339759\n",
      "Iteration 21, loss = 0.11242638\n",
      "Iteration 22, loss = 0.11303329\n",
      "Iteration 23, loss = 0.10410041\n",
      "Iteration 1, loss = 1.06563253\n",
      "Iteration 24, loss = 0.09817950\n",
      "Iteration 2, loss = 0.47155004\n",
      "Iteration 25, loss = 0.08717300\n",
      "Iteration 3, loss = 0.40848483\n",
      "Iteration 26, loss = 0.09063701\n",
      "Iteration 4, loss = 0.37886167\n",
      "Iteration 27, loss = 0.07532956\n",
      "Iteration 5, loss = 0.32714532\n",
      "Iteration 28, loss = 0.12302745\n",
      "Iteration 6, loss = 0.29011756\n",
      "Iteration 29, loss = 0.12340228\n",
      "Iteration 7, loss = 0.26876829\n",
      "Iteration 30, loss = 0.10335390\n",
      "Iteration 8, loss = 0.24474862\n",
      "Iteration 31, loss = 0.14950221\n",
      "Iteration 9, loss = 0.23291932\n",
      "Iteration 32, loss = 0.19616806\n",
      "Iteration 10, loss = 0.21534411\n",
      "Iteration 33, loss = 0.15032277\n",
      "Iteration 11, loss = 0.19826602\n",
      "Iteration 34, loss = 0.14072076\n",
      "Iteration 12, loss = 0.18483666\n",
      "Iteration 35, loss = 0.09411017\n",
      "Iteration 13, loss = 0.17527760\n",
      "Iteration 36, loss = 0.07558648\n",
      "Iteration 14, loss = 0.19333005\n",
      "Iteration 37, loss = 0.07292396\n",
      "Iteration 15, loss = 0.20269193\n",
      "Iteration 38, loss = 0.07487551\n",
      "Iteration 16, loss = 0.19388395\n",
      "Iteration 39, loss = 0.08968360\n",
      "Iteration 17, loss = 0.17872158\n",
      "Iteration 40, loss = 0.10626137\n",
      "Iteration 18, loss = 0.16237625\n",
      "Iteration 41, loss = 0.06660088\n",
      "Iteration 19, loss = 0.14940215\n",
      "Iteration 42, loss = 0.06883647\n",
      "Iteration 20, loss = 0.14417834\n",
      "Iteration 43, loss = 0.06366403\n",
      "Iteration 21, loss = 0.12800837\n",
      "Iteration 44, loss = 0.05308449\n",
      "Iteration 22, loss = 0.12063243\n",
      "Iteration 45, loss = 0.05944948\n",
      "Iteration 23, loss = 0.11244837\n",
      "Iteration 46, loss = 0.05610544\n",
      "Iteration 24, loss = 0.11406730\n",
      "Iteration 47, loss = 0.04215989\n",
      "Iteration 25, loss = 0.12061919\n",
      "Iteration 48, loss = 0.04053820\n",
      "Iteration 26, loss = 0.11424519\n",
      "Iteration 49, loss = 0.04555917\n",
      "Iteration 27, loss = 0.11516481\n",
      "Iteration 50, loss = 0.06487332\n",
      "Iteration 28, loss = 0.10735878\n",
      "Iteration 51, loss = 0.04758838\n",
      "Iteration 29, loss = 0.10533707\n",
      "Iteration 52, loss = 0.03389890\n",
      "Iteration 30, loss = 0.09619335\n",
      "Iteration 53, loss = 0.02953117\n",
      "Iteration 31, loss = 0.09577188\n",
      "Iteration 54, loss = 0.03261520\n",
      "Iteration 32, loss = 0.09088566\n",
      "Iteration 55, loss = 0.02504466\n",
      "Iteration 33, loss = 0.09128838\n",
      "Iteration 56, loss = 0.03049698\n",
      "Iteration 34, loss = 0.09221066\n",
      "Iteration 57, loss = 0.03090438\n",
      "Iteration 35, loss = 0.08955010\n",
      "Iteration 58, loss = 0.03386900\n",
      "Iteration 36, loss = 0.09129930\n",
      "Iteration 59, loss = 0.04386612\n",
      "Iteration 37, loss = 0.08423431\n",
      "Iteration 60, loss = 0.02818821\n",
      "Iteration 38, loss = 0.08794661\n",
      "Iteration 61, loss = 0.03328278\n",
      "Iteration 39, loss = 0.07316251\n",
      "Iteration 62, loss = 0.02556929\n",
      "Iteration 40, loss = 0.07382917\n",
      "Iteration 63, loss = 0.03576789\n",
      "Iteration 41, loss = 0.06844496\n",
      "Iteration 64, loss = 0.02792637\n",
      "Iteration 42, loss = 0.07834959\n",
      "Iteration 65, loss = 0.03014424\n",
      "Iteration 43, loss = 0.12231625\n",
      "Iteration 66, loss = 0.02838955\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.11100126\n",
      "Iteration 45, loss = 0.11857855\n",
      "Iteration 46, loss = 0.12553500\n",
      "Iteration 47, loss = 0.14536388\n",
      "Iteration 48, loss = 0.10973544\n",
      "Iteration 49, loss = 0.10224905\n",
      "Iteration 50, loss = 0.09155889\n",
      "Iteration 51, loss = 0.07088103\n",
      "Iteration 52, loss = 0.07493748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93347536\n",
      "Iteration 2, loss = 0.48313855\n",
      "Iteration 3, loss = 0.42370204\n",
      "Iteration 4, loss = 0.35839858\n",
      "Iteration 5, loss = 0.32665595\n",
      "Iteration 6, loss = 0.29598927\n",
      "Iteration 7, loss = 0.26992046\n",
      "Iteration 8, loss = 0.25836149\n",
      "Iteration 9, loss = 0.25097022\n",
      "Iteration 1, loss = 0.92927197\n",
      "Iteration 10, loss = 0.23478765\n",
      "Iteration 2, loss = 0.54987573\n",
      "Iteration 11, loss = 0.21648474\n",
      "Iteration 3, loss = 0.41310466\n",
      "Iteration 12, loss = 0.21594074\n",
      "Iteration 4, loss = 0.34337987\n",
      "Iteration 13, loss = 0.18519432\n",
      "Iteration 5, loss = 0.30546508\n",
      "Iteration 14, loss = 0.16705969\n",
      "Iteration 6, loss = 0.29327611\n",
      "Iteration 15, loss = 0.16711449\n",
      "Iteration 7, loss = 0.27573371\n",
      "Iteration 16, loss = 0.15564854\n",
      "Iteration 8, loss = 0.24112684\n",
      "Iteration 17, loss = 0.14944726\n",
      "Iteration 9, loss = 0.23031365\n",
      "Iteration 18, loss = 0.14519560\n",
      "Iteration 10, loss = 0.20856620\n",
      "Iteration 19, loss = 0.14227172\n",
      "Iteration 11, loss = 0.20229777\n",
      "Iteration 20, loss = 0.12354089\n",
      "Iteration 12, loss = 0.19258772\n",
      "Iteration 21, loss = 0.12250307\n",
      "Iteration 13, loss = 0.17821465\n",
      "Iteration 22, loss = 0.12817272\n",
      "Iteration 14, loss = 0.16234144\n",
      "Iteration 23, loss = 0.13182323\n",
      "Iteration 15, loss = 0.15730142\n",
      "Iteration 24, loss = 0.15229664\n",
      "Iteration 16, loss = 0.15999660\n",
      "Iteration 25, loss = 0.14764410\n",
      "Iteration 17, loss = 0.16937397\n",
      "Iteration 26, loss = 0.14145968\n",
      "Iteration 18, loss = 0.16475328\n",
      "Iteration 27, loss = 0.14962806\n",
      "Iteration 19, loss = 0.15799477\n",
      "Iteration 28, loss = 0.14036707\n",
      "Iteration 20, loss = 0.14631991\n",
      "Iteration 29, loss = 0.12157055\n",
      "Iteration 21, loss = 0.15287030\n",
      "Iteration 30, loss = 0.10953506\n",
      "Iteration 22, loss = 0.12596007\n",
      "Iteration 31, loss = 0.11226922\n",
      "Iteration 23, loss = 0.11338905\n",
      "Iteration 32, loss = 0.09753285\n",
      "Iteration 24, loss = 0.10834067\n",
      "Iteration 33, loss = 0.10725068\n",
      "Iteration 25, loss = 0.12588155\n",
      "Iteration 34, loss = 0.09506967\n",
      "Iteration 26, loss = 0.11481185\n",
      "Iteration 35, loss = 0.10489327\n",
      "Iteration 27, loss = 0.10488298\n",
      "Iteration 36, loss = 0.08594042\n",
      "Iteration 28, loss = 0.10024101\n",
      "Iteration 37, loss = 0.08389485\n",
      "Iteration 29, loss = 0.09606179\n",
      "Iteration 38, loss = 0.07676433\n",
      "Iteration 30, loss = 0.09330443\n",
      "Iteration 39, loss = 0.06977666\n",
      "Iteration 31, loss = 0.08877979\n",
      "Iteration 40, loss = 0.07351243\n",
      "Iteration 32, loss = 0.08017109\n",
      "Iteration 41, loss = 0.08568879\n",
      "Iteration 33, loss = 0.07405288\n",
      "Iteration 42, loss = 0.09012408\n",
      "Iteration 34, loss = 0.06855344\n",
      "Iteration 43, loss = 0.07735716\n",
      "Iteration 35, loss = 0.07226683\n",
      "Iteration 44, loss = 0.09896498\n",
      "Iteration 45, loss = 0.16723814\n",
      "Iteration 36, loss = 0.06414653\n",
      "Iteration 46, loss = 0.15767759\n",
      "Iteration 37, loss = 0.06260339\n",
      "Iteration 47, loss = 0.11683466\n",
      "Iteration 38, loss = 0.06817192\n",
      "Iteration 39, loss = 0.07383428\n",
      "Iteration 48, loss = 0.12020652\n",
      "Iteration 40, loss = 0.09901218\n",
      "Iteration 49, loss = 0.08588236\n",
      "Iteration 41, loss = 0.07849587\n",
      "Iteration 50, loss = 0.08485383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.09688266\n",
      "Iteration 43, loss = 0.07492020\n",
      "Iteration 44, loss = 0.06958703\n",
      "Iteration 45, loss = 0.07261868\n",
      "Iteration 46, loss = 0.06570823\n",
      "Iteration 47, loss = 0.05121103\n",
      "Iteration 48, loss = 0.06417421\n",
      "Iteration 49, loss = 0.09267228\n",
      "Iteration 50, loss = 0.08049116\n",
      "Iteration 51, loss = 0.09399702\n",
      "Iteration 52, loss = 0.09419789\n",
      "Iteration 53, loss = 0.07574676\n",
      "Iteration 54, loss = 0.05990347\n",
      "Iteration 1, loss = 1.02363780\n",
      "Iteration 55, loss = 0.06646046\n",
      "Iteration 2, loss = 0.51100064\n",
      "Iteration 56, loss = 0.05737380\n",
      "Iteration 3, loss = 0.45182768\n",
      "Iteration 57, loss = 0.04723610\n",
      "Iteration 4, loss = 0.39572746\n",
      "Iteration 58, loss = 0.04410903\n",
      "Iteration 5, loss = 0.37060247\n",
      "Iteration 59, loss = 0.04412337\n",
      "Iteration 6, loss = 0.33606800\n",
      "Iteration 60, loss = 0.05654554\n",
      "Iteration 7, loss = 0.29898089\n",
      "Iteration 61, loss = 0.04974820\n",
      "Iteration 8, loss = 0.28111090\n",
      "Iteration 62, loss = 0.03863157\n",
      "Iteration 9, loss = 0.25663540\n",
      "Iteration 63, loss = 0.04685890\n",
      "Iteration 10, loss = 0.22504320\n",
      "Iteration 64, loss = 0.04548541\n",
      "Iteration 11, loss = 0.21759237\n",
      "Iteration 65, loss = 0.04075189\n",
      "Iteration 12, loss = 0.20973054\n",
      "Iteration 66, loss = 0.03676167\n",
      "Iteration 13, loss = 0.19835829\n",
      "Iteration 67, loss = 0.03403671\n",
      "Iteration 14, loss = 0.18787819\n",
      "Iteration 68, loss = 0.03399802\n",
      "Iteration 15, loss = 0.17317125\n",
      "Iteration 69, loss = 0.03502239\n",
      "Iteration 16, loss = 0.17029977\n",
      "Iteration 70, loss = 0.03672412\n",
      "Iteration 17, loss = 0.16472923\n",
      "Iteration 71, loss = 0.03285663\n",
      "Iteration 18, loss = 0.14941341\n",
      "Iteration 72, loss = 0.04524020\n",
      "Iteration 19, loss = 0.14478926\n",
      "Iteration 73, loss = 0.05047641\n",
      "Iteration 20, loss = 0.13564228\n",
      "Iteration 74, loss = 0.05187215\n",
      "Iteration 21, loss = 0.12593397\n",
      "Iteration 75, loss = 0.05975594\n",
      "Iteration 22, loss = 0.13311989\n",
      "Iteration 76, loss = 0.07206903\n",
      "Iteration 23, loss = 0.13255281\n",
      "Iteration 77, loss = 0.05670416\n",
      "Iteration 24, loss = 0.16264072\n",
      "Iteration 78, loss = 0.05659358\n",
      "Iteration 25, loss = 0.14081856\n",
      "Iteration 26, loss = 0.14869128\n",
      "Iteration 79, loss = 0.05066774\n",
      "Iteration 27, loss = 0.13418753\n",
      "Iteration 80, loss = 0.03392413\n",
      "Iteration 28, loss = 0.11498510\n",
      "Iteration 81, loss = 0.03221866\n",
      "Iteration 29, loss = 0.12829714\n",
      "Iteration 82, loss = 0.03857331\n",
      "Iteration 30, loss = 0.10910734\n",
      "Iteration 83, loss = 0.03746880\n",
      "Iteration 84, loss = 0.05101114\n",
      "Iteration 31, loss = 0.11486645\n",
      "Iteration 85, loss = 0.04505913\n",
      "Iteration 32, loss = 0.10340158\n",
      "Iteration 86, loss = 0.03473498\n",
      "Iteration 33, loss = 0.10811595\n",
      "Iteration 34, loss = 0.10307899\n",
      "Iteration 87, loss = 0.05037900\n",
      "Iteration 35, loss = 0.08260457\n",
      "Iteration 88, loss = 0.05340417\n",
      "Iteration 36, loss = 0.10920415\n",
      "Iteration 89, loss = 0.04555313\n",
      "Iteration 37, loss = 0.10100837\n",
      "Iteration 90, loss = 0.05237520\n",
      "Iteration 38, loss = 0.11055271\n",
      "Iteration 91, loss = 0.08569298\n",
      "Iteration 39, loss = 0.08802449\n",
      "Iteration 92, loss = 0.13267082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.08981450\n",
      "Iteration 41, loss = 0.08327979\n",
      "Iteration 42, loss = 0.06943060\n",
      "Iteration 43, loss = 0.06814361\n",
      "Iteration 44, loss = 0.08073672\n",
      "Iteration 45, loss = 0.08000858\n",
      "Iteration 46, loss = 0.07605733\n",
      "Iteration 47, loss = 0.06755836\n",
      "Iteration 48, loss = 0.05717645\n",
      "Iteration 49, loss = 0.05879570\n",
      "Iteration 50, loss = 0.05689553\n",
      "Iteration 51, loss = 0.06043403\n",
      "Iteration 1, loss = 0.92429619\n",
      "Iteration 52, loss = 0.05066989\n",
      "Iteration 2, loss = 0.46771072\n",
      "Iteration 53, loss = 0.05798372\n",
      "Iteration 54, loss = 0.08408792\n",
      "Iteration 3, loss = 0.38919621\n",
      "Iteration 4, loss = 0.34966562\n",
      "Iteration 55, loss = 0.07481747\n",
      "Iteration 5, loss = 0.31014219\n",
      "Iteration 56, loss = 0.05899733\n",
      "Iteration 6, loss = 0.29236146\n",
      "Iteration 57, loss = 0.06110794\n",
      "Iteration 7, loss = 0.27528296\n",
      "Iteration 58, loss = 0.06276440\n",
      "Iteration 8, loss = 0.24336284\n",
      "Iteration 59, loss = 0.06716807\n",
      "Iteration 9, loss = 0.23002591\n",
      "Iteration 60, loss = 0.06843337\n",
      "Iteration 10, loss = 0.21230973\n",
      "Iteration 61, loss = 0.06464177\n",
      "Iteration 11, loss = 0.19650071\n",
      "Iteration 62, loss = 0.10093197\n",
      "Iteration 12, loss = 0.18965338\n",
      "Iteration 63, loss = 0.15371331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.17749999\n",
      "Iteration 14, loss = 0.16638772\n",
      "Iteration 15, loss = 0.15455111\n",
      "Iteration 16, loss = 0.14866692\n",
      "Iteration 17, loss = 0.15917002\n",
      "Iteration 18, loss = 0.12555146\n",
      "Iteration 19, loss = 0.13702385\n",
      "Iteration 20, loss = 0.12669060\n",
      "Iteration 21, loss = 0.10918214\n",
      "Iteration 22, loss = 0.10762233\n",
      "Iteration 23, loss = 0.10762817\n",
      "Iteration 24, loss = 0.11500826\n",
      "Iteration 1, loss = 0.90264350\n",
      "Iteration 25, loss = 0.10325520\n",
      "Iteration 2, loss = 0.51653585\n",
      "Iteration 26, loss = 0.09598225\n",
      "Iteration 3, loss = 0.42099437\n",
      "Iteration 27, loss = 0.09232552\n",
      "Iteration 4, loss = 0.37579511\n",
      "Iteration 28, loss = 0.09529898\n",
      "Iteration 5, loss = 0.35140599\n",
      "Iteration 29, loss = 0.08726173\n",
      "Iteration 6, loss = 0.30906048\n",
      "Iteration 30, loss = 0.09818321\n",
      "Iteration 7, loss = 0.27733416\n",
      "Iteration 31, loss = 0.13933098\n",
      "Iteration 8, loss = 0.26665198\n",
      "Iteration 32, loss = 0.10548549\n",
      "Iteration 9, loss = 0.25714804\n",
      "Iteration 33, loss = 0.09797174\n",
      "Iteration 10, loss = 0.22892818\n",
      "Iteration 34, loss = 0.14870100\n",
      "Iteration 11, loss = 0.21415079\n",
      "Iteration 35, loss = 0.11845456\n",
      "Iteration 12, loss = 0.20174368\n",
      "Iteration 36, loss = 0.15191194\n",
      "Iteration 13, loss = 0.19790205\n",
      "Iteration 37, loss = 0.12353927\n",
      "Iteration 14, loss = 0.19024779\n",
      "Iteration 38, loss = 0.12373676\n",
      "Iteration 15, loss = 0.17133018\n",
      "Iteration 39, loss = 0.10543738\n",
      "Iteration 16, loss = 0.16397385\n",
      "Iteration 40, loss = 0.09420906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.16490323\n",
      "Iteration 18, loss = 0.15749559\n",
      "Iteration 19, loss = 0.16048518\n",
      "Iteration 20, loss = 0.14087013\n",
      "Iteration 21, loss = 0.15449431\n",
      "Iteration 22, loss = 0.13508215\n",
      "Iteration 23, loss = 0.11370243\n",
      "Iteration 24, loss = 0.12077956\n",
      "Iteration 25, loss = 0.12297146\n",
      "Iteration 26, loss = 0.13827578\n",
      "Iteration 27, loss = 0.11126404\n",
      "Iteration 28, loss = 0.10058812\n",
      "Iteration 29, loss = 0.10429359\n",
      "Iteration 1, loss = 1.04813457\n",
      "Iteration 30, loss = 0.11969630\n",
      "Iteration 2, loss = 0.46777439\n",
      "Iteration 31, loss = 0.09376290\n",
      "Iteration 3, loss = 0.40774930\n",
      "Iteration 32, loss = 0.11694784\n",
      "Iteration 4, loss = 0.35223084\n",
      "Iteration 33, loss = 0.10428030\n",
      "Iteration 5, loss = 0.31207598\n",
      "Iteration 34, loss = 0.08887666\n",
      "Iteration 6, loss = 0.27643637\n",
      "Iteration 35, loss = 0.07643107\n",
      "Iteration 7, loss = 0.25381655\n",
      "Iteration 36, loss = 0.08076795\n",
      "Iteration 8, loss = 0.22933125\n",
      "Iteration 37, loss = 0.07421985\n",
      "Iteration 9, loss = 0.20874812\n",
      "Iteration 38, loss = 0.06821231\n",
      "Iteration 10, loss = 0.22130051\n",
      "Iteration 39, loss = 0.07588469\n",
      "Iteration 11, loss = 0.19121417\n",
      "Iteration 40, loss = 0.09078063\n",
      "Iteration 12, loss = 0.18304876\n",
      "Iteration 41, loss = 0.08306922\n",
      "Iteration 13, loss = 0.17062256\n",
      "Iteration 42, loss = 0.07021447\n",
      "Iteration 14, loss = 0.16844345\n",
      "Iteration 43, loss = 0.08322697\n",
      "Iteration 15, loss = 0.14645466\n",
      "Iteration 44, loss = 0.08529832\n",
      "Iteration 16, loss = 0.13382399\n",
      "Iteration 45, loss = 0.08988175\n",
      "Iteration 17, loss = 0.12602777\n",
      "Iteration 46, loss = 0.08705180\n",
      "Iteration 18, loss = 0.11700373\n",
      "Iteration 47, loss = 0.07246469\n",
      "Iteration 19, loss = 0.11174791\n",
      "Iteration 48, loss = 0.08200535\n",
      "Iteration 20, loss = 0.09822439\n",
      "Iteration 49, loss = 0.10051964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.10984159\n",
      "Iteration 22, loss = 0.13355732\n",
      "Iteration 23, loss = 0.12256950\n",
      "Iteration 24, loss = 0.12636345\n",
      "Iteration 25, loss = 0.10462806\n",
      "Iteration 26, loss = 0.10016704\n",
      "Iteration 27, loss = 0.09464295\n",
      "Iteration 28, loss = 0.07973604\n",
      "Iteration 29, loss = 0.07349339\n",
      "Iteration 30, loss = 0.08313884\n",
      "Iteration 31, loss = 0.06886210\n",
      "Iteration 32, loss = 0.06378327\n",
      "Iteration 33, loss = 0.07294610\n",
      "Iteration 1, loss = 1.13558205\n",
      "Iteration 34, loss = 0.06514142\n",
      "Iteration 2, loss = 0.48297118\n",
      "Iteration 35, loss = 0.05769215\n",
      "Iteration 3, loss = 0.38999569\n",
      "Iteration 36, loss = 0.05190581\n",
      "Iteration 4, loss = 0.33584510\n",
      "Iteration 37, loss = 0.06426891\n",
      "Iteration 5, loss = 0.30900548\n",
      "Iteration 38, loss = 0.06198544\n",
      "Iteration 6, loss = 0.26974539\n",
      "Iteration 39, loss = 0.05558040\n",
      "Iteration 7, loss = 0.24710038\n",
      "Iteration 40, loss = 0.07757291\n",
      "Iteration 8, loss = 0.22536113\n",
      "Iteration 41, loss = 0.09551717\n",
      "Iteration 9, loss = 0.20799464\n",
      "Iteration 42, loss = 0.08655683\n",
      "Iteration 10, loss = 0.20087118\n",
      "Iteration 43, loss = 0.06803114\n",
      "Iteration 11, loss = 0.18795983\n",
      "Iteration 44, loss = 0.07939014\n",
      "Iteration 12, loss = 0.17877752\n",
      "Iteration 45, loss = 0.06890132\n",
      "Iteration 13, loss = 0.16158571\n",
      "Iteration 46, loss = 0.04666440\n",
      "Iteration 14, loss = 0.16001009\n",
      "Iteration 47, loss = 0.04375461\n",
      "Iteration 15, loss = 0.15982471\n",
      "Iteration 48, loss = 0.04044532\n",
      "Iteration 16, loss = 0.14125429\n",
      "Iteration 49, loss = 0.03721413\n",
      "Iteration 17, loss = 0.13573571\n",
      "Iteration 50, loss = 0.03426467\n",
      "Iteration 18, loss = 0.14144049\n",
      "Iteration 51, loss = 0.04241764\n",
      "Iteration 19, loss = 0.11486800\n",
      "Iteration 52, loss = 0.03347125\n",
      "Iteration 20, loss = 0.11486540\n",
      "Iteration 53, loss = 0.03553360\n",
      "Iteration 21, loss = 0.11250185\n",
      "Iteration 54, loss = 0.06252823\n",
      "Iteration 22, loss = 0.10879013\n",
      "Iteration 55, loss = 0.04396444\n",
      "Iteration 23, loss = 0.09518698\n",
      "Iteration 56, loss = 0.04327538\n",
      "Iteration 24, loss = 0.09159876\n",
      "Iteration 57, loss = 0.04056841\n",
      "Iteration 25, loss = 0.08936868\n",
      "Iteration 58, loss = 0.03479887\n",
      "Iteration 26, loss = 0.09109822\n",
      "Iteration 59, loss = 0.02764513\n",
      "Iteration 27, loss = 0.09269109\n",
      "Iteration 60, loss = 0.03085558\n",
      "Iteration 28, loss = 0.07916841\n",
      "Iteration 61, loss = 0.03110105\n",
      "Iteration 29, loss = 0.07746873\n",
      "Iteration 62, loss = 0.02952491\n",
      "Iteration 30, loss = 0.07770969\n",
      "Iteration 63, loss = 0.02617412\n",
      "Iteration 31, loss = 0.07802671\n",
      "Iteration 64, loss = 0.03693897\n",
      "Iteration 32, loss = 0.07861972\n",
      "Iteration 65, loss = 0.02855927\n",
      "Iteration 33, loss = 0.08559099\n",
      "Iteration 66, loss = 0.02942028\n",
      "Iteration 34, loss = 0.07715301\n",
      "Iteration 67, loss = 0.02394432\n",
      "Iteration 35, loss = 0.07103188\n",
      "Iteration 68, loss = 0.02580418\n",
      "Iteration 36, loss = 0.10277173\n",
      "Iteration 69, loss = 0.03049552\n",
      "Iteration 37, loss = 0.14186593\n",
      "Iteration 70, loss = 0.02445863\n",
      "Iteration 38, loss = 0.11619599\n",
      "Iteration 71, loss = 0.03012569\n",
      "Iteration 39, loss = 0.13789532\n",
      "Iteration 72, loss = 0.03742279\n",
      "Iteration 40, loss = 0.12402976\n",
      "Iteration 73, loss = 0.03881284\n",
      "Iteration 41, loss = 0.09660978\n",
      "Iteration 74, loss = 0.03086492\n",
      "Iteration 42, loss = 0.09150734\n",
      "Iteration 75, loss = 0.01975682\n",
      "Iteration 43, loss = 0.07342969\n",
      "Iteration 76, loss = 0.03961846\n",
      "Iteration 44, loss = 0.08055360\n",
      "Iteration 77, loss = 0.03795765\n",
      "Iteration 45, loss = 0.05957093\n",
      "Iteration 78, loss = 0.05942344\n",
      "Iteration 46, loss = 0.04962539\n",
      "Iteration 79, loss = 0.06878434\n",
      "Iteration 47, loss = 0.04539179\n",
      "Iteration 80, loss = 0.05298549\n",
      "Iteration 48, loss = 0.05412182\n",
      "Iteration 49, loss = 0.06728837\n",
      "Iteration 81, loss = 0.07992450\n",
      "Iteration 50, loss = 0.08582514\n",
      "Iteration 82, loss = 0.10623377\n",
      "Iteration 51, loss = 0.06898046\n",
      "Iteration 83, loss = 0.09635411\n",
      "Iteration 52, loss = 0.06097313\n",
      "Iteration 84, loss = 0.12199007\n",
      "Iteration 53, loss = 0.05230115\n",
      "Iteration 85, loss = 0.13461878\n",
      "Iteration 86, loss = 0.13215110\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.04364328\n",
      "Iteration 55, loss = 0.03791873\n",
      "Iteration 56, loss = 0.03935607\n",
      "Iteration 57, loss = 0.03812878\n",
      "Iteration 58, loss = 0.03498171\n",
      "Iteration 59, loss = 0.03768161\n",
      "Iteration 60, loss = 0.03471884\n",
      "Iteration 61, loss = 0.03426362\n",
      "Iteration 62, loss = 0.03390106\n",
      "Iteration 63, loss = 0.03946662\n",
      "Iteration 64, loss = 0.03850621\n",
      "Iteration 65, loss = 0.03599377\n",
      "Iteration 66, loss = 0.03101583\n",
      "Iteration 1, loss = 1.03681207\n",
      "Iteration 67, loss = 0.03148139\n",
      "Iteration 2, loss = 0.50991241\n",
      "Iteration 68, loss = 0.03869021\n",
      "Iteration 3, loss = 0.39789365\n",
      "Iteration 69, loss = 0.03853531\n",
      "Iteration 4, loss = 0.35990546\n",
      "Iteration 70, loss = 0.04161854\n",
      "Iteration 5, loss = 0.33849713\n",
      "Iteration 71, loss = 0.03599288\n",
      "Iteration 6, loss = 0.30233374\n",
      "Iteration 72, loss = 0.03514818\n",
      "Iteration 7, loss = 0.28105150\n",
      "Iteration 73, loss = 0.03353862\n",
      "Iteration 8, loss = 0.25413350\n",
      "Iteration 74, loss = 0.04691658\n",
      "Iteration 9, loss = 0.24293357\n",
      "Iteration 75, loss = 0.03931183\n",
      "Iteration 10, loss = 0.21893740\n",
      "Iteration 76, loss = 0.05903126\n",
      "Iteration 11, loss = 0.19987336\n",
      "Iteration 77, loss = 0.05086220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.18895193\n",
      "Iteration 13, loss = 0.18318485\n",
      "Iteration 14, loss = 0.17410313\n",
      "Iteration 15, loss = 0.17355145\n",
      "Iteration 16, loss = 0.14835388\n",
      "Iteration 17, loss = 0.15521206\n",
      "Iteration 18, loss = 0.15568820\n",
      "Iteration 19, loss = 0.14482443\n",
      "Iteration 20, loss = 0.12695285\n",
      "Iteration 21, loss = 0.12436690\n",
      "Iteration 22, loss = 0.11714320\n",
      "Iteration 23, loss = 0.13335006\n",
      "Iteration 24, loss = 0.13392416\n",
      "Iteration 1, loss = 1.11193824\n",
      "Iteration 25, loss = 0.16332448\n",
      "Iteration 2, loss = 0.50117420\n",
      "Iteration 26, loss = 0.13052445\n",
      "Iteration 3, loss = 0.42823878\n",
      "Iteration 27, loss = 0.16823906\n",
      "Iteration 4, loss = 0.35802088\n",
      "Iteration 28, loss = 0.15498048\n",
      "Iteration 5, loss = 0.31712538\n",
      "Iteration 29, loss = 0.12916203\n",
      "Iteration 6, loss = 0.30200218\n",
      "Iteration 30, loss = 0.14242254\n",
      "Iteration 7, loss = 0.26791259\n",
      "Iteration 31, loss = 0.10834769\n",
      "Iteration 8, loss = 0.24897251\n",
      "Iteration 32, loss = 0.09672212\n",
      "Iteration 9, loss = 0.22598718\n",
      "Iteration 33, loss = 0.08647492\n",
      "Iteration 10, loss = 0.21293384\n",
      "Iteration 11, loss = 0.19856181\n",
      "Iteration 34, loss = 0.08340641\n",
      "Iteration 12, loss = 0.18690474\n",
      "Iteration 35, loss = 0.08535965\n",
      "Iteration 13, loss = 0.16966218\n",
      "Iteration 36, loss = 0.07960021\n",
      "Iteration 14, loss = 0.16675539\n",
      "Iteration 37, loss = 0.07469442\n",
      "Iteration 15, loss = 0.14790172\n",
      "Iteration 38, loss = 0.07922661\n",
      "Iteration 16, loss = 0.15909639\n",
      "Iteration 39, loss = 0.08048146\n",
      "Iteration 17, loss = 0.15035674\n",
      "Iteration 40, loss = 0.07251370\n",
      "Iteration 18, loss = 0.14178863\n",
      "Iteration 41, loss = 0.08377564\n",
      "Iteration 19, loss = 0.13917190\n",
      "Iteration 42, loss = 0.08686678\n",
      "Iteration 20, loss = 0.11602113\n",
      "Iteration 43, loss = 0.07505956\n",
      "Iteration 21, loss = 0.11225212\n",
      "Iteration 44, loss = 0.08801436\n",
      "Iteration 22, loss = 0.12239921\n",
      "Iteration 45, loss = 0.07626691\n",
      "Iteration 23, loss = 0.09244077\n",
      "Iteration 46, loss = 0.07648192\n",
      "Iteration 24, loss = 0.09390565\n",
      "Iteration 47, loss = 0.08772881\n",
      "Iteration 25, loss = 0.08580005\n",
      "Iteration 48, loss = 0.07711365\n",
      "Iteration 26, loss = 0.08543593\n",
      "Iteration 49, loss = 0.06724918\n",
      "Iteration 27, loss = 0.08274090\n",
      "Iteration 50, loss = 0.06784917\n",
      "Iteration 28, loss = 0.07751755\n",
      "Iteration 51, loss = 0.06865034\n",
      "Iteration 29, loss = 0.06850559\n",
      "Iteration 52, loss = 0.05650620\n",
      "Iteration 30, loss = 0.07281438\n",
      "Iteration 53, loss = 0.06569624\n",
      "Iteration 31, loss = 0.07258625\n",
      "Iteration 54, loss = 0.04794367\n",
      "Iteration 32, loss = 0.06807558\n",
      "Iteration 55, loss = 0.04741788\n",
      "Iteration 33, loss = 0.05619147\n",
      "Iteration 56, loss = 0.04480697\n",
      "Iteration 34, loss = 0.06551747\n",
      "Iteration 57, loss = 0.03833851\n",
      "Iteration 35, loss = 0.05838225\n",
      "Iteration 58, loss = 0.03776492\n",
      "Iteration 36, loss = 0.06394226\n",
      "Iteration 59, loss = 0.03598622\n",
      "Iteration 37, loss = 0.09430802\n",
      "Iteration 60, loss = 0.03866543\n",
      "Iteration 38, loss = 0.09558936\n",
      "Iteration 61, loss = 0.03760611\n",
      "Iteration 39, loss = 0.12827097\n",
      "Iteration 62, loss = 0.03597202\n",
      "Iteration 40, loss = 0.10367899\n",
      "Iteration 63, loss = 0.03982188\n",
      "Iteration 41, loss = 0.11639585\n",
      "Iteration 64, loss = 0.04508008\n",
      "Iteration 42, loss = 0.20193464\n",
      "Iteration 65, loss = 0.04067343\n",
      "Iteration 43, loss = 0.12395673\n",
      "Iteration 66, loss = 0.04432987\n",
      "Iteration 44, loss = 0.14802748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 67, loss = 0.04853351\n",
      "Iteration 68, loss = 0.03492291\n",
      "Iteration 69, loss = 0.03468002\n",
      "Iteration 70, loss = 0.03502514\n",
      "Iteration 71, loss = 0.03517197\n",
      "Iteration 72, loss = 0.03977672\n",
      "Iteration 73, loss = 0.03588864\n",
      "Iteration 74, loss = 0.04266412\n",
      "Iteration 75, loss = 0.04307903\n",
      "Iteration 76, loss = 0.03691536\n",
      "Iteration 77, loss = 0.05801406\n",
      "Iteration 78, loss = 0.05602066\n",
      "Iteration 79, loss = 0.04291545\n",
      "Iteration 1, loss = 0.85734741\n",
      "Iteration 80, loss = 0.03546616\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.45046531\n",
      "Iteration 3, loss = 0.40048826\n",
      "Iteration 4, loss = 0.33493064\n",
      "Iteration 5, loss = 0.30135400\n",
      "Iteration 6, loss = 0.28598391\n",
      "Iteration 7, loss = 0.26177320\n",
      "Iteration 8, loss = 0.24036516\n",
      "Iteration 9, loss = 0.21681184\n",
      "Iteration 10, loss = 0.21384107\n",
      "Iteration 11, loss = 0.18925156\n",
      "Iteration 12, loss = 0.17336179\n",
      "Iteration 13, loss = 0.16563573\n",
      "Iteration 14, loss = 0.16124539\n",
      "Iteration 1, loss = 0.85394801\n",
      "Iteration 15, loss = 0.15895686\n",
      "Iteration 2, loss = 0.44258092\n",
      "Iteration 16, loss = 0.15592818\n",
      "Iteration 3, loss = 0.39733863\n",
      "Iteration 17, loss = 0.14006274\n",
      "Iteration 4, loss = 0.34339161\n",
      "Iteration 18, loss = 0.16647789\n",
      "Iteration 5, loss = 0.32635357\n",
      "Iteration 19, loss = 0.16109222\n",
      "Iteration 6, loss = 0.28340123\n",
      "Iteration 20, loss = 0.18683783\n",
      "Iteration 7, loss = 0.25364995\n",
      "Iteration 21, loss = 0.17816486\n",
      "Iteration 8, loss = 0.24741252\n",
      "Iteration 22, loss = 0.18086819\n",
      "Iteration 9, loss = 0.23955615\n",
      "Iteration 23, loss = 0.17902099\n",
      "Iteration 10, loss = 0.20146575\n",
      "Iteration 24, loss = 0.15580881\n",
      "Iteration 11, loss = 0.18578948\n",
      "Iteration 25, loss = 0.13509832\n",
      "Iteration 12, loss = 0.18392911\n",
      "Iteration 26, loss = 0.11222458\n",
      "Iteration 13, loss = 0.20652979\n",
      "Iteration 27, loss = 0.10901319\n",
      "Iteration 14, loss = 0.17388670\n",
      "Iteration 28, loss = 0.10608978\n",
      "Iteration 15, loss = 0.15748227\n",
      "Iteration 29, loss = 0.10220190\n",
      "Iteration 16, loss = 0.14235670\n",
      "Iteration 30, loss = 0.09507760\n",
      "Iteration 17, loss = 0.13575619\n",
      "Iteration 31, loss = 0.10170698\n",
      "Iteration 18, loss = 0.13304373\n",
      "Iteration 32, loss = 0.09565402\n",
      "Iteration 19, loss = 0.11703160\n",
      "Iteration 33, loss = 0.09164120\n",
      "Iteration 20, loss = 0.11628572\n",
      "Iteration 34, loss = 0.09942681\n",
      "Iteration 21, loss = 0.10802727\n",
      "Iteration 35, loss = 0.09956764\n",
      "Iteration 22, loss = 0.10364142\n",
      "Iteration 36, loss = 0.11961299\n",
      "Iteration 23, loss = 0.09893056\n",
      "Iteration 37, loss = 0.10849361\n",
      "Iteration 24, loss = 0.09089472\n",
      "Iteration 38, loss = 0.10541431\n",
      "Iteration 25, loss = 0.11917082\n",
      "Iteration 39, loss = 0.11134215\n",
      "Iteration 26, loss = 0.09704238\n",
      "Iteration 40, loss = 0.10976623\n",
      "Iteration 27, loss = 0.12074459\n",
      "Iteration 41, loss = 0.12365117\n",
      "Iteration 28, loss = 0.09661291\n",
      "Iteration 42, loss = 0.13499668\n",
      "Iteration 29, loss = 0.13932475\n",
      "Iteration 43, loss = 0.11300207\n",
      "Iteration 30, loss = 0.13658375\n",
      "Iteration 44, loss = 0.08865714\n",
      "Iteration 31, loss = 0.12547976\n",
      "Iteration 45, loss = 0.10225227\n",
      "Iteration 32, loss = 0.10240752\n",
      "Iteration 46, loss = 0.08566188\n",
      "Iteration 33, loss = 0.13560125\n",
      "Iteration 47, loss = 0.07336342\n",
      "Iteration 34, loss = 0.08788282\n",
      "Iteration 48, loss = 0.08444754\n",
      "Iteration 35, loss = 0.11594954\n",
      "Iteration 49, loss = 0.07744945\n",
      "Iteration 36, loss = 0.16900636\n",
      "Iteration 50, loss = 0.07369951\n",
      "Iteration 37, loss = 0.12354952\n",
      "Iteration 51, loss = 0.07322772\n",
      "Iteration 38, loss = 0.15789687\n",
      "Iteration 52, loss = 0.07099034\n",
      "Iteration 39, loss = 0.16242436\n",
      "Iteration 53, loss = 0.06748754\n",
      "Iteration 40, loss = 0.13976344\n",
      "Iteration 54, loss = 0.08000939\n",
      "Iteration 41, loss = 0.09956656\n",
      "Iteration 55, loss = 0.07424398\n",
      "Iteration 42, loss = 0.07805781\n",
      "Iteration 56, loss = 0.06663239\n",
      "Iteration 43, loss = 0.08645040\n",
      "Iteration 57, loss = 0.08417379\n",
      "Iteration 44, loss = 0.10073279\n",
      "Iteration 58, loss = 0.10579190\n",
      "Iteration 45, loss = 0.08325541\n",
      "Iteration 59, loss = 0.11421539\n",
      "Iteration 46, loss = 0.10508315\n",
      "Iteration 60, loss = 0.09541055\n",
      "Iteration 47, loss = 0.07596397\n",
      "Iteration 61, loss = 0.07681356\n",
      "Iteration 48, loss = 0.06876852\n",
      "Iteration 62, loss = 0.10079556\n",
      "Iteration 49, loss = 0.10081811\n",
      "Iteration 63, loss = 0.08586510\n",
      "Iteration 50, loss = 0.06028835\n",
      "Iteration 64, loss = 0.07402389\n",
      "Iteration 51, loss = 0.05209844\n",
      "Iteration 65, loss = 0.08638930\n",
      "Iteration 66, loss = 0.07249636\n",
      "Iteration 52, loss = 0.04984898\n",
      "Iteration 53, loss = 0.04634356\n",
      "Iteration 67, loss = 0.06936237\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.04908273\n",
      "Iteration 55, loss = 0.05733906\n",
      "Iteration 56, loss = 0.05142963\n",
      "Iteration 57, loss = 0.04371093\n",
      "Iteration 58, loss = 0.05523071\n",
      "Iteration 59, loss = 0.06812136\n",
      "Iteration 60, loss = 0.06464171\n",
      "Iteration 61, loss = 0.05708513\n",
      "Iteration 62, loss = 0.04188125\n",
      "Iteration 63, loss = 0.04551934\n",
      "Iteration 64, loss = 0.06168897\n",
      "Iteration 65, loss = 0.04537139\n",
      "Iteration 1, loss = 0.88920401\n",
      "Iteration 66, loss = 0.04781103\n",
      "Iteration 2, loss = 0.51439713\n",
      "Iteration 67, loss = 0.04067622\n",
      "Iteration 3, loss = 0.40740943\n",
      "Iteration 68, loss = 0.03795499\n",
      "Iteration 4, loss = 0.34625610\n",
      "Iteration 69, loss = 0.03477433\n",
      "Iteration 5, loss = 0.32282402\n",
      "Iteration 70, loss = 0.03582979\n",
      "Iteration 6, loss = 0.31279044\n",
      "Iteration 71, loss = 0.03636305\n",
      "Iteration 72, loss = 0.03281215\n",
      "Iteration 7, loss = 0.28119489\n",
      "Iteration 73, loss = 0.03235863\n",
      "Iteration 8, loss = 0.28344103\n",
      "Iteration 74, loss = 0.02522452\n",
      "Iteration 9, loss = 0.24041700\n",
      "Iteration 75, loss = 0.02878776\n",
      "Iteration 10, loss = 0.21736083\n",
      "Iteration 76, loss = 0.02496781\n",
      "Iteration 11, loss = 0.19919425\n",
      "Iteration 77, loss = 0.05833473\n",
      "Iteration 12, loss = 0.18950131\n",
      "Iteration 78, loss = 0.07065419\n",
      "Iteration 13, loss = 0.18131086\n",
      "Iteration 79, loss = 0.08255878\n",
      "Iteration 14, loss = 0.16556488\n",
      "Iteration 80, loss = 0.08423358\n",
      "Iteration 15, loss = 0.16864037\n",
      "Iteration 81, loss = 0.10722497\n",
      "Iteration 16, loss = 0.16871324\n",
      "Iteration 82, loss = 0.06896847\n",
      "Iteration 17, loss = 0.16332751\n",
      "Iteration 83, loss = 0.07634933\n",
      "Iteration 18, loss = 0.14905868\n",
      "Iteration 84, loss = 0.10311094\n",
      "Iteration 19, loss = 0.15541343\n",
      "Iteration 85, loss = 0.09012591\n",
      "Iteration 20, loss = 0.14510620\n",
      "Iteration 86, loss = 0.06889366\n",
      "Iteration 21, loss = 0.14052270\n",
      "Iteration 87, loss = 0.06005806\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.12543481\n",
      "Iteration 23, loss = 0.11530027\n",
      "Iteration 24, loss = 0.11041082\n",
      "Iteration 25, loss = 0.12110555\n",
      "Iteration 26, loss = 0.10894945\n",
      "Iteration 27, loss = 0.09183949\n",
      "Iteration 28, loss = 0.10713562\n",
      "Iteration 29, loss = 0.09730121\n",
      "Iteration 30, loss = 0.08680439\n",
      "Iteration 31, loss = 0.09708140\n",
      "Iteration 32, loss = 0.09045702\n",
      "Iteration 33, loss = 0.09865111\n",
      "Iteration 34, loss = 0.08313290\n",
      "Iteration 1, loss = 1.01216296\n",
      "Iteration 35, loss = 0.08620162\n",
      "Iteration 2, loss = 0.49705949\n",
      "Iteration 36, loss = 0.09212174\n",
      "Iteration 3, loss = 0.39752675\n",
      "Iteration 37, loss = 0.09416369\n",
      "Iteration 4, loss = 0.34367045\n",
      "Iteration 38, loss = 0.11214883\n",
      "Iteration 5, loss = 0.32309620\n",
      "Iteration 39, loss = 0.16455171\n",
      "Iteration 6, loss = 0.31490405\n",
      "Iteration 40, loss = 0.14438446\n",
      "Iteration 7, loss = 0.30173179\n",
      "Iteration 41, loss = 0.17149943\n",
      "Iteration 8, loss = 0.26802142\n",
      "Iteration 42, loss = 0.12661572\n",
      "Iteration 9, loss = 0.26740424\n",
      "Iteration 43, loss = 0.14352047\n",
      "Iteration 10, loss = 0.24954884\n",
      "Iteration 44, loss = 0.12892060\n",
      "Iteration 11, loss = 0.24005836\n",
      "Iteration 45, loss = 0.12843526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.21453905\n",
      "Iteration 13, loss = 0.17769349\n",
      "Iteration 14, loss = 0.17479454\n",
      "Iteration 15, loss = 0.16089963\n",
      "Iteration 16, loss = 0.15109178\n",
      "Iteration 17, loss = 0.14647928\n",
      "Iteration 18, loss = 0.14312375\n",
      "Iteration 19, loss = 0.13027596\n",
      "Iteration 20, loss = 0.12643624\n",
      "Iteration 21, loss = 0.12878400\n",
      "Iteration 22, loss = 0.11703035\n",
      "Iteration 23, loss = 0.13273337\n",
      "Iteration 24, loss = 0.12156105\n",
      "Iteration 1, loss = 0.89449612\n",
      "Iteration 25, loss = 0.12759019\n",
      "Iteration 2, loss = 0.45973253\n",
      "Iteration 26, loss = 0.15225814\n",
      "Iteration 3, loss = 0.35830979\n",
      "Iteration 27, loss = 0.12685756\n",
      "Iteration 4, loss = 0.32665877\n",
      "Iteration 28, loss = 0.13362368\n",
      "Iteration 5, loss = 0.30507835\n",
      "Iteration 29, loss = 0.12614514\n",
      "Iteration 6, loss = 0.27389025\n",
      "Iteration 30, loss = 0.11050849\n",
      "Iteration 7, loss = 0.25778246\n",
      "Iteration 31, loss = 0.11603195\n",
      "Iteration 8, loss = 0.23826813\n",
      "Iteration 32, loss = 0.10971713\n",
      "Iteration 9, loss = 0.23846426\n",
      "Iteration 33, loss = 0.11538495\n",
      "Iteration 10, loss = 0.22339455\n",
      "Iteration 34, loss = 0.10240278\n",
      "Iteration 11, loss = 0.20614538\n",
      "Iteration 35, loss = 0.09225587\n",
      "Iteration 12, loss = 0.20266805\n",
      "Iteration 36, loss = 0.07992888\n",
      "Iteration 13, loss = 0.19507551\n",
      "Iteration 37, loss = 0.09153005\n",
      "Iteration 14, loss = 0.17077261\n",
      "Iteration 38, loss = 0.07505401\n",
      "Iteration 15, loss = 0.16402298\n",
      "Iteration 39, loss = 0.07531241\n",
      "Iteration 16, loss = 0.15569905\n",
      "Iteration 40, loss = 0.07632028\n",
      "Iteration 17, loss = 0.14802245\n",
      "Iteration 41, loss = 0.07357457\n",
      "Iteration 18, loss = 0.14955155\n",
      "Iteration 42, loss = 0.06778610\n",
      "Iteration 19, loss = 0.17312662\n",
      "Iteration 43, loss = 0.06869307\n",
      "Iteration 20, loss = 0.15415485\n",
      "Iteration 44, loss = 0.05608204\n",
      "Iteration 21, loss = 0.17616688\n",
      "Iteration 45, loss = 0.07371132\n",
      "Iteration 22, loss = 0.13006217\n",
      "Iteration 46, loss = 0.09723929\n",
      "Iteration 23, loss = 0.14111010\n",
      "Iteration 47, loss = 0.08301136\n",
      "Iteration 24, loss = 0.11785144\n",
      "Iteration 48, loss = 0.12091407\n",
      "Iteration 25, loss = 0.11840347\n",
      "Iteration 49, loss = 0.09173348\n",
      "Iteration 26, loss = 0.10730002\n",
      "Iteration 50, loss = 0.08939527\n",
      "Iteration 27, loss = 0.10411331\n",
      "Iteration 51, loss = 0.07104496\n",
      "Iteration 28, loss = 0.12329654\n",
      "Iteration 52, loss = 0.07401435\n",
      "Iteration 29, loss = 0.10997120\n",
      "Iteration 53, loss = 0.07226887\n",
      "Iteration 30, loss = 0.08965732\n",
      "Iteration 54, loss = 0.06533596\n",
      "Iteration 31, loss = 0.09783356\n",
      "Iteration 55, loss = 0.05943353\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.09589095\n",
      "Iteration 33, loss = 0.08457679\n",
      "Iteration 34, loss = 0.07056382\n",
      "Iteration 35, loss = 0.06749359\n",
      "Iteration 36, loss = 0.06639777\n",
      "Iteration 37, loss = 0.09057066\n",
      "Iteration 38, loss = 0.07679104\n",
      "Iteration 39, loss = 0.06414819\n",
      "Iteration 40, loss = 0.06253138\n",
      "Iteration 41, loss = 0.05829371\n",
      "Iteration 42, loss = 0.07039632\n",
      "Iteration 43, loss = 0.06180202\n",
      "Iteration 44, loss = 0.05323631\n",
      "Iteration 1, loss = 0.94506175\n",
      "Iteration 45, loss = 0.05198648\n",
      "Iteration 2, loss = 0.46200782\n",
      "Iteration 46, loss = 0.07385864\n",
      "Iteration 3, loss = 0.38715796\n",
      "Iteration 47, loss = 0.06619528\n",
      "Iteration 4, loss = 0.36328822\n",
      "Iteration 48, loss = 0.08901362\n",
      "Iteration 5, loss = 0.31921396\n",
      "Iteration 49, loss = 0.05417732\n",
      "Iteration 6, loss = 0.28538044\n",
      "Iteration 50, loss = 0.04466347\n",
      "Iteration 7, loss = 0.26455387\n",
      "Iteration 51, loss = 0.06817981\n",
      "Iteration 8, loss = 0.24380876\n",
      "Iteration 52, loss = 0.05059051\n",
      "Iteration 9, loss = 0.23118440\n",
      "Iteration 53, loss = 0.04095436\n",
      "Iteration 10, loss = 0.21938531\n",
      "Iteration 54, loss = 0.04828053\n",
      "Iteration 11, loss = 0.20351686\n",
      "Iteration 55, loss = 0.03551845\n",
      "Iteration 12, loss = 0.18551935\n",
      "Iteration 56, loss = 0.05022677\n",
      "Iteration 13, loss = 0.17535726\n",
      "Iteration 57, loss = 0.04754966\n",
      "Iteration 14, loss = 0.15673703\n",
      "Iteration 58, loss = 0.04379293\n",
      "Iteration 15, loss = 0.14870362\n",
      "Iteration 59, loss = 0.04443260\n",
      "Iteration 16, loss = 0.15810324\n",
      "Iteration 60, loss = 0.04011849\n",
      "Iteration 17, loss = 0.14003521\n",
      "Iteration 61, loss = 0.04778136\n",
      "Iteration 18, loss = 0.14529276\n",
      "Iteration 62, loss = 0.04016359\n",
      "Iteration 19, loss = 0.14850172\n",
      "Iteration 63, loss = 0.04400341\n",
      "Iteration 20, loss = 0.17916051\n",
      "Iteration 21, loss = 0.21262683\n",
      "Iteration 64, loss = 0.03796656\n",
      "Iteration 22, loss = 0.22172215\n",
      "Iteration 65, loss = 0.03464364\n",
      "Iteration 23, loss = 0.17728953\n",
      "Iteration 66, loss = 0.04261923\n",
      "Iteration 67, loss = 0.04640862\n",
      "Iteration 24, loss = 0.18572872\n",
      "Iteration 68, loss = 0.06960963\n",
      "Iteration 25, loss = 0.17108383\n",
      "Iteration 26, loss = 0.15724048\n",
      "Iteration 69, loss = 0.07906671\n",
      "Iteration 27, loss = 0.18795114\n",
      "Iteration 70, loss = 0.07884349\n",
      "Iteration 28, loss = 0.14146673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 71, loss = 0.06740820\n",
      "Iteration 72, loss = 0.05679601\n",
      "Iteration 73, loss = 0.05656162\n",
      "Iteration 74, loss = 0.07095829\n",
      "Iteration 75, loss = 0.07177502\n",
      "Iteration 76, loss = 0.10260208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09158339\n",
      "Iteration 2, loss = 0.52308007\n",
      "Iteration 3, loss = 0.41234771\n",
      "Iteration 4, loss = 0.35587472\n",
      "Iteration 5, loss = 0.30908821\n",
      "Iteration 6, loss = 0.28687625\n",
      "Iteration 1, loss = 0.76390462\n",
      "Iteration 7, loss = 0.26265386\n",
      "Iteration 2, loss = 0.40452910\n",
      "Iteration 8, loss = 0.23227878\n",
      "Iteration 3, loss = 0.35572369\n",
      "Iteration 9, loss = 0.21083054\n",
      "Iteration 4, loss = 0.31419574\n",
      "Iteration 10, loss = 0.19897615\n",
      "Iteration 5, loss = 0.27339891\n",
      "Iteration 11, loss = 0.20094796\n",
      "Iteration 6, loss = 0.27658713\n",
      "Iteration 12, loss = 0.16805727\n",
      "Iteration 7, loss = 0.25269335\n",
      "Iteration 13, loss = 0.16062024\n",
      "Iteration 8, loss = 0.24097985\n",
      "Iteration 14, loss = 0.14544530\n",
      "Iteration 9, loss = 0.23606998\n",
      "Iteration 15, loss = 0.14623400\n",
      "Iteration 10, loss = 0.22435008\n",
      "Iteration 16, loss = 0.14452086\n",
      "Iteration 11, loss = 0.18849424\n",
      "Iteration 17, loss = 0.12234069\n",
      "Iteration 12, loss = 0.17180674\n",
      "Iteration 13, loss = 0.15745427Iteration 18, loss = 0.12538484\n",
      "\n",
      "Iteration 19, loss = 0.13072790\n",
      "Iteration 14, loss = 0.14948390\n",
      "Iteration 20, loss = 0.11587955\n",
      "Iteration 15, loss = 0.14515335\n",
      "Iteration 21, loss = 0.11015474\n",
      "Iteration 16, loss = 0.13275868\n",
      "Iteration 22, loss = 0.12339809\n",
      "Iteration 17, loss = 0.12252138\n",
      "Iteration 23, loss = 0.09760882\n",
      "Iteration 18, loss = 0.12442349\n",
      "Iteration 24, loss = 0.11002511\n",
      "Iteration 19, loss = 0.11911814\n",
      "Iteration 25, loss = 0.09603109\n",
      "Iteration 20, loss = 0.13055063\n",
      "Iteration 26, loss = 0.09763925\n",
      "Iteration 21, loss = 0.11339336\n",
      "Iteration 27, loss = 0.09002186\n",
      "Iteration 22, loss = 0.11494810\n",
      "Iteration 28, loss = 0.08943323\n",
      "Iteration 23, loss = 0.10338268\n",
      "Iteration 29, loss = 0.07787518\n",
      "Iteration 24, loss = 0.09573344\n",
      "Iteration 30, loss = 0.06515638\n",
      "Iteration 25, loss = 0.09846356\n",
      "Iteration 31, loss = 0.06439320\n",
      "Iteration 26, loss = 0.12522748\n",
      "Iteration 32, loss = 0.07411979\n",
      "Iteration 27, loss = 0.09848633\n",
      "Iteration 33, loss = 0.06857931\n",
      "Iteration 28, loss = 0.12278111\n",
      "Iteration 34, loss = 0.06463867\n",
      "Iteration 29, loss = 0.08836814\n",
      "Iteration 35, loss = 0.05493997\n",
      "Iteration 30, loss = 0.09871731\n",
      "Iteration 36, loss = 0.05254548\n",
      "Iteration 31, loss = 0.09355628\n",
      "Iteration 37, loss = 0.05015802\n",
      "Iteration 32, loss = 0.09265623\n",
      "Iteration 38, loss = 0.04763208\n",
      "Iteration 33, loss = 0.09051805\n",
      "Iteration 39, loss = 0.06336761\n",
      "Iteration 34, loss = 0.10085805\n",
      "Iteration 40, loss = 0.06403464\n",
      "Iteration 35, loss = 0.08347912\n",
      "Iteration 41, loss = 0.05302798\n",
      "Iteration 36, loss = 0.08579379\n",
      "Iteration 42, loss = 0.04845851\n",
      "Iteration 37, loss = 0.08274866\n",
      "Iteration 43, loss = 0.05991191\n",
      "Iteration 38, loss = 0.06532559\n",
      "Iteration 44, loss = 0.06506367\n",
      "Iteration 39, loss = 0.05965326\n",
      "Iteration 45, loss = 0.07698871\n",
      "Iteration 40, loss = 0.05331970\n",
      "Iteration 46, loss = 0.05564485\n",
      "Iteration 41, loss = 0.06599817\n",
      "Iteration 47, loss = 0.05062900\n",
      "Iteration 42, loss = 0.05513583\n",
      "Iteration 48, loss = 0.05416841\n",
      "Iteration 43, loss = 0.07115533\n",
      "Iteration 44, loss = 0.07066700Iteration 49, loss = 0.03735434\n",
      "\n",
      "Iteration 45, loss = 0.09419953\n",
      "Iteration 50, loss = 0.05219251\n",
      "Iteration 51, loss = 0.06076340\n",
      "Iteration 46, loss = 0.13306288\n",
      "Iteration 47, loss = 0.10189990\n",
      "Iteration 52, loss = 0.06449578\n",
      "Iteration 48, loss = 0.09095735\n",
      "Iteration 53, loss = 0.09860531\n",
      "Iteration 49, loss = 0.06951394\n",
      "Iteration 54, loss = 0.06882912\n",
      "Iteration 50, loss = 0.06063861\n",
      "Iteration 55, loss = 0.05262544\n",
      "Iteration 51, loss = 0.07021066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.04912761\n",
      "Iteration 57, loss = 0.04407318\n",
      "Iteration 58, loss = 0.03247893\n",
      "Iteration 59, loss = 0.04879963\n",
      "Iteration 60, loss = 0.04350291\n",
      "Iteration 61, loss = 0.03826147\n",
      "Iteration 62, loss = 0.03047168\n",
      "Iteration 63, loss = 0.03208265\n",
      "Iteration 64, loss = 0.02933670\n",
      "Iteration 65, loss = 0.02750118\n",
      "Iteration 66, loss = 0.02885400\n",
      "Iteration 67, loss = 0.04123523\n",
      "Iteration 68, loss = 0.07595035\n",
      "Iteration 1, loss = 1.10822485\n",
      "Iteration 69, loss = 0.07419294\n",
      "Iteration 2, loss = 0.58799093\n",
      "Iteration 70, loss = 0.07208433\n",
      "Iteration 3, loss = 0.45222967\n",
      "Iteration 71, loss = 0.10859894\n",
      "Iteration 4, loss = 0.40498627\n",
      "Iteration 72, loss = 0.06353461\n",
      "Iteration 5, loss = 0.34150760\n",
      "Iteration 73, loss = 0.05348159\n",
      "Iteration 6, loss = 0.32084926\n",
      "Iteration 74, loss = 0.04698506\n",
      "Iteration 7, loss = 0.30244709\n",
      "Iteration 75, loss = 0.04498692\n",
      "Iteration 8, loss = 0.27431881\n",
      "Iteration 76, loss = 0.03281348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.25701708\n",
      "Iteration 10, loss = 0.23998535\n",
      "Iteration 11, loss = 0.22679269\n",
      "Iteration 12, loss = 0.21444612\n",
      "Iteration 13, loss = 0.19971798\n",
      "Iteration 14, loss = 0.18509914\n",
      "Iteration 15, loss = 0.18046624\n",
      "Iteration 16, loss = 0.17153227\n",
      "Iteration 17, loss = 0.16798489\n",
      "Iteration 18, loss = 0.15884900\n",
      "Iteration 19, loss = 0.15355926\n",
      "Iteration 20, loss = 0.13907104\n",
      "Iteration 21, loss = 0.13108345\n",
      "Iteration 1, loss = 0.87849691\n",
      "Iteration 22, loss = 0.13348314\n",
      "Iteration 2, loss = 0.50902709\n",
      "Iteration 23, loss = 0.12349359\n",
      "Iteration 3, loss = 0.40777197\n",
      "Iteration 24, loss = 0.11736326\n",
      "Iteration 4, loss = 0.36056207\n",
      "Iteration 25, loss = 0.12290261\n",
      "Iteration 5, loss = 0.31752845\n",
      "Iteration 26, loss = 0.10750836\n",
      "Iteration 6, loss = 0.29016367\n",
      "Iteration 27, loss = 0.11595483\n",
      "Iteration 7, loss = 0.26622914\n",
      "Iteration 28, loss = 0.13663933\n",
      "Iteration 8, loss = 0.25285729\n",
      "Iteration 29, loss = 0.13211195\n",
      "Iteration 9, loss = 0.22280072\n",
      "Iteration 30, loss = 0.11924257\n",
      "Iteration 10, loss = 0.23443252\n",
      "Iteration 31, loss = 0.09278233\n",
      "Iteration 11, loss = 0.23082305\n",
      "Iteration 32, loss = 0.08645535\n",
      "Iteration 12, loss = 0.21366662\n",
      "Iteration 33, loss = 0.08294440\n",
      "Iteration 13, loss = 0.19832365\n",
      "Iteration 34, loss = 0.07486471\n",
      "Iteration 14, loss = 0.16719527\n",
      "Iteration 35, loss = 0.07755508\n",
      "Iteration 15, loss = 0.16479618\n",
      "Iteration 36, loss = 0.07718539\n",
      "Iteration 16, loss = 0.15326707\n",
      "Iteration 37, loss = 0.08562079\n",
      "Iteration 17, loss = 0.14929036\n",
      "Iteration 38, loss = 0.09960127\n",
      "Iteration 18, loss = 0.13589130\n",
      "Iteration 39, loss = 0.08522227\n",
      "Iteration 19, loss = 0.14447675\n",
      "Iteration 40, loss = 0.09078179\n",
      "Iteration 20, loss = 0.15026867\n",
      "Iteration 41, loss = 0.09077723\n",
      "Iteration 21, loss = 0.11587633\n",
      "Iteration 42, loss = 0.08184513\n",
      "Iteration 22, loss = 0.10768594\n",
      "Iteration 43, loss = 0.07558656\n",
      "Iteration 23, loss = 0.11032122\n",
      "Iteration 44, loss = 0.09426877\n",
      "Iteration 24, loss = 0.11405720\n",
      "Iteration 45, loss = 0.07462710\n",
      "Iteration 25, loss = 0.10077895\n",
      "Iteration 46, loss = 0.07674724\n",
      "Iteration 26, loss = 0.11476812\n",
      "Iteration 47, loss = 0.08246645\n",
      "Iteration 27, loss = 0.10917065\n",
      "Iteration 48, loss = 0.06401887\n",
      "Iteration 28, loss = 0.10333981\n",
      "Iteration 49, loss = 0.06987497\n",
      "Iteration 29, loss = 0.09633140\n",
      "Iteration 50, loss = 0.07344649\n",
      "Iteration 30, loss = 0.10829061\n",
      "Iteration 51, loss = 0.07083774\n",
      "Iteration 31, loss = 0.09033220\n",
      "Iteration 52, loss = 0.07782333\n",
      "Iteration 32, loss = 0.08777451\n",
      "Iteration 53, loss = 0.07566160\n",
      "Iteration 33, loss = 0.09090408\n",
      "Iteration 54, loss = 0.07042239\n",
      "Iteration 34, loss = 0.07171767\n",
      "Iteration 55, loss = 0.06301978\n",
      "Iteration 35, loss = 0.07151036\n",
      "Iteration 56, loss = 0.05499316\n",
      "Iteration 36, loss = 0.05426574\n",
      "Iteration 57, loss = 0.07050663\n",
      "Iteration 37, loss = 0.05489876\n",
      "Iteration 58, loss = 0.05132687\n",
      "Iteration 38, loss = 0.06112134\n",
      "Iteration 59, loss = 0.05957758\n",
      "Iteration 39, loss = 0.05879576\n",
      "Iteration 60, loss = 0.05757186\n",
      "Iteration 40, loss = 0.06956620\n",
      "Iteration 61, loss = 0.04703476\n",
      "Iteration 41, loss = 0.10105446\n",
      "Iteration 62, loss = 0.05433151\n",
      "Iteration 42, loss = 0.05244501\n",
      "Iteration 63, loss = 0.05081049\n",
      "Iteration 43, loss = 0.04242620\n",
      "Iteration 64, loss = 0.04142669\n",
      "Iteration 44, loss = 0.04626169\n",
      "Iteration 65, loss = 0.04217054\n",
      "Iteration 45, loss = 0.05618792\n",
      "Iteration 66, loss = 0.04743518\n",
      "Iteration 46, loss = 0.04198944\n",
      "Iteration 67, loss = 0.06464834\n",
      "Iteration 47, loss = 0.04211076\n",
      "Iteration 68, loss = 0.05779257\n",
      "Iteration 48, loss = 0.05921045\n",
      "Iteration 69, loss = 0.06526137\n",
      "Iteration 49, loss = 0.03878253\n",
      "Iteration 70, loss = 0.08782443\n",
      "Iteration 50, loss = 0.03703340\n",
      "Iteration 71, loss = 0.08169811\n",
      "Iteration 51, loss = 0.03867744\n",
      "Iteration 72, loss = 0.06022064\n",
      "Iteration 52, loss = 0.05824741\n",
      "Iteration 73, loss = 0.07875086\n",
      "Iteration 53, loss = 0.05877658\n",
      "Iteration 74, loss = 0.11711277\n",
      "Iteration 54, loss = 0.06163027\n",
      "Iteration 75, loss = 0.10115072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 0.06059897\n",
      "Iteration 56, loss = 0.08187743\n",
      "Iteration 57, loss = 0.08780491\n",
      "Iteration 58, loss = 0.06834473\n",
      "Iteration 59, loss = 0.08941745\n",
      "Iteration 60, loss = 0.07030114\n",
      "Iteration 61, loss = 0.07915231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01075480\n",
      "Iteration 2, loss = 0.49248031\n",
      "Iteration 3, loss = 0.40660282\n",
      "Iteration 4, loss = 0.35074998\n",
      "Iteration 5, loss = 0.31686238\n",
      "Iteration 6, loss = 0.30752533\n",
      "Iteration 7, loss = 0.26437259\n",
      "Iteration 1, loss = 1.11272676\n",
      "Iteration 8, loss = 0.25266614\n",
      "Iteration 2, loss = 0.57485009\n",
      "Iteration 9, loss = 0.23522914\n",
      "Iteration 3, loss = 0.44970247\n",
      "Iteration 10, loss = 0.21805633\n",
      "Iteration 4, loss = 0.41510532\n",
      "Iteration 11, loss = 0.21100359\n",
      "Iteration 5, loss = 0.36060089\n",
      "Iteration 12, loss = 0.19438732\n",
      "Iteration 6, loss = 0.32599585\n",
      "Iteration 13, loss = 0.19126185\n",
      "Iteration 7, loss = 0.30458147\n",
      "Iteration 14, loss = 0.18102437\n",
      "Iteration 8, loss = 0.26380126\n",
      "Iteration 15, loss = 0.15990018\n",
      "Iteration 9, loss = 0.25918039\n",
      "Iteration 16, loss = 0.15771414\n",
      "Iteration 10, loss = 0.23543547\n",
      "Iteration 17, loss = 0.15787106\n",
      "Iteration 11, loss = 0.21500838\n",
      "Iteration 18, loss = 0.14640783\n",
      "Iteration 12, loss = 0.20252605\n",
      "Iteration 19, loss = 0.14869131\n",
      "Iteration 13, loss = 0.18703011\n",
      "Iteration 20, loss = 0.12573442\n",
      "Iteration 14, loss = 0.17204435\n",
      "Iteration 21, loss = 0.12774477\n",
      "Iteration 15, loss = 0.16779731\n",
      "Iteration 22, loss = 0.13051294\n",
      "Iteration 16, loss = 0.17036698\n",
      "Iteration 23, loss = 0.11797722\n",
      "Iteration 17, loss = 0.15177897\n",
      "Iteration 24, loss = 0.10670119\n",
      "Iteration 18, loss = 0.15035013\n",
      "Iteration 25, loss = 0.12644094\n",
      "Iteration 19, loss = 0.13539584\n",
      "Iteration 26, loss = 0.11835353\n",
      "Iteration 20, loss = 0.12980309\n",
      "Iteration 27, loss = 0.10876352\n",
      "Iteration 21, loss = 0.11865402\n",
      "Iteration 28, loss = 0.10091230\n",
      "Iteration 22, loss = 0.12260953\n",
      "Iteration 29, loss = 0.12294000\n",
      "Iteration 23, loss = 0.12308899\n",
      "Iteration 30, loss = 0.10316805\n",
      "Iteration 24, loss = 0.13065391\n",
      "Iteration 31, loss = 0.08938734\n",
      "Iteration 25, loss = 0.11554127\n",
      "Iteration 32, loss = 0.10523953\n",
      "Iteration 26, loss = 0.10252497\n",
      "Iteration 33, loss = 0.09117892\n",
      "Iteration 27, loss = 0.09671326\n",
      "Iteration 34, loss = 0.08922859\n",
      "Iteration 28, loss = 0.08885593\n",
      "Iteration 35, loss = 0.07945572\n",
      "Iteration 29, loss = 0.10384872\n",
      "Iteration 36, loss = 0.08415947\n",
      "Iteration 30, loss = 0.10231395\n",
      "Iteration 37, loss = 0.11210439\n",
      "Iteration 31, loss = 0.09757444\n",
      "Iteration 38, loss = 0.10882808\n",
      "Iteration 32, loss = 0.08754658\n",
      "Iteration 39, loss = 0.12707691\n",
      "Iteration 33, loss = 0.07652964\n",
      "Iteration 40, loss = 0.09476684\n",
      "Iteration 34, loss = 0.09298331\n",
      "Iteration 41, loss = 0.09682556\n",
      "Iteration 35, loss = 0.09214000\n",
      "Iteration 42, loss = 0.09575229\n",
      "Iteration 36, loss = 0.13187903\n",
      "Iteration 43, loss = 0.11272691\n",
      "Iteration 37, loss = 0.13425515\n",
      "Iteration 44, loss = 0.10784866\n",
      "Iteration 38, loss = 0.12272684\n",
      "Iteration 45, loss = 0.09127314\n",
      "Iteration 39, loss = 0.12146160\n",
      "Iteration 46, loss = 0.10421888\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.10829915\n",
      "Iteration 41, loss = 0.15111115\n",
      "Iteration 42, loss = 0.10757793\n",
      "Iteration 43, loss = 0.08245943\n",
      "Iteration 44, loss = 0.09258717\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94606042\n",
      "Iteration 2, loss = 0.45136811\n",
      "Iteration 3, loss = 0.41547033\n",
      "Iteration 4, loss = 0.36065884\n",
      "Iteration 5, loss = 0.31128555\n",
      "Iteration 1, loss = 0.89572418\n",
      "Iteration 6, loss = 0.28713023\n",
      "Iteration 2, loss = 0.45896276\n",
      "Iteration 7, loss = 0.27126743\n",
      "Iteration 3, loss = 0.38041134\n",
      "Iteration 8, loss = 0.24754562\n",
      "Iteration 4, loss = 0.34534288\n",
      "Iteration 9, loss = 0.22781682\n",
      "Iteration 5, loss = 0.31167515\n",
      "Iteration 10, loss = 0.21706340\n",
      "Iteration 6, loss = 0.27601379\n",
      "Iteration 11, loss = 0.20205699\n",
      "Iteration 7, loss = 0.24753127\n",
      "Iteration 12, loss = 0.18745506\n",
      "Iteration 8, loss = 0.22850571\n",
      "Iteration 13, loss = 0.17697086\n",
      "Iteration 9, loss = 0.21249669\n",
      "Iteration 14, loss = 0.16443063\n",
      "Iteration 10, loss = 0.20594067\n",
      "Iteration 15, loss = 0.16228411\n",
      "Iteration 11, loss = 0.19305539\n",
      "Iteration 16, loss = 0.15350444\n",
      "Iteration 12, loss = 0.18559221\n",
      "Iteration 17, loss = 0.14069133\n",
      "Iteration 18, loss = 0.13691545\n",
      "Iteration 13, loss = 0.17899958\n",
      "Iteration 19, loss = 0.13565696\n",
      "Iteration 14, loss = 0.16356164\n",
      "Iteration 20, loss = 0.13513262\n",
      "Iteration 15, loss = 0.15223704\n",
      "Iteration 21, loss = 0.11664916\n",
      "Iteration 16, loss = 0.14491634\n",
      "Iteration 22, loss = 0.11490161\n",
      "Iteration 17, loss = 0.13295998\n",
      "Iteration 23, loss = 0.12992853\n",
      "Iteration 18, loss = 0.13225568\n",
      "Iteration 24, loss = 0.13024078\n",
      "Iteration 19, loss = 0.14107414\n",
      "Iteration 25, loss = 0.14051386\n",
      "Iteration 20, loss = 0.12879757\n",
      "Iteration 26, loss = 0.11078964\n",
      "Iteration 21, loss = 0.12457236\n",
      "Iteration 27, loss = 0.11004175\n",
      "Iteration 22, loss = 0.11993343\n",
      "Iteration 28, loss = 0.10578779\n",
      "Iteration 23, loss = 0.11188542\n",
      "Iteration 29, loss = 0.09012772\n",
      "Iteration 24, loss = 0.10999337\n",
      "Iteration 30, loss = 0.09308099\n",
      "Iteration 25, loss = 0.11582771\n",
      "Iteration 31, loss = 0.10092608\n",
      "Iteration 26, loss = 0.12325783\n",
      "Iteration 32, loss = 0.12999754\n",
      "Iteration 27, loss = 0.12573795\n",
      "Iteration 33, loss = 0.12375414\n",
      "Iteration 28, loss = 0.12185963\n",
      "Iteration 34, loss = 0.10477306\n",
      "Iteration 29, loss = 0.10921028\n",
      "Iteration 35, loss = 0.09569555\n",
      "Iteration 30, loss = 0.09702325\n",
      "Iteration 36, loss = 0.11022479\n",
      "Iteration 31, loss = 0.08556050\n",
      "Iteration 37, loss = 0.08475536\n",
      "Iteration 32, loss = 0.08672435\n",
      "Iteration 38, loss = 0.07670959\n",
      "Iteration 33, loss = 0.10611073\n",
      "Iteration 39, loss = 0.07397367\n",
      "Iteration 34, loss = 0.08652560\n",
      "Iteration 40, loss = 0.07994898\n",
      "Iteration 35, loss = 0.08536530\n",
      "Iteration 41, loss = 0.08417075\n",
      "Iteration 36, loss = 0.09435922\n",
      "Iteration 42, loss = 0.08042250\n",
      "Iteration 37, loss = 0.12596517\n",
      "Iteration 43, loss = 0.06539322\n",
      "Iteration 38, loss = 0.10777194\n",
      "Iteration 44, loss = 0.06286968\n",
      "Iteration 39, loss = 0.11485191\n",
      "Iteration 45, loss = 0.05713391\n",
      "Iteration 40, loss = 0.09683594\n",
      "Iteration 46, loss = 0.05617015\n",
      "Iteration 41, loss = 0.07650828\n",
      "Iteration 47, loss = 0.05232782\n",
      "Iteration 42, loss = 0.08872208\n",
      "Iteration 48, loss = 0.05464150\n",
      "Iteration 43, loss = 0.09025710\n",
      "Iteration 49, loss = 0.05290804\n",
      "Iteration 44, loss = 0.09475221\n",
      "Iteration 50, loss = 0.05141803\n",
      "Iteration 45, loss = 0.09743442\n",
      "Iteration 51, loss = 0.04293130\n",
      "Iteration 46, loss = 0.06534380\n",
      "Iteration 52, loss = 0.04774299\n",
      "Iteration 47, loss = 0.08914322\n",
      "Iteration 53, loss = 0.04959670\n",
      "Iteration 48, loss = 0.07899320\n",
      "Iteration 54, loss = 0.04514998\n",
      "Iteration 49, loss = 0.08724497\n",
      "Iteration 55, loss = 0.09732225\n",
      "Iteration 50, loss = 0.07035204\n",
      "Iteration 56, loss = 0.14440532\n",
      "Iteration 51, loss = 0.07806085\n",
      "Iteration 57, loss = 0.12429193\n",
      "Iteration 52, loss = 0.07783688\n",
      "Iteration 58, loss = 0.11432317\n",
      "Iteration 53, loss = 0.05709992\n",
      "Iteration 59, loss = 0.07826909\n",
      "Iteration 54, loss = 0.05046142\n",
      "Iteration 60, loss = 0.06857487\n",
      "Iteration 55, loss = 0.05425624\n",
      "Iteration 61, loss = 0.07936586\n",
      "Iteration 56, loss = 0.05052991\n",
      "Iteration 62, loss = 0.05620178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.06635755\n",
      "Iteration 58, loss = 0.06937008\n",
      "Iteration 59, loss = 0.04597910\n",
      "Iteration 60, loss = 0.05225971\n",
      "Iteration 61, loss = 0.06283752\n",
      "Iteration 62, loss = 0.05591741\n",
      "Iteration 63, loss = 0.05440006\n",
      "Iteration 64, loss = 0.06581974\n",
      "Iteration 65, loss = 0.05047027\n",
      "Iteration 66, loss = 0.07790470\n",
      "Iteration 67, loss = 0.07495545\n",
      "Iteration 68, loss = 0.11963799\n",
      "Iteration 69, loss = 0.08895776\n",
      "Iteration 1, loss = 1.03581340\n",
      "Iteration 70, loss = 0.09691836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 0.50300098\n",
      "Iteration 3, loss = 0.44891091\n",
      "Iteration 4, loss = 0.36611055\n",
      "Iteration 5, loss = 0.35013539\n",
      "Iteration 6, loss = 0.30797912\n",
      "Iteration 7, loss = 0.28934089\n",
      "Iteration 8, loss = 0.26928206\n",
      "Iteration 9, loss = 0.24347602\n",
      "Iteration 10, loss = 0.23372099\n",
      "Iteration 11, loss = 0.21543809\n",
      "Iteration 12, loss = 0.20498820\n",
      "Iteration 13, loss = 0.19620268\n",
      "Iteration 14, loss = 0.18846796\n",
      "Iteration 1, loss = 0.99923924\n",
      "Iteration 15, loss = 0.17569670\n",
      "Iteration 2, loss = 0.48895336\n",
      "Iteration 16, loss = 0.16453071\n",
      "Iteration 3, loss = 0.42522184\n",
      "Iteration 17, loss = 0.15739447\n",
      "Iteration 4, loss = 0.36714619\n",
      "Iteration 18, loss = 0.14116709\n",
      "Iteration 5, loss = 0.33326170\n",
      "Iteration 19, loss = 0.13486572\n",
      "Iteration 6, loss = 0.29988454\n",
      "Iteration 20, loss = 0.12314512\n",
      "Iteration 7, loss = 0.27640421\n",
      "Iteration 21, loss = 0.12767861\n",
      "Iteration 8, loss = 0.25621504\n",
      "Iteration 22, loss = 0.12642242\n",
      "Iteration 9, loss = 0.23992181\n",
      "Iteration 23, loss = 0.13282623\n",
      "Iteration 10, loss = 0.22656641\n",
      "Iteration 24, loss = 0.11466882\n",
      "Iteration 11, loss = 0.21658828\n",
      "Iteration 25, loss = 0.11255467\n",
      "Iteration 12, loss = 0.19959715\n",
      "Iteration 26, loss = 0.10364091\n",
      "Iteration 13, loss = 0.18561296\n",
      "Iteration 14, loss = 0.17119033\n",
      "Iteration 27, loss = 0.10846805\n",
      "Iteration 15, loss = 0.16033033\n",
      "Iteration 28, loss = 0.11328649\n",
      "Iteration 16, loss = 0.16488581\n",
      "Iteration 29, loss = 0.09541662\n",
      "Iteration 17, loss = 0.15818244\n",
      "Iteration 30, loss = 0.09724316\n",
      "Iteration 18, loss = 0.21417418\n",
      "Iteration 31, loss = 0.14740132\n",
      "Iteration 19, loss = 0.15746410\n",
      "Iteration 32, loss = 0.12957851\n",
      "Iteration 20, loss = 0.16854689\n",
      "Iteration 33, loss = 0.13651533\n",
      "Iteration 21, loss = 0.13792642\n",
      "Iteration 34, loss = 0.12413197\n",
      "Iteration 22, loss = 0.12962339\n",
      "Iteration 35, loss = 0.10472950\n",
      "Iteration 23, loss = 0.13155167\n",
      "Iteration 36, loss = 0.13354819\n",
      "Iteration 24, loss = 0.11362769\n",
      "Iteration 37, loss = 0.10380031\n",
      "Iteration 25, loss = 0.11321190\n",
      "Iteration 38, loss = 0.09206226\n",
      "Iteration 26, loss = 0.10054457\n",
      "Iteration 39, loss = 0.07666169\n",
      "Iteration 27, loss = 0.10506334\n",
      "Iteration 40, loss = 0.07327708\n",
      "Iteration 28, loss = 0.11044922\n",
      "Iteration 41, loss = 0.06626668\n",
      "Iteration 29, loss = 0.13074206\n",
      "Iteration 42, loss = 0.07326207\n",
      "Iteration 30, loss = 0.13962578\n",
      "Iteration 43, loss = 0.05992403\n",
      "Iteration 31, loss = 0.13239658\n",
      "Iteration 44, loss = 0.05691246\n",
      "Iteration 32, loss = 0.11038931\n",
      "Iteration 45, loss = 0.05062683\n",
      "Iteration 33, loss = 0.10067055\n",
      "Iteration 46, loss = 0.04829600\n",
      "Iteration 34, loss = 0.09410579\n",
      "Iteration 47, loss = 0.04336566\n",
      "Iteration 35, loss = 0.08746369\n",
      "Iteration 48, loss = 0.04853092\n",
      "Iteration 36, loss = 0.07911492\n",
      "Iteration 49, loss = 0.05708352\n",
      "Iteration 37, loss = 0.07851334\n",
      "Iteration 50, loss = 0.05955898\n",
      "Iteration 38, loss = 0.06920741\n",
      "Iteration 51, loss = 0.06925061\n",
      "Iteration 39, loss = 0.06925201\n",
      "Iteration 52, loss = 0.06808018\n",
      "Iteration 40, loss = 0.06722301\n",
      "Iteration 53, loss = 0.06449843\n",
      "Iteration 41, loss = 0.06074116\n",
      "Iteration 54, loss = 0.04991447\n",
      "Iteration 42, loss = 0.06213643\n",
      "Iteration 55, loss = 0.07685808\n",
      "Iteration 43, loss = 0.06091141\n",
      "Iteration 56, loss = 0.06582097\n",
      "Iteration 44, loss = 0.06089194\n",
      "Iteration 57, loss = 0.05173905\n",
      "Iteration 45, loss = 0.06313341\n",
      "Iteration 58, loss = 0.04879297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.06113088\n",
      "Iteration 47, loss = 0.06483163\n",
      "Iteration 48, loss = 0.07557515\n",
      "Iteration 49, loss = 0.06631247\n",
      "Iteration 50, loss = 0.07428218\n",
      "Iteration 51, loss = 0.11864602\n",
      "Iteration 52, loss = 0.13797982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75010621\n",
      "Iteration 2, loss = 0.44798871\n",
      "Iteration 3, loss = 0.35992597\n",
      "Iteration 4, loss = 0.30522799\n",
      "Iteration 5, loss = 0.26330208\n",
      "Iteration 6, loss = 0.24036488\n",
      "Iteration 7, loss = 0.22389020\n",
      "Iteration 1, loss = 0.87356002\n",
      "Iteration 8, loss = 0.21002275\n",
      "Iteration 2, loss = 0.46035932\n",
      "Iteration 9, loss = 0.20252006\n",
      "Iteration 3, loss = 0.40514196\n",
      "Iteration 10, loss = 0.20342695\n",
      "Iteration 4, loss = 0.34038528\n",
      "Iteration 11, loss = 0.20428959\n",
      "Iteration 5, loss = 0.31307584\n",
      "Iteration 12, loss = 0.19201716\n",
      "Iteration 6, loss = 0.26860572\n",
      "Iteration 13, loss = 0.16728858\n",
      "Iteration 7, loss = 0.24303367\n",
      "Iteration 14, loss = 0.14346377\n",
      "Iteration 8, loss = 0.23287361\n",
      "Iteration 15, loss = 0.13868334\n",
      "Iteration 9, loss = 0.21295573\n",
      "Iteration 16, loss = 0.14377738\n",
      "Iteration 10, loss = 0.20014956\n",
      "Iteration 17, loss = 0.13576541\n",
      "Iteration 11, loss = 0.18296109\n",
      "Iteration 18, loss = 0.12259153\n",
      "Iteration 12, loss = 0.18317951\n",
      "Iteration 19, loss = 0.11084469\n",
      "Iteration 13, loss = 0.20280604\n",
      "Iteration 20, loss = 0.09079406\n",
      "Iteration 14, loss = 0.17324150\n",
      "Iteration 21, loss = 0.08733372\n",
      "Iteration 15, loss = 0.17051588\n",
      "Iteration 22, loss = 0.08703278\n",
      "Iteration 23, loss = 0.08311363\n",
      "Iteration 16, loss = 0.14851824\n",
      "Iteration 24, loss = 0.07344156\n",
      "Iteration 17, loss = 0.14154323\n",
      "Iteration 25, loss = 0.06704311\n",
      "Iteration 18, loss = 0.12956546\n",
      "Iteration 26, loss = 0.07343348\n",
      "Iteration 19, loss = 0.11859352\n",
      "Iteration 27, loss = 0.07716646\n",
      "Iteration 20, loss = 0.11322644\n",
      "Iteration 28, loss = 0.06799579\n",
      "Iteration 21, loss = 0.11080758\n",
      "Iteration 29, loss = 0.07351721\n",
      "Iteration 22, loss = 0.10347604\n",
      "Iteration 30, loss = 0.06466674\n",
      "Iteration 23, loss = 0.10251182\n",
      "Iteration 31, loss = 0.06934994\n",
      "Iteration 24, loss = 0.09203775\n",
      "Iteration 32, loss = 0.07664657\n",
      "Iteration 25, loss = 0.08966430\n",
      "Iteration 33, loss = 0.07400635\n",
      "Iteration 26, loss = 0.09078207\n",
      "Iteration 34, loss = 0.06972157\n",
      "Iteration 27, loss = 0.10117902\n",
      "Iteration 35, loss = 0.05784962\n",
      "Iteration 28, loss = 0.09738359\n",
      "Iteration 36, loss = 0.05701364\n",
      "Iteration 29, loss = 0.10860446\n",
      "Iteration 37, loss = 0.08164826\n",
      "Iteration 30, loss = 0.12470138\n",
      "Iteration 38, loss = 0.06772444\n",
      "Iteration 31, loss = 0.10377984\n",
      "Iteration 39, loss = 0.06202856\n",
      "Iteration 32, loss = 0.08754110\n",
      "Iteration 40, loss = 0.05439857\n",
      "Iteration 33, loss = 0.09460923\n",
      "Iteration 41, loss = 0.05874121\n",
      "Iteration 34, loss = 0.09415163\n",
      "Iteration 42, loss = 0.05700596\n",
      "Iteration 35, loss = 0.09289033\n",
      "Iteration 43, loss = 0.04073888\n",
      "Iteration 36, loss = 0.08160534\n",
      "Iteration 44, loss = 0.04247368\n",
      "Iteration 37, loss = 0.06820222\n",
      "Iteration 45, loss = 0.04832647\n",
      "Iteration 38, loss = 0.07478160\n",
      "Iteration 46, loss = 0.06399637\n",
      "Iteration 39, loss = 0.07082786\n",
      "Iteration 47, loss = 0.06705659\n",
      "Iteration 40, loss = 0.07353742\n",
      "Iteration 48, loss = 0.13386293\n",
      "Iteration 41, loss = 0.07000420\n",
      "Iteration 49, loss = 0.08722986\n",
      "Iteration 42, loss = 0.05922792\n",
      "Iteration 50, loss = 0.10564024\n",
      "Iteration 43, loss = 0.05982565\n",
      "Iteration 44, loss = 0.07150488\n",
      "Iteration 51, loss = 0.06994002\n",
      "Iteration 45, loss = 0.08007058\n",
      "Iteration 52, loss = 0.07001545\n",
      "Iteration 46, loss = 0.06474068\n",
      "Iteration 53, loss = 0.06223380\n",
      "Iteration 47, loss = 0.05589667\n",
      "Iteration 54, loss = 0.04183869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.05687158\n",
      "Iteration 49, loss = 0.07892287\n",
      "Iteration 50, loss = 0.10169622\n",
      "Iteration 51, loss = 0.07259533\n",
      "Iteration 52, loss = 0.10552306\n",
      "Iteration 53, loss = 0.07713997\n",
      "Iteration 54, loss = 0.08631706\n",
      "Iteration 55, loss = 0.06248384\n",
      "Iteration 56, loss = 0.05311625\n",
      "Iteration 57, loss = 0.04995387\n",
      "Iteration 58, loss = 0.04838340\n",
      "Iteration 59, loss = 0.06709310\n",
      "Iteration 60, loss = 0.04568322\n",
      "Iteration 1, loss = 1.15115572\n",
      "Iteration 61, loss = 0.04249914\n",
      "Iteration 2, loss = 0.52173692\n",
      "Iteration 62, loss = 0.03701080Iteration 3, loss = 0.42013249\n",
      "\n",
      "Iteration 4, loss = 0.36943292\n",
      "Iteration 63, loss = 0.04652433\n",
      "Iteration 5, loss = 0.34163143\n",
      "Iteration 64, loss = 0.03199961\n",
      "Iteration 6, loss = 0.30651774\n",
      "Iteration 65, loss = 0.04041304\n",
      "Iteration 7, loss = 0.28398724\n",
      "Iteration 66, loss = 0.03183007\n",
      "Iteration 8, loss = 0.26922497\n",
      "Iteration 67, loss = 0.02923609\n",
      "Iteration 9, loss = 0.26169237\n",
      "Iteration 68, loss = 0.03695348\n",
      "Iteration 10, loss = 0.23107689\n",
      "Iteration 69, loss = 0.03447925\n",
      "Iteration 11, loss = 0.21960136\n",
      "Iteration 70, loss = 0.03410529\n",
      "Iteration 12, loss = 0.21335320\n",
      "Iteration 71, loss = 0.03251812\n",
      "Iteration 13, loss = 0.19287578\n",
      "Iteration 72, loss = 0.02585079\n",
      "Iteration 14, loss = 0.18564868\n",
      "Iteration 73, loss = 0.02779354\n",
      "Iteration 15, loss = 0.17155238\n",
      "Iteration 74, loss = 0.02503360\n",
      "Iteration 16, loss = 0.15517906\n",
      "Iteration 75, loss = 0.03260626\n",
      "Iteration 17, loss = 0.15031446\n",
      "Iteration 76, loss = 0.02782850\n",
      "Iteration 18, loss = 0.15559647\n",
      "Iteration 77, loss = 0.03534070\n",
      "Iteration 19, loss = 0.16715689\n",
      "Iteration 78, loss = 0.03473437\n",
      "Iteration 20, loss = 0.15128308\n",
      "Iteration 79, loss = 0.03577648\n",
      "Iteration 21, loss = 0.15253670\n",
      "Iteration 80, loss = 0.05041324\n",
      "Iteration 22, loss = 0.16902116\n",
      "Iteration 81, loss = 0.04347969\n",
      "Iteration 23, loss = 0.16163948\n",
      "Iteration 82, loss = 0.05778485\n",
      "Iteration 24, loss = 0.15676855\n",
      "Iteration 83, loss = 0.04220411\n",
      "Iteration 25, loss = 0.14621420\n",
      "Iteration 84, loss = 0.07185986\n",
      "Iteration 26, loss = 0.13495681\n",
      "Iteration 85, loss = 0.04457367\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.13260932\n",
      "Iteration 28, loss = 0.11898129\n",
      "Iteration 29, loss = 0.09726257\n",
      "Iteration 30, loss = 0.08943346\n",
      "Iteration 31, loss = 0.08663069\n",
      "Iteration 32, loss = 0.08662196\n",
      "Iteration 33, loss = 0.08175719\n",
      "Iteration 34, loss = 0.07913336\n",
      "Iteration 35, loss = 0.07404682\n",
      "Iteration 36, loss = 0.08166260\n",
      "Iteration 37, loss = 0.07076716\n",
      "Iteration 38, loss = 0.06938897\n",
      "Iteration 39, loss = 0.08109854\n",
      "Iteration 1, loss = 0.91852790\n",
      "Iteration 40, loss = 0.07295559\n",
      "Iteration 2, loss = 0.47003543\n",
      "Iteration 41, loss = 0.08433534\n",
      "Iteration 3, loss = 0.39529657\n",
      "Iteration 42, loss = 0.10849843\n",
      "Iteration 4, loss = 0.33715336\n",
      "Iteration 43, loss = 0.08338094\n",
      "Iteration 5, loss = 0.30541328\n",
      "Iteration 44, loss = 0.06566612\n",
      "Iteration 6, loss = 0.27202749\n",
      "Iteration 45, loss = 0.08197635\n",
      "Iteration 7, loss = 0.24905680\n",
      "Iteration 46, loss = 0.07442412\n",
      "Iteration 8, loss = 0.23031249\n",
      "Iteration 47, loss = 0.06809080\n",
      "Iteration 9, loss = 0.21558877\n",
      "Iteration 48, loss = 0.06884540\n",
      "Iteration 10, loss = 0.23400807\n",
      "Iteration 49, loss = 0.12626988\n",
      "Iteration 11, loss = 0.18943831\n",
      "Iteration 50, loss = 0.09502563\n",
      "Iteration 12, loss = 0.17394297\n",
      "Iteration 51, loss = 0.08008955\n",
      "Iteration 13, loss = 0.15401981\n",
      "Iteration 52, loss = 0.08347056\n",
      "Iteration 14, loss = 0.15375191\n",
      "Iteration 53, loss = 0.08153445\n",
      "Iteration 15, loss = 0.15456441\n",
      "Iteration 54, loss = 0.07474890\n",
      "Iteration 16, loss = 0.15504255\n",
      "Iteration 55, loss = 0.06384174\n",
      "Iteration 17, loss = 0.15169918\n",
      "Iteration 56, loss = 0.05722808\n",
      "Iteration 18, loss = 0.12706733\n",
      "Iteration 57, loss = 0.07318801\n",
      "Iteration 19, loss = 0.12654472\n",
      "Iteration 58, loss = 0.06866034\n",
      "Iteration 20, loss = 0.14525127\n",
      "Iteration 59, loss = 0.08825333\n",
      "Iteration 21, loss = 0.15107760\n",
      "Iteration 60, loss = 0.07573099\n",
      "Iteration 22, loss = 0.10292578\n",
      "Iteration 61, loss = 0.09777518\n",
      "Iteration 23, loss = 0.10722903\n",
      "Iteration 62, loss = 0.10445941\n",
      "Iteration 24, loss = 0.09774450\n",
      "Iteration 63, loss = 0.11547041\n",
      "Iteration 25, loss = 0.09555549\n",
      "Iteration 64, loss = 0.11730997\n",
      "Iteration 26, loss = 0.08468588\n",
      "Iteration 65, loss = 0.07930100\n",
      "Iteration 27, loss = 0.07608415\n",
      "Iteration 66, loss = 0.09293653\n",
      "Iteration 28, loss = 0.07391387\n",
      "Iteration 67, loss = 0.07417273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.08211255\n",
      "Iteration 30, loss = 0.07737778\n",
      "Iteration 31, loss = 0.08059747\n",
      "Iteration 32, loss = 0.07423460\n",
      "Iteration 33, loss = 0.08388074\n",
      "Iteration 34, loss = 0.06685483\n",
      "Iteration 35, loss = 0.06863415\n",
      "Iteration 36, loss = 0.07757406\n",
      "Iteration 37, loss = 0.06219768\n",
      "Iteration 38, loss = 0.05891388\n",
      "Iteration 39, loss = 0.04903016\n",
      "Iteration 40, loss = 0.05015891\n",
      "Iteration 41, loss = 0.05054394\n",
      "Iteration 1, loss = 0.96048577\n",
      "Iteration 42, loss = 0.04110494\n",
      "Iteration 2, loss = 0.49679848\n",
      "Iteration 43, loss = 0.04151922\n",
      "Iteration 3, loss = 0.40366519\n",
      "Iteration 44, loss = 0.06002094\n",
      "Iteration 4, loss = 0.37832371\n",
      "Iteration 45, loss = 0.08264339\n",
      "Iteration 5, loss = 0.33719921\n",
      "Iteration 46, loss = 0.06543150\n",
      "Iteration 6, loss = 0.32178232\n",
      "Iteration 47, loss = 0.06305330\n",
      "Iteration 7, loss = 0.29390655\n",
      "Iteration 48, loss = 0.10224980\n",
      "Iteration 8, loss = 0.25480367\n",
      "Iteration 49, loss = 0.13548097\n",
      "Iteration 9, loss = 0.23877027\n",
      "Iteration 50, loss = 0.15646736\n",
      "Iteration 10, loss = 0.21464297\n",
      "Iteration 51, loss = 0.14765587\n",
      "Iteration 11, loss = 0.19883222\n",
      "Iteration 52, loss = 0.13160097\n",
      "Iteration 12, loss = 0.18542838\n",
      "Iteration 53, loss = 0.14062916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.18024122\n",
      "Iteration 14, loss = 0.17156398\n",
      "Iteration 15, loss = 0.15426153\n",
      "Iteration 16, loss = 0.14880371\n",
      "Iteration 17, loss = 0.14652722\n",
      "Iteration 18, loss = 0.15629072\n",
      "Iteration 19, loss = 0.17334738\n",
      "Iteration 20, loss = 0.18755549\n",
      "Iteration 21, loss = 0.13989134\n",
      "Iteration 22, loss = 0.13917457\n",
      "Iteration 23, loss = 0.12872438\n",
      "Iteration 24, loss = 0.12304109\n",
      "Iteration 25, loss = 0.11748421\n",
      "Iteration 1, loss = 0.87256581\n",
      "Iteration 26, loss = 0.11409696\n",
      "Iteration 2, loss = 0.48633913\n",
      "Iteration 27, loss = 0.12861470\n",
      "Iteration 3, loss = 0.37106411\n",
      "Iteration 28, loss = 0.14197202\n",
      "Iteration 4, loss = 0.32707036\n",
      "Iteration 29, loss = 0.16568592\n",
      "Iteration 5, loss = 0.28682960\n",
      "Iteration 30, loss = 0.11283077\n",
      "Iteration 6, loss = 0.27560547\n",
      "Iteration 31, loss = 0.11372423\n",
      "Iteration 7, loss = 0.26259734\n",
      "Iteration 32, loss = 0.09582541\n",
      "Iteration 8, loss = 0.23221729\n",
      "Iteration 33, loss = 0.09857953\n",
      "Iteration 9, loss = 0.23045742\n",
      "Iteration 34, loss = 0.09862213\n",
      "Iteration 10, loss = 0.19785622\n",
      "Iteration 35, loss = 0.09980784\n",
      "Iteration 11, loss = 0.17850545\n",
      "Iteration 36, loss = 0.09363772\n",
      "Iteration 12, loss = 0.16025186\n",
      "Iteration 37, loss = 0.10834358\n",
      "Iteration 13, loss = 0.16628991\n",
      "Iteration 38, loss = 0.09957471\n",
      "Iteration 14, loss = 0.14269325\n",
      "Iteration 39, loss = 0.08766776\n",
      "Iteration 15, loss = 0.13303255\n",
      "Iteration 40, loss = 0.08009583\n",
      "Iteration 16, loss = 0.13810206\n",
      "Iteration 41, loss = 0.09490632\n",
      "Iteration 17, loss = 0.12189264\n",
      "Iteration 42, loss = 0.08265381\n",
      "Iteration 18, loss = 0.11368612\n",
      "Iteration 43, loss = 0.09903651\n",
      "Iteration 19, loss = 0.10933774\n",
      "Iteration 44, loss = 0.10220103\n",
      "Iteration 20, loss = 0.12405070\n",
      "Iteration 45, loss = 0.10269242\n",
      "Iteration 21, loss = 0.11013329\n",
      "Iteration 46, loss = 0.08031221\n",
      "Iteration 22, loss = 0.13358293\n",
      "Iteration 47, loss = 0.07636494\n",
      "Iteration 23, loss = 0.10138111\n",
      "Iteration 48, loss = 0.08682616\n",
      "Iteration 24, loss = 0.11870866\n",
      "Iteration 49, loss = 0.08473463\n",
      "Iteration 25, loss = 0.13300811\n",
      "Iteration 50, loss = 0.07446902\n",
      "Iteration 26, loss = 0.12062808\n",
      "Iteration 51, loss = 0.07863487\n",
      "Iteration 27, loss = 0.10835150\n",
      "Iteration 52, loss = 0.07834941\n",
      "Iteration 28, loss = 0.12463751\n",
      "Iteration 53, loss = 0.10100388\n",
      "Iteration 29, loss = 0.13582204\n",
      "Iteration 54, loss = 0.12763583\n",
      "Iteration 30, loss = 0.14294942\n",
      "Iteration 55, loss = 0.10429025\n",
      "Iteration 31, loss = 0.16032047\n",
      "Iteration 56, loss = 0.10639860\n",
      "Iteration 32, loss = 0.10878880\n",
      "Iteration 57, loss = 0.10085986\n",
      "Iteration 33, loss = 0.09512361\n",
      "Iteration 58, loss = 0.14392578\n",
      "Iteration 34, loss = 0.10335656\n",
      "Iteration 59, loss = 0.09127066\n",
      "Iteration 35, loss = 0.12922154\n",
      "Iteration 60, loss = 0.07512526\n",
      "Iteration 36, loss = 0.10133728\n",
      "Iteration 61, loss = 0.08871438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.09526821\n",
      "Iteration 38, loss = 0.08162950\n",
      "Iteration 39, loss = 0.07826040\n",
      "Iteration 40, loss = 0.06620854\n",
      "Iteration 41, loss = 0.06695187\n",
      "Iteration 42, loss = 0.07570880\n",
      "Iteration 43, loss = 0.06328079\n",
      "Iteration 44, loss = 0.05551165\n",
      "Iteration 45, loss = 0.04990627\n",
      "Iteration 46, loss = 0.05511786\n",
      "Iteration 47, loss = 0.05927262\n",
      "Iteration 48, loss = 0.06190474\n",
      "Iteration 1, loss = 1.46112448\n",
      "Iteration 49, loss = 0.06866479\n",
      "Iteration 2, loss = 1.01378674\n",
      "Iteration 50, loss = 0.11772540\n",
      "Iteration 3, loss = 0.79939427\n",
      "Iteration 51, loss = 0.07753543\n",
      "Iteration 4, loss = 0.68232984\n",
      "Iteration 52, loss = 0.09228139\n",
      "Iteration 5, loss = 0.59441664\n",
      "Iteration 53, loss = 0.06510964\n",
      "Iteration 6, loss = 0.53968622\n",
      "Iteration 54, loss = 0.06621680\n",
      "Iteration 7, loss = 0.49936768\n",
      "Iteration 55, loss = 0.05383009\n",
      "Iteration 8, loss = 0.46867231\n",
      "Iteration 56, loss = 0.06348058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.44592840\n",
      "Iteration 10, loss = 0.42647743\n",
      "Iteration 11, loss = 0.41019125\n",
      "Iteration 12, loss = 0.39680771\n",
      "Iteration 13, loss = 0.38500212\n",
      "Iteration 14, loss = 0.37454147\n",
      "Iteration 15, loss = 0.36314889\n",
      "Iteration 16, loss = 0.35588801\n",
      "Iteration 17, loss = 0.34700999\n",
      "Iteration 18, loss = 0.33900643\n",
      "Iteration 19, loss = 0.33196473\n",
      "Iteration 20, loss = 0.32517718\n",
      "Iteration 21, loss = 0.31869991\n",
      "Iteration 1, loss = 1.11453370\n",
      "Iteration 22, loss = 0.31217075\n",
      "Iteration 2, loss = 0.86591769\n",
      "Iteration 23, loss = 0.30569087\n",
      "Iteration 3, loss = 0.72397125\n",
      "Iteration 4, loss = 0.64159763Iteration 24, loss = 0.29980679\n",
      "\n",
      "Iteration 5, loss = 0.58007295\n",
      "Iteration 25, loss = 0.29429018\n",
      "Iteration 26, loss = 0.28903058\n",
      "Iteration 6, loss = 0.53354416\n",
      "Iteration 27, loss = 0.28329049\n",
      "Iteration 7, loss = 0.49962389\n",
      "Iteration 28, loss = 0.27777439\n",
      "Iteration 8, loss = 0.47275102\n",
      "Iteration 29, loss = 0.27390592\n",
      "Iteration 9, loss = 0.44938225\n",
      "Iteration 30, loss = 0.26775848\n",
      "Iteration 10, loss = 0.42972203\n",
      "Iteration 31, loss = 0.26239569\n",
      "Iteration 11, loss = 0.41272188\n",
      "Iteration 32, loss = 0.25770790\n",
      "Iteration 12, loss = 0.39836257\n",
      "Iteration 33, loss = 0.25395354\n",
      "Iteration 13, loss = 0.38412015\n",
      "Iteration 34, loss = 0.24875345\n",
      "Iteration 14, loss = 0.37274403\n",
      "Iteration 15, loss = 0.36206875\n",
      "Iteration 35, loss = 0.24591900\n",
      "Iteration 16, loss = 0.35335724\n",
      "Iteration 36, loss = 0.24218143\n",
      "Iteration 17, loss = 0.34277067\n",
      "Iteration 37, loss = 0.23677280\n",
      "Iteration 18, loss = 0.33567030\n",
      "Iteration 38, loss = 0.23232491\n",
      "Iteration 39, loss = 0.23003774\n",
      "Iteration 19, loss = 0.32875590\n",
      "Iteration 40, loss = 0.22560423\n",
      "Iteration 20, loss = 0.32012034\n",
      "Iteration 41, loss = 0.22233109\n",
      "Iteration 21, loss = 0.31147331\n",
      "Iteration 42, loss = 0.21835947\n",
      "Iteration 22, loss = 0.30560701\n",
      "Iteration 43, loss = 0.21576677\n",
      "Iteration 23, loss = 0.29953742\n",
      "Iteration 44, loss = 0.21189717\n",
      "Iteration 24, loss = 0.29343032\n",
      "Iteration 45, loss = 0.20778053\n",
      "Iteration 25, loss = 0.28663128\n",
      "Iteration 46, loss = 0.20386210\n",
      "Iteration 26, loss = 0.28159159\n",
      "Iteration 47, loss = 0.20181982\n",
      "Iteration 27, loss = 0.27528052\n",
      "Iteration 48, loss = 0.19948092\n",
      "Iteration 28, loss = 0.26999526\n",
      "Iteration 49, loss = 0.19598076\n",
      "Iteration 29, loss = 0.26470978\n",
      "Iteration 50, loss = 0.19256534\n",
      "Iteration 30, loss = 0.26137954\n",
      "Iteration 51, loss = 0.19009975\n",
      "Iteration 31, loss = 0.25440565\n",
      "Iteration 52, loss = 0.18743152\n",
      "Iteration 32, loss = 0.25201638\n",
      "Iteration 53, loss = 0.18550613\n",
      "Iteration 33, loss = 0.24468846\n",
      "Iteration 54, loss = 0.18155563\n",
      "Iteration 34, loss = 0.24103872\n",
      "Iteration 55, loss = 0.17888986\n",
      "Iteration 35, loss = 0.23431130\n",
      "Iteration 56, loss = 0.17579026\n",
      "Iteration 36, loss = 0.23159070\n",
      "Iteration 37, loss = 0.22601457\n",
      "Iteration 57, loss = 0.17303349\n",
      "Iteration 38, loss = 0.22159485\n",
      "Iteration 58, loss = 0.17046965\n",
      "Iteration 39, loss = 0.21933061\n",
      "Iteration 59, loss = 0.16769034\n",
      "Iteration 40, loss = 0.21355366\n",
      "Iteration 60, loss = 0.16598018\n",
      "Iteration 41, loss = 0.21019533\n",
      "Iteration 61, loss = 0.16314792\n",
      "Iteration 42, loss = 0.20785852\n",
      "Iteration 62, loss = 0.16135960\n",
      "Iteration 43, loss = 0.20334363\n",
      "Iteration 63, loss = 0.15855856\n",
      "Iteration 44, loss = 0.20247423\n",
      "Iteration 64, loss = 0.15789268\n",
      "Iteration 45, loss = 0.19757838\n",
      "Iteration 65, loss = 0.15543067\n",
      "Iteration 46, loss = 0.19462892\n",
      "Iteration 66, loss = 0.15289686\n",
      "Iteration 47, loss = 0.19086163\n",
      "Iteration 67, loss = 0.15115620\n",
      "Iteration 48, loss = 0.18888193\n",
      "Iteration 68, loss = 0.14794188\n",
      "Iteration 49, loss = 0.18596701\n",
      "Iteration 69, loss = 0.14637792\n",
      "Iteration 50, loss = 0.18486067\n",
      "Iteration 70, loss = 0.14454146\n",
      "Iteration 51, loss = 0.18102868\n",
      "Iteration 71, loss = 0.14269925\n",
      "Iteration 52, loss = 0.17892315\n",
      "Iteration 72, loss = 0.14058654\n",
      "Iteration 53, loss = 0.17838806\n",
      "Iteration 73, loss = 0.13982928\n",
      "Iteration 54, loss = 0.17630322\n",
      "Iteration 74, loss = 0.13829094\n",
      "Iteration 55, loss = 0.17161055\n",
      "Iteration 75, loss = 0.13349288\n",
      "Iteration 56, loss = 0.16866751\n",
      "Iteration 76, loss = 0.13683758\n",
      "Iteration 57, loss = 0.16610484\n",
      "Iteration 77, loss = 0.13156159\n",
      "Iteration 58, loss = 0.16338924\n",
      "Iteration 78, loss = 0.13064693\n",
      "Iteration 59, loss = 0.16241020\n",
      "Iteration 79, loss = 0.12818125\n",
      "Iteration 60, loss = 0.15985272\n",
      "Iteration 80, loss = 0.12815980\n",
      "Iteration 61, loss = 0.15807947\n",
      "Iteration 81, loss = 0.12842587\n",
      "Iteration 62, loss = 0.15550041\n",
      "Iteration 82, loss = 0.12496028\n",
      "Iteration 63, loss = 0.15465818\n",
      "Iteration 83, loss = 0.12250064\n",
      "Iteration 64, loss = 0.15578380\n",
      "Iteration 84, loss = 0.12217066\n",
      "Iteration 65, loss = 0.15045794\n",
      "Iteration 85, loss = 0.12315675\n",
      "Iteration 66, loss = 0.15015206\n",
      "Iteration 86, loss = 0.12177754\n",
      "Iteration 67, loss = 0.14612371\n",
      "Iteration 87, loss = 0.11889647\n",
      "Iteration 68, loss = 0.14737230\n",
      "Iteration 88, loss = 0.11685614\n",
      "Iteration 69, loss = 0.14540148\n",
      "Iteration 89, loss = 0.11681300\n",
      "Iteration 70, loss = 0.14136519\n",
      "Iteration 90, loss = 0.11414175\n",
      "Iteration 71, loss = 0.14243874\n",
      "Iteration 91, loss = 0.11295879\n",
      "Iteration 72, loss = 0.14377656\n",
      "Iteration 92, loss = 0.11211957\n",
      "Iteration 73, loss = 0.13977003\n",
      "Iteration 93, loss = 0.11142401\n",
      "Iteration 74, loss = 0.13691182\n",
      "Iteration 94, loss = 0.10787458\n",
      "Iteration 75, loss = 0.13460470\n",
      "Iteration 95, loss = 0.10932141\n",
      "Iteration 76, loss = 0.13252443\n",
      "Iteration 96, loss = 0.10720664\n",
      "Iteration 77, loss = 0.13095312\n",
      "Iteration 97, loss = 0.10385381\n",
      "Iteration 78, loss = 0.13165420\n",
      "Iteration 98, loss = 0.10446426\n",
      "Iteration 79, loss = 0.12755839\n",
      "Iteration 99, loss = 0.10717837\n",
      "Iteration 80, loss = 0.12827320\n",
      "Iteration 100, loss = 0.10185354\n",
      "Iteration 81, loss = 0.12581494\n",
      "Iteration 82, loss = 0.12306298\n",
      "Iteration 83, loss = 0.12240026\n",
      "Iteration 84, loss = 0.12029472\n",
      "Iteration 85, loss = 0.11961215\n",
      "Iteration 86, loss = 0.11997003\n",
      "Iteration 87, loss = 0.11679750\n",
      "Iteration 88, loss = 0.11559790\n",
      "Iteration 89, loss = 0.11472031\n",
      "Iteration 90, loss = 0.11362646\n",
      "Iteration 91, loss = 0.11393593\n",
      "Iteration 92, loss = 0.10854260\n",
      "Iteration 93, loss = 0.11298760\n",
      "Iteration 1, loss = 0.99531900\n",
      "Iteration 94, loss = 0.10963962\n",
      "Iteration 2, loss = 0.78040106\n",
      "Iteration 95, loss = 0.10826506\n",
      "Iteration 3, loss = 0.65082870\n",
      "Iteration 96, loss = 0.10700460\n",
      "Iteration 4, loss = 0.56798063\n",
      "Iteration 97, loss = 0.10625157\n",
      "Iteration 5, loss = 0.51241376\n",
      "Iteration 98, loss = 0.10324431\n",
      "Iteration 6, loss = 0.47338776\n",
      "Iteration 99, loss = 0.10371155\n",
      "Iteration 7, loss = 0.44373389\n",
      "Iteration 100, loss = 0.10278412\n",
      "Iteration 8, loss = 0.42004881\n",
      "Iteration 9, loss = 0.40074568\n",
      "Iteration 10, loss = 0.38537105\n",
      "Iteration 11, loss = 0.37095178\n",
      "Iteration 12, loss = 0.35917269\n",
      "Iteration 13, loss = 0.34768294\n",
      "Iteration 14, loss = 0.33835777\n",
      "Iteration 15, loss = 0.32865630\n",
      "Iteration 16, loss = 0.31984423\n",
      "Iteration 17, loss = 0.31263488\n",
      "Iteration 18, loss = 0.30427437\n",
      "Iteration 19, loss = 0.29793813\n",
      "Iteration 20, loss = 0.29318222\n",
      "Iteration 1, loss = 1.35074486\n",
      "Iteration 21, loss = 0.28416320\n",
      "Iteration 2, loss = 0.96037893\n",
      "Iteration 22, loss = 0.27869178\n",
      "Iteration 3, loss = 0.75211189\n",
      "Iteration 23, loss = 0.27270567\n",
      "Iteration 4, loss = 0.63524098\n",
      "Iteration 24, loss = 0.26807091\n",
      "Iteration 5, loss = 0.56499365\n",
      "Iteration 25, loss = 0.26215165\n",
      "Iteration 6, loss = 0.51677589\n",
      "Iteration 26, loss = 0.25860471\n",
      "Iteration 7, loss = 0.48559322\n",
      "Iteration 27, loss = 0.25254167\n",
      "Iteration 8, loss = 0.45745108\n",
      "Iteration 28, loss = 0.24771737\n",
      "Iteration 9, loss = 0.43683173\n",
      "Iteration 29, loss = 0.24498059\n",
      "Iteration 10, loss = 0.41969180\n",
      "Iteration 30, loss = 0.23917361\n",
      "Iteration 11, loss = 0.40568615\n",
      "Iteration 31, loss = 0.23534321\n",
      "Iteration 12, loss = 0.39248902\n",
      "Iteration 32, loss = 0.23040743\n",
      "Iteration 13, loss = 0.37946201\n",
      "Iteration 33, loss = 0.22599374\n",
      "Iteration 14, loss = 0.36947515\n",
      "Iteration 34, loss = 0.22132712\n",
      "Iteration 15, loss = 0.35990032\n",
      "Iteration 35, loss = 0.21826753\n",
      "Iteration 16, loss = 0.35015214\n",
      "Iteration 36, loss = 0.21461065\n",
      "Iteration 17, loss = 0.34201392\n",
      "Iteration 37, loss = 0.21054025\n",
      "Iteration 18, loss = 0.33355824\n",
      "Iteration 38, loss = 0.20737971\n",
      "Iteration 19, loss = 0.32578146\n",
      "Iteration 39, loss = 0.20408590\n",
      "Iteration 20, loss = 0.31988374\n",
      "Iteration 40, loss = 0.19978428\n",
      "Iteration 21, loss = 0.31073680\n",
      "Iteration 41, loss = 0.19788882\n",
      "Iteration 22, loss = 0.30527344\n",
      "Iteration 42, loss = 0.19407382\n",
      "Iteration 23, loss = 0.29746699\n",
      "Iteration 43, loss = 0.19103201\n",
      "Iteration 24, loss = 0.29278824\n",
      "Iteration 44, loss = 0.18811300\n",
      "Iteration 25, loss = 0.28588389\n",
      "Iteration 45, loss = 0.18496058\n",
      "Iteration 26, loss = 0.28171670\n",
      "Iteration 46, loss = 0.18217909\n",
      "Iteration 27, loss = 0.27607374\n",
      "Iteration 47, loss = 0.17916607\n",
      "Iteration 28, loss = 0.27056929\n",
      "Iteration 48, loss = 0.17638701\n",
      "Iteration 29, loss = 0.26406274\n",
      "Iteration 49, loss = 0.17320841\n",
      "Iteration 30, loss = 0.25914053\n",
      "Iteration 50, loss = 0.17061327\n",
      "Iteration 51, loss = 0.16821712\n",
      "Iteration 31, loss = 0.25417509\n",
      "Iteration 52, loss = 0.16587728\n",
      "Iteration 32, loss = 0.24890946\n",
      "Iteration 53, loss = 0.16551318\n",
      "Iteration 33, loss = 0.24454370\n",
      "Iteration 34, loss = 0.23959367Iteration 54, loss = 0.16051720\n",
      "\n",
      "Iteration 55, loss = 0.15826932\n",
      "Iteration 35, loss = 0.23463735\n",
      "Iteration 36, loss = 0.23146352\n",
      "Iteration 56, loss = 0.15664033\n",
      "Iteration 37, loss = 0.22819176\n",
      "Iteration 57, loss = 0.15320982\n",
      "Iteration 38, loss = 0.22619403\n",
      "Iteration 58, loss = 0.15233869\n",
      "Iteration 59, loss = 0.14862847\n",
      "Iteration 39, loss = 0.22039575\n",
      "Iteration 60, loss = 0.14732880\n",
      "Iteration 40, loss = 0.21688487\n",
      "Iteration 61, loss = 0.14473858\n",
      "Iteration 41, loss = 0.21564723\n",
      "Iteration 62, loss = 0.14530813\n",
      "Iteration 42, loss = 0.21083514\n",
      "Iteration 63, loss = 0.14176123\n",
      "Iteration 43, loss = 0.20587931\n",
      "Iteration 64, loss = 0.13850415\n",
      "Iteration 44, loss = 0.20325283\n",
      "Iteration 65, loss = 0.13799360\n",
      "Iteration 45, loss = 0.20303092\n",
      "Iteration 66, loss = 0.13632993\n",
      "Iteration 46, loss = 0.19832186\n",
      "Iteration 67, loss = 0.13516666\n",
      "Iteration 47, loss = 0.19457212\n",
      "Iteration 68, loss = 0.13398137\n",
      "Iteration 48, loss = 0.19311807\n",
      "Iteration 69, loss = 0.13121165\n",
      "Iteration 49, loss = 0.18959737\n",
      "Iteration 70, loss = 0.12920470\n",
      "Iteration 50, loss = 0.18642973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 0.12568400\n",
      "Iteration 51, loss = 0.18592621\n",
      "Iteration 72, loss = 0.12748595\n",
      "Iteration 52, loss = 0.18089368\n",
      "Iteration 73, loss = 0.12444260\n",
      "Iteration 53, loss = 0.17919851\n",
      "Iteration 74, loss = 0.12410933\n",
      "Iteration 54, loss = 0.17637136\n",
      "Iteration 75, loss = 0.12137489\n",
      "Iteration 55, loss = 0.17193842\n",
      "Iteration 76, loss = 0.11803664\n",
      "Iteration 56, loss = 0.17183975\n",
      "Iteration 77, loss = 0.11839078\n",
      "Iteration 57, loss = 0.17004452\n",
      "Iteration 78, loss = 0.11669164\n",
      "Iteration 58, loss = 0.16513977\n",
      "Iteration 79, loss = 0.11464996\n",
      "Iteration 59, loss = 0.16655519\n",
      "Iteration 80, loss = 0.11199175\n",
      "Iteration 60, loss = 0.16241138\n",
      "Iteration 81, loss = 0.11253172\n",
      "Iteration 61, loss = 0.15905397\n",
      "Iteration 82, loss = 0.11238227\n",
      "Iteration 62, loss = 0.15862205\n",
      "Iteration 83, loss = 0.10801534\n",
      "Iteration 63, loss = 0.15726465\n",
      "Iteration 84, loss = 0.10723756\n",
      "Iteration 64, loss = 0.15265484\n",
      "Iteration 85, loss = 0.10550585\n",
      "Iteration 65, loss = 0.15135813\n",
      "Iteration 86, loss = 0.10508208\n",
      "Iteration 66, loss = 0.15227401\n",
      "Iteration 87, loss = 0.10122883\n",
      "Iteration 67, loss = 0.14713619\n",
      "Iteration 88, loss = 0.10315345\n",
      "Iteration 68, loss = 0.14582115\n",
      "Iteration 89, loss = 0.10052941\n",
      "Iteration 69, loss = 0.14457341\n",
      "Iteration 90, loss = 0.09850615\n",
      "Iteration 70, loss = 0.14205207\n",
      "Iteration 91, loss = 0.09723514\n",
      "Iteration 71, loss = 0.13979371\n",
      "Iteration 92, loss = 0.09618714\n",
      "Iteration 72, loss = 0.13761929\n",
      "Iteration 93, loss = 0.09377888\n",
      "Iteration 73, loss = 0.13567302\n",
      "Iteration 94, loss = 0.09508387\n",
      "Iteration 74, loss = 0.13607839\n",
      "Iteration 95, loss = 0.09461469\n",
      "Iteration 75, loss = 0.13335664\n",
      "Iteration 96, loss = 0.09461360\n",
      "Iteration 76, loss = 0.13032764\n",
      "Iteration 97, loss = 0.08901865\n",
      "Iteration 77, loss = 0.13039150\n",
      "Iteration 98, loss = 0.08945425\n",
      "Iteration 78, loss = 0.12725214\n",
      "Iteration 99, loss = 0.08684055\n",
      "Iteration 79, loss = 0.12789114\n",
      "Iteration 100, loss = 0.08802079\n",
      "Iteration 80, loss = 0.12771992\n",
      "Iteration 81, loss = 0.12331221\n",
      "Iteration 82, loss = 0.12433699\n",
      "Iteration 83, loss = 0.12082767\n",
      "Iteration 84, loss = 0.12011513\n",
      "Iteration 85, loss = 0.11874752\n",
      "Iteration 86, loss = 0.11713845\n",
      "Iteration 87, loss = 0.11569704\n",
      "Iteration 88, loss = 0.11358342\n",
      "Iteration 89, loss = 0.11531064\n",
      "Iteration 90, loss = 0.11063999\n",
      "Iteration 91, loss = 0.11125638\n",
      "Iteration 92, loss = 0.11099589\n",
      "Iteration 1, loss = 1.12471877\n",
      "Iteration 93, loss = 0.10708713\n",
      "Iteration 2, loss = 0.78087864\n",
      "Iteration 94, loss = 0.10971561\n",
      "Iteration 3, loss = 0.62592258\n",
      "Iteration 95, loss = 0.10637334\n",
      "Iteration 4, loss = 0.54837027\n",
      "Iteration 96, loss = 0.10768576\n",
      "Iteration 5, loss = 0.49598837\n",
      "Iteration 97, loss = 0.10438700\n",
      "Iteration 6, loss = 0.46341646\n",
      "Iteration 98, loss = 0.10513228\n",
      "Iteration 7, loss = 0.43568361\n",
      "Iteration 99, loss = 0.10500064\n",
      "Iteration 8, loss = 0.41368696\n",
      "Iteration 100, loss = 0.10116201\n",
      "Iteration 9, loss = 0.39565138\n",
      "Iteration 10, loss = 0.37998139\n",
      "Iteration 11, loss = 0.36671791\n",
      "Iteration 12, loss = 0.35451024\n",
      "Iteration 13, loss = 0.34231405\n",
      "Iteration 14, loss = 0.33207045\n",
      "Iteration 15, loss = 0.32203147\n",
      "Iteration 16, loss = 0.31255889\n",
      "Iteration 17, loss = 0.30388636\n",
      "Iteration 18, loss = 0.29512465\n",
      "Iteration 19, loss = 0.28814117\n",
      "Iteration 20, loss = 0.28276541\n",
      "Iteration 21, loss = 0.27490890\n",
      "Iteration 1, loss = 0.93733962\n",
      "Iteration 22, loss = 0.26785003\n",
      "Iteration 2, loss = 0.66495362\n",
      "Iteration 23, loss = 0.26063821\n",
      "Iteration 3, loss = 0.55223497\n",
      "Iteration 24, loss = 0.25606707\n",
      "Iteration 4, loss = 0.48939958\n",
      "Iteration 25, loss = 0.24890121\n",
      "Iteration 5, loss = 0.45237061\n",
      "Iteration 26, loss = 0.24554359\n",
      "Iteration 6, loss = 0.42478761\n",
      "Iteration 27, loss = 0.23930939\n",
      "Iteration 7, loss = 0.40107938\n",
      "Iteration 28, loss = 0.23288891\n",
      "Iteration 8, loss = 0.38741954\n",
      "Iteration 29, loss = 0.22676180\n",
      "Iteration 9, loss = 0.36987428\n",
      "Iteration 30, loss = 0.22252188\n",
      "Iteration 10, loss = 0.35820383\n",
      "Iteration 31, loss = 0.21808842\n",
      "Iteration 11, loss = 0.34718278\n",
      "Iteration 32, loss = 0.21398048\n",
      "Iteration 12, loss = 0.33658935\n",
      "Iteration 33, loss = 0.20765127\n",
      "Iteration 13, loss = 0.32809325\n",
      "Iteration 34, loss = 0.20411563\n",
      "Iteration 14, loss = 0.31903798\n",
      "Iteration 35, loss = 0.19871044\n",
      "Iteration 15, loss = 0.31079387\n",
      "Iteration 36, loss = 0.19435461\n",
      "Iteration 16, loss = 0.30372774\n",
      "Iteration 37, loss = 0.19130048\n",
      "Iteration 17, loss = 0.29647676\n",
      "Iteration 38, loss = 0.18699676\n",
      "Iteration 18, loss = 0.28840546\n",
      "Iteration 39, loss = 0.18388404\n",
      "Iteration 19, loss = 0.28140424\n",
      "Iteration 40, loss = 0.17980107\n",
      "Iteration 20, loss = 0.27559699\n",
      "Iteration 41, loss = 0.17686574\n",
      "Iteration 21, loss = 0.26830197\n",
      "Iteration 42, loss = 0.17441806\n",
      "Iteration 22, loss = 0.26312830\n",
      "Iteration 43, loss = 0.16965552\n",
      "Iteration 23, loss = 0.25727697\n",
      "Iteration 44, loss = 0.16677893\n",
      "Iteration 24, loss = 0.25075550\n",
      "Iteration 45, loss = 0.16468974\n",
      "Iteration 25, loss = 0.24632015\n",
      "Iteration 46, loss = 0.16078334\n",
      "Iteration 26, loss = 0.24057197\n",
      "Iteration 47, loss = 0.15878901\n",
      "Iteration 27, loss = 0.23697193\n",
      "Iteration 48, loss = 0.15443907\n",
      "Iteration 28, loss = 0.23016945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, loss = 0.15204372\n",
      "Iteration 29, loss = 0.22668728\n",
      "Iteration 50, loss = 0.14915543\n",
      "Iteration 30, loss = 0.22165561\n",
      "Iteration 51, loss = 0.14793141\n",
      "Iteration 31, loss = 0.21867770\n",
      "Iteration 52, loss = 0.14358687\n",
      "Iteration 32, loss = 0.21374482\n",
      "Iteration 53, loss = 0.14471657\n",
      "Iteration 33, loss = 0.20930250\n",
      "Iteration 54, loss = 0.14031608\n",
      "Iteration 34, loss = 0.20703529\n",
      "Iteration 55, loss = 0.14128237\n",
      "Iteration 35, loss = 0.20315772\n",
      "Iteration 56, loss = 0.13611582\n",
      "Iteration 36, loss = 0.20037327\n",
      "Iteration 57, loss = 0.13172742\n",
      "Iteration 37, loss = 0.19519165\n",
      "Iteration 58, loss = 0.13405491\n",
      "Iteration 38, loss = 0.19394413\n",
      "Iteration 59, loss = 0.12756353\n",
      "Iteration 39, loss = 0.18920008\n",
      "Iteration 60, loss = 0.12896041\n",
      "Iteration 40, loss = 0.18642344\n",
      "Iteration 61, loss = 0.12284179\n",
      "Iteration 41, loss = 0.18134065\n",
      "Iteration 62, loss = 0.12180306\n",
      "Iteration 42, loss = 0.17957553\n",
      "Iteration 63, loss = 0.11882099\n",
      "Iteration 43, loss = 0.17669461\n",
      "Iteration 64, loss = 0.11757331\n",
      "Iteration 44, loss = 0.17387139\n",
      "Iteration 65, loss = 0.11523085\n",
      "Iteration 45, loss = 0.17080363\n",
      "Iteration 66, loss = 0.11241047\n",
      "Iteration 46, loss = 0.16759540\n",
      "Iteration 67, loss = 0.11162204\n",
      "Iteration 47, loss = 0.16494754\n",
      "Iteration 68, loss = 0.10941747\n",
      "Iteration 48, loss = 0.16263729\n",
      "Iteration 69, loss = 0.10875005\n",
      "Iteration 49, loss = 0.15987808\n",
      "Iteration 70, loss = 0.10587358\n",
      "Iteration 50, loss = 0.15848362\n",
      "Iteration 71, loss = 0.10680132\n",
      "Iteration 51, loss = 0.15546861\n",
      "Iteration 72, loss = 0.10256704\n",
      "Iteration 52, loss = 0.15368819\n",
      "Iteration 73, loss = 0.10470359\n",
      "Iteration 53, loss = 0.15208669\n",
      "Iteration 74, loss = 0.10225364\n",
      "Iteration 54, loss = 0.14847198\n",
      "Iteration 75, loss = 0.10239941\n",
      "Iteration 55, loss = 0.14623490\n",
      "Iteration 76, loss = 0.09753999\n",
      "Iteration 56, loss = 0.14694692\n",
      "Iteration 77, loss = 0.09805599\n",
      "Iteration 57, loss = 0.14651370\n",
      "Iteration 78, loss = 0.09544478\n",
      "Iteration 58, loss = 0.14317861\n",
      "Iteration 79, loss = 0.09443754\n",
      "Iteration 59, loss = 0.13930612\n",
      "Iteration 80, loss = 0.09271029\n",
      "Iteration 60, loss = 0.14035939\n",
      "Iteration 81, loss = 0.09275454\n",
      "Iteration 61, loss = 0.13493268\n",
      "Iteration 82, loss = 0.08954070\n",
      "Iteration 62, loss = 0.13425480\n",
      "Iteration 83, loss = 0.09108125\n",
      "Iteration 63, loss = 0.13166477\n",
      "Iteration 84, loss = 0.08879621\n",
      "Iteration 64, loss = 0.13110687\n",
      "Iteration 85, loss = 0.08852051\n",
      "Iteration 65, loss = 0.12749947\n",
      "Iteration 86, loss = 0.08990238\n",
      "Iteration 66, loss = 0.12619025\n",
      "Iteration 87, loss = 0.08525099\n",
      "Iteration 67, loss = 0.12472855\n",
      "Iteration 88, loss = 0.08450093\n",
      "Iteration 68, loss = 0.12191015\n",
      "Iteration 89, loss = 0.08459465\n",
      "Iteration 69, loss = 0.12117240\n",
      "Iteration 90, loss = 0.08097879\n",
      "Iteration 70, loss = 0.11927023\n",
      "Iteration 91, loss = 0.08104689\n",
      "Iteration 71, loss = 0.11747914\n",
      "Iteration 92, loss = 0.07943000\n",
      "Iteration 72, loss = 0.11551051\n",
      "Iteration 93, loss = 0.07730719\n",
      "Iteration 73, loss = 0.11584500\n",
      "Iteration 94, loss = 0.07685364\n",
      "Iteration 74, loss = 0.11424743\n",
      "Iteration 95, loss = 0.07583832\n",
      "Iteration 75, loss = 0.11143899\n",
      "Iteration 96, loss = 0.07502893\n",
      "Iteration 76, loss = 0.11165110\n",
      "Iteration 97, loss = 0.07506119\n",
      "Iteration 77, loss = 0.10983098\n",
      "Iteration 98, loss = 0.07564698\n",
      "Iteration 78, loss = 0.10901474\n",
      "Iteration 99, loss = 0.07200337\n",
      "Iteration 79, loss = 0.10992213\n",
      "Iteration 100, loss = 0.07152678\n",
      "Iteration 80, loss = 0.10820576\n",
      "Iteration 81, loss = 0.10555227\n",
      "Iteration 82, loss = 0.10314665\n",
      "Iteration 83, loss = 0.10358003\n",
      "Iteration 84, loss = 0.10601762\n",
      "Iteration 85, loss = 0.10607995\n",
      "Iteration 86, loss = 0.09701952\n",
      "Iteration 87, loss = 0.09930212\n",
      "Iteration 88, loss = 0.09702849\n",
      "Iteration 89, loss = 0.09543451\n",
      "Iteration 90, loss = 0.09337132\n",
      "Iteration 91, loss = 0.09219953\n",
      "Iteration 92, loss = 0.09360953\n",
      "Iteration 1, loss = 1.22482499\n",
      "Iteration 93, loss = 0.09271829\n",
      "Iteration 2, loss = 0.93768967\n",
      "Iteration 94, loss = 0.08912228\n",
      "Iteration 3, loss = 0.77372549\n",
      "Iteration 95, loss = 0.08979763\n",
      "Iteration 4, loss = 0.66532509\n",
      "Iteration 96, loss = 0.08878163\n",
      "Iteration 5, loss = 0.59155812\n",
      "Iteration 97, loss = 0.08621632\n",
      "Iteration 6, loss = 0.54001262\n",
      "Iteration 98, loss = 0.08585972\n",
      "Iteration 7, loss = 0.50064318\n",
      "Iteration 99, loss = 0.08492385\n",
      "Iteration 8, loss = 0.47169021\n",
      "Iteration 100, loss = 0.08573691\n",
      "Iteration 9, loss = 0.44677390\n",
      "Iteration 10, loss = 0.42634844\n",
      "Iteration 11, loss = 0.41028194\n",
      "Iteration 12, loss = 0.39568758\n",
      "Iteration 13, loss = 0.38238120\n",
      "Iteration 14, loss = 0.37218067\n",
      "Iteration 15, loss = 0.36133672\n",
      "Iteration 16, loss = 0.35112323\n",
      "Iteration 17, loss = 0.34263043\n",
      "Iteration 18, loss = 0.33485494\n",
      "Iteration 19, loss = 0.32710016\n",
      "Iteration 20, loss = 0.32169197\n",
      "Iteration 21, loss = 0.31255295\n",
      "Iteration 1, loss = 1.12214594\n",
      "Iteration 22, loss = 0.30764924\n",
      "Iteration 2, loss = 0.89413304\n",
      "Iteration 23, loss = 0.29989407\n",
      "Iteration 3, loss = 0.74321381\n",
      "Iteration 24, loss = 0.29548579\n",
      "Iteration 4, loss = 0.65204309\n",
      "Iteration 25, loss = 0.28871604\n",
      "Iteration 5, loss = 0.58662239\n",
      "Iteration 26, loss = 0.28279028\n",
      "Iteration 6, loss = 0.53870527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.27826113\n",
      "Iteration 7, loss = 0.50143902\n",
      "Iteration 28, loss = 0.27266486\n",
      "Iteration 8, loss = 0.47287938\n",
      "Iteration 29, loss = 0.26654271\n",
      "Iteration 9, loss = 0.45136183\n",
      "Iteration 30, loss = 0.26226887\n",
      "Iteration 10, loss = 0.43201452\n",
      "Iteration 31, loss = 0.25625901\n",
      "Iteration 11, loss = 0.41540094\n",
      "Iteration 32, loss = 0.25302617\n",
      "Iteration 12, loss = 0.40166402\n",
      "Iteration 33, loss = 0.24668590\n",
      "Iteration 13, loss = 0.38754154\n",
      "Iteration 34, loss = 0.24399262\n",
      "Iteration 14, loss = 0.37632239\n",
      "Iteration 35, loss = 0.24181447\n",
      "Iteration 15, loss = 0.36631315\n",
      "Iteration 36, loss = 0.23464101\n",
      "Iteration 16, loss = 0.35617146\n",
      "Iteration 37, loss = 0.23077721\n",
      "Iteration 17, loss = 0.34622348\n",
      "Iteration 38, loss = 0.22750728\n",
      "Iteration 18, loss = 0.33828131\n",
      "Iteration 39, loss = 0.22061426\n",
      "Iteration 19, loss = 0.33215400\n",
      "Iteration 40, loss = 0.21880265\n",
      "Iteration 20, loss = 0.32214336\n",
      "Iteration 41, loss = 0.21578374\n",
      "Iteration 21, loss = 0.31519628\n",
      "Iteration 42, loss = 0.21041016\n",
      "Iteration 22, loss = 0.30958370\n",
      "Iteration 43, loss = 0.20721492\n",
      "Iteration 23, loss = 0.30088632\n",
      "Iteration 44, loss = 0.20387870\n",
      "Iteration 24, loss = 0.29576033\n",
      "Iteration 45, loss = 0.20103438\n",
      "Iteration 25, loss = 0.29123961\n",
      "Iteration 46, loss = 0.19679183\n",
      "Iteration 26, loss = 0.28282465\n",
      "Iteration 47, loss = 0.19357286\n",
      "Iteration 27, loss = 0.27522378\n",
      "Iteration 48, loss = 0.18996134\n",
      "Iteration 28, loss = 0.26923503\n",
      "Iteration 49, loss = 0.18682369\n",
      "Iteration 29, loss = 0.26415377\n",
      "Iteration 50, loss = 0.18445129\n",
      "Iteration 30, loss = 0.25943011\n",
      "Iteration 51, loss = 0.18017441\n",
      "Iteration 31, loss = 0.25370213\n",
      "Iteration 52, loss = 0.17911015\n",
      "Iteration 32, loss = 0.24810630\n",
      "Iteration 53, loss = 0.17466236\n",
      "Iteration 33, loss = 0.24149249\n",
      "Iteration 54, loss = 0.17299450\n",
      "Iteration 34, loss = 0.23761212\n",
      "Iteration 55, loss = 0.17288308\n",
      "Iteration 35, loss = 0.23359131\n",
      "Iteration 56, loss = 0.16806703\n",
      "Iteration 36, loss = 0.22820995\n",
      "Iteration 57, loss = 0.16536487\n",
      "Iteration 37, loss = 0.22326420\n",
      "Iteration 58, loss = 0.16339387\n",
      "Iteration 38, loss = 0.22019227\n",
      "Iteration 59, loss = 0.15918660\n",
      "Iteration 39, loss = 0.21495812\n",
      "Iteration 60, loss = 0.15916633\n",
      "Iteration 40, loss = 0.21106519\n",
      "Iteration 61, loss = 0.15508626\n",
      "Iteration 41, loss = 0.20711984\n",
      "Iteration 62, loss = 0.15372702\n",
      "Iteration 42, loss = 0.20341468\n",
      "Iteration 63, loss = 0.15131991\n",
      "Iteration 43, loss = 0.19947496\n",
      "Iteration 64, loss = 0.14835596\n",
      "Iteration 44, loss = 0.19616429\n",
      "Iteration 65, loss = 0.14598217\n",
      "Iteration 45, loss = 0.19345880\n",
      "Iteration 66, loss = 0.14420274\n",
      "Iteration 46, loss = 0.18953745\n",
      "Iteration 67, loss = 0.14208135\n",
      "Iteration 47, loss = 0.18780743\n",
      "Iteration 68, loss = 0.14059935\n",
      "Iteration 48, loss = 0.18280646\n",
      "Iteration 69, loss = 0.13791468\n",
      "Iteration 49, loss = 0.18085476\n",
      "Iteration 70, loss = 0.13682591\n",
      "Iteration 50, loss = 0.17726511\n",
      "Iteration 71, loss = 0.13718780\n",
      "Iteration 51, loss = 0.17404397\n",
      "Iteration 72, loss = 0.13480552\n",
      "Iteration 52, loss = 0.16957251\n",
      "Iteration 73, loss = 0.13261316\n",
      "Iteration 53, loss = 0.16734916\n",
      "Iteration 74, loss = 0.13031728\n",
      "Iteration 54, loss = 0.16432822\n",
      "Iteration 75, loss = 0.12731268\n",
      "Iteration 55, loss = 0.16214077\n",
      "Iteration 76, loss = 0.12597508\n",
      "Iteration 56, loss = 0.16059048\n",
      "Iteration 77, loss = 0.12443356\n",
      "Iteration 57, loss = 0.15643702\n",
      "Iteration 78, loss = 0.12223962\n",
      "Iteration 58, loss = 0.15402879\n",
      "Iteration 79, loss = 0.12221997\n",
      "Iteration 59, loss = 0.15116719\n",
      "Iteration 80, loss = 0.12136054\n",
      "Iteration 60, loss = 0.14864330\n",
      "Iteration 81, loss = 0.11984066\n",
      "Iteration 61, loss = 0.14793087\n",
      "Iteration 82, loss = 0.11929252\n",
      "Iteration 62, loss = 0.14363230\n",
      "Iteration 83, loss = 0.11659406\n",
      "Iteration 63, loss = 0.14301570\n",
      "Iteration 84, loss = 0.11474423\n",
      "Iteration 64, loss = 0.14112902\n",
      "Iteration 85, loss = 0.11343436\n",
      "Iteration 65, loss = 0.13826827\n",
      "Iteration 86, loss = 0.11105244\n",
      "Iteration 66, loss = 0.13733465\n",
      "Iteration 87, loss = 0.11013816\n",
      "Iteration 67, loss = 0.13447328\n",
      "Iteration 88, loss = 0.10695575\n",
      "Iteration 68, loss = 0.13286394\n",
      "Iteration 89, loss = 0.10739049\n",
      "Iteration 69, loss = 0.12896327\n",
      "Iteration 90, loss = 0.10568976\n",
      "Iteration 70, loss = 0.12780922\n",
      "Iteration 91, loss = 0.10627682\n",
      "Iteration 71, loss = 0.12572314\n",
      "Iteration 92, loss = 0.10193615\n",
      "Iteration 72, loss = 0.12586292\n",
      "Iteration 93, loss = 0.10094223\n",
      "Iteration 73, loss = 0.12357356\n",
      "Iteration 94, loss = 0.10160152\n",
      "Iteration 74, loss = 0.12075352\n",
      "Iteration 95, loss = 0.10129187\n",
      "Iteration 75, loss = 0.12076243\n",
      "Iteration 96, loss = 0.09967970\n",
      "Iteration 76, loss = 0.11719470\n",
      "Iteration 97, loss = 0.09568905\n",
      "Iteration 77, loss = 0.11517410\n",
      "Iteration 98, loss = 0.09791176\n",
      "Iteration 78, loss = 0.11378346\n",
      "Iteration 99, loss = 0.09499895\n",
      "Iteration 79, loss = 0.11161607\n",
      "Iteration 100, loss = 0.09732035\n",
      "Iteration 80, loss = 0.11012512\n",
      "Iteration 81, loss = 0.10720455\n",
      "Iteration 82, loss = 0.10572336\n",
      "Iteration 83, loss = 0.10496749\n",
      "Iteration 84, loss = 0.10267523\n",
      "Iteration 85, loss = 0.10227966\n",
      "Iteration 86, loss = 0.10119354\n",
      "Iteration 87, loss = 0.09880417\n",
      "Iteration 88, loss = 0.09766464\n",
      "Iteration 89, loss = 0.09676774\n",
      "Iteration 90, loss = 0.09952868\n",
      "Iteration 91, loss = 0.09725417\n",
      "Iteration 92, loss = 0.09640760\n",
      "Iteration 1, loss = 1.28897798\n",
      "Iteration 93, loss = 0.09766381\n",
      "Iteration 2, loss = 0.96824890\n",
      "Iteration 94, loss = 0.09530210\n",
      "Iteration 3, loss = 0.78051851\n",
      "Iteration 95, loss = 0.09093198\n",
      "Iteration 4, loss = 0.67016390\n",
      "Iteration 96, loss = 0.08763994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.59664373\n",
      "Iteration 97, loss = 0.09034375\n",
      "Iteration 6, loss = 0.54290324\n",
      "Iteration 98, loss = 0.09039817\n",
      "Iteration 7, loss = 0.50451313\n",
      "Iteration 99, loss = 0.08352519\n",
      "Iteration 8, loss = 0.47275587\n",
      "Iteration 100, loss = 0.07976185\n",
      "Iteration 9, loss = 0.44901582\n",
      "Iteration 10, loss = 0.42808234\n",
      "Iteration 11, loss = 0.41188632\n",
      "Iteration 12, loss = 0.39620153\n",
      "Iteration 13, loss = 0.38249381\n",
      "Iteration 14, loss = 0.37019950\n",
      "Iteration 15, loss = 0.36003890\n",
      "Iteration 16, loss = 0.35020262\n",
      "Iteration 17, loss = 0.34496061\n",
      "Iteration 18, loss = 0.33239390\n",
      "Iteration 19, loss = 0.32552071\n",
      "Iteration 20, loss = 0.31572863\n",
      "Iteration 21, loss = 0.31004976\n",
      "Iteration 1, loss = 1.21802032\n",
      "Iteration 22, loss = 0.30214814\n",
      "Iteration 2, loss = 0.90976817\n",
      "Iteration 23, loss = 0.29672410\n",
      "Iteration 3, loss = 0.75965896\n",
      "Iteration 24, loss = 0.28890453\n",
      "Iteration 4, loss = 0.65183629\n",
      "Iteration 25, loss = 0.28348628\n",
      "Iteration 5, loss = 0.58426508\n",
      "Iteration 26, loss = 0.27874324\n",
      "Iteration 6, loss = 0.53356095\n",
      "Iteration 27, loss = 0.27253928\n",
      "Iteration 7, loss = 0.49225835\n",
      "Iteration 28, loss = 0.26752053\n",
      "Iteration 8, loss = 0.46324556\n",
      "Iteration 29, loss = 0.26152499\n",
      "Iteration 9, loss = 0.43928544\n",
      "Iteration 30, loss = 0.25694109\n",
      "Iteration 10, loss = 0.41841029\n",
      "Iteration 31, loss = 0.25360535\n",
      "Iteration 11, loss = 0.40100704\n",
      "Iteration 32, loss = 0.24752665\n",
      "Iteration 12, loss = 0.38799265\n",
      "Iteration 33, loss = 0.24383951\n",
      "Iteration 13, loss = 0.37541120\n",
      "Iteration 34, loss = 0.24083096\n",
      "Iteration 14, loss = 0.36527885\n",
      "Iteration 35, loss = 0.23464094\n",
      "Iteration 15, loss = 0.35581841\n",
      "Iteration 36, loss = 0.23271017\n",
      "Iteration 16, loss = 0.34465244\n",
      "Iteration 37, loss = 0.22719062\n",
      "Iteration 17, loss = 0.33955556\n",
      "Iteration 38, loss = 0.22441777\n",
      "Iteration 18, loss = 0.32917847\n",
      "Iteration 39, loss = 0.22201485\n",
      "Iteration 19, loss = 0.32380993\n",
      "Iteration 40, loss = 0.22232495\n",
      "Iteration 20, loss = 0.31486642\n",
      "Iteration 41, loss = 0.21538977\n",
      "Iteration 21, loss = 0.30782774\n",
      "Iteration 42, loss = 0.21174502\n",
      "Iteration 22, loss = 0.30048593\n",
      "Iteration 23, loss = 0.29424551\n",
      "Iteration 43, loss = 0.20818668\n",
      "Iteration 24, loss = 0.28730228\n",
      "Iteration 44, loss = 0.20665437\n",
      "Iteration 45, loss = 0.20294502\n",
      "Iteration 25, loss = 0.28219873\n",
      "Iteration 46, loss = 0.19807538\n",
      "Iteration 26, loss = 0.27660972\n",
      "Iteration 47, loss = 0.19538912\n",
      "Iteration 27, loss = 0.27001915\n",
      "Iteration 48, loss = 0.19244584\n",
      "Iteration 28, loss = 0.26585629\n",
      "Iteration 49, loss = 0.18807590\n",
      "Iteration 29, loss = 0.25904958\n",
      "Iteration 30, loss = 0.25454146\n",
      "Iteration 50, loss = 0.18917478\n",
      "Iteration 31, loss = 0.24935355\n",
      "Iteration 51, loss = 0.18437028\n",
      "Iteration 32, loss = 0.24446244\n",
      "Iteration 52, loss = 0.18447813\n",
      "Iteration 33, loss = 0.23909579\n",
      "Iteration 53, loss = 0.17799955\n",
      "Iteration 34, loss = 0.23399057\n",
      "Iteration 54, loss = 0.17763051\n",
      "Iteration 35, loss = 0.22946650\n",
      "Iteration 55, loss = 0.17357269\n",
      "Iteration 36, loss = 0.22511793\n",
      "Iteration 56, loss = 0.17146579\n",
      "Iteration 37, loss = 0.22004670\n",
      "Iteration 57, loss = 0.16768401\n",
      "Iteration 38, loss = 0.21615109\n",
      "Iteration 58, loss = 0.16679994\n",
      "Iteration 39, loss = 0.21446415\n",
      "Iteration 59, loss = 0.16606276\n",
      "Iteration 40, loss = 0.20835409\n",
      "Iteration 60, loss = 0.16439369\n",
      "Iteration 41, loss = 0.20477695\n",
      "Iteration 61, loss = 0.16037238\n",
      "Iteration 42, loss = 0.20530725\n",
      "Iteration 62, loss = 0.15731999\n",
      "Iteration 43, loss = 0.19662704\n",
      "Iteration 63, loss = 0.15615549\n",
      "Iteration 44, loss = 0.19555597\n",
      "Iteration 64, loss = 0.15264781\n",
      "Iteration 45, loss = 0.18969300\n",
      "Iteration 65, loss = 0.15149189\n",
      "Iteration 66, loss = 0.15018958\n",
      "Iteration 46, loss = 0.18775430\n",
      "Iteration 67, loss = 0.14852765\n",
      "Iteration 47, loss = 0.18311041\n",
      "Iteration 68, loss = 0.14697610\n",
      "Iteration 48, loss = 0.18111937\n",
      "Iteration 69, loss = 0.14492585\n",
      "Iteration 49, loss = 0.17798191\n",
      "Iteration 70, loss = 0.14259549\n",
      "Iteration 50, loss = 0.17541084\n",
      "Iteration 51, loss = 0.17286096\n",
      "Iteration 71, loss = 0.14228645\n",
      "Iteration 52, loss = 0.17025215\n",
      "Iteration 72, loss = 0.13941120\n",
      "Iteration 53, loss = 0.16768914\n",
      "Iteration 73, loss = 0.13740048\n",
      "Iteration 54, loss = 0.16538868\n",
      "Iteration 74, loss = 0.13618691\n",
      "Iteration 55, loss = 0.16215106\n",
      "Iteration 75, loss = 0.13617961\n",
      "Iteration 56, loss = 0.15992395\n",
      "Iteration 76, loss = 0.13649422\n",
      "Iteration 57, loss = 0.15586851\n",
      "Iteration 77, loss = 0.13287643\n",
      "Iteration 58, loss = 0.15625017\n",
      "Iteration 78, loss = 0.13192564\n",
      "Iteration 59, loss = 0.15241500\n",
      "Iteration 79, loss = 0.13030297\n",
      "Iteration 60, loss = 0.15442042\n",
      "Iteration 80, loss = 0.12925470\n",
      "Iteration 61, loss = 0.14798375\n",
      "Iteration 81, loss = 0.13046434\n",
      "Iteration 62, loss = 0.14537942\n",
      "Iteration 82, loss = 0.12861181\n",
      "Iteration 63, loss = 0.14436901\n",
      "Iteration 83, loss = 0.12591749\n",
      "Iteration 64, loss = 0.14203714\n",
      "Iteration 84, loss = 0.12417907\n",
      "Iteration 65, loss = 0.13982893\n",
      "Iteration 85, loss = 0.12235618\n",
      "Iteration 66, loss = 0.13935003\n",
      "Iteration 86, loss = 0.12069922\n",
      "Iteration 67, loss = 0.13615861\n",
      "Iteration 87, loss = 0.12013204\n",
      "Iteration 68, loss = 0.13839700\n",
      "Iteration 88, loss = 0.11896899\n",
      "Iteration 69, loss = 0.13252804\n",
      "Iteration 89, loss = 0.11913891\n",
      "Iteration 70, loss = 0.13029035\n",
      "Iteration 90, loss = 0.11800551\n",
      "Iteration 71, loss = 0.12910320\n",
      "Iteration 91, loss = 0.11462312\n",
      "Iteration 72, loss = 0.12612728\n",
      "Iteration 92, loss = 0.11515160\n",
      "Iteration 73, loss = 0.12497413\n",
      "Iteration 93, loss = 0.11403879\n",
      "Iteration 74, loss = 0.12372250\n",
      "Iteration 94, loss = 0.11173273\n",
      "Iteration 75, loss = 0.12192536\n",
      "Iteration 95, loss = 0.11158378\n",
      "Iteration 76, loss = 0.11976147\n",
      "Iteration 96, loss = 0.11057716\n",
      "Iteration 77, loss = 0.11978141\n",
      "Iteration 97, loss = 0.11199390\n",
      "Iteration 78, loss = 0.11868393\n",
      "Iteration 98, loss = 0.11028393\n",
      "Iteration 79, loss = 0.11571472\n",
      "Iteration 99, loss = 0.10725331\n",
      "Iteration 80, loss = 0.11499014\n",
      "Iteration 100, loss = 0.10678871\n",
      "Iteration 81, loss = 0.11258199\n",
      "Iteration 82, loss = 0.11239782\n",
      "Iteration 83, loss = 0.11033256\n",
      "Iteration 84, loss = 0.11075366\n",
      "Iteration 85, loss = 0.10748549\n",
      "Iteration 86, loss = 0.10819839\n",
      "Iteration 87, loss = 0.10962059\n",
      "Iteration 88, loss = 0.10487402\n",
      "Iteration 89, loss = 0.10359300\n",
      "Iteration 90, loss = 0.10411750\n",
      "Iteration 91, loss = 0.10314697\n",
      "Iteration 92, loss = 0.10022653\n",
      "Iteration 93, loss = 0.09875771\n",
      "Iteration 1, loss = 1.10890573\n",
      "Iteration 94, loss = 0.09688292\n",
      "Iteration 2, loss = 0.80343234\n",
      "Iteration 95, loss = 0.09930998\n",
      "Iteration 3, loss = 0.64713148\n",
      "Iteration 96, loss = 0.09974926\n",
      "Iteration 4, loss = 0.56154470\n",
      "Iteration 97, loss = 0.09370793\n",
      "Iteration 5, loss = 0.50708404\n",
      "Iteration 98, loss = 0.09465774\n",
      "Iteration 6, loss = 0.47055032\n",
      "Iteration 99, loss = 0.09166402\n",
      "Iteration 7, loss = 0.44365527\n",
      "Iteration 100, loss = 0.09118332\n",
      "Iteration 8, loss = 0.42176442\n",
      "Iteration 9, loss = 0.40538382\n",
      "Iteration 10, loss = 0.39162773\n",
      "Iteration 11, loss = 0.37761307\n",
      "Iteration 12, loss = 0.36716313\n",
      "Iteration 13, loss = 0.35751627\n",
      "Iteration 14, loss = 0.35035480\n",
      "Iteration 15, loss = 0.34015562\n",
      "Iteration 16, loss = 0.33374118\n",
      "Iteration 17, loss = 0.32536191\n",
      "Iteration 18, loss = 0.31753143\n",
      "Iteration 19, loss = 0.31031428\n",
      "Iteration 20, loss = 0.30426635\n",
      "Iteration 1, loss = 1.21115288\n",
      "Iteration 21, loss = 0.29777267\n",
      "Iteration 2, loss = 0.90469462\n",
      "Iteration 22, loss = 0.29227511\n",
      "Iteration 3, loss = 0.74231724\n",
      "Iteration 23, loss = 0.28472389\n",
      "Iteration 4, loss = 0.63072742\n",
      "Iteration 24, loss = 0.27893687\n",
      "Iteration 5, loss = 0.56019359\n",
      "Iteration 25, loss = 0.27284933\n",
      "Iteration 6, loss = 0.51161051\n",
      "Iteration 26, loss = 0.26874681\n",
      "Iteration 7, loss = 0.47514937\n",
      "Iteration 27, loss = 0.26248166\n",
      "Iteration 8, loss = 0.44545566\n",
      "Iteration 28, loss = 0.25888580\n",
      "Iteration 9, loss = 0.42131273\n",
      "Iteration 29, loss = 0.25172923\n",
      "Iteration 10, loss = 0.40290849\n",
      "Iteration 30, loss = 0.24698518\n",
      "Iteration 11, loss = 0.38580388\n",
      "Iteration 31, loss = 0.24260366\n",
      "Iteration 12, loss = 0.37332000\n",
      "Iteration 32, loss = 0.23700765\n",
      "Iteration 13, loss = 0.36106808\n",
      "Iteration 33, loss = 0.23308460\n",
      "Iteration 14, loss = 0.34940170\n",
      "Iteration 34, loss = 0.22896616\n",
      "Iteration 15, loss = 0.34068621\n",
      "Iteration 35, loss = 0.22442792\n",
      "Iteration 16, loss = 0.33087018\n",
      "Iteration 36, loss = 0.21950980\n",
      "Iteration 17, loss = 0.32348937\n",
      "Iteration 37, loss = 0.21647872\n",
      "Iteration 18, loss = 0.31791961\n",
      "Iteration 38, loss = 0.21108865\n",
      "Iteration 19, loss = 0.30933011\n",
      "Iteration 39, loss = 0.20764471\n",
      "Iteration 20, loss = 0.30236784\n",
      "Iteration 40, loss = 0.20400541\n",
      "Iteration 21, loss = 0.29504285\n",
      "Iteration 41, loss = 0.20104618\n",
      "Iteration 22, loss = 0.29134924\n",
      "Iteration 42, loss = 0.19714350\n",
      "Iteration 23, loss = 0.28298772\n",
      "Iteration 43, loss = 0.19544742\n",
      "Iteration 24, loss = 0.27786103\n",
      "Iteration 44, loss = 0.18944562\n",
      "Iteration 25, loss = 0.27225114\n",
      "Iteration 45, loss = 0.18844357\n",
      "Iteration 26, loss = 0.26667334\n",
      "Iteration 46, loss = 0.18399602\n",
      "Iteration 27, loss = 0.26260125\n",
      "Iteration 47, loss = 0.18051909\n",
      "Iteration 28, loss = 0.25507888\n",
      "Iteration 48, loss = 0.17736250\n",
      "Iteration 29, loss = 0.25038208\n",
      "Iteration 49, loss = 0.17609813\n",
      "Iteration 30, loss = 0.24495152\n",
      "Iteration 50, loss = 0.17096680\n",
      "Iteration 31, loss = 0.23995161\n",
      "Iteration 51, loss = 0.16864165\n",
      "Iteration 32, loss = 0.23580524\n",
      "Iteration 52, loss = 0.16478868\n",
      "Iteration 33, loss = 0.23059764\n",
      "Iteration 53, loss = 0.16214039\n",
      "Iteration 34, loss = 0.22711057\n",
      "Iteration 54, loss = 0.15968373\n",
      "Iteration 35, loss = 0.22302892\n",
      "Iteration 55, loss = 0.15801756\n",
      "Iteration 36, loss = 0.21913676\n",
      "Iteration 56, loss = 0.15715635\n",
      "Iteration 37, loss = 0.21605694\n",
      "Iteration 57, loss = 0.15257272\n",
      "Iteration 38, loss = 0.21361798\n",
      "Iteration 58, loss = 0.15111611\n",
      "Iteration 39, loss = 0.20819380\n",
      "Iteration 59, loss = 0.14861463\n",
      "Iteration 40, loss = 0.20706852\n",
      "Iteration 60, loss = 0.14870649\n",
      "Iteration 41, loss = 0.20351908\n",
      "Iteration 61, loss = 0.14410059\n",
      "Iteration 42, loss = 0.19833452\n",
      "Iteration 62, loss = 0.14194795\n",
      "Iteration 43, loss = 0.19874818\n",
      "Iteration 63, loss = 0.14246594\n",
      "Iteration 44, loss = 0.19434662\n",
      "Iteration 64, loss = 0.14366070\n",
      "Iteration 45, loss = 0.18983048\n",
      "Iteration 65, loss = 0.14152624\n",
      "Iteration 46, loss = 0.18751008\n",
      "Iteration 66, loss = 0.13868047\n",
      "Iteration 47, loss = 0.18460221\n",
      "Iteration 67, loss = 0.13405428\n",
      "Iteration 48, loss = 0.18365991\n",
      "Iteration 68, loss = 0.13298496\n",
      "Iteration 49, loss = 0.17900029\n",
      "Iteration 69, loss = 0.12953854\n",
      "Iteration 50, loss = 0.17730740\n",
      "Iteration 51, loss = 0.17746051\n",
      "Iteration 70, loss = 0.12780670\n",
      "Iteration 52, loss = 0.17620346\n",
      "Iteration 71, loss = 0.12648388\n",
      "Iteration 53, loss = 0.17120635\n",
      "Iteration 72, loss = 0.12499273\n",
      "Iteration 54, loss = 0.17063583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.12285291\n",
      "Iteration 55, loss = 0.16663633\n",
      "Iteration 74, loss = 0.11989704\n",
      "Iteration 56, loss = 0.16553625\n",
      "Iteration 75, loss = 0.11956344\n",
      "Iteration 57, loss = 0.16557250\n",
      "Iteration 76, loss = 0.11703001\n",
      "Iteration 58, loss = 0.15805814\n",
      "Iteration 77, loss = 0.11534714\n",
      "Iteration 59, loss = 0.16097094\n",
      "Iteration 78, loss = 0.11412490\n",
      "Iteration 60, loss = 0.15729559\n",
      "Iteration 79, loss = 0.11454870\n",
      "Iteration 61, loss = 0.15687020\n",
      "Iteration 80, loss = 0.11348740\n",
      "Iteration 62, loss = 0.15119798\n",
      "Iteration 81, loss = 0.11234744\n",
      "Iteration 63, loss = 0.15325396\n",
      "Iteration 82, loss = 0.10932298\n",
      "Iteration 64, loss = 0.14933469\n",
      "Iteration 83, loss = 0.10831394\n",
      "Iteration 65, loss = 0.14609880\n",
      "Iteration 84, loss = 0.10736254\n",
      "Iteration 66, loss = 0.14511922\n",
      "Iteration 85, loss = 0.10604891\n",
      "Iteration 67, loss = 0.14354271\n",
      "Iteration 86, loss = 0.10644457\n",
      "Iteration 68, loss = 0.14098000\n",
      "Iteration 87, loss = 0.10457690\n",
      "Iteration 69, loss = 0.14250547\n",
      "Iteration 88, loss = 0.10173574\n",
      "Iteration 70, loss = 0.13910467\n",
      "Iteration 89, loss = 0.10311076\n",
      "Iteration 71, loss = 0.13735875\n",
      "Iteration 90, loss = 0.09905970\n",
      "Iteration 72, loss = 0.13579893\n",
      "Iteration 91, loss = 0.09866800\n",
      "Iteration 73, loss = 0.13433019\n",
      "Iteration 92, loss = 0.09896394\n",
      "Iteration 74, loss = 0.13342337\n",
      "Iteration 93, loss = 0.09673888\n",
      "Iteration 75, loss = 0.13121208\n",
      "Iteration 94, loss = 0.09682981\n",
      "Iteration 76, loss = 0.13060347\n",
      "Iteration 95, loss = 0.09435958\n",
      "Iteration 77, loss = 0.12931431\n",
      "Iteration 96, loss = 0.09591898\n",
      "Iteration 78, loss = 0.12821304\n",
      "Iteration 97, loss = 0.09517210\n",
      "Iteration 79, loss = 0.12680607\n",
      "Iteration 98, loss = 0.09163659\n",
      "Iteration 80, loss = 0.12597650\n",
      "Iteration 99, loss = 0.09065996\n",
      "Iteration 81, loss = 0.12587758\n",
      "Iteration 100, loss = 0.08918995\n",
      "Iteration 82, loss = 0.12334499\n",
      "Iteration 101, loss = 0.08931711\n",
      "Iteration 83, loss = 0.12225490\n",
      "Iteration 102, loss = 0.08747301\n",
      "Iteration 84, loss = 0.12060923\n",
      "Iteration 103, loss = 0.08667911\n",
      "Iteration 85, loss = 0.11991929\n",
      "Iteration 104, loss = 0.08594901\n",
      "Iteration 86, loss = 0.11893543\n",
      "Iteration 105, loss = 0.08420004\n",
      "Iteration 87, loss = 0.11726655\n",
      "Iteration 106, loss = 0.08444890\n",
      "Iteration 88, loss = 0.11524567\n",
      "Iteration 107, loss = 0.08095590\n",
      "Iteration 89, loss = 0.11495897\n",
      "Iteration 108, loss = 0.08273561\n",
      "Iteration 90, loss = 0.11431329\n",
      "Iteration 109, loss = 0.07963260\n",
      "Iteration 91, loss = 0.11340691\n",
      "Iteration 110, loss = 0.08390556\n",
      "Iteration 92, loss = 0.11068105\n",
      "Iteration 111, loss = 0.08000760\n",
      "Iteration 93, loss = 0.11230486\n",
      "Iteration 112, loss = 0.07815950\n",
      "Iteration 94, loss = 0.10955469\n",
      "Iteration 113, loss = 0.07895172\n",
      "Iteration 95, loss = 0.10683697\n",
      "Iteration 114, loss = 0.07460716\n",
      "Iteration 96, loss = 0.10637170\n",
      "Iteration 115, loss = 0.07702021\n",
      "Iteration 97, loss = 0.10579031\n",
      "Iteration 116, loss = 0.08040364\n",
      "Iteration 98, loss = 0.10588703\n",
      "Iteration 117, loss = 0.08311281\n",
      "Iteration 99, loss = 0.10843060\n",
      "Iteration 118, loss = 0.07880390\n",
      "Iteration 100, loss = 0.10432378\n",
      "Iteration 119, loss = 0.07862301\n",
      "Iteration 101, loss = 0.10155952\n",
      "Iteration 120, loss = 0.07451164\n",
      "Iteration 102, loss = 0.10150222\n",
      "Iteration 121, loss = 0.07126359\n",
      "Iteration 103, loss = 0.10091632\n",
      "Iteration 122, loss = 0.06922712\n",
      "Iteration 104, loss = 0.09853717\n",
      "Iteration 123, loss = 0.07216550\n",
      "Iteration 105, loss = 0.09849499\n",
      "Iteration 124, loss = 0.07022259\n",
      "Iteration 106, loss = 0.09745674\n",
      "Iteration 125, loss = 0.06703177\n",
      "Iteration 107, loss = 0.09616069\n",
      "Iteration 126, loss = 0.06663908\n",
      "Iteration 108, loss = 0.09773401\n",
      "Iteration 127, loss = 0.06695706\n",
      "Iteration 109, loss = 0.09956177\n",
      "Iteration 128, loss = 0.06730358\n",
      "Iteration 110, loss = 0.09323746\n",
      "Iteration 129, loss = 0.06416870\n",
      "Iteration 111, loss = 0.09340497\n",
      "Iteration 130, loss = 0.06503624\n",
      "Iteration 112, loss = 0.09084986\n",
      "Iteration 131, loss = 0.06410807\n",
      "Iteration 113, loss = 0.08965011\n",
      "Iteration 132, loss = 0.06363219\n",
      "Iteration 114, loss = 0.09164086\n",
      "Iteration 133, loss = 0.06117370\n",
      "Iteration 115, loss = 0.08874485\n",
      "Iteration 134, loss = 0.06278721\n",
      "Iteration 116, loss = 0.08857430\n",
      "Iteration 135, loss = 0.06344485\n",
      "Iteration 117, loss = 0.08839779\n",
      "Iteration 136, loss = 0.06101745\n",
      "Iteration 118, loss = 0.08575437\n",
      "Iteration 137, loss = 0.06032758\n",
      "Iteration 119, loss = 0.08663156\n",
      "Iteration 138, loss = 0.05999031\n",
      "Iteration 120, loss = 0.08621001\n",
      "Iteration 139, loss = 0.05967821\n",
      "Iteration 121, loss = 0.08421688\n",
      "Iteration 140, loss = 0.05781961\n",
      "Iteration 122, loss = 0.08313313\n",
      "Iteration 141, loss = 0.05886999\n",
      "Iteration 123, loss = 0.08285446\n",
      "Iteration 142, loss = 0.05653862\n",
      "Iteration 124, loss = 0.08140922\n",
      "Iteration 143, loss = 0.05664389\n",
      "Iteration 125, loss = 0.08109613\n",
      "Iteration 144, loss = 0.05595466\n",
      "Iteration 126, loss = 0.07956084\n",
      "Iteration 145, loss = 0.05570921\n",
      "Iteration 127, loss = 0.08048772\n",
      "Iteration 146, loss = 0.05733380\n",
      "Iteration 128, loss = 0.07840823\n",
      "Iteration 147, loss = 0.05853259\n",
      "Iteration 129, loss = 0.07978821\n",
      "Iteration 148, loss = 0.05662402\n",
      "Iteration 130, loss = 0.07683594\n",
      "Iteration 149, loss = 0.05518769\n",
      "Iteration 131, loss = 0.07713667\n",
      "Iteration 150, loss = 0.05250034\n",
      "Iteration 132, loss = 0.07564898\n",
      "Iteration 151, loss = 0.05502100\n",
      "Iteration 133, loss = 0.07677049\n",
      "Iteration 152, loss = 0.05242450\n",
      "Iteration 134, loss = 0.07524485\n",
      "Iteration 153, loss = 0.05258803\n",
      "Iteration 135, loss = 0.07542800\n",
      "Iteration 154, loss = 0.05204489\n",
      "Iteration 136, loss = 0.07540010\n",
      "Iteration 155, loss = 0.04948760\n",
      "Iteration 137, loss = 0.07322497\n",
      "Iteration 156, loss = 0.05088342\n",
      "Iteration 138, loss = 0.07334973\n",
      "Iteration 157, loss = 0.04996361\n",
      "Iteration 139, loss = 0.07182567\n",
      "Iteration 158, loss = 0.05065132\n",
      "Iteration 140, loss = 0.07204399\n",
      "Iteration 159, loss = 0.05007652\n",
      "Iteration 141, loss = 0.06997202\n",
      "Iteration 160, loss = 0.04849399\n",
      "Iteration 142, loss = 0.06972874\n",
      "Iteration 161, loss = 0.04797361\n",
      "Iteration 143, loss = 0.07104890\n",
      "Iteration 162, loss = 0.04786299\n",
      "Iteration 144, loss = 0.06865970\n",
      "Iteration 145, loss = 0.06916733\n",
      "Iteration 163, loss = 0.04702727\n",
      "Iteration 146, loss = 0.06677762\n",
      "Iteration 164, loss = 0.04768927\n",
      "Iteration 147, loss = 0.06638952\n",
      "Iteration 165, loss = 0.04638370\n",
      "Iteration 148, loss = 0.06981784\n",
      "Iteration 166, loss = 0.04580898\n",
      "Iteration 149, loss = 0.06576028\n",
      "Iteration 167, loss = 0.04544386\n",
      "Iteration 150, loss = 0.06491410\n",
      "Iteration 168, loss = 0.04538322\n",
      "Iteration 151, loss = 0.06478407\n",
      "Iteration 169, loss = 0.04541209\n",
      "Iteration 152, loss = 0.06476121\n",
      "Iteration 170, loss = 0.04423965\n",
      "Iteration 153, loss = 0.06333406\n",
      "Iteration 171, loss = 0.04425346\n",
      "Iteration 154, loss = 0.06423548\n",
      "Iteration 172, loss = 0.04754863\n",
      "Iteration 155, loss = 0.06325420\n",
      "Iteration 173, loss = 0.04991253\n",
      "Iteration 156, loss = 0.06163194\n",
      "Iteration 174, loss = 0.05087165\n",
      "Iteration 157, loss = 0.06077506\n",
      "Iteration 175, loss = 0.04672051\n",
      "Iteration 158, loss = 0.06058002\n",
      "Iteration 176, loss = 0.04693141\n",
      "Iteration 159, loss = 0.06038829\n",
      "Iteration 177, loss = 0.04461843\n",
      "Iteration 160, loss = 0.06066419\n",
      "Iteration 178, loss = 0.04254788\n",
      "Iteration 161, loss = 0.05823767\n",
      "Iteration 179, loss = 0.04227563\n",
      "Iteration 162, loss = 0.05880746\n",
      "Iteration 180, loss = 0.04376658\n",
      "Iteration 163, loss = 0.05777186\n",
      "Iteration 181, loss = 0.04625925\n",
      "Iteration 164, loss = 0.05809924\n",
      "Iteration 182, loss = 0.04021753\n",
      "Iteration 165, loss = 0.05659515\n",
      "Iteration 183, loss = 0.04053567\n",
      "Iteration 166, loss = 0.05809073\n",
      "Iteration 184, loss = 0.04032362\n",
      "Iteration 167, loss = 0.05647465\n",
      "Iteration 185, loss = 0.03974247\n",
      "Iteration 168, loss = 0.05585387\n",
      "Iteration 186, loss = 0.03893241\n",
      "Iteration 169, loss = 0.05616192\n",
      "Iteration 187, loss = 0.04053921\n",
      "Iteration 170, loss = 0.05943326\n",
      "Iteration 188, loss = 0.03842338\n",
      "Iteration 171, loss = 0.05626878\n",
      "Iteration 189, loss = 0.03939695\n",
      "Iteration 172, loss = 0.05364643\n",
      "Iteration 190, loss = 0.03887802\n",
      "Iteration 173, loss = 0.05382616\n",
      "Iteration 191, loss = 0.03870069\n",
      "Iteration 174, loss = 0.05371300\n",
      "Iteration 192, loss = 0.03751811\n",
      "Iteration 175, loss = 0.05509098\n",
      "Iteration 193, loss = 0.03693069\n",
      "Iteration 176, loss = 0.05168250\n",
      "Iteration 194, loss = 0.03746310\n",
      "Iteration 177, loss = 0.05183093\n",
      "Iteration 195, loss = 0.03681031\n",
      "Iteration 178, loss = 0.05214759\n",
      "Iteration 196, loss = 0.03948337\n",
      "Iteration 179, loss = 0.05397886\n",
      "Iteration 197, loss = 0.04968563\n",
      "Iteration 180, loss = 0.05099397\n",
      "Iteration 198, loss = 0.04777395\n",
      "Iteration 181, loss = 0.04902068\n",
      "Iteration 199, loss = 0.04044123\n",
      "Iteration 182, loss = 0.04946097\n",
      "Iteration 200, loss = 0.03603358\n",
      "Iteration 183, loss = 0.04851661\n",
      "Iteration 184, loss = 0.04938598\n",
      "Iteration 185, loss = 0.05430924\n",
      "Iteration 186, loss = 0.05132758\n",
      "Iteration 187, loss = 0.04862597\n",
      "Iteration 188, loss = 0.04710470\n",
      "Iteration 189, loss = 0.04641340\n",
      "Iteration 190, loss = 0.04731904\n",
      "Iteration 191, loss = 0.04762836\n",
      "Iteration 192, loss = 0.05297387\n",
      "Iteration 193, loss = 0.04749475\n",
      "Iteration 194, loss = 0.04927747\n",
      "Iteration 195, loss = 0.04420294\n",
      "Iteration 1, loss = 1.41789291\n",
      "Iteration 196, loss = 0.04608465\n",
      "Iteration 2, loss = 0.95233800\n",
      "Iteration 197, loss = 0.04444671\n",
      "Iteration 3, loss = 0.80018898\n",
      "Iteration 198, loss = 0.04764072\n",
      "Iteration 4, loss = 0.65310970\n",
      "Iteration 199, loss = 0.04558357\n",
      "Iteration 5, loss = 0.57898925\n",
      "Iteration 200, loss = 0.04358612\n",
      "Iteration 6, loss = 0.52434110\n",
      "Iteration 7, loss = 0.48138094\n",
      "Iteration 8, loss = 0.45192995\n",
      "Iteration 9, loss = 0.42913466\n",
      "Iteration 10, loss = 0.41116751\n",
      "Iteration 11, loss = 0.39406575\n",
      "Iteration 12, loss = 0.38013714\n",
      "Iteration 13, loss = 0.36815196\n",
      "Iteration 14, loss = 0.35735517\n",
      "Iteration 15, loss = 0.34782646\n",
      "Iteration 16, loss = 0.33824398\n",
      "Iteration 17, loss = 0.32989463\n",
      "Iteration 18, loss = 0.32240652\n",
      "Iteration 19, loss = 0.31705244\n",
      "Iteration 1, loss = 1.43489374\n",
      "Iteration 20, loss = 0.30788418\n",
      "Iteration 2, loss = 1.03674308\n",
      "Iteration 21, loss = 0.30235643\n",
      "Iteration 3, loss = 0.80572103\n",
      "Iteration 22, loss = 0.29698686\n",
      "Iteration 4, loss = 0.67331661\n",
      "Iteration 23, loss = 0.28979240\n",
      "Iteration 5, loss = 0.59306131\n",
      "Iteration 24, loss = 0.28387114\n",
      "Iteration 6, loss = 0.54151263\n",
      "Iteration 7, loss = 0.50100209\n",
      "Iteration 25, loss = 0.27825081\n",
      "Iteration 8, loss = 0.47192663\n",
      "Iteration 26, loss = 0.27326886\n",
      "Iteration 9, loss = 0.44821692\n",
      "Iteration 27, loss = 0.26807291\n",
      "Iteration 10, loss = 0.42864062\n",
      "Iteration 28, loss = 0.26410523\n",
      "Iteration 11, loss = 0.41235660\n",
      "Iteration 29, loss = 0.25866367\n",
      "Iteration 12, loss = 0.39879943\n",
      "Iteration 30, loss = 0.25477962\n",
      "Iteration 13, loss = 0.38839938\n",
      "Iteration 31, loss = 0.25015146\n",
      "Iteration 14, loss = 0.37845661\n",
      "Iteration 32, loss = 0.24600412\n",
      "Iteration 15, loss = 0.36740526\n",
      "Iteration 33, loss = 0.24280218\n",
      "Iteration 16, loss = 0.35653252\n",
      "Iteration 34, loss = 0.23855835\n",
      "Iteration 17, loss = 0.34807684\n",
      "Iteration 35, loss = 0.23420407\n",
      "Iteration 18, loss = 0.33853090\n",
      "Iteration 36, loss = 0.23094420\n",
      "Iteration 19, loss = 0.33170210\n",
      "Iteration 37, loss = 0.22678901\n",
      "Iteration 20, loss = 0.32440770\n",
      "Iteration 38, loss = 0.22386242\n",
      "Iteration 21, loss = 0.31749177\n",
      "Iteration 39, loss = 0.22039680\n",
      "Iteration 22, loss = 0.30913531\n",
      "Iteration 40, loss = 0.21732770\n",
      "Iteration 23, loss = 0.30399098\n",
      "Iteration 41, loss = 0.21414015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.29780505\n",
      "Iteration 42, loss = 0.21109336\n",
      "Iteration 25, loss = 0.29137836\n",
      "Iteration 43, loss = 0.20851483\n",
      "Iteration 26, loss = 0.28833663\n",
      "Iteration 44, loss = 0.20433249\n",
      "Iteration 27, loss = 0.27976927\n",
      "Iteration 45, loss = 0.20152465\n",
      "Iteration 28, loss = 0.27670524\n",
      "Iteration 46, loss = 0.19916025\n",
      "Iteration 29, loss = 0.27060505\n",
      "Iteration 47, loss = 0.19581507\n",
      "Iteration 30, loss = 0.26636130\n",
      "Iteration 48, loss = 0.19333178\n",
      "Iteration 31, loss = 0.26012185\n",
      "Iteration 49, loss = 0.19201636\n",
      "Iteration 32, loss = 0.25494288\n",
      "Iteration 50, loss = 0.18714068\n",
      "Iteration 33, loss = 0.25074459\n",
      "Iteration 51, loss = 0.18644327\n",
      "Iteration 34, loss = 0.24650104\n",
      "Iteration 52, loss = 0.18336702\n",
      "Iteration 35, loss = 0.24211449\n",
      "Iteration 53, loss = 0.18038096\n",
      "Iteration 36, loss = 0.23835869\n",
      "Iteration 54, loss = 0.17800692\n",
      "Iteration 37, loss = 0.23487187\n",
      "Iteration 55, loss = 0.17507456\n",
      "Iteration 38, loss = 0.23041633\n",
      "Iteration 56, loss = 0.17322888\n",
      "Iteration 39, loss = 0.22696124\n",
      "Iteration 57, loss = 0.17057806\n",
      "Iteration 40, loss = 0.22405464\n",
      "Iteration 58, loss = 0.16789739\n",
      "Iteration 41, loss = 0.21949347\n",
      "Iteration 59, loss = 0.16559870\n",
      "Iteration 42, loss = 0.21614972\n",
      "Iteration 60, loss = 0.16368790\n",
      "Iteration 43, loss = 0.21324240\n",
      "Iteration 61, loss = 0.16233758\n",
      "Iteration 44, loss = 0.20996953\n",
      "Iteration 62, loss = 0.15881552\n",
      "Iteration 45, loss = 0.20713327\n",
      "Iteration 63, loss = 0.15929569\n",
      "Iteration 46, loss = 0.20407838\n",
      "Iteration 64, loss = 0.15554699\n",
      "Iteration 47, loss = 0.19929222\n",
      "Iteration 65, loss = 0.15350052\n",
      "Iteration 48, loss = 0.19806034\n",
      "Iteration 66, loss = 0.15376332\n",
      "Iteration 49, loss = 0.19719575\n",
      "Iteration 67, loss = 0.15041353\n",
      "Iteration 50, loss = 0.19204340\n",
      "Iteration 68, loss = 0.14821113\n",
      "Iteration 51, loss = 0.18778378\n",
      "Iteration 69, loss = 0.14936249\n",
      "Iteration 52, loss = 0.18505598\n",
      "Iteration 70, loss = 0.14398950\n",
      "Iteration 53, loss = 0.18243714\n",
      "Iteration 71, loss = 0.14456271\n",
      "Iteration 54, loss = 0.18046767\n",
      "Iteration 72, loss = 0.14287897\n",
      "Iteration 55, loss = 0.17844676\n",
      "Iteration 73, loss = 0.14052834\n",
      "Iteration 56, loss = 0.17442734\n",
      "Iteration 74, loss = 0.13796862\n",
      "Iteration 57, loss = 0.17258689\n",
      "Iteration 75, loss = 0.13550774\n",
      "Iteration 58, loss = 0.17043548\n",
      "Iteration 76, loss = 0.13493559\n",
      "Iteration 59, loss = 0.16731505\n",
      "Iteration 77, loss = 0.13338510\n",
      "Iteration 60, loss = 0.16846407\n",
      "Iteration 78, loss = 0.13042379\n",
      "Iteration 61, loss = 0.16381119\n",
      "Iteration 79, loss = 0.13001338\n",
      "Iteration 62, loss = 0.16099734\n",
      "Iteration 80, loss = 0.12771982\n",
      "Iteration 63, loss = 0.16030566\n",
      "Iteration 81, loss = 0.12613582\n",
      "Iteration 64, loss = 0.15630354\n",
      "Iteration 82, loss = 0.12498600\n",
      "Iteration 65, loss = 0.15431095\n",
      "Iteration 83, loss = 0.12385354\n",
      "Iteration 66, loss = 0.15202206\n",
      "Iteration 84, loss = 0.12223740\n",
      "Iteration 67, loss = 0.15048572\n",
      "Iteration 85, loss = 0.12018443\n",
      "Iteration 68, loss = 0.14876771\n",
      "Iteration 86, loss = 0.11892941\n",
      "Iteration 69, loss = 0.14722350\n",
      "Iteration 87, loss = 0.11869599\n",
      "Iteration 70, loss = 0.14428256\n",
      "Iteration 88, loss = 0.11932711\n",
      "Iteration 71, loss = 0.14248937\n",
      "Iteration 89, loss = 0.12111653\n",
      "Iteration 72, loss = 0.14087575\n",
      "Iteration 90, loss = 0.11628440\n",
      "Iteration 73, loss = 0.13985956\n",
      "Iteration 91, loss = 0.11583767\n",
      "Iteration 74, loss = 0.13983920\n",
      "Iteration 92, loss = 0.11208980\n",
      "Iteration 75, loss = 0.13734719\n",
      "Iteration 93, loss = 0.10998465\n",
      "Iteration 76, loss = 0.13511925\n",
      "Iteration 94, loss = 0.10809864\n",
      "Iteration 77, loss = 0.13551346\n",
      "Iteration 95, loss = 0.10607147\n",
      "Iteration 78, loss = 0.13214099\n",
      "Iteration 96, loss = 0.10588657\n",
      "Iteration 79, loss = 0.13077654\n",
      "Iteration 97, loss = 0.10330313\n",
      "Iteration 80, loss = 0.12803702\n",
      "Iteration 98, loss = 0.10328036\n",
      "Iteration 81, loss = 0.12927921\n",
      "Iteration 99, loss = 0.10236793\n",
      "Iteration 82, loss = 0.12529886\n",
      "Iteration 100, loss = 0.10105902\n",
      "Iteration 83, loss = 0.12659694\n",
      "Iteration 101, loss = 0.10280620\n",
      "Iteration 84, loss = 0.12297637\n",
      "Iteration 102, loss = 0.09834064\n",
      "Iteration 85, loss = 0.12048164\n",
      "Iteration 103, loss = 0.09610451\n",
      "Iteration 86, loss = 0.11971287\n",
      "Iteration 104, loss = 0.09626894\n",
      "Iteration 87, loss = 0.12050058\n",
      "Iteration 105, loss = 0.10022836\n",
      "Iteration 88, loss = 0.11836223\n",
      "Iteration 106, loss = 0.09744677\n",
      "Iteration 89, loss = 0.11723443\n",
      "Iteration 107, loss = 0.09378748\n",
      "Iteration 90, loss = 0.11423437\n",
      "Iteration 108, loss = 0.09364791\n",
      "Iteration 91, loss = 0.11417165\n",
      "Iteration 109, loss = 0.09482464\n",
      "Iteration 110, loss = 0.09058248\n",
      "Iteration 92, loss = 0.11125741\n",
      "Iteration 111, loss = 0.09068615Iteration 93, loss = 0.11107354\n",
      "\n",
      "Iteration 112, loss = 0.08788354\n",
      "Iteration 94, loss = 0.10883030\n",
      "Iteration 95, loss = 0.10732016\n",
      "Iteration 113, loss = 0.08484195\n",
      "Iteration 114, loss = 0.08713608\n",
      "Iteration 96, loss = 0.10787835\n",
      "Iteration 115, loss = 0.08496145\n",
      "Iteration 97, loss = 0.10604661\n",
      "Iteration 116, loss = 0.08372162\n",
      "Iteration 98, loss = 0.10348224\n",
      "Iteration 117, loss = 0.08352800\n",
      "Iteration 99, loss = 0.10460425\n",
      "Iteration 118, loss = 0.08144122\n",
      "Iteration 100, loss = 0.10243922\n",
      "Iteration 119, loss = 0.08004497\n",
      "Iteration 101, loss = 0.10080018\n",
      "Iteration 102, loss = 0.10243124\n",
      "Iteration 120, loss = 0.07894838\n",
      "Iteration 103, loss = 0.09954348\n",
      "Iteration 121, loss = 0.07725097\n",
      "Iteration 104, loss = 0.09781380\n",
      "Iteration 122, loss = 0.07733866\n",
      "Iteration 105, loss = 0.09752111\n",
      "Iteration 123, loss = 0.07604210\n",
      "Iteration 106, loss = 0.09589953\n",
      "Iteration 124, loss = 0.07659089\n",
      "Iteration 107, loss = 0.09626201\n",
      "Iteration 125, loss = 0.07437990\n",
      "Iteration 108, loss = 0.09306032\n",
      "Iteration 126, loss = 0.07399432\n",
      "Iteration 109, loss = 0.09304044\n",
      "Iteration 127, loss = 0.07374124\n",
      "Iteration 110, loss = 0.09172608\n",
      "Iteration 128, loss = 0.07145535\n",
      "Iteration 111, loss = 0.09034253\n",
      "Iteration 129, loss = 0.07157415\n",
      "Iteration 112, loss = 0.08922158\n",
      "Iteration 130, loss = 0.07078931\n",
      "Iteration 113, loss = 0.08797982\n",
      "Iteration 131, loss = 0.06845783\n",
      "Iteration 114, loss = 0.08774725\n",
      "Iteration 132, loss = 0.06818109\n",
      "Iteration 115, loss = 0.08667699\n",
      "Iteration 133, loss = 0.06788775\n",
      "Iteration 116, loss = 0.08715400\n",
      "Iteration 134, loss = 0.06614939\n",
      "Iteration 117, loss = 0.08844684\n",
      "Iteration 135, loss = 0.06636842\n",
      "Iteration 118, loss = 0.09049887\n",
      "Iteration 136, loss = 0.06741867\n",
      "Iteration 119, loss = 0.08568680\n",
      "Iteration 137, loss = 0.06555769\n",
      "Iteration 120, loss = 0.08423459\n",
      "Iteration 138, loss = 0.06344636\n",
      "Iteration 121, loss = 0.08342360\n",
      "Iteration 139, loss = 0.06319174\n",
      "Iteration 122, loss = 0.08161969\n",
      "Iteration 140, loss = 0.06345861\n",
      "Iteration 123, loss = 0.08030216\n",
      "Iteration 141, loss = 0.06253881\n",
      "Iteration 124, loss = 0.08118120\n",
      "Iteration 142, loss = 0.06261704\n",
      "Iteration 125, loss = 0.08067988\n",
      "Iteration 143, loss = 0.06103841\n",
      "Iteration 144, loss = 0.06072241\n",
      "Iteration 126, loss = 0.07847836\n",
      "Iteration 145, loss = 0.06177525\n",
      "Iteration 127, loss = 0.07877341\n",
      "Iteration 146, loss = 0.05933437\n",
      "Iteration 128, loss = 0.07652600\n",
      "Iteration 147, loss = 0.05992652\n",
      "Iteration 129, loss = 0.07839787\n",
      "Iteration 148, loss = 0.05834400\n",
      "Iteration 130, loss = 0.07617252\n",
      "Iteration 149, loss = 0.05863890\n",
      "Iteration 131, loss = 0.07386729\n",
      "Iteration 150, loss = 0.05678576\n",
      "Iteration 132, loss = 0.07378315\n",
      "Iteration 151, loss = 0.05710300\n",
      "Iteration 133, loss = 0.07204499\n",
      "Iteration 152, loss = 0.05645340\n",
      "Iteration 134, loss = 0.07226251\n",
      "Iteration 153, loss = 0.05501502\n",
      "Iteration 135, loss = 0.06985910\n",
      "Iteration 154, loss = 0.05412946\n",
      "Iteration 136, loss = 0.07008417\n",
      "Iteration 155, loss = 0.05416121\n",
      "Iteration 137, loss = 0.06989888\n",
      "Iteration 138, loss = 0.07008948\n",
      "Iteration 156, loss = 0.05518424\n",
      "Iteration 139, loss = 0.07060812\n",
      "Iteration 157, loss = 0.05692928\n",
      "Iteration 140, loss = 0.06702909\n",
      "Iteration 158, loss = 0.05370044\n",
      "Iteration 141, loss = 0.07073118\n",
      "Iteration 159, loss = 0.05217539\n",
      "Iteration 160, loss = 0.05264732\n",
      "Iteration 142, loss = 0.07049009\n",
      "Iteration 161, loss = 0.05191650\n",
      "Iteration 143, loss = 0.06941028\n",
      "Iteration 162, loss = 0.05129403\n",
      "Iteration 144, loss = 0.06586118\n",
      "Iteration 163, loss = 0.05523483\n",
      "Iteration 145, loss = 0.07060357\n",
      "Iteration 164, loss = 0.05344187\n",
      "Iteration 146, loss = 0.07288013\n",
      "Iteration 165, loss = 0.05167171\n",
      "Iteration 147, loss = 0.06836636\n",
      "Iteration 166, loss = 0.05154748\n",
      "Iteration 148, loss = 0.06528346\n",
      "Iteration 167, loss = 0.05048061\n",
      "Iteration 149, loss = 0.06389940\n",
      "Iteration 168, loss = 0.04750934\n",
      "Iteration 150, loss = 0.06070297\n",
      "Iteration 169, loss = 0.05167417\n",
      "Iteration 151, loss = 0.06236005\n",
      "Iteration 170, loss = 0.04962448\n",
      "Iteration 152, loss = 0.06093037\n",
      "Iteration 171, loss = 0.04696329\n",
      "Iteration 153, loss = 0.05944022\n",
      "Iteration 172, loss = 0.04757835\n",
      "Iteration 154, loss = 0.05875528\n",
      "Iteration 173, loss = 0.04634469\n",
      "Iteration 155, loss = 0.05968887\n",
      "Iteration 174, loss = 0.04573804\n",
      "Iteration 156, loss = 0.05803670\n",
      "Iteration 175, loss = 0.04568668\n",
      "Iteration 157, loss = 0.05811358\n",
      "Iteration 176, loss = 0.04906464\n",
      "Iteration 158, loss = 0.05962966\n",
      "Iteration 159, loss = 0.05788890Iteration 177, loss = 0.04823495\n",
      "\n",
      "Iteration 160, loss = 0.05673946\n",
      "Iteration 178, loss = 0.05009747\n",
      "Iteration 179, loss = 0.05023489\n",
      "Iteration 161, loss = 0.05565141\n",
      "Iteration 180, loss = 0.04547196\n",
      "Iteration 162, loss = 0.05556966\n",
      "Iteration 181, loss = 0.04314751\n",
      "Iteration 163, loss = 0.05594610\n",
      "Iteration 182, loss = 0.04352000\n",
      "Iteration 164, loss = 0.05373521\n",
      "Iteration 183, loss = 0.04432444\n",
      "Iteration 165, loss = 0.05274244\n",
      "Iteration 166, loss = 0.05278124\n",
      "Iteration 184, loss = 0.04308298\n",
      "Iteration 185, loss = 0.04341878\n",
      "Iteration 167, loss = 0.05233442\n",
      "Iteration 186, loss = 0.04239597\n",
      "Iteration 168, loss = 0.05245505\n",
      "Iteration 187, loss = 0.04153500\n",
      "Iteration 169, loss = 0.05435915\n",
      "Iteration 188, loss = 0.04154992\n",
      "Iteration 170, loss = 0.05219368\n",
      "Iteration 189, loss = 0.04111368\n",
      "Iteration 171, loss = 0.05249275\n",
      "Iteration 190, loss = 0.04179862\n",
      "Iteration 172, loss = 0.05352126\n",
      "Iteration 191, loss = 0.04100553\n",
      "Iteration 173, loss = 0.05497199\n",
      "Iteration 192, loss = 0.04135893\n",
      "Iteration 174, loss = 0.05499566\n",
      "Iteration 193, loss = 0.03943348\n",
      "Iteration 175, loss = 0.06364467\n",
      "Iteration 194, loss = 0.04061046\n",
      "Iteration 176, loss = 0.05789403\n",
      "Iteration 195, loss = 0.03995619\n",
      "Iteration 177, loss = 0.05498794\n",
      "Iteration 196, loss = 0.04033140\n",
      "Iteration 178, loss = 0.05713479\n",
      "Iteration 197, loss = 0.03815241\n",
      "Iteration 179, loss = 0.04775834\n",
      "Iteration 198, loss = 0.03914626\n",
      "Iteration 180, loss = 0.04986424\n",
      "Iteration 199, loss = 0.03950355\n",
      "Iteration 181, loss = 0.05025183\n",
      "Iteration 200, loss = 0.04121132\n",
      "Iteration 182, loss = 0.04967137\n",
      "Iteration 183, loss = 0.04917562\n",
      "Iteration 184, loss = 0.04641666\n",
      "Iteration 185, loss = 0.04949880\n",
      "Iteration 186, loss = 0.04814429\n",
      "Iteration 187, loss = 0.04661384\n",
      "Iteration 188, loss = 0.04481536\n",
      "Iteration 189, loss = 0.04628909\n",
      "Iteration 190, loss = 0.04506666\n",
      "Iteration 191, loss = 0.04351322\n",
      "Iteration 192, loss = 0.04619881\n",
      "Iteration 193, loss = 0.04360637\n",
      "Iteration 194, loss = 0.04296788\n",
      "Iteration 195, loss = 0.04502424\n",
      "Iteration 1, loss = 0.98912385\n",
      "Iteration 196, loss = 0.04161937\n",
      "Iteration 2, loss = 0.77440838\n",
      "Iteration 197, loss = 0.04129977\n",
      "Iteration 3, loss = 0.64344908\n",
      "Iteration 198, loss = 0.04216687\n",
      "Iteration 4, loss = 0.56900577\n",
      "Iteration 199, loss = 0.04177025\n",
      "Iteration 5, loss = 0.51407886\n",
      "Iteration 200, loss = 0.04298096\n",
      "Iteration 6, loss = 0.47580391\n",
      "Iteration 7, loss = 0.44577262\n",
      "Iteration 8, loss = 0.42174254\n",
      "Iteration 9, loss = 0.40088925\n",
      "Iteration 10, loss = 0.38507585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.37072522\n",
      "Iteration 12, loss = 0.35828942\n",
      "Iteration 13, loss = 0.34550920\n",
      "Iteration 14, loss = 0.33591316\n",
      "Iteration 15, loss = 0.32569254\n",
      "Iteration 16, loss = 0.31665568\n",
      "Iteration 17, loss = 0.30887993\n",
      "Iteration 18, loss = 0.29960653\n",
      "Iteration 1, loss = 1.06445807\n",
      "Iteration 19, loss = 0.29251094\n",
      "Iteration 2, loss = 0.77123820\n",
      "Iteration 20, loss = 0.28589458\n",
      "Iteration 3, loss = 0.63553568\n",
      "Iteration 21, loss = 0.27683065\n",
      "Iteration 4, loss = 0.54825269\n",
      "Iteration 22, loss = 0.27080557\n",
      "Iteration 5, loss = 0.49664422\n",
      "Iteration 23, loss = 0.26343136\n",
      "Iteration 6, loss = 0.46013306\n",
      "Iteration 24, loss = 0.25758348\n",
      "Iteration 7, loss = 0.43342317\n",
      "Iteration 25, loss = 0.25078288\n",
      "Iteration 8, loss = 0.41391080\n",
      "Iteration 26, loss = 0.24456530\n",
      "Iteration 9, loss = 0.39565469\n",
      "Iteration 27, loss = 0.23919974\n",
      "Iteration 10, loss = 0.38159592\n",
      "Iteration 28, loss = 0.23344578\n",
      "Iteration 11, loss = 0.36671589\n",
      "Iteration 29, loss = 0.22842473\n",
      "Iteration 12, loss = 0.35534228\n",
      "Iteration 30, loss = 0.22377048\n",
      "Iteration 13, loss = 0.34394340\n",
      "Iteration 31, loss = 0.21790667\n",
      "Iteration 14, loss = 0.33508307\n",
      "Iteration 32, loss = 0.21429980\n",
      "Iteration 15, loss = 0.32472064\n",
      "Iteration 33, loss = 0.20797225\n",
      "Iteration 16, loss = 0.31585830\n",
      "Iteration 34, loss = 0.20310182\n",
      "Iteration 17, loss = 0.30646159\n",
      "Iteration 35, loss = 0.20015797\n",
      "Iteration 18, loss = 0.29837513\n",
      "Iteration 36, loss = 0.19422586\n",
      "Iteration 19, loss = 0.29117176\n",
      "Iteration 37, loss = 0.19011304\n",
      "Iteration 20, loss = 0.28407597\n",
      "Iteration 38, loss = 0.18627246\n",
      "Iteration 21, loss = 0.27543396\n",
      "Iteration 39, loss = 0.18230187\n",
      "Iteration 22, loss = 0.26863867\n",
      "Iteration 40, loss = 0.18038293\n",
      "Iteration 23, loss = 0.26212242\n",
      "Iteration 41, loss = 0.17585107\n",
      "Iteration 24, loss = 0.25644451\n",
      "Iteration 42, loss = 0.17162600\n",
      "Iteration 25, loss = 0.25103601\n",
      "Iteration 43, loss = 0.16898705\n",
      "Iteration 26, loss = 0.24486955\n",
      "Iteration 44, loss = 0.16505039\n",
      "Iteration 27, loss = 0.24041111\n",
      "Iteration 45, loss = 0.16268342\n",
      "Iteration 28, loss = 0.23542822\n",
      "Iteration 46, loss = 0.15897108\n",
      "Iteration 29, loss = 0.23023858\n",
      "Iteration 47, loss = 0.15572232\n",
      "Iteration 30, loss = 0.22663030\n",
      "Iteration 48, loss = 0.15313111\n",
      "Iteration 31, loss = 0.22156681\n",
      "Iteration 49, loss = 0.14936996\n",
      "Iteration 32, loss = 0.21879394\n",
      "Iteration 50, loss = 0.14677074\n",
      "Iteration 33, loss = 0.21378864\n",
      "Iteration 34, loss = 0.21074874\n",
      "Iteration 51, loss = 0.14387288\n",
      "Iteration 35, loss = 0.20644143\n",
      "Iteration 52, loss = 0.14135533\n",
      "Iteration 36, loss = 0.20458666\n",
      "Iteration 53, loss = 0.13856310\n",
      "Iteration 37, loss = 0.20295441\n",
      "Iteration 54, loss = 0.13611410\n",
      "Iteration 38, loss = 0.20122101\n",
      "Iteration 55, loss = 0.13426394\n",
      "Iteration 39, loss = 0.19529769\n",
      "Iteration 56, loss = 0.13276374\n",
      "Iteration 40, loss = 0.19197877\n",
      "Iteration 57, loss = 0.12910925\n",
      "Iteration 41, loss = 0.18896649\n",
      "Iteration 58, loss = 0.12826130\n",
      "Iteration 42, loss = 0.18359441\n",
      "Iteration 59, loss = 0.12585205\n",
      "Iteration 43, loss = 0.18418245\n",
      "Iteration 60, loss = 0.12292446\n",
      "Iteration 61, loss = 0.12182169\n",
      "Iteration 44, loss = 0.17981910\n",
      "Iteration 62, loss = 0.11899500\n",
      "Iteration 45, loss = 0.17753508\n",
      "Iteration 63, loss = 0.11966369\n",
      "Iteration 46, loss = 0.17489586\n",
      "Iteration 64, loss = 0.11668859\n",
      "Iteration 47, loss = 0.17045250\n",
      "Iteration 65, loss = 0.11486734\n",
      "Iteration 48, loss = 0.16947482\n",
      "Iteration 66, loss = 0.11677778\n",
      "Iteration 49, loss = 0.16471411\n",
      "Iteration 67, loss = 0.10901331\n",
      "Iteration 50, loss = 0.16286547\n",
      "Iteration 68, loss = 0.11145402\n",
      "Iteration 51, loss = 0.16070541\n",
      "Iteration 69, loss = 0.10888089\n",
      "Iteration 52, loss = 0.15954047\n",
      "Iteration 70, loss = 0.10641549\n",
      "Iteration 53, loss = 0.15722348\n",
      "Iteration 71, loss = 0.10404021\n",
      "Iteration 54, loss = 0.15355330\n",
      "Iteration 72, loss = 0.10365992\n",
      "Iteration 55, loss = 0.15358565\n",
      "Iteration 73, loss = 0.10218715\n",
      "Iteration 56, loss = 0.14878846\n",
      "Iteration 74, loss = 0.09836395\n",
      "Iteration 57, loss = 0.14641999\n",
      "Iteration 75, loss = 0.09771699\n",
      "Iteration 58, loss = 0.14455403\n",
      "Iteration 76, loss = 0.09648510\n",
      "Iteration 59, loss = 0.14367787\n",
      "Iteration 77, loss = 0.09458292\n",
      "Iteration 60, loss = 0.14099078\n",
      "Iteration 78, loss = 0.09465492\n",
      "Iteration 61, loss = 0.13760067\n",
      "Iteration 79, loss = 0.09479021\n",
      "Iteration 62, loss = 0.13709075\n",
      "Iteration 80, loss = 0.09131455\n",
      "Iteration 63, loss = 0.13472388\n",
      "Iteration 81, loss = 0.09015260\n",
      "Iteration 64, loss = 0.13298984\n",
      "Iteration 82, loss = 0.08814644\n",
      "Iteration 65, loss = 0.13188799\n",
      "Iteration 66, loss = 0.12897469\n",
      "Iteration 83, loss = 0.09109163\n",
      "Iteration 67, loss = 0.12822722\n",
      "Iteration 84, loss = 0.09073702\n",
      "Iteration 68, loss = 0.12599202\n",
      "Iteration 85, loss = 0.08926206\n",
      "Iteration 69, loss = 0.12603925\n",
      "Iteration 86, loss = 0.08469869\n",
      "Iteration 70, loss = 0.12592724\n",
      "Iteration 87, loss = 0.08278870\n",
      "Iteration 71, loss = 0.12071710\n",
      "Iteration 88, loss = 0.08276411\n",
      "Iteration 72, loss = 0.12168861\n",
      "Iteration 89, loss = 0.08169560\n",
      "Iteration 73, loss = 0.11960938\n",
      "Iteration 90, loss = 0.07991954\n",
      "Iteration 91, loss = 0.07839728\n",
      "Iteration 74, loss = 0.11702262\n",
      "Iteration 92, loss = 0.07788937\n",
      "Iteration 75, loss = 0.11755252\n",
      "Iteration 93, loss = 0.07656955\n",
      "Iteration 76, loss = 0.11577457\n",
      "Iteration 94, loss = 0.07555991\n",
      "Iteration 77, loss = 0.11472382\n",
      "Iteration 95, loss = 0.07474735\n",
      "Iteration 78, loss = 0.11081360\n",
      "Iteration 96, loss = 0.07438459\n",
      "Iteration 79, loss = 0.11087296\n",
      "Iteration 97, loss = 0.07558716\n",
      "Iteration 80, loss = 0.11068949\n",
      "Iteration 98, loss = 0.07274463\n",
      "Iteration 81, loss = 0.10781206\n",
      "Iteration 99, loss = 0.07153103\n",
      "Iteration 82, loss = 0.10655167\n",
      "Iteration 100, loss = 0.07108681\n",
      "Iteration 83, loss = 0.10571231\n",
      "Iteration 101, loss = 0.06903266\n",
      "Iteration 84, loss = 0.10416247\n",
      "Iteration 102, loss = 0.06872700\n",
      "Iteration 85, loss = 0.10366200\n",
      "Iteration 103, loss = 0.06712344\n",
      "Iteration 86, loss = 0.10021524\n",
      "Iteration 104, loss = 0.06670203\n",
      "Iteration 87, loss = 0.10049023\n",
      "Iteration 105, loss = 0.06607805\n",
      "Iteration 88, loss = 0.10087331\n",
      "Iteration 106, loss = 0.06533144\n",
      "Iteration 89, loss = 0.10020648\n",
      "Iteration 107, loss = 0.06466806\n",
      "Iteration 90, loss = 0.09536771\n",
      "Iteration 108, loss = 0.06326613\n",
      "Iteration 91, loss = 0.10042406\n",
      "Iteration 109, loss = 0.06283732\n",
      "Iteration 92, loss = 0.09756092\n",
      "Iteration 110, loss = 0.06155727\n",
      "Iteration 93, loss = 0.09320235\n",
      "Iteration 111, loss = 0.06155746\n",
      "Iteration 94, loss = 0.09268151\n",
      "Iteration 112, loss = 0.06036644\n",
      "Iteration 95, loss = 0.09764658\n",
      "Iteration 113, loss = 0.06102907\n",
      "Iteration 96, loss = 0.09887404\n",
      "Iteration 114, loss = 0.05925174\n",
      "Iteration 97, loss = 0.09100179\n",
      "Iteration 115, loss = 0.05810972\n",
      "Iteration 98, loss = 0.08927588\n",
      "Iteration 116, loss = 0.05834336\n",
      "Iteration 99, loss = 0.08866962\n",
      "Iteration 117, loss = 0.05883873\n",
      "Iteration 100, loss = 0.08627547\n",
      "Iteration 118, loss = 0.05895764\n",
      "Iteration 101, loss = 0.08513931\n",
      "Iteration 119, loss = 0.05973073\n",
      "Iteration 102, loss = 0.08693106\n",
      "Iteration 120, loss = 0.05435230\n",
      "Iteration 103, loss = 0.08740927\n",
      "Iteration 121, loss = 0.05706666\n",
      "Iteration 104, loss = 0.08369154\n",
      "Iteration 122, loss = 0.05493712\n",
      "Iteration 105, loss = 0.08093395\n",
      "Iteration 123, loss = 0.05436336\n",
      "Iteration 106, loss = 0.08123410\n",
      "Iteration 124, loss = 0.05632245\n",
      "Iteration 107, loss = 0.07932707\n",
      "Iteration 125, loss = 0.05490891\n",
      "Iteration 108, loss = 0.07938956\n",
      "Iteration 126, loss = 0.05550240\n",
      "Iteration 109, loss = 0.07754335\n",
      "Iteration 127, loss = 0.05192606\n",
      "Iteration 110, loss = 0.07803016\n",
      "Iteration 128, loss = 0.05073706\n",
      "Iteration 111, loss = 0.07665226\n",
      "Iteration 129, loss = 0.05291659\n",
      "Iteration 112, loss = 0.07522753\n",
      "Iteration 130, loss = 0.05021480\n",
      "Iteration 113, loss = 0.07444611\n",
      "Iteration 131, loss = 0.04878164\n",
      "Iteration 114, loss = 0.07869777\n",
      "Iteration 132, loss = 0.04926129\n",
      "Iteration 115, loss = 0.08559629\n",
      "Iteration 133, loss = 0.04888344\n",
      "Iteration 116, loss = 0.08015264\n",
      "Iteration 134, loss = 0.04857732\n",
      "Iteration 117, loss = 0.07869859\n",
      "Iteration 135, loss = 0.04933641\n",
      "Iteration 118, loss = 0.08179440\n",
      "Iteration 136, loss = 0.04793522\n",
      "Iteration 119, loss = 0.07180165\n",
      "Iteration 137, loss = 0.04833534\n",
      "Iteration 120, loss = 0.07129319\n",
      "Iteration 138, loss = 0.04635216\n",
      "Iteration 121, loss = 0.07260537\n",
      "Iteration 139, loss = 0.04772278\n",
      "Iteration 122, loss = 0.06975466\n",
      "Iteration 140, loss = 0.04521027\n",
      "Iteration 123, loss = 0.06961116\n",
      "Iteration 141, loss = 0.04477161\n",
      "Iteration 124, loss = 0.06715293\n",
      "Iteration 142, loss = 0.04407927\n",
      "Iteration 125, loss = 0.06818056\n",
      "Iteration 143, loss = 0.04291881\n",
      "Iteration 126, loss = 0.06596614\n",
      "Iteration 144, loss = 0.04429861\n",
      "Iteration 127, loss = 0.06556248\n",
      "Iteration 145, loss = 0.04482207\n",
      "Iteration 128, loss = 0.06581630\n",
      "Iteration 146, loss = 0.04569017\n",
      "Iteration 129, loss = 0.06301313\n",
      "Iteration 147, loss = 0.04530163\n",
      "Iteration 130, loss = 0.06215622\n",
      "Iteration 148, loss = 0.04315837\n",
      "Iteration 131, loss = 0.06339649\n",
      "Iteration 149, loss = 0.04211210\n",
      "Iteration 132, loss = 0.06270902\n",
      "Iteration 150, loss = 0.04176614\n",
      "Iteration 133, loss = 0.06119097\n",
      "Iteration 151, loss = 0.04067744\n",
      "Iteration 134, loss = 0.06006116\n",
      "Iteration 152, loss = 0.03964258\n",
      "Iteration 135, loss = 0.05827812\n",
      "Iteration 153, loss = 0.04024780\n",
      "Iteration 136, loss = 0.05800615\n",
      "Iteration 154, loss = 0.03844834\n",
      "Iteration 137, loss = 0.05940748\n",
      "Iteration 155, loss = 0.03963004\n",
      "Iteration 138, loss = 0.05920278\n",
      "Iteration 156, loss = 0.03903197\n",
      "Iteration 139, loss = 0.05822761\n",
      "Iteration 157, loss = 0.03851403\n",
      "Iteration 140, loss = 0.05647111\n",
      "Iteration 158, loss = 0.03757333\n",
      "Iteration 141, loss = 0.05790283\n",
      "Iteration 159, loss = 0.03727296\n",
      "Iteration 142, loss = 0.05456249\n",
      "Iteration 160, loss = 0.03815226\n",
      "Iteration 143, loss = 0.05564095\n",
      "Iteration 161, loss = 0.03672022\n",
      "Iteration 144, loss = 0.05845794\n",
      "Iteration 162, loss = 0.03685068\n",
      "Iteration 145, loss = 0.05874095\n",
      "Iteration 163, loss = 0.03592323\n",
      "Iteration 146, loss = 0.05442464\n",
      "Iteration 164, loss = 0.03761164\n",
      "Iteration 147, loss = 0.05528523\n",
      "Iteration 165, loss = 0.03773651\n",
      "Iteration 148, loss = 0.05286470\n",
      "Iteration 166, loss = 0.03602941\n",
      "Iteration 149, loss = 0.05222528\n",
      "Iteration 167, loss = 0.03764571\n",
      "Iteration 150, loss = 0.05074384\n",
      "Iteration 168, loss = 0.03660367\n",
      "Iteration 151, loss = 0.05085906\n",
      "Iteration 169, loss = 0.03608323\n",
      "Iteration 152, loss = 0.05048781\n",
      "Iteration 170, loss = 0.03376572\n",
      "Iteration 153, loss = 0.04838488\n",
      "Iteration 171, loss = 0.03707155\n",
      "Iteration 154, loss = 0.05114436\n",
      "Iteration 172, loss = 0.03495323\n",
      "Iteration 155, loss = 0.05073412\n",
      "Iteration 173, loss = 0.03371558\n",
      "Iteration 156, loss = 0.04849979\n",
      "Iteration 174, loss = 0.03273616\n",
      "Iteration 157, loss = 0.05030474\n",
      "Iteration 175, loss = 0.03291013\n",
      "Iteration 158, loss = 0.05374577\n",
      "Iteration 176, loss = 0.03208787\n",
      "Iteration 159, loss = 0.05050206\n",
      "Iteration 177, loss = 0.03234959\n",
      "Iteration 160, loss = 0.04756096\n",
      "Iteration 178, loss = 0.03350900\n",
      "Iteration 161, loss = 0.04863496\n",
      "Iteration 179, loss = 0.03252539\n",
      "Iteration 162, loss = 0.04826299\n",
      "Iteration 180, loss = 0.03177915\n",
      "Iteration 163, loss = 0.04901678\n",
      "Iteration 181, loss = 0.03160589\n",
      "Iteration 164, loss = 0.04939719\n",
      "Iteration 182, loss = 0.03094115\n",
      "Iteration 165, loss = 0.04357172\n",
      "Iteration 183, loss = 0.03038153\n",
      "Iteration 166, loss = 0.04528158\n",
      "Iteration 184, loss = 0.02998399\n",
      "Iteration 167, loss = 0.04387928\n",
      "Iteration 185, loss = 0.03165153\n",
      "Iteration 168, loss = 0.04342991\n",
      "Iteration 186, loss = 0.03151290\n",
      "Iteration 169, loss = 0.04270637\n",
      "Iteration 187, loss = 0.02999824\n",
      "Iteration 170, loss = 0.04309226\n",
      "Iteration 188, loss = 0.03060684\n",
      "Iteration 171, loss = 0.04298416\n",
      "Iteration 189, loss = 0.03110419\n",
      "Iteration 172, loss = 0.04253464\n",
      "Iteration 190, loss = 0.02857669\n",
      "Iteration 173, loss = 0.04717946\n",
      "Iteration 191, loss = 0.02897949\n",
      "Iteration 174, loss = 0.04350910\n",
      "Iteration 192, loss = 0.02799262\n",
      "Iteration 175, loss = 0.04555175\n",
      "Iteration 193, loss = 0.02821875\n",
      "Iteration 176, loss = 0.04796488\n",
      "Iteration 194, loss = 0.02838870\n",
      "Iteration 177, loss = 0.04579533\n",
      "Iteration 195, loss = 0.02781967\n",
      "Iteration 178, loss = 0.04296044\n",
      "Iteration 196, loss = 0.02749735\n",
      "Iteration 179, loss = 0.04266366\n",
      "Iteration 197, loss = 0.02723617\n",
      "Iteration 180, loss = 0.04023150\n",
      "Iteration 198, loss = 0.02716931\n",
      "Iteration 181, loss = 0.03952275\n",
      "Iteration 199, loss = 0.02643045\n",
      "Iteration 182, loss = 0.04039400\n",
      "Iteration 200, loss = 0.02694747\n",
      "Iteration 183, loss = 0.03908353\n",
      "Iteration 184, loss = 0.04008072\n",
      "Iteration 185, loss = 0.03857354\n",
      "Iteration 186, loss = 0.03779357\n",
      "Iteration 187, loss = 0.03700779\n",
      "Iteration 188, loss = 0.03687137\n",
      "Iteration 189, loss = 0.03656341\n",
      "Iteration 190, loss = 0.03568423\n",
      "Iteration 191, loss = 0.03587192\n",
      "Iteration 192, loss = 0.03569310\n",
      "Iteration 193, loss = 0.03541026\n",
      "Iteration 194, loss = 0.03758825\n",
      "Iteration 195, loss = 0.03571790\n",
      "Iteration 1, loss = 1.25072635\n",
      "Iteration 196, loss = 0.03518317\n",
      "Iteration 2, loss = 0.87066507\n",
      "Iteration 197, loss = 0.03886781\n",
      "Iteration 3, loss = 0.68288637\n",
      "Iteration 198, loss = 0.03548372\n",
      "Iteration 4, loss = 0.57669070\n",
      "Iteration 199, loss = 0.03454175\n",
      "Iteration 5, loss = 0.51929043\n",
      "Iteration 200, loss = 0.03603043\n",
      "Iteration 6, loss = 0.47496437\n",
      "Iteration 7, loss = 0.44801405\n",
      "Iteration 8, loss = 0.42371414\n",
      "Iteration 9, loss = 0.40676811\n",
      "Iteration 10, loss = 0.39123410\n",
      "Iteration 11, loss = 0.37857523\n",
      "Iteration 12, loss = 0.36710500\n",
      "Iteration 13, loss = 0.35593787\n",
      "Iteration 14, loss = 0.34759843\n",
      "Iteration 15, loss = 0.33781980\n",
      "Iteration 16, loss = 0.32879984\n",
      "Iteration 17, loss = 0.32148880\n",
      "Iteration 18, loss = 0.31352916\n",
      "Iteration 1, loss = 1.43718338\n",
      "Iteration 19, loss = 0.30787178\n",
      "Iteration 2, loss = 0.94050717\n",
      "Iteration 20, loss = 0.29963204\n",
      "Iteration 3, loss = 0.77135475\n",
      "Iteration 21, loss = 0.29345075\n",
      "Iteration 4, loss = 0.66581549\n",
      "Iteration 22, loss = 0.28746818\n",
      "Iteration 5, loss = 0.58781093\n",
      "Iteration 23, loss = 0.28248684\n",
      "Iteration 6, loss = 0.53401514\n",
      "Iteration 24, loss = 0.27638155\n",
      "Iteration 7, loss = 0.49960648\n",
      "Iteration 25, loss = 0.27046404\n",
      "Iteration 8, loss = 0.46956283\n",
      "Iteration 26, loss = 0.26583067\n",
      "Iteration 9, loss = 0.44779369\n",
      "Iteration 27, loss = 0.25921220\n",
      "Iteration 10, loss = 0.42862979\n",
      "Iteration 28, loss = 0.25511448\n",
      "Iteration 11, loss = 0.41363820\n",
      "Iteration 29, loss = 0.24923809\n",
      "Iteration 12, loss = 0.40030694\n",
      "Iteration 30, loss = 0.24658111\n",
      "Iteration 13, loss = 0.38819696\n",
      "Iteration 31, loss = 0.24126806\n",
      "Iteration 14, loss = 0.37851935\n",
      "Iteration 32, loss = 0.23621474\n",
      "Iteration 15, loss = 0.36657169\n",
      "Iteration 33, loss = 0.23193021\n",
      "Iteration 16, loss = 0.35772969\n",
      "Iteration 34, loss = 0.22837213\n",
      "Iteration 17, loss = 0.34948260\n",
      "Iteration 35, loss = 0.22571276\n",
      "Iteration 18, loss = 0.34088571\n",
      "Iteration 36, loss = 0.22055145\n",
      "Iteration 19, loss = 0.33366839\n",
      "Iteration 37, loss = 0.21630879\n",
      "Iteration 20, loss = 0.32530519\n",
      "Iteration 38, loss = 0.21443235\n",
      "Iteration 21, loss = 0.31879459\n",
      "Iteration 39, loss = 0.20950532\n",
      "Iteration 22, loss = 0.31123715\n",
      "Iteration 40, loss = 0.20559162\n",
      "Iteration 23, loss = 0.30623521\n",
      "Iteration 24, loss = 0.29804039\n",
      "Iteration 41, loss = 0.20276816\n",
      "Iteration 25, loss = 0.29184894\n",
      "Iteration 42, loss = 0.19737397\n",
      "Iteration 26, loss = 0.28566869\n",
      "Iteration 43, loss = 0.19835838\n",
      "Iteration 27, loss = 0.27971222\n",
      "Iteration 44, loss = 0.19205167\n",
      "Iteration 28, loss = 0.27378082\n",
      "Iteration 45, loss = 0.19134997\n",
      "Iteration 46, loss = 0.18636578\n",
      "Iteration 29, loss = 0.26779907\n",
      "Iteration 47, loss = 0.18287298\n",
      "Iteration 30, loss = 0.26350993\n",
      "Iteration 48, loss = 0.18113200\n",
      "Iteration 31, loss = 0.25777832\n",
      "Iteration 49, loss = 0.17821811\n",
      "Iteration 32, loss = 0.25262513\n",
      "Iteration 50, loss = 0.17523143\n",
      "Iteration 33, loss = 0.24854242\n",
      "Iteration 51, loss = 0.17371337\n",
      "Iteration 34, loss = 0.24290219\n",
      "Iteration 52, loss = 0.17227523\n",
      "Iteration 35, loss = 0.23995514\n",
      "Iteration 53, loss = 0.16707870\n",
      "Iteration 36, loss = 0.23444457\n",
      "Iteration 54, loss = 0.16637608\n",
      "Iteration 37, loss = 0.23001537\n",
      "Iteration 55, loss = 0.16319664\n",
      "Iteration 38, loss = 0.22605017\n",
      "Iteration 56, loss = 0.16131880\n",
      "Iteration 39, loss = 0.22321095\n",
      "Iteration 57, loss = 0.15808613\n",
      "Iteration 40, loss = 0.21921092\n",
      "Iteration 58, loss = 0.15729831\n",
      "Iteration 41, loss = 0.21435032\n",
      "Iteration 59, loss = 0.15376339\n",
      "Iteration 42, loss = 0.21114176\n",
      "Iteration 60, loss = 0.15167834\n",
      "Iteration 43, loss = 0.20731610\n",
      "Iteration 61, loss = 0.14936071\n",
      "Iteration 44, loss = 0.20380882\n",
      "Iteration 62, loss = 0.14808049\n",
      "Iteration 45, loss = 0.20085157\n",
      "Iteration 63, loss = 0.14541014\n",
      "Iteration 46, loss = 0.19763317\n",
      "Iteration 64, loss = 0.14314507\n",
      "Iteration 47, loss = 0.19407221\n",
      "Iteration 65, loss = 0.14123920\n",
      "Iteration 48, loss = 0.19134527\n",
      "Iteration 66, loss = 0.14017080\n",
      "Iteration 49, loss = 0.18838817\n",
      "Iteration 67, loss = 0.13948716\n",
      "Iteration 50, loss = 0.18611506\n",
      "Iteration 68, loss = 0.13612399\n",
      "Iteration 51, loss = 0.18195081\n",
      "Iteration 69, loss = 0.13612160\n",
      "Iteration 52, loss = 0.18209815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.13569980\n",
      "Iteration 53, loss = 0.17634720\n",
      "Iteration 54, loss = 0.17418276\n",
      "Iteration 71, loss = 0.13023086\n",
      "Iteration 55, loss = 0.17189440\n",
      "Iteration 72, loss = 0.13043088\n",
      "Iteration 56, loss = 0.16916944\n",
      "Iteration 73, loss = 0.13025557\n",
      "Iteration 57, loss = 0.16627980\n",
      "Iteration 74, loss = 0.12696778\n",
      "Iteration 58, loss = 0.16520593\n",
      "Iteration 75, loss = 0.12629542\n",
      "Iteration 59, loss = 0.16291507\n",
      "Iteration 76, loss = 0.12393032\n",
      "Iteration 60, loss = 0.15978607\n",
      "Iteration 77, loss = 0.12347466\n",
      "Iteration 61, loss = 0.15721721\n",
      "Iteration 78, loss = 0.12218722\n",
      "Iteration 62, loss = 0.15604188\n",
      "Iteration 79, loss = 0.12015263\n",
      "Iteration 63, loss = 0.15172609\n",
      "Iteration 80, loss = 0.11867736\n",
      "Iteration 64, loss = 0.14952748\n",
      "Iteration 81, loss = 0.11801937\n",
      "Iteration 65, loss = 0.14889144\n",
      "Iteration 82, loss = 0.11846166\n",
      "Iteration 66, loss = 0.14579987\n",
      "Iteration 83, loss = 0.11800365\n",
      "Iteration 67, loss = 0.14384779\n",
      "Iteration 84, loss = 0.11740131\n",
      "Iteration 68, loss = 0.14100727\n",
      "Iteration 85, loss = 0.11118801\n",
      "Iteration 69, loss = 0.14109040\n",
      "Iteration 86, loss = 0.11209865\n",
      "Iteration 70, loss = 0.13816982\n",
      "Iteration 87, loss = 0.11245753\n",
      "Iteration 71, loss = 0.13750812\n",
      "Iteration 88, loss = 0.11122363\n",
      "Iteration 72, loss = 0.13509011\n",
      "Iteration 89, loss = 0.10689738\n",
      "Iteration 73, loss = 0.13310820\n",
      "Iteration 90, loss = 0.10757155\n",
      "Iteration 74, loss = 0.13017987\n",
      "Iteration 91, loss = 0.10463655\n",
      "Iteration 75, loss = 0.12899320\n",
      "Iteration 92, loss = 0.10395403\n",
      "Iteration 76, loss = 0.12852518\n",
      "Iteration 93, loss = 0.10118052\n",
      "Iteration 77, loss = 0.12757159\n",
      "Iteration 94, loss = 0.10435077\n",
      "Iteration 78, loss = 0.12201932\n",
      "Iteration 95, loss = 0.09901913\n",
      "Iteration 79, loss = 0.12739946\n",
      "Iteration 96, loss = 0.09805495\n",
      "Iteration 80, loss = 0.12247780\n",
      "Iteration 97, loss = 0.09676970\n",
      "Iteration 81, loss = 0.11880754\n",
      "Iteration 98, loss = 0.09642571\n",
      "Iteration 82, loss = 0.11575428\n",
      "Iteration 99, loss = 0.09563316\n",
      "Iteration 83, loss = 0.11552429\n",
      "Iteration 100, loss = 0.09512951\n",
      "Iteration 84, loss = 0.11595388\n",
      "Iteration 101, loss = 0.09323356\n",
      "Iteration 85, loss = 0.11096964\n",
      "Iteration 102, loss = 0.09135978\n",
      "Iteration 86, loss = 0.11220837\n",
      "Iteration 103, loss = 0.09143453\n",
      "Iteration 87, loss = 0.10916868\n",
      "Iteration 104, loss = 0.09271557\n",
      "Iteration 88, loss = 0.10769507\n",
      "Iteration 105, loss = 0.09226425\n",
      "Iteration 89, loss = 0.10621926\n",
      "Iteration 106, loss = 0.09360450\n",
      "Iteration 107, loss = 0.09355622\n",
      "Iteration 90, loss = 0.10516310\n",
      "Iteration 108, loss = 0.08642497\n",
      "Iteration 91, loss = 0.10524487\n",
      "Iteration 109, loss = 0.08678232\n",
      "Iteration 92, loss = 0.10324327\n",
      "Iteration 110, loss = 0.08715797\n",
      "Iteration 93, loss = 0.10127784\n",
      "Iteration 111, loss = 0.08501877\n",
      "Iteration 94, loss = 0.10073957\n",
      "Iteration 112, loss = 0.08474080\n",
      "Iteration 95, loss = 0.10174710\n",
      "Iteration 113, loss = 0.08387749\n",
      "Iteration 96, loss = 0.09933897\n",
      "Iteration 114, loss = 0.08266519\n",
      "Iteration 97, loss = 0.09552095\n",
      "Iteration 115, loss = 0.08174341\n",
      "Iteration 98, loss = 0.09609962\n",
      "Iteration 116, loss = 0.07960590\n",
      "Iteration 99, loss = 0.09336703\n",
      "Iteration 117, loss = 0.07986829\n",
      "Iteration 100, loss = 0.09345017\n",
      "Iteration 118, loss = 0.07931152\n",
      "Iteration 101, loss = 0.09127919\n",
      "Iteration 119, loss = 0.07807730\n",
      "Iteration 102, loss = 0.08958914\n",
      "Iteration 120, loss = 0.07835582\n",
      "Iteration 103, loss = 0.08842369\n",
      "Iteration 121, loss = 0.07613682\n",
      "Iteration 104, loss = 0.08690137\n",
      "Iteration 122, loss = 0.07505968\n",
      "Iteration 105, loss = 0.08753850\n",
      "Iteration 123, loss = 0.07635117\n",
      "Iteration 106, loss = 0.08688376\n",
      "Iteration 124, loss = 0.07566931\n",
      "Iteration 107, loss = 0.08518831\n",
      "Iteration 125, loss = 0.07286534\n",
      "Iteration 108, loss = 0.08386078\n",
      "Iteration 126, loss = 0.07147177\n",
      "Iteration 109, loss = 0.08673147\n",
      "Iteration 127, loss = 0.07106162\n",
      "Iteration 110, loss = 0.08323612\n",
      "Iteration 128, loss = 0.07128802\n",
      "Iteration 111, loss = 0.08248011\n",
      "Iteration 129, loss = 0.07185247\n",
      "Iteration 112, loss = 0.08527230\n",
      "Iteration 130, loss = 0.07028692\n",
      "Iteration 113, loss = 0.07967092\n",
      "Iteration 131, loss = 0.06816995\n",
      "Iteration 114, loss = 0.07862531\n",
      "Iteration 132, loss = 0.06787032\n",
      "Iteration 115, loss = 0.07807094\n",
      "Iteration 133, loss = 0.06719654\n",
      "Iteration 116, loss = 0.08017181\n",
      "Iteration 117, loss = 0.07335412\n",
      "Iteration 134, loss = 0.06560507\n",
      "Iteration 118, loss = 0.07537110\n",
      "Iteration 135, loss = 0.06533246\n",
      "Iteration 119, loss = 0.07258395\n",
      "Iteration 136, loss = 0.06519501\n",
      "Iteration 120, loss = 0.07264146\n",
      "Iteration 137, loss = 0.06484892\n",
      "Iteration 121, loss = 0.07047721\n",
      "Iteration 138, loss = 0.06395404\n",
      "Iteration 122, loss = 0.06879448\n",
      "Iteration 139, loss = 0.06403594\n",
      "Iteration 123, loss = 0.07196100\n",
      "Iteration 140, loss = 0.06371252\n",
      "Iteration 124, loss = 0.06771071\n",
      "Iteration 141, loss = 0.06112824\n",
      "Iteration 125, loss = 0.06724113\n",
      "Iteration 142, loss = 0.06175588\n",
      "Iteration 126, loss = 0.06409666\n",
      "Iteration 143, loss = 0.06087409\n",
      "Iteration 127, loss = 0.06534254\n",
      "Iteration 144, loss = 0.05899289\n",
      "Iteration 128, loss = 0.06273542\n",
      "Iteration 145, loss = 0.06074455\n",
      "Iteration 129, loss = 0.06239716\n",
      "Iteration 146, loss = 0.05899874\n",
      "Iteration 130, loss = 0.06114288\n",
      "Iteration 147, loss = 0.05781709\n",
      "Iteration 131, loss = 0.06141500\n",
      "Iteration 148, loss = 0.06028558\n",
      "Iteration 132, loss = 0.06111654\n",
      "Iteration 149, loss = 0.06343193\n",
      "Iteration 133, loss = 0.05955624\n",
      "Iteration 150, loss = 0.06145362\n",
      "Iteration 134, loss = 0.05865787\n",
      "Iteration 151, loss = 0.06005765\n",
      "Iteration 135, loss = 0.05772639\n",
      "Iteration 152, loss = 0.06261923\n",
      "Iteration 136, loss = 0.05685329\n",
      "Iteration 153, loss = 0.05591018\n",
      "Iteration 137, loss = 0.05625996\n",
      "Iteration 154, loss = 0.05518697\n",
      "Iteration 138, loss = 0.05607248\n",
      "Iteration 155, loss = 0.05677074\n",
      "Iteration 139, loss = 0.05627518\n",
      "Iteration 156, loss = 0.05505631\n",
      "Iteration 140, loss = 0.05451314\n",
      "Iteration 157, loss = 0.05501190\n",
      "Iteration 141, loss = 0.05540015\n",
      "Iteration 158, loss = 0.05323218\n",
      "Iteration 142, loss = 0.05324112\n",
      "Iteration 159, loss = 0.05285811\n",
      "Iteration 143, loss = 0.05257272\n",
      "Iteration 160, loss = 0.05158631\n",
      "Iteration 144, loss = 0.05234847\n",
      "Iteration 161, loss = 0.05253386\n",
      "Iteration 145, loss = 0.05178081\n",
      "Iteration 162, loss = 0.05257485\n",
      "Iteration 146, loss = 0.05089310\n",
      "Iteration 163, loss = 0.05054160\n",
      "Iteration 147, loss = 0.05357990\n",
      "Iteration 164, loss = 0.04988489\n",
      "Iteration 148, loss = 0.05128870\n",
      "Iteration 165, loss = 0.04952826\n",
      "Iteration 149, loss = 0.04833998\n",
      "Iteration 166, loss = 0.04962108\n",
      "Iteration 150, loss = 0.04808958\n",
      "Iteration 167, loss = 0.04909914\n",
      "Iteration 151, loss = 0.04969454\n",
      "Iteration 168, loss = 0.04982808\n",
      "Iteration 152, loss = 0.05227873\n",
      "Iteration 169, loss = 0.05419054\n",
      "Iteration 153, loss = 0.04615997\n",
      "Iteration 170, loss = 0.05934434\n",
      "Iteration 154, loss = 0.04442138\n",
      "Iteration 171, loss = 0.05164873\n",
      "Iteration 155, loss = 0.04475516\n",
      "Iteration 172, loss = 0.04957387\n",
      "Iteration 156, loss = 0.04443212\n",
      "Iteration 173, loss = 0.04984058\n",
      "Iteration 157, loss = 0.04335863\n",
      "Iteration 174, loss = 0.04881116\n",
      "Iteration 158, loss = 0.04339928\n",
      "Iteration 175, loss = 0.04761691\n",
      "Iteration 159, loss = 0.04388167\n",
      "Iteration 176, loss = 0.04793764\n",
      "Iteration 160, loss = 0.04269994\n",
      "Iteration 177, loss = 0.04677139\n",
      "Iteration 161, loss = 0.04113644\n",
      "Iteration 178, loss = 0.04628978\n",
      "Iteration 162, loss = 0.04210973\n",
      "Iteration 179, loss = 0.04427429\n",
      "Iteration 163, loss = 0.04221534\n",
      "Iteration 180, loss = 0.04492002\n",
      "Iteration 164, loss = 0.04080247\n",
      "Iteration 181, loss = 0.04561140\n",
      "Iteration 165, loss = 0.04162584\n",
      "Iteration 182, loss = 0.04506590\n",
      "Iteration 166, loss = 0.03941120\n",
      "Iteration 183, loss = 0.04381323\n",
      "Iteration 167, loss = 0.03915338\n",
      "Iteration 184, loss = 0.04387137\n",
      "Iteration 168, loss = 0.03922591\n",
      "Iteration 185, loss = 0.04274676\n",
      "Iteration 169, loss = 0.03903663\n",
      "Iteration 186, loss = 0.04252674\n",
      "Iteration 170, loss = 0.03685320\n",
      "Iteration 187, loss = 0.04548492\n",
      "Iteration 171, loss = 0.03706600\n",
      "Iteration 188, loss = 0.04538460\n",
      "Iteration 172, loss = 0.03688524\n",
      "Iteration 189, loss = 0.04366870\n",
      "Iteration 173, loss = 0.03685292\n",
      "Iteration 190, loss = 0.04174708\n",
      "Iteration 174, loss = 0.03823753\n",
      "Iteration 191, loss = 0.04071326\n",
      "Iteration 175, loss = 0.03948139\n",
      "Iteration 192, loss = 0.04049505\n",
      "Iteration 176, loss = 0.03623354\n",
      "Iteration 193, loss = 0.03958814\n",
      "Iteration 194, loss = 0.03914235\n",
      "Iteration 177, loss = 0.03646873\n",
      "Iteration 195, loss = 0.03952907\n",
      "Iteration 178, loss = 0.03534026\n",
      "Iteration 196, loss = 0.04002191\n",
      "Iteration 179, loss = 0.03406219\n",
      "Iteration 197, loss = 0.03961035\n",
      "Iteration 180, loss = 0.03322207\n",
      "Iteration 198, loss = 0.03908830\n",
      "Iteration 181, loss = 0.03414480\n",
      "Iteration 199, loss = 0.03810796\n",
      "Iteration 182, loss = 0.03393992\n",
      "Iteration 200, loss = 0.04023988\n",
      "Iteration 183, loss = 0.03333423\n",
      "Iteration 184, loss = 0.03237995\n",
      "Iteration 185, loss = 0.03331089\n",
      "Iteration 186, loss = 0.03503388\n",
      "Iteration 187, loss = 0.03528781\n",
      "Iteration 188, loss = 0.03611092\n",
      "Iteration 189, loss = 0.03384041\n",
      "Iteration 190, loss = 0.03427525\n",
      "Iteration 191, loss = 0.03043775\n",
      "Iteration 192, loss = 0.03181685\n",
      "Iteration 193, loss = 0.03537508\n",
      "Iteration 194, loss = 0.03125144\n",
      "Iteration 195, loss = 0.02752362\n",
      "Iteration 1, loss = 1.17751994\n",
      "Iteration 196, loss = 0.02868563\n",
      "Iteration 2, loss = 0.89459893\n",
      "Iteration 197, loss = 0.03009668\n",
      "Iteration 3, loss = 0.75498961\n",
      "Iteration 198, loss = 0.02832201\n",
      "Iteration 4, loss = 0.63360134\n",
      "Iteration 199, loss = 0.02835723\n",
      "Iteration 5, loss = 0.58349700\n",
      "Iteration 200, loss = 0.02834709\n",
      "Iteration 6, loss = 0.53340827\n",
      "Iteration 7, loss = 0.49485855\n",
      "Iteration 8, loss = 0.47080392\n",
      "Iteration 9, loss = 0.44722593\n",
      "Iteration 10, loss = 0.42784213\n",
      "Iteration 11, loss = 0.41205428\n",
      "Iteration 12, loss = 0.39764003\n",
      "Iteration 13, loss = 0.38519641\n",
      "Iteration 14, loss = 0.37482082\n",
      "Iteration 15, loss = 0.36241623\n",
      "Iteration 16, loss = 0.35259939\n",
      "Iteration 17, loss = 0.34342101\n",
      "Iteration 18, loss = 0.33392751\n",
      "Iteration 1, loss = 1.31980647\n",
      "Iteration 19, loss = 0.32708151\n",
      "Iteration 2, loss = 0.89763981\n",
      "Iteration 20, loss = 0.31821190\n",
      "Iteration 3, loss = 0.70755076\n",
      "Iteration 21, loss = 0.31050811\n",
      "Iteration 4, loss = 0.61016853\n",
      "Iteration 22, loss = 0.30252445\n",
      "Iteration 5, loss = 0.54935718\n",
      "Iteration 23, loss = 0.29900530\n",
      "Iteration 6, loss = 0.50774745\n",
      "Iteration 24, loss = 0.29041003\n",
      "Iteration 7, loss = 0.47512106\n",
      "Iteration 25, loss = 0.28393096\n",
      "Iteration 8, loss = 0.44936475\n",
      "Iteration 26, loss = 0.27736273\n",
      "Iteration 9, loss = 0.42820353\n",
      "Iteration 27, loss = 0.27164267\n",
      "Iteration 10, loss = 0.41153816\n",
      "Iteration 28, loss = 0.26556133\n",
      "Iteration 11, loss = 0.39724566\n",
      "Iteration 29, loss = 0.26022213\n",
      "Iteration 12, loss = 0.38501823\n",
      "Iteration 30, loss = 0.25537741\n",
      "Iteration 13, loss = 0.37408100\n",
      "Iteration 31, loss = 0.24918477\n",
      "Iteration 14, loss = 0.36512993\n",
      "Iteration 32, loss = 0.24489423\n",
      "Iteration 15, loss = 0.35497357\n",
      "Iteration 33, loss = 0.23946637\n",
      "Iteration 16, loss = 0.34769151\n",
      "Iteration 34, loss = 0.23589811\n",
      "Iteration 17, loss = 0.34015651\n",
      "Iteration 35, loss = 0.23073096\n",
      "Iteration 18, loss = 0.33395812\n",
      "Iteration 36, loss = 0.22562412\n",
      "Iteration 19, loss = 0.32543831\n",
      "Iteration 37, loss = 0.22183423\n",
      "Iteration 20, loss = 0.31983208\n",
      "Iteration 38, loss = 0.21846904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.31284012\n",
      "Iteration 39, loss = 0.21417398\n",
      "Iteration 22, loss = 0.30652322\n",
      "Iteration 40, loss = 0.21027782\n",
      "Iteration 23, loss = 0.30167454\n",
      "Iteration 41, loss = 0.20680977\n",
      "Iteration 24, loss = 0.29567976\n",
      "Iteration 42, loss = 0.20276173\n",
      "Iteration 25, loss = 0.29019035\n",
      "Iteration 43, loss = 0.20138439\n",
      "Iteration 26, loss = 0.28510003\n",
      "Iteration 44, loss = 0.19603306\n",
      "Iteration 27, loss = 0.28021746\n",
      "Iteration 45, loss = 0.19317009\n",
      "Iteration 28, loss = 0.27422470\n",
      "Iteration 46, loss = 0.18919859\n",
      "Iteration 29, loss = 0.27018915\n",
      "Iteration 47, loss = 0.18718923\n",
      "Iteration 30, loss = 0.26594194\n",
      "Iteration 48, loss = 0.18359362\n",
      "Iteration 31, loss = 0.26064377\n",
      "Iteration 49, loss = 0.18285786\n",
      "Iteration 32, loss = 0.25532297\n",
      "Iteration 50, loss = 0.17887690\n",
      "Iteration 33, loss = 0.25143506\n",
      "Iteration 51, loss = 0.17631595\n",
      "Iteration 34, loss = 0.24747041\n",
      "Iteration 52, loss = 0.17423160\n",
      "Iteration 35, loss = 0.24344676\n",
      "Iteration 36, loss = 0.24014238\n",
      "Iteration 53, loss = 0.17138398\n",
      "Iteration 37, loss = 0.23585871\n",
      "Iteration 54, loss = 0.16861029\n",
      "Iteration 38, loss = 0.23169588\n",
      "Iteration 55, loss = 0.16581256\n",
      "Iteration 56, loss = 0.16340232\n",
      "Iteration 39, loss = 0.22814980\n",
      "Iteration 57, loss = 0.16224491\n",
      "Iteration 40, loss = 0.22551286\n",
      "Iteration 58, loss = 0.15888336\n",
      "Iteration 41, loss = 0.21964070\n",
      "Iteration 59, loss = 0.15778601\n",
      "Iteration 42, loss = 0.21819137\n",
      "Iteration 43, loss = 0.21521910\n",
      "Iteration 60, loss = 0.15570801\n",
      "Iteration 44, loss = 0.20840162\n",
      "Iteration 61, loss = 0.15214249\n",
      "Iteration 45, loss = 0.20758363\n",
      "Iteration 62, loss = 0.14995397\n",
      "Iteration 63, loss = 0.14797495\n",
      "Iteration 46, loss = 0.20227304\n",
      "Iteration 64, loss = 0.14671288\n",
      "Iteration 47, loss = 0.20087287\n",
      "Iteration 65, loss = 0.14442069\n",
      "Iteration 48, loss = 0.19727399\n",
      "Iteration 66, loss = 0.14367165\n",
      "Iteration 49, loss = 0.19401778\n",
      "Iteration 67, loss = 0.14329939\n",
      "Iteration 50, loss = 0.19270283\n",
      "Iteration 68, loss = 0.13930897\n",
      "Iteration 51, loss = 0.18719307\n",
      "Iteration 69, loss = 0.13908509\n",
      "Iteration 52, loss = 0.18747181\n",
      "Iteration 70, loss = 0.13716584\n",
      "Iteration 53, loss = 0.18201643\n",
      "Iteration 71, loss = 0.13769077\n",
      "Iteration 54, loss = 0.18082617\n",
      "Iteration 72, loss = 0.13370053\n",
      "Iteration 55, loss = 0.17700282\n",
      "Iteration 73, loss = 0.13202308\n",
      "Iteration 56, loss = 0.17657116\n",
      "Iteration 74, loss = 0.13061743\n",
      "Iteration 57, loss = 0.17003548\n",
      "Iteration 75, loss = 0.12954172\n",
      "Iteration 58, loss = 0.17039007\n",
      "Iteration 76, loss = 0.12867272\n",
      "Iteration 59, loss = 0.16690330\n",
      "Iteration 77, loss = 0.12735892\n",
      "Iteration 60, loss = 0.16343939\n",
      "Iteration 78, loss = 0.12545108\n",
      "Iteration 61, loss = 0.16215399\n",
      "Iteration 79, loss = 0.12370831\n",
      "Iteration 62, loss = 0.15911515\n",
      "Iteration 80, loss = 0.12332253\n",
      "Iteration 63, loss = 0.15655858\n",
      "Iteration 81, loss = 0.12001904\n",
      "Iteration 64, loss = 0.15502176\n",
      "Iteration 82, loss = 0.12004070\n",
      "Iteration 65, loss = 0.15219081\n",
      "Iteration 83, loss = 0.12163010\n",
      "Iteration 66, loss = 0.15158827\n",
      "Iteration 84, loss = 0.11656730\n",
      "Iteration 67, loss = 0.14877206\n",
      "Iteration 85, loss = 0.11723234\n",
      "Iteration 68, loss = 0.14728783\n",
      "Iteration 86, loss = 0.11694984\n",
      "Iteration 69, loss = 0.14537060\n",
      "Iteration 87, loss = 0.11435363\n",
      "Iteration 70, loss = 0.14337947\n",
      "Iteration 88, loss = 0.11364332\n",
      "Iteration 71, loss = 0.14423866\n",
      "Iteration 89, loss = 0.11136983\n",
      "Iteration 72, loss = 0.14050188\n",
      "Iteration 90, loss = 0.11173830\n",
      "Iteration 73, loss = 0.13929080\n",
      "Iteration 91, loss = 0.10939021\n",
      "Iteration 74, loss = 0.13512891\n",
      "Iteration 92, loss = 0.11098164\n",
      "Iteration 75, loss = 0.13702702\n",
      "Iteration 93, loss = 0.10986524\n",
      "Iteration 76, loss = 0.13244247\n",
      "Iteration 94, loss = 0.11247137\n",
      "Iteration 77, loss = 0.13094051\n",
      "Iteration 95, loss = 0.10986874\n",
      "Iteration 78, loss = 0.13368321\n",
      "Iteration 96, loss = 0.10582983\n",
      "Iteration 79, loss = 0.13222913\n",
      "Iteration 97, loss = 0.10718981\n",
      "Iteration 80, loss = 0.12618675\n",
      "Iteration 98, loss = 0.10505240\n",
      "Iteration 81, loss = 0.12909328\n",
      "Iteration 99, loss = 0.10530886\n",
      "Iteration 82, loss = 0.12593602\n",
      "Iteration 100, loss = 0.10357115\n",
      "Iteration 83, loss = 0.12149063\n",
      "Iteration 101, loss = 0.10006996\n",
      "Iteration 84, loss = 0.12090504\n",
      "Iteration 102, loss = 0.10299124\n",
      "Iteration 85, loss = 0.11908494\n",
      "Iteration 103, loss = 0.09940784\n",
      "Iteration 86, loss = 0.11678133\n",
      "Iteration 104, loss = 0.09904644\n",
      "Iteration 87, loss = 0.11558839\n",
      "Iteration 105, loss = 0.09520045\n",
      "Iteration 88, loss = 0.11483578\n",
      "Iteration 106, loss = 0.09729505\n",
      "Iteration 89, loss = 0.11494139\n",
      "Iteration 107, loss = 0.09966377\n",
      "Iteration 90, loss = 0.11360405\n",
      "Iteration 108, loss = 0.10009478\n",
      "Iteration 91, loss = 0.11355656\n",
      "Iteration 109, loss = 0.10137611\n",
      "Iteration 92, loss = 0.11042664\n",
      "Iteration 110, loss = 0.09391975\n",
      "Iteration 93, loss = 0.10885385\n",
      "Iteration 111, loss = 0.09599875\n",
      "Iteration 94, loss = 0.11032822\n",
      "Iteration 112, loss = 0.09187920\n",
      "Iteration 95, loss = 0.10699276\n",
      "Iteration 113, loss = 0.09587382\n",
      "Iteration 96, loss = 0.10455965\n",
      "Iteration 114, loss = 0.09207314\n",
      "Iteration 97, loss = 0.10561687\n",
      "Iteration 115, loss = 0.09032063\n",
      "Iteration 98, loss = 0.10437847\n",
      "Iteration 116, loss = 0.09118643\n",
      "Iteration 99, loss = 0.10954718\n",
      "Iteration 117, loss = 0.08661365\n",
      "Iteration 100, loss = 0.10346707\n",
      "Iteration 118, loss = 0.08573070\n",
      "Iteration 101, loss = 0.10060544\n",
      "Iteration 119, loss = 0.08453466\n",
      "Iteration 102, loss = 0.10247253\n",
      "Iteration 120, loss = 0.08522056\n",
      "Iteration 103, loss = 0.09883604\n",
      "Iteration 121, loss = 0.08364077\n",
      "Iteration 104, loss = 0.09806269\n",
      "Iteration 122, loss = 0.08644433\n",
      "Iteration 105, loss = 0.09544884\n",
      "Iteration 123, loss = 0.08755987\n",
      "Iteration 106, loss = 0.09456799\n",
      "Iteration 124, loss = 0.08170845\n",
      "Iteration 107, loss = 0.09406137\n",
      "Iteration 125, loss = 0.08130216\n",
      "Iteration 108, loss = 0.09271528\n",
      "Iteration 126, loss = 0.08032237\n",
      "Iteration 109, loss = 0.09138137\n",
      "Iteration 127, loss = 0.08101574\n",
      "Iteration 110, loss = 0.09129446\n",
      "Iteration 128, loss = 0.08156693\n",
      "Iteration 111, loss = 0.09076953\n",
      "Iteration 129, loss = 0.08634538\n",
      "Iteration 112, loss = 0.08877194\n",
      "Iteration 113, loss = 0.08961572\n",
      "Iteration 130, loss = 0.08053249\n",
      "Iteration 131, loss = 0.07872754\n",
      "Iteration 114, loss = 0.08669087\n",
      "Iteration 132, loss = 0.07825836\n",
      "Iteration 115, loss = 0.08617540\n",
      "Iteration 116, loss = 0.08478068\n",
      "Iteration 133, loss = 0.07595180\n",
      "Iteration 117, loss = 0.08455619\n",
      "Iteration 134, loss = 0.07691224\n",
      "Iteration 118, loss = 0.08636802\n",
      "Iteration 135, loss = 0.08064233\n",
      "Iteration 119, loss = 0.08373438\n",
      "Iteration 136, loss = 0.07038273\n",
      "Iteration 120, loss = 0.08076440\n",
      "Iteration 137, loss = 0.07572673\n",
      "Iteration 121, loss = 0.08180900\n",
      "Iteration 138, loss = 0.08002750\n",
      "Iteration 122, loss = 0.07996347\n",
      "Iteration 139, loss = 0.08244704\n",
      "Iteration 123, loss = 0.08220707\n",
      "Iteration 140, loss = 0.07190258\n",
      "Iteration 124, loss = 0.07967544\n",
      "Iteration 141, loss = 0.07122370\n",
      "Iteration 125, loss = 0.07979117\n",
      "Iteration 142, loss = 0.07018484\n",
      "Iteration 126, loss = 0.08097099\n",
      "Iteration 143, loss = 0.07045610\n",
      "Iteration 127, loss = 0.07643454\n",
      "Iteration 144, loss = 0.07707008\n",
      "Iteration 128, loss = 0.07781962\n",
      "Iteration 145, loss = 0.07170401\n",
      "Iteration 129, loss = 0.07481513\n",
      "Iteration 146, loss = 0.07247440\n",
      "Iteration 130, loss = 0.07221361\n",
      "Iteration 147, loss = 0.06825497\n",
      "Iteration 131, loss = 0.07320235\n",
      "Iteration 148, loss = 0.06785493\n",
      "Iteration 132, loss = 0.07263919\n",
      "Iteration 149, loss = 0.06608325\n",
      "Iteration 133, loss = 0.07225888\n",
      "Iteration 150, loss = 0.06838895\n",
      "Iteration 134, loss = 0.07259160\n",
      "Iteration 151, loss = 0.06366310\n",
      "Iteration 135, loss = 0.07342937\n",
      "Iteration 152, loss = 0.06634134\n",
      "Iteration 136, loss = 0.06824412\n",
      "Iteration 153, loss = 0.06675040\n",
      "Iteration 137, loss = 0.07203461\n",
      "Iteration 154, loss = 0.06279934\n",
      "Iteration 138, loss = 0.06743880\n",
      "Iteration 155, loss = 0.06403952\n",
      "Iteration 139, loss = 0.06708316\n",
      "Iteration 156, loss = 0.06321172\n",
      "Iteration 140, loss = 0.06492843\n",
      "Iteration 157, loss = 0.06488797\n",
      "Iteration 141, loss = 0.06518322\n",
      "Iteration 158, loss = 0.06154949\n",
      "Iteration 142, loss = 0.06659559\n",
      "Iteration 159, loss = 0.05947157\n",
      "Iteration 143, loss = 0.06500563\n",
      "Iteration 160, loss = 0.06129369\n",
      "Iteration 144, loss = 0.06435203\n",
      "Iteration 161, loss = 0.06333569\n",
      "Iteration 162, loss = 0.05918421\n",
      "Iteration 145, loss = 0.06400367\n",
      "Iteration 163, loss = 0.05871212\n",
      "Iteration 146, loss = 0.06223303\n",
      "Iteration 147, loss = 0.06445189\n",
      "Iteration 164, loss = 0.05940125\n",
      "Iteration 148, loss = 0.06413896\n",
      "Iteration 165, loss = 0.05731121\n",
      "Iteration 149, loss = 0.06267807\n",
      "Iteration 166, loss = 0.05652600\n",
      "Iteration 150, loss = 0.05900275\n",
      "Iteration 167, loss = 0.05708209\n",
      "Iteration 151, loss = 0.06108031\n",
      "Iteration 168, loss = 0.05664914\n",
      "Iteration 152, loss = 0.06091071\n",
      "Iteration 169, loss = 0.05491381\n",
      "Iteration 170, loss = 0.05420160\n",
      "Iteration 153, loss = 0.05871227\n",
      "Iteration 171, loss = 0.05417497\n",
      "Iteration 154, loss = 0.05672589\n",
      "Iteration 172, loss = 0.05360687\n",
      "Iteration 155, loss = 0.05704900\n",
      "Iteration 173, loss = 0.05426214\n",
      "Iteration 156, loss = 0.05670763\n",
      "Iteration 174, loss = 0.05290010\n",
      "Iteration 157, loss = 0.06168841\n",
      "Iteration 175, loss = 0.05492127\n",
      "Iteration 158, loss = 0.05970862\n",
      "Iteration 176, loss = 0.05283391\n",
      "Iteration 159, loss = 0.05648411\n",
      "Iteration 177, loss = 0.05206994\n",
      "Iteration 160, loss = 0.05230171\n",
      "Iteration 178, loss = 0.05206302\n",
      "Iteration 161, loss = 0.05548105\n",
      "Iteration 179, loss = 0.05253030\n",
      "Iteration 162, loss = 0.05397283\n",
      "Iteration 180, loss = 0.05009665\n",
      "Iteration 163, loss = 0.05226608\n",
      "Iteration 181, loss = 0.05304870\n",
      "Iteration 164, loss = 0.05415269\n",
      "Iteration 182, loss = 0.05538135\n",
      "Iteration 165, loss = 0.05184576\n",
      "Iteration 183, loss = 0.05127854\n",
      "Iteration 166, loss = 0.05213855\n",
      "Iteration 184, loss = 0.04890665\n",
      "Iteration 167, loss = 0.05212375\n",
      "Iteration 185, loss = 0.04875093\n",
      "Iteration 168, loss = 0.05396460\n",
      "Iteration 186, loss = 0.05030973\n",
      "Iteration 169, loss = 0.05057178\n",
      "Iteration 187, loss = 0.05279963\n",
      "Iteration 170, loss = 0.05118729\n",
      "Iteration 188, loss = 0.04970508\n",
      "Iteration 171, loss = 0.04891710\n",
      "Iteration 189, loss = 0.04878756\n",
      "Iteration 172, loss = 0.04805614\n",
      "Iteration 190, loss = 0.04673806\n",
      "Iteration 173, loss = 0.04982515\n",
      "Iteration 191, loss = 0.04869560\n",
      "Iteration 174, loss = 0.05469277\n",
      "Iteration 192, loss = 0.04585897\n",
      "Iteration 175, loss = 0.05583100\n",
      "Iteration 193, loss = 0.04729665\n",
      "Iteration 176, loss = 0.05285468\n",
      "Iteration 194, loss = 0.04960242\n",
      "Iteration 177, loss = 0.04692485\n",
      "Iteration 195, loss = 0.05143589\n",
      "Iteration 178, loss = 0.04537548\n",
      "Iteration 196, loss = 0.04784986\n",
      "Iteration 179, loss = 0.04746705\n",
      "Iteration 197, loss = 0.04456769\n",
      "Iteration 180, loss = 0.04755302\n",
      "Iteration 198, loss = 0.04384789\n",
      "Iteration 181, loss = 0.04875816\n",
      "Iteration 199, loss = 0.04492573\n",
      "Iteration 182, loss = 0.04743967\n",
      "Iteration 200, loss = 0.04409093\n",
      "Iteration 183, loss = 0.04332769\n",
      "Iteration 184, loss = 0.04261769\n",
      "Iteration 185, loss = 0.04721190\n",
      "Iteration 186, loss = 0.04444768\n",
      "Iteration 187, loss = 0.04210998\n",
      "Iteration 188, loss = 0.04187140\n",
      "Iteration 189, loss = 0.04111512\n",
      "Iteration 190, loss = 0.04152729\n",
      "Iteration 191, loss = 0.04304526\n",
      "Iteration 192, loss = 0.04223063\n",
      "Iteration 193, loss = 0.04010085\n",
      "Iteration 194, loss = 0.04006009\n",
      "Iteration 195, loss = 0.03947529\n",
      "Iteration 1, loss = 1.42746635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 196, loss = 0.03825081\n",
      "Iteration 2, loss = 1.02445431\n",
      "Iteration 197, loss = 0.03833119\n",
      "Iteration 3, loss = 0.81396213\n",
      "Iteration 198, loss = 0.03796153\n",
      "Iteration 4, loss = 0.68272526\n",
      "Iteration 199, loss = 0.03901086\n",
      "Iteration 5, loss = 0.60746476\n",
      "Iteration 200, loss = 0.03903106\n",
      "Iteration 6, loss = 0.55295164\n",
      "Iteration 7, loss = 0.51133712\n",
      "Iteration 8, loss = 0.48194190\n",
      "Iteration 9, loss = 0.45420646\n",
      "Iteration 10, loss = 0.43501385\n",
      "Iteration 11, loss = 0.41686719\n",
      "Iteration 12, loss = 0.40311432\n",
      "Iteration 13, loss = 0.38928250\n",
      "Iteration 14, loss = 0.37753366\n",
      "Iteration 15, loss = 0.36674100\n",
      "Iteration 16, loss = 0.35770054\n",
      "Iteration 17, loss = 0.34796528\n",
      "Iteration 18, loss = 0.34053686\n",
      "Iteration 1, loss = 1.14321749\n",
      "Iteration 19, loss = 0.33197947\n",
      "Iteration 2, loss = 0.80036988\n",
      "Iteration 20, loss = 0.32529603\n",
      "Iteration 3, loss = 0.65763191\n",
      "Iteration 21, loss = 0.31765119\n",
      "Iteration 4, loss = 0.57210103\n",
      "Iteration 22, loss = 0.31090206\n",
      "Iteration 5, loss = 0.51529184\n",
      "Iteration 23, loss = 0.30455220\n",
      "Iteration 6, loss = 0.47771259\n",
      "Iteration 24, loss = 0.29831121\n",
      "Iteration 7, loss = 0.45071664\n",
      "Iteration 25, loss = 0.29270706\n",
      "Iteration 8, loss = 0.42773171\n",
      "Iteration 26, loss = 0.28620422\n",
      "Iteration 9, loss = 0.41113198\n",
      "Iteration 27, loss = 0.28040233\n",
      "Iteration 10, loss = 0.39342662\n",
      "Iteration 28, loss = 0.27431281\n",
      "Iteration 11, loss = 0.38043861\n",
      "Iteration 29, loss = 0.27017172\n",
      "Iteration 12, loss = 0.36855697\n",
      "Iteration 30, loss = 0.26458832\n",
      "Iteration 13, loss = 0.35705166\n",
      "Iteration 31, loss = 0.25861359\n",
      "Iteration 14, loss = 0.34763264\n",
      "Iteration 32, loss = 0.25453356\n",
      "Iteration 15, loss = 0.33805538\n",
      "Iteration 33, loss = 0.24905640\n",
      "Iteration 16, loss = 0.32927762\n",
      "Iteration 34, loss = 0.24372578\n",
      "Iteration 17, loss = 0.32203696\n",
      "Iteration 35, loss = 0.23889147\n",
      "Iteration 18, loss = 0.31353539\n",
      "Iteration 36, loss = 0.23524374\n",
      "Iteration 19, loss = 0.30605371\n",
      "Iteration 37, loss = 0.22919718\n",
      "Iteration 20, loss = 0.29942389\n",
      "Iteration 38, loss = 0.22630985\n",
      "Iteration 21, loss = 0.29174219\n",
      "Iteration 39, loss = 0.22199483\n",
      "Iteration 22, loss = 0.28494483\n",
      "Iteration 40, loss = 0.21799002\n",
      "Iteration 23, loss = 0.27948311\n",
      "Iteration 41, loss = 0.21470881\n",
      "Iteration 24, loss = 0.27355438\n",
      "Iteration 42, loss = 0.21103421\n",
      "Iteration 25, loss = 0.26658350\n",
      "Iteration 43, loss = 0.20688800\n",
      "Iteration 26, loss = 0.26131732\n",
      "Iteration 44, loss = 0.20459006\n",
      "Iteration 27, loss = 0.25620225\n",
      "Iteration 45, loss = 0.19961618\n",
      "Iteration 28, loss = 0.25056829\n",
      "Iteration 46, loss = 0.19677284\n",
      "Iteration 29, loss = 0.24636504\n",
      "Iteration 47, loss = 0.19308670\n",
      "Iteration 30, loss = 0.24094790\n",
      "Iteration 48, loss = 0.19034536\n",
      "Iteration 31, loss = 0.23696913\n",
      "Iteration 49, loss = 0.18667422\n",
      "Iteration 32, loss = 0.23217437\n",
      "Iteration 50, loss = 0.18374125\n",
      "Iteration 33, loss = 0.22807520\n",
      "Iteration 51, loss = 0.18170497\n",
      "Iteration 34, loss = 0.22367312\n",
      "Iteration 52, loss = 0.17854821\n",
      "Iteration 35, loss = 0.22120008\n",
      "Iteration 53, loss = 0.17506017\n",
      "Iteration 36, loss = 0.21501066\n",
      "Iteration 54, loss = 0.17296910\n",
      "Iteration 37, loss = 0.21329915\n",
      "Iteration 55, loss = 0.17077517\n",
      "Iteration 38, loss = 0.21028519\n",
      "Iteration 56, loss = 0.16996163\n",
      "Iteration 39, loss = 0.20444725\n",
      "Iteration 57, loss = 0.16942613\n",
      "Iteration 40, loss = 0.20354244\n",
      "Iteration 58, loss = 0.16210818\n",
      "Iteration 41, loss = 0.20179775\n",
      "Iteration 59, loss = 0.16303906\n",
      "Iteration 42, loss = 0.19551318\n",
      "Iteration 60, loss = 0.16519113\n",
      "Iteration 43, loss = 0.19282991\n",
      "Iteration 61, loss = 0.15494102\n",
      "Iteration 44, loss = 0.18902075\n",
      "Iteration 62, loss = 0.15623987\n",
      "Iteration 45, loss = 0.18727557\n",
      "Iteration 63, loss = 0.15130109\n",
      "Iteration 46, loss = 0.18336891\n",
      "Iteration 64, loss = 0.14981287\n",
      "Iteration 47, loss = 0.18133128\n",
      "Iteration 65, loss = 0.14751306\n",
      "Iteration 48, loss = 0.17848101\n",
      "Iteration 66, loss = 0.14394833\n",
      "Iteration 49, loss = 0.17790892\n",
      "Iteration 67, loss = 0.14375120\n",
      "Iteration 50, loss = 0.17268837\n",
      "Iteration 68, loss = 0.13996962\n",
      "Iteration 51, loss = 0.17131206\n",
      "Iteration 69, loss = 0.13908311\n",
      "Iteration 52, loss = 0.16779258\n",
      "Iteration 70, loss = 0.13786567\n",
      "Iteration 53, loss = 0.16695664\n",
      "Iteration 71, loss = 0.13620156\n",
      "Iteration 54, loss = 0.16418956\n",
      "Iteration 72, loss = 0.13351475\n",
      "Iteration 55, loss = 0.16298192\n",
      "Iteration 73, loss = 0.13233470\n",
      "Iteration 56, loss = 0.16024422\n",
      "Iteration 74, loss = 0.13176103\n",
      "Iteration 57, loss = 0.15676967\n",
      "Iteration 75, loss = 0.12790618\n",
      "Iteration 58, loss = 0.15699608\n",
      "Iteration 76, loss = 0.12720940\n",
      "Iteration 59, loss = 0.15433654\n",
      "Iteration 77, loss = 0.12593105\n",
      "Iteration 60, loss = 0.15157147\n",
      "Iteration 78, loss = 0.12345077\n",
      "Iteration 79, loss = 0.12229636\n",
      "Iteration 61, loss = 0.15051972\n",
      "Iteration 80, loss = 0.12121904\n",
      "Iteration 62, loss = 0.14965039\n",
      "Iteration 81, loss = 0.11950851\n",
      "Iteration 63, loss = 0.14723726\n",
      "Iteration 82, loss = 0.11787391\n",
      "Iteration 64, loss = 0.14571733\n",
      "Iteration 83, loss = 0.11781189\n",
      "Iteration 65, loss = 0.14317267\n",
      "Iteration 84, loss = 0.11389266\n",
      "Iteration 66, loss = 0.14442598\n",
      "Iteration 85, loss = 0.11517711\n",
      "Iteration 67, loss = 0.14348229\n",
      "Iteration 86, loss = 0.11174642\n",
      "Iteration 68, loss = 0.13795256\n",
      "Iteration 87, loss = 0.11053542\n",
      "Iteration 69, loss = 0.13932048\n",
      "Iteration 70, loss = 0.13465962\n",
      "Iteration 88, loss = 0.11229792\n",
      "Iteration 71, loss = 0.13363485\n",
      "Iteration 89, loss = 0.11066184\n",
      "Iteration 72, loss = 0.13242632\n",
      "Iteration 90, loss = 0.10947858\n",
      "Iteration 73, loss = 0.12997343\n",
      "Iteration 91, loss = 0.10698913\n",
      "Iteration 74, loss = 0.13002499\n",
      "Iteration 92, loss = 0.10695699\n",
      "Iteration 75, loss = 0.12975108\n",
      "Iteration 93, loss = 0.10361848\n",
      "Iteration 76, loss = 0.12843742\n",
      "Iteration 94, loss = 0.10363979\n",
      "Iteration 95, loss = 0.10099275\n",
      "Iteration 77, loss = 0.12666645\n",
      "Iteration 78, loss = 0.12455431\n",
      "Iteration 96, loss = 0.10204703\n",
      "Iteration 79, loss = 0.12402192\n",
      "Iteration 97, loss = 0.10145743\n",
      "Iteration 80, loss = 0.12222619\n",
      "Iteration 98, loss = 0.10019527\n",
      "Iteration 81, loss = 0.12062315\n",
      "Iteration 99, loss = 0.09883900\n",
      "Iteration 82, loss = 0.11968167\n",
      "Iteration 100, loss = 0.09638870\n",
      "Iteration 83, loss = 0.11783729\n",
      "Iteration 101, loss = 0.09413935\n",
      "Iteration 84, loss = 0.11642021\n",
      "Iteration 102, loss = 0.09377554\n",
      "Iteration 85, loss = 0.11597093\n",
      "Iteration 103, loss = 0.09586929\n",
      "Iteration 86, loss = 0.11470610\n",
      "Iteration 104, loss = 0.09306196\n",
      "Iteration 87, loss = 0.11151652\n",
      "Iteration 105, loss = 0.09086946\n",
      "Iteration 88, loss = 0.11088808\n",
      "Iteration 106, loss = 0.08922123\n",
      "Iteration 89, loss = 0.11222289\n",
      "Iteration 107, loss = 0.08835563\n",
      "Iteration 90, loss = 0.11523097\n",
      "Iteration 108, loss = 0.08836740\n",
      "Iteration 91, loss = 0.11740196\n",
      "Iteration 109, loss = 0.08713381\n",
      "Iteration 92, loss = 0.11077937\n",
      "Iteration 110, loss = 0.08713303\n",
      "Iteration 93, loss = 0.10885552\n",
      "Iteration 111, loss = 0.08462512\n",
      "Iteration 112, loss = 0.08540542\n",
      "Iteration 94, loss = 0.10809042\n",
      "Iteration 113, loss = 0.08535923\n",
      "Iteration 95, loss = 0.10824946\n",
      "Iteration 114, loss = 0.08413195\n",
      "Iteration 96, loss = 0.10323174\n",
      "Iteration 115, loss = 0.08561048\n",
      "Iteration 97, loss = 0.10232982\n",
      "Iteration 116, loss = 0.08012618\n",
      "Iteration 98, loss = 0.10043567\n",
      "Iteration 117, loss = 0.07968073\n",
      "Iteration 99, loss = 0.10134898\n",
      "Iteration 118, loss = 0.07991477\n",
      "Iteration 100, loss = 0.09946292\n",
      "Iteration 119, loss = 0.08026697\n",
      "Iteration 101, loss = 0.10057153\n",
      "Iteration 120, loss = 0.07663254\n",
      "Iteration 102, loss = 0.09754922\n",
      "Iteration 121, loss = 0.07790748\n",
      "Iteration 103, loss = 0.09619860\n",
      "Iteration 104, loss = 0.09789876\n",
      "Iteration 122, loss = 0.07673945\n",
      "Iteration 123, loss = 0.07609276\n",
      "Iteration 105, loss = 0.09511687\n",
      "Iteration 124, loss = 0.07343516\n",
      "Iteration 106, loss = 0.09329358\n",
      "Iteration 125, loss = 0.07391524\n",
      "Iteration 107, loss = 0.09526284\n",
      "Iteration 126, loss = 0.07376881\n",
      "Iteration 108, loss = 0.09140900\n",
      "Iteration 127, loss = 0.07284690\n",
      "Iteration 109, loss = 0.08959008\n",
      "Iteration 110, loss = 0.09132253\n",
      "Iteration 128, loss = 0.07127275\n",
      "Iteration 129, loss = 0.07174848\n",
      "Iteration 111, loss = 0.08796826\n",
      "Iteration 130, loss = 0.07008638\n",
      "Iteration 112, loss = 0.08910371\n",
      "Iteration 131, loss = 0.06802718\n",
      "Iteration 113, loss = 0.08863148\n",
      "Iteration 132, loss = 0.06953680\n",
      "Iteration 114, loss = 0.08992066\n",
      "Iteration 133, loss = 0.07062120\n",
      "Iteration 115, loss = 0.08886639\n",
      "Iteration 134, loss = 0.06980084\n",
      "Iteration 116, loss = 0.08580618\n",
      "Iteration 135, loss = 0.06921505\n",
      "Iteration 117, loss = 0.08435944\n",
      "Iteration 136, loss = 0.06625962\n",
      "Iteration 118, loss = 0.08235148\n",
      "Iteration 137, loss = 0.06554494\n",
      "Iteration 119, loss = 0.08682550\n",
      "Iteration 138, loss = 0.06489411\n",
      "Iteration 120, loss = 0.08299401\n",
      "Iteration 139, loss = 0.06383464\n",
      "Iteration 121, loss = 0.08247029\n",
      "Iteration 140, loss = 0.06449335\n",
      "Iteration 122, loss = 0.08046609\n",
      "Iteration 141, loss = 0.06503631\n",
      "Iteration 123, loss = 0.08021305\n",
      "Iteration 142, loss = 0.06105005\n",
      "Iteration 124, loss = 0.07725363\n",
      "Iteration 143, loss = 0.06237362\n",
      "Iteration 125, loss = 0.08328795\n",
      "Iteration 144, loss = 0.06195090\n",
      "Iteration 126, loss = 0.07703491\n",
      "Iteration 145, loss = 0.06136886\n",
      "Iteration 127, loss = 0.07563347\n",
      "Iteration 146, loss = 0.06222499\n",
      "Iteration 128, loss = 0.08162730\n",
      "Iteration 147, loss = 0.06264044\n",
      "Iteration 129, loss = 0.08114202\n",
      "Iteration 148, loss = 0.06062652\n",
      "Iteration 130, loss = 0.07915918\n",
      "Iteration 149, loss = 0.05999020\n",
      "Iteration 131, loss = 0.07914165\n",
      "Iteration 150, loss = 0.06008667\n",
      "Iteration 132, loss = 0.07550288\n",
      "Iteration 151, loss = 0.05769490\n",
      "Iteration 133, loss = 0.07085170\n",
      "Iteration 152, loss = 0.05774828\n",
      "Iteration 134, loss = 0.07541368\n",
      "Iteration 153, loss = 0.05555330\n",
      "Iteration 135, loss = 0.07160324\n",
      "Iteration 154, loss = 0.05591996\n",
      "Iteration 136, loss = 0.07125352\n",
      "Iteration 155, loss = 0.05647181\n",
      "Iteration 137, loss = 0.07336524\n",
      "Iteration 156, loss = 0.05578133\n",
      "Iteration 138, loss = 0.06929456\n",
      "Iteration 157, loss = 0.05331944\n",
      "Iteration 139, loss = 0.06763496\n",
      "Iteration 158, loss = 0.05276341\n",
      "Iteration 140, loss = 0.06828171\n",
      "Iteration 159, loss = 0.05387156\n",
      "Iteration 141, loss = 0.06542912\n",
      "Iteration 160, loss = 0.05408818\n",
      "Iteration 142, loss = 0.07091012\n",
      "Iteration 161, loss = 0.05179673\n",
      "Iteration 143, loss = 0.06429027\n",
      "Iteration 162, loss = 0.05462017\n",
      "Iteration 144, loss = 0.06467489\n",
      "Iteration 163, loss = 0.05243520\n",
      "Iteration 145, loss = 0.06299424\n",
      "Iteration 164, loss = 0.05304641\n",
      "Iteration 146, loss = 0.06356677\n",
      "Iteration 165, loss = 0.05464259\n",
      "Iteration 147, loss = 0.06251034\n",
      "Iteration 166, loss = 0.05263462\n",
      "Iteration 148, loss = 0.06340972\n",
      "Iteration 167, loss = 0.05208419\n",
      "Iteration 149, loss = 0.06074282\n",
      "Iteration 168, loss = 0.04914049\n",
      "Iteration 150, loss = 0.06040017\n",
      "Iteration 169, loss = 0.04944450\n",
      "Iteration 151, loss = 0.06134216\n",
      "Iteration 170, loss = 0.04830129\n",
      "Iteration 152, loss = 0.05891556\n",
      "Iteration 171, loss = 0.04893197\n",
      "Iteration 153, loss = 0.06011112\n",
      "Iteration 172, loss = 0.04908934\n",
      "Iteration 154, loss = 0.05835446\n",
      "Iteration 173, loss = 0.04740156\n",
      "Iteration 155, loss = 0.05954179\n",
      "Iteration 174, loss = 0.04689218\n",
      "Iteration 156, loss = 0.05839603\n",
      "Iteration 175, loss = 0.04692597\n",
      "Iteration 157, loss = 0.05954007\n",
      "Iteration 176, loss = 0.04789807\n",
      "Iteration 158, loss = 0.05703519\n",
      "Iteration 177, loss = 0.04963230\n",
      "Iteration 159, loss = 0.05594338\n",
      "Iteration 178, loss = 0.04881257\n",
      "Iteration 160, loss = 0.05745753\n",
      "Iteration 179, loss = 0.04505175\n",
      "Iteration 161, loss = 0.05522761\n",
      "Iteration 180, loss = 0.04561980\n",
      "Iteration 162, loss = 0.05682133\n",
      "Iteration 181, loss = 0.04600607\n",
      "Iteration 163, loss = 0.05949145\n",
      "Iteration 182, loss = 0.04634290\n",
      "Iteration 164, loss = 0.06681093\n",
      "Iteration 183, loss = 0.04463689\n",
      "Iteration 165, loss = 0.05818418\n",
      "Iteration 184, loss = 0.04297366\n",
      "Iteration 166, loss = 0.05457181\n",
      "Iteration 185, loss = 0.04439387\n",
      "Iteration 167, loss = 0.05378691\n",
      "Iteration 186, loss = 0.04511479\n",
      "Iteration 168, loss = 0.05252946\n",
      "Iteration 187, loss = 0.05269520\n",
      "Iteration 169, loss = 0.05226850\n",
      "Iteration 188, loss = 0.05758429\n",
      "Iteration 170, loss = 0.05190532\n",
      "Iteration 189, loss = 0.04667638\n",
      "Iteration 171, loss = 0.05047933\n",
      "Iteration 190, loss = 0.04278740\n",
      "Iteration 172, loss = 0.05217193\n",
      "Iteration 191, loss = 0.04171259\n",
      "Iteration 173, loss = 0.04941313\n",
      "Iteration 192, loss = 0.04220279\n",
      "Iteration 174, loss = 0.04994393\n",
      "Iteration 193, loss = 0.03978942\n",
      "Iteration 175, loss = 0.04962672\n",
      "Iteration 194, loss = 0.04046520\n",
      "Iteration 176, loss = 0.05144774\n",
      "Iteration 195, loss = 0.03996430\n",
      "Iteration 177, loss = 0.04871698\n",
      "Iteration 196, loss = 0.03991917\n",
      "Iteration 178, loss = 0.04830005\n",
      "Iteration 197, loss = 0.03971714\n",
      "Iteration 179, loss = 0.04867282\n",
      "Iteration 198, loss = 0.04443345\n",
      "Iteration 180, loss = 0.05095435\n",
      "Iteration 199, loss = 0.04216555\n",
      "Iteration 181, loss = 0.04568070\n",
      "Iteration 200, loss = 0.04073440\n",
      "Iteration 182, loss = 0.04701937\n",
      "Iteration 201, loss = 0.04529865\n",
      "Iteration 183, loss = 0.05067940\n",
      "Iteration 202, loss = 0.03760502\n",
      "Iteration 184, loss = 0.05218557\n",
      "Iteration 203, loss = 0.03885908\n",
      "Iteration 185, loss = 0.05026865\n",
      "Iteration 204, loss = 0.03739259\n",
      "Iteration 186, loss = 0.04667048\n",
      "Iteration 205, loss = 0.03704868\n",
      "Iteration 187, loss = 0.04806773\n",
      "Iteration 206, loss = 0.03843713\n",
      "Iteration 188, loss = 0.04324320\n",
      "Iteration 207, loss = 0.04069022\n",
      "Iteration 189, loss = 0.04677330\n",
      "Iteration 208, loss = 0.03637054\n",
      "Iteration 190, loss = 0.05037548\n",
      "Iteration 209, loss = 0.03601521\n",
      "Iteration 191, loss = 0.04774552\n",
      "Iteration 210, loss = 0.03518498\n",
      "Iteration 192, loss = 0.04373614\n",
      "Iteration 211, loss = 0.03576026\n",
      "Iteration 193, loss = 0.04206549\n",
      "Iteration 212, loss = 0.03499742\n",
      "Iteration 194, loss = 0.04450340\n",
      "Iteration 213, loss = 0.03639360\n",
      "Iteration 195, loss = 0.04578053\n",
      "Iteration 214, loss = 0.03781829\n",
      "Iteration 196, loss = 0.04137974\n",
      "Iteration 215, loss = 0.03905787\n",
      "Iteration 197, loss = 0.04177427\n",
      "Iteration 216, loss = 0.03413321\n",
      "Iteration 198, loss = 0.04076385\n",
      "Iteration 217, loss = 0.03397738\n",
      "Iteration 199, loss = 0.04078594\n",
      "Iteration 218, loss = 0.03388887\n",
      "Iteration 200, loss = 0.04012328\n",
      "Iteration 219, loss = 0.03597511\n",
      "Iteration 201, loss = 0.04081231\n",
      "Iteration 220, loss = 0.03391243\n",
      "Iteration 202, loss = 0.04448527\n",
      "Iteration 221, loss = 0.03318256\n",
      "Iteration 203, loss = 0.04571968\n",
      "Iteration 222, loss = 0.03274279\n",
      "Iteration 204, loss = 0.04214981\n",
      "Iteration 223, loss = 0.03228259\n",
      "Iteration 205, loss = 0.04421727\n",
      "Iteration 224, loss = 0.03264159\n",
      "Iteration 206, loss = 0.04277947\n",
      "Iteration 225, loss = 0.03403826\n",
      "Iteration 207, loss = 0.04182653\n",
      "Iteration 226, loss = 0.03186858\n",
      "Iteration 208, loss = 0.03735057\n",
      "Iteration 227, loss = 0.03412618\n",
      "Iteration 209, loss = 0.04165529\n",
      "Iteration 228, loss = 0.03391425\n",
      "Iteration 210, loss = 0.04162933\n",
      "Iteration 229, loss = 0.03034778\n",
      "Iteration 211, loss = 0.04777832\n",
      "Iteration 230, loss = 0.03222806\n",
      "Iteration 212, loss = 0.04774698\n",
      "Iteration 231, loss = 0.03015810\n",
      "Iteration 213, loss = 0.04462372\n",
      "Iteration 232, loss = 0.03063669\n",
      "Iteration 214, loss = 0.04161748\n",
      "Iteration 233, loss = 0.03077237\n",
      "Iteration 215, loss = 0.03982171\n",
      "Iteration 234, loss = 0.03095306\n",
      "Iteration 216, loss = 0.04006657\n",
      "Iteration 235, loss = 0.02924842\n",
      "Iteration 217, loss = 0.04048744\n",
      "Iteration 236, loss = 0.02998354\n",
      "Iteration 218, loss = 0.03678409\n",
      "Iteration 237, loss = 0.03226173\n",
      "Iteration 219, loss = 0.03644734\n",
      "Iteration 238, loss = 0.03137631\n",
      "Iteration 220, loss = 0.03654018\n",
      "Iteration 239, loss = 0.03108339\n",
      "Iteration 221, loss = 0.03560649\n",
      "Iteration 240, loss = 0.02973631\n",
      "Iteration 222, loss = 0.03582784\n",
      "Iteration 241, loss = 0.02816235\n",
      "Iteration 223, loss = 0.03521507\n",
      "Iteration 242, loss = 0.02878728\n",
      "Iteration 224, loss = 0.03416906\n",
      "Iteration 243, loss = 0.02892575\n",
      "Iteration 225, loss = 0.03344733\n",
      "Iteration 244, loss = 0.02878866\n",
      "Iteration 226, loss = 0.03345564\n",
      "Iteration 245, loss = 0.02832015\n",
      "Iteration 227, loss = 0.03305219\n",
      "Iteration 246, loss = 0.02841163\n",
      "Iteration 228, loss = 0.03446409\n",
      "Iteration 247, loss = 0.02796643\n",
      "Iteration 229, loss = 0.03348192\n",
      "Iteration 248, loss = 0.02919692\n",
      "Iteration 230, loss = 0.03271542\n",
      "Iteration 249, loss = 0.03089040\n",
      "Iteration 231, loss = 0.03199559\n",
      "Iteration 250, loss = 0.03107714\n",
      "Iteration 232, loss = 0.03280533\n",
      "Iteration 251, loss = 0.02906478\n",
      "Iteration 233, loss = 0.03219560\n",
      "Iteration 252, loss = 0.03037310\n",
      "Iteration 234, loss = 0.03257347\n",
      "Iteration 253, loss = 0.02704060\n",
      "Iteration 235, loss = 0.03211614\n",
      "Iteration 254, loss = 0.02761590\n",
      "Iteration 236, loss = 0.03161930\n",
      "Iteration 255, loss = 0.02765751\n",
      "Iteration 237, loss = 0.03235542\n",
      "Iteration 256, loss = 0.02726452\n",
      "Iteration 238, loss = 0.03144343\n",
      "Iteration 257, loss = 0.02724348\n",
      "Iteration 239, loss = 0.03072215\n",
      "Iteration 258, loss = 0.02574506\n",
      "Iteration 240, loss = 0.03175761\n",
      "Iteration 259, loss = 0.02589957\n",
      "Iteration 241, loss = 0.03057035\n",
      "Iteration 260, loss = 0.02645994\n",
      "Iteration 242, loss = 0.02987473\n",
      "Iteration 261, loss = 0.02550183\n",
      "Iteration 243, loss = 0.03021128\n",
      "Iteration 262, loss = 0.02594276\n",
      "Iteration 244, loss = 0.02943190\n",
      "Iteration 263, loss = 0.02550616\n",
      "Iteration 245, loss = 0.02935579\n",
      "Iteration 264, loss = 0.02570452\n",
      "Iteration 246, loss = 0.03191502\n",
      "Iteration 265, loss = 0.02493020\n",
      "Iteration 247, loss = 0.03406188\n",
      "Iteration 266, loss = 0.02573994\n",
      "Iteration 248, loss = 0.03350121\n",
      "Iteration 267, loss = 0.02434884\n",
      "Iteration 249, loss = 0.04100431\n",
      "Iteration 268, loss = 0.02556013\n",
      "Iteration 250, loss = 0.03373651\n",
      "Iteration 269, loss = 0.02510708\n",
      "Iteration 251, loss = 0.03333626\n",
      "Iteration 270, loss = 0.02663639\n",
      "Iteration 252, loss = 0.02894192\n",
      "Iteration 271, loss = 0.03445935\n",
      "Iteration 253, loss = 0.03124098\n",
      "Iteration 272, loss = 0.02821509\n",
      "Iteration 254, loss = 0.03276195\n",
      "Iteration 273, loss = 0.02541261\n",
      "Iteration 255, loss = 0.03304975\n",
      "Iteration 274, loss = 0.02397444\n",
      "Iteration 256, loss = 0.03128169\n",
      "Iteration 275, loss = 0.02415781\n",
      "Iteration 257, loss = 0.03129013\n",
      "Iteration 276, loss = 0.02369350\n",
      "Iteration 258, loss = 0.02797432\n",
      "Iteration 277, loss = 0.02628265\n",
      "Iteration 259, loss = 0.03036900\n",
      "Iteration 278, loss = 0.02539068\n",
      "Iteration 260, loss = 0.02760812\n",
      "Iteration 279, loss = 0.02515496\n",
      "Iteration 261, loss = 0.02786554\n",
      "Iteration 280, loss = 0.02406621\n",
      "Iteration 262, loss = 0.02741846\n",
      "Iteration 281, loss = 0.02284848\n",
      "Iteration 263, loss = 0.02834707\n",
      "Iteration 282, loss = 0.02284787\n",
      "Iteration 264, loss = 0.02866268\n",
      "Iteration 283, loss = 0.02410241\n",
      "Iteration 265, loss = 0.02769428\n",
      "Iteration 284, loss = 0.02281213\n",
      "Iteration 266, loss = 0.02869648\n",
      "Iteration 285, loss = 0.02364080\n",
      "Iteration 267, loss = 0.03103111\n",
      "Iteration 286, loss = 0.02231090\n",
      "Iteration 268, loss = 0.03013987\n",
      "Iteration 287, loss = 0.02180770\n",
      "Iteration 269, loss = 0.02928601\n",
      "Iteration 288, loss = 0.02159957\n",
      "Iteration 270, loss = 0.02603125\n",
      "Iteration 289, loss = 0.02188271\n",
      "Iteration 271, loss = 0.02648365\n",
      "Iteration 290, loss = 0.02342565\n",
      "Iteration 272, loss = 0.02637372\n",
      "Iteration 291, loss = 0.02400974\n",
      "Iteration 273, loss = 0.02553314\n",
      "Iteration 292, loss = 0.02330731\n",
      "Iteration 274, loss = 0.02719385\n",
      "Iteration 293, loss = 0.02279165\n",
      "Iteration 275, loss = 0.02424175\n",
      "Iteration 294, loss = 0.02465548\n",
      "Iteration 276, loss = 0.02553247\n",
      "Iteration 295, loss = 0.02176411\n",
      "Iteration 277, loss = 0.02620370\n",
      "Iteration 296, loss = 0.02363492\n",
      "Iteration 297, loss = 0.02035401\n",
      "Iteration 278, loss = 0.02567316\n",
      "Iteration 298, loss = 0.02126745\n",
      "Iteration 279, loss = 0.02453296\n",
      "Iteration 299, loss = 0.02300649\n",
      "Iteration 280, loss = 0.02406764\n",
      "Iteration 281, loss = 0.02441002\n",
      "Iteration 300, loss = 0.02448569\n",
      "Iteration 282, loss = 0.02427937\n",
      "Iteration 301, loss = 0.02051737\n",
      "Iteration 283, loss = 0.02370862\n",
      "Iteration 302, loss = 0.02083347\n",
      "Iteration 284, loss = 0.02420535\n",
      "Iteration 303, loss = 0.02060230\n",
      "Iteration 285, loss = 0.02377639\n",
      "Iteration 304, loss = 0.02109934\n",
      "Iteration 286, loss = 0.02322658\n",
      "Iteration 305, loss = 0.02300904\n",
      "Iteration 287, loss = 0.02350662\n",
      "Iteration 306, loss = 0.02419887\n",
      "Iteration 288, loss = 0.02375124\n",
      "Iteration 307, loss = 0.02376449\n",
      "Iteration 289, loss = 0.02342247\n",
      "Iteration 308, loss = 0.02167967\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 290, loss = 0.02793288\n",
      "Iteration 291, loss = 0.03356579\n",
      "Iteration 292, loss = 0.03196085\n",
      "Iteration 293, loss = 0.02315545\n",
      "Iteration 294, loss = 0.02318757\n",
      "Iteration 295, loss = 0.02235665\n",
      "Iteration 296, loss = 0.02202242\n",
      "Iteration 297, loss = 0.02233667\n",
      "Iteration 298, loss = 0.02490584\n",
      "Iteration 299, loss = 0.02212631\n",
      "Iteration 300, loss = 0.02170566\n",
      "Iteration 301, loss = 0.02149679\n",
      "Iteration 302, loss = 0.02342668\n",
      "Iteration 1, loss = 1.07867942\n",
      "Iteration 303, loss = 0.02207573\n",
      "Iteration 2, loss = 0.78757856\n",
      "Iteration 304, loss = 0.02185440\n",
      "Iteration 3, loss = 0.64774422\n",
      "Iteration 305, loss = 0.02587195\n",
      "Iteration 4, loss = 0.56630175\n",
      "Iteration 306, loss = 0.02775090\n",
      "Iteration 5, loss = 0.50766922\n",
      "Iteration 307, loss = 0.03270523\n",
      "Iteration 6, loss = 0.46753368\n",
      "Iteration 308, loss = 0.03882676\n",
      "Iteration 7, loss = 0.43955142\n",
      "Iteration 309, loss = 0.03073074\n",
      "Iteration 8, loss = 0.41597169\n",
      "Iteration 310, loss = 0.02440370\n",
      "Iteration 9, loss = 0.39726204\n",
      "Iteration 311, loss = 0.02213487\n",
      "Iteration 10, loss = 0.38253944\n",
      "Iteration 312, loss = 0.02173415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.37010587\n",
      "Iteration 12, loss = 0.35868695\n",
      "Iteration 13, loss = 0.34897332\n",
      "Iteration 14, loss = 0.34030586\n",
      "Iteration 15, loss = 0.33106103\n",
      "Iteration 16, loss = 0.32394820\n",
      "Iteration 17, loss = 0.31675195\n",
      "Iteration 18, loss = 0.31195673\n",
      "Iteration 19, loss = 0.30307201\n",
      "Iteration 20, loss = 0.29752289\n",
      "Iteration 21, loss = 0.29231228\n",
      "Iteration 22, loss = 0.28589600\n",
      "Iteration 23, loss = 0.28072664\n",
      "Iteration 1, loss = 1.24011595\n",
      "Iteration 24, loss = 0.27492196\n",
      "Iteration 2, loss = 0.90444579\n",
      "Iteration 25, loss = 0.26983273\n",
      "Iteration 3, loss = 0.72968205\n",
      "Iteration 26, loss = 0.26453743\n",
      "Iteration 4, loss = 0.61603381\n",
      "Iteration 27, loss = 0.25891648\n",
      "Iteration 5, loss = 0.55885188\n",
      "Iteration 28, loss = 0.25505886\n",
      "Iteration 6, loss = 0.50827529\n",
      "Iteration 29, loss = 0.25108829\n",
      "Iteration 7, loss = 0.47854768\n",
      "Iteration 30, loss = 0.24585902\n",
      "Iteration 8, loss = 0.45029067\n",
      "Iteration 31, loss = 0.24205644\n",
      "Iteration 9, loss = 0.43215795\n",
      "Iteration 32, loss = 0.23697147\n",
      "Iteration 10, loss = 0.41400764\n",
      "Iteration 33, loss = 0.23488848\n",
      "Iteration 34, loss = 0.22925117\n",
      "Iteration 11, loss = 0.40043945\n",
      "Iteration 35, loss = 0.22548410\n",
      "Iteration 12, loss = 0.38727783\n",
      "Iteration 36, loss = 0.22111932\n",
      "Iteration 13, loss = 0.37565151\n",
      "Iteration 37, loss = 0.21743841\n",
      "Iteration 14, loss = 0.36492693\n",
      "Iteration 38, loss = 0.21404232\n",
      "Iteration 15, loss = 0.35462384\n",
      "Iteration 39, loss = 0.21077166\n",
      "Iteration 16, loss = 0.34656902\n",
      "Iteration 40, loss = 0.20877985\n",
      "Iteration 17, loss = 0.33711024\n",
      "Iteration 41, loss = 0.20442256\n",
      "Iteration 18, loss = 0.32854234\n",
      "Iteration 42, loss = 0.20070315\n",
      "Iteration 19, loss = 0.32162451\n",
      "Iteration 43, loss = 0.19677031\n",
      "Iteration 20, loss = 0.31474038\n",
      "Iteration 44, loss = 0.19486569\n",
      "Iteration 21, loss = 0.30701122\n",
      "Iteration 45, loss = 0.19150080\n",
      "Iteration 22, loss = 0.30091904\n",
      "Iteration 46, loss = 0.18763854\n",
      "Iteration 23, loss = 0.29298375\n",
      "Iteration 47, loss = 0.18709370\n",
      "Iteration 24, loss = 0.28690341\n",
      "Iteration 48, loss = 0.18364082\n",
      "Iteration 25, loss = 0.28044315\n",
      "Iteration 49, loss = 0.18088484\n",
      "Iteration 26, loss = 0.27469936\n",
      "Iteration 50, loss = 0.17753149\n",
      "Iteration 27, loss = 0.26903760\n",
      "Iteration 51, loss = 0.17524892\n",
      "Iteration 28, loss = 0.26449323\n",
      "Iteration 52, loss = 0.17526105\n",
      "Iteration 29, loss = 0.26166446\n",
      "Iteration 53, loss = 0.16879793\n",
      "Iteration 30, loss = 0.25335606\n",
      "Iteration 54, loss = 0.16935547\n",
      "Iteration 31, loss = 0.24873713\n",
      "Iteration 55, loss = 0.16598676\n",
      "Iteration 32, loss = 0.24479561\n",
      "Iteration 56, loss = 0.16238100\n",
      "Iteration 33, loss = 0.23985773\n",
      "Iteration 57, loss = 0.15966874\n",
      "Iteration 34, loss = 0.23562202\n",
      "Iteration 58, loss = 0.15697790\n",
      "Iteration 35, loss = 0.23246502\n",
      "Iteration 59, loss = 0.15494410\n",
      "Iteration 36, loss = 0.22751068\n",
      "Iteration 60, loss = 0.15461868\n",
      "Iteration 37, loss = 0.22439227\n",
      "Iteration 61, loss = 0.15054014\n",
      "Iteration 38, loss = 0.21784703\n",
      "Iteration 62, loss = 0.14945720\n",
      "Iteration 39, loss = 0.21583004\n",
      "Iteration 63, loss = 0.14738560\n",
      "Iteration 40, loss = 0.20975003\n",
      "Iteration 64, loss = 0.14486453\n",
      "Iteration 41, loss = 0.20947083\n",
      "Iteration 65, loss = 0.14681923\n",
      "Iteration 42, loss = 0.20401064\n",
      "Iteration 66, loss = 0.14303142\n",
      "Iteration 43, loss = 0.20053025\n",
      "Iteration 67, loss = 0.14415447\n",
      "Iteration 44, loss = 0.19741453\n",
      "Iteration 68, loss = 0.14435565\n",
      "Iteration 45, loss = 0.19385912\n",
      "Iteration 69, loss = 0.13573807\n",
      "Iteration 46, loss = 0.19052705\n",
      "Iteration 70, loss = 0.13689828\n",
      "Iteration 47, loss = 0.18736616\n",
      "Iteration 71, loss = 0.13428762\n",
      "Iteration 48, loss = 0.18485696\n",
      "Iteration 72, loss = 0.13040739\n",
      "Iteration 49, loss = 0.18224190\n",
      "Iteration 73, loss = 0.13057817\n",
      "Iteration 50, loss = 0.17925082\n",
      "Iteration 74, loss = 0.12770734\n",
      "Iteration 51, loss = 0.17720880\n",
      "Iteration 75, loss = 0.12653478\n",
      "Iteration 52, loss = 0.17605516\n",
      "Iteration 76, loss = 0.12393685\n",
      "Iteration 53, loss = 0.17147163\n",
      "Iteration 77, loss = 0.12340988\n",
      "Iteration 54, loss = 0.16974623\n",
      "Iteration 78, loss = 0.12154114\n",
      "Iteration 55, loss = 0.16738233\n",
      "Iteration 79, loss = 0.12065174\n",
      "Iteration 56, loss = 0.16748005\n",
      "Iteration 80, loss = 0.11828851\n",
      "Iteration 57, loss = 0.16307468\n",
      "Iteration 81, loss = 0.11771356\n",
      "Iteration 58, loss = 0.16135822\n",
      "Iteration 82, loss = 0.11532522\n",
      "Iteration 59, loss = 0.15883763\n",
      "Iteration 83, loss = 0.11457241\n",
      "Iteration 60, loss = 0.15560921\n",
      "Iteration 84, loss = 0.11292057\n",
      "Iteration 61, loss = 0.15499695\n",
      "Iteration 85, loss = 0.11032765\n",
      "Iteration 62, loss = 0.15182881\n",
      "Iteration 86, loss = 0.11324311\n",
      "Iteration 63, loss = 0.14955528\n",
      "Iteration 87, loss = 0.11037786\n",
      "Iteration 64, loss = 0.14777281\n",
      "Iteration 88, loss = 0.10728544\n",
      "Iteration 65, loss = 0.14683848\n",
      "Iteration 89, loss = 0.10709078\n",
      "Iteration 66, loss = 0.14333059\n",
      "Iteration 90, loss = 0.10438760\n",
      "Iteration 67, loss = 0.14267172\n",
      "Iteration 91, loss = 0.10416389\n",
      "Iteration 68, loss = 0.14281590\n",
      "Iteration 92, loss = 0.10348770\n",
      "Iteration 69, loss = 0.13825448\n",
      "Iteration 93, loss = 0.10044745\n",
      "Iteration 70, loss = 0.13639740\n",
      "Iteration 94, loss = 0.09986184\n",
      "Iteration 71, loss = 0.13881966\n",
      "Iteration 95, loss = 0.09726204\n",
      "Iteration 72, loss = 0.13164647\n",
      "Iteration 96, loss = 0.09767712\n",
      "Iteration 73, loss = 0.13219172\n",
      "Iteration 97, loss = 0.09571971\n",
      "Iteration 74, loss = 0.12980981\n",
      "Iteration 98, loss = 0.09483576\n",
      "Iteration 75, loss = 0.12849220\n",
      "Iteration 99, loss = 0.09332860\n",
      "Iteration 76, loss = 0.12644742\n",
      "Iteration 100, loss = 0.09503403\n",
      "Iteration 77, loss = 0.12531968\n",
      "Iteration 101, loss = 0.09160222\n",
      "Iteration 78, loss = 0.12350163\n",
      "Iteration 102, loss = 0.09077910\n",
      "Iteration 79, loss = 0.12352984\n",
      "Iteration 103, loss = 0.08994727\n",
      "Iteration 80, loss = 0.12026509\n",
      "Iteration 104, loss = 0.08885017\n",
      "Iteration 81, loss = 0.12147564\n",
      "Iteration 105, loss = 0.08530508\n",
      "Iteration 82, loss = 0.11937901\n",
      "Iteration 106, loss = 0.08651976\n",
      "Iteration 83, loss = 0.11764682\n",
      "Iteration 107, loss = 0.08587662\n",
      "Iteration 84, loss = 0.11745231\n",
      "Iteration 108, loss = 0.08719115\n",
      "Iteration 85, loss = 0.11354532\n",
      "Iteration 109, loss = 0.08437804\n",
      "Iteration 86, loss = 0.11389959\n",
      "Iteration 110, loss = 0.08015828\n",
      "Iteration 87, loss = 0.11259888\n",
      "Iteration 111, loss = 0.08128563\n",
      "Iteration 88, loss = 0.11351040\n",
      "Iteration 112, loss = 0.08129310\n",
      "Iteration 89, loss = 0.11213531\n",
      "Iteration 113, loss = 0.08207962\n",
      "Iteration 90, loss = 0.11385042\n",
      "Iteration 114, loss = 0.07842929\n",
      "Iteration 91, loss = 0.10833085\n",
      "Iteration 115, loss = 0.07663300\n",
      "Iteration 92, loss = 0.10598942\n",
      "Iteration 116, loss = 0.07533693\n",
      "Iteration 93, loss = 0.10587316\n",
      "Iteration 117, loss = 0.07363260\n",
      "Iteration 94, loss = 0.10508718\n",
      "Iteration 118, loss = 0.07281600\n",
      "Iteration 95, loss = 0.10410417\n",
      "Iteration 119, loss = 0.07172901\n",
      "Iteration 96, loss = 0.10440841\n",
      "Iteration 120, loss = 0.07367256\n",
      "Iteration 97, loss = 0.10340227\n",
      "Iteration 121, loss = 0.07303106\n",
      "Iteration 98, loss = 0.09878218\n",
      "Iteration 122, loss = 0.07018797\n",
      "Iteration 99, loss = 0.10212840\n",
      "Iteration 123, loss = 0.07084322\n",
      "Iteration 100, loss = 0.09906471\n",
      "Iteration 124, loss = 0.07033046\n",
      "Iteration 101, loss = 0.09800260\n",
      "Iteration 125, loss = 0.07007067\n",
      "Iteration 102, loss = 0.09881682\n",
      "Iteration 126, loss = 0.06966398\n",
      "Iteration 103, loss = 0.09331735\n",
      "Iteration 127, loss = 0.06562529\n",
      "Iteration 104, loss = 0.09370799\n",
      "Iteration 128, loss = 0.06588404\n",
      "Iteration 105, loss = 0.09343933\n",
      "Iteration 129, loss = 0.06310262\n",
      "Iteration 106, loss = 0.09361083\n",
      "Iteration 130, loss = 0.06399245\n",
      "Iteration 107, loss = 0.09026905\n",
      "Iteration 131, loss = 0.06388034\n",
      "Iteration 108, loss = 0.08877396\n",
      "Iteration 132, loss = 0.06087093\n",
      "Iteration 109, loss = 0.08834525\n",
      "Iteration 133, loss = 0.06086148\n",
      "Iteration 110, loss = 0.08715031\n",
      "Iteration 134, loss = 0.06108446\n",
      "Iteration 111, loss = 0.08578963\n",
      "Iteration 135, loss = 0.06135834\n",
      "Iteration 112, loss = 0.08693240\n",
      "Iteration 136, loss = 0.05977346\n",
      "Iteration 113, loss = 0.08379504\n",
      "Iteration 137, loss = 0.05905511\n",
      "Iteration 114, loss = 0.08318459\n",
      "Iteration 138, loss = 0.05871271\n",
      "Iteration 115, loss = 0.08232768\n",
      "Iteration 139, loss = 0.05696190\n",
      "Iteration 116, loss = 0.08182398\n",
      "Iteration 140, loss = 0.05826001\n",
      "Iteration 117, loss = 0.08082945\n",
      "Iteration 141, loss = 0.05819128\n",
      "Iteration 118, loss = 0.08026605\n",
      "Iteration 142, loss = 0.05536986\n",
      "Iteration 119, loss = 0.07975525\n",
      "Iteration 143, loss = 0.05494883\n",
      "Iteration 120, loss = 0.07985871\n",
      "Iteration 144, loss = 0.05420127\n",
      "Iteration 121, loss = 0.07734680\n",
      "Iteration 145, loss = 0.05386712\n",
      "Iteration 122, loss = 0.07654514\n",
      "Iteration 146, loss = 0.05329536\n",
      "Iteration 123, loss = 0.07606185\n",
      "Iteration 147, loss = 0.05270748\n",
      "Iteration 124, loss = 0.07579157\n",
      "Iteration 148, loss = 0.05194246\n",
      "Iteration 125, loss = 0.07578562\n",
      "Iteration 149, loss = 0.05276883\n",
      "Iteration 126, loss = 0.07478701\n",
      "Iteration 150, loss = 0.05199403\n",
      "Iteration 127, loss = 0.07410161\n",
      "Iteration 151, loss = 0.05230807\n",
      "Iteration 128, loss = 0.07306718\n",
      "Iteration 152, loss = 0.05105279\n",
      "Iteration 129, loss = 0.07437430\n",
      "Iteration 153, loss = 0.05057248\n",
      "Iteration 130, loss = 0.07568187\n",
      "Iteration 154, loss = 0.05043412\n",
      "Iteration 131, loss = 0.07263720\n",
      "Iteration 155, loss = 0.04911415\n",
      "Iteration 132, loss = 0.07326201\n",
      "Iteration 156, loss = 0.04919853\n",
      "Iteration 133, loss = 0.07854481\n",
      "Iteration 157, loss = 0.05187086\n",
      "Iteration 134, loss = 0.07040322\n",
      "Iteration 158, loss = 0.04895842\n",
      "Iteration 135, loss = 0.07124757\n",
      "Iteration 159, loss = 0.04683822\n",
      "Iteration 136, loss = 0.06818649\n",
      "Iteration 160, loss = 0.04654803\n",
      "Iteration 137, loss = 0.06657502\n",
      "Iteration 161, loss = 0.04576956\n",
      "Iteration 138, loss = 0.06574667\n",
      "Iteration 162, loss = 0.04585903\n",
      "Iteration 139, loss = 0.06843554\n",
      "Iteration 163, loss = 0.04567094\n",
      "Iteration 140, loss = 0.06897350\n",
      "Iteration 164, loss = 0.04691645\n",
      "Iteration 141, loss = 0.06238573\n",
      "Iteration 165, loss = 0.04486873\n",
      "Iteration 142, loss = 0.06407936\n",
      "Iteration 166, loss = 0.04487401\n",
      "Iteration 143, loss = 0.06272006\n",
      "Iteration 167, loss = 0.04609951\n",
      "Iteration 144, loss = 0.06340018\n",
      "Iteration 168, loss = 0.05447890\n",
      "Iteration 145, loss = 0.06143213\n",
      "Iteration 169, loss = 0.05281159\n",
      "Iteration 146, loss = 0.06156134\n",
      "Iteration 170, loss = 0.05147936\n",
      "Iteration 147, loss = 0.06041728\n",
      "Iteration 171, loss = 0.05034231\n",
      "Iteration 148, loss = 0.06097245\n",
      "Iteration 172, loss = 0.04348910\n",
      "Iteration 149, loss = 0.06074048\n",
      "Iteration 173, loss = 0.04345090\n",
      "Iteration 150, loss = 0.05928302\n",
      "Iteration 174, loss = 0.04172764\n",
      "Iteration 151, loss = 0.05832078\n",
      "Iteration 175, loss = 0.04301174\n",
      "Iteration 152, loss = 0.05852653\n",
      "Iteration 176, loss = 0.04021157\n",
      "Iteration 153, loss = 0.05700759\n",
      "Iteration 177, loss = 0.04049339\n",
      "Iteration 154, loss = 0.05794383\n",
      "Iteration 178, loss = 0.04596620\n",
      "Iteration 155, loss = 0.05635665\n",
      "Iteration 179, loss = 0.04173046\n",
      "Iteration 156, loss = 0.06088487\n",
      "Iteration 180, loss = 0.04007787\n",
      "Iteration 157, loss = 0.05901201\n",
      "Iteration 181, loss = 0.04227323\n",
      "Iteration 158, loss = 0.05700246\n",
      "Iteration 182, loss = 0.04316770\n",
      "Iteration 159, loss = 0.05507744\n",
      "Iteration 183, loss = 0.04336123\n",
      "Iteration 160, loss = 0.05394294\n",
      "Iteration 184, loss = 0.03936603\n",
      "Iteration 161, loss = 0.05241674\n",
      "Iteration 185, loss = 0.03801526\n",
      "Iteration 162, loss = 0.05209080\n",
      "Iteration 186, loss = 0.04248595\n",
      "Iteration 163, loss = 0.05243884\n",
      "Iteration 187, loss = 0.04174477\n",
      "Iteration 164, loss = 0.05126941\n",
      "Iteration 188, loss = 0.04169681\n",
      "Iteration 165, loss = 0.05179416\n",
      "Iteration 189, loss = 0.04324184\n",
      "Iteration 166, loss = 0.05312491\n",
      "Iteration 190, loss = 0.03949372\n",
      "Iteration 167, loss = 0.05069324\n",
      "Iteration 191, loss = 0.03714981\n",
      "Iteration 168, loss = 0.05055670\n",
      "Iteration 192, loss = 0.03794509\n",
      "Iteration 169, loss = 0.04959733\n",
      "Iteration 193, loss = 0.03937763\n",
      "Iteration 170, loss = 0.05033585\n",
      "Iteration 194, loss = 0.03913865\n",
      "Iteration 171, loss = 0.05088374\n",
      "Iteration 195, loss = 0.03682413\n",
      "Iteration 172, loss = 0.05643120\n",
      "Iteration 196, loss = 0.03633679\n",
      "Iteration 173, loss = 0.04810711\n",
      "Iteration 197, loss = 0.03998434\n",
      "Iteration 174, loss = 0.04851552\n",
      "Iteration 198, loss = 0.03830320\n",
      "Iteration 175, loss = 0.05153854\n",
      "Iteration 199, loss = 0.03504345\n",
      "Iteration 176, loss = 0.05130045\n",
      "Iteration 200, loss = 0.03647340\n",
      "Iteration 177, loss = 0.05240550\n",
      "Iteration 201, loss = 0.04146446\n",
      "Iteration 178, loss = 0.04638051\n",
      "Iteration 202, loss = 0.04318680\n",
      "Iteration 179, loss = 0.04877494\n",
      "Iteration 203, loss = 0.03771940\n",
      "Iteration 180, loss = 0.04955287\n",
      "Iteration 204, loss = 0.03287336\n",
      "Iteration 181, loss = 0.05009883\n",
      "Iteration 205, loss = 0.03510726\n",
      "Iteration 182, loss = 0.04704235\n",
      "Iteration 206, loss = 0.03317616\n",
      "Iteration 183, loss = 0.04859704\n",
      "Iteration 207, loss = 0.03266786\n",
      "Iteration 184, loss = 0.04739140\n",
      "Iteration 208, loss = 0.03229660\n",
      "Iteration 185, loss = 0.04431511\n",
      "Iteration 209, loss = 0.03299599\n",
      "Iteration 186, loss = 0.04568139\n",
      "Iteration 210, loss = 0.03511030\n",
      "Iteration 187, loss = 0.04632669\n",
      "Iteration 211, loss = 0.03409277\n",
      "Iteration 188, loss = 0.04667998\n",
      "Iteration 212, loss = 0.03706182\n",
      "Iteration 189, loss = 0.04589737\n",
      "Iteration 213, loss = 0.03549336\n",
      "Iteration 190, loss = 0.04178877\n",
      "Iteration 214, loss = 0.03755626\n",
      "Iteration 191, loss = 0.04367354\n",
      "Iteration 215, loss = 0.03743400\n",
      "Iteration 192, loss = 0.04122578\n",
      "Iteration 216, loss = 0.03226218\n",
      "Iteration 193, loss = 0.04214673\n",
      "Iteration 217, loss = 0.03247462\n",
      "Iteration 194, loss = 0.04236716\n",
      "Iteration 218, loss = 0.03414849\n",
      "Iteration 195, loss = 0.04038754\n",
      "Iteration 219, loss = 0.03401654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 196, loss = 0.03994512\n",
      "Iteration 197, loss = 0.04008143\n",
      "Iteration 198, loss = 0.03934004\n",
      "Iteration 199, loss = 0.03900046\n",
      "Iteration 200, loss = 0.03895204\n",
      "Iteration 201, loss = 0.04074656\n",
      "Iteration 202, loss = 0.04219326\n",
      "Iteration 203, loss = 0.04711602\n",
      "Iteration 204, loss = 0.04256125\n",
      "Iteration 205, loss = 0.04188631\n",
      "Iteration 206, loss = 0.03801839\n",
      "Iteration 207, loss = 0.03863991\n",
      "Iteration 208, loss = 0.03619097\n",
      "Iteration 1, loss = 1.34161026\n",
      "Iteration 209, loss = 0.03638575\n",
      "Iteration 2, loss = 0.89508121\n",
      "Iteration 210, loss = 0.03724771\n",
      "Iteration 3, loss = 0.74510986\n",
      "Iteration 211, loss = 0.03543276\n",
      "Iteration 4, loss = 0.64691814\n",
      "Iteration 212, loss = 0.03669374\n",
      "Iteration 5, loss = 0.57004158\n",
      "Iteration 213, loss = 0.03525501\n",
      "Iteration 6, loss = 0.52113845\n",
      "Iteration 214, loss = 0.03577150\n",
      "Iteration 7, loss = 0.48661916\n",
      "Iteration 215, loss = 0.03474430\n",
      "Iteration 8, loss = 0.45884461\n",
      "Iteration 216, loss = 0.03492320\n",
      "Iteration 9, loss = 0.43764126\n",
      "Iteration 10, loss = 0.41946112\n",
      "Iteration 217, loss = 0.03544527\n",
      "Iteration 11, loss = 0.40430719\n",
      "Iteration 218, loss = 0.03698564\n",
      "Iteration 12, loss = 0.39095927\n",
      "Iteration 219, loss = 0.03585355\n",
      "Iteration 13, loss = 0.37891079\n",
      "Iteration 220, loss = 0.03327065\n",
      "Iteration 14, loss = 0.36782510\n",
      "Iteration 221, loss = 0.03348992\n",
      "Iteration 15, loss = 0.35769283\n",
      "Iteration 222, loss = 0.03356479\n",
      "Iteration 16, loss = 0.34876430\n",
      "Iteration 223, loss = 0.03370545\n",
      "Iteration 17, loss = 0.33952637\n",
      "Iteration 224, loss = 0.03356272\n",
      "Iteration 18, loss = 0.33095520\n",
      "Iteration 225, loss = 0.03262034\n",
      "Iteration 226, loss = 0.03227352\n",
      "Iteration 19, loss = 0.32458773\n",
      "Iteration 227, loss = 0.03175559\n",
      "Iteration 20, loss = 0.31666019\n",
      "Iteration 228, loss = 0.03113760\n",
      "Iteration 21, loss = 0.30903845\n",
      "Iteration 229, loss = 0.03106146\n",
      "Iteration 22, loss = 0.30044329\n",
      "Iteration 230, loss = 0.03130773\n",
      "Iteration 23, loss = 0.29491993\n",
      "Iteration 231, loss = 0.03146087\n",
      "Iteration 24, loss = 0.28817693\n",
      "Iteration 25, loss = 0.28067477\n",
      "Iteration 232, loss = 0.03094517\n",
      "Iteration 26, loss = 0.27664501\n",
      "Iteration 233, loss = 0.02991481\n",
      "Iteration 234, loss = 0.03187902\n",
      "Iteration 27, loss = 0.26916435\n",
      "Iteration 235, loss = 0.03403876\n",
      "Iteration 28, loss = 0.26272481\n",
      "Iteration 236, loss = 0.03183727\n",
      "Iteration 29, loss = 0.25696581\n",
      "Iteration 237, loss = 0.03204166Iteration 30, loss = 0.25172907\n",
      "\n",
      "Iteration 31, loss = 0.24611920\n",
      "Iteration 238, loss = 0.03569153\n",
      "Iteration 32, loss = 0.24042542\n",
      "Iteration 239, loss = 0.03478173\n",
      "Iteration 240, loss = 0.02935191\n",
      "Iteration 33, loss = 0.23543571\n",
      "Iteration 34, loss = 0.23073893\n",
      "Iteration 241, loss = 0.03000260\n",
      "Iteration 35, loss = 0.22509883\n",
      "Iteration 242, loss = 0.02788872\n",
      "Iteration 36, loss = 0.21982802\n",
      "Iteration 243, loss = 0.03034051\n",
      "Iteration 37, loss = 0.21624897\n",
      "Iteration 244, loss = 0.02824156\n",
      "Iteration 38, loss = 0.21152130\n",
      "Iteration 245, loss = 0.02736301\n",
      "Iteration 39, loss = 0.20694213\n",
      "Iteration 246, loss = 0.02904001\n",
      "Iteration 40, loss = 0.20362226\n",
      "Iteration 247, loss = 0.02740882\n",
      "Iteration 41, loss = 0.19844155\n",
      "Iteration 248, loss = 0.02822533\n",
      "Iteration 42, loss = 0.19526077\n",
      "Iteration 249, loss = 0.03220674\n",
      "Iteration 43, loss = 0.19081328\n",
      "Iteration 250, loss = 0.03194199\n",
      "Iteration 44, loss = 0.18746475\n",
      "Iteration 251, loss = 0.03086913\n",
      "Iteration 45, loss = 0.18388793\n",
      "Iteration 252, loss = 0.02761277\n",
      "Iteration 46, loss = 0.18001593\n",
      "Iteration 253, loss = 0.02729042\n",
      "Iteration 47, loss = 0.17924037\n",
      "Iteration 254, loss = 0.02674421\n",
      "Iteration 48, loss = 0.17263269\n",
      "Iteration 255, loss = 0.02774090\n",
      "Iteration 49, loss = 0.17048188\n",
      "Iteration 256, loss = 0.02811198\n",
      "Iteration 50, loss = 0.16691955\n",
      "Iteration 257, loss = 0.02915220\n",
      "Iteration 51, loss = 0.16301349\n",
      "Iteration 258, loss = 0.02756640\n",
      "Iteration 52, loss = 0.16105030\n",
      "Iteration 259, loss = 0.02506162\n",
      "Iteration 53, loss = 0.15757752\n",
      "Iteration 260, loss = 0.02544567\n",
      "Iteration 54, loss = 0.15594059\n",
      "Iteration 261, loss = 0.02767813\n",
      "Iteration 55, loss = 0.15162348\n",
      "Iteration 262, loss = 0.02783564\n",
      "Iteration 56, loss = 0.14935745\n",
      "Iteration 263, loss = 0.02609208\n",
      "Iteration 57, loss = 0.14818867\n",
      "Iteration 264, loss = 0.02524943\n",
      "Iteration 58, loss = 0.14468840\n",
      "Iteration 265, loss = 0.02692826\n",
      "Iteration 59, loss = 0.14151722\n",
      "Iteration 266, loss = 0.02975086\n",
      "Iteration 60, loss = 0.13939980\n",
      "Iteration 267, loss = 0.02778053\n",
      "Iteration 61, loss = 0.13659053\n",
      "Iteration 268, loss = 0.03273721\n",
      "Iteration 62, loss = 0.13469736\n",
      "Iteration 269, loss = 0.02670991\n",
      "Iteration 63, loss = 0.13245394\n",
      "Iteration 270, loss = 0.02560319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.12979254\n",
      "Iteration 65, loss = 0.12938484\n",
      "Iteration 66, loss = 0.12591135\n",
      "Iteration 67, loss = 0.12353899\n",
      "Iteration 68, loss = 0.12161175\n",
      "Iteration 69, loss = 0.11865572\n",
      "Iteration 70, loss = 0.11800250\n",
      "Iteration 71, loss = 0.11501904\n",
      "Iteration 72, loss = 0.11362507\n",
      "Iteration 73, loss = 0.11258781\n",
      "Iteration 74, loss = 0.10982718\n",
      "Iteration 75, loss = 0.10850112\n",
      "Iteration 76, loss = 0.10697398\n",
      "Iteration 1, loss = 1.24686918\n",
      "Iteration 77, loss = 0.10596373\n",
      "Iteration 2, loss = 0.94098377\n",
      "Iteration 78, loss = 0.10519879\n",
      "Iteration 3, loss = 0.78748138\n",
      "Iteration 79, loss = 0.10184392\n",
      "Iteration 4, loss = 0.67195003\n",
      "Iteration 80, loss = 0.10244447\n",
      "Iteration 5, loss = 0.59564674\n",
      "Iteration 81, loss = 0.09928597\n",
      "Iteration 6, loss = 0.54457165\n",
      "Iteration 82, loss = 0.09792009\n",
      "Iteration 7, loss = 0.50521482\n",
      "Iteration 83, loss = 0.09710131\n",
      "Iteration 8, loss = 0.47544388\n",
      "Iteration 84, loss = 0.09506117\n",
      "Iteration 9, loss = 0.45079014\n",
      "Iteration 85, loss = 0.09355427\n",
      "Iteration 10, loss = 0.43072722\n",
      "Iteration 86, loss = 0.09404842\n",
      "Iteration 11, loss = 0.41173061\n",
      "Iteration 87, loss = 0.09170614\n",
      "Iteration 12, loss = 0.39672956\n",
      "Iteration 88, loss = 0.09096451\n",
      "Iteration 13, loss = 0.38420644\n",
      "Iteration 89, loss = 0.08779524\n",
      "Iteration 14, loss = 0.37204168\n",
      "Iteration 90, loss = 0.08999354\n",
      "Iteration 15, loss = 0.35994246\n",
      "Iteration 91, loss = 0.09196026\n",
      "Iteration 16, loss = 0.34993088\n",
      "Iteration 92, loss = 0.08848719\n",
      "Iteration 17, loss = 0.33994969\n",
      "Iteration 93, loss = 0.08388570\n",
      "Iteration 18, loss = 0.33174705\n",
      "Iteration 94, loss = 0.08501978\n",
      "Iteration 19, loss = 0.32276198\n",
      "Iteration 95, loss = 0.08360473\n",
      "Iteration 20, loss = 0.31502032\n",
      "Iteration 96, loss = 0.08172193\n",
      "Iteration 21, loss = 0.30679115\n",
      "Iteration 97, loss = 0.08328581\n",
      "Iteration 22, loss = 0.29868327\n",
      "Iteration 98, loss = 0.07858052\n",
      "Iteration 23, loss = 0.29279511\n",
      "Iteration 99, loss = 0.08002894\n",
      "Iteration 24, loss = 0.28509306\n",
      "Iteration 100, loss = 0.07948368\n",
      "Iteration 25, loss = 0.27713043\n",
      "Iteration 101, loss = 0.07607119\n",
      "Iteration 26, loss = 0.27160464\n",
      "Iteration 102, loss = 0.07604521\n",
      "Iteration 27, loss = 0.26372134\n",
      "Iteration 103, loss = 0.07454125\n",
      "Iteration 28, loss = 0.25909214\n",
      "Iteration 104, loss = 0.07282039\n",
      "Iteration 29, loss = 0.25445671\n",
      "Iteration 105, loss = 0.07249164\n",
      "Iteration 30, loss = 0.24799989\n",
      "Iteration 106, loss = 0.07133840\n",
      "Iteration 31, loss = 0.24308391\n",
      "Iteration 107, loss = 0.07026149\n",
      "Iteration 32, loss = 0.23782330\n",
      "Iteration 108, loss = 0.07058920\n",
      "Iteration 33, loss = 0.23361038\n",
      "Iteration 109, loss = 0.07010295\n",
      "Iteration 34, loss = 0.22926281\n",
      "Iteration 110, loss = 0.06762087\n",
      "Iteration 35, loss = 0.22485642\n",
      "Iteration 111, loss = 0.06991020\n",
      "Iteration 36, loss = 0.22075970\n",
      "Iteration 112, loss = 0.06796268\n",
      "Iteration 37, loss = 0.21701817\n",
      "Iteration 113, loss = 0.06864583\n",
      "Iteration 38, loss = 0.21340885\n",
      "Iteration 114, loss = 0.06732191\n",
      "Iteration 39, loss = 0.20816371\n",
      "Iteration 115, loss = 0.06419626\n",
      "Iteration 40, loss = 0.20516705\n",
      "Iteration 116, loss = 0.06444300\n",
      "Iteration 41, loss = 0.20177040\n",
      "Iteration 117, loss = 0.06336793\n",
      "Iteration 42, loss = 0.19777506\n",
      "Iteration 118, loss = 0.06176852\n",
      "Iteration 43, loss = 0.19503402\n",
      "Iteration 119, loss = 0.06231765\n",
      "Iteration 44, loss = 0.19065788\n",
      "Iteration 120, loss = 0.06178215\n",
      "Iteration 45, loss = 0.18930858\n",
      "Iteration 121, loss = 0.06125389\n",
      "Iteration 46, loss = 0.18378933\n",
      "Iteration 122, loss = 0.05832070\n",
      "Iteration 123, loss = 0.06208830\n",
      "Iteration 47, loss = 0.18252193\n",
      "Iteration 124, loss = 0.05844231\n",
      "Iteration 48, loss = 0.17821880\n",
      "Iteration 125, loss = 0.05990354\n",
      "Iteration 49, loss = 0.17531166\n",
      "Iteration 126, loss = 0.05708168\n",
      "Iteration 50, loss = 0.17290045\n",
      "Iteration 127, loss = 0.05625187\n",
      "Iteration 51, loss = 0.17031798\n",
      "Iteration 128, loss = 0.05606275\n",
      "Iteration 52, loss = 0.17078861\n",
      "Iteration 53, loss = 0.16444433\n",
      "Iteration 129, loss = 0.05489085\n",
      "Iteration 54, loss = 0.16246241\n",
      "Iteration 130, loss = 0.05414010\n",
      "Iteration 55, loss = 0.16035852\n",
      "Iteration 131, loss = 0.05408589\n",
      "Iteration 56, loss = 0.15676283\n",
      "Iteration 132, loss = 0.05458019\n",
      "Iteration 57, loss = 0.15325412\n",
      "Iteration 133, loss = 0.05388625\n",
      "Iteration 58, loss = 0.15158511\n",
      "Iteration 134, loss = 0.05235063\n",
      "Iteration 59, loss = 0.14906590\n",
      "Iteration 135, loss = 0.05141562\n",
      "Iteration 60, loss = 0.14685523\n",
      "Iteration 136, loss = 0.05185506\n",
      "Iteration 61, loss = 0.14470881\n",
      "Iteration 137, loss = 0.05075751\n",
      "Iteration 138, loss = 0.05104928\n",
      "Iteration 62, loss = 0.14229472\n",
      "Iteration 139, loss = 0.05188855\n",
      "Iteration 63, loss = 0.13988173\n",
      "Iteration 140, loss = 0.05425632\n",
      "Iteration 64, loss = 0.13812111\n",
      "Iteration 141, loss = 0.04822476\n",
      "Iteration 65, loss = 0.13639745\n",
      "Iteration 142, loss = 0.05010754\n",
      "Iteration 66, loss = 0.13543962\n",
      "Iteration 143, loss = 0.04852509\n",
      "Iteration 67, loss = 0.13087107\n",
      "Iteration 144, loss = 0.04717840\n",
      "Iteration 68, loss = 0.13103500\n",
      "Iteration 145, loss = 0.04815230\n",
      "Iteration 69, loss = 0.12955779\n",
      "Iteration 146, loss = 0.04821410\n",
      "Iteration 70, loss = 0.12653475\n",
      "Iteration 147, loss = 0.04880136\n",
      "Iteration 71, loss = 0.12515603\n",
      "Iteration 148, loss = 0.05083389\n",
      "Iteration 72, loss = 0.12320559\n",
      "Iteration 149, loss = 0.04745022\n",
      "Iteration 73, loss = 0.12086315\n",
      "Iteration 150, loss = 0.04464564\n",
      "Iteration 74, loss = 0.12242842\n",
      "Iteration 151, loss = 0.04576246\n",
      "Iteration 75, loss = 0.11910894\n",
      "Iteration 76, loss = 0.11771798\n",
      "Iteration 152, loss = 0.04504675\n",
      "Iteration 77, loss = 0.11688413\n",
      "Iteration 153, loss = 0.04374605\n",
      "Iteration 154, loss = 0.04404080\n",
      "Iteration 78, loss = 0.11356309\n",
      "Iteration 155, loss = 0.04311213\n",
      "Iteration 79, loss = 0.11188822\n",
      "Iteration 80, loss = 0.11054015\n",
      "Iteration 156, loss = 0.04213263\n",
      "Iteration 81, loss = 0.11008462\n",
      "Iteration 157, loss = 0.04245943\n",
      "Iteration 82, loss = 0.10843285\n",
      "Iteration 158, loss = 0.04136490\n",
      "Iteration 159, loss = 0.04073939\n",
      "Iteration 83, loss = 0.10728654\n",
      "Iteration 160, loss = 0.04056366\n",
      "Iteration 84, loss = 0.10825698\n",
      "Iteration 161, loss = 0.04014212\n",
      "Iteration 85, loss = 0.10871717\n",
      "Iteration 162, loss = 0.03964700\n",
      "Iteration 86, loss = 0.10821750\n",
      "Iteration 163, loss = 0.04035849\n",
      "Iteration 87, loss = 0.10668714\n",
      "Iteration 164, loss = 0.04178060\n",
      "Iteration 88, loss = 0.10348183\n",
      "Iteration 165, loss = 0.04545048\n",
      "Iteration 89, loss = 0.10022458\n",
      "Iteration 166, loss = 0.04265305\n",
      "Iteration 90, loss = 0.10066718\n",
      "Iteration 167, loss = 0.03735496\n",
      "Iteration 91, loss = 0.09995807\n",
      "Iteration 168, loss = 0.03938071\n",
      "Iteration 92, loss = 0.09785761\n",
      "Iteration 169, loss = 0.04058911\n",
      "Iteration 93, loss = 0.09574586\n",
      "Iteration 170, loss = 0.04185649\n",
      "Iteration 94, loss = 0.09389571\n",
      "Iteration 171, loss = 0.03522713\n",
      "Iteration 95, loss = 0.09361805\n",
      "Iteration 172, loss = 0.03871148\n",
      "Iteration 96, loss = 0.09658139\n",
      "Iteration 173, loss = 0.03709612\n",
      "Iteration 97, loss = 0.08822684\n",
      "Iteration 174, loss = 0.03535914\n",
      "Iteration 98, loss = 0.09258111\n",
      "Iteration 175, loss = 0.03574784\n",
      "Iteration 99, loss = 0.09345250\n",
      "Iteration 176, loss = 0.03589925\n",
      "Iteration 100, loss = 0.08713449\n",
      "Iteration 177, loss = 0.03581805\n",
      "Iteration 101, loss = 0.08567453\n",
      "Iteration 178, loss = 0.03403749\n",
      "Iteration 102, loss = 0.08439336\n",
      "Iteration 179, loss = 0.03497275\n",
      "Iteration 103, loss = 0.08295391\n",
      "Iteration 180, loss = 0.03397720\n",
      "Iteration 104, loss = 0.08159618\n",
      "Iteration 181, loss = 0.03367088\n",
      "Iteration 105, loss = 0.08168889\n",
      "Iteration 182, loss = 0.03340225\n",
      "Iteration 106, loss = 0.08035615\n",
      "Iteration 183, loss = 0.03262750\n",
      "Iteration 107, loss = 0.07973263\n",
      "Iteration 184, loss = 0.03296620\n",
      "Iteration 108, loss = 0.07856839\n",
      "Iteration 185, loss = 0.03262424\n",
      "Iteration 109, loss = 0.07635523\n",
      "Iteration 186, loss = 0.03203587\n",
      "Iteration 110, loss = 0.07663818\n",
      "Iteration 187, loss = 0.03232268\n",
      "Iteration 111, loss = 0.07727405\n",
      "Iteration 188, loss = 0.03488982\n",
      "Iteration 112, loss = 0.07785379\n",
      "Iteration 189, loss = 0.03582674\n",
      "Iteration 113, loss = 0.07515086\n",
      "Iteration 190, loss = 0.03423772\n",
      "Iteration 114, loss = 0.07458728\n",
      "Iteration 191, loss = 0.03222510\n",
      "Iteration 115, loss = 0.07388355\n",
      "Iteration 192, loss = 0.03348569\n",
      "Iteration 116, loss = 0.07273704\n",
      "Iteration 193, loss = 0.03191452\n",
      "Iteration 117, loss = 0.07199416\n",
      "Iteration 194, loss = 0.03040392\n",
      "Iteration 118, loss = 0.07152568\n",
      "Iteration 195, loss = 0.03494090\n",
      "Iteration 119, loss = 0.06945802\n",
      "Iteration 196, loss = 0.03682538\n",
      "Iteration 120, loss = 0.06889137\n",
      "Iteration 197, loss = 0.03323709\n",
      "Iteration 121, loss = 0.06689914\n",
      "Iteration 198, loss = 0.03272447\n",
      "Iteration 122, loss = 0.06688859\n",
      "Iteration 199, loss = 0.03303753\n",
      "Iteration 123, loss = 0.06584729\n",
      "Iteration 200, loss = 0.03156711\n",
      "Iteration 124, loss = 0.06736475\n",
      "Iteration 201, loss = 0.02911018\n",
      "Iteration 125, loss = 0.06477536\n",
      "Iteration 202, loss = 0.03033148\n",
      "Iteration 126, loss = 0.06418182\n",
      "Iteration 203, loss = 0.02978041\n",
      "Iteration 127, loss = 0.06269717\n",
      "Iteration 204, loss = 0.02923245\n",
      "Iteration 128, loss = 0.06252638\n",
      "Iteration 205, loss = 0.02940537\n",
      "Iteration 129, loss = 0.06170101\n",
      "Iteration 206, loss = 0.02793878\n",
      "Iteration 130, loss = 0.06123430\n",
      "Iteration 207, loss = 0.02795248\n",
      "Iteration 131, loss = 0.06409936\n",
      "Iteration 208, loss = 0.02686568\n",
      "Iteration 132, loss = 0.05957828\n",
      "Iteration 209, loss = 0.02695548\n",
      "Iteration 133, loss = 0.05800905\n",
      "Iteration 210, loss = 0.02727339\n",
      "Iteration 134, loss = 0.05828054\n",
      "Iteration 211, loss = 0.02677827\n",
      "Iteration 135, loss = 0.05809048\n",
      "Iteration 212, loss = 0.02726383\n",
      "Iteration 136, loss = 0.05750282\n",
      "Iteration 213, loss = 0.02558384\n",
      "Iteration 137, loss = 0.05640789\n",
      "Iteration 214, loss = 0.02712973\n",
      "Iteration 138, loss = 0.05592835\n",
      "Iteration 215, loss = 0.02715285\n",
      "Iteration 139, loss = 0.05483073\n",
      "Iteration 216, loss = 0.02669233\n",
      "Iteration 140, loss = 0.05415662\n",
      "Iteration 217, loss = 0.02733907\n",
      "Iteration 141, loss = 0.05464907\n",
      "Iteration 218, loss = 0.02457074\n",
      "Iteration 142, loss = 0.05436087\n",
      "Iteration 219, loss = 0.02679802\n",
      "Iteration 143, loss = 0.05278290\n",
      "Iteration 220, loss = 0.02962308\n",
      "Iteration 144, loss = 0.05259887\n",
      "Iteration 221, loss = 0.03146531\n",
      "Iteration 145, loss = 0.05140535\n",
      "Iteration 222, loss = 0.02671965\n",
      "Iteration 146, loss = 0.05081019\n",
      "Iteration 223, loss = 0.02404404\n",
      "Iteration 147, loss = 0.05006035\n",
      "Iteration 224, loss = 0.02629999\n",
      "Iteration 148, loss = 0.05026739\n",
      "Iteration 225, loss = 0.02608633\n",
      "Iteration 149, loss = 0.04951118\n",
      "Iteration 226, loss = 0.02701570\n",
      "Iteration 150, loss = 0.04926046\n",
      "Iteration 227, loss = 0.02567209\n",
      "Iteration 151, loss = 0.04820745\n",
      "Iteration 228, loss = 0.02701720\n",
      "Iteration 152, loss = 0.04882850\n",
      "Iteration 229, loss = 0.02293296\n",
      "Iteration 153, loss = 0.04820738\n",
      "Iteration 230, loss = 0.02790917\n",
      "Iteration 154, loss = 0.04731205\n",
      "Iteration 231, loss = 0.02420620\n",
      "Iteration 155, loss = 0.04754895\n",
      "Iteration 232, loss = 0.02428892\n",
      "Iteration 156, loss = 0.04645991\n",
      "Iteration 233, loss = 0.02333837\n",
      "Iteration 157, loss = 0.04610581\n",
      "Iteration 234, loss = 0.02309722\n",
      "Iteration 158, loss = 0.04527411\n",
      "Iteration 235, loss = 0.02287417\n",
      "Iteration 159, loss = 0.05150837\n",
      "Iteration 236, loss = 0.02333211\n",
      "Iteration 160, loss = 0.04795634\n",
      "Iteration 237, loss = 0.02175053\n",
      "Iteration 161, loss = 0.04535022\n",
      "Iteration 238, loss = 0.02206666\n",
      "Iteration 162, loss = 0.04507670\n",
      "Iteration 239, loss = 0.02301143\n",
      "Iteration 163, loss = 0.04633660\n",
      "Iteration 240, loss = 0.02250056\n",
      "Iteration 164, loss = 0.04208222\n",
      "Iteration 241, loss = 0.02206042\n",
      "Iteration 165, loss = 0.04400330\n",
      "Iteration 242, loss = 0.02333051\n",
      "Iteration 166, loss = 0.04223355\n",
      "Iteration 243, loss = 0.02463954\n",
      "Iteration 167, loss = 0.04322658\n",
      "Iteration 244, loss = 0.02359743\n",
      "Iteration 168, loss = 0.04204928\n",
      "Iteration 245, loss = 0.02471679\n",
      "Iteration 169, loss = 0.04248904\n",
      "Iteration 246, loss = 0.02133835\n",
      "Iteration 170, loss = 0.04480310\n",
      "Iteration 247, loss = 0.02082933\n",
      "Iteration 171, loss = 0.04063888\n",
      "Iteration 248, loss = 0.02377530\n",
      "Iteration 172, loss = 0.04047053\n",
      "Iteration 249, loss = 0.02868120\n",
      "Iteration 173, loss = 0.03894555\n",
      "Iteration 250, loss = 0.02394319\n",
      "Iteration 174, loss = 0.03945331\n",
      "Iteration 251, loss = 0.02405041\n",
      "Iteration 175, loss = 0.03974919\n",
      "Iteration 252, loss = 0.02645905\n",
      "Iteration 176, loss = 0.03865345\n",
      "Iteration 253, loss = 0.02572756\n",
      "Iteration 177, loss = 0.03835210\n",
      "Iteration 254, loss = 0.02016698\n",
      "Iteration 178, loss = 0.03789531\n",
      "Iteration 255, loss = 0.02028527\n",
      "Iteration 179, loss = 0.03772643\n",
      "Iteration 256, loss = 0.02550869\n",
      "Iteration 180, loss = 0.03718720\n",
      "Iteration 257, loss = 0.02044394\n",
      "Iteration 181, loss = 0.03806536\n",
      "Iteration 258, loss = 0.01921856\n",
      "Iteration 182, loss = 0.03770113\n",
      "Iteration 259, loss = 0.02029814\n",
      "Iteration 183, loss = 0.03638432\n",
      "Iteration 260, loss = 0.02084162\n",
      "Iteration 261, loss = 0.01839069\n",
      "Iteration 184, loss = 0.03716091\n",
      "Iteration 185, loss = 0.03580136\n",
      "Iteration 262, loss = 0.02011763\n",
      "Iteration 186, loss = 0.03586928\n",
      "Iteration 263, loss = 0.01946987\n",
      "Iteration 187, loss = 0.03503544\n",
      "Iteration 264, loss = 0.01837385\n",
      "Iteration 188, loss = 0.03595895\n",
      "Iteration 265, loss = 0.01907001\n",
      "Iteration 189, loss = 0.03563055\n",
      "Iteration 266, loss = 0.01956745\n",
      "Iteration 190, loss = 0.03629262\n",
      "Iteration 267, loss = 0.01991249\n",
      "Iteration 268, loss = 0.01850299\n",
      "Iteration 191, loss = 0.03419224\n",
      "Iteration 269, loss = 0.02297642\n",
      "Iteration 192, loss = 0.03406654\n",
      "Iteration 270, loss = 0.02176843\n",
      "Iteration 193, loss = 0.03458453\n",
      "Iteration 271, loss = 0.01977913\n",
      "Iteration 194, loss = 0.03380992\n",
      "Iteration 272, loss = 0.02087140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 195, loss = 0.03249135\n",
      "Iteration 196, loss = 0.03364569\n",
      "Iteration 197, loss = 0.03604164\n",
      "Iteration 198, loss = 0.03947597\n",
      "Iteration 199, loss = 0.04002519\n",
      "Iteration 200, loss = 0.03871315\n",
      "Iteration 201, loss = 0.03379114\n",
      "Iteration 202, loss = 0.03286950\n",
      "Iteration 203, loss = 0.03146101\n",
      "Iteration 204, loss = 0.03141027\n",
      "Iteration 205, loss = 0.03221714\n",
      "Iteration 206, loss = 0.03416636\n",
      "Iteration 207, loss = 0.03039214\n",
      "Iteration 208, loss = 0.03085412\n",
      "Iteration 1, loss = 1.14942470\n",
      "Iteration 2, loss = 0.90040189\n",
      "Iteration 209, loss = 0.03166536\n",
      "Iteration 3, loss = 0.74040410\n",
      "Iteration 210, loss = 0.02922973\n",
      "Iteration 4, loss = 0.64226999\n",
      "Iteration 211, loss = 0.03044980\n",
      "Iteration 5, loss = 0.57394170\n",
      "Iteration 212, loss = 0.03107473\n",
      "Iteration 6, loss = 0.52476551\n",
      "Iteration 213, loss = 0.03110464\n",
      "Iteration 7, loss = 0.48892424\n",
      "Iteration 214, loss = 0.02982513\n",
      "Iteration 8, loss = 0.46201461\n",
      "Iteration 215, loss = 0.02908816\n",
      "Iteration 9, loss = 0.43628514\n",
      "Iteration 216, loss = 0.02931912\n",
      "Iteration 10, loss = 0.41824773\n",
      "Iteration 217, loss = 0.02784792\n",
      "Iteration 11, loss = 0.40107081\n",
      "Iteration 218, loss = 0.02865662\n",
      "Iteration 12, loss = 0.38745123\n",
      "Iteration 219, loss = 0.02911814\n",
      "Iteration 13, loss = 0.37523749\n",
      "Iteration 220, loss = 0.02901571\n",
      "Iteration 14, loss = 0.36342562\n",
      "Iteration 221, loss = 0.02769519\n",
      "Iteration 15, loss = 0.35289603\n",
      "Iteration 222, loss = 0.02839704\n",
      "Iteration 16, loss = 0.34359525\n",
      "Iteration 223, loss = 0.02991810\n",
      "Iteration 17, loss = 0.33456605\n",
      "Iteration 224, loss = 0.03275462\n",
      "Iteration 18, loss = 0.32827538\n",
      "Iteration 225, loss = 0.03160920\n",
      "Iteration 19, loss = 0.31874014\n",
      "Iteration 226, loss = 0.03028916\n",
      "Iteration 20, loss = 0.31066473\n",
      "Iteration 227, loss = 0.02980628\n",
      "Iteration 21, loss = 0.30403956\n",
      "Iteration 228, loss = 0.03034807\n",
      "Iteration 22, loss = 0.29658969\n",
      "Iteration 229, loss = 0.02820148\n",
      "Iteration 23, loss = 0.29017954\n",
      "Iteration 230, loss = 0.02750826\n",
      "Iteration 24, loss = 0.28434377\n",
      "Iteration 231, loss = 0.03259111\n",
      "Iteration 25, loss = 0.27860399\n",
      "Iteration 232, loss = 0.03022291\n",
      "Iteration 26, loss = 0.27277750\n",
      "Iteration 233, loss = 0.02703334\n",
      "Iteration 27, loss = 0.26718052\n",
      "Iteration 234, loss = 0.02536796\n",
      "Iteration 28, loss = 0.26249243\n",
      "Iteration 235, loss = 0.02515202\n",
      "Iteration 29, loss = 0.25844270\n",
      "Iteration 236, loss = 0.02859031\n",
      "Iteration 30, loss = 0.25277171\n",
      "Iteration 237, loss = 0.03189797\n",
      "Iteration 31, loss = 0.24722024\n",
      "Iteration 238, loss = 0.02649246\n",
      "Iteration 32, loss = 0.24152419\n",
      "Iteration 239, loss = 0.02620994\n",
      "Iteration 33, loss = 0.23691248\n",
      "Iteration 240, loss = 0.02689496\n",
      "Iteration 34, loss = 0.23266797\n",
      "Iteration 241, loss = 0.02582156\n",
      "Iteration 35, loss = 0.22777894\n",
      "Iteration 242, loss = 0.02476962\n",
      "Iteration 36, loss = 0.22398568\n",
      "Iteration 243, loss = 0.02533401\n",
      "Iteration 37, loss = 0.22075779\n",
      "Iteration 244, loss = 0.02376319\n",
      "Iteration 38, loss = 0.21636462\n",
      "Iteration 245, loss = 0.02353790\n",
      "Iteration 39, loss = 0.21214182\n",
      "Iteration 246, loss = 0.02454346\n",
      "Iteration 40, loss = 0.20884855\n",
      "Iteration 247, loss = 0.02372085\n",
      "Iteration 41, loss = 0.20464954\n",
      "Iteration 248, loss = 0.02316220\n",
      "Iteration 42, loss = 0.20185120\n",
      "Iteration 249, loss = 0.02576262\n",
      "Iteration 43, loss = 0.20103001\n",
      "Iteration 250, loss = 0.02344501\n",
      "Iteration 44, loss = 0.19959426\n",
      "Iteration 251, loss = 0.02309559\n",
      "Iteration 45, loss = 0.19515801\n",
      "Iteration 252, loss = 0.02447200\n",
      "Iteration 46, loss = 0.19117780\n",
      "Iteration 253, loss = 0.02302744\n",
      "Iteration 47, loss = 0.18748560\n",
      "Iteration 254, loss = 0.02348014\n",
      "Iteration 48, loss = 0.18359080\n",
      "Iteration 255, loss = 0.02565216\n",
      "Iteration 49, loss = 0.18126143\n",
      "Iteration 256, loss = 0.02557269\n",
      "Iteration 50, loss = 0.17792142\n",
      "Iteration 257, loss = 0.02516158\n",
      "Iteration 51, loss = 0.17403654\n",
      "Iteration 258, loss = 0.02436684\n",
      "Iteration 52, loss = 0.17417254\n",
      "Iteration 259, loss = 0.02289830\n",
      "Iteration 53, loss = 0.17099238\n",
      "Iteration 260, loss = 0.02320974\n",
      "Iteration 54, loss = 0.16684225\n",
      "Iteration 261, loss = 0.02168578\n",
      "Iteration 55, loss = 0.16496647\n",
      "Iteration 262, loss = 0.02221648\n",
      "Iteration 56, loss = 0.16289003\n",
      "Iteration 263, loss = 0.02184563\n",
      "Iteration 57, loss = 0.16210547\n",
      "Iteration 264, loss = 0.02256323\n",
      "Iteration 58, loss = 0.16061663\n",
      "Iteration 265, loss = 0.02488981\n",
      "Iteration 59, loss = 0.15565837\n",
      "Iteration 266, loss = 0.02940005\n",
      "Iteration 60, loss = 0.15445153\n",
      "Iteration 61, loss = 0.15128686\n",
      "Iteration 267, loss = 0.02572627\n",
      "Iteration 62, loss = 0.14877901\n",
      "Iteration 268, loss = 0.02421259\n",
      "Iteration 63, loss = 0.14794661\n",
      "Iteration 269, loss = 0.02222606\n",
      "Iteration 270, loss = 0.02161631\n",
      "Iteration 64, loss = 0.14486641\n",
      "Iteration 271, loss = 0.02267348\n",
      "Iteration 65, loss = 0.14342896\n",
      "Iteration 66, loss = 0.14291525\n",
      "Iteration 272, loss = 0.02122858\n",
      "Iteration 67, loss = 0.13945413\n",
      "Iteration 273, loss = 0.02057182\n",
      "Iteration 68, loss = 0.13828536\n",
      "Iteration 274, loss = 0.02105028\n",
      "Iteration 69, loss = 0.13586818\n",
      "Iteration 275, loss = 0.02068026\n",
      "Iteration 70, loss = 0.13468117\n",
      "Iteration 276, loss = 0.02046715\n",
      "Iteration 71, loss = 0.13176063\n",
      "Iteration 277, loss = 0.02025571\n",
      "Iteration 72, loss = 0.13281580\n",
      "Iteration 278, loss = 0.02060196\n",
      "Iteration 73, loss = 0.13153430\n",
      "Iteration 279, loss = 0.02008951\n",
      "Iteration 74, loss = 0.12646174\n",
      "Iteration 280, loss = 0.01937777\n",
      "Iteration 75, loss = 0.12635177\n",
      "Iteration 281, loss = 0.02103081\n",
      "Iteration 76, loss = 0.12523744\n",
      "Iteration 282, loss = 0.02336867\n",
      "Iteration 77, loss = 0.12342957\n",
      "Iteration 283, loss = 0.02057052\n",
      "Iteration 78, loss = 0.12088053\n",
      "Iteration 284, loss = 0.02028345\n",
      "Iteration 79, loss = 0.12169935\n",
      "Iteration 285, loss = 0.01949139\n",
      "Iteration 80, loss = 0.12088689\n",
      "Iteration 286, loss = 0.01994444\n",
      "Iteration 81, loss = 0.12125908\n",
      "Iteration 287, loss = 0.01906702\n",
      "Iteration 82, loss = 0.11687533\n",
      "Iteration 288, loss = 0.01943454\n",
      "Iteration 83, loss = 0.11590956\n",
      "Iteration 289, loss = 0.01903145\n",
      "Iteration 84, loss = 0.11679165\n",
      "Iteration 290, loss = 0.01849007\n",
      "Iteration 85, loss = 0.11238383\n",
      "Iteration 291, loss = 0.01840854\n",
      "Iteration 86, loss = 0.11118868\n",
      "Iteration 292, loss = 0.01874372\n",
      "Iteration 87, loss = 0.10863201\n",
      "Iteration 293, loss = 0.01860382\n",
      "Iteration 88, loss = 0.11023011\n",
      "Iteration 294, loss = 0.01902943\n",
      "Iteration 89, loss = 0.11271663\n",
      "Iteration 295, loss = 0.02116346\n",
      "Iteration 90, loss = 0.10901130\n",
      "Iteration 296, loss = 0.02157283\n",
      "Iteration 91, loss = 0.10780318\n",
      "Iteration 297, loss = 0.02295466\n",
      "Iteration 92, loss = 0.10371484\n",
      "Iteration 298, loss = 0.03079353\n",
      "Iteration 93, loss = 0.10410715\n",
      "Iteration 299, loss = 0.04266571\n",
      "Iteration 94, loss = 0.10129011\n",
      "Iteration 300, loss = 0.04607006\n",
      "Iteration 95, loss = 0.10091928\n",
      "Iteration 301, loss = 0.02819416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 96, loss = 0.09887199\n",
      "Iteration 97, loss = 0.09866359\n",
      "Iteration 98, loss = 0.09751146\n",
      "Iteration 99, loss = 0.09751814\n",
      "Iteration 100, loss = 0.09430749\n",
      "Iteration 101, loss = 0.09320148\n",
      "Iteration 102, loss = 0.09962839\n",
      "Iteration 103, loss = 0.09405076\n",
      "Iteration 104, loss = 0.09034014\n",
      "Iteration 105, loss = 0.09201397\n",
      "Iteration 106, loss = 0.08997615\n",
      "Iteration 107, loss = 0.08728752\n",
      "Iteration 108, loss = 0.08782783\n",
      "Iteration 1, loss = 1.07391618\n",
      "Iteration 109, loss = 0.08431962\n",
      "Iteration 2, loss = 0.78334157\n",
      "Iteration 110, loss = 0.08320072\n",
      "Iteration 3, loss = 0.64495641\n",
      "Iteration 111, loss = 0.08425994\n",
      "Iteration 4, loss = 0.56421976\n",
      "Iteration 112, loss = 0.08166697\n",
      "Iteration 5, loss = 0.51523465\n",
      "Iteration 113, loss = 0.08232585\n",
      "Iteration 6, loss = 0.47650494\n",
      "Iteration 114, loss = 0.07928326\n",
      "Iteration 7, loss = 0.44915213\n",
      "Iteration 115, loss = 0.07960233\n",
      "Iteration 8, loss = 0.42538999\n",
      "Iteration 116, loss = 0.07784677\n",
      "Iteration 9, loss = 0.40818591\n",
      "Iteration 117, loss = 0.07868096\n",
      "Iteration 10, loss = 0.39200815\n",
      "Iteration 118, loss = 0.07686468\n",
      "Iteration 11, loss = 0.38045217\n",
      "Iteration 119, loss = 0.07636773\n",
      "Iteration 12, loss = 0.36798113\n",
      "Iteration 120, loss = 0.07836233\n",
      "Iteration 13, loss = 0.35678313\n",
      "Iteration 121, loss = 0.07909303\n",
      "Iteration 14, loss = 0.34640498\n",
      "Iteration 122, loss = 0.07713330\n",
      "Iteration 15, loss = 0.33701583\n",
      "Iteration 123, loss = 0.08143808\n",
      "Iteration 16, loss = 0.32946618\n",
      "Iteration 124, loss = 0.07704764\n",
      "Iteration 17, loss = 0.32280894\n",
      "Iteration 125, loss = 0.07588002\n",
      "Iteration 18, loss = 0.31288759\n",
      "Iteration 126, loss = 0.07218126\n",
      "Iteration 19, loss = 0.30590390\n",
      "Iteration 127, loss = 0.06955899\n",
      "Iteration 20, loss = 0.30076649\n",
      "Iteration 128, loss = 0.06905398\n",
      "Iteration 21, loss = 0.29177775\n",
      "Iteration 129, loss = 0.06891408\n",
      "Iteration 22, loss = 0.28667352\n",
      "Iteration 130, loss = 0.06752902\n",
      "Iteration 23, loss = 0.27972444\n",
      "Iteration 131, loss = 0.06766035\n",
      "Iteration 24, loss = 0.27312840\n",
      "Iteration 132, loss = 0.06769190\n",
      "Iteration 25, loss = 0.26866158\n",
      "Iteration 133, loss = 0.06704378\n",
      "Iteration 26, loss = 0.26269510\n",
      "Iteration 134, loss = 0.07000593\n",
      "Iteration 27, loss = 0.25789071\n",
      "Iteration 135, loss = 0.06538079\n",
      "Iteration 28, loss = 0.25172019\n",
      "Iteration 136, loss = 0.06820636\n",
      "Iteration 29, loss = 0.24777228\n",
      "Iteration 137, loss = 0.06463978\n",
      "Iteration 30, loss = 0.24190127\n",
      "Iteration 138, loss = 0.06630528\n",
      "Iteration 31, loss = 0.23831539\n",
      "Iteration 139, loss = 0.06365666\n",
      "Iteration 32, loss = 0.23311570\n",
      "Iteration 140, loss = 0.06068972\n",
      "Iteration 33, loss = 0.22757275\n",
      "Iteration 141, loss = 0.06326264\n",
      "Iteration 34, loss = 0.22328406\n",
      "Iteration 142, loss = 0.05973629\n",
      "Iteration 35, loss = 0.22031726\n",
      "Iteration 143, loss = 0.06035908\n",
      "Iteration 36, loss = 0.21487821\n",
      "Iteration 144, loss = 0.06181599\n",
      "Iteration 37, loss = 0.21361978\n",
      "Iteration 145, loss = 0.06275237\n",
      "Iteration 38, loss = 0.20802681\n",
      "Iteration 146, loss = 0.06828259\n",
      "Iteration 39, loss = 0.20479313\n",
      "Iteration 147, loss = 0.06372270\n",
      "Iteration 40, loss = 0.20141400\n",
      "Iteration 148, loss = 0.05909363\n",
      "Iteration 41, loss = 0.19510623\n",
      "Iteration 149, loss = 0.05624031\n",
      "Iteration 42, loss = 0.19479566\n",
      "Iteration 150, loss = 0.05798348\n",
      "Iteration 43, loss = 0.19225237\n",
      "Iteration 151, loss = 0.05855749\n",
      "Iteration 44, loss = 0.18667006\n",
      "Iteration 152, loss = 0.05600344\n",
      "Iteration 45, loss = 0.18391223\n",
      "Iteration 153, loss = 0.05395680\n",
      "Iteration 46, loss = 0.18166863\n",
      "Iteration 154, loss = 0.05284796\n",
      "Iteration 47, loss = 0.17857856\n",
      "Iteration 155, loss = 0.05241935\n",
      "Iteration 48, loss = 0.17473842\n",
      "Iteration 156, loss = 0.05236680\n",
      "Iteration 49, loss = 0.17176930\n",
      "Iteration 157, loss = 0.05109915\n",
      "Iteration 50, loss = 0.16859322\n",
      "Iteration 158, loss = 0.05093828\n",
      "Iteration 51, loss = 0.16518971\n",
      "Iteration 159, loss = 0.05048960\n",
      "Iteration 52, loss = 0.16279620\n",
      "Iteration 160, loss = 0.05069971\n",
      "Iteration 53, loss = 0.16229744\n",
      "Iteration 161, loss = 0.05048948\n",
      "Iteration 54, loss = 0.15770933\n",
      "Iteration 162, loss = 0.04924879\n",
      "Iteration 55, loss = 0.15607965\n",
      "Iteration 163, loss = 0.04821080\n",
      "Iteration 56, loss = 0.15420342\n",
      "Iteration 164, loss = 0.04969814\n",
      "Iteration 57, loss = 0.15020895\n",
      "Iteration 165, loss = 0.05087849\n",
      "Iteration 58, loss = 0.14941474\n",
      "Iteration 166, loss = 0.04907403\n",
      "Iteration 59, loss = 0.14814930\n",
      "Iteration 167, loss = 0.04915250\n",
      "Iteration 60, loss = 0.14335717\n",
      "Iteration 168, loss = 0.04998443\n",
      "Iteration 61, loss = 0.14132797\n",
      "Iteration 169, loss = 0.04925582\n",
      "Iteration 62, loss = 0.13912168\n",
      "Iteration 170, loss = 0.04720359\n",
      "Iteration 63, loss = 0.13821031\n",
      "Iteration 171, loss = 0.04591767\n",
      "Iteration 64, loss = 0.13698584\n",
      "Iteration 172, loss = 0.04630610\n",
      "Iteration 65, loss = 0.13392073\n",
      "Iteration 173, loss = 0.04596164\n",
      "Iteration 66, loss = 0.13100121\n",
      "Iteration 174, loss = 0.04747773\n",
      "Iteration 67, loss = 0.13026786\n",
      "Iteration 175, loss = 0.04389948\n",
      "Iteration 68, loss = 0.12690115\n",
      "Iteration 176, loss = 0.04407261\n",
      "Iteration 69, loss = 0.12656980\n",
      "Iteration 177, loss = 0.04388155\n",
      "Iteration 70, loss = 0.12917244\n",
      "Iteration 178, loss = 0.04333164\n",
      "Iteration 71, loss = 0.12669202\n",
      "Iteration 179, loss = 0.04352929\n",
      "Iteration 72, loss = 0.12573146\n",
      "Iteration 180, loss = 0.04276718\n",
      "Iteration 73, loss = 0.11956153\n",
      "Iteration 181, loss = 0.04198263\n",
      "Iteration 74, loss = 0.11862057\n",
      "Iteration 182, loss = 0.04272169\n",
      "Iteration 75, loss = 0.11701325\n",
      "Iteration 183, loss = 0.04145815\n",
      "Iteration 76, loss = 0.11513620\n",
      "Iteration 184, loss = 0.04367278\n",
      "Iteration 77, loss = 0.11154429\n",
      "Iteration 185, loss = 0.04127007\n",
      "Iteration 78, loss = 0.11286864\n",
      "Iteration 186, loss = 0.04051598\n",
      "Iteration 79, loss = 0.11238430\n",
      "Iteration 187, loss = 0.04046580\n",
      "Iteration 80, loss = 0.10657791\n",
      "Iteration 188, loss = 0.03971805\n",
      "Iteration 81, loss = 0.10709171\n",
      "Iteration 189, loss = 0.04019376\n",
      "Iteration 82, loss = 0.10518457\n",
      "Iteration 190, loss = 0.03925565\n",
      "Iteration 83, loss = 0.10894029\n",
      "Iteration 191, loss = 0.03873541\n",
      "Iteration 84, loss = 0.10273663\n",
      "Iteration 192, loss = 0.03932196\n",
      "Iteration 85, loss = 0.10213201\n",
      "Iteration 193, loss = 0.03855119\n",
      "Iteration 86, loss = 0.10016793\n",
      "Iteration 194, loss = 0.03742953\n",
      "Iteration 87, loss = 0.09877283\n",
      "Iteration 195, loss = 0.03791254\n",
      "Iteration 88, loss = 0.09660594\n",
      "Iteration 196, loss = 0.03843214\n",
      "Iteration 89, loss = 0.09358925\n",
      "Iteration 197, loss = 0.04102299\n",
      "Iteration 90, loss = 0.09572191\n",
      "Iteration 198, loss = 0.04210177\n",
      "Iteration 91, loss = 0.09242103\n",
      "Iteration 199, loss = 0.04616419\n",
      "Iteration 92, loss = 0.09326472\n",
      "Iteration 200, loss = 0.05319584\n",
      "Iteration 93, loss = 0.09157004\n",
      "Iteration 201, loss = 0.04403594\n",
      "Iteration 94, loss = 0.09079066\n",
      "Iteration 202, loss = 0.04069052\n",
      "Iteration 95, loss = 0.08826986\n",
      "Iteration 203, loss = 0.04145410\n",
      "Iteration 96, loss = 0.08597518\n",
      "Iteration 204, loss = 0.03682340\n",
      "Iteration 97, loss = 0.08362445\n",
      "Iteration 205, loss = 0.03557276\n",
      "Iteration 98, loss = 0.08515278\n",
      "Iteration 206, loss = 0.03666658\n",
      "Iteration 99, loss = 0.08288825\n",
      "Iteration 207, loss = 0.03685490\n",
      "Iteration 100, loss = 0.08346967\n",
      "Iteration 208, loss = 0.03724222\n",
      "Iteration 101, loss = 0.08030320\n",
      "Iteration 209, loss = 0.03507798\n",
      "Iteration 102, loss = 0.08011044\n",
      "Iteration 210, loss = 0.03513572\n",
      "Iteration 103, loss = 0.07761748\n",
      "Iteration 211, loss = 0.03483454\n",
      "Iteration 104, loss = 0.07660714\n",
      "Iteration 212, loss = 0.03705950\n",
      "Iteration 105, loss = 0.07516579\n",
      "Iteration 213, loss = 0.03817681\n",
      "Iteration 106, loss = 0.07533774\n",
      "Iteration 214, loss = 0.03558734\n",
      "Iteration 107, loss = 0.07498345\n",
      "Iteration 215, loss = 0.03397857\n",
      "Iteration 108, loss = 0.07396268\n",
      "Iteration 216, loss = 0.03276015\n",
      "Iteration 109, loss = 0.07188653\n",
      "Iteration 217, loss = 0.03290400\n",
      "Iteration 110, loss = 0.06916217\n",
      "Iteration 218, loss = 0.03261848\n",
      "Iteration 111, loss = 0.07049888\n",
      "Iteration 219, loss = 0.03394701\n",
      "Iteration 112, loss = 0.07188739\n",
      "Iteration 220, loss = 0.03303428\n",
      "Iteration 113, loss = 0.06883171\n",
      "Iteration 221, loss = 0.03168989\n",
      "Iteration 114, loss = 0.06566084\n",
      "Iteration 222, loss = 0.03315545\n",
      "Iteration 115, loss = 0.06715681\n",
      "Iteration 223, loss = 0.03161313\n",
      "Iteration 116, loss = 0.07177216\n",
      "Iteration 224, loss = 0.03193048\n",
      "Iteration 117, loss = 0.06671302\n",
      "Iteration 225, loss = 0.03118834\n",
      "Iteration 118, loss = 0.06547001\n",
      "Iteration 226, loss = 0.03175404\n",
      "Iteration 119, loss = 0.06225357\n",
      "Iteration 227, loss = 0.03135959\n",
      "Iteration 120, loss = 0.06260990\n",
      "Iteration 228, loss = 0.03159571\n",
      "Iteration 121, loss = 0.06315276\n",
      "Iteration 229, loss = 0.03145526\n",
      "Iteration 122, loss = 0.05813426\n",
      "Iteration 230, loss = 0.03420966\n",
      "Iteration 123, loss = 0.06024273\n",
      "Iteration 231, loss = 0.03671456\n",
      "Iteration 124, loss = 0.05841980\n",
      "Iteration 232, loss = 0.03258605\n",
      "Iteration 125, loss = 0.05797928\n",
      "Iteration 233, loss = 0.03416002\n",
      "Iteration 126, loss = 0.05748961\n",
      "Iteration 234, loss = 0.03539803\n",
      "Iteration 127, loss = 0.05569570\n",
      "Iteration 235, loss = 0.03062438\n",
      "Iteration 128, loss = 0.05475119\n",
      "Iteration 236, loss = 0.03021712\n",
      "Iteration 129, loss = 0.05623538\n",
      "Iteration 237, loss = 0.02950495\n",
      "Iteration 130, loss = 0.05377188\n",
      "Iteration 238, loss = 0.03111960\n",
      "Iteration 131, loss = 0.05247189\n",
      "Iteration 239, loss = 0.02988571\n",
      "Iteration 132, loss = 0.05177122\n",
      "Iteration 240, loss = 0.02880618\n",
      "Iteration 133, loss = 0.05159753\n",
      "Iteration 241, loss = 0.02914593\n",
      "Iteration 134, loss = 0.05045485\n",
      "Iteration 242, loss = 0.02954142\n",
      "Iteration 135, loss = 0.05084467\n",
      "Iteration 243, loss = 0.03166149\n",
      "Iteration 136, loss = 0.05065234\n",
      "Iteration 244, loss = 0.02866270\n",
      "Iteration 137, loss = 0.05228471\n",
      "Iteration 245, loss = 0.02878948\n",
      "Iteration 138, loss = 0.05273297\n",
      "Iteration 246, loss = 0.03123355\n",
      "Iteration 139, loss = 0.04798617\n",
      "Iteration 247, loss = 0.03311330\n",
      "Iteration 140, loss = 0.04682547\n",
      "Iteration 248, loss = 0.03665537\n",
      "Iteration 141, loss = 0.04772374\n",
      "Iteration 249, loss = 0.03668615\n",
      "Iteration 142, loss = 0.04665347\n",
      "Iteration 250, loss = 0.03463110\n",
      "Iteration 143, loss = 0.04580444\n",
      "Iteration 251, loss = 0.04110539\n",
      "Iteration 144, loss = 0.04413618\n",
      "Iteration 252, loss = 0.04120116\n",
      "Iteration 145, loss = 0.04393150\n",
      "Iteration 253, loss = 0.03452216\n",
      "Iteration 146, loss = 0.04417770\n",
      "Iteration 254, loss = 0.03140956\n",
      "Iteration 147, loss = 0.04516087\n",
      "Iteration 255, loss = 0.02857247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 148, loss = 0.04485893\n",
      "Iteration 149, loss = 0.04687024\n",
      "Iteration 150, loss = 0.04503722\n",
      "Iteration 151, loss = 0.04626370\n",
      "Iteration 152, loss = 0.04537292\n",
      "Iteration 153, loss = 0.04248080\n",
      "Iteration 154, loss = 0.04070508\n",
      "Iteration 155, loss = 0.03863658\n",
      "Iteration 156, loss = 0.03948615\n",
      "Iteration 157, loss = 0.04108630\n",
      "Iteration 158, loss = 0.04165056\n",
      "Iteration 159, loss = 0.03814969\n",
      "Iteration 1, loss = 0.96602511\n",
      "Iteration 160, loss = 0.03737510\n",
      "Iteration 2, loss = 0.76244019\n",
      "Iteration 161, loss = 0.03694085\n",
      "Iteration 3, loss = 0.65152488\n",
      "Iteration 162, loss = 0.03510894\n",
      "Iteration 4, loss = 0.57430635\n",
      "Iteration 163, loss = 0.03574335\n",
      "Iteration 5, loss = 0.52382050\n",
      "Iteration 164, loss = 0.03541889\n",
      "Iteration 6, loss = 0.48292884\n",
      "Iteration 165, loss = 0.03630962\n",
      "Iteration 7, loss = 0.45489734\n",
      "Iteration 166, loss = 0.03434602\n",
      "Iteration 8, loss = 0.43311891\n",
      "Iteration 167, loss = 0.03362319\n",
      "Iteration 9, loss = 0.41410035\n",
      "Iteration 168, loss = 0.03643238\n",
      "Iteration 10, loss = 0.39761000\n",
      "Iteration 169, loss = 0.03394374\n",
      "Iteration 11, loss = 0.38345843\n",
      "Iteration 170, loss = 0.03222356\n",
      "Iteration 12, loss = 0.36956300\n",
      "Iteration 171, loss = 0.03302999\n",
      "Iteration 13, loss = 0.35717471\n",
      "Iteration 172, loss = 0.03599191\n",
      "Iteration 14, loss = 0.34631684\n",
      "Iteration 173, loss = 0.03438175\n",
      "Iteration 15, loss = 0.33722425\n",
      "Iteration 174, loss = 0.03260514\n",
      "Iteration 16, loss = 0.32764635\n",
      "Iteration 175, loss = 0.03484771\n",
      "Iteration 17, loss = 0.31971135\n",
      "Iteration 176, loss = 0.03042403\n",
      "Iteration 18, loss = 0.31398869\n",
      "Iteration 177, loss = 0.03016492\n",
      "Iteration 19, loss = 0.30367065\n",
      "Iteration 178, loss = 0.03039024\n",
      "Iteration 20, loss = 0.29574456\n",
      "Iteration 179, loss = 0.03111043\n",
      "Iteration 21, loss = 0.28918678\n",
      "Iteration 180, loss = 0.03001351\n",
      "Iteration 22, loss = 0.28214545\n",
      "Iteration 181, loss = 0.03015003\n",
      "Iteration 23, loss = 0.27747312\n",
      "Iteration 182, loss = 0.02885252\n",
      "Iteration 24, loss = 0.26967613\n",
      "Iteration 183, loss = 0.02771426\n",
      "Iteration 25, loss = 0.26489024\n",
      "Iteration 184, loss = 0.02871691\n",
      "Iteration 26, loss = 0.25897458\n",
      "Iteration 185, loss = 0.03087341\n",
      "Iteration 27, loss = 0.25457575\n",
      "Iteration 186, loss = 0.02836939\n",
      "Iteration 28, loss = 0.24695662\n",
      "Iteration 187, loss = 0.02875981\n",
      "Iteration 29, loss = 0.24254398\n",
      "Iteration 188, loss = 0.02772476\n",
      "Iteration 30, loss = 0.24109886\n",
      "Iteration 189, loss = 0.02814875\n",
      "Iteration 31, loss = 0.23637213\n",
      "Iteration 190, loss = 0.02688712\n",
      "Iteration 32, loss = 0.22894588\n",
      "Iteration 191, loss = 0.02863290\n",
      "Iteration 33, loss = 0.22802749\n",
      "Iteration 192, loss = 0.02797679\n",
      "Iteration 34, loss = 0.22052338\n",
      "Iteration 193, loss = 0.03108448\n",
      "Iteration 35, loss = 0.21680165\n",
      "Iteration 194, loss = 0.03001852\n",
      "Iteration 36, loss = 0.21122361\n",
      "Iteration 195, loss = 0.02590600\n",
      "Iteration 37, loss = 0.20784707\n",
      "Iteration 196, loss = 0.02458254\n",
      "Iteration 38, loss = 0.20516639\n",
      "Iteration 197, loss = 0.02482405\n",
      "Iteration 39, loss = 0.20586178\n",
      "Iteration 198, loss = 0.02534536\n",
      "Iteration 40, loss = 0.20227329\n",
      "Iteration 199, loss = 0.02340853\n",
      "Iteration 41, loss = 0.19624121\n",
      "Iteration 42, loss = 0.19259279\n",
      "Iteration 200, loss = 0.02368477\n",
      "Iteration 43, loss = 0.18916545\n",
      "Iteration 201, loss = 0.02359935\n",
      "Iteration 44, loss = 0.18349623\n",
      "Iteration 202, loss = 0.02333789\n",
      "Iteration 45, loss = 0.18076374\n",
      "Iteration 203, loss = 0.02268060\n",
      "Iteration 46, loss = 0.17978496\n",
      "Iteration 204, loss = 0.02277823\n",
      "Iteration 47, loss = 0.17632914\n",
      "Iteration 205, loss = 0.02284000\n",
      "Iteration 48, loss = 0.17097699\n",
      "Iteration 206, loss = 0.02224750\n",
      "Iteration 49, loss = 0.17139192\n",
      "Iteration 207, loss = 0.02248559\n",
      "Iteration 50, loss = 0.16550559\n",
      "Iteration 208, loss = 0.02175924\n",
      "Iteration 51, loss = 0.16415935\n",
      "Iteration 209, loss = 0.02228976\n",
      "Iteration 52, loss = 0.16153374\n",
      "Iteration 210, loss = 0.02161706\n",
      "Iteration 53, loss = 0.15861529\n",
      "Iteration 211, loss = 0.02114912\n",
      "Iteration 54, loss = 0.15713875\n",
      "Iteration 212, loss = 0.02105979\n",
      "Iteration 55, loss = 0.15420115\n",
      "Iteration 213, loss = 0.02065054\n",
      "Iteration 56, loss = 0.15190916\n",
      "Iteration 214, loss = 0.02049996\n",
      "Iteration 57, loss = 0.15143152\n",
      "Iteration 215, loss = 0.02111955\n",
      "Iteration 58, loss = 0.14786842\n",
      "Iteration 216, loss = 0.02168824\n",
      "Iteration 59, loss = 0.14659196\n",
      "Iteration 217, loss = 0.01987956\n",
      "Iteration 60, loss = 0.14555892\n",
      "Iteration 218, loss = 0.01956904\n",
      "Iteration 61, loss = 0.14387328\n",
      "Iteration 219, loss = 0.01929151\n",
      "Iteration 62, loss = 0.14082873\n",
      "Iteration 220, loss = 0.01958310\n",
      "Iteration 63, loss = 0.13999255\n",
      "Iteration 221, loss = 0.01942705\n",
      "Iteration 64, loss = 0.13594306\n",
      "Iteration 222, loss = 0.01843336\n",
      "Iteration 65, loss = 0.13510450\n",
      "Iteration 223, loss = 0.01867508\n",
      "Iteration 66, loss = 0.13511953\n",
      "Iteration 224, loss = 0.01846320\n",
      "Iteration 67, loss = 0.13438053\n",
      "Iteration 225, loss = 0.01983602\n",
      "Iteration 68, loss = 0.13143148\n",
      "Iteration 226, loss = 0.01994463\n",
      "Iteration 69, loss = 0.13223289\n",
      "Iteration 227, loss = 0.01939474\n",
      "Iteration 70, loss = 0.12943193\n",
      "Iteration 228, loss = 0.01923321\n",
      "Iteration 71, loss = 0.12610760\n",
      "Iteration 229, loss = 0.01750301\n",
      "Iteration 72, loss = 0.12499163\n",
      "Iteration 230, loss = 0.01755153\n",
      "Iteration 73, loss = 0.12335199\n",
      "Iteration 231, loss = 0.01722670\n",
      "Iteration 74, loss = 0.12195321\n",
      "Iteration 232, loss = 0.01785293\n",
      "Iteration 75, loss = 0.11972488\n",
      "Iteration 233, loss = 0.01683768\n",
      "Iteration 76, loss = 0.11857351\n",
      "Iteration 234, loss = 0.01654834\n",
      "Iteration 77, loss = 0.11812591\n",
      "Iteration 235, loss = 0.01651997\n",
      "Iteration 78, loss = 0.11671985\n",
      "Iteration 236, loss = 0.01624992\n",
      "Iteration 79, loss = 0.11607871\n",
      "Iteration 237, loss = 0.01631610\n",
      "Iteration 80, loss = 0.11404868\n",
      "Iteration 238, loss = 0.01601259\n",
      "Iteration 81, loss = 0.11351229\n",
      "Iteration 239, loss = 0.01632664\n",
      "Iteration 82, loss = 0.11329234\n",
      "Iteration 240, loss = 0.01582669\n",
      "Iteration 83, loss = 0.11191787\n",
      "Iteration 241, loss = 0.01679943\n",
      "Iteration 84, loss = 0.11202947\n",
      "Iteration 242, loss = 0.01797406\n",
      "Iteration 85, loss = 0.11117398\n",
      "Iteration 243, loss = 0.02016396\n",
      "Iteration 86, loss = 0.10764330\n",
      "Iteration 244, loss = 0.02016630\n",
      "Iteration 87, loss = 0.10598583\n",
      "Iteration 245, loss = 0.01779681\n",
      "Iteration 88, loss = 0.10448170\n",
      "Iteration 246, loss = 0.01735004\n",
      "Iteration 89, loss = 0.10438822\n",
      "Iteration 247, loss = 0.01470180\n",
      "Iteration 90, loss = 0.10282536\n",
      "Iteration 248, loss = 0.01632913\n",
      "Iteration 91, loss = 0.10245549\n",
      "Iteration 249, loss = 0.01637472\n",
      "Iteration 92, loss = 0.10397937\n",
      "Iteration 250, loss = 0.01549047\n",
      "Iteration 93, loss = 0.10280850\n",
      "Iteration 251, loss = 0.01550291\n",
      "Iteration 94, loss = 0.10025764\n",
      "Iteration 252, loss = 0.01558724\n",
      "Iteration 95, loss = 0.09838507\n",
      "Iteration 253, loss = 0.01471578\n",
      "Iteration 96, loss = 0.09688434\n",
      "Iteration 254, loss = 0.01501090\n",
      "Iteration 97, loss = 0.09491273\n",
      "Iteration 255, loss = 0.01463495\n",
      "Iteration 98, loss = 0.09614227\n",
      "Iteration 256, loss = 0.01395825\n",
      "Iteration 99, loss = 0.09301643\n",
      "Iteration 257, loss = 0.01386959\n",
      "Iteration 100, loss = 0.09258230\n",
      "Iteration 258, loss = 0.01354160\n",
      "Iteration 101, loss = 0.09166794\n",
      "Iteration 259, loss = 0.01305848\n",
      "Iteration 102, loss = 0.09179130\n",
      "Iteration 260, loss = 0.01345852\n",
      "Iteration 103, loss = 0.09041371\n",
      "Iteration 261, loss = 0.01318357\n",
      "Iteration 104, loss = 0.08937791\n",
      "Iteration 262, loss = 0.01339315\n",
      "Iteration 105, loss = 0.09079440\n",
      "Iteration 263, loss = 0.01297425\n",
      "Iteration 106, loss = 0.08747464\n",
      "Iteration 264, loss = 0.01283905\n",
      "Iteration 107, loss = 0.08709295\n",
      "Iteration 265, loss = 0.01275147\n",
      "Iteration 108, loss = 0.08609229\n",
      "Iteration 266, loss = 0.01270012\n",
      "Iteration 109, loss = 0.08606786\n",
      "Iteration 267, loss = 0.01316229\n",
      "Iteration 110, loss = 0.08726446\n",
      "Iteration 268, loss = 0.01353504\n",
      "Iteration 111, loss = 0.08851056\n",
      "Iteration 269, loss = 0.01332141\n",
      "Iteration 112, loss = 0.08049221\n",
      "Iteration 270, loss = 0.01259217\n",
      "Iteration 113, loss = 0.08530524\n",
      "Iteration 271, loss = 0.01208178\n",
      "Iteration 114, loss = 0.08458612\n",
      "Iteration 115, loss = 0.08643973\n",
      "Iteration 272, loss = 0.01308055\n",
      "Iteration 116, loss = 0.08172362\n",
      "Iteration 273, loss = 0.01339550\n",
      "Iteration 117, loss = 0.08217452\n",
      "Iteration 274, loss = 0.01212569\n",
      "Iteration 118, loss = 0.08674826\n",
      "Iteration 275, loss = 0.01302969\n",
      "Iteration 119, loss = 0.08114882\n",
      "Iteration 276, loss = 0.01464261\n",
      "Iteration 120, loss = 0.07887611\n",
      "Iteration 277, loss = 0.01437104\n",
      "Iteration 121, loss = 0.07602260\n",
      "Iteration 278, loss = 0.01380761\n",
      "Iteration 122, loss = 0.07543406\n",
      "Iteration 279, loss = 0.01210733\n",
      "Iteration 123, loss = 0.07522679\n",
      "Iteration 280, loss = 0.01151652\n",
      "Iteration 124, loss = 0.07315074\n",
      "Iteration 281, loss = 0.01178027\n",
      "Iteration 125, loss = 0.07480971\n",
      "Iteration 282, loss = 0.01134286\n",
      "Iteration 126, loss = 0.07183916\n",
      "Iteration 283, loss = 0.01128149\n",
      "Iteration 127, loss = 0.07090232\n",
      "Iteration 284, loss = 0.01090240\n",
      "Iteration 128, loss = 0.07185353\n",
      "Iteration 285, loss = 0.01095572\n",
      "Iteration 129, loss = 0.07316692\n",
      "Iteration 286, loss = 0.01082264\n",
      "Iteration 130, loss = 0.07003070\n",
      "Iteration 287, loss = 0.01081339\n",
      "Iteration 288, loss = 0.01125839\n",
      "Iteration 131, loss = 0.06789827\n",
      "Iteration 289, loss = 0.01026624\n",
      "Iteration 132, loss = 0.06913059\n",
      "Iteration 290, loss = 0.01060354\n",
      "Iteration 133, loss = 0.06685815\n",
      "Iteration 291, loss = 0.01193108\n",
      "Iteration 134, loss = 0.06623543\n",
      "Iteration 292, loss = 0.01340934\n",
      "Iteration 135, loss = 0.06633283\n",
      "Iteration 293, loss = 0.01359976\n",
      "Iteration 136, loss = 0.06699198\n",
      "Iteration 294, loss = 0.01227420\n",
      "Iteration 137, loss = 0.06667744\n",
      "Iteration 295, loss = 0.01173640\n",
      "Iteration 138, loss = 0.06352320\n",
      "Iteration 296, loss = 0.01324044\n",
      "Iteration 139, loss = 0.06706209\n",
      "Iteration 297, loss = 0.01258831\n",
      "Iteration 140, loss = 0.06468616\n",
      "Iteration 298, loss = 0.01196800\n",
      "Iteration 141, loss = 0.06252701\n",
      "Iteration 299, loss = 0.01120786\n",
      "Iteration 142, loss = 0.06123704\n",
      "Iteration 300, loss = 0.01367316\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 143, loss = 0.06684456\n",
      "Iteration 144, loss = 0.06442713\n",
      "Iteration 145, loss = 0.06507864\n",
      "Iteration 146, loss = 0.06286366\n",
      "Iteration 147, loss = 0.06960196\n",
      "Iteration 148, loss = 0.06935947\n",
      "Iteration 149, loss = 0.06401388\n",
      "Iteration 150, loss = 0.05856104\n",
      "Iteration 151, loss = 0.05815503\n",
      "Iteration 152, loss = 0.06310064\n",
      "Iteration 153, loss = 0.05699238\n",
      "Iteration 154, loss = 0.05779650\n",
      "Iteration 155, loss = 0.05527033\n",
      "Iteration 156, loss = 0.05443432Iteration 1, loss = 1.46784268\n",
      "\n",
      "Iteration 157, loss = 0.05410900Iteration 2, loss = 1.02518111\n",
      "\n",
      "Iteration 158, loss = 0.05299906\n",
      "Iteration 3, loss = 0.80038533\n",
      "Iteration 4, loss = 0.67209167\n",
      "Iteration 159, loss = 0.05406278\n",
      "Iteration 5, loss = 0.59271610\n",
      "Iteration 160, loss = 0.05434954\n",
      "Iteration 6, loss = 0.54083633\n",
      "Iteration 161, loss = 0.05544378\n",
      "Iteration 7, loss = 0.50379022\n",
      "Iteration 162, loss = 0.05292798\n",
      "Iteration 8, loss = 0.47648470\n",
      "Iteration 163, loss = 0.05333044\n",
      "Iteration 9, loss = 0.45385012\n",
      "Iteration 164, loss = 0.05585028\n",
      "Iteration 10, loss = 0.43326906\n",
      "Iteration 165, loss = 0.05435639\n",
      "Iteration 11, loss = 0.41876351\n",
      "Iteration 166, loss = 0.05396419\n",
      "Iteration 12, loss = 0.40464800\n",
      "Iteration 167, loss = 0.05397607\n",
      "Iteration 13, loss = 0.39259750\n",
      "Iteration 168, loss = 0.05009022\n",
      "Iteration 14, loss = 0.38295874\n",
      "Iteration 169, loss = 0.05433863\n",
      "Iteration 15, loss = 0.37195354\n",
      "Iteration 170, loss = 0.05008316\n",
      "Iteration 16, loss = 0.36289441\n",
      "Iteration 171, loss = 0.05012419\n",
      "Iteration 17, loss = 0.35371361\n",
      "Iteration 172, loss = 0.05018244\n",
      "Iteration 18, loss = 0.34612485\n",
      "Iteration 173, loss = 0.05397426\n",
      "Iteration 19, loss = 0.33973159\n",
      "Iteration 174, loss = 0.05244916\n",
      "Iteration 20, loss = 0.33147514\n",
      "Iteration 175, loss = 0.04785980\n",
      "Iteration 21, loss = 0.32434497\n",
      "Iteration 176, loss = 0.04663690\n",
      "Iteration 177, loss = 0.04542467\n",
      "Iteration 22, loss = 0.31716422\n",
      "Iteration 178, loss = 0.04517781\n",
      "Iteration 23, loss = 0.31028866\n",
      "Iteration 179, loss = 0.04468511\n",
      "Iteration 24, loss = 0.30453079\n",
      "Iteration 180, loss = 0.04865936\n",
      "Iteration 25, loss = 0.29775853\n",
      "Iteration 26, loss = 0.29230301\n",
      "Iteration 181, loss = 0.04396762\n",
      "Iteration 27, loss = 0.28585627\n",
      "Iteration 182, loss = 0.04333201\n",
      "Iteration 28, loss = 0.27966778\n",
      "Iteration 183, loss = 0.04446369\n",
      "Iteration 29, loss = 0.27451455\n",
      "Iteration 184, loss = 0.04749553\n",
      "Iteration 30, loss = 0.26886824\n",
      "Iteration 185, loss = 0.04648587\n",
      "Iteration 31, loss = 0.26362980\n",
      "Iteration 186, loss = 0.04559932\n",
      "Iteration 187, loss = 0.04308642\n",
      "Iteration 32, loss = 0.25812141\n",
      "Iteration 188, loss = 0.04491129\n",
      "Iteration 33, loss = 0.25478009\n",
      "Iteration 189, loss = 0.04176634\n",
      "Iteration 34, loss = 0.24930679\n",
      "Iteration 190, loss = 0.04120923\n",
      "Iteration 35, loss = 0.24715104\n",
      "Iteration 191, loss = 0.04140821\n",
      "Iteration 36, loss = 0.23925842\n",
      "Iteration 192, loss = 0.04078693\n",
      "Iteration 37, loss = 0.23452568\n",
      "Iteration 193, loss = 0.04072107\n",
      "Iteration 38, loss = 0.23007582\n",
      "Iteration 194, loss = 0.04197168\n",
      "Iteration 39, loss = 0.22638550\n",
      "Iteration 195, loss = 0.03955288\n",
      "Iteration 40, loss = 0.22211623\n",
      "Iteration 196, loss = 0.04151497\n",
      "Iteration 41, loss = 0.21701567\n",
      "Iteration 197, loss = 0.04703194\n",
      "Iteration 42, loss = 0.21350203\n",
      "Iteration 198, loss = 0.04443221\n",
      "Iteration 43, loss = 0.20899068\n",
      "Iteration 199, loss = 0.04061564\n",
      "Iteration 44, loss = 0.20724368\n",
      "Iteration 200, loss = 0.04309098\n",
      "Iteration 45, loss = 0.20130203\n",
      "Iteration 201, loss = 0.04081343\n",
      "Iteration 46, loss = 0.19984885\n",
      "Iteration 202, loss = 0.04216572\n",
      "Iteration 47, loss = 0.19560569\n",
      "Iteration 203, loss = 0.03814152\n",
      "Iteration 48, loss = 0.19115262\n",
      "Iteration 204, loss = 0.03888283\n",
      "Iteration 49, loss = 0.18862291\n",
      "Iteration 205, loss = 0.03721350\n",
      "Iteration 50, loss = 0.18475864\n",
      "Iteration 206, loss = 0.03586363\n",
      "Iteration 51, loss = 0.18167512\n",
      "Iteration 207, loss = 0.03584614\n",
      "Iteration 52, loss = 0.17855648\n",
      "Iteration 208, loss = 0.03612056\n",
      "Iteration 53, loss = 0.17945079\n",
      "Iteration 209, loss = 0.03571907\n",
      "Iteration 54, loss = 0.17286784\n",
      "Iteration 210, loss = 0.03491147\n",
      "Iteration 55, loss = 0.17163565\n",
      "Iteration 211, loss = 0.03772512\n",
      "Iteration 56, loss = 0.16822780\n",
      "Iteration 212, loss = 0.03874484\n",
      "Iteration 57, loss = 0.16736774\n",
      "Iteration 213, loss = 0.03558496\n",
      "Iteration 58, loss = 0.16527290\n",
      "Iteration 214, loss = 0.03592042\n",
      "Iteration 59, loss = 0.16118646\n",
      "Iteration 215, loss = 0.03592333\n",
      "Iteration 60, loss = 0.16037354\n",
      "Iteration 216, loss = 0.03936275\n",
      "Iteration 61, loss = 0.15638551\n",
      "Iteration 217, loss = 0.03356165\n",
      "Iteration 62, loss = 0.15332060\n",
      "Iteration 218, loss = 0.03555345\n",
      "Iteration 63, loss = 0.15188408\n",
      "Iteration 219, loss = 0.04442836\n",
      "Iteration 64, loss = 0.14851263\n",
      "Iteration 220, loss = 0.04377799\n",
      "Iteration 65, loss = 0.14563939\n",
      "Iteration 221, loss = 0.03601347\n",
      "Iteration 66, loss = 0.14457638\n",
      "Iteration 222, loss = 0.03574349\n",
      "Iteration 67, loss = 0.14278951\n",
      "Iteration 223, loss = 0.03514945\n",
      "Iteration 68, loss = 0.14006244\n",
      "Iteration 224, loss = 0.03338551\n",
      "Iteration 69, loss = 0.13813135\n",
      "Iteration 225, loss = 0.03251891\n",
      "Iteration 70, loss = 0.13647709\n",
      "Iteration 226, loss = 0.03291484\n",
      "Iteration 71, loss = 0.13599736\n",
      "Iteration 227, loss = 0.03189884\n",
      "Iteration 72, loss = 0.13211887\n",
      "Iteration 228, loss = 0.03220395\n",
      "Iteration 73, loss = 0.13282562\n",
      "Iteration 229, loss = 0.03147654\n",
      "Iteration 74, loss = 0.12999051\n",
      "Iteration 230, loss = 0.03178054\n",
      "Iteration 75, loss = 0.12788047\n",
      "Iteration 231, loss = 0.03021927\n",
      "Iteration 76, loss = 0.12666313\n",
      "Iteration 232, loss = 0.03282467\n",
      "Iteration 77, loss = 0.12693938\n",
      "Iteration 233, loss = 0.03605814\n",
      "Iteration 78, loss = 0.12403419\n",
      "Iteration 234, loss = 0.04296288\n",
      "Iteration 79, loss = 0.12144579\n",
      "Iteration 235, loss = 0.03253731\n",
      "Iteration 80, loss = 0.12067154\n",
      "Iteration 236, loss = 0.03324714\n",
      "Iteration 81, loss = 0.11947339\n",
      "Iteration 237, loss = 0.03190092\n",
      "Iteration 82, loss = 0.11821242\n",
      "Iteration 238, loss = 0.03286563\n",
      "Iteration 83, loss = 0.11565437\n",
      "Iteration 239, loss = 0.03726845\n",
      "Iteration 84, loss = 0.11431714\n",
      "Iteration 240, loss = 0.03467028\n",
      "Iteration 85, loss = 0.11317789\n",
      "Iteration 241, loss = 0.02917946\n",
      "Iteration 86, loss = 0.11152225\n",
      "Iteration 242, loss = 0.02965913\n",
      "Iteration 87, loss = 0.11280618\n",
      "Iteration 243, loss = 0.02812962\n",
      "Iteration 88, loss = 0.10970839\n",
      "Iteration 244, loss = 0.02923648\n",
      "Iteration 89, loss = 0.10891395\n",
      "Iteration 245, loss = 0.02949467\n",
      "Iteration 90, loss = 0.10682190\n",
      "Iteration 246, loss = 0.02818148\n",
      "Iteration 91, loss = 0.10608472\n",
      "Iteration 247, loss = 0.02897014\n",
      "Iteration 92, loss = 0.10455072\n",
      "Iteration 248, loss = 0.02752213\n",
      "Iteration 93, loss = 0.10715383\n",
      "Iteration 249, loss = 0.02805115\n",
      "Iteration 94, loss = 0.10547662\n",
      "Iteration 250, loss = 0.02772428\n",
      "Iteration 95, loss = 0.10049710\n",
      "Iteration 251, loss = 0.03169958\n",
      "Iteration 96, loss = 0.09990714\n",
      "Iteration 252, loss = 0.03037432\n",
      "Iteration 97, loss = 0.09996711\n",
      "Iteration 253, loss = 0.03251498\n",
      "Iteration 98, loss = 0.09700704\n",
      "Iteration 254, loss = 0.02723203\n",
      "Iteration 99, loss = 0.09535795\n",
      "Iteration 255, loss = 0.02670973\n",
      "Iteration 100, loss = 0.09553744\n",
      "Iteration 256, loss = 0.02653951\n",
      "Iteration 101, loss = 0.09403351\n",
      "Iteration 257, loss = 0.02651638\n",
      "Iteration 102, loss = 0.09261422\n",
      "Iteration 258, loss = 0.02661251\n",
      "Iteration 103, loss = 0.09373856\n",
      "Iteration 259, loss = 0.02646405\n",
      "Iteration 104, loss = 0.09204930\n",
      "Iteration 260, loss = 0.02606232\n",
      "Iteration 105, loss = 0.09076100\n",
      "Iteration 261, loss = 0.02605687\n",
      "Iteration 106, loss = 0.08979906\n",
      "Iteration 262, loss = 0.02576659\n",
      "Iteration 107, loss = 0.08740080\n",
      "Iteration 263, loss = 0.02864003\n",
      "Iteration 108, loss = 0.08789711\n",
      "Iteration 264, loss = 0.02579680\n",
      "Iteration 109, loss = 0.08594157\n",
      "Iteration 265, loss = 0.02606775\n",
      "Iteration 110, loss = 0.08708367\n",
      "Iteration 266, loss = 0.02485902\n",
      "Iteration 111, loss = 0.08886089\n",
      "Iteration 267, loss = 0.02487571\n",
      "Iteration 112, loss = 0.08416569\n",
      "Iteration 268, loss = 0.02381368\n",
      "Iteration 113, loss = 0.08287198\n",
      "Iteration 269, loss = 0.02551750\n",
      "Iteration 114, loss = 0.08083342\n",
      "Iteration 270, loss = 0.02477526\n",
      "Iteration 115, loss = 0.08262720\n",
      "Iteration 271, loss = 0.02446587\n",
      "Iteration 116, loss = 0.08003830\n",
      "Iteration 272, loss = 0.02484049\n",
      "Iteration 117, loss = 0.07866490\n",
      "Iteration 273, loss = 0.02527879\n",
      "Iteration 118, loss = 0.07810713\n",
      "Iteration 274, loss = 0.02818492\n",
      "Iteration 119, loss = 0.07725215\n",
      "Iteration 275, loss = 0.03432489\n",
      "Iteration 120, loss = 0.07651069\n",
      "Iteration 276, loss = 0.03078897\n",
      "Iteration 121, loss = 0.07516579\n",
      "Iteration 277, loss = 0.02731742\n",
      "Iteration 122, loss = 0.07506161\n",
      "Iteration 278, loss = 0.03287363\n",
      "Iteration 123, loss = 0.07511003\n",
      "Iteration 279, loss = 0.03050770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 124, loss = 0.07689753\n",
      "Iteration 125, loss = 0.07441193\n",
      "Iteration 126, loss = 0.07437245\n",
      "Iteration 127, loss = 0.07008322\n",
      "Iteration 128, loss = 0.07007215\n",
      "Iteration 129, loss = 0.07089543\n",
      "Iteration 130, loss = 0.06951927\n",
      "Iteration 131, loss = 0.06981276\n",
      "Iteration 132, loss = 0.07048994\n",
      "Iteration 133, loss = 0.06960994\n",
      "Iteration 134, loss = 0.06900938\n",
      "Iteration 135, loss = 0.06651038\n",
      "Iteration 136, loss = 0.06473383\n",
      "Iteration 1, loss = 0.96699739\n",
      "Iteration 137, loss = 0.06560761\n",
      "Iteration 2, loss = 0.75781368\n",
      "Iteration 138, loss = 0.06346201\n",
      "Iteration 3, loss = 0.63551603\n",
      "Iteration 139, loss = 0.06402529\n",
      "Iteration 4, loss = 0.55188517\n",
      "Iteration 140, loss = 0.06368048\n",
      "Iteration 5, loss = 0.49739870\n",
      "Iteration 141, loss = 0.06112055\n",
      "Iteration 6, loss = 0.46223670\n",
      "Iteration 142, loss = 0.06138771\n",
      "Iteration 7, loss = 0.43474021\n",
      "Iteration 143, loss = 0.06106740\n",
      "Iteration 8, loss = 0.41484656\n",
      "Iteration 144, loss = 0.05879673\n",
      "Iteration 9, loss = 0.39813943\n",
      "Iteration 145, loss = 0.05945871\n",
      "Iteration 10, loss = 0.38401424\n",
      "Iteration 146, loss = 0.05832988\n",
      "Iteration 11, loss = 0.37281145\n",
      "Iteration 147, loss = 0.06006340\n",
      "Iteration 12, loss = 0.36195086\n",
      "Iteration 148, loss = 0.05794241\n",
      "Iteration 13, loss = 0.35308419\n",
      "Iteration 149, loss = 0.06057307\n",
      "Iteration 14, loss = 0.34376115\n",
      "Iteration 150, loss = 0.05986640\n",
      "Iteration 15, loss = 0.33694679\n",
      "Iteration 151, loss = 0.06225232\n",
      "Iteration 16, loss = 0.32886177\n",
      "Iteration 152, loss = 0.05607049\n",
      "Iteration 17, loss = 0.32103910\n",
      "Iteration 153, loss = 0.05459569\n",
      "Iteration 18, loss = 0.31512713\n",
      "Iteration 154, loss = 0.05523029\n",
      "Iteration 19, loss = 0.30796378\n",
      "Iteration 155, loss = 0.05352725\n",
      "Iteration 20, loss = 0.30310588\n",
      "Iteration 156, loss = 0.05246072\n",
      "Iteration 21, loss = 0.29675196\n",
      "Iteration 157, loss = 0.05294078\n",
      "Iteration 22, loss = 0.29124135\n",
      "Iteration 158, loss = 0.05550961\n",
      "Iteration 23, loss = 0.28474711\n",
      "Iteration 159, loss = 0.05319537\n",
      "Iteration 24, loss = 0.27956284\n",
      "Iteration 160, loss = 0.05175711\n",
      "Iteration 25, loss = 0.27405814\n",
      "Iteration 161, loss = 0.05279691\n",
      "Iteration 26, loss = 0.26774753\n",
      "Iteration 162, loss = 0.05211330\n",
      "Iteration 27, loss = 0.26299425\n",
      "Iteration 163, loss = 0.04954014\n",
      "Iteration 28, loss = 0.25787319\n",
      "Iteration 164, loss = 0.04909669\n",
      "Iteration 29, loss = 0.25142220\n",
      "Iteration 165, loss = 0.05036758\n",
      "Iteration 30, loss = 0.24733280\n",
      "Iteration 166, loss = 0.04903629\n",
      "Iteration 31, loss = 0.24250178\n",
      "Iteration 167, loss = 0.04987764\n",
      "Iteration 32, loss = 0.23772521\n",
      "Iteration 168, loss = 0.05073494\n",
      "Iteration 33, loss = 0.23215016\n",
      "Iteration 169, loss = 0.05148825\n",
      "Iteration 34, loss = 0.22811379\n",
      "Iteration 170, loss = 0.04848393\n",
      "Iteration 35, loss = 0.22340959\n",
      "Iteration 171, loss = 0.04816981\n",
      "Iteration 36, loss = 0.22015781\n",
      "Iteration 172, loss = 0.04861259\n",
      "Iteration 37, loss = 0.21517555\n",
      "Iteration 173, loss = 0.04674908\n",
      "Iteration 38, loss = 0.21114893\n",
      "Iteration 174, loss = 0.04571709\n",
      "Iteration 39, loss = 0.20762028\n",
      "Iteration 175, loss = 0.04734688\n",
      "Iteration 40, loss = 0.20381286\n",
      "Iteration 176, loss = 0.04524709\n",
      "Iteration 41, loss = 0.20231274\n",
      "Iteration 177, loss = 0.04491390\n",
      "Iteration 42, loss = 0.19733037\n",
      "Iteration 178, loss = 0.04418498\n",
      "Iteration 43, loss = 0.19313460\n",
      "Iteration 179, loss = 0.04404470\n",
      "Iteration 44, loss = 0.19217007\n",
      "Iteration 180, loss = 0.04311448\n",
      "Iteration 45, loss = 0.18735796\n",
      "Iteration 181, loss = 0.04253328\n",
      "Iteration 46, loss = 0.18527696\n",
      "Iteration 182, loss = 0.04340880\n",
      "Iteration 47, loss = 0.18050932\n",
      "Iteration 183, loss = 0.04391647\n",
      "Iteration 48, loss = 0.17725295\n",
      "Iteration 184, loss = 0.04330991\n",
      "Iteration 49, loss = 0.17508702\n",
      "Iteration 185, loss = 0.04117372\n",
      "Iteration 50, loss = 0.17180601\n",
      "Iteration 186, loss = 0.04076223\n",
      "Iteration 51, loss = 0.17102395\n",
      "Iteration 187, loss = 0.04173244\n",
      "Iteration 52, loss = 0.16653847\n",
      "Iteration 188, loss = 0.04171156\n",
      "Iteration 53, loss = 0.16438765\n",
      "Iteration 189, loss = 0.04238976\n",
      "Iteration 54, loss = 0.16287928\n",
      "Iteration 190, loss = 0.03925964\n",
      "Iteration 55, loss = 0.15860725\n",
      "Iteration 191, loss = 0.03902874\n",
      "Iteration 56, loss = 0.15865146\n",
      "Iteration 192, loss = 0.03842330\n",
      "Iteration 57, loss = 0.15388641\n",
      "Iteration 193, loss = 0.03837622\n",
      "Iteration 58, loss = 0.15102955\n",
      "Iteration 194, loss = 0.03971802\n",
      "Iteration 59, loss = 0.15081911\n",
      "Iteration 195, loss = 0.03730705\n",
      "Iteration 60, loss = 0.14827727\n",
      "Iteration 196, loss = 0.03748742\n",
      "Iteration 61, loss = 0.14652996\n",
      "Iteration 62, loss = 0.14413706\n",
      "Iteration 197, loss = 0.04060631\n",
      "Iteration 63, loss = 0.14089960\n",
      "Iteration 198, loss = 0.04821690\n",
      "Iteration 64, loss = 0.13884164\n",
      "Iteration 199, loss = 0.05289345\n",
      "Iteration 65, loss = 0.13753930\n",
      "Iteration 200, loss = 0.05136068\n",
      "Iteration 66, loss = 0.13655953\n",
      "Iteration 201, loss = 0.04358184\n",
      "Iteration 67, loss = 0.13401419\n",
      "Iteration 202, loss = 0.03928921\n",
      "Iteration 68, loss = 0.13152803\n",
      "Iteration 203, loss = 0.04047438\n",
      "Iteration 69, loss = 0.13114125\n",
      "Iteration 204, loss = 0.04298327\n",
      "Iteration 70, loss = 0.12928346\n",
      "Iteration 205, loss = 0.03640778\n",
      "Iteration 71, loss = 0.12795433\n",
      "Iteration 206, loss = 0.03781547\n",
      "Iteration 72, loss = 0.12798823\n",
      "Iteration 207, loss = 0.03912455\n",
      "Iteration 73, loss = 0.12768224\n",
      "Iteration 208, loss = 0.03686794\n",
      "Iteration 74, loss = 0.12130522\n",
      "Iteration 209, loss = 0.03426411\n",
      "Iteration 75, loss = 0.12128755\n",
      "Iteration 210, loss = 0.03704040\n",
      "Iteration 76, loss = 0.12107771\n",
      "Iteration 211, loss = 0.03596455\n",
      "Iteration 77, loss = 0.12020412\n",
      "Iteration 212, loss = 0.03341718\n",
      "Iteration 78, loss = 0.11833903\n",
      "Iteration 213, loss = 0.03262551\n",
      "Iteration 79, loss = 0.11535175\n",
      "Iteration 214, loss = 0.03217659\n",
      "Iteration 80, loss = 0.11431054\n",
      "Iteration 215, loss = 0.03203446\n",
      "Iteration 81, loss = 0.11252530\n",
      "Iteration 216, loss = 0.03148179\n",
      "Iteration 82, loss = 0.11143055\n",
      "Iteration 217, loss = 0.03225014\n",
      "Iteration 83, loss = 0.11186858\n",
      "Iteration 218, loss = 0.03237441\n",
      "Iteration 84, loss = 0.10850891\n",
      "Iteration 219, loss = 0.03140087\n",
      "Iteration 85, loss = 0.10788456\n",
      "Iteration 220, loss = 0.03166488\n",
      "Iteration 86, loss = 0.10790350\n",
      "Iteration 221, loss = 0.03150762\n",
      "Iteration 87, loss = 0.10438749\n",
      "Iteration 222, loss = 0.02983265\n",
      "Iteration 88, loss = 0.10582343\n",
      "Iteration 223, loss = 0.03027552\n",
      "Iteration 89, loss = 0.10159057\n",
      "Iteration 224, loss = 0.02980490\n",
      "Iteration 90, loss = 0.10050986\n",
      "Iteration 225, loss = 0.03027017\n",
      "Iteration 91, loss = 0.10160357\n",
      "Iteration 226, loss = 0.02926516\n",
      "Iteration 92, loss = 0.09882183\n",
      "Iteration 227, loss = 0.02897470\n",
      "Iteration 93, loss = 0.09862252\n",
      "Iteration 228, loss = 0.02912030\n",
      "Iteration 94, loss = 0.10028252\n",
      "Iteration 229, loss = 0.03037716\n",
      "Iteration 95, loss = 0.09475091\n",
      "Iteration 230, loss = 0.02843833\n",
      "Iteration 96, loss = 0.09417606\n",
      "Iteration 231, loss = 0.02769396\n",
      "Iteration 97, loss = 0.09500214\n",
      "Iteration 232, loss = 0.02865000\n",
      "Iteration 98, loss = 0.09322774\n",
      "Iteration 233, loss = 0.02917257\n",
      "Iteration 99, loss = 0.09060632\n",
      "Iteration 234, loss = 0.02705943\n",
      "Iteration 100, loss = 0.09165626\n",
      "Iteration 235, loss = 0.02802872\n",
      "Iteration 101, loss = 0.08983781\n",
      "Iteration 236, loss = 0.02796932\n",
      "Iteration 102, loss = 0.09264791\n",
      "Iteration 237, loss = 0.02914786\n",
      "Iteration 103, loss = 0.09525343\n",
      "Iteration 238, loss = 0.02929397\n",
      "Iteration 104, loss = 0.08772447\n",
      "Iteration 239, loss = 0.02794394\n",
      "Iteration 105, loss = 0.09642868\n",
      "Iteration 240, loss = 0.03170740\n",
      "Iteration 106, loss = 0.08603704\n",
      "Iteration 241, loss = 0.02843880\n",
      "Iteration 107, loss = 0.08447520\n",
      "Iteration 242, loss = 0.02746236\n",
      "Iteration 108, loss = 0.08491903\n",
      "Iteration 243, loss = 0.02611641\n",
      "Iteration 109, loss = 0.08592011\n",
      "Iteration 244, loss = 0.02694298\n",
      "Iteration 110, loss = 0.08229523\n",
      "Iteration 245, loss = 0.02518960\n",
      "Iteration 111, loss = 0.08004168\n",
      "Iteration 246, loss = 0.02523911\n",
      "Iteration 112, loss = 0.08091340\n",
      "Iteration 247, loss = 0.02522334\n",
      "Iteration 113, loss = 0.07713743\n",
      "Iteration 248, loss = 0.02497304\n",
      "Iteration 114, loss = 0.07800441\n",
      "Iteration 249, loss = 0.02639190\n",
      "Iteration 115, loss = 0.08034193\n",
      "Iteration 250, loss = 0.02443195\n",
      "Iteration 116, loss = 0.08001223\n",
      "Iteration 251, loss = 0.02496976\n",
      "Iteration 117, loss = 0.07800916\n",
      "Iteration 252, loss = 0.02514115\n",
      "Iteration 118, loss = 0.07697022\n",
      "Iteration 253, loss = 0.02421445\n",
      "Iteration 119, loss = 0.07382984\n",
      "Iteration 254, loss = 0.02595873\n",
      "Iteration 120, loss = 0.07351676\n",
      "Iteration 255, loss = 0.02318142\n",
      "Iteration 121, loss = 0.07038246\n",
      "Iteration 256, loss = 0.02398277\n",
      "Iteration 122, loss = 0.07014859\n",
      "Iteration 257, loss = 0.02552170\n",
      "Iteration 123, loss = 0.06895271\n",
      "Iteration 258, loss = 0.02879310\n",
      "Iteration 124, loss = 0.06895880\n",
      "Iteration 259, loss = 0.02678856\n",
      "Iteration 125, loss = 0.06881750\n",
      "Iteration 260, loss = 0.02414363\n",
      "Iteration 126, loss = 0.06904120\n",
      "Iteration 261, loss = 0.02291044\n",
      "Iteration 127, loss = 0.06600354\n",
      "Iteration 262, loss = 0.02236523\n",
      "Iteration 128, loss = 0.06723435\n",
      "Iteration 263, loss = 0.02254533\n",
      "Iteration 129, loss = 0.06628105\n",
      "Iteration 264, loss = 0.02229527\n",
      "Iteration 130, loss = 0.06777263\n",
      "Iteration 265, loss = 0.02296631\n",
      "Iteration 131, loss = 0.06442824\n",
      "Iteration 266, loss = 0.02185647\n",
      "Iteration 132, loss = 0.06380785\n",
      "Iteration 267, loss = 0.02140997\n",
      "Iteration 133, loss = 0.06273997\n",
      "Iteration 268, loss = 0.02206938\n",
      "Iteration 134, loss = 0.06192023\n",
      "Iteration 269, loss = 0.02195229\n",
      "Iteration 135, loss = 0.06301612\n",
      "Iteration 270, loss = 0.02339553\n",
      "Iteration 136, loss = 0.06267159\n",
      "Iteration 271, loss = 0.02624699\n",
      "Iteration 137, loss = 0.06105108\n",
      "Iteration 272, loss = 0.02692207\n",
      "Iteration 138, loss = 0.05888843\n",
      "Iteration 273, loss = 0.02686354\n",
      "Iteration 139, loss = 0.05978757\n",
      "Iteration 274, loss = 0.02232434\n",
      "Iteration 140, loss = 0.05831535\n",
      "Iteration 275, loss = 0.02080205\n",
      "Iteration 141, loss = 0.05766489\n",
      "Iteration 142, loss = 0.05789493Iteration 276, loss = 0.02013037\n",
      "\n",
      "Iteration 143, loss = 0.05775026\n",
      "Iteration 277, loss = 0.01988600\n",
      "Iteration 144, loss = 0.05776708\n",
      "Iteration 278, loss = 0.01964228\n",
      "Iteration 145, loss = 0.05776112\n",
      "Iteration 279, loss = 0.02015323\n",
      "Iteration 280, loss = 0.02048049\n",
      "Iteration 146, loss = 0.05482781\n",
      "Iteration 281, loss = 0.02155470\n",
      "Iteration 147, loss = 0.05631645\n",
      "Iteration 282, loss = 0.02107945\n",
      "Iteration 148, loss = 0.05381745\n",
      "Iteration 283, loss = 0.02179291\n",
      "Iteration 149, loss = 0.05369523\n",
      "Iteration 284, loss = 0.02256218\n",
      "Iteration 150, loss = 0.05273331\n",
      "Iteration 285, loss = 0.02203579\n",
      "Iteration 151, loss = 0.05254096\n",
      "Iteration 152, loss = 0.05255157\n",
      "Iteration 286, loss = 0.02476091\n",
      "Iteration 287, loss = 0.02090562\n",
      "Iteration 153, loss = 0.05388046\n",
      "Iteration 288, loss = 0.01841864\n",
      "Iteration 154, loss = 0.05055321\n",
      "Iteration 289, loss = 0.01887391\n",
      "Iteration 155, loss = 0.05289435\n",
      "Iteration 290, loss = 0.01908566\n",
      "Iteration 156, loss = 0.05232416\n",
      "Iteration 157, loss = 0.05270510\n",
      "Iteration 291, loss = 0.01844122\n",
      "Iteration 292, loss = 0.01846049\n",
      "Iteration 158, loss = 0.05142751\n",
      "Iteration 293, loss = 0.01803963\n",
      "Iteration 159, loss = 0.04920703\n",
      "Iteration 294, loss = 0.01873008\n",
      "Iteration 160, loss = 0.05442788\n",
      "Iteration 295, loss = 0.01784237\n",
      "Iteration 161, loss = 0.05264148\n",
      "Iteration 296, loss = 0.01715459\n",
      "Iteration 162, loss = 0.05522302\n",
      "Iteration 297, loss = 0.01786048\n",
      "Iteration 163, loss = 0.05147101\n",
      "Iteration 298, loss = 0.01882804\n",
      "Iteration 164, loss = 0.05130688\n",
      "Iteration 299, loss = 0.01884443\n",
      "Iteration 165, loss = 0.04882136\n",
      "Iteration 300, loss = 0.01743497\n",
      "Iteration 166, loss = 0.04800853\n",
      "Iteration 301, loss = 0.02048488\n",
      "Iteration 167, loss = 0.04796423\n",
      "Iteration 302, loss = 0.01948047\n",
      "Iteration 168, loss = 0.04630792\n",
      "Iteration 303, loss = 0.02014232\n",
      "Iteration 169, loss = 0.04507598\n",
      "Iteration 304, loss = 0.01714172\n",
      "Iteration 170, loss = 0.04600675\n",
      "Iteration 305, loss = 0.01592672\n",
      "Iteration 171, loss = 0.04384523\n",
      "Iteration 306, loss = 0.01589307\n",
      "Iteration 172, loss = 0.04394875\n",
      "Iteration 307, loss = 0.01633128\n",
      "Iteration 173, loss = 0.04450321\n",
      "Iteration 174, loss = 0.04372241\n",
      "Iteration 308, loss = 0.01639900\n",
      "Iteration 175, loss = 0.04376191\n",
      "Iteration 309, loss = 0.01586237\n",
      "Iteration 176, loss = 0.04405294\n",
      "Iteration 310, loss = 0.01659447\n",
      "Iteration 177, loss = 0.04409127\n",
      "Iteration 311, loss = 0.01534207\n",
      "Iteration 178, loss = 0.04463507\n",
      "Iteration 312, loss = 0.01506348\n",
      "Iteration 179, loss = 0.04213293\n",
      "Iteration 313, loss = 0.01604644\n",
      "Iteration 180, loss = 0.04007034\n",
      "Iteration 314, loss = 0.01545590\n",
      "Iteration 181, loss = 0.04284920\n",
      "Iteration 315, loss = 0.01823929\n",
      "Iteration 182, loss = 0.04463241\n",
      "Iteration 316, loss = 0.01721371\n",
      "Iteration 183, loss = 0.04063287\n",
      "Iteration 317, loss = 0.01844392\n",
      "Iteration 184, loss = 0.03952123\n",
      "Iteration 318, loss = 0.01591681\n",
      "Iteration 319, loss = 0.01504269\n",
      "Iteration 185, loss = 0.03937768\n",
      "Iteration 320, loss = 0.01471669\n",
      "Iteration 186, loss = 0.03892933\n",
      "Iteration 321, loss = 0.01568623\n",
      "Iteration 187, loss = 0.03947056\n",
      "Iteration 322, loss = 0.01472606\n",
      "Iteration 188, loss = 0.03842380\n",
      "Iteration 323, loss = 0.01470111\n",
      "Iteration 189, loss = 0.03883856\n",
      "Iteration 324, loss = 0.01725538\n",
      "Iteration 190, loss = 0.03877766\n",
      "Iteration 325, loss = 0.01830035\n",
      "Iteration 191, loss = 0.03823620\n",
      "Iteration 326, loss = 0.01759065\n",
      "Iteration 192, loss = 0.03784945\n",
      "Iteration 327, loss = 0.01526657\n",
      "Iteration 193, loss = 0.03708429\n",
      "Iteration 328, loss = 0.01389091\n",
      "Iteration 194, loss = 0.03637426\n",
      "Iteration 329, loss = 0.01422919\n",
      "Iteration 195, loss = 0.03694880\n",
      "Iteration 330, loss = 0.01520495\n",
      "Iteration 196, loss = 0.03646107\n",
      "Iteration 331, loss = 0.01443381\n",
      "Iteration 197, loss = 0.03653147\n",
      "Iteration 332, loss = 0.01729738\n",
      "Iteration 198, loss = 0.03739812\n",
      "Iteration 333, loss = 0.01805678\n",
      "Iteration 199, loss = 0.03479756\n",
      "Iteration 334, loss = 0.01845443\n",
      "Iteration 200, loss = 0.03548967\n",
      "Iteration 335, loss = 0.01914010\n",
      "Iteration 201, loss = 0.03598379\n",
      "Iteration 336, loss = 0.01889953\n",
      "Iteration 202, loss = 0.03464174\n",
      "Iteration 337, loss = 0.01921542\n",
      "Iteration 203, loss = 0.03420140\n",
      "Iteration 338, loss = 0.01459056\n",
      "Iteration 204, loss = 0.03403670\n",
      "Iteration 205, loss = 0.03517818\n",
      "Iteration 339, loss = 0.01439918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 206, loss = 0.03295741\n",
      "Iteration 207, loss = 0.03384710\n",
      "Iteration 208, loss = 0.03632816\n",
      "Iteration 209, loss = 0.03679097\n",
      "Iteration 210, loss = 0.03500851\n",
      "Iteration 211, loss = 0.03196130\n",
      "Iteration 212, loss = 0.03242339\n",
      "Iteration 213, loss = 0.03209898\n",
      "Iteration 214, loss = 0.03130497\n",
      "Iteration 215, loss = 0.03154192\n",
      "Iteration 216, loss = 0.03426338\n",
      "Iteration 217, loss = 0.03333428\n",
      "Iteration 218, loss = 0.03286142\n",
      "Iteration 1, loss = 1.27952147\n",
      "Iteration 219, loss = 0.03605551\n",
      "Iteration 2, loss = 0.99818146\n",
      "Iteration 220, loss = 0.03794210\n",
      "Iteration 3, loss = 0.81913926\n",
      "Iteration 4, loss = 0.69785561\n",
      "Iteration 221, loss = 0.03596383\n",
      "Iteration 5, loss = 0.61202272\n",
      "Iteration 222, loss = 0.03198889\n",
      "Iteration 6, loss = 0.55535541\n",
      "Iteration 223, loss = 0.03064109\n",
      "Iteration 7, loss = 0.51140776\n",
      "Iteration 224, loss = 0.02965163\n",
      "Iteration 8, loss = 0.47701459\n",
      "Iteration 225, loss = 0.03870131\n",
      "Iteration 9, loss = 0.44943144\n",
      "Iteration 226, loss = 0.03340170\n",
      "Iteration 10, loss = 0.42493287\n",
      "Iteration 227, loss = 0.03259476\n",
      "Iteration 11, loss = 0.40728982\n",
      "Iteration 228, loss = 0.02990772\n",
      "Iteration 12, loss = 0.39132977\n",
      "Iteration 229, loss = 0.03187414\n",
      "Iteration 13, loss = 0.37678899\n",
      "Iteration 230, loss = 0.02881073\n",
      "Iteration 14, loss = 0.36472002\n",
      "Iteration 231, loss = 0.02876006\n",
      "Iteration 15, loss = 0.35399275\n",
      "Iteration 232, loss = 0.02906565\n",
      "Iteration 16, loss = 0.34318993\n",
      "Iteration 233, loss = 0.02824620\n",
      "Iteration 17, loss = 0.33410556\n",
      "Iteration 234, loss = 0.02726462\n",
      "Iteration 18, loss = 0.32663036\n",
      "Iteration 235, loss = 0.02747253\n",
      "Iteration 19, loss = 0.31889330\n",
      "Iteration 236, loss = 0.02796709\n",
      "Iteration 20, loss = 0.31045503\n",
      "Iteration 237, loss = 0.02726429\n",
      "Iteration 21, loss = 0.30213739\n",
      "Iteration 238, loss = 0.02694921\n",
      "Iteration 22, loss = 0.29810738\n",
      "Iteration 239, loss = 0.02710745\n",
      "Iteration 23, loss = 0.29089688\n",
      "Iteration 240, loss = 0.02655129\n",
      "Iteration 24, loss = 0.28470313\n",
      "Iteration 241, loss = 0.02668500\n",
      "Iteration 25, loss = 0.27824780\n",
      "Iteration 242, loss = 0.02659624\n",
      "Iteration 26, loss = 0.27800992\n",
      "Iteration 243, loss = 0.02637710\n",
      "Iteration 27, loss = 0.26788082\n",
      "Iteration 244, loss = 0.02621228\n",
      "Iteration 28, loss = 0.26497172\n",
      "Iteration 245, loss = 0.02609361\n",
      "Iteration 29, loss = 0.25867482\n",
      "Iteration 246, loss = 0.02592994\n",
      "Iteration 30, loss = 0.25453302\n",
      "Iteration 247, loss = 0.02608142\n",
      "Iteration 31, loss = 0.24853583\n",
      "Iteration 248, loss = 0.02624547\n",
      "Iteration 32, loss = 0.24382617\n",
      "Iteration 249, loss = 0.02529278\n",
      "Iteration 33, loss = 0.23994291\n",
      "Iteration 250, loss = 0.02529067\n",
      "Iteration 34, loss = 0.23706601\n",
      "Iteration 251, loss = 0.02552420\n",
      "Iteration 35, loss = 0.23245814\n",
      "Iteration 252, loss = 0.02483693\n",
      "Iteration 36, loss = 0.22829590\n",
      "Iteration 253, loss = 0.02564634\n",
      "Iteration 37, loss = 0.22533464\n",
      "Iteration 254, loss = 0.02581011\n",
      "Iteration 38, loss = 0.22161314\n",
      "Iteration 255, loss = 0.02559735\n",
      "Iteration 39, loss = 0.21711813\n",
      "Iteration 256, loss = 0.02550702\n",
      "Iteration 40, loss = 0.21436914\n",
      "Iteration 257, loss = 0.02551908\n",
      "Iteration 41, loss = 0.21073032\n",
      "Iteration 258, loss = 0.02527352\n",
      "Iteration 42, loss = 0.20731849\n",
      "Iteration 259, loss = 0.02753673\n",
      "Iteration 43, loss = 0.20486617\n",
      "Iteration 260, loss = 0.02579902\n",
      "Iteration 44, loss = 0.20150456\n",
      "Iteration 261, loss = 0.02593467\n",
      "Iteration 45, loss = 0.19877758\n",
      "Iteration 262, loss = 0.02896161\n",
      "Iteration 46, loss = 0.19574615\n",
      "Iteration 263, loss = 0.02721140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.19356364\n",
      "Iteration 48, loss = 0.19041013\n",
      "Iteration 49, loss = 0.18886302\n",
      "Iteration 50, loss = 0.18534300\n",
      "Iteration 51, loss = 0.18313449\n",
      "Iteration 52, loss = 0.18163923\n",
      "Iteration 53, loss = 0.17990212\n",
      "Iteration 54, loss = 0.17452296\n",
      "Iteration 55, loss = 0.17332263\n",
      "Iteration 56, loss = 0.17091277\n",
      "Iteration 57, loss = 0.17093005\n",
      "Iteration 58, loss = 0.16720378\n",
      "Iteration 1, loss = 1.16303172\n",
      "Iteration 59, loss = 0.16656811\n",
      "Iteration 2, loss = 0.86829855\n",
      "Iteration 60, loss = 0.16519653\n",
      "Iteration 3, loss = 0.70664894\n",
      "Iteration 61, loss = 0.16163776\n",
      "Iteration 4, loss = 0.61413118\n",
      "Iteration 62, loss = 0.15928247\n",
      "Iteration 5, loss = 0.54772535\n",
      "Iteration 63, loss = 0.15667125\n",
      "Iteration 6, loss = 0.50795552\n",
      "Iteration 64, loss = 0.15647689\n",
      "Iteration 7, loss = 0.46985870\n",
      "Iteration 65, loss = 0.15540211\n",
      "Iteration 8, loss = 0.44225315\n",
      "Iteration 66, loss = 0.15235751\n",
      "Iteration 9, loss = 0.42091199\n",
      "Iteration 67, loss = 0.14973620\n",
      "Iteration 10, loss = 0.40186028\n",
      "Iteration 68, loss = 0.14905534\n",
      "Iteration 11, loss = 0.39027634\n",
      "Iteration 69, loss = 0.14609472\n",
      "Iteration 12, loss = 0.37562650\n",
      "Iteration 70, loss = 0.14988415\n",
      "Iteration 13, loss = 0.36362701\n",
      "Iteration 71, loss = 0.14385712\n",
      "Iteration 14, loss = 0.35419237\n",
      "Iteration 72, loss = 0.14211478\n",
      "Iteration 15, loss = 0.34334760\n",
      "Iteration 73, loss = 0.14133426\n",
      "Iteration 16, loss = 0.33456649\n",
      "Iteration 74, loss = 0.13914540\n",
      "Iteration 17, loss = 0.32708752\n",
      "Iteration 75, loss = 0.13874918\n",
      "Iteration 18, loss = 0.31875571\n",
      "Iteration 76, loss = 0.13850239\n",
      "Iteration 19, loss = 0.31208650\n",
      "Iteration 77, loss = 0.13456679\n",
      "Iteration 20, loss = 0.30535665\n",
      "Iteration 78, loss = 0.13450403\n",
      "Iteration 21, loss = 0.29944362\n",
      "Iteration 79, loss = 0.13468149\n",
      "Iteration 22, loss = 0.29377921\n",
      "Iteration 80, loss = 0.13048431\n",
      "Iteration 23, loss = 0.28705241\n",
      "Iteration 81, loss = 0.12998101\n",
      "Iteration 24, loss = 0.28152103\n",
      "Iteration 82, loss = 0.12958434\n",
      "Iteration 25, loss = 0.27634544\n",
      "Iteration 83, loss = 0.12675890\n",
      "Iteration 26, loss = 0.27079581\n",
      "Iteration 84, loss = 0.12614941\n",
      "Iteration 27, loss = 0.26591706\n",
      "Iteration 85, loss = 0.12638770\n",
      "Iteration 28, loss = 0.26182404\n",
      "Iteration 86, loss = 0.12334805\n",
      "Iteration 29, loss = 0.25837503\n",
      "Iteration 87, loss = 0.12151459\n",
      "Iteration 30, loss = 0.25291396\n",
      "Iteration 88, loss = 0.12056702\n",
      "Iteration 31, loss = 0.24772004\n",
      "Iteration 89, loss = 0.12704531\n",
      "Iteration 32, loss = 0.24387809\n",
      "Iteration 90, loss = 0.12053644\n",
      "Iteration 33, loss = 0.23900768\n",
      "Iteration 91, loss = 0.12450487\n",
      "Iteration 34, loss = 0.23489247\n",
      "Iteration 92, loss = 0.11817625\n",
      "Iteration 35, loss = 0.23181807\n",
      "Iteration 93, loss = 0.11523015\n",
      "Iteration 36, loss = 0.22805300\n",
      "Iteration 94, loss = 0.11458666\n",
      "Iteration 37, loss = 0.22300340\n",
      "Iteration 95, loss = 0.11269884\n",
      "Iteration 38, loss = 0.22000119\n",
      "Iteration 96, loss = 0.11101668\n",
      "Iteration 39, loss = 0.21568427\n",
      "Iteration 97, loss = 0.10976381\n",
      "Iteration 40, loss = 0.21508523\n",
      "Iteration 98, loss = 0.10855020\n",
      "Iteration 41, loss = 0.20864948\n",
      "Iteration 99, loss = 0.10877543\n",
      "Iteration 42, loss = 0.20696198\n",
      "Iteration 100, loss = 0.10714383\n",
      "Iteration 43, loss = 0.20191684\n",
      "Iteration 101, loss = 0.11199463\n",
      "Iteration 44, loss = 0.20061041\n",
      "Iteration 102, loss = 0.10742301\n",
      "Iteration 45, loss = 0.19608506\n",
      "Iteration 103, loss = 0.10716810\n",
      "Iteration 46, loss = 0.19359471\n",
      "Iteration 104, loss = 0.10472387\n",
      "Iteration 47, loss = 0.19077623\n",
      "Iteration 105, loss = 0.10183927\n",
      "Iteration 48, loss = 0.18791149\n",
      "Iteration 106, loss = 0.10106344\n",
      "Iteration 49, loss = 0.18575932\n",
      "Iteration 107, loss = 0.10088854\n",
      "Iteration 50, loss = 0.18163812\n",
      "Iteration 108, loss = 0.10247759\n",
      "Iteration 51, loss = 0.17986190\n",
      "Iteration 109, loss = 0.10356025\n",
      "Iteration 52, loss = 0.17647951\n",
      "Iteration 110, loss = 0.10014394\n",
      "Iteration 53, loss = 0.17744040\n",
      "Iteration 111, loss = 0.09704493\n",
      "Iteration 54, loss = 0.17060072\n",
      "Iteration 112, loss = 0.09569584\n",
      "Iteration 55, loss = 0.16990317\n",
      "Iteration 113, loss = 0.09480269\n",
      "Iteration 56, loss = 0.16762549\n",
      "Iteration 114, loss = 0.09364622\n",
      "Iteration 57, loss = 0.16283587\n",
      "Iteration 115, loss = 0.09267433\n",
      "Iteration 58, loss = 0.16223122\n",
      "Iteration 116, loss = 0.09267946\n",
      "Iteration 59, loss = 0.15810289\n",
      "Iteration 117, loss = 0.09096528\n",
      "Iteration 60, loss = 0.15719307\n",
      "Iteration 118, loss = 0.08973171\n",
      "Iteration 61, loss = 0.15472320\n",
      "Iteration 119, loss = 0.09035209\n",
      "Iteration 62, loss = 0.15223307\n",
      "Iteration 63, loss = 0.15068044\n",
      "Iteration 120, loss = 0.08858344\n",
      "Iteration 64, loss = 0.14847688\n",
      "Iteration 121, loss = 0.08881224\n",
      "Iteration 65, loss = 0.14724900\n",
      "Iteration 122, loss = 0.08772355\n",
      "Iteration 66, loss = 0.14512597\n",
      "Iteration 123, loss = 0.08766341\n",
      "Iteration 67, loss = 0.14267823\n",
      "Iteration 124, loss = 0.08462324\n",
      "Iteration 68, loss = 0.14253147\n",
      "Iteration 125, loss = 0.08431430\n",
      "Iteration 69, loss = 0.13931361\n",
      "Iteration 126, loss = 0.08279033\n",
      "Iteration 70, loss = 0.13703604\n",
      "Iteration 127, loss = 0.08275510\n",
      "Iteration 71, loss = 0.13627509\n",
      "Iteration 128, loss = 0.08365336\n",
      "Iteration 72, loss = 0.13588235\n",
      "Iteration 129, loss = 0.08827792\n",
      "Iteration 73, loss = 0.13250174\n",
      "Iteration 130, loss = 0.08288224\n",
      "Iteration 74, loss = 0.13153922\n",
      "Iteration 131, loss = 0.08068187\n",
      "Iteration 75, loss = 0.13120104\n",
      "Iteration 132, loss = 0.07866592\n",
      "Iteration 76, loss = 0.12798027\n",
      "Iteration 133, loss = 0.07864938\n",
      "Iteration 77, loss = 0.12596431\n",
      "Iteration 134, loss = 0.08165921\n",
      "Iteration 78, loss = 0.12605673\n",
      "Iteration 135, loss = 0.08073248\n",
      "Iteration 79, loss = 0.12252480\n",
      "Iteration 136, loss = 0.08158696\n",
      "Iteration 80, loss = 0.12085575\n",
      "Iteration 137, loss = 0.07712482\n",
      "Iteration 81, loss = 0.12049504\n",
      "Iteration 138, loss = 0.07784730\n",
      "Iteration 82, loss = 0.12010132\n",
      "Iteration 139, loss = 0.07625589\n",
      "Iteration 83, loss = 0.11766812\n",
      "Iteration 140, loss = 0.07631871\n",
      "Iteration 84, loss = 0.11387323\n",
      "Iteration 141, loss = 0.07692122\n",
      "Iteration 85, loss = 0.11633637\n",
      "Iteration 142, loss = 0.08243867\n",
      "Iteration 86, loss = 0.11349596\n",
      "Iteration 143, loss = 0.07583701\n",
      "Iteration 87, loss = 0.11187419\n",
      "Iteration 144, loss = 0.07006813\n",
      "Iteration 88, loss = 0.11008791\n",
      "Iteration 145, loss = 0.07785172\n",
      "Iteration 89, loss = 0.10858810\n",
      "Iteration 146, loss = 0.07103415\n",
      "Iteration 90, loss = 0.10637914\n",
      "Iteration 147, loss = 0.06927197\n",
      "Iteration 91, loss = 0.10560709\n",
      "Iteration 148, loss = 0.07056012\n",
      "Iteration 92, loss = 0.10435851\n",
      "Iteration 149, loss = 0.06896423\n",
      "Iteration 93, loss = 0.10186898\n",
      "Iteration 150, loss = 0.06815988\n",
      "Iteration 94, loss = 0.10207121\n",
      "Iteration 151, loss = 0.06824095\n",
      "Iteration 95, loss = 0.10175907\n",
      "Iteration 152, loss = 0.06640864\n",
      "Iteration 96, loss = 0.09843231\n",
      "Iteration 153, loss = 0.06723432\n",
      "Iteration 97, loss = 0.09689935\n",
      "Iteration 154, loss = 0.06569690\n",
      "Iteration 98, loss = 0.09622272\n",
      "Iteration 155, loss = 0.06442845\n",
      "Iteration 99, loss = 0.09526848\n",
      "Iteration 156, loss = 0.06394177\n",
      "Iteration 100, loss = 0.09426394\n",
      "Iteration 157, loss = 0.06433444\n",
      "Iteration 101, loss = 0.09276781\n",
      "Iteration 158, loss = 0.06200420\n",
      "Iteration 102, loss = 0.09138790\n",
      "Iteration 159, loss = 0.06267224\n",
      "Iteration 103, loss = 0.08996527\n",
      "Iteration 160, loss = 0.06035439\n",
      "Iteration 104, loss = 0.08870134\n",
      "Iteration 161, loss = 0.06067611\n",
      "Iteration 105, loss = 0.08978690\n",
      "Iteration 162, loss = 0.06046125\n",
      "Iteration 106, loss = 0.08674570\n",
      "Iteration 163, loss = 0.05985406\n",
      "Iteration 107, loss = 0.08571082\n",
      "Iteration 164, loss = 0.05894059\n",
      "Iteration 108, loss = 0.08720743\n",
      "Iteration 165, loss = 0.06056817\n",
      "Iteration 109, loss = 0.08354923\n",
      "Iteration 166, loss = 0.06188545\n",
      "Iteration 110, loss = 0.08302701\n",
      "Iteration 167, loss = 0.05792531\n",
      "Iteration 111, loss = 0.08254011\n",
      "Iteration 168, loss = 0.06342283\n",
      "Iteration 112, loss = 0.08114755\n",
      "Iteration 169, loss = 0.05902320\n",
      "Iteration 113, loss = 0.08079371\n",
      "Iteration 170, loss = 0.05676835\n",
      "Iteration 114, loss = 0.08539062\n",
      "Iteration 115, loss = 0.08262528\n",
      "Iteration 171, loss = 0.05867016\n",
      "Iteration 116, loss = 0.08201884\n",
      "Iteration 172, loss = 0.05694000\n",
      "Iteration 117, loss = 0.07343779\n",
      "Iteration 173, loss = 0.05657493\n",
      "Iteration 118, loss = 0.07934951\n",
      "Iteration 174, loss = 0.05519835\n",
      "Iteration 119, loss = 0.08152321\n",
      "Iteration 175, loss = 0.05669556\n",
      "Iteration 120, loss = 0.08009692\n",
      "Iteration 176, loss = 0.05891123\n",
      "Iteration 121, loss = 0.07330597\n",
      "Iteration 177, loss = 0.05463256\n",
      "Iteration 122, loss = 0.07258713\n",
      "Iteration 178, loss = 0.05492260\n",
      "Iteration 123, loss = 0.07326377\n",
      "Iteration 179, loss = 0.05636063\n",
      "Iteration 124, loss = 0.07006935\n",
      "Iteration 180, loss = 0.05628362\n",
      "Iteration 125, loss = 0.06758970\n",
      "Iteration 181, loss = 0.05444524\n",
      "Iteration 182, loss = 0.05333914\n",
      "Iteration 126, loss = 0.07104281\n",
      "Iteration 183, loss = 0.05137187\n",
      "Iteration 127, loss = 0.06610627\n",
      "Iteration 184, loss = 0.05014385\n",
      "Iteration 128, loss = 0.06619175\n",
      "Iteration 129, loss = 0.06682638\n",
      "Iteration 185, loss = 0.05122401\n",
      "Iteration 130, loss = 0.06911046\n",
      "Iteration 186, loss = 0.05003263\n",
      "Iteration 131, loss = 0.06721210\n",
      "Iteration 187, loss = 0.04914861\n",
      "Iteration 132, loss = 0.06306577\n",
      "Iteration 188, loss = 0.05093731\n",
      "Iteration 133, loss = 0.06257023\n",
      "Iteration 189, loss = 0.05227233\n",
      "Iteration 134, loss = 0.06338281\n",
      "Iteration 190, loss = 0.05283358\n",
      "Iteration 135, loss = 0.06177858\n",
      "Iteration 191, loss = 0.05266867\n",
      "Iteration 136, loss = 0.06183988\n",
      "Iteration 192, loss = 0.04674509\n",
      "Iteration 137, loss = 0.06023883\n",
      "Iteration 193, loss = 0.04795264\n",
      "Iteration 138, loss = 0.05948478\n",
      "Iteration 194, loss = 0.04629892\n",
      "Iteration 139, loss = 0.05959743\n",
      "Iteration 195, loss = 0.04707758\n",
      "Iteration 140, loss = 0.05921672\n",
      "Iteration 196, loss = 0.04644311\n",
      "Iteration 141, loss = 0.05785209\n",
      "Iteration 197, loss = 0.04727096\n",
      "Iteration 142, loss = 0.05777756\n",
      "Iteration 198, loss = 0.04551162\n",
      "Iteration 143, loss = 0.05571355\n",
      "Iteration 199, loss = 0.04567900\n",
      "Iteration 144, loss = 0.05649675\n",
      "Iteration 200, loss = 0.04640075\n",
      "Iteration 145, loss = 0.05602157\n",
      "Iteration 201, loss = 0.04510070\n",
      "Iteration 146, loss = 0.05553761\n",
      "Iteration 202, loss = 0.04385794\n",
      "Iteration 147, loss = 0.05515165\n",
      "Iteration 203, loss = 0.04430673\n",
      "Iteration 148, loss = 0.05366189\n",
      "Iteration 204, loss = 0.04299782\n",
      "Iteration 149, loss = 0.05389850\n",
      "Iteration 205, loss = 0.04572793\n",
      "Iteration 150, loss = 0.05296196\n",
      "Iteration 206, loss = 0.05145384\n",
      "Iteration 151, loss = 0.05172551\n",
      "Iteration 207, loss = 0.05574010\n",
      "Iteration 152, loss = 0.05176685\n",
      "Iteration 208, loss = 0.05783756\n",
      "Iteration 153, loss = 0.05025594\n",
      "Iteration 209, loss = 0.04456983\n",
      "Iteration 154, loss = 0.05189732\n",
      "Iteration 210, loss = 0.04344581\n",
      "Iteration 155, loss = 0.05088645\n",
      "Iteration 211, loss = 0.04229967\n",
      "Iteration 156, loss = 0.05085196\n",
      "Iteration 212, loss = 0.04071052\n",
      "Iteration 157, loss = 0.05097304\n",
      "Iteration 213, loss = 0.04284483\n",
      "Iteration 158, loss = 0.04835777\n",
      "Iteration 214, loss = 0.04123104\n",
      "Iteration 159, loss = 0.04913645\n",
      "Iteration 215, loss = 0.04047331\n",
      "Iteration 160, loss = 0.04791021\n",
      "Iteration 216, loss = 0.04067595\n",
      "Iteration 161, loss = 0.04930354\n",
      "Iteration 217, loss = 0.04079069\n",
      "Iteration 162, loss = 0.04683014\n",
      "Iteration 218, loss = 0.03995976\n",
      "Iteration 163, loss = 0.04683481\n",
      "Iteration 219, loss = 0.04012357\n",
      "Iteration 164, loss = 0.04692233\n",
      "Iteration 220, loss = 0.04033384\n",
      "Iteration 165, loss = 0.05168749\n",
      "Iteration 221, loss = 0.04010794\n",
      "Iteration 166, loss = 0.05042960\n",
      "Iteration 222, loss = 0.04643150\n",
      "Iteration 167, loss = 0.04740445\n",
      "Iteration 223, loss = 0.04775027\n",
      "Iteration 168, loss = 0.04923510\n",
      "Iteration 224, loss = 0.05202342\n",
      "Iteration 169, loss = 0.04445684\n",
      "Iteration 225, loss = 0.05160717\n",
      "Iteration 170, loss = 0.04728774\n",
      "Iteration 226, loss = 0.03608628\n",
      "Iteration 171, loss = 0.04283627\n",
      "Iteration 227, loss = 0.04215651\n",
      "Iteration 172, loss = 0.04481909\n",
      "Iteration 228, loss = 0.04122573\n",
      "Iteration 173, loss = 0.04582387\n",
      "Iteration 229, loss = 0.03932679\n",
      "Iteration 174, loss = 0.04389767\n",
      "Iteration 175, loss = 0.04441507\n",
      "Iteration 230, loss = 0.04141980\n",
      "Iteration 176, loss = 0.04214841\n",
      "Iteration 231, loss = 0.03857715\n",
      "Iteration 177, loss = 0.04385985\n",
      "Iteration 232, loss = 0.03787741\n",
      "Iteration 178, loss = 0.04317035\n",
      "Iteration 233, loss = 0.03901345\n",
      "Iteration 179, loss = 0.04391597\n",
      "Iteration 234, loss = 0.03484657\n",
      "Iteration 180, loss = 0.04141486\n",
      "Iteration 235, loss = 0.03584529\n",
      "Iteration 181, loss = 0.04019203\n",
      "Iteration 236, loss = 0.03616067\n",
      "Iteration 182, loss = 0.04092747\n",
      "Iteration 237, loss = 0.03469878\n",
      "Iteration 183, loss = 0.04225218\n",
      "Iteration 238, loss = 0.03570712\n",
      "Iteration 184, loss = 0.04053822\n",
      "Iteration 239, loss = 0.03497225\n",
      "Iteration 185, loss = 0.04011718\n",
      "Iteration 240, loss = 0.03412691\n",
      "Iteration 186, loss = 0.03916597\n",
      "Iteration 241, loss = 0.03529204\n",
      "Iteration 187, loss = 0.04042580\n",
      "Iteration 242, loss = 0.03562965\n",
      "Iteration 188, loss = 0.03890379\n",
      "Iteration 243, loss = 0.03400977\n",
      "Iteration 189, loss = 0.03919359\n",
      "Iteration 244, loss = 0.03366043\n",
      "Iteration 190, loss = 0.04209087\n",
      "Iteration 245, loss = 0.03355688\n",
      "Iteration 191, loss = 0.04689313\n",
      "Iteration 246, loss = 0.03375209\n",
      "Iteration 192, loss = 0.04259292\n",
      "Iteration 247, loss = 0.03450036\n",
      "Iteration 193, loss = 0.04549492\n",
      "Iteration 248, loss = 0.03287814\n",
      "Iteration 194, loss = 0.04227214\n",
      "Iteration 249, loss = 0.03643156\n",
      "Iteration 195, loss = 0.04356858\n",
      "Iteration 250, loss = 0.03432004\n",
      "Iteration 196, loss = 0.03905764\n",
      "Iteration 251, loss = 0.03291735\n",
      "Iteration 197, loss = 0.03831621\n",
      "Iteration 252, loss = 0.03136408\n",
      "Iteration 198, loss = 0.03676103\n",
      "Iteration 253, loss = 0.03213709\n",
      "Iteration 199, loss = 0.03568917\n",
      "Iteration 254, loss = 0.03123955\n",
      "Iteration 200, loss = 0.03510477\n",
      "Iteration 255, loss = 0.03144560\n",
      "Iteration 201, loss = 0.03741321\n",
      "Iteration 256, loss = 0.03207689\n",
      "Iteration 202, loss = 0.03805215\n",
      "Iteration 257, loss = 0.03085775\n",
      "Iteration 203, loss = 0.03845356\n",
      "Iteration 258, loss = 0.03285932\n",
      "Iteration 204, loss = 0.03644657\n",
      "Iteration 259, loss = 0.03117366\n",
      "Iteration 205, loss = 0.03472449\n",
      "Iteration 260, loss = 0.03109996\n",
      "Iteration 206, loss = 0.03417791\n",
      "Iteration 261, loss = 0.02919466\n",
      "Iteration 207, loss = 0.03546716\n",
      "Iteration 262, loss = 0.03129269\n",
      "Iteration 208, loss = 0.03465751\n",
      "Iteration 263, loss = 0.03079062\n",
      "Iteration 209, loss = 0.03458379\n",
      "Iteration 264, loss = 0.03211905\n",
      "Iteration 210, loss = 0.03528366\n",
      "Iteration 265, loss = 0.03190121\n",
      "Iteration 211, loss = 0.03322329\n",
      "Iteration 266, loss = 0.03028808\n",
      "Iteration 212, loss = 0.03479491\n",
      "Iteration 267, loss = 0.03080027\n",
      "Iteration 213, loss = 0.04146005\n",
      "Iteration 268, loss = 0.03068451\n",
      "Iteration 214, loss = 0.03716922\n",
      "Iteration 269, loss = 0.03013209\n",
      "Iteration 215, loss = 0.03678212\n",
      "Iteration 270, loss = 0.02903162\n",
      "Iteration 216, loss = 0.03328348\n",
      "Iteration 271, loss = 0.02826246\n",
      "Iteration 217, loss = 0.03175307\n",
      "Iteration 272, loss = 0.02827283\n",
      "Iteration 218, loss = 0.03162276\n",
      "Iteration 273, loss = 0.02899234\n",
      "Iteration 219, loss = 0.03177348\n",
      "Iteration 274, loss = 0.02894033\n",
      "Iteration 220, loss = 0.03231815\n",
      "Iteration 275, loss = 0.03384489\n",
      "Iteration 221, loss = 0.03085373\n",
      "Iteration 276, loss = 0.03124078\n",
      "Iteration 222, loss = 0.03047035\n",
      "Iteration 277, loss = 0.02922615\n",
      "Iteration 223, loss = 0.03069989\n",
      "Iteration 278, loss = 0.02811250\n",
      "Iteration 224, loss = 0.03059276\n",
      "Iteration 279, loss = 0.02794741\n",
      "Iteration 225, loss = 0.03105690\n",
      "Iteration 280, loss = 0.02856880\n",
      "Iteration 226, loss = 0.03105090\n",
      "Iteration 281, loss = 0.02731306\n",
      "Iteration 227, loss = 0.03124606\n",
      "Iteration 282, loss = 0.02724809\n",
      "Iteration 228, loss = 0.03046610\n",
      "Iteration 283, loss = 0.02676073\n",
      "Iteration 229, loss = 0.03107800\n",
      "Iteration 284, loss = 0.02632521\n",
      "Iteration 230, loss = 0.03134386\n",
      "Iteration 285, loss = 0.02620413\n",
      "Iteration 231, loss = 0.03026891\n",
      "Iteration 286, loss = 0.02637083\n",
      "Iteration 232, loss = 0.02919007\n",
      "Iteration 287, loss = 0.02670417\n",
      "Iteration 233, loss = 0.02871965\n",
      "Iteration 288, loss = 0.02836861\n",
      "Iteration 234, loss = 0.02897166\n",
      "Iteration 289, loss = 0.02626221\n",
      "Iteration 235, loss = 0.02827560\n",
      "Iteration 290, loss = 0.02576275\n",
      "Iteration 236, loss = 0.02783572\n",
      "Iteration 291, loss = 0.02548259\n",
      "Iteration 237, loss = 0.02980907\n",
      "Iteration 292, loss = 0.02531623\n",
      "Iteration 238, loss = 0.03005632\n",
      "Iteration 293, loss = 0.02518581\n",
      "Iteration 239, loss = 0.02986774\n",
      "Iteration 294, loss = 0.02490815\n",
      "Iteration 240, loss = 0.02956038\n",
      "Iteration 295, loss = 0.02484856\n",
      "Iteration 241, loss = 0.02870984\n",
      "Iteration 296, loss = 0.02505975\n",
      "Iteration 242, loss = 0.02818217\n",
      "Iteration 297, loss = 0.02475049\n",
      "Iteration 243, loss = 0.02762536\n",
      "Iteration 298, loss = 0.02598733\n",
      "Iteration 244, loss = 0.02949823\n",
      "Iteration 299, loss = 0.02708283\n",
      "Iteration 245, loss = 0.03195692\n",
      "Iteration 300, loss = 0.02975001\n",
      "Iteration 246, loss = 0.02933569\n",
      "Iteration 301, loss = 0.02728940\n",
      "Iteration 247, loss = 0.02787243\n",
      "Iteration 302, loss = 0.02915088\n",
      "Iteration 248, loss = 0.02720550\n",
      "Iteration 303, loss = 0.02735298\n",
      "Iteration 249, loss = 0.02694237\n",
      "Iteration 304, loss = 0.02817464\n",
      "Iteration 250, loss = 0.02708241\n",
      "Iteration 251, loss = 0.02626986\n",
      "Iteration 305, loss = 0.03072910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 252, loss = 0.02634143\n",
      "Iteration 253, loss = 0.02560775\n",
      "Iteration 254, loss = 0.02525061\n",
      "Iteration 255, loss = 0.02690174\n",
      "Iteration 256, loss = 0.02642479\n",
      "Iteration 257, loss = 0.02647211\n",
      "Iteration 258, loss = 0.02729568\n",
      "Iteration 259, loss = 0.02470749\n",
      "Iteration 260, loss = 0.02611497\n",
      "Iteration 261, loss = 0.02505118\n",
      "Iteration 262, loss = 0.02587934\n",
      "Iteration 263, loss = 0.02507887\n",
      "Iteration 1, loss = 1.35307837\n",
      "Iteration 264, loss = 0.02580477\n",
      "Iteration 2, loss = 0.99310638\n",
      "Iteration 265, loss = 0.02619338\n",
      "Iteration 3, loss = 0.77866937\n",
      "Iteration 266, loss = 0.02536070\n",
      "Iteration 4, loss = 0.66196961\n",
      "Iteration 267, loss = 0.02425403\n",
      "Iteration 5, loss = 0.58795212\n",
      "Iteration 268, loss = 0.02481670\n",
      "Iteration 6, loss = 0.54023510\n",
      "Iteration 269, loss = 0.02353820\n",
      "Iteration 7, loss = 0.50503238\n",
      "Iteration 270, loss = 0.02597174\n",
      "Iteration 8, loss = 0.47618847\n",
      "Iteration 271, loss = 0.02539630\n",
      "Iteration 9, loss = 0.45438782\n",
      "Iteration 272, loss = 0.02415511\n",
      "Iteration 10, loss = 0.43390886\n",
      "Iteration 273, loss = 0.02668596\n",
      "Iteration 11, loss = 0.41879515\n",
      "Iteration 274, loss = 0.02478614\n",
      "Iteration 12, loss = 0.40412265\n",
      "Iteration 275, loss = 0.02420575\n",
      "Iteration 13, loss = 0.39258663\n",
      "Iteration 276, loss = 0.02437931\n",
      "Iteration 14, loss = 0.38088041\n",
      "Iteration 277, loss = 0.02330373\n",
      "Iteration 15, loss = 0.36976903\n",
      "Iteration 278, loss = 0.02288912\n",
      "Iteration 16, loss = 0.35995302\n",
      "Iteration 279, loss = 0.02251775\n",
      "Iteration 17, loss = 0.35082602\n",
      "Iteration 280, loss = 0.02385372\n",
      "Iteration 18, loss = 0.34225200\n",
      "Iteration 281, loss = 0.02252337\n",
      "Iteration 19, loss = 0.33361711\n",
      "Iteration 282, loss = 0.02339503\n",
      "Iteration 20, loss = 0.32657995\n",
      "Iteration 283, loss = 0.02702267\n",
      "Iteration 21, loss = 0.31960158\n",
      "Iteration 284, loss = 0.02477387\n",
      "Iteration 22, loss = 0.31311402\n",
      "Iteration 285, loss = 0.02748179\n",
      "Iteration 23, loss = 0.30598618\n",
      "Iteration 286, loss = 0.03236927\n",
      "Iteration 24, loss = 0.29913211\n",
      "Iteration 287, loss = 0.03057307\n",
      "Iteration 25, loss = 0.29212842\n",
      "Iteration 288, loss = 0.03002894\n",
      "Iteration 26, loss = 0.28794967\n",
      "Iteration 289, loss = 0.02774765\n",
      "Iteration 27, loss = 0.28002321\n",
      "Iteration 290, loss = 0.02528775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.27616817\n",
      "Iteration 29, loss = 0.26992717\n",
      "Iteration 30, loss = 0.26489177\n",
      "Iteration 31, loss = 0.25985414\n",
      "Iteration 32, loss = 0.25485604\n",
      "Iteration 33, loss = 0.25075653\n",
      "Iteration 34, loss = 0.24615690\n",
      "Iteration 35, loss = 0.24182900\n",
      "Iteration 36, loss = 0.23737730\n",
      "Iteration 37, loss = 0.23311376\n",
      "Iteration 38, loss = 0.22875165\n",
      "Iteration 39, loss = 0.22448964\n",
      "Iteration 40, loss = 0.22071532\n",
      "Iteration 1, loss = 1.07289520\n",
      "Iteration 41, loss = 0.21799700\n",
      "Iteration 2, loss = 0.81789586\n",
      "Iteration 42, loss = 0.21380847\n",
      "Iteration 3, loss = 0.67692553\n",
      "Iteration 43, loss = 0.21202725\n",
      "Iteration 4, loss = 0.59201041\n",
      "Iteration 44, loss = 0.20709767\n",
      "Iteration 5, loss = 0.53974872\n",
      "Iteration 45, loss = 0.20473494\n",
      "Iteration 6, loss = 0.49718904\n",
      "Iteration 46, loss = 0.20070571\n",
      "Iteration 7, loss = 0.46408996\n",
      "Iteration 47, loss = 0.19740122\n",
      "Iteration 8, loss = 0.43970545\n",
      "Iteration 48, loss = 0.19569907\n",
      "Iteration 9, loss = 0.42022067\n",
      "Iteration 49, loss = 0.19169622\n",
      "Iteration 10, loss = 0.40340498\n",
      "Iteration 50, loss = 0.19025224\n",
      "Iteration 11, loss = 0.38784790\n",
      "Iteration 51, loss = 0.19082733\n",
      "Iteration 12, loss = 0.37396605\n",
      "Iteration 52, loss = 0.18603504\n",
      "Iteration 13, loss = 0.36217991\n",
      "Iteration 53, loss = 0.18305747\n",
      "Iteration 14, loss = 0.35097774\n",
      "Iteration 54, loss = 0.17802115\n",
      "Iteration 15, loss = 0.34091670\n",
      "Iteration 55, loss = 0.17797654\n",
      "Iteration 16, loss = 0.33167309\n",
      "Iteration 56, loss = 0.17185752\n",
      "Iteration 17, loss = 0.32252294\n",
      "Iteration 57, loss = 0.17231000\n",
      "Iteration 18, loss = 0.31470327\n",
      "Iteration 58, loss = 0.16750818\n",
      "Iteration 19, loss = 0.30542845\n",
      "Iteration 59, loss = 0.16525025\n",
      "Iteration 20, loss = 0.30286030\n",
      "Iteration 60, loss = 0.16452299\n",
      "Iteration 21, loss = 0.29224939\n",
      "Iteration 61, loss = 0.16332153\n",
      "Iteration 22, loss = 0.28647472\n",
      "Iteration 62, loss = 0.15829411\n",
      "Iteration 23, loss = 0.27878620\n",
      "Iteration 63, loss = 0.15920775\n",
      "Iteration 24, loss = 0.27317251\n",
      "Iteration 64, loss = 0.16005749\n",
      "Iteration 25, loss = 0.26572053\n",
      "Iteration 65, loss = 0.15988354\n",
      "Iteration 26, loss = 0.25989650\n",
      "Iteration 66, loss = 0.15185077\n",
      "Iteration 27, loss = 0.25310922\n",
      "Iteration 67, loss = 0.15129272\n",
      "Iteration 28, loss = 0.24864597\n",
      "Iteration 68, loss = 0.15384308\n",
      "Iteration 29, loss = 0.24335992\n",
      "Iteration 69, loss = 0.14509340\n",
      "Iteration 30, loss = 0.23797436\n",
      "Iteration 70, loss = 0.14522455\n",
      "Iteration 31, loss = 0.23114980\n",
      "Iteration 71, loss = 0.14242850\n",
      "Iteration 32, loss = 0.22567961\n",
      "Iteration 72, loss = 0.14096280\n",
      "Iteration 33, loss = 0.22104752\n",
      "Iteration 73, loss = 0.13798914\n",
      "Iteration 34, loss = 0.21612552\n",
      "Iteration 74, loss = 0.13674474\n",
      "Iteration 35, loss = 0.21211329\n",
      "Iteration 75, loss = 0.13558122\n",
      "Iteration 36, loss = 0.20820657\n",
      "Iteration 76, loss = 0.13463426\n",
      "Iteration 37, loss = 0.20327774\n",
      "Iteration 77, loss = 0.13240628\n",
      "Iteration 38, loss = 0.19934891\n",
      "Iteration 78, loss = 0.13157057\n",
      "Iteration 39, loss = 0.19753449\n",
      "Iteration 79, loss = 0.12938764\n",
      "Iteration 40, loss = 0.19171989\n",
      "Iteration 80, loss = 0.12811487\n",
      "Iteration 41, loss = 0.18796590\n",
      "Iteration 81, loss = 0.12638912\n",
      "Iteration 42, loss = 0.18438511\n",
      "Iteration 82, loss = 0.12628208\n",
      "Iteration 43, loss = 0.18061145\n",
      "Iteration 83, loss = 0.12334170\n",
      "Iteration 44, loss = 0.17645075\n",
      "Iteration 84, loss = 0.12353970\n",
      "Iteration 45, loss = 0.17248856\n",
      "Iteration 85, loss = 0.12306609\n",
      "Iteration 46, loss = 0.16875848\n",
      "Iteration 86, loss = 0.12172703\n",
      "Iteration 47, loss = 0.16539041\n",
      "Iteration 87, loss = 0.12052402\n",
      "Iteration 48, loss = 0.16439483\n",
      "Iteration 88, loss = 0.11726048\n",
      "Iteration 49, loss = 0.15966586\n",
      "Iteration 89, loss = 0.11738062\n",
      "Iteration 50, loss = 0.15643628\n",
      "Iteration 90, loss = 0.11750434\n",
      "Iteration 51, loss = 0.15528306\n",
      "Iteration 91, loss = 0.11362613\n",
      "Iteration 52, loss = 0.15124249\n",
      "Iteration 92, loss = 0.11203490\n",
      "Iteration 53, loss = 0.14913080\n",
      "Iteration 93, loss = 0.11204738\n",
      "Iteration 54, loss = 0.14575561\n",
      "Iteration 94, loss = 0.11461708\n",
      "Iteration 55, loss = 0.14530363\n",
      "Iteration 95, loss = 0.10910895\n",
      "Iteration 56, loss = 0.14219382\n",
      "Iteration 96, loss = 0.10748321\n",
      "Iteration 57, loss = 0.14023597\n",
      "Iteration 97, loss = 0.10621975\n",
      "Iteration 58, loss = 0.13597074\n",
      "Iteration 98, loss = 0.10569163\n",
      "Iteration 59, loss = 0.13560725\n",
      "Iteration 99, loss = 0.10228145\n",
      "Iteration 60, loss = 0.13256984\n",
      "Iteration 100, loss = 0.10271791\n",
      "Iteration 61, loss = 0.12934858\n",
      "Iteration 101, loss = 0.10194402\n",
      "Iteration 62, loss = 0.12898392\n",
      "Iteration 102, loss = 0.10066758\n",
      "Iteration 63, loss = 0.12567357\n",
      "Iteration 103, loss = 0.09875467\n",
      "Iteration 64, loss = 0.12325325\n",
      "Iteration 104, loss = 0.09721330\n",
      "Iteration 65, loss = 0.12161958\n",
      "Iteration 105, loss = 0.09810689\n",
      "Iteration 66, loss = 0.11901122\n",
      "Iteration 106, loss = 0.09695137\n",
      "Iteration 67, loss = 0.11777305\n",
      "Iteration 107, loss = 0.09649831\n",
      "Iteration 68, loss = 0.11531145\n",
      "Iteration 108, loss = 0.09467919\n",
      "Iteration 69, loss = 0.11538100\n",
      "Iteration 109, loss = 0.09660217\n",
      "Iteration 70, loss = 0.11447182\n",
      "Iteration 110, loss = 0.09521306\n",
      "Iteration 71, loss = 0.11465512\n",
      "Iteration 111, loss = 0.09385007\n",
      "Iteration 72, loss = 0.10886131\n",
      "Iteration 112, loss = 0.09156937\n",
      "Iteration 73, loss = 0.10867156\n",
      "Iteration 113, loss = 0.08946902\n",
      "Iteration 74, loss = 0.10724087\n",
      "Iteration 114, loss = 0.09049126\n",
      "Iteration 75, loss = 0.10396610\n",
      "Iteration 115, loss = 0.08973031\n",
      "Iteration 76, loss = 0.10595660\n",
      "Iteration 116, loss = 0.08817651\n",
      "Iteration 77, loss = 0.10190710\n",
      "Iteration 117, loss = 0.08685789\n",
      "Iteration 78, loss = 0.10065363\n",
      "Iteration 118, loss = 0.08346521\n",
      "Iteration 79, loss = 0.10023847\n",
      "Iteration 119, loss = 0.08601694\n",
      "Iteration 80, loss = 0.09702214\n",
      "Iteration 120, loss = 0.08489908\n",
      "Iteration 81, loss = 0.09728779\n",
      "Iteration 121, loss = 0.08131271\n",
      "Iteration 82, loss = 0.09479665\n",
      "Iteration 122, loss = 0.08181639\n",
      "Iteration 83, loss = 0.09402303\n",
      "Iteration 123, loss = 0.08131023\n",
      "Iteration 84, loss = 0.09307222\n",
      "Iteration 124, loss = 0.08051030\n",
      "Iteration 85, loss = 0.09637042\n",
      "Iteration 125, loss = 0.08077967\n",
      "Iteration 86, loss = 0.09102976\n",
      "Iteration 126, loss = 0.07932048\n",
      "Iteration 87, loss = 0.09106204\n",
      "Iteration 127, loss = 0.07640024\n",
      "Iteration 88, loss = 0.08780241\n",
      "Iteration 128, loss = 0.07606046\n",
      "Iteration 89, loss = 0.08885311\n",
      "Iteration 129, loss = 0.07511671\n",
      "Iteration 90, loss = 0.08830559\n",
      "Iteration 130, loss = 0.07488947\n",
      "Iteration 91, loss = 0.08598524\n",
      "Iteration 131, loss = 0.07604962\n",
      "Iteration 92, loss = 0.08562246\n",
      "Iteration 132, loss = 0.07237907\n",
      "Iteration 93, loss = 0.08542904\n",
      "Iteration 133, loss = 0.07366024\n",
      "Iteration 94, loss = 0.08396347\n",
      "Iteration 134, loss = 0.07177080\n",
      "Iteration 95, loss = 0.08181598\n",
      "Iteration 135, loss = 0.07129730\n",
      "Iteration 96, loss = 0.08067166\n",
      "Iteration 136, loss = 0.07165240\n",
      "Iteration 97, loss = 0.07900411\n",
      "Iteration 137, loss = 0.06943356\n",
      "Iteration 98, loss = 0.07849022\n",
      "Iteration 138, loss = 0.06892994\n",
      "Iteration 99, loss = 0.07819002\n",
      "Iteration 139, loss = 0.07244298\n",
      "Iteration 100, loss = 0.07599230\n",
      "Iteration 140, loss = 0.07006786\n",
      "Iteration 101, loss = 0.07548463\n",
      "Iteration 141, loss = 0.07011084\n",
      "Iteration 102, loss = 0.07433800\n",
      "Iteration 142, loss = 0.06575920\n",
      "Iteration 103, loss = 0.07359496\n",
      "Iteration 143, loss = 0.06613512\n",
      "Iteration 104, loss = 0.07216579\n",
      "Iteration 144, loss = 0.06776895\n",
      "Iteration 105, loss = 0.07295964\n",
      "Iteration 145, loss = 0.06602221\n",
      "Iteration 106, loss = 0.07093190\n",
      "Iteration 146, loss = 0.06691845\n",
      "Iteration 107, loss = 0.07058441\n",
      "Iteration 147, loss = 0.06442184\n",
      "Iteration 108, loss = 0.07163377\n",
      "Iteration 148, loss = 0.06398589\n",
      "Iteration 109, loss = 0.07092493\n",
      "Iteration 149, loss = 0.06300083\n",
      "Iteration 110, loss = 0.07139031\n",
      "Iteration 150, loss = 0.06216525\n",
      "Iteration 111, loss = 0.06838716\n",
      "Iteration 151, loss = 0.06190953\n",
      "Iteration 112, loss = 0.06797499\n",
      "Iteration 152, loss = 0.06011945\n",
      "Iteration 113, loss = 0.06857137\n",
      "Iteration 153, loss = 0.05959605\n",
      "Iteration 114, loss = 0.06490833\n",
      "Iteration 154, loss = 0.06007107\n",
      "Iteration 115, loss = 0.06576943\n",
      "Iteration 155, loss = 0.06120351\n",
      "Iteration 116, loss = 0.06529769\n",
      "Iteration 156, loss = 0.05911418\n",
      "Iteration 117, loss = 0.06357139\n",
      "Iteration 157, loss = 0.05784911\n",
      "Iteration 118, loss = 0.06300512\n",
      "Iteration 158, loss = 0.05649017\n",
      "Iteration 119, loss = 0.06194432\n",
      "Iteration 159, loss = 0.05962654\n",
      "Iteration 120, loss = 0.06183497\n",
      "Iteration 160, loss = 0.05614086\n",
      "Iteration 121, loss = 0.06087910\n",
      "Iteration 161, loss = 0.05669787\n",
      "Iteration 122, loss = 0.06148073\n",
      "Iteration 162, loss = 0.05544943\n",
      "Iteration 123, loss = 0.05969178\n",
      "Iteration 163, loss = 0.05666507\n",
      "Iteration 124, loss = 0.06014944\n",
      "Iteration 164, loss = 0.05397127\n",
      "Iteration 125, loss = 0.05851998\n",
      "Iteration 165, loss = 0.05395833\n",
      "Iteration 126, loss = 0.05931960\n",
      "Iteration 166, loss = 0.05546302\n",
      "Iteration 127, loss = 0.05770157\n",
      "Iteration 167, loss = 0.05292302\n",
      "Iteration 128, loss = 0.05644652\n",
      "Iteration 168, loss = 0.05269280\n",
      "Iteration 129, loss = 0.05587892\n",
      "Iteration 169, loss = 0.05712588\n",
      "Iteration 130, loss = 0.05444313\n",
      "Iteration 170, loss = 0.06076121\n",
      "Iteration 131, loss = 0.05472407\n",
      "Iteration 171, loss = 0.05927140\n",
      "Iteration 132, loss = 0.05621007\n",
      "Iteration 172, loss = 0.06151874\n",
      "Iteration 133, loss = 0.05501891\n",
      "Iteration 173, loss = 0.05690291\n",
      "Iteration 134, loss = 0.05384185\n",
      "Iteration 174, loss = 0.05554668\n",
      "Iteration 135, loss = 0.05627433\n",
      "Iteration 175, loss = 0.05043843\n",
      "Iteration 136, loss = 0.05164617\n",
      "Iteration 176, loss = 0.04947027\n",
      "Iteration 137, loss = 0.05254145\n",
      "Iteration 177, loss = 0.05090266\n",
      "Iteration 138, loss = 0.05174083\n",
      "Iteration 178, loss = 0.04938535\n",
      "Iteration 139, loss = 0.04993883\n",
      "Iteration 179, loss = 0.04873567\n",
      "Iteration 140, loss = 0.04937946\n",
      "Iteration 180, loss = 0.04859296\n",
      "Iteration 141, loss = 0.04895878\n",
      "Iteration 181, loss = 0.04757256\n",
      "Iteration 142, loss = 0.04910369\n",
      "Iteration 182, loss = 0.04672538\n",
      "Iteration 143, loss = 0.04893977\n",
      "Iteration 183, loss = 0.04728110\n",
      "Iteration 144, loss = 0.04787657\n",
      "Iteration 184, loss = 0.04733974\n",
      "Iteration 145, loss = 0.04840483\n",
      "Iteration 185, loss = 0.04574177\n",
      "Iteration 146, loss = 0.04736267\n",
      "Iteration 186, loss = 0.04720391\n",
      "Iteration 147, loss = 0.04943370\n",
      "Iteration 187, loss = 0.04830287\n",
      "Iteration 148, loss = 0.04748561\n",
      "Iteration 188, loss = 0.04478115\n",
      "Iteration 149, loss = 0.04524739\n",
      "Iteration 189, loss = 0.04734402\n",
      "Iteration 150, loss = 0.04724371\n",
      "Iteration 190, loss = 0.04299188\n",
      "Iteration 151, loss = 0.05255888\n",
      "Iteration 191, loss = 0.04441470\n",
      "Iteration 152, loss = 0.05069727\n",
      "Iteration 192, loss = 0.04507700\n",
      "Iteration 153, loss = 0.04291860\n",
      "Iteration 193, loss = 0.04347579\n",
      "Iteration 154, loss = 0.04630733\n",
      "Iteration 194, loss = 0.04363771\n",
      "Iteration 155, loss = 0.04701974\n",
      "Iteration 195, loss = 0.04327599\n",
      "Iteration 156, loss = 0.04427749\n",
      "Iteration 196, loss = 0.04230057\n",
      "Iteration 197, loss = 0.04278064\n",
      "Iteration 157, loss = 0.04414038\n",
      "Iteration 198, loss = 0.04203730\n",
      "Iteration 158, loss = 0.04290694\n",
      "Iteration 159, loss = 0.04233917\n",
      "Iteration 199, loss = 0.04219125\n",
      "Iteration 160, loss = 0.04283053\n",
      "Iteration 200, loss = 0.04290906\n",
      "Iteration 161, loss = 0.04059300\n",
      "Iteration 201, loss = 0.03979333\n",
      "Iteration 162, loss = 0.04027237\n",
      "Iteration 202, loss = 0.04168006\n",
      "Iteration 163, loss = 0.03952555\n",
      "Iteration 203, loss = 0.04226061\n",
      "Iteration 164, loss = 0.03997333\n",
      "Iteration 204, loss = 0.04035839\n",
      "Iteration 165, loss = 0.03958791\n",
      "Iteration 205, loss = 0.04138518\n",
      "Iteration 166, loss = 0.03804412\n",
      "Iteration 206, loss = 0.03980656\n",
      "Iteration 167, loss = 0.04202016\n",
      "Iteration 207, loss = 0.04003691\n",
      "Iteration 168, loss = 0.04148097\n",
      "Iteration 208, loss = 0.03903635\n",
      "Iteration 169, loss = 0.03772780\n",
      "Iteration 209, loss = 0.03919215\n",
      "Iteration 170, loss = 0.03699072\n",
      "Iteration 210, loss = 0.03961313\n",
      "Iteration 171, loss = 0.03859952\n",
      "Iteration 211, loss = 0.03872531\n",
      "Iteration 172, loss = 0.03646249\n",
      "Iteration 212, loss = 0.03909924\n",
      "Iteration 173, loss = 0.03623823\n",
      "Iteration 213, loss = 0.03746161\n",
      "Iteration 174, loss = 0.03851934\n",
      "Iteration 214, loss = 0.03703881\n",
      "Iteration 175, loss = 0.03587357\n",
      "Iteration 215, loss = 0.03721782\n",
      "Iteration 176, loss = 0.03559419\n",
      "Iteration 216, loss = 0.03834139\n",
      "Iteration 177, loss = 0.03692231\n",
      "Iteration 217, loss = 0.03822936\n",
      "Iteration 178, loss = 0.03620169\n",
      "Iteration 218, loss = 0.03593307\n",
      "Iteration 179, loss = 0.03733542\n",
      "Iteration 219, loss = 0.03525335\n",
      "Iteration 180, loss = 0.03616174\n",
      "Iteration 220, loss = 0.03556178\n",
      "Iteration 221, loss = 0.03452398\n",
      "Iteration 181, loss = 0.03636398\n",
      "Iteration 222, loss = 0.03487781\n",
      "Iteration 182, loss = 0.03772629\n",
      "Iteration 223, loss = 0.03424944\n",
      "Iteration 183, loss = 0.03533707\n",
      "Iteration 224, loss = 0.03412933\n",
      "Iteration 184, loss = 0.03434834\n",
      "Iteration 185, loss = 0.03538858\n",
      "Iteration 225, loss = 0.03423683\n",
      "Iteration 186, loss = 0.03327347\n",
      "Iteration 226, loss = 0.03509876\n",
      "Iteration 227, loss = 0.03391656\n",
      "Iteration 187, loss = 0.03675121\n",
      "Iteration 228, loss = 0.03647670\n",
      "Iteration 188, loss = 0.03573153\n",
      "Iteration 229, loss = 0.04099456\n",
      "Iteration 189, loss = 0.03604074\n",
      "Iteration 230, loss = 0.03449326\n",
      "Iteration 190, loss = 0.03448464\n",
      "Iteration 231, loss = 0.03628056\n",
      "Iteration 191, loss = 0.03074641\n",
      "Iteration 232, loss = 0.03525329\n",
      "Iteration 192, loss = 0.03131644\n",
      "Iteration 233, loss = 0.03631577\n",
      "Iteration 193, loss = 0.03245295\n",
      "Iteration 234, loss = 0.03219725\n",
      "Iteration 194, loss = 0.03088598\n",
      "Iteration 195, loss = 0.03218766\n",
      "Iteration 235, loss = 0.03113841\n",
      "Iteration 196, loss = 0.03062829\n",
      "Iteration 236, loss = 0.03236687\n",
      "Iteration 197, loss = 0.03187278\n",
      "Iteration 237, loss = 0.03137107\n",
      "Iteration 238, loss = 0.03140291\n",
      "Iteration 198, loss = 0.03176893\n",
      "Iteration 239, loss = 0.03242569\n",
      "Iteration 199, loss = 0.03039438\n",
      "Iteration 240, loss = 0.03114753Iteration 200, loss = 0.02913467\n",
      "\n",
      "Iteration 201, loss = 0.03013104\n",
      "Iteration 241, loss = 0.02995299\n",
      "Iteration 202, loss = 0.02836579\n",
      "Iteration 242, loss = 0.03252448\n",
      "Iteration 203, loss = 0.02860893\n",
      "Iteration 243, loss = 0.03157792\n",
      "Iteration 204, loss = 0.02952642\n",
      "Iteration 244, loss = 0.03366168\n",
      "Iteration 205, loss = 0.02945228\n",
      "Iteration 245, loss = 0.03236443\n",
      "Iteration 206, loss = 0.03070699\n",
      "Iteration 246, loss = 0.03283385\n",
      "Iteration 207, loss = 0.02990424\n",
      "Iteration 247, loss = 0.03130848\n",
      "Iteration 208, loss = 0.02941092\n",
      "Iteration 248, loss = 0.03427104\n",
      "Iteration 209, loss = 0.02712106\n",
      "Iteration 249, loss = 0.03205708\n",
      "Iteration 210, loss = 0.02925997\n",
      "Iteration 250, loss = 0.03007815\n",
      "Iteration 211, loss = 0.02664550\n",
      "Iteration 251, loss = 0.02909427\n",
      "Iteration 212, loss = 0.02770181\n",
      "Iteration 252, loss = 0.02852477\n",
      "Iteration 213, loss = 0.02729444\n",
      "Iteration 253, loss = 0.02795156\n",
      "Iteration 214, loss = 0.02613870\n",
      "Iteration 254, loss = 0.02874940\n",
      "Iteration 215, loss = 0.02561569\n",
      "Iteration 255, loss = 0.03167519\n",
      "Iteration 216, loss = 0.02579435\n",
      "Iteration 256, loss = 0.03172506\n",
      "Iteration 217, loss = 0.02712345\n",
      "Iteration 257, loss = 0.02959187\n",
      "Iteration 218, loss = 0.02599607\n",
      "Iteration 258, loss = 0.03337356\n",
      "Iteration 259, loss = 0.03339301\n",
      "Iteration 219, loss = 0.02573183\n",
      "Iteration 260, loss = 0.03238920\n",
      "Iteration 220, loss = 0.02522464\n",
      "Iteration 261, loss = 0.03323146\n",
      "Iteration 221, loss = 0.02647261\n",
      "Iteration 262, loss = 0.03092542\n",
      "Iteration 222, loss = 0.02584361\n",
      "Iteration 263, loss = 0.02924917\n",
      "Iteration 223, loss = 0.02474456\n",
      "Iteration 264, loss = 0.03120913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 224, loss = 0.02559933\n",
      "Iteration 225, loss = 0.02513368\n",
      "Iteration 226, loss = 0.02378598\n",
      "Iteration 227, loss = 0.02361121\n",
      "Iteration 228, loss = 0.02351045\n",
      "Iteration 229, loss = 0.02314769\n",
      "Iteration 230, loss = 0.02349502\n",
      "Iteration 231, loss = 0.02314169\n",
      "Iteration 232, loss = 0.02325608\n",
      "Iteration 233, loss = 0.02303959\n",
      "Iteration 234, loss = 0.02257772\n",
      "Iteration 235, loss = 0.02236713\n",
      "Iteration 236, loss = 0.02196912\n",
      "Iteration 1, loss = 1.60835785\n",
      "Iteration 237, loss = 0.02165469\n",
      "Iteration 2, loss = 1.18492908\n",
      "Iteration 238, loss = 0.02172729\n",
      "Iteration 3, loss = 0.94842546\n",
      "Iteration 239, loss = 0.02206054\n",
      "Iteration 4, loss = 0.79072966\n",
      "Iteration 240, loss = 0.02154698\n",
      "Iteration 5, loss = 0.70064458\n",
      "Iteration 241, loss = 0.02121129\n",
      "Iteration 6, loss = 0.61961697\n",
      "Iteration 242, loss = 0.02120569\n",
      "Iteration 7, loss = 0.57139073\n",
      "Iteration 243, loss = 0.02186671\n",
      "Iteration 8, loss = 0.52973924\n",
      "Iteration 244, loss = 0.02167420\n",
      "Iteration 9, loss = 0.49837995\n",
      "Iteration 245, loss = 0.02131529\n",
      "Iteration 10, loss = 0.47369156\n",
      "Iteration 246, loss = 0.02140590\n",
      "Iteration 11, loss = 0.45273864\n",
      "Iteration 247, loss = 0.02174004\n",
      "Iteration 12, loss = 0.43569252\n",
      "Iteration 248, loss = 0.02257738\n",
      "Iteration 13, loss = 0.41960921\n",
      "Iteration 249, loss = 0.02506500\n",
      "Iteration 14, loss = 0.40533252\n",
      "Iteration 250, loss = 0.02343532\n",
      "Iteration 15, loss = 0.39205106\n",
      "Iteration 251, loss = 0.02301304\n",
      "Iteration 16, loss = 0.37999165\n",
      "Iteration 252, loss = 0.02302458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.36863116\n",
      "Iteration 18, loss = 0.35888764\n",
      "Iteration 19, loss = 0.34861934\n",
      "Iteration 20, loss = 0.34067112\n",
      "Iteration 21, loss = 0.33122746\n",
      "Iteration 22, loss = 0.32385767\n",
      "Iteration 23, loss = 0.31703519\n",
      "Iteration 24, loss = 0.30877394\n",
      "Iteration 25, loss = 0.30238982\n",
      "Iteration 26, loss = 0.29576055\n",
      "Iteration 27, loss = 0.28909746\n",
      "Iteration 28, loss = 0.28370107\n",
      "Iteration 29, loss = 0.27831356\n",
      "Iteration 1, loss = 1.20356249\n",
      "Iteration 30, loss = 0.27337768\n",
      "Iteration 2, loss = 0.91100184\n",
      "Iteration 31, loss = 0.26820206\n",
      "Iteration 3, loss = 0.73980167\n",
      "Iteration 32, loss = 0.26329934\n",
      "Iteration 4, loss = 0.63553147\n",
      "Iteration 33, loss = 0.25916577\n",
      "Iteration 5, loss = 0.57562191\n",
      "Iteration 34, loss = 0.25450333\n",
      "Iteration 6, loss = 0.52232362\n",
      "Iteration 35, loss = 0.24967237\n",
      "Iteration 7, loss = 0.48689999\n",
      "Iteration 36, loss = 0.24655358\n",
      "Iteration 8, loss = 0.45992973\n",
      "Iteration 37, loss = 0.24263288\n",
      "Iteration 9, loss = 0.43743658\n",
      "Iteration 38, loss = 0.23814203\n",
      "Iteration 10, loss = 0.41839253\n",
      "Iteration 39, loss = 0.23566505\n",
      "Iteration 11, loss = 0.40336935\n",
      "Iteration 40, loss = 0.23053452\n",
      "Iteration 12, loss = 0.38950175\n",
      "Iteration 41, loss = 0.22691291\n",
      "Iteration 13, loss = 0.37698899\n",
      "Iteration 42, loss = 0.22465319\n",
      "Iteration 14, loss = 0.36588459\n",
      "Iteration 43, loss = 0.22261012\n",
      "Iteration 15, loss = 0.35488113\n",
      "Iteration 44, loss = 0.21662314\n",
      "Iteration 16, loss = 0.34588117\n",
      "Iteration 45, loss = 0.21562529\n",
      "Iteration 17, loss = 0.33756351\n",
      "Iteration 46, loss = 0.21197652\n",
      "Iteration 18, loss = 0.32905926\n",
      "Iteration 47, loss = 0.20822315\n",
      "Iteration 19, loss = 0.32196396\n",
      "Iteration 48, loss = 0.20512119\n",
      "Iteration 20, loss = 0.31439326\n",
      "Iteration 49, loss = 0.20181741\n",
      "Iteration 21, loss = 0.30783318\n",
      "Iteration 50, loss = 0.20048793\n",
      "Iteration 22, loss = 0.30091280\n",
      "Iteration 51, loss = 0.19658798Iteration 23, loss = 0.29683940\n",
      "\n",
      "Iteration 24, loss = 0.28881077\n",
      "Iteration 52, loss = 0.19440911\n",
      "Iteration 25, loss = 0.28394203\n",
      "Iteration 53, loss = 0.19313730\n",
      "Iteration 54, loss = 0.18976106\n",
      "Iteration 26, loss = 0.27853664\n",
      "Iteration 55, loss = 0.18656360\n",
      "Iteration 27, loss = 0.27223155\n",
      "Iteration 56, loss = 0.18395327\n",
      "Iteration 28, loss = 0.26784477\n",
      "Iteration 57, loss = 0.18073868\n",
      "Iteration 29, loss = 0.26167401\n",
      "Iteration 58, loss = 0.17906574\n",
      "Iteration 30, loss = 0.25754675\n",
      "Iteration 59, loss = 0.17694276\n",
      "Iteration 31, loss = 0.25188165\n",
      "Iteration 60, loss = 0.17487011\n",
      "Iteration 32, loss = 0.24846169\n",
      "Iteration 61, loss = 0.17213423\n",
      "Iteration 33, loss = 0.24408104\n",
      "Iteration 62, loss = 0.17049533\n",
      "Iteration 34, loss = 0.23934559\n",
      "Iteration 35, loss = 0.23563502\n",
      "Iteration 63, loss = 0.16991916\n",
      "Iteration 36, loss = 0.23160252\n",
      "Iteration 64, loss = 0.16476839\n",
      "Iteration 37, loss = 0.22764830\n",
      "Iteration 65, loss = 0.16550724\n",
      "Iteration 38, loss = 0.22670864\n",
      "Iteration 66, loss = 0.16224240\n",
      "Iteration 39, loss = 0.21894961\n",
      "Iteration 67, loss = 0.15900595\n",
      "Iteration 40, loss = 0.22185035\n",
      "Iteration 68, loss = 0.15885974\n",
      "Iteration 41, loss = 0.21673763\n",
      "Iteration 69, loss = 0.15870113\n",
      "Iteration 42, loss = 0.21230247\n",
      "Iteration 70, loss = 0.15324676\n",
      "Iteration 43, loss = 0.20808762\n",
      "Iteration 71, loss = 0.15171797\n",
      "Iteration 44, loss = 0.20415811\n",
      "Iteration 72, loss = 0.14990454\n",
      "Iteration 73, loss = 0.14837475\n",
      "Iteration 45, loss = 0.19889596\n",
      "Iteration 74, loss = 0.14582921\n",
      "Iteration 46, loss = 0.19827320\n",
      "Iteration 75, loss = 0.15077933\n",
      "Iteration 47, loss = 0.19687338\n",
      "Iteration 76, loss = 0.14048517\n",
      "Iteration 48, loss = 0.19029464\n",
      "Iteration 77, loss = 0.14251772\n",
      "Iteration 49, loss = 0.18912929\n",
      "Iteration 78, loss = 0.14002522\n",
      "Iteration 50, loss = 0.18722336\n",
      "Iteration 79, loss = 0.13632843\n",
      "Iteration 51, loss = 0.18175302\n",
      "Iteration 80, loss = 0.13730522\n",
      "Iteration 52, loss = 0.17950651\n",
      "Iteration 81, loss = 0.13342372\n",
      "Iteration 53, loss = 0.17593328\n",
      "Iteration 82, loss = 0.13378663\n",
      "Iteration 54, loss = 0.17292211\n",
      "Iteration 83, loss = 0.13114288\n",
      "Iteration 55, loss = 0.16968408\n",
      "Iteration 84, loss = 0.12936721\n",
      "Iteration 56, loss = 0.17016283\n",
      "Iteration 85, loss = 0.12735108\n",
      "Iteration 57, loss = 0.16520022\n",
      "Iteration 86, loss = 0.12570494\n",
      "Iteration 58, loss = 0.16366991\n",
      "Iteration 87, loss = 0.12515324\n",
      "Iteration 59, loss = 0.16356180\n",
      "Iteration 88, loss = 0.12392590\n",
      "Iteration 60, loss = 0.16004642\n",
      "Iteration 89, loss = 0.12301239\n",
      "Iteration 61, loss = 0.15831287\n",
      "Iteration 90, loss = 0.12116966\n",
      "Iteration 62, loss = 0.15504382\n",
      "Iteration 91, loss = 0.11901340\n",
      "Iteration 63, loss = 0.15355644\n",
      "Iteration 92, loss = 0.11836327\n",
      "Iteration 64, loss = 0.15259050\n",
      "Iteration 93, loss = 0.11847937\n",
      "Iteration 65, loss = 0.15153028\n",
      "Iteration 94, loss = 0.11750150\n",
      "Iteration 66, loss = 0.14517874\n",
      "Iteration 95, loss = 0.11414422\n",
      "Iteration 67, loss = 0.14677694\n",
      "Iteration 96, loss = 0.11688013\n",
      "Iteration 68, loss = 0.14249006\n",
      "Iteration 97, loss = 0.11340429\n",
      "Iteration 69, loss = 0.14064802\n",
      "Iteration 98, loss = 0.11176063\n",
      "Iteration 70, loss = 0.13926146\n",
      "Iteration 99, loss = 0.11023091\n",
      "Iteration 71, loss = 0.13682257\n",
      "Iteration 100, loss = 0.10798020\n",
      "Iteration 72, loss = 0.13615576\n",
      "Iteration 101, loss = 0.10956061\n",
      "Iteration 73, loss = 0.13486251\n",
      "Iteration 102, loss = 0.10810152\n",
      "Iteration 74, loss = 0.13343539\n",
      "Iteration 103, loss = 0.10457064\n",
      "Iteration 75, loss = 0.13222126\n",
      "Iteration 104, loss = 0.10624360\n",
      "Iteration 76, loss = 0.12979923\n",
      "Iteration 105, loss = 0.10618011\n",
      "Iteration 77, loss = 0.12774047\n",
      "Iteration 106, loss = 0.10100994\n",
      "Iteration 78, loss = 0.12624430\n",
      "Iteration 107, loss = 0.10103543\n",
      "Iteration 79, loss = 0.12796594\n",
      "Iteration 108, loss = 0.09868083\n",
      "Iteration 80, loss = 0.12370637\n",
      "Iteration 109, loss = 0.09803467\n",
      "Iteration 81, loss = 0.12488531\n",
      "Iteration 110, loss = 0.09786251\n",
      "Iteration 82, loss = 0.12325411\n",
      "Iteration 111, loss = 0.09760374\n",
      "Iteration 83, loss = 0.11988674\n",
      "Iteration 112, loss = 0.09516605\n",
      "Iteration 84, loss = 0.11897949\n",
      "Iteration 113, loss = 0.09562405\n",
      "Iteration 85, loss = 0.11725469\n",
      "Iteration 114, loss = 0.09710804\n",
      "Iteration 86, loss = 0.11484245\n",
      "Iteration 115, loss = 0.09592144\n",
      "Iteration 87, loss = 0.11499926\n",
      "Iteration 116, loss = 0.09498650\n",
      "Iteration 88, loss = 0.11149485\n",
      "Iteration 117, loss = 0.09162306\n",
      "Iteration 89, loss = 0.11074153\n",
      "Iteration 118, loss = 0.08928192\n",
      "Iteration 90, loss = 0.10893479\n",
      "Iteration 119, loss = 0.08976373\n",
      "Iteration 91, loss = 0.10741629\n",
      "Iteration 120, loss = 0.09132153\n",
      "Iteration 92, loss = 0.11043700\n",
      "Iteration 121, loss = 0.08832002\n",
      "Iteration 93, loss = 0.11489435\n",
      "Iteration 122, loss = 0.08573056\n",
      "Iteration 94, loss = 0.10942292\n",
      "Iteration 123, loss = 0.08578823\n",
      "Iteration 95, loss = 0.10269010\n",
      "Iteration 124, loss = 0.08495443\n",
      "Iteration 96, loss = 0.10632901\n",
      "Iteration 125, loss = 0.08449134\n",
      "Iteration 97, loss = 0.10111215\n",
      "Iteration 126, loss = 0.08447558\n",
      "Iteration 98, loss = 0.10087884\n",
      "Iteration 127, loss = 0.08298578\n",
      "Iteration 99, loss = 0.09869813\n",
      "Iteration 128, loss = 0.08098103\n",
      "Iteration 100, loss = 0.09869920\n",
      "Iteration 129, loss = 0.08117839\n",
      "Iteration 101, loss = 0.09598571\n",
      "Iteration 130, loss = 0.08029870\n",
      "Iteration 102, loss = 0.09578431\n",
      "Iteration 131, loss = 0.07913567\n",
      "Iteration 103, loss = 0.09439630\n",
      "Iteration 132, loss = 0.07718494\n",
      "Iteration 104, loss = 0.09340585\n",
      "Iteration 133, loss = 0.07671133\n",
      "Iteration 105, loss = 0.09191864\n",
      "Iteration 134, loss = 0.07647677\n",
      "Iteration 106, loss = 0.09207368\n",
      "Iteration 135, loss = 0.08060393\n",
      "Iteration 107, loss = 0.08988270\n",
      "Iteration 136, loss = 0.07682589\n",
      "Iteration 108, loss = 0.09078846\n",
      "Iteration 137, loss = 0.07467707\n",
      "Iteration 109, loss = 0.09071981\n",
      "Iteration 138, loss = 0.07570656\n",
      "Iteration 110, loss = 0.08896884\n",
      "Iteration 139, loss = 0.07318797\n",
      "Iteration 111, loss = 0.08882268\n",
      "Iteration 140, loss = 0.07154552\n",
      "Iteration 112, loss = 0.08421403\n",
      "Iteration 113, loss = 0.08499135\n",
      "Iteration 141, loss = 0.07386872\n",
      "Iteration 114, loss = 0.08604807\n",
      "Iteration 142, loss = 0.07935017\n",
      "Iteration 115, loss = 0.08507852\n",
      "Iteration 143, loss = 0.08021769\n",
      "Iteration 116, loss = 0.08205974\n",
      "Iteration 144, loss = 0.07805690\n",
      "Iteration 117, loss = 0.08052739\n",
      "Iteration 145, loss = 0.06768799\n",
      "Iteration 118, loss = 0.08196931\n",
      "Iteration 146, loss = 0.07378744\n",
      "Iteration 119, loss = 0.07870523\n",
      "Iteration 147, loss = 0.06786547\n",
      "Iteration 120, loss = 0.07703535\n",
      "Iteration 148, loss = 0.06794334\n",
      "Iteration 121, loss = 0.07695267\n",
      "Iteration 149, loss = 0.06525356\n",
      "Iteration 122, loss = 0.07882511\n",
      "Iteration 150, loss = 0.06497865\n",
      "Iteration 123, loss = 0.08012539\n",
      "Iteration 151, loss = 0.06375046\n",
      "Iteration 124, loss = 0.07593985\n",
      "Iteration 152, loss = 0.06330079\n",
      "Iteration 125, loss = 0.07404583\n",
      "Iteration 153, loss = 0.06334104\n",
      "Iteration 126, loss = 0.07596099\n",
      "Iteration 154, loss = 0.06080203\n",
      "Iteration 127, loss = 0.07183913\n",
      "Iteration 155, loss = 0.06165408\n",
      "Iteration 128, loss = 0.07143665\n",
      "Iteration 156, loss = 0.06278775\n",
      "Iteration 129, loss = 0.07022917\n",
      "Iteration 157, loss = 0.05987906\n",
      "Iteration 130, loss = 0.06917426\n",
      "Iteration 158, loss = 0.05962415\n",
      "Iteration 131, loss = 0.06998726\n",
      "Iteration 159, loss = 0.05923256\n",
      "Iteration 132, loss = 0.07188156\n",
      "Iteration 160, loss = 0.06073805\n",
      "Iteration 133, loss = 0.06761135\n",
      "Iteration 161, loss = 0.06081234\n",
      "Iteration 134, loss = 0.06806785\n",
      "Iteration 162, loss = 0.06504380\n",
      "Iteration 135, loss = 0.06655340\n",
      "Iteration 163, loss = 0.05751529\n",
      "Iteration 136, loss = 0.06600124\n",
      "Iteration 164, loss = 0.05628884\n",
      "Iteration 137, loss = 0.06445213\n",
      "Iteration 165, loss = 0.05745567\n",
      "Iteration 138, loss = 0.06393075\n",
      "Iteration 166, loss = 0.05790545\n",
      "Iteration 139, loss = 0.06647631\n",
      "Iteration 167, loss = 0.05440533\n",
      "Iteration 140, loss = 0.07082861\n",
      "Iteration 168, loss = 0.05543536\n",
      "Iteration 141, loss = 0.07417861\n",
      "Iteration 169, loss = 0.05517680\n",
      "Iteration 142, loss = 0.06834039\n",
      "Iteration 170, loss = 0.05334829\n",
      "Iteration 143, loss = 0.06275532\n",
      "Iteration 171, loss = 0.05408028\n",
      "Iteration 144, loss = 0.06193697\n",
      "Iteration 172, loss = 0.05402970\n",
      "Iteration 145, loss = 0.06271090\n",
      "Iteration 173, loss = 0.05734695\n",
      "Iteration 146, loss = 0.05960252\n",
      "Iteration 174, loss = 0.05254527\n",
      "Iteration 147, loss = 0.05811681\n",
      "Iteration 175, loss = 0.05114407\n",
      "Iteration 148, loss = 0.06083018\n",
      "Iteration 176, loss = 0.05031146\n",
      "Iteration 149, loss = 0.06943053\n",
      "Iteration 177, loss = 0.05079226\n",
      "Iteration 150, loss = 0.06676579\n",
      "Iteration 178, loss = 0.05133104\n",
      "Iteration 151, loss = 0.06190389\n",
      "Iteration 179, loss = 0.04905196\n",
      "Iteration 152, loss = 0.06128170\n",
      "Iteration 180, loss = 0.04907152\n",
      "Iteration 153, loss = 0.05967138\n",
      "Iteration 181, loss = 0.05084912\n",
      "Iteration 154, loss = 0.06758720\n",
      "Iteration 182, loss = 0.05247711\n",
      "Iteration 155, loss = 0.06506161\n",
      "Iteration 183, loss = 0.05113346\n",
      "Iteration 156, loss = 0.05983324\n",
      "Iteration 184, loss = 0.05224009\n",
      "Iteration 157, loss = 0.05947523\n",
      "Iteration 185, loss = 0.05009508\n",
      "Iteration 158, loss = 0.05435354\n",
      "Iteration 186, loss = 0.04713156\n",
      "Iteration 159, loss = 0.05539574\n",
      "Iteration 187, loss = 0.04521717\n",
      "Iteration 160, loss = 0.05372767\n",
      "Iteration 188, loss = 0.04833212\n",
      "Iteration 161, loss = 0.05322892\n",
      "Iteration 189, loss = 0.04856473\n",
      "Iteration 162, loss = 0.05233144\n",
      "Iteration 190, loss = 0.04589231\n",
      "Iteration 163, loss = 0.05053149\n",
      "Iteration 164, loss = 0.05008971\n",
      "Iteration 191, loss = 0.04590242\n",
      "Iteration 165, loss = 0.05114942\n",
      "Iteration 192, loss = 0.04413475\n",
      "Iteration 166, loss = 0.05218219\n",
      "Iteration 193, loss = 0.04386806\n",
      "Iteration 167, loss = 0.05046759\n",
      "Iteration 194, loss = 0.04590434\n",
      "Iteration 168, loss = 0.05159540\n",
      "Iteration 195, loss = 0.04482194\n",
      "Iteration 169, loss = 0.05304859\n",
      "Iteration 196, loss = 0.04628762\n",
      "Iteration 170, loss = 0.04722341\n",
      "Iteration 197, loss = 0.04796758\n",
      "Iteration 171, loss = 0.04877439\n",
      "Iteration 198, loss = 0.04058037\n",
      "Iteration 172, loss = 0.04794294\n",
      "Iteration 199, loss = 0.04648609\n",
      "Iteration 173, loss = 0.04911303\n",
      "Iteration 200, loss = 0.04507206\n",
      "Iteration 174, loss = 0.04641412\n",
      "Iteration 201, loss = 0.04267600\n",
      "Iteration 175, loss = 0.04549115\n",
      "Iteration 202, loss = 0.04013150\n",
      "Iteration 176, loss = 0.04811658\n",
      "Iteration 203, loss = 0.04017968\n",
      "Iteration 177, loss = 0.04841199\n",
      "Iteration 204, loss = 0.04278711\n",
      "Iteration 178, loss = 0.04634895\n",
      "Iteration 205, loss = 0.04077875\n",
      "Iteration 179, loss = 0.04511274\n",
      "Iteration 206, loss = 0.04127136\n",
      "Iteration 180, loss = 0.04442279\n",
      "Iteration 207, loss = 0.03973892\n",
      "Iteration 181, loss = 0.04474443\n",
      "Iteration 208, loss = 0.03929514\n",
      "Iteration 209, loss = 0.03910897\n",
      "Iteration 182, loss = 0.04433161\n",
      "Iteration 210, loss = 0.03855372\n",
      "Iteration 183, loss = 0.04371781\n",
      "Iteration 211, loss = 0.03920448\n",
      "Iteration 184, loss = 0.04211478\n",
      "Iteration 212, loss = 0.03970804\n",
      "Iteration 185, loss = 0.04354145\n",
      "Iteration 213, loss = 0.03883682\n",
      "Iteration 186, loss = 0.04226875\n",
      "Iteration 214, loss = 0.03806154\n",
      "Iteration 187, loss = 0.04188275\n",
      "Iteration 215, loss = 0.03961551\n",
      "Iteration 188, loss = 0.04228525\n",
      "Iteration 216, loss = 0.03914776\n",
      "Iteration 189, loss = 0.04272692\n",
      "Iteration 217, loss = 0.03640099\n",
      "Iteration 190, loss = 0.04419149\n",
      "Iteration 218, loss = 0.03880668\n",
      "Iteration 191, loss = 0.04178230\n",
      "Iteration 219, loss = 0.03565357\n",
      "Iteration 192, loss = 0.04255375\n",
      "Iteration 220, loss = 0.03882842\n",
      "Iteration 193, loss = 0.03875391\n",
      "Iteration 221, loss = 0.03760812\n",
      "Iteration 194, loss = 0.04090426\n",
      "Iteration 222, loss = 0.03888640\n",
      "Iteration 195, loss = 0.04002034\n",
      "Iteration 223, loss = 0.03491029\n",
      "Iteration 196, loss = 0.04217852\n",
      "Iteration 224, loss = 0.03625591\n",
      "Iteration 197, loss = 0.04104731\n",
      "Iteration 225, loss = 0.03602304\n",
      "Iteration 198, loss = 0.04409307\n",
      "Iteration 226, loss = 0.03635266\n",
      "Iteration 199, loss = 0.04393194\n",
      "Iteration 227, loss = 0.03526908\n",
      "Iteration 200, loss = 0.04030533\n",
      "Iteration 228, loss = 0.03370368\n",
      "Iteration 201, loss = 0.03669891\n",
      "Iteration 202, loss = 0.03909253\n",
      "Iteration 229, loss = 0.03460760\n",
      "Iteration 203, loss = 0.03794405\n",
      "Iteration 230, loss = 0.03448570\n",
      "Iteration 204, loss = 0.04017930\n",
      "Iteration 231, loss = 0.03619361\n",
      "Iteration 205, loss = 0.03939281\n",
      "Iteration 232, loss = 0.03767294\n",
      "Iteration 206, loss = 0.03742984\n",
      "Iteration 233, loss = 0.03322066\n",
      "Iteration 207, loss = 0.03699178\n",
      "Iteration 234, loss = 0.03381122\n",
      "Iteration 208, loss = 0.03807073\n",
      "Iteration 235, loss = 0.03213756\n",
      "Iteration 209, loss = 0.03809360\n",
      "Iteration 236, loss = 0.03122977\n",
      "Iteration 210, loss = 0.03643067\n",
      "Iteration 237, loss = 0.03168016\n",
      "Iteration 211, loss = 0.03581034\n",
      "Iteration 238, loss = 0.03204685\n",
      "Iteration 212, loss = 0.03572869\n",
      "Iteration 239, loss = 0.03194018\n",
      "Iteration 213, loss = 0.03556821\n",
      "Iteration 240, loss = 0.03077533\n",
      "Iteration 214, loss = 0.03465514\n",
      "Iteration 241, loss = 0.03137291\n",
      "Iteration 215, loss = 0.03476111\n",
      "Iteration 242, loss = 0.03233227\n",
      "Iteration 216, loss = 0.03454136\n",
      "Iteration 243, loss = 0.03320259\n",
      "Iteration 217, loss = 0.03397959\n",
      "Iteration 244, loss = 0.03469936\n",
      "Iteration 218, loss = 0.03667191\n",
      "Iteration 245, loss = 0.03267802\n",
      "Iteration 219, loss = 0.03512761\n",
      "Iteration 246, loss = 0.03444574\n",
      "Iteration 220, loss = 0.03510795\n",
      "Iteration 247, loss = 0.02892029\n",
      "Iteration 221, loss = 0.03361153\n",
      "Iteration 248, loss = 0.03255524\n",
      "Iteration 222, loss = 0.03353774\n",
      "Iteration 249, loss = 0.03072391\n",
      "Iteration 223, loss = 0.03239417\n",
      "Iteration 250, loss = 0.02994270\n",
      "Iteration 224, loss = 0.03286866\n",
      "Iteration 251, loss = 0.02818108\n",
      "Iteration 225, loss = 0.03463288\n",
      "Iteration 252, loss = 0.02908521\n",
      "Iteration 226, loss = 0.03541339\n",
      "Iteration 253, loss = 0.02888580\n",
      "Iteration 227, loss = 0.03609938\n",
      "Iteration 254, loss = 0.02878296\n",
      "Iteration 228, loss = 0.03369970\n",
      "Iteration 255, loss = 0.03095248\n",
      "Iteration 229, loss = 0.03659912\n",
      "Iteration 256, loss = 0.03175199\n",
      "Iteration 230, loss = 0.03510417Iteration 257, loss = 0.02939877\n",
      "\n",
      "Iteration 231, loss = 0.03302607\n",
      "Iteration 258, loss = 0.03072340\n",
      "Iteration 232, loss = 0.03106287\n",
      "Iteration 259, loss = 0.02807167\n",
      "Iteration 233, loss = 0.03249858\n",
      "Iteration 260, loss = 0.02667314\n",
      "Iteration 234, loss = 0.03253495\n",
      "Iteration 261, loss = 0.02750362\n",
      "Iteration 235, loss = 0.03227452\n",
      "Iteration 262, loss = 0.03120907\n",
      "Iteration 236, loss = 0.03396912\n",
      "Iteration 263, loss = 0.03272427\n",
      "Iteration 237, loss = 0.03120255\n",
      "Iteration 264, loss = 0.02775310\n",
      "Iteration 238, loss = 0.03026697\n",
      "Iteration 265, loss = 0.02999751\n",
      "Iteration 239, loss = 0.02954884\n",
      "Iteration 266, loss = 0.02790853\n",
      "Iteration 240, loss = 0.03131604\n",
      "Iteration 267, loss = 0.02715740\n",
      "Iteration 241, loss = 0.03179585\n",
      "Iteration 268, loss = 0.02674221\n",
      "Iteration 242, loss = 0.02869636\n",
      "Iteration 269, loss = 0.02765496\n",
      "Iteration 243, loss = 0.03030659\n",
      "Iteration 270, loss = 0.02605870\n",
      "Iteration 244, loss = 0.02929221\n",
      "Iteration 271, loss = 0.02797289\n",
      "Iteration 245, loss = 0.03089369\n",
      "Iteration 272, loss = 0.02747755\n",
      "Iteration 246, loss = 0.02849443\n",
      "Iteration 273, loss = 0.02603166\n",
      "Iteration 247, loss = 0.02844181\n",
      "Iteration 274, loss = 0.02578335\n",
      "Iteration 248, loss = 0.02831443\n",
      "Iteration 275, loss = 0.02455010\n",
      "Iteration 249, loss = 0.02854111\n",
      "Iteration 276, loss = 0.02602524\n",
      "Iteration 250, loss = 0.02778452\n",
      "Iteration 277, loss = 0.02689436\n",
      "Iteration 251, loss = 0.02874203\n",
      "Iteration 278, loss = 0.02702066\n",
      "Iteration 252, loss = 0.02809253\n",
      "Iteration 279, loss = 0.02623687\n",
      "Iteration 253, loss = 0.02948023\n",
      "Iteration 280, loss = 0.02390583\n",
      "Iteration 254, loss = 0.02901329\n",
      "Iteration 281, loss = 0.02418852\n",
      "Iteration 255, loss = 0.02767138\n",
      "Iteration 282, loss = 0.02530095\n",
      "Iteration 283, loss = 0.02995019\n",
      "Iteration 256, loss = 0.02750587\n",
      "Iteration 284, loss = 0.03655884\n",
      "Iteration 257, loss = 0.02716076\n",
      "Iteration 258, loss = 0.02721466\n",
      "Iteration 285, loss = 0.03257571\n",
      "Iteration 259, loss = 0.02821397\n",
      "Iteration 286, loss = 0.02993881\n",
      "Iteration 260, loss = 0.02667886\n",
      "Iteration 287, loss = 0.02891783\n",
      "Iteration 261, loss = 0.02653591\n",
      "Iteration 288, loss = 0.02823809\n",
      "Iteration 262, loss = 0.02650402\n",
      "Iteration 289, loss = 0.02431911\n",
      "Iteration 263, loss = 0.02676279\n",
      "Iteration 290, loss = 0.02462916\n",
      "Iteration 264, loss = 0.02612768\n",
      "Iteration 291, loss = 0.02669171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 265, loss = 0.02603873\n",
      "Iteration 266, loss = 0.02561008\n",
      "Iteration 267, loss = 0.02628183\n",
      "Iteration 268, loss = 0.02615574\n",
      "Iteration 269, loss = 0.02645141\n",
      "Iteration 270, loss = 0.02633418\n",
      "Iteration 271, loss = 0.02619747\n",
      "Iteration 272, loss = 0.02520107\n",
      "Iteration 273, loss = 0.02535166\n",
      "Iteration 274, loss = 0.02514232\n",
      "Iteration 275, loss = 0.02498847\n",
      "Iteration 276, loss = 0.02511759\n",
      "Iteration 277, loss = 0.02568579\n",
      "Iteration 1, loss = 1.63856794\n",
      "Iteration 278, loss = 0.02546009\n",
      "Iteration 2, loss = 1.08528246\n",
      "Iteration 279, loss = 0.02465503\n",
      "Iteration 3, loss = 0.84700517\n",
      "Iteration 280, loss = 0.02567287\n",
      "Iteration 4, loss = 0.70425732\n",
      "Iteration 281, loss = 0.02525851\n",
      "Iteration 5, loss = 0.61534046\n",
      "Iteration 282, loss = 0.02414420\n",
      "Iteration 6, loss = 0.56067930\n",
      "Iteration 283, loss = 0.02574920\n",
      "Iteration 7, loss = 0.51866730\n",
      "Iteration 284, loss = 0.02476919\n",
      "Iteration 8, loss = 0.48884019\n",
      "Iteration 285, loss = 0.02376485\n",
      "Iteration 9, loss = 0.46542154\n",
      "Iteration 286, loss = 0.02561431\n",
      "Iteration 10, loss = 0.44607957\n",
      "Iteration 287, loss = 0.02404457\n",
      "Iteration 11, loss = 0.42912480\n",
      "Iteration 288, loss = 0.02310971\n",
      "Iteration 12, loss = 0.41443850\n",
      "Iteration 289, loss = 0.02333599\n",
      "Iteration 13, loss = 0.40250456\n",
      "Iteration 290, loss = 0.02357410\n",
      "Iteration 14, loss = 0.39083546\n",
      "Iteration 291, loss = 0.02381095\n",
      "Iteration 15, loss = 0.38003922\n",
      "Iteration 292, loss = 0.02337030\n",
      "Iteration 16, loss = 0.37051855\n",
      "Iteration 293, loss = 0.02330464\n",
      "Iteration 17, loss = 0.36165894\n",
      "Iteration 294, loss = 0.02265880\n",
      "Iteration 18, loss = 0.35344737\n",
      "Iteration 295, loss = 0.02578110\n",
      "Iteration 19, loss = 0.34525036\n",
      "Iteration 296, loss = 0.02634227\n",
      "Iteration 20, loss = 0.33686447\n",
      "Iteration 297, loss = 0.02618194\n",
      "Iteration 21, loss = 0.32988151\n",
      "Iteration 298, loss = 0.02351178\n",
      "Iteration 22, loss = 0.32270805\n",
      "Iteration 299, loss = 0.02350192\n",
      "Iteration 23, loss = 0.31625931\n",
      "Iteration 300, loss = 0.02371101\n",
      "Iteration 24, loss = 0.31026195\n",
      "Iteration 301, loss = 0.02327135\n",
      "Iteration 25, loss = 0.30538858\n",
      "Iteration 302, loss = 0.02558957\n",
      "Iteration 26, loss = 0.29844196\n",
      "Iteration 303, loss = 0.02140420\n",
      "Iteration 27, loss = 0.29210029\n",
      "Iteration 304, loss = 0.02474440\n",
      "Iteration 28, loss = 0.28672462\n",
      "Iteration 305, loss = 0.02150079\n",
      "Iteration 29, loss = 0.28197129\n",
      "Iteration 306, loss = 0.02207802\n",
      "Iteration 30, loss = 0.27632193\n",
      "Iteration 307, loss = 0.02166343\n",
      "Iteration 31, loss = 0.27177917\n",
      "Iteration 308, loss = 0.02211768\n",
      "Iteration 32, loss = 0.26793672\n",
      "Iteration 309, loss = 0.02321577\n",
      "Iteration 33, loss = 0.26193951\n",
      "Iteration 310, loss = 0.02130012\n",
      "Iteration 34, loss = 0.25752757\n",
      "Iteration 311, loss = 0.02113053\n",
      "Iteration 35, loss = 0.25335794\n",
      "Iteration 312, loss = 0.02092747\n",
      "Iteration 36, loss = 0.24945718\n",
      "Iteration 313, loss = 0.02142175\n",
      "Iteration 37, loss = 0.24510380\n",
      "Iteration 314, loss = 0.02196086\n",
      "Iteration 38, loss = 0.24188435\n",
      "Iteration 315, loss = 0.02069171\n",
      "Iteration 39, loss = 0.23833436\n",
      "Iteration 316, loss = 0.02127791\n",
      "Iteration 40, loss = 0.23552241\n",
      "Iteration 317, loss = 0.02003565\n",
      "Iteration 41, loss = 0.23122997\n",
      "Iteration 318, loss = 0.02034869\n",
      "Iteration 42, loss = 0.22784412\n",
      "Iteration 319, loss = 0.02107134\n",
      "Iteration 43, loss = 0.22333662\n",
      "Iteration 320, loss = 0.02144766\n",
      "Iteration 44, loss = 0.21981497\n",
      "Iteration 321, loss = 0.02046621\n",
      "Iteration 45, loss = 0.21613796\n",
      "Iteration 322, loss = 0.02206145\n",
      "Iteration 46, loss = 0.21342412\n",
      "Iteration 323, loss = 0.02401420\n",
      "Iteration 47, loss = 0.21140803\n",
      "Iteration 324, loss = 0.03107298\n",
      "Iteration 48, loss = 0.20945795\n",
      "Iteration 325, loss = 0.02773615\n",
      "Iteration 49, loss = 0.20350202\n",
      "Iteration 326, loss = 0.02878210\n",
      "Iteration 50, loss = 0.20134764\n",
      "Iteration 327, loss = 0.03616539\n",
      "Iteration 51, loss = 0.19766357\n",
      "Iteration 328, loss = 0.02228668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.19613581\n",
      "Iteration 53, loss = 0.19118651\n",
      "Iteration 54, loss = 0.19036926\n",
      "Iteration 55, loss = 0.18582016\n",
      "Iteration 56, loss = 0.18326280\n",
      "Iteration 57, loss = 0.18107157\n",
      "Iteration 58, loss = 0.17788735\n",
      "Iteration 59, loss = 0.17585656\n",
      "Iteration 60, loss = 0.17371316\n",
      "Iteration 61, loss = 0.17047060\n",
      "Iteration 62, loss = 0.16916956\n",
      "Iteration 63, loss = 0.16473163\n",
      "Iteration 64, loss = 0.16463327\n",
      "Iteration 1, loss = 1.67987477\n",
      "Iteration 65, loss = 0.16158178\n",
      "Iteration 2, loss = 1.01068574\n",
      "Iteration 66, loss = 0.15953964\n",
      "Iteration 3, loss = 0.76687647\n",
      "Iteration 67, loss = 0.15590048\n",
      "Iteration 4, loss = 0.62335667\n",
      "Iteration 68, loss = 0.15546225\n",
      "Iteration 5, loss = 0.56108260\n",
      "Iteration 69, loss = 0.15223080\n",
      "Iteration 6, loss = 0.51598468\n",
      "Iteration 70, loss = 0.15055651\n",
      "Iteration 7, loss = 0.47760908\n",
      "Iteration 71, loss = 0.14754665\n",
      "Iteration 8, loss = 0.45481364\n",
      "Iteration 72, loss = 0.14686884\n",
      "Iteration 9, loss = 0.43467821\n",
      "Iteration 73, loss = 0.14477809\n",
      "Iteration 10, loss = 0.41560209\n",
      "Iteration 74, loss = 0.14153710\n",
      "Iteration 11, loss = 0.40268683\n",
      "Iteration 75, loss = 0.14204096\n",
      "Iteration 12, loss = 0.38775390\n",
      "Iteration 76, loss = 0.13910101\n",
      "Iteration 13, loss = 0.37614028\n",
      "Iteration 77, loss = 0.13759928\n",
      "Iteration 14, loss = 0.36629745\n",
      "Iteration 78, loss = 0.13410655\n",
      "Iteration 15, loss = 0.35636765\n",
      "Iteration 79, loss = 0.13357061\n",
      "Iteration 16, loss = 0.34791534\n",
      "Iteration 80, loss = 0.13371129\n",
      "Iteration 17, loss = 0.33864949\n",
      "Iteration 81, loss = 0.13027980\n",
      "Iteration 18, loss = 0.33030998\n",
      "Iteration 82, loss = 0.12812398\n",
      "Iteration 19, loss = 0.32333161\n",
      "Iteration 83, loss = 0.13002856\n",
      "Iteration 20, loss = 0.31666579\n",
      "Iteration 84, loss = 0.12501498\n",
      "Iteration 21, loss = 0.30911316\n",
      "Iteration 85, loss = 0.12184253\n",
      "Iteration 22, loss = 0.30382171\n",
      "Iteration 86, loss = 0.12114412\n",
      "Iteration 23, loss = 0.29685099\n",
      "Iteration 87, loss = 0.11989203\n",
      "Iteration 24, loss = 0.29063822\n",
      "Iteration 88, loss = 0.11855355\n",
      "Iteration 25, loss = 0.28474685\n",
      "Iteration 89, loss = 0.11661748\n",
      "Iteration 26, loss = 0.27986127\n",
      "Iteration 90, loss = 0.11514764\n",
      "Iteration 27, loss = 0.27485699\n",
      "Iteration 91, loss = 0.11268643\n",
      "Iteration 28, loss = 0.26893304\n",
      "Iteration 92, loss = 0.11233248\n",
      "Iteration 29, loss = 0.26420319\n",
      "Iteration 93, loss = 0.10973105\n",
      "Iteration 30, loss = 0.26086151\n",
      "Iteration 94, loss = 0.10895289\n",
      "Iteration 31, loss = 0.25569911\n",
      "Iteration 95, loss = 0.10758167\n",
      "Iteration 32, loss = 0.25083380\n",
      "Iteration 96, loss = 0.10628596\n",
      "Iteration 33, loss = 0.24618723\n",
      "Iteration 97, loss = 0.10509935\n",
      "Iteration 34, loss = 0.24210492\n",
      "Iteration 98, loss = 0.10443740\n",
      "Iteration 35, loss = 0.23840780\n",
      "Iteration 99, loss = 0.10382900\n",
      "Iteration 36, loss = 0.23556281\n",
      "Iteration 100, loss = 0.10182345\n",
      "Iteration 37, loss = 0.23044967\n",
      "Iteration 101, loss = 0.09905041\n",
      "Iteration 38, loss = 0.22780853\n",
      "Iteration 39, loss = 0.22259589\n",
      "Iteration 102, loss = 0.09858318\n",
      "Iteration 103, loss = 0.09823399\n",
      "Iteration 40, loss = 0.22060402\n",
      "Iteration 104, loss = 0.10140435\n",
      "Iteration 41, loss = 0.21645655\n",
      "Iteration 105, loss = 0.09549291\n",
      "Iteration 42, loss = 0.21341797\n",
      "Iteration 106, loss = 0.09439339\n",
      "Iteration 43, loss = 0.21047189\n",
      "Iteration 107, loss = 0.09447439\n",
      "Iteration 44, loss = 0.20718064\n",
      "Iteration 108, loss = 0.09284987\n",
      "Iteration 45, loss = 0.20369983\n",
      "Iteration 109, loss = 0.09022020\n",
      "Iteration 46, loss = 0.20308110\n",
      "Iteration 110, loss = 0.09184010\n",
      "Iteration 47, loss = 0.19974592\n",
      "Iteration 111, loss = 0.08706475\n",
      "Iteration 48, loss = 0.19503294\n",
      "Iteration 49, loss = 0.19158398\n",
      "Iteration 112, loss = 0.08624581\n",
      "Iteration 50, loss = 0.19020582\n",
      "Iteration 113, loss = 0.08622755\n",
      "Iteration 51, loss = 0.18524297\n",
      "Iteration 114, loss = 0.08370562\n",
      "Iteration 52, loss = 0.18362094\n",
      "Iteration 115, loss = 0.08424644\n",
      "Iteration 116, loss = 0.08221215\n",
      "Iteration 53, loss = 0.18204178\n",
      "Iteration 117, loss = 0.08217596\n",
      "Iteration 54, loss = 0.17900997\n",
      "Iteration 118, loss = 0.08087182\n",
      "Iteration 55, loss = 0.17859501\n",
      "Iteration 56, loss = 0.17308767\n",
      "Iteration 119, loss = 0.08062983\n",
      "Iteration 120, loss = 0.07969075\n",
      "Iteration 57, loss = 0.17334966\n",
      "Iteration 58, loss = 0.17113045\n",
      "Iteration 121, loss = 0.07785421\n",
      "Iteration 59, loss = 0.16665485\n",
      "Iteration 122, loss = 0.07649047\n",
      "Iteration 123, loss = 0.07510673\n",
      "Iteration 60, loss = 0.16838262\n",
      "Iteration 61, loss = 0.16220065\n",
      "Iteration 124, loss = 0.07319753\n",
      "Iteration 62, loss = 0.16443773\n",
      "Iteration 125, loss = 0.07326197\n",
      "Iteration 63, loss = 0.16040195\n",
      "Iteration 126, loss = 0.07229272\n",
      "Iteration 64, loss = 0.15649760\n",
      "Iteration 127, loss = 0.07329057\n",
      "Iteration 65, loss = 0.15533019\n",
      "Iteration 128, loss = 0.07401883\n",
      "Iteration 66, loss = 0.15272052\n",
      "Iteration 129, loss = 0.06827103\n",
      "Iteration 67, loss = 0.15178467\n",
      "Iteration 130, loss = 0.07022110\n",
      "Iteration 68, loss = 0.14866446\n",
      "Iteration 131, loss = 0.06856883\n",
      "Iteration 69, loss = 0.14825579\n",
      "Iteration 132, loss = 0.06784833\n",
      "Iteration 70, loss = 0.14577182\n",
      "Iteration 133, loss = 0.06416167\n",
      "Iteration 71, loss = 0.14506535\n",
      "Iteration 134, loss = 0.06619199\n",
      "Iteration 72, loss = 0.14258254\n",
      "Iteration 135, loss = 0.06535541\n",
      "Iteration 73, loss = 0.14100957\n",
      "Iteration 136, loss = 0.06415317\n",
      "Iteration 74, loss = 0.13977291\n",
      "Iteration 137, loss = 0.06310012\n",
      "Iteration 75, loss = 0.13829166\n",
      "Iteration 138, loss = 0.06207782\n",
      "Iteration 76, loss = 0.13786076\n",
      "Iteration 139, loss = 0.06350425\n",
      "Iteration 77, loss = 0.13518132\n",
      "Iteration 140, loss = 0.06331513\n",
      "Iteration 78, loss = 0.13496977\n",
      "Iteration 141, loss = 0.06107915\n",
      "Iteration 79, loss = 0.13173532\n",
      "Iteration 142, loss = 0.05851115\n",
      "Iteration 80, loss = 0.13143085\n",
      "Iteration 143, loss = 0.05909043\n",
      "Iteration 81, loss = 0.13073524\n",
      "Iteration 144, loss = 0.05869242\n",
      "Iteration 82, loss = 0.12835506\n",
      "Iteration 145, loss = 0.05639832\n",
      "Iteration 83, loss = 0.12718935\n",
      "Iteration 146, loss = 0.05625531\n",
      "Iteration 84, loss = 0.12630480\n",
      "Iteration 147, loss = 0.05535732\n",
      "Iteration 85, loss = 0.12590128\n",
      "Iteration 148, loss = 0.05367405\n",
      "Iteration 86, loss = 0.12359077\n",
      "Iteration 149, loss = 0.05321978\n",
      "Iteration 87, loss = 0.12364584\n",
      "Iteration 150, loss = 0.05360132\n",
      "Iteration 88, loss = 0.12533538\n",
      "Iteration 151, loss = 0.05184408\n",
      "Iteration 89, loss = 0.12218206\n",
      "Iteration 152, loss = 0.05173674\n",
      "Iteration 90, loss = 0.11952323\n",
      "Iteration 153, loss = 0.05340420\n",
      "Iteration 91, loss = 0.11868189\n",
      "Iteration 154, loss = 0.05093517\n",
      "Iteration 92, loss = 0.11730743\n",
      "Iteration 155, loss = 0.05127921\n",
      "Iteration 93, loss = 0.11628052\n",
      "Iteration 156, loss = 0.05043007\n",
      "Iteration 94, loss = 0.11680664\n",
      "Iteration 157, loss = 0.04880281\n",
      "Iteration 95, loss = 0.12001609\n",
      "Iteration 158, loss = 0.04958275\n",
      "Iteration 96, loss = 0.11407139\n",
      "Iteration 159, loss = 0.04725195\n",
      "Iteration 97, loss = 0.11385476\n",
      "Iteration 160, loss = 0.04810997\n",
      "Iteration 98, loss = 0.11242609\n",
      "Iteration 161, loss = 0.04649518\n",
      "Iteration 99, loss = 0.11139797\n",
      "Iteration 162, loss = 0.04526749\n",
      "Iteration 100, loss = 0.10884389\n",
      "Iteration 163, loss = 0.04411427\n",
      "Iteration 101, loss = 0.10811841\n",
      "Iteration 164, loss = 0.04345663\n",
      "Iteration 102, loss = 0.10689721\n",
      "Iteration 165, loss = 0.04393581\n",
      "Iteration 103, loss = 0.10615588\n",
      "Iteration 166, loss = 0.04290622\n",
      "Iteration 104, loss = 0.10714155\n",
      "Iteration 167, loss = 0.04257041\n",
      "Iteration 105, loss = 0.10585404\n",
      "Iteration 168, loss = 0.04371456\n",
      "Iteration 106, loss = 0.10421568\n",
      "Iteration 169, loss = 0.04183551\n",
      "Iteration 107, loss = 0.10289576\n",
      "Iteration 170, loss = 0.04396218\n",
      "Iteration 108, loss = 0.10193704\n",
      "Iteration 171, loss = 0.04640713\n",
      "Iteration 109, loss = 0.10042954\n",
      "Iteration 172, loss = 0.04251492\n",
      "Iteration 110, loss = 0.09966374\n",
      "Iteration 173, loss = 0.03971352\n",
      "Iteration 111, loss = 0.09898291\n",
      "Iteration 174, loss = 0.03878554\n",
      "Iteration 112, loss = 0.09962702\n",
      "Iteration 175, loss = 0.04021375\n",
      "Iteration 113, loss = 0.09837022\n",
      "Iteration 176, loss = 0.04062381\n",
      "Iteration 114, loss = 0.09768849\n",
      "Iteration 177, loss = 0.03809663\n",
      "Iteration 115, loss = 0.09557322\n",
      "Iteration 178, loss = 0.03695697\n",
      "Iteration 116, loss = 0.09590372\n",
      "Iteration 179, loss = 0.03676764\n",
      "Iteration 117, loss = 0.09388623\n",
      "Iteration 180, loss = 0.03719274\n",
      "Iteration 118, loss = 0.09480373\n",
      "Iteration 181, loss = 0.03844128\n",
      "Iteration 119, loss = 0.09162849\n",
      "Iteration 182, loss = 0.03572805\n",
      "Iteration 120, loss = 0.09231421\n",
      "Iteration 183, loss = 0.03515146\n",
      "Iteration 121, loss = 0.09382070\n",
      "Iteration 184, loss = 0.03503054\n",
      "Iteration 122, loss = 0.09474645\n",
      "Iteration 185, loss = 0.03492882\n",
      "Iteration 123, loss = 0.09145111\n",
      "Iteration 186, loss = 0.03423506\n",
      "Iteration 124, loss = 0.09229912\n",
      "Iteration 187, loss = 0.03707394\n",
      "Iteration 125, loss = 0.09060801\n",
      "Iteration 188, loss = 0.03238333\n",
      "Iteration 126, loss = 0.08629833\n",
      "Iteration 189, loss = 0.03397231\n",
      "Iteration 127, loss = 0.08787283\n",
      "Iteration 190, loss = 0.03344381\n",
      "Iteration 128, loss = 0.08752303\n",
      "Iteration 191, loss = 0.03383710\n",
      "Iteration 129, loss = 0.08491573\n",
      "Iteration 192, loss = 0.03437293\n",
      "Iteration 130, loss = 0.08381244\n",
      "Iteration 193, loss = 0.03384616\n",
      "Iteration 131, loss = 0.08353286\n",
      "Iteration 194, loss = 0.03182881\n",
      "Iteration 132, loss = 0.08186646\n",
      "Iteration 195, loss = 0.03255858\n",
      "Iteration 133, loss = 0.08295973\n",
      "Iteration 196, loss = 0.03239084\n",
      "Iteration 134, loss = 0.08072993\n",
      "Iteration 197, loss = 0.03155703\n",
      "Iteration 135, loss = 0.08071560\n",
      "Iteration 198, loss = 0.03020843\n",
      "Iteration 136, loss = 0.08133108\n",
      "Iteration 199, loss = 0.02992541\n",
      "Iteration 137, loss = 0.07943383\n",
      "Iteration 200, loss = 0.03245192\n",
      "Iteration 138, loss = 0.07819227\n",
      "Iteration 201, loss = 0.02916200\n",
      "Iteration 139, loss = 0.08018515\n",
      "Iteration 202, loss = 0.02934957\n",
      "Iteration 203, loss = 0.02744413\n",
      "Iteration 140, loss = 0.07734480\n",
      "Iteration 204, loss = 0.02829632\n",
      "Iteration 141, loss = 0.07601413\n",
      "Iteration 205, loss = 0.02742342\n",
      "Iteration 142, loss = 0.07598837\n",
      "Iteration 143, loss = 0.07604953\n",
      "Iteration 206, loss = 0.02812194\n",
      "Iteration 207, loss = 0.02657753\n",
      "Iteration 144, loss = 0.07506571\n",
      "Iteration 208, loss = 0.02664290\n",
      "Iteration 145, loss = 0.07569727\n",
      "Iteration 209, loss = 0.02628703\n",
      "Iteration 146, loss = 0.07515486\n",
      "Iteration 147, loss = 0.07092344\n",
      "Iteration 210, loss = 0.02584590\n",
      "Iteration 148, loss = 0.07457588\n",
      "Iteration 211, loss = 0.02608470\n",
      "Iteration 149, loss = 0.07288719\n",
      "Iteration 212, loss = 0.02573872\n",
      "Iteration 150, loss = 0.07238637\n",
      "Iteration 213, loss = 0.02492569\n",
      "Iteration 151, loss = 0.07187652\n",
      "Iteration 214, loss = 0.02562134\n",
      "Iteration 152, loss = 0.07043775\n",
      "Iteration 215, loss = 0.02531950\n",
      "Iteration 153, loss = 0.06970792\n",
      "Iteration 216, loss = 0.02833759\n",
      "Iteration 154, loss = 0.06898161\n",
      "Iteration 217, loss = 0.02501457\n",
      "Iteration 155, loss = 0.07045159\n",
      "Iteration 218, loss = 0.02556642\n",
      "Iteration 156, loss = 0.07341867\n",
      "Iteration 219, loss = 0.02308435\n",
      "Iteration 157, loss = 0.07194330\n",
      "Iteration 220, loss = 0.02353110\n",
      "Iteration 158, loss = 0.06709793\n",
      "Iteration 221, loss = 0.02335207\n",
      "Iteration 159, loss = 0.06754487\n",
      "Iteration 222, loss = 0.02283724\n",
      "Iteration 160, loss = 0.06606870\n",
      "Iteration 223, loss = 0.02507714\n",
      "Iteration 161, loss = 0.06633553\n",
      "Iteration 224, loss = 0.02609690\n",
      "Iteration 162, loss = 0.06472318\n",
      "Iteration 225, loss = 0.02404626\n",
      "Iteration 163, loss = 0.06550148\n",
      "Iteration 226, loss = 0.02220789\n",
      "Iteration 164, loss = 0.06272143\n",
      "Iteration 227, loss = 0.02212403\n",
      "Iteration 165, loss = 0.06354980\n",
      "Iteration 228, loss = 0.02129489\n",
      "Iteration 166, loss = 0.06278161\n",
      "Iteration 229, loss = 0.02207047\n",
      "Iteration 167, loss = 0.06071083\n",
      "Iteration 230, loss = 0.02279672\n",
      "Iteration 168, loss = 0.06227420\n",
      "Iteration 231, loss = 0.02219221\n",
      "Iteration 169, loss = 0.06130898\n",
      "Iteration 232, loss = 0.02228522\n",
      "Iteration 170, loss = 0.06076395\n",
      "Iteration 233, loss = 0.02061080\n",
      "Iteration 171, loss = 0.06007353\n",
      "Iteration 234, loss = 0.02025271\n",
      "Iteration 172, loss = 0.05839378\n",
      "Iteration 235, loss = 0.02000657\n",
      "Iteration 173, loss = 0.05919788\n",
      "Iteration 236, loss = 0.02026944\n",
      "Iteration 174, loss = 0.05881857\n",
      "Iteration 237, loss = 0.02080949\n",
      "Iteration 175, loss = 0.06188177\n",
      "Iteration 238, loss = 0.01939140\n",
      "Iteration 176, loss = 0.06234535\n",
      "Iteration 239, loss = 0.01933140\n",
      "Iteration 177, loss = 0.05924472\n",
      "Iteration 240, loss = 0.01957164\n",
      "Iteration 178, loss = 0.06005874\n",
      "Iteration 241, loss = 0.02152536\n",
      "Iteration 179, loss = 0.05868368\n",
      "Iteration 242, loss = 0.01991271\n",
      "Iteration 180, loss = 0.05829253\n",
      "Iteration 243, loss = 0.01915896\n",
      "Iteration 181, loss = 0.05675724\n",
      "Iteration 244, loss = 0.02020039\n",
      "Iteration 182, loss = 0.05888054\n",
      "Iteration 245, loss = 0.02134849\n",
      "Iteration 183, loss = 0.05723038\n",
      "Iteration 246, loss = 0.01973313\n",
      "Iteration 184, loss = 0.05421111\n",
      "Iteration 247, loss = 0.01876639\n",
      "Iteration 185, loss = 0.05249023\n",
      "Iteration 248, loss = 0.01889861\n",
      "Iteration 186, loss = 0.05231396\n",
      "Iteration 249, loss = 0.01815961\n",
      "Iteration 187, loss = 0.05337229\n",
      "Iteration 250, loss = 0.01746165\n",
      "Iteration 188, loss = 0.05212016\n",
      "Iteration 251, loss = 0.01860359\n",
      "Iteration 189, loss = 0.05142122\n",
      "Iteration 252, loss = 0.01718726\n",
      "Iteration 190, loss = 0.05053135\n",
      "Iteration 253, loss = 0.01829300\n",
      "Iteration 191, loss = 0.05004190\n",
      "Iteration 254, loss = 0.01677471\n",
      "Iteration 192, loss = 0.05025433\n",
      "Iteration 255, loss = 0.01741012\n",
      "Iteration 193, loss = 0.05177344\n",
      "Iteration 256, loss = 0.01656797\n",
      "Iteration 194, loss = 0.05173546\n",
      "Iteration 257, loss = 0.01627532\n",
      "Iteration 195, loss = 0.04866889\n",
      "Iteration 258, loss = 0.01651177\n",
      "Iteration 196, loss = 0.04869829\n",
      "Iteration 259, loss = 0.01604651\n",
      "Iteration 197, loss = 0.04840402\n",
      "Iteration 260, loss = 0.01580774\n",
      "Iteration 198, loss = 0.05025404\n",
      "Iteration 261, loss = 0.01632786\n",
      "Iteration 199, loss = 0.05131720\n",
      "Iteration 262, loss = 0.01576432\n",
      "Iteration 200, loss = 0.04805469\n",
      "Iteration 263, loss = 0.01545732\n",
      "Iteration 201, loss = 0.04855129\n",
      "Iteration 264, loss = 0.01538132\n",
      "Iteration 202, loss = 0.04685360\n",
      "Iteration 265, loss = 0.01553996\n",
      "Iteration 203, loss = 0.04633034\n",
      "Iteration 266, loss = 0.01692295\n",
      "Iteration 204, loss = 0.04678596\n",
      "Iteration 267, loss = 0.01522927\n",
      "Iteration 205, loss = 0.05185528\n",
      "Iteration 268, loss = 0.01475338\n",
      "Iteration 206, loss = 0.05185231\n",
      "Iteration 269, loss = 0.01537453\n",
      "Iteration 207, loss = 0.05414570\n",
      "Iteration 270, loss = 0.01439026\n",
      "Iteration 208, loss = 0.04784118\n",
      "Iteration 271, loss = 0.01461488\n",
      "Iteration 209, loss = 0.04402065\n",
      "Iteration 272, loss = 0.01454554\n",
      "Iteration 210, loss = 0.04869473\n",
      "Iteration 273, loss = 0.01410288\n",
      "Iteration 211, loss = 0.04541243\n",
      "Iteration 274, loss = 0.01444501\n",
      "Iteration 212, loss = 0.04435438\n",
      "Iteration 275, loss = 0.01403218\n",
      "Iteration 213, loss = 0.04400622\n",
      "Iteration 276, loss = 0.01406544\n",
      "Iteration 214, loss = 0.04245394\n",
      "Iteration 277, loss = 0.01421222\n",
      "Iteration 215, loss = 0.04168877\n",
      "Iteration 278, loss = 0.01364234\n",
      "Iteration 216, loss = 0.04179631\n",
      "Iteration 279, loss = 0.01515310\n",
      "Iteration 217, loss = 0.04195459\n",
      "Iteration 280, loss = 0.01443245\n",
      "Iteration 218, loss = 0.04133591\n",
      "Iteration 281, loss = 0.01483076\n",
      "Iteration 219, loss = 0.04112588\n",
      "Iteration 282, loss = 0.01408530\n",
      "Iteration 220, loss = 0.04115944\n",
      "Iteration 283, loss = 0.01363554\n",
      "Iteration 221, loss = 0.04157224\n",
      "Iteration 284, loss = 0.01365576\n",
      "Iteration 222, loss = 0.04203363\n",
      "Iteration 285, loss = 0.01313410\n",
      "Iteration 223, loss = 0.04365045\n",
      "Iteration 286, loss = 0.01294926\n",
      "Iteration 224, loss = 0.04430524\n",
      "Iteration 287, loss = 0.01256886\n",
      "Iteration 225, loss = 0.04190178\n",
      "Iteration 288, loss = 0.01262707\n",
      "Iteration 226, loss = 0.03793113\n",
      "Iteration 289, loss = 0.01289068\n",
      "Iteration 227, loss = 0.04113350\n",
      "Iteration 290, loss = 0.01276878\n",
      "Iteration 228, loss = 0.04050969\n",
      "Iteration 291, loss = 0.01309962\n",
      "Iteration 229, loss = 0.03876747\n",
      "Iteration 292, loss = 0.01258077\n",
      "Iteration 230, loss = 0.03806188\n",
      "Iteration 293, loss = 0.01282652\n",
      "Iteration 231, loss = 0.03861697\n",
      "Iteration 294, loss = 0.01193769\n",
      "Iteration 232, loss = 0.04081156\n",
      "Iteration 295, loss = 0.01179941\n",
      "Iteration 233, loss = 0.04081395\n",
      "Iteration 296, loss = 0.01155486\n",
      "Iteration 234, loss = 0.03910844\n",
      "Iteration 297, loss = 0.01179214\n",
      "Iteration 235, loss = 0.03966186\n",
      "Iteration 298, loss = 0.01151997\n",
      "Iteration 236, loss = 0.03784179\n",
      "Iteration 299, loss = 0.01225291\n",
      "Iteration 237, loss = 0.03870122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 300, loss = 0.01135157\n",
      "Iteration 301, loss = 0.01148926\n",
      "Iteration 302, loss = 0.01110017\n",
      "Iteration 303, loss = 0.01102769\n",
      "Iteration 304, loss = 0.01154942\n",
      "Iteration 305, loss = 0.01215753\n",
      "Iteration 306, loss = 0.01158940\n",
      "Iteration 307, loss = 0.01117430\n",
      "Iteration 308, loss = 0.01117673\n",
      "Iteration 309, loss = 0.01179994\n",
      "Iteration 310, loss = 0.01221419\n",
      "Iteration 311, loss = 0.01088419\n",
      "Iteration 312, loss = 0.01037373\n",
      "Iteration 1, loss = 0.99589684\n",
      "Iteration 2, loss = 0.77992680\n",
      "Iteration 313, loss = 0.01026147\n",
      "Iteration 314, loss = 0.01022488\n",
      "Iteration 3, loss = 0.64731979\n",
      "Iteration 315, loss = 0.01021040\n",
      "Iteration 4, loss = 0.56695137\n",
      "Iteration 316, loss = 0.00994127\n",
      "Iteration 5, loss = 0.51631837\n",
      "Iteration 317, loss = 0.01048396\n",
      "Iteration 6, loss = 0.47399725\n",
      "Iteration 318, loss = 0.01012689\n",
      "Iteration 7, loss = 0.44604147\n",
      "Iteration 319, loss = 0.01043229\n",
      "Iteration 8, loss = 0.42321600\n",
      "Iteration 320, loss = 0.01033130\n",
      "Iteration 9, loss = 0.40516047\n",
      "Iteration 10, loss = 0.39014010\n",
      "Iteration 321, loss = 0.01101149\n",
      "Iteration 11, loss = 0.37667399\n",
      "Iteration 322, loss = 0.00979748\n",
      "Iteration 12, loss = 0.36528858\n",
      "Iteration 323, loss = 0.00984540\n",
      "Iteration 324, loss = 0.00942921\n",
      "Iteration 13, loss = 0.35524330\n",
      "Iteration 325, loss = 0.00935819\n",
      "Iteration 14, loss = 0.34679965\n",
      "Iteration 326, loss = 0.01002122Iteration 15, loss = 0.33653999\n",
      "\n",
      "Iteration 16, loss = 0.32805662\n",
      "Iteration 327, loss = 0.00909749\n",
      "Iteration 17, loss = 0.32133158\n",
      "Iteration 328, loss = 0.01003428\n",
      "Iteration 18, loss = 0.31340515\n",
      "Iteration 329, loss = 0.00925876\n",
      "Iteration 19, loss = 0.30614447\n",
      "Iteration 330, loss = 0.00917733\n",
      "Iteration 331, loss = 0.00923954\n",
      "Iteration 20, loss = 0.29964221\n",
      "Iteration 332, loss = 0.01116408\n",
      "Iteration 21, loss = 0.29264501\n",
      "Iteration 333, loss = 0.01168376\n",
      "Iteration 22, loss = 0.28645126\n",
      "Iteration 334, loss = 0.01174879\n",
      "Iteration 23, loss = 0.28131761\n",
      "Iteration 24, loss = 0.27478769\n",
      "Iteration 335, loss = 0.00994234\n",
      "Iteration 25, loss = 0.27175166\n",
      "Iteration 336, loss = 0.00948453\n",
      "Iteration 26, loss = 0.26443731\n",
      "Iteration 337, loss = 0.00869293\n",
      "Iteration 27, loss = 0.25876852\n",
      "Iteration 338, loss = 0.00907857\n",
      "Iteration 28, loss = 0.25357972\n",
      "Iteration 339, loss = 0.01041239\n",
      "Iteration 29, loss = 0.24747701\n",
      "Iteration 340, loss = 0.00924653\n",
      "Iteration 30, loss = 0.24425301\n",
      "Iteration 341, loss = 0.00905397\n",
      "Iteration 31, loss = 0.23874166\n",
      "Iteration 342, loss = 0.00872803\n",
      "Iteration 32, loss = 0.23499547\n",
      "Iteration 343, loss = 0.00938110\n",
      "Iteration 33, loss = 0.22958882\n",
      "Iteration 344, loss = 0.00919584\n",
      "Iteration 34, loss = 0.22539603\n",
      "Iteration 345, loss = 0.00819317\n",
      "Iteration 35, loss = 0.22068926\n",
      "Iteration 346, loss = 0.00971672\n",
      "Iteration 36, loss = 0.21706641\n",
      "Iteration 347, loss = 0.01171247\n",
      "Iteration 37, loss = 0.21391489\n",
      "Iteration 348, loss = 0.01641884\n",
      "Iteration 38, loss = 0.20967753\n",
      "Iteration 349, loss = 0.01612519\n",
      "Iteration 39, loss = 0.20529141\n",
      "Iteration 350, loss = 0.01304215\n",
      "Iteration 40, loss = 0.20276847\n",
      "Iteration 351, loss = 0.00868169\n",
      "Iteration 41, loss = 0.20030351\n",
      "Iteration 352, loss = 0.00824373\n",
      "Iteration 42, loss = 0.19488836\n",
      "Iteration 353, loss = 0.00921582\n",
      "Iteration 43, loss = 0.19286402\n",
      "Iteration 354, loss = 0.00821806\n",
      "Iteration 44, loss = 0.18949679\n",
      "Iteration 355, loss = 0.00794548\n",
      "Iteration 45, loss = 0.18514930\n",
      "Iteration 356, loss = 0.00745991\n",
      "Iteration 46, loss = 0.18214007\n",
      "Iteration 357, loss = 0.00872235\n",
      "Iteration 47, loss = 0.17843030\n",
      "Iteration 358, loss = 0.00813453\n",
      "Iteration 48, loss = 0.17667579\n",
      "Iteration 359, loss = 0.00731735\n",
      "Iteration 49, loss = 0.17510460\n",
      "Iteration 360, loss = 0.00756242\n",
      "Iteration 50, loss = 0.17140042\n",
      "Iteration 361, loss = 0.00774159\n",
      "Iteration 51, loss = 0.16739342\n",
      "Iteration 362, loss = 0.00715315\n",
      "Iteration 52, loss = 0.16444621\n",
      "Iteration 363, loss = 0.00711601\n",
      "Iteration 53, loss = 0.16182946\n",
      "Iteration 364, loss = 0.00774460\n",
      "Iteration 54, loss = 0.16015535\n",
      "Iteration 365, loss = 0.00720635\n",
      "Iteration 55, loss = 0.16077316\n",
      "Iteration 366, loss = 0.00766507\n",
      "Iteration 56, loss = 0.15637863\n",
      "Iteration 367, loss = 0.00736449\n",
      "Iteration 57, loss = 0.15277346\n",
      "Iteration 368, loss = 0.00783158\n",
      "Iteration 58, loss = 0.15008741\n",
      "Iteration 369, loss = 0.00684467\n",
      "Iteration 59, loss = 0.14611885\n",
      "Iteration 370, loss = 0.00726779\n",
      "Iteration 60, loss = 0.14495078\n",
      "Iteration 371, loss = 0.00663985\n",
      "Iteration 61, loss = 0.14286007\n",
      "Iteration 372, loss = 0.00702223\n",
      "Iteration 62, loss = 0.14019921\n",
      "Iteration 373, loss = 0.00675811\n",
      "Iteration 63, loss = 0.13903121\n",
      "Iteration 374, loss = 0.00668734\n",
      "Iteration 64, loss = 0.13575379\n",
      "Iteration 375, loss = 0.00664804\n",
      "Iteration 65, loss = 0.13526645\n",
      "Iteration 376, loss = 0.00695138\n",
      "Iteration 66, loss = 0.13341023\n",
      "Iteration 377, loss = 0.00703669\n",
      "Iteration 67, loss = 0.13066087\n",
      "Iteration 378, loss = 0.00683880\n",
      "Iteration 68, loss = 0.12881905\n",
      "Iteration 379, loss = 0.00691271\n",
      "Iteration 69, loss = 0.12869350\n",
      "Iteration 380, loss = 0.00678522\n",
      "Iteration 70, loss = 0.12632988\n",
      "Iteration 381, loss = 0.00642232\n",
      "Iteration 71, loss = 0.12409390\n",
      "Iteration 382, loss = 0.00716311\n",
      "Iteration 72, loss = 0.12285367\n",
      "Iteration 383, loss = 0.00654671\n",
      "Iteration 73, loss = 0.12038783\n",
      "Iteration 384, loss = 0.00653805\n",
      "Iteration 74, loss = 0.11794928\n",
      "Iteration 385, loss = 0.00666010\n",
      "Iteration 75, loss = 0.11759009\n",
      "Iteration 386, loss = 0.00608042\n",
      "Iteration 76, loss = 0.11946048\n",
      "Iteration 387, loss = 0.00612982\n",
      "Iteration 77, loss = 0.11775414\n",
      "Iteration 388, loss = 0.00615419\n",
      "Iteration 78, loss = 0.11336124\n",
      "Iteration 389, loss = 0.00615237\n",
      "Iteration 79, loss = 0.11300592\n",
      "Iteration 80, loss = 0.11414641\n",
      "Iteration 390, loss = 0.00624856\n",
      "Iteration 81, loss = 0.11086696\n",
      "Iteration 391, loss = 0.00620219\n",
      "Iteration 82, loss = 0.10687194\n",
      "Iteration 392, loss = 0.00619530\n",
      "Iteration 83, loss = 0.10777597\n",
      "Iteration 393, loss = 0.00587778\n",
      "Iteration 84, loss = 0.10788798\n",
      "Iteration 394, loss = 0.00585956\n",
      "Iteration 85, loss = 0.11046442\n",
      "Iteration 395, loss = 0.00575044\n",
      "Iteration 86, loss = 0.10878119\n",
      "Iteration 396, loss = 0.00591522\n",
      "Iteration 87, loss = 0.09879105\n",
      "Iteration 397, loss = 0.00565563\n",
      "Iteration 88, loss = 0.10220243\n",
      "Iteration 398, loss = 0.00575672\n",
      "Iteration 89, loss = 0.09995359\n",
      "Iteration 399, loss = 0.00584844\n",
      "Iteration 90, loss = 0.10209995\n",
      "Iteration 400, loss = 0.00552722\n",
      "Iteration 91, loss = 0.10049876\n",
      "Iteration 401, loss = 0.00562781\n",
      "Iteration 92, loss = 0.09893275\n",
      "Iteration 402, loss = 0.00548198\n",
      "Iteration 93, loss = 0.09655525\n",
      "Iteration 403, loss = 0.00570640\n",
      "Iteration 94, loss = 0.09420600\n",
      "Iteration 404, loss = 0.00610741\n",
      "Iteration 95, loss = 0.09319313\n",
      "Iteration 405, loss = 0.00581690\n",
      "Iteration 406, loss = 0.00581517\n",
      "Iteration 96, loss = 0.09208891\n",
      "Iteration 97, loss = 0.09256212\n",
      "Iteration 407, loss = 0.00600910\n",
      "Iteration 98, loss = 0.08999192\n",
      "Iteration 408, loss = 0.00556921\n",
      "Iteration 99, loss = 0.09067254\n",
      "Iteration 409, loss = 0.00631481\n",
      "Iteration 100, loss = 0.08856128\n",
      "Iteration 410, loss = 0.00641858\n",
      "Iteration 101, loss = 0.08859569\n",
      "Iteration 411, loss = 0.00752628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 102, loss = 0.08643700\n",
      "Iteration 103, loss = 0.08301635\n",
      "Iteration 104, loss = 0.08359372\n",
      "Iteration 105, loss = 0.08405949\n",
      "Iteration 106, loss = 0.08205469\n",
      "Iteration 107, loss = 0.08010862\n",
      "Iteration 108, loss = 0.08003801\n",
      "Iteration 109, loss = 0.07752705\n",
      "Iteration 110, loss = 0.07874390\n",
      "Iteration 111, loss = 0.07736416\n",
      "Iteration 112, loss = 0.07746563\n",
      "Iteration 113, loss = 0.07460011\n",
      "Iteration 114, loss = 0.07458108\n",
      "Iteration 1, loss = 16.53615043\n",
      "Iteration 115, loss = 0.07441365\n",
      "Iteration 2, loss = 13.98815879\n",
      "Iteration 116, loss = 0.07198103\n",
      "Iteration 3, loss = 10.42246449\n",
      "Iteration 117, loss = 0.07391928\n",
      "Iteration 4, loss = 10.47012573\n",
      "Iteration 118, loss = 0.07185246\n",
      "Iteration 5, loss = 6.47262834\n",
      "Iteration 119, loss = 0.06963428\n",
      "Iteration 6, loss = 5.41491687\n",
      "Iteration 120, loss = 0.06938694\n",
      "Iteration 7, loss = 4.72109144\n",
      "Iteration 121, loss = 0.06846627\n",
      "Iteration 8, loss = 4.84429282\n",
      "Iteration 122, loss = 0.06814833\n",
      "Iteration 9, loss = 4.59136708\n",
      "Iteration 123, loss = 0.06691674\n",
      "Iteration 10, loss = 10.16776718\n",
      "Iteration 124, loss = 0.06772247\n",
      "Iteration 11, loss = 6.54217943\n",
      "Iteration 125, loss = 0.06488302\n",
      "Iteration 12, loss = 6.29162407\n",
      "Iteration 126, loss = 0.06497957\n",
      "Iteration 13, loss = 5.42036928\n",
      "Iteration 127, loss = 0.06402705\n",
      "Iteration 14, loss = 4.11737629\n",
      "Iteration 128, loss = 0.06314413\n",
      "Iteration 15, loss = 3.70865337\n",
      "Iteration 129, loss = 0.06184523\n",
      "Iteration 16, loss = 3.74599110\n",
      "Iteration 130, loss = 0.06268626\n",
      "Iteration 17, loss = 3.94450039\n",
      "Iteration 131, loss = 0.06181174\n",
      "Iteration 18, loss = 3.70114179\n",
      "Iteration 132, loss = 0.06080516\n",
      "Iteration 19, loss = 4.88103881\n",
      "Iteration 133, loss = 0.05974586\n",
      "Iteration 20, loss = 3.19003486\n",
      "Iteration 134, loss = 0.05956767\n",
      "Iteration 21, loss = 2.78302255\n",
      "Iteration 135, loss = 0.06141496\n",
      "Iteration 22, loss = 2.52217310\n",
      "Iteration 136, loss = 0.06088730\n",
      "Iteration 23, loss = 2.41448764\n",
      "Iteration 137, loss = 0.05897202\n",
      "Iteration 24, loss = 1.81119050\n",
      "Iteration 138, loss = 0.05851151\n",
      "Iteration 25, loss = 2.39054798\n",
      "Iteration 139, loss = 0.05924228\n",
      "Iteration 26, loss = 2.03199678\n",
      "Iteration 140, loss = 0.05765818\n",
      "Iteration 27, loss = 2.01307128\n",
      "Iteration 141, loss = 0.05527976\n",
      "Iteration 28, loss = 1.33053038\n",
      "Iteration 142, loss = 0.05603116\n",
      "Iteration 29, loss = 1.56985786\n",
      "Iteration 143, loss = 0.05356377\n",
      "Iteration 30, loss = 2.33681827\n",
      "Iteration 144, loss = 0.05305296\n",
      "Iteration 31, loss = 2.38876758\n",
      "Iteration 145, loss = 0.05324967\n",
      "Iteration 32, loss = 2.37492512\n",
      "Iteration 146, loss = 0.05158194\n",
      "Iteration 33, loss = 2.06838104\n",
      "Iteration 147, loss = 0.05368715\n",
      "Iteration 34, loss = 1.84438852\n",
      "Iteration 148, loss = 0.05452390\n",
      "Iteration 35, loss = 1.88069214\n",
      "Iteration 149, loss = 0.05396297\n",
      "Iteration 36, loss = 1.64026246\n",
      "Iteration 150, loss = 0.05196329\n",
      "Iteration 37, loss = 2.14825937\n",
      "Iteration 151, loss = 0.04970314\n",
      "Iteration 38, loss = 1.75402475\n",
      "Iteration 152, loss = 0.04914250\n",
      "Iteration 39, loss = 1.98929055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 153, loss = 0.04917797\n",
      "Iteration 154, loss = 0.04776738\n",
      "Iteration 155, loss = 0.04990032\n",
      "Iteration 156, loss = 0.04800990\n",
      "Iteration 157, loss = 0.04725219\n",
      "Iteration 158, loss = 0.04742007\n",
      "Iteration 159, loss = 0.04698613\n",
      "Iteration 160, loss = 0.04622628\n",
      "Iteration 161, loss = 0.04737349\n",
      "Iteration 162, loss = 0.04590145\n",
      "Iteration 163, loss = 0.04654709\n",
      "Iteration 164, loss = 0.04515568\n",
      "Iteration 165, loss = 0.04292890\n",
      "Iteration 1, loss = 17.18236712\n",
      "Iteration 166, loss = 0.04214765\n",
      "Iteration 2, loss = 16.99118992\n",
      "Iteration 167, loss = 0.04174021\n",
      "Iteration 3, loss = 13.81449366\n",
      "Iteration 168, loss = 0.04233016\n",
      "Iteration 4, loss = 8.74030361\n",
      "Iteration 169, loss = 0.04152224\n",
      "Iteration 5, loss = 10.50049871\n",
      "Iteration 170, loss = 0.04225062\n",
      "Iteration 6, loss = 6.61061871\n",
      "Iteration 171, loss = 0.04170154\n",
      "Iteration 7, loss = 9.09435229\n",
      "Iteration 172, loss = 0.04038545\n",
      "Iteration 8, loss = 10.98424068\n",
      "Iteration 173, loss = 0.04077187\n",
      "Iteration 9, loss = 12.11984575\n",
      "Iteration 174, loss = 0.04415475\n",
      "Iteration 10, loss = 9.70326357\n",
      "Iteration 175, loss = 0.04498338\n",
      "Iteration 11, loss = 6.43353980\n",
      "Iteration 176, loss = 0.04339314\n",
      "Iteration 12, loss = 6.04566348\n",
      "Iteration 177, loss = 0.04327900\n",
      "Iteration 13, loss = 4.79224668\n",
      "Iteration 178, loss = 0.04515147\n",
      "Iteration 14, loss = 4.14508426\n",
      "Iteration 179, loss = 0.04406292\n",
      "Iteration 15, loss = 5.16995202\n",
      "Iteration 180, loss = 0.04411630\n",
      "Iteration 16, loss = 5.00607346\n",
      "Iteration 181, loss = 0.03985152\n",
      "Iteration 17, loss = 4.03845513\n",
      "Iteration 182, loss = 0.03980793\n",
      "Iteration 18, loss = 2.81399669\n",
      "Iteration 183, loss = 0.03771134\n",
      "Iteration 19, loss = 3.83210853\n",
      "Iteration 184, loss = 0.03629557\n",
      "Iteration 20, loss = 4.06432799\n",
      "Iteration 185, loss = 0.03744934\n",
      "Iteration 21, loss = 3.63114556\n",
      "Iteration 186, loss = 0.04182573\n",
      "Iteration 22, loss = 4.90037750\n",
      "Iteration 187, loss = 0.03998051\n",
      "Iteration 23, loss = 4.00181140\n",
      "Iteration 188, loss = 0.03719495\n",
      "Iteration 24, loss = 2.46228563\n",
      "Iteration 189, loss = 0.03488311\n",
      "Iteration 25, loss = 2.40234589\n",
      "Iteration 190, loss = 0.03446774\n",
      "Iteration 26, loss = 2.11816841\n",
      "Iteration 191, loss = 0.03841418\n",
      "Iteration 27, loss = 2.45065728\n",
      "Iteration 192, loss = 0.04206833\n",
      "Iteration 28, loss = 2.21804486\n",
      "Iteration 193, loss = 0.04382258\n",
      "Iteration 29, loss = 2.65538413\n",
      "Iteration 194, loss = 0.05314987\n",
      "Iteration 30, loss = 2.31639634\n",
      "Iteration 195, loss = 0.05226603\n",
      "Iteration 31, loss = 2.60695845\n",
      "Iteration 196, loss = 0.04492835\n",
      "Iteration 32, loss = 2.41023445\n",
      "Iteration 197, loss = 0.03731588\n",
      "Iteration 33, loss = 2.56024378\n",
      "Iteration 198, loss = 0.03984969\n",
      "Iteration 34, loss = 2.45642080\n",
      "Iteration 199, loss = 0.03360670\n",
      "Iteration 35, loss = 2.38905367\n",
      "Iteration 200, loss = 0.03475135\n",
      "Iteration 36, loss = 2.01170431\n",
      "Iteration 201, loss = 0.03451655\n",
      "Iteration 37, loss = 3.03205602\n",
      "Iteration 202, loss = 0.03157532\n",
      "Iteration 38, loss = 2.12582957\n",
      "Iteration 203, loss = 0.03460115\n",
      "Iteration 39, loss = 2.34385120\n",
      "Iteration 204, loss = 0.03154661\n",
      "Iteration 40, loss = 1.82247200\n",
      "Iteration 205, loss = 0.03106801\n",
      "Iteration 41, loss = 1.80098462\n",
      "Iteration 206, loss = 0.03083809\n",
      "Iteration 42, loss = 1.82920141\n",
      "Iteration 207, loss = 0.03038668\n",
      "Iteration 43, loss = 1.69329607\n",
      "Iteration 208, loss = 0.03342590\n",
      "Iteration 44, loss = 2.19792490\n",
      "Iteration 209, loss = 0.03194890\n",
      "Iteration 45, loss = 2.59248587\n",
      "Iteration 210, loss = 0.02915391\n",
      "Iteration 46, loss = 1.86281305\n",
      "Iteration 211, loss = 0.03115927\n",
      "Iteration 47, loss = 2.01482509\n",
      "Iteration 212, loss = 0.02829237\n",
      "Iteration 48, loss = 1.58177796\n",
      "Iteration 213, loss = 0.02959169\n",
      "Iteration 49, loss = 1.55246614\n",
      "Iteration 214, loss = 0.02973922\n",
      "Iteration 50, loss = 1.59344745\n",
      "Iteration 215, loss = 0.02909440\n",
      "Iteration 51, loss = 1.60705744\n",
      "Iteration 216, loss = 0.02901430\n",
      "Iteration 52, loss = 1.93289912\n",
      "Iteration 217, loss = 0.02856626\n",
      "Iteration 53, loss = 1.61869756\n",
      "Iteration 218, loss = 0.02744140\n",
      "Iteration 54, loss = 1.34212168\n",
      "Iteration 219, loss = 0.02683440\n",
      "Iteration 55, loss = 1.09059414\n",
      "Iteration 220, loss = 0.02776239\n",
      "Iteration 56, loss = 0.90548279\n",
      "Iteration 221, loss = 0.03358091\n",
      "Iteration 57, loss = 1.26720435\n",
      "Iteration 222, loss = 0.03631367\n",
      "Iteration 58, loss = 1.14828798\n",
      "Iteration 223, loss = 0.04004951\n",
      "Iteration 59, loss = 1.20138483\n",
      "Iteration 224, loss = 0.04108280\n",
      "Iteration 60, loss = 0.79744437\n",
      "Iteration 225, loss = 0.03308526\n",
      "Iteration 61, loss = 0.75697453\n",
      "Iteration 226, loss = 0.03315665\n",
      "Iteration 62, loss = 0.72823668\n",
      "Iteration 227, loss = 0.02833390\n",
      "Iteration 63, loss = 1.15365243\n",
      "Iteration 228, loss = 0.02411622\n",
      "Iteration 64, loss = 1.30607196\n",
      "Iteration 229, loss = 0.02818588\n",
      "Iteration 65, loss = 1.86720581\n",
      "Iteration 230, loss = 0.02752751\n",
      "Iteration 66, loss = 1.76616982\n",
      "Iteration 231, loss = 0.02642409\n",
      "Iteration 67, loss = 1.17005645\n",
      "Iteration 232, loss = 0.02629139\n",
      "Iteration 68, loss = 1.47298082\n",
      "Iteration 233, loss = 0.02757809\n",
      "Iteration 69, loss = 1.31266952\n",
      "Iteration 234, loss = 0.02511955\n",
      "Iteration 70, loss = 0.92625604\n",
      "Iteration 235, loss = 0.02581510\n",
      "Iteration 71, loss = 1.11923967\n",
      "Iteration 236, loss = 0.02310140\n",
      "Iteration 72, loss = 1.03483550\n",
      "Iteration 237, loss = 0.02458153\n",
      "Iteration 73, loss = 1.54010255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 238, loss = 0.02492972\n",
      "Iteration 239, loss = 0.02400105\n",
      "Iteration 240, loss = 0.02406336\n",
      "Iteration 241, loss = 0.02335558\n",
      "Iteration 242, loss = 0.02398958\n",
      "Iteration 243, loss = 0.02337624\n",
      "Iteration 244, loss = 0.02341166\n",
      "Iteration 245, loss = 0.02659046\n",
      "Iteration 246, loss = 0.02546725\n",
      "Iteration 247, loss = 0.02101847\n",
      "Iteration 248, loss = 0.02251768\n",
      "Iteration 249, loss = 0.02481158\n",
      "Iteration 250, loss = 0.02526481\n",
      "Iteration 1, loss = 19.27140435\n",
      "Iteration 251, loss = 0.02380142\n",
      "Iteration 2, loss = 16.05427507\n",
      "Iteration 252, loss = 0.02254543\n",
      "Iteration 3, loss = 11.48013787\n",
      "Iteration 253, loss = 0.02340202\n",
      "Iteration 4, loss = 7.16324943\n",
      "Iteration 254, loss = 0.02051437\n",
      "Iteration 5, loss = 6.97459267\n",
      "Iteration 255, loss = 0.02080082\n",
      "Iteration 6, loss = 5.06043524\n",
      "Iteration 256, loss = 0.02064927\n",
      "Iteration 7, loss = 6.63654005\n",
      "Iteration 257, loss = 0.02404048\n",
      "Iteration 8, loss = 7.36230377\n",
      "Iteration 258, loss = 0.02800595\n",
      "Iteration 9, loss = 6.03492790\n",
      "Iteration 259, loss = 0.02290949\n",
      "Iteration 10, loss = 6.50867450\n",
      "Iteration 260, loss = 0.02366817\n",
      "Iteration 11, loss = 4.61657003\n",
      "Iteration 261, loss = 0.02549962\n",
      "Iteration 12, loss = 3.99406195\n",
      "Iteration 262, loss = 0.02389853\n",
      "Iteration 13, loss = 3.60945749\n",
      "Iteration 263, loss = 0.02683074\n",
      "Iteration 14, loss = 4.00934736\n",
      "Iteration 264, loss = 0.02290431\n",
      "Iteration 15, loss = 3.58584475\n",
      "Iteration 265, loss = 0.02088168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 8.18982185\n",
      "Iteration 17, loss = 4.39927180\n",
      "Iteration 18, loss = 3.18064001\n",
      "Iteration 19, loss = 2.83579187\n",
      "Iteration 20, loss = 2.76787283\n",
      "Iteration 21, loss = 3.14328560\n",
      "Iteration 22, loss = 2.95624378\n",
      "Iteration 23, loss = 1.66796241\n",
      "Iteration 24, loss = 2.76937734\n",
      "Iteration 25, loss = 2.44033870\n",
      "Iteration 26, loss = 2.18459667\n",
      "Iteration 27, loss = 3.23520145\n",
      "Iteration 28, loss = 2.63023941\n",
      "Iteration 1, loss = 18.09241151\n",
      "Iteration 29, loss = 2.11222176\n",
      "Iteration 2, loss = 17.46215574\n",
      "Iteration 30, loss = 1.93425842\n",
      "Iteration 3, loss = 9.02724956\n",
      "Iteration 31, loss = 1.76160781\n",
      "Iteration 4, loss = 8.49272751\n",
      "Iteration 32, loss = 1.79400367\n",
      "Iteration 5, loss = 7.55804983\n",
      "Iteration 33, loss = 1.16355142\n",
      "Iteration 6, loss = 7.55426789\n",
      "Iteration 34, loss = 1.58228222\n",
      "Iteration 7, loss = 7.17780084\n",
      "Iteration 35, loss = 1.56894613\n",
      "Iteration 8, loss = 6.12415767\n",
      "Iteration 36, loss = 1.40319965\n",
      "Iteration 9, loss = 4.84871735\n",
      "Iteration 37, loss = 1.29273480\n",
      "Iteration 10, loss = 7.26238440\n",
      "Iteration 38, loss = 1.73106442\n",
      "Iteration 11, loss = 6.42090796\n",
      "Iteration 39, loss = 2.06923854\n",
      "Iteration 12, loss = 5.38231564\n",
      "Iteration 40, loss = 1.63992128\n",
      "Iteration 13, loss = 3.77804587\n",
      "Iteration 41, loss = 1.19571684\n",
      "Iteration 14, loss = 4.49735744\n",
      "Iteration 42, loss = 0.92871984\n",
      "Iteration 15, loss = 4.34396150\n",
      "Iteration 43, loss = 0.58877229\n",
      "Iteration 16, loss = 4.66482422\n",
      "Iteration 44, loss = 0.98574586\n",
      "Iteration 17, loss = 4.60038561\n",
      "Iteration 45, loss = 1.11912935\n",
      "Iteration 18, loss = 3.45773835\n",
      "Iteration 46, loss = 1.25135174\n",
      "Iteration 19, loss = 3.90834201\n",
      "Iteration 47, loss = 1.06235436\n",
      "Iteration 20, loss = 2.98598488\n",
      "Iteration 48, loss = 1.72389535\n",
      "Iteration 21, loss = 2.89186902\n",
      "Iteration 49, loss = 1.56873798\n",
      "Iteration 22, loss = 3.12977541\n",
      "Iteration 50, loss = 1.94179527\n",
      "Iteration 23, loss = 2.74227658\n",
      "Iteration 51, loss = 1.89016226\n",
      "Iteration 24, loss = 2.74948162\n",
      "Iteration 52, loss = 2.05002232\n",
      "Iteration 25, loss = 2.42223816\n",
      "Iteration 53, loss = 1.94680138\n",
      "Iteration 26, loss = 2.13172669\n",
      "Iteration 54, loss = 1.44214346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 2.31529582\n",
      "Iteration 28, loss = 2.22072220\n",
      "Iteration 29, loss = 2.00684783\n",
      "Iteration 30, loss = 1.75615132\n",
      "Iteration 31, loss = 2.03234242\n",
      "Iteration 32, loss = 1.93287323\n",
      "Iteration 33, loss = 1.42020551\n",
      "Iteration 34, loss = 1.65994667\n",
      "Iteration 35, loss = 1.56613080\n",
      "Iteration 36, loss = 1.51981838\n",
      "Iteration 37, loss = 1.67811512\n",
      "Iteration 38, loss = 1.74780671\n",
      "Iteration 39, loss = 1.46590541\n",
      "Iteration 1, loss = 19.02825598\n",
      "Iteration 40, loss = 1.74389434\n",
      "Iteration 2, loss = 17.20804981\n",
      "Iteration 41, loss = 1.91677145\n",
      "Iteration 3, loss = 12.00275079\n",
      "Iteration 42, loss = 1.81325749\n",
      "Iteration 4, loss = 9.09001886\n",
      "Iteration 43, loss = 2.30138252\n",
      "Iteration 5, loss = 8.27560205\n",
      "Iteration 44, loss = 1.32591055\n",
      "Iteration 6, loss = 11.28595436\n",
      "Iteration 45, loss = 1.60475294\n",
      "Iteration 7, loss = 5.04386150\n",
      "Iteration 46, loss = 1.33838091\n",
      "Iteration 8, loss = 5.43978839\n",
      "Iteration 47, loss = 1.60173720\n",
      "Iteration 9, loss = 5.38581383\n",
      "Iteration 48, loss = 1.43427693\n",
      "Iteration 10, loss = 3.64212482\n",
      "Iteration 49, loss = 1.17831255\n",
      "Iteration 11, loss = 3.83566892\n",
      "Iteration 50, loss = 1.58512163\n",
      "Iteration 12, loss = 4.50077401\n",
      "Iteration 51, loss = 1.75765132\n",
      "Iteration 13, loss = 6.25607715\n",
      "Iteration 52, loss = 1.73378627\n",
      "Iteration 14, loss = 6.07542827\n",
      "Iteration 53, loss = 1.12411332\n",
      "Iteration 15, loss = 7.30782136\n",
      "Iteration 54, loss = 1.27272404\n",
      "Iteration 16, loss = 3.60757509\n",
      "Iteration 55, loss = 1.33898189\n",
      "Iteration 17, loss = 3.27810536\n",
      "Iteration 56, loss = 1.31311754\n",
      "Iteration 18, loss = 2.79382392\n",
      "Iteration 57, loss = 3.76789319\n",
      "Iteration 19, loss = 3.15807788\n",
      "Iteration 58, loss = 2.53181726\n",
      "Iteration 20, loss = 3.56809191\n",
      "Iteration 59, loss = 1.94225723\n",
      "Iteration 21, loss = 3.33387094\n",
      "Iteration 60, loss = 2.19123073\n",
      "Iteration 22, loss = 3.17034466\n",
      "Iteration 61, loss = 2.43592865\n",
      "Iteration 23, loss = 2.31171299\n",
      "Iteration 62, loss = 2.69035577\n",
      "Iteration 24, loss = 1.96422244\n",
      "Iteration 63, loss = 2.86378283\n",
      "Iteration 25, loss = 2.74574012\n",
      "Iteration 64, loss = 3.12862871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 2.27823737\n",
      "Iteration 27, loss = 1.82323845\n",
      "Iteration 28, loss = 1.74784431\n",
      "Iteration 29, loss = 1.84829558\n",
      "Iteration 30, loss = 1.19732434\n",
      "Iteration 31, loss = 1.21463244\n",
      "Iteration 32, loss = 1.39348385\n",
      "Iteration 33, loss = 1.74256689\n",
      "Iteration 34, loss = 1.60911981\n",
      "Iteration 35, loss = 1.20578317\n",
      "Iteration 36, loss = 1.37485266\n",
      "Iteration 37, loss = 1.79844399\n",
      "Iteration 38, loss = 1.67832250\n",
      "Iteration 1, loss = 15.73969031\n",
      "Iteration 39, loss = 0.98010196\n",
      "Iteration 2, loss = 11.22425928\n",
      "Iteration 40, loss = 1.49646420\n",
      "Iteration 3, loss = 9.54752055\n",
      "Iteration 41, loss = 2.03932091\n",
      "Iteration 4, loss = 9.43702038\n",
      "Iteration 42, loss = 1.65986848\n",
      "Iteration 5, loss = 7.01649708\n",
      "Iteration 43, loss = 1.42712939\n",
      "Iteration 6, loss = 7.63299782\n",
      "Iteration 44, loss = 1.32456623\n",
      "Iteration 7, loss = 7.31643877\n",
      "Iteration 45, loss = 1.57593815\n",
      "Iteration 8, loss = 4.66652858\n",
      "Iteration 46, loss = 1.74564777\n",
      "Iteration 9, loss = 4.75657457\n",
      "Iteration 47, loss = 1.57059686\n",
      "Iteration 10, loss = 10.04858533\n",
      "Iteration 48, loss = 1.54466455\n",
      "Iteration 11, loss = 6.54527854\n",
      "Iteration 49, loss = 1.82023083\n",
      "Iteration 12, loss = 4.55084126\n",
      "Iteration 50, loss = 1.46103911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 4.52411301\n",
      "Iteration 14, loss = 4.14120590\n",
      "Iteration 15, loss = 3.46039410\n",
      "Iteration 16, loss = 3.74404722\n",
      "Iteration 17, loss = 4.28700657\n",
      "Iteration 18, loss = 4.07188411\n",
      "Iteration 19, loss = 3.41858524\n",
      "Iteration 20, loss = 3.00419262\n",
      "Iteration 21, loss = 2.84262161\n",
      "Iteration 22, loss = 2.58252399\n",
      "Iteration 23, loss = 2.19925108\n",
      "Iteration 24, loss = 1.58274773\n",
      "Iteration 25, loss = 1.71450804\n",
      "Iteration 1, loss = 17.45255033\n",
      "Iteration 26, loss = 2.20790217\n",
      "Iteration 2, loss = 16.44490562\n",
      "Iteration 27, loss = 3.16822942\n",
      "Iteration 3, loss = 14.00434158\n",
      "Iteration 28, loss = 2.53427734\n",
      "Iteration 4, loss = 7.12392273\n",
      "Iteration 29, loss = 2.63073650\n",
      "Iteration 5, loss = 6.21487726\n",
      "Iteration 30, loss = 2.56092677\n",
      "Iteration 6, loss = 6.31715056\n",
      "Iteration 31, loss = 2.17875936\n",
      "Iteration 7, loss = 7.67184971\n",
      "Iteration 32, loss = 1.71495678\n",
      "Iteration 8, loss = 6.24580521\n",
      "Iteration 33, loss = 2.40214181\n",
      "Iteration 9, loss = 6.42071020\n",
      "Iteration 34, loss = 2.31415043\n",
      "Iteration 10, loss = 6.54315120\n",
      "Iteration 35, loss = 1.83942630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 6.06423227\n",
      "Iteration 12, loss = 5.15935520\n",
      "Iteration 13, loss = 4.02423619\n",
      "Iteration 14, loss = 4.28137410\n",
      "Iteration 15, loss = 3.49458940\n",
      "Iteration 16, loss = 3.53077039\n",
      "Iteration 17, loss = 3.92885006\n",
      "Iteration 18, loss = 3.47633948\n",
      "Iteration 19, loss = 4.59148215\n",
      "Iteration 20, loss = 6.11747927\n",
      "Iteration 21, loss = 4.39489641\n",
      "Iteration 22, loss = 4.42067815\n",
      "Iteration 23, loss = 4.29486050\n",
      "Iteration 1, loss = 15.75572666\n",
      "Iteration 24, loss = 3.75126843\n",
      "Iteration 2, loss = 13.53226758\n",
      "Iteration 25, loss = 2.44073935\n",
      "Iteration 3, loss = 14.88124697\n",
      "Iteration 26, loss = 2.94886919\n",
      "Iteration 4, loss = 14.64124353\n",
      "Iteration 27, loss = 2.51051634\n",
      "Iteration 5, loss = 10.34946164\n",
      "Iteration 28, loss = 2.45690594\n",
      "Iteration 6, loss = 10.27317949\n",
      "Iteration 29, loss = 1.98137357\n",
      "Iteration 7, loss = 6.12658639\n",
      "Iteration 30, loss = 1.78953954\n",
      "Iteration 8, loss = 6.60833294\n",
      "Iteration 31, loss = 1.72376529\n",
      "Iteration 9, loss = 8.48013635\n",
      "Iteration 32, loss = 2.42623826\n",
      "Iteration 10, loss = 9.81487281\n",
      "Iteration 33, loss = 1.89618652\n",
      "Iteration 11, loss = 5.15163960\n",
      "Iteration 34, loss = 1.77052775\n",
      "Iteration 12, loss = 4.65685500\n",
      "Iteration 35, loss = 2.49834641\n",
      "Iteration 13, loss = 4.50295061\n",
      "Iteration 36, loss = 1.91229163\n",
      "Iteration 14, loss = 6.26617941\n",
      "Iteration 37, loss = 1.82609662\n",
      "Iteration 38, loss = 1.92215157\n",
      "Iteration 15, loss = 3.89157370\n",
      "Iteration 39, loss = 3.11262058\n",
      "Iteration 16, loss = 3.84154630\n",
      "Iteration 17, loss = 4.37500339\n",
      "Iteration 40, loss = 2.51273572\n",
      "Iteration 18, loss = 5.44539673\n",
      "Iteration 41, loss = 1.95556622\n",
      "Iteration 19, loss = 6.44480842\n",
      "Iteration 42, loss = 2.58609177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 3.98006687\n",
      "Iteration 21, loss = 3.93180789\n",
      "Iteration 22, loss = 2.95989397\n",
      "Iteration 23, loss = 3.82134244\n",
      "Iteration 24, loss = 5.27643829\n",
      "Iteration 25, loss = 3.71945570\n",
      "Iteration 26, loss = 3.47729235\n",
      "Iteration 27, loss = 2.66883339\n",
      "Iteration 28, loss = 3.08403920\n",
      "Iteration 29, loss = 2.46280774\n",
      "Iteration 30, loss = 2.21719801\n",
      "Iteration 31, loss = 2.55018951\n",
      "Iteration 32, loss = 2.34393702\n",
      "Iteration 1, loss = 17.90045792\n",
      "Iteration 33, loss = 2.27917249\n",
      "Iteration 2, loss = 13.40419265\n",
      "Iteration 34, loss = 1.82887890\n",
      "Iteration 3, loss = 8.33645973\n",
      "Iteration 35, loss = 1.76338540\n",
      "Iteration 4, loss = 13.26968324\n",
      "Iteration 36, loss = 1.50220605\n",
      "Iteration 5, loss = 11.27736067\n",
      "Iteration 37, loss = 1.16106536\n",
      "Iteration 6, loss = 9.00589951\n",
      "Iteration 38, loss = 1.44990748\n",
      "Iteration 7, loss = 7.87244323\n",
      "Iteration 39, loss = 1.36666230\n",
      "Iteration 8, loss = 7.99365618\n",
      "Iteration 40, loss = 1.31541163\n",
      "Iteration 9, loss = 6.55430348\n",
      "Iteration 41, loss = 1.51968916\n",
      "Iteration 10, loss = 6.28790240\n",
      "Iteration 42, loss = 1.19825146\n",
      "Iteration 11, loss = 6.15590831\n",
      "Iteration 43, loss = 1.03411676\n",
      "Iteration 12, loss = 5.30206599\n",
      "Iteration 44, loss = 1.72667802\n",
      "Iteration 13, loss = 9.19312060\n",
      "Iteration 45, loss = 1.43011174\n",
      "Iteration 14, loss = 7.89107752\n",
      "Iteration 46, loss = 1.61480760\n",
      "Iteration 15, loss = 4.95385419\n",
      "Iteration 47, loss = 1.95799533\n",
      "Iteration 16, loss = 5.70634075\n",
      "Iteration 48, loss = 2.27006437\n",
      "Iteration 17, loss = 5.01703036\n",
      "Iteration 49, loss = 1.75587208\n",
      "Iteration 18, loss = 4.56395922\n",
      "Iteration 50, loss = 1.40975034\n",
      "Iteration 19, loss = 4.05263336\n",
      "Iteration 51, loss = 1.35422984\n",
      "Iteration 20, loss = 3.70082535\n",
      "Iteration 52, loss = 1.41882129\n",
      "Iteration 21, loss = 3.69859206\n",
      "Iteration 53, loss = 1.60578066\n",
      "Iteration 22, loss = 3.77250630\n",
      "Iteration 54, loss = 1.41468240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 3.51709276\n",
      "Iteration 24, loss = 3.17436104\n",
      "Iteration 25, loss = 4.15102539\n",
      "Iteration 26, loss = 3.10567796\n",
      "Iteration 27, loss = 3.49959161\n",
      "Iteration 28, loss = 2.78819647\n",
      "Iteration 29, loss = 2.90201755\n",
      "Iteration 30, loss = 2.94919240\n",
      "Iteration 31, loss = 2.78000347\n",
      "Iteration 32, loss = 2.13106283\n",
      "Iteration 33, loss = 2.47024943\n",
      "Iteration 34, loss = 1.97262098\n",
      "Iteration 35, loss = 3.54694547\n",
      "Iteration 1, loss = 17.13083630\n",
      "Iteration 36, loss = 2.96815349\n",
      "Iteration 2, loss = 15.35923400\n",
      "Iteration 37, loss = 2.12297461\n",
      "Iteration 3, loss = 15.11528841\n",
      "Iteration 38, loss = 1.66135310\n",
      "Iteration 4, loss = 11.59159202\n",
      "Iteration 39, loss = 1.46389164\n",
      "Iteration 5, loss = 6.93203255\n",
      "Iteration 40, loss = 1.78093003\n",
      "Iteration 6, loss = 7.32704367\n",
      "Iteration 41, loss = 1.87776372\n",
      "Iteration 7, loss = 7.86414985\n",
      "Iteration 42, loss = 2.55486831\n",
      "Iteration 8, loss = 7.02150996\n",
      "Iteration 43, loss = 2.06214016\n",
      "Iteration 9, loss = 7.03458333\n",
      "Iteration 44, loss = 2.20150002\n",
      "Iteration 10, loss = 6.38725929\n",
      "Iteration 45, loss = 2.42692884\n",
      "Iteration 11, loss = 5.49669677\n",
      "Iteration 46, loss = 1.96164393\n",
      "Iteration 12, loss = 4.47561372\n",
      "Iteration 47, loss = 2.16476775\n",
      "Iteration 13, loss = 4.46540190\n",
      "Iteration 48, loss = 2.03721105\n",
      "Iteration 14, loss = 4.42499005\n",
      "Iteration 49, loss = 1.64822822\n",
      "Iteration 15, loss = 4.79151217\n",
      "Iteration 50, loss = 1.38639985\n",
      "Iteration 16, loss = 4.20757094\n",
      "Iteration 51, loss = 1.41640717\n",
      "Iteration 17, loss = 3.90715840\n",
      "Iteration 52, loss = 1.35515482\n",
      "Iteration 18, loss = 3.58872548\n",
      "Iteration 53, loss = 1.34764938\n",
      "Iteration 19, loss = 4.13009886\n",
      "Iteration 54, loss = 1.58360312\n",
      "Iteration 20, loss = 2.41532875\n",
      "Iteration 55, loss = 0.95473206\n",
      "Iteration 21, loss = 2.75311754\n",
      "Iteration 56, loss = 0.87498443\n",
      "Iteration 22, loss = 2.60581964\n",
      "Iteration 57, loss = 0.85151045\n",
      "Iteration 23, loss = 2.54940618\n",
      "Iteration 58, loss = 0.63783801\n",
      "Iteration 24, loss = 2.80633608\n",
      "Iteration 59, loss = 0.63442382\n",
      "Iteration 25, loss = 2.24910049\n",
      "Iteration 60, loss = 0.87383812\n",
      "Iteration 26, loss = 2.67547497\n",
      "Iteration 61, loss = 1.12454114\n",
      "Iteration 27, loss = 2.58078885\n",
      "Iteration 62, loss = 1.15020844\n",
      "Iteration 28, loss = 2.17112161\n",
      "Iteration 63, loss = 1.09200521\n",
      "Iteration 29, loss = 2.40618399\n",
      "Iteration 64, loss = 1.11799833\n",
      "Iteration 30, loss = 2.01345329\n",
      "Iteration 65, loss = 0.75591074\n",
      "Iteration 31, loss = 1.56672782\n",
      "Iteration 66, loss = 0.68913974\n",
      "Iteration 32, loss = 1.93540179\n",
      "Iteration 67, loss = 0.65950397\n",
      "Iteration 33, loss = 2.40012048\n",
      "Iteration 68, loss = 0.71164630\n",
      "Iteration 34, loss = 1.73577259\n",
      "Iteration 69, loss = 0.66032125\n",
      "Iteration 35, loss = 1.69357712\n",
      "Iteration 70, loss = 0.77235722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 1.53018773\n",
      "Iteration 37, loss = 1.84646634\n",
      "Iteration 38, loss = 2.67965081\n",
      "Iteration 39, loss = 1.77301960\n",
      "Iteration 40, loss = 1.60024475\n",
      "Iteration 41, loss = 1.51122913\n",
      "Iteration 42, loss = 1.35488886\n",
      "Iteration 43, loss = 1.41307124\n",
      "Iteration 44, loss = 1.29587246\n",
      "Iteration 45, loss = 1.13236021\n",
      "Iteration 46, loss = 1.61578575\n",
      "Iteration 47, loss = 1.39500183\n",
      "Iteration 48, loss = 2.04232159\n",
      "Iteration 1, loss = 17.27348016\n",
      "Iteration 49, loss = 1.67555325\n",
      "Iteration 2, loss = 13.23084476\n",
      "Iteration 50, loss = 1.28481777\n",
      "Iteration 3, loss = 10.35497767\n",
      "Iteration 51, loss = 1.33164515\n",
      "Iteration 4, loss = 10.15721540\n",
      "Iteration 52, loss = 0.93460863\n",
      "Iteration 5, loss = 12.60044839\n",
      "Iteration 53, loss = 1.04855326\n",
      "Iteration 6, loss = 13.04091462\n",
      "Iteration 54, loss = 0.96836481\n",
      "Iteration 7, loss = 10.31533098\n",
      "Iteration 55, loss = 1.37886165\n",
      "Iteration 8, loss = 8.74313959\n",
      "Iteration 56, loss = 1.35832694\n",
      "Iteration 9, loss = 8.59297264\n",
      "Iteration 57, loss = 1.32718651\n",
      "Iteration 10, loss = 6.04941922\n",
      "Iteration 58, loss = 1.21733766\n",
      "Iteration 11, loss = 6.30719102\n",
      "Iteration 59, loss = 1.00856607\n",
      "Iteration 12, loss = 5.29112278\n",
      "Iteration 60, loss = 0.84239612\n",
      "Iteration 13, loss = 5.11742751\n",
      "Iteration 61, loss = 0.93728818\n",
      "Iteration 14, loss = 5.14040324\n",
      "Iteration 62, loss = 0.78221883\n",
      "Iteration 15, loss = 5.67070061\n",
      "Iteration 63, loss = 1.16498300\n",
      "Iteration 16, loss = 5.53462191\n",
      "Iteration 64, loss = 0.87855727\n",
      "Iteration 17, loss = 4.88934603\n",
      "Iteration 65, loss = 1.27626464\n",
      "Iteration 18, loss = 4.82172201\n",
      "Iteration 66, loss = 1.40977510\n",
      "Iteration 19, loss = 5.18366611\n",
      "Iteration 67, loss = 0.96434746\n",
      "Iteration 20, loss = 4.00159521\n",
      "Iteration 68, loss = 0.73354880\n",
      "Iteration 21, loss = 3.23485398\n",
      "Iteration 69, loss = 1.12115204\n",
      "Iteration 22, loss = 2.89881496\n",
      "Iteration 70, loss = 0.94604420\n",
      "Iteration 23, loss = 2.87104185\n",
      "Iteration 71, loss = 1.09476931\n",
      "Iteration 24, loss = 2.39877085\n",
      "Iteration 72, loss = 1.57299110\n",
      "Iteration 25, loss = 2.33476584\n",
      "Iteration 73, loss = 1.41968978\n",
      "Iteration 26, loss = 2.29214633\n",
      "Iteration 74, loss = 0.87166976\n",
      "Iteration 27, loss = 2.39893129\n",
      "Iteration 75, loss = 1.54071855\n",
      "Iteration 28, loss = 2.83237860\n",
      "Iteration 76, loss = 1.41061072\n",
      "Iteration 29, loss = 1.69629045\n",
      "Iteration 77, loss = 1.73064251\n",
      "Iteration 30, loss = 2.04900529\n",
      "Iteration 78, loss = 1.75720346\n",
      "Iteration 31, loss = 2.56454867\n",
      "Iteration 79, loss = 1.40935613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 2.72998672\n",
      "Iteration 33, loss = 2.35695693\n",
      "Iteration 34, loss = 2.09265494\n",
      "Iteration 35, loss = 3.69884078\n",
      "Iteration 36, loss = 3.18869623\n",
      "Iteration 37, loss = 3.18488599\n",
      "Iteration 38, loss = 2.60509046\n",
      "Iteration 39, loss = 2.76538035\n",
      "Iteration 40, loss = 2.07618634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.37748382\n",
      "Iteration 2, loss = 11.42135765\n",
      "Iteration 3, loss = 6.19828916\n",
      "Iteration 4, loss = 6.63544809\n",
      "Iteration 5, loss = 11.80519226\n",
      "Iteration 6, loss = 6.94297945\n",
      "Iteration 7, loss = 7.00212382\n",
      "Iteration 8, loss = 6.98516835\n",
      "Iteration 9, loss = 5.19393365\n",
      "Iteration 1, loss = 14.75940899\n",
      "Iteration 10, loss = 5.36436065\n",
      "Iteration 2, loss = 13.22896866\n",
      "Iteration 11, loss = 5.12281377\n",
      "Iteration 3, loss = 10.84495758\n",
      "Iteration 12, loss = 4.72354903\n",
      "Iteration 4, loss = 9.39700676\n",
      "Iteration 13, loss = 4.48325753\n",
      "Iteration 5, loss = 6.42765350\n",
      "Iteration 14, loss = 4.93774879\n",
      "Iteration 6, loss = 6.97608465\n",
      "Iteration 15, loss = 4.03335273\n",
      "Iteration 7, loss = 7.26678868\n",
      "Iteration 16, loss = 3.54444832\n",
      "Iteration 8, loss = 6.19094609\n",
      "Iteration 17, loss = 3.68684856\n",
      "Iteration 9, loss = 6.24868996\n",
      "Iteration 18, loss = 2.78004539\n",
      "Iteration 10, loss = 4.59955806\n",
      "Iteration 19, loss = 2.50453269\n",
      "Iteration 11, loss = 4.90053498\n",
      "Iteration 20, loss = 4.15301492\n",
      "Iteration 12, loss = 3.79964615\n",
      "Iteration 21, loss = 2.97458129\n",
      "Iteration 13, loss = 4.58690291\n",
      "Iteration 22, loss = 3.91709019\n",
      "Iteration 14, loss = 4.90935146\n",
      "Iteration 23, loss = 2.82123752\n",
      "Iteration 15, loss = 3.99095253\n",
      "Iteration 24, loss = 3.16963705\n",
      "Iteration 16, loss = 3.22212478\n",
      "Iteration 25, loss = 3.11168446\n",
      "Iteration 17, loss = 3.00872501\n",
      "Iteration 26, loss = 2.82422245\n",
      "Iteration 18, loss = 3.41820381\n",
      "Iteration 27, loss = 2.68906605\n",
      "Iteration 19, loss = 3.44525473\n",
      "Iteration 28, loss = 2.48158970\n",
      "Iteration 20, loss = 4.13313741\n",
      "Iteration 29, loss = 2.25884752\n",
      "Iteration 21, loss = 2.72887227\n",
      "Iteration 30, loss = 2.32179772\n",
      "Iteration 22, loss = 2.69213679\n",
      "Iteration 31, loss = 3.49509959\n",
      "Iteration 23, loss = 3.15415337\n",
      "Iteration 32, loss = 2.92758970\n",
      "Iteration 24, loss = 2.25215002\n",
      "Iteration 33, loss = 2.56073319\n",
      "Iteration 25, loss = 1.57889518\n",
      "Iteration 34, loss = 2.79138843\n",
      "Iteration 26, loss = 2.68912820\n",
      "Iteration 35, loss = 1.73379490\n",
      "Iteration 27, loss = 2.61891714\n",
      "Iteration 36, loss = 2.21001214\n",
      "Iteration 28, loss = 4.28443860\n",
      "Iteration 37, loss = 1.78369352\n",
      "Iteration 29, loss = 3.56054065\n",
      "Iteration 38, loss = 2.18861773\n",
      "Iteration 30, loss = 3.69512585\n",
      "Iteration 39, loss = 2.01625236\n",
      "Iteration 31, loss = 3.23435655\n",
      "Iteration 40, loss = 1.74397649\n",
      "Iteration 32, loss = 4.21151917\n",
      "Iteration 41, loss = 2.10732070\n",
      "Iteration 33, loss = 4.74753297\n",
      "Iteration 42, loss = 2.00323416\n",
      "Iteration 34, loss = 4.40971601\n",
      "Iteration 43, loss = 4.90177580\n",
      "Iteration 35, loss = 4.02823376\n",
      "Iteration 44, loss = 3.74392506\n",
      "Iteration 36, loss = 2.97760504\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 4.67370879\n",
      "Iteration 46, loss = 3.51124776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.69144218\n",
      "Iteration 2, loss = 18.12938681\n",
      "Iteration 1, loss = 14.76067290\n",
      "Iteration 3, loss = 12.60035989\n",
      "Iteration 2, loss = 15.36595571\n",
      "Iteration 4, loss = 10.65633608\n",
      "Iteration 3, loss = 14.01918858\n",
      "Iteration 5, loss = 7.67747617\n",
      "Iteration 4, loss = 10.65328913\n",
      "Iteration 6, loss = 6.44441272\n",
      "Iteration 5, loss = 7.12921366\n",
      "Iteration 7, loss = 8.57158787\n",
      "Iteration 6, loss = 8.54772815\n",
      "Iteration 8, loss = 7.63800889\n",
      "Iteration 7, loss = 7.08800436\n",
      "Iteration 9, loss = 6.83492252\n",
      "Iteration 8, loss = 8.79419155\n",
      "Iteration 10, loss = 4.80108129\n",
      "Iteration 9, loss = 9.26461936\n",
      "Iteration 11, loss = 4.52311387\n",
      "Iteration 10, loss = 7.23467524\n",
      "Iteration 12, loss = 4.56059165\n",
      "Iteration 11, loss = 6.35885622\n",
      "Iteration 13, loss = 4.57624652\n",
      "Iteration 12, loss = 3.31595229\n",
      "Iteration 14, loss = 3.15682192\n",
      "Iteration 13, loss = 4.03233554\n",
      "Iteration 15, loss = 3.57392523\n",
      "Iteration 14, loss = 3.80971337\n",
      "Iteration 16, loss = 7.52306261\n",
      "Iteration 15, loss = 3.07346549\n",
      "Iteration 17, loss = 3.49314271\n",
      "Iteration 16, loss = 3.03631966\n",
      "Iteration 18, loss = 3.85152790\n",
      "Iteration 17, loss = 6.12270773\n",
      "Iteration 19, loss = 2.27477273\n",
      "Iteration 18, loss = 4.27869180\n",
      "Iteration 20, loss = 3.14074109\n",
      "Iteration 19, loss = 3.54115255\n",
      "Iteration 21, loss = 2.62599703\n",
      "Iteration 20, loss = 5.06887755\n",
      "Iteration 22, loss = 2.63608703\n",
      "Iteration 21, loss = 3.32739580\n",
      "Iteration 23, loss = 1.70126512\n",
      "Iteration 22, loss = 2.97536848\n",
      "Iteration 24, loss = 2.39778396\n",
      "Iteration 23, loss = 2.61495688\n",
      "Iteration 25, loss = 2.93339764\n",
      "Iteration 24, loss = 2.31048644\n",
      "Iteration 26, loss = 3.02168880\n",
      "Iteration 25, loss = 2.91283673\n",
      "Iteration 27, loss = 4.75938689\n",
      "Iteration 26, loss = 2.65315097\n",
      "Iteration 28, loss = 3.21303420\n",
      "Iteration 27, loss = 2.41591870\n",
      "Iteration 29, loss = 2.87324926\n",
      "Iteration 28, loss = 2.09242784\n",
      "Iteration 30, loss = 2.49256398\n",
      "Iteration 29, loss = 1.51710669\n",
      "Iteration 31, loss = 1.85552030\n",
      "Iteration 30, loss = 1.89853004\n",
      "Iteration 32, loss = 2.22664911\n",
      "Iteration 31, loss = 1.35083594\n",
      "Iteration 33, loss = 2.55382679\n",
      "Iteration 32, loss = 1.19943776\n",
      "Iteration 34, loss = 1.99048540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 1.20066163\n",
      "Iteration 34, loss = 1.19011179\n",
      "Iteration 35, loss = 1.05193873\n",
      "Iteration 36, loss = 1.32860905\n",
      "Iteration 37, loss = 1.12966892\n",
      "Iteration 38, loss = 1.24857370\n",
      "Iteration 39, loss = 1.12822185\n",
      "Iteration 40, loss = 1.08813709\n",
      "Iteration 41, loss = 1.21203828\n",
      "Iteration 42, loss = 1.73640245\n",
      "Iteration 43, loss = 1.21195746\n",
      "Iteration 44, loss = 0.95080960\n",
      "Iteration 45, loss = 1.11949568\n",
      "Iteration 1, loss = 17.64606684\n",
      "Iteration 46, loss = 1.03173051\n",
      "Iteration 2, loss = 13.71852101\n",
      "Iteration 47, loss = 0.96103313\n",
      "Iteration 3, loss = 10.72559119\n",
      "Iteration 48, loss = 0.36349505\n",
      "Iteration 4, loss = 8.30199765\n",
      "Iteration 49, loss = 0.54862010\n",
      "Iteration 5, loss = 6.34517012\n",
      "Iteration 50, loss = 0.47506150\n",
      "Iteration 6, loss = 7.58002886\n",
      "Iteration 51, loss = 0.47692995\n",
      "Iteration 7, loss = 7.33014650\n",
      "Iteration 52, loss = 0.53803134\n",
      "Iteration 8, loss = 5.67565907\n",
      "Iteration 53, loss = 0.46250488\n",
      "Iteration 9, loss = 5.60222905\n",
      "Iteration 54, loss = 0.42127872\n",
      "Iteration 10, loss = 4.53098475\n",
      "Iteration 55, loss = 0.56680046\n",
      "Iteration 11, loss = 5.28265312\n",
      "Iteration 56, loss = 0.74992919\n",
      "Iteration 12, loss = 4.08563990\n",
      "Iteration 57, loss = 0.87585140\n",
      "Iteration 13, loss = 4.65096636\n",
      "Iteration 58, loss = 0.67256812\n",
      "Iteration 14, loss = 3.20690519\n",
      "Iteration 59, loss = 0.66997489\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 3.56000782\n",
      "Iteration 16, loss = 5.80214135\n",
      "Iteration 17, loss = 4.12529549\n",
      "Iteration 18, loss = 3.63933626\n",
      "Iteration 19, loss = 3.30160831\n",
      "Iteration 20, loss = 4.75301246\n",
      "Iteration 21, loss = 4.08966288\n",
      "Iteration 22, loss = 2.35815995\n",
      "Iteration 23, loss = 2.03159644\n",
      "Iteration 24, loss = 2.15351194\n",
      "Iteration 25, loss = 2.61640062\n",
      "Iteration 26, loss = 2.62756015\n",
      "Iteration 27, loss = 2.07945655\n",
      "Iteration 1, loss = 18.02776650\n",
      "Iteration 28, loss = 2.07976351\n",
      "Iteration 2, loss = 14.05174165\n",
      "Iteration 29, loss = 2.09133599\n",
      "Iteration 3, loss = 14.52999852\n",
      "Iteration 30, loss = 1.76153393\n",
      "Iteration 4, loss = 8.41939913\n",
      "Iteration 31, loss = 2.09497216\n",
      "Iteration 5, loss = 7.96367902\n",
      "Iteration 32, loss = 2.27683970\n",
      "Iteration 6, loss = 12.04649220\n",
      "Iteration 33, loss = 2.13790719\n",
      "Iteration 7, loss = 6.26564520\n",
      "Iteration 34, loss = 2.02013468\n",
      "Iteration 8, loss = 7.91179323\n",
      "Iteration 35, loss = 1.69214622\n",
      "Iteration 9, loss = 6.50402333\n",
      "Iteration 36, loss = 1.84375617\n",
      "Iteration 10, loss = 5.78736957\n",
      "Iteration 37, loss = 1.41852611\n",
      "Iteration 11, loss = 4.95584790\n",
      "Iteration 38, loss = 1.48744264\n",
      "Iteration 12, loss = 4.18124386\n",
      "Iteration 39, loss = 1.89592619\n",
      "Iteration 13, loss = 6.66943977\n",
      "Iteration 40, loss = 1.27084379\n",
      "Iteration 14, loss = 8.61735914\n",
      "Iteration 41, loss = 1.54273904\n",
      "Iteration 15, loss = 5.49765312\n",
      "Iteration 42, loss = 1.66652017\n",
      "Iteration 16, loss = 3.76217865\n",
      "Iteration 43, loss = 1.33158699\n",
      "Iteration 17, loss = 3.99931427\n",
      "Iteration 44, loss = 1.15149232\n",
      "Iteration 18, loss = 4.10791730\n",
      "Iteration 45, loss = 1.82714524\n",
      "Iteration 19, loss = 4.29928983\n",
      "Iteration 46, loss = 1.70742261\n",
      "Iteration 20, loss = 3.75459978\n",
      "Iteration 47, loss = 2.11905161\n",
      "Iteration 21, loss = 4.15575216\n",
      "Iteration 48, loss = 1.12252281\n",
      "Iteration 22, loss = 3.32177270\n",
      "Iteration 49, loss = 2.33122961\n",
      "Iteration 23, loss = 3.79760757\n",
      "Iteration 24, loss = 2.82193812\n",
      "Iteration 50, loss = 1.22029293\n",
      "Iteration 25, loss = 3.14035114\n",
      "Iteration 51, loss = 1.12504387\n",
      "Iteration 26, loss = 2.98884147\n",
      "Iteration 52, loss = 1.53921308\n",
      "Iteration 53, loss = 1.21288166\n",
      "Iteration 27, loss = 3.05882460\n",
      "Iteration 54, loss = 1.01181475\n",
      "Iteration 28, loss = 3.20099797\n",
      "Iteration 29, loss = 2.47352389\n",
      "Iteration 55, loss = 1.36854480\n",
      "Iteration 30, loss = 2.41227039\n",
      "Iteration 56, loss = 1.51674709\n",
      "Iteration 31, loss = 2.34118874\n",
      "Iteration 57, loss = 1.24017080\n",
      "Iteration 58, loss = 2.13735044\n",
      "Iteration 32, loss = 1.66187998\n",
      "Iteration 59, loss = 2.45868006\n",
      "Iteration 33, loss = 1.79530898\n",
      "Iteration 60, loss = 7.29202685\n",
      "Iteration 34, loss = 1.98979052\n",
      "Iteration 35, loss = 1.94136712\n",
      "Iteration 61, loss = 5.21245484\n",
      "Iteration 36, loss = 1.65682243\n",
      "Iteration 62, loss = 8.50869160\n",
      "Iteration 37, loss = 1.51105046\n",
      "Iteration 63, loss = 5.01393459\n",
      "Iteration 38, loss = 2.87614271\n",
      "Iteration 64, loss = 6.22696870\n",
      "Iteration 39, loss = 1.93543179\n",
      "Iteration 65, loss = 5.19431073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 2.19494418\n",
      "Iteration 41, loss = 1.98720901\n",
      "Iteration 42, loss = 1.80184692\n",
      "Iteration 43, loss = 2.60187824\n",
      "Iteration 44, loss = 2.86853174\n",
      "Iteration 45, loss = 2.43600294\n",
      "Iteration 46, loss = 1.86992199\n",
      "Iteration 47, loss = 2.38045595\n",
      "Iteration 48, loss = 2.05300470\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.77556339\n",
      "Iteration 2, loss = 12.99421328\n",
      "Iteration 3, loss = 11.26703753\n",
      "Iteration 4, loss = 9.68040100\n",
      "Iteration 5, loss = 8.27393091\n",
      "Iteration 6, loss = 6.50377528\n",
      "Iteration 7, loss = 6.49229614\n",
      "Iteration 8, loss = 5.99973998\n",
      "Iteration 9, loss = 6.82452873\n",
      "Iteration 10, loss = 5.75562064\n",
      "Iteration 1, loss = 15.66232149\n",
      "Iteration 2, loss = 13.59888647\n",
      "Iteration 11, loss = 5.56800338\n",
      "Iteration 12, loss = 3.63998223\n",
      "Iteration 3, loss = 10.71644149\n",
      "Iteration 13, loss = 4.59950876Iteration 4, loss = 11.28328580\n",
      "\n",
      "Iteration 5, loss = 7.36852406\n",
      "Iteration 14, loss = 4.30728553\n",
      "Iteration 6, loss = 6.27686861\n",
      "Iteration 15, loss = 3.58991905\n",
      "Iteration 7, loss = 9.26333929\n",
      "Iteration 16, loss = 5.23917438\n",
      "Iteration 8, loss = 6.71088768\n",
      "Iteration 17, loss = 3.89776135\n",
      "Iteration 9, loss = 8.37985886\n",
      "Iteration 18, loss = 3.92713603\n",
      "Iteration 10, loss = 5.72507065\n",
      "Iteration 19, loss = 3.21905241\n",
      "Iteration 11, loss = 4.54015103\n",
      "Iteration 20, loss = 2.71307385\n",
      "Iteration 12, loss = 6.01258801\n",
      "Iteration 21, loss = 2.93651796\n",
      "Iteration 13, loss = 4.88531048\n",
      "Iteration 22, loss = 3.95861734\n",
      "Iteration 14, loss = 4.31184602\n",
      "Iteration 23, loss = 3.31728524\n",
      "Iteration 15, loss = 3.89199234\n",
      "Iteration 24, loss = 2.82941942\n",
      "Iteration 16, loss = 3.10646048\n",
      "Iteration 25, loss = 2.98411724\n",
      "Iteration 17, loss = 3.25015088\n",
      "Iteration 26, loss = 2.50782793\n",
      "Iteration 18, loss = 2.54359859\n",
      "Iteration 27, loss = 2.13153441\n",
      "Iteration 19, loss = 2.35842117\n",
      "Iteration 28, loss = 3.02140128\n",
      "Iteration 20, loss = 2.81870492\n",
      "Iteration 29, loss = 2.20571126\n",
      "Iteration 21, loss = 3.04156821\n",
      "Iteration 30, loss = 2.45052015\n",
      "Iteration 22, loss = 2.12565989\n",
      "Iteration 31, loss = 2.39564799\n",
      "Iteration 23, loss = 2.21924540\n",
      "Iteration 32, loss = 1.84494797\n",
      "Iteration 24, loss = 2.97681198\n",
      "Iteration 33, loss = 2.04508392\n",
      "Iteration 25, loss = 3.04662035\n",
      "Iteration 34, loss = 1.59087339\n",
      "Iteration 26, loss = 3.10724469\n",
      "Iteration 35, loss = 1.43593854\n",
      "Iteration 27, loss = 2.40255210\n",
      "Iteration 36, loss = 1.39893862\n",
      "Iteration 28, loss = 2.64161586\n",
      "Iteration 37, loss = 1.79283921\n",
      "Iteration 29, loss = 2.64714410\n",
      "Iteration 38, loss = 1.63736390\n",
      "Iteration 30, loss = 2.34327937\n",
      "Iteration 39, loss = 1.73005027\n",
      "Iteration 31, loss = 2.22826539\n",
      "Iteration 40, loss = 1.68317741\n",
      "Iteration 32, loss = 2.84855288\n",
      "Iteration 41, loss = 2.21774452\n",
      "Iteration 33, loss = 2.49831272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 1.78556510\n",
      "Iteration 43, loss = 1.67727758\n",
      "Iteration 44, loss = 2.48521153\n",
      "Iteration 45, loss = 1.33227031\n",
      "Iteration 46, loss = 2.18356018\n",
      "Iteration 47, loss = 1.57366828\n",
      "Iteration 48, loss = 1.63238638\n",
      "Iteration 49, loss = 1.92618041\n",
      "Iteration 50, loss = 2.72979073\n",
      "Iteration 51, loss = 2.56759781\n",
      "Iteration 52, loss = 2.48562661\n",
      "Iteration 53, loss = 2.64372035\n",
      "Iteration 54, loss = 1.97987692\n",
      "Iteration 1, loss = 18.67877532\n",
      "Iteration 55, loss = 2.34095700\n",
      "Iteration 2, loss = 18.16690150\n",
      "Iteration 56, loss = 1.89559464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 10.41043403\n",
      "Iteration 4, loss = 9.27536586\n",
      "Iteration 5, loss = 8.26537968\n",
      "Iteration 6, loss = 7.72144229\n",
      "Iteration 7, loss = 6.42796164\n",
      "Iteration 8, loss = 6.06793086\n",
      "Iteration 9, loss = 8.92297869\n",
      "Iteration 10, loss = 6.36668819\n",
      "Iteration 11, loss = 4.44771454\n",
      "Iteration 12, loss = 4.16246465\n",
      "Iteration 13, loss = 3.94248613\n",
      "Iteration 14, loss = 3.49772431\n",
      "Iteration 15, loss = 3.17482198\n",
      "Iteration 1, loss = 19.14527989\n",
      "Iteration 16, loss = 4.03659248\n",
      "Iteration 2, loss = 23.74404856\n",
      "Iteration 17, loss = 3.87465858\n",
      "Iteration 3, loss = 15.83179469\n",
      "Iteration 18, loss = 3.03616032\n",
      "Iteration 4, loss = 14.99939746\n",
      "Iteration 19, loss = 4.36954160\n",
      "Iteration 5, loss = 9.28645208\n",
      "Iteration 20, loss = 3.39460040\n",
      "Iteration 6, loss = 7.67883508\n",
      "Iteration 21, loss = 3.08659025\n",
      "Iteration 7, loss = 8.15971371\n",
      "Iteration 22, loss = 3.04340001\n",
      "Iteration 8, loss = 8.39898905\n",
      "Iteration 23, loss = 2.78105885\n",
      "Iteration 9, loss = 6.16868420\n",
      "Iteration 24, loss = 3.14214803\n",
      "Iteration 10, loss = 6.14374055\n",
      "Iteration 25, loss = 1.97182579\n",
      "Iteration 11, loss = 7.19383241\n",
      "Iteration 26, loss = 2.54648678\n",
      "Iteration 12, loss = 4.79112036\n",
      "Iteration 27, loss = 2.76743424\n",
      "Iteration 13, loss = 4.22313247\n",
      "Iteration 28, loss = 2.04500270\n",
      "Iteration 14, loss = 4.19798072\n",
      "Iteration 29, loss = 2.51313439\n",
      "Iteration 15, loss = 3.06882419\n",
      "Iteration 30, loss = 2.38135883\n",
      "Iteration 16, loss = 3.45627779\n",
      "Iteration 31, loss = 2.12377719\n",
      "Iteration 17, loss = 3.06746402\n",
      "Iteration 32, loss = 2.89752890\n",
      "Iteration 18, loss = 2.85449069\n",
      "Iteration 33, loss = 2.86842497\n",
      "Iteration 19, loss = 3.24877936\n",
      "Iteration 34, loss = 2.57530001\n",
      "Iteration 20, loss = 2.28658110\n",
      "Iteration 35, loss = 1.82595656\n",
      "Iteration 21, loss = 2.04593671\n",
      "Iteration 36, loss = 1.37253321\n",
      "Iteration 22, loss = 2.45595062\n",
      "Iteration 37, loss = 1.92199364\n",
      "Iteration 23, loss = 1.82950726\n",
      "Iteration 38, loss = 1.88131973\n",
      "Iteration 24, loss = 2.45980236\n",
      "Iteration 39, loss = 1.83758621\n",
      "Iteration 25, loss = 1.89902683\n",
      "Iteration 40, loss = 1.44318459\n",
      "Iteration 26, loss = 1.50022133\n",
      "Iteration 41, loss = 1.24101123\n",
      "Iteration 27, loss = 2.51713694\n",
      "Iteration 42, loss = 1.26452571\n",
      "Iteration 28, loss = 2.61957401\n",
      "Iteration 43, loss = 1.04772657\n",
      "Iteration 29, loss = 2.88477445\n",
      "Iteration 44, loss = 1.96316051\n",
      "Iteration 30, loss = 2.83725196\n",
      "Iteration 45, loss = 3.00379236\n",
      "Iteration 31, loss = 2.27787926\n",
      "Iteration 46, loss = 2.07415696\n",
      "Iteration 32, loss = 2.09775353\n",
      "Iteration 47, loss = 1.44007868\n",
      "Iteration 33, loss = 1.89752468\n",
      "Iteration 48, loss = 1.48576723\n",
      "Iteration 34, loss = 1.26425650\n",
      "Iteration 49, loss = 2.30637111\n",
      "Iteration 35, loss = 1.23109394\n",
      "Iteration 50, loss = 2.51671556\n",
      "Iteration 36, loss = 3.01366114\n",
      "Iteration 51, loss = 2.55487113\n",
      "Iteration 37, loss = 2.40029111\n",
      "Iteration 52, loss = 2.58280650\n",
      "Iteration 38, loss = 1.56644462\n",
      "Iteration 53, loss = 1.24803294\n",
      "Iteration 39, loss = 1.50930684\n",
      "Iteration 54, loss = 1.61639352\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 1.85701896\n",
      "Iteration 41, loss = 2.27605772\n",
      "Iteration 42, loss = 2.49360523\n",
      "Iteration 43, loss = 1.47386525\n",
      "Iteration 44, loss = 1.86998000\n",
      "Iteration 45, loss = 1.33320267\n",
      "Iteration 46, loss = 1.20062479\n",
      "Iteration 47, loss = 1.92970263\n",
      "Iteration 48, loss = 1.22379122\n",
      "Iteration 49, loss = 1.68096311\n",
      "Iteration 50, loss = 1.34185093\n",
      "Iteration 51, loss = 1.05686089\n",
      "Iteration 52, loss = 0.94156271\n",
      "Iteration 1, loss = 19.81932461\n",
      "Iteration 53, loss = 0.88302999\n",
      "Iteration 2, loss = 15.19471844\n",
      "Iteration 54, loss = 1.12140064\n",
      "Iteration 3, loss = 12.37586827\n",
      "Iteration 55, loss = 1.12902792\n",
      "Iteration 4, loss = 10.69256225\n",
      "Iteration 56, loss = 1.60099062\n",
      "Iteration 5, loss = 11.10579969\n",
      "Iteration 57, loss = 1.69237028\n",
      "Iteration 6, loss = 10.95232852\n",
      "Iteration 58, loss = 1.00985225\n",
      "Iteration 7, loss = 9.11220804\n",
      "Iteration 59, loss = 1.21605160\n",
      "Iteration 8, loss = 9.18795667\n",
      "Iteration 60, loss = 0.86370759\n",
      "Iteration 9, loss = 7.89361504\n",
      "Iteration 61, loss = 1.41443230\n",
      "Iteration 10, loss = 6.06162043\n",
      "Iteration 62, loss = 0.78752615\n",
      "Iteration 11, loss = 6.51460204\n",
      "Iteration 63, loss = 1.09808106\n",
      "Iteration 12, loss = 6.63804288\n",
      "Iteration 64, loss = 1.09135123\n",
      "Iteration 13, loss = 7.66333670\n",
      "Iteration 65, loss = 0.68448085\n",
      "Iteration 14, loss = 5.14413309\n",
      "Iteration 66, loss = 0.65973729\n",
      "Iteration 15, loss = 4.98272161\n",
      "Iteration 67, loss = 0.47440683\n",
      "Iteration 16, loss = 4.61829933\n",
      "Iteration 68, loss = 0.69093179\n",
      "Iteration 17, loss = 4.36849926\n",
      "Iteration 69, loss = 0.52290626\n",
      "Iteration 18, loss = 5.31304340\n",
      "Iteration 70, loss = 0.53127207\n",
      "Iteration 19, loss = 5.50141634\n",
      "Iteration 71, loss = 0.46278833\n",
      "Iteration 20, loss = 4.76520564\n",
      "Iteration 72, loss = 0.62919574\n",
      "Iteration 21, loss = 3.92244796\n",
      "Iteration 22, loss = 4.12732633\n",
      "Iteration 73, loss = 0.83856184\n",
      "Iteration 23, loss = 3.56626027\n",
      "Iteration 74, loss = 0.88404410\n",
      "Iteration 24, loss = 2.96523914\n",
      "Iteration 75, loss = 2.09748047\n",
      "Iteration 76, loss = 1.62986887\n",
      "Iteration 25, loss = 3.25471023\n",
      "Iteration 77, loss = 0.97690386\n",
      "Iteration 26, loss = 4.15243581\n",
      "Iteration 78, loss = 1.51790287\n",
      "Iteration 27, loss = 4.07158238\n",
      "Iteration 79, loss = 1.48159505\n",
      "Iteration 28, loss = 4.10048391\n",
      "Iteration 80, loss = 0.75172463\n",
      "Iteration 29, loss = 2.94763240\n",
      "Iteration 81, loss = 0.86178133\n",
      "Iteration 30, loss = 3.36545725\n",
      "Iteration 82, loss = 0.64060118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 3.13738791\n",
      "Iteration 32, loss = 3.25026579\n",
      "Iteration 33, loss = 2.61632132\n",
      "Iteration 34, loss = 2.03244347\n",
      "Iteration 35, loss = 2.14892395\n",
      "Iteration 36, loss = 2.40458976\n",
      "Iteration 37, loss = 3.27735566\n",
      "Iteration 38, loss = 2.48264899\n",
      "Iteration 39, loss = 2.46556895\n",
      "Iteration 40, loss = 2.74434059\n",
      "Iteration 41, loss = 2.03129257\n",
      "Iteration 42, loss = 2.37460805\n",
      "Iteration 43, loss = 1.92201676\n",
      "Iteration 1, loss = 17.03320560\n",
      "Iteration 44, loss = 1.55911658\n",
      "Iteration 2, loss = 14.00831491\n",
      "Iteration 45, loss = 1.51813814\n",
      "Iteration 3, loss = 15.13843336\n",
      "Iteration 46, loss = 1.43278665\n",
      "Iteration 4, loss = 9.55365625\n",
      "Iteration 47, loss = 1.40770063\n",
      "Iteration 5, loss = 6.30530378\n",
      "Iteration 48, loss = 1.79168301\n",
      "Iteration 6, loss = 5.97838339\n",
      "Iteration 49, loss = 1.77405899\n",
      "Iteration 7, loss = 4.66791091\n",
      "Iteration 50, loss = 1.82054377\n",
      "Iteration 8, loss = 4.73020688\n",
      "Iteration 51, loss = 1.82200901\n",
      "Iteration 9, loss = 4.01890171\n",
      "Iteration 52, loss = 1.87364606\n",
      "Iteration 10, loss = 4.21773810\n",
      "Iteration 53, loss = 1.72019360\n",
      "Iteration 11, loss = 9.97110022\n",
      "Iteration 54, loss = 1.14135534\n",
      "Iteration 12, loss = 5.62920489\n",
      "Iteration 55, loss = 1.58184666\n",
      "Iteration 13, loss = 5.64720667\n",
      "Iteration 56, loss = 1.23019598\n",
      "Iteration 14, loss = 4.43435790\n",
      "Iteration 57, loss = 1.95904280\n",
      "Iteration 15, loss = 3.67916013\n",
      "Iteration 58, loss = 1.33249368\n",
      "Iteration 16, loss = 3.76077132\n",
      "Iteration 59, loss = 1.33772810\n",
      "Iteration 17, loss = 3.99914582\n",
      "Iteration 60, loss = 0.97134930\n",
      "Iteration 18, loss = 3.77705631\n",
      "Iteration 61, loss = 1.04828700\n",
      "Iteration 19, loss = 3.61922748\n",
      "Iteration 62, loss = 1.04119214\n",
      "Iteration 20, loss = 2.95095580\n",
      "Iteration 63, loss = 0.80028968\n",
      "Iteration 21, loss = 2.28051876\n",
      "Iteration 64, loss = 1.26905199\n",
      "Iteration 22, loss = 2.41899053\n",
      "Iteration 65, loss = 1.16775209\n",
      "Iteration 23, loss = 1.86965474\n",
      "Iteration 66, loss = 1.13395891\n",
      "Iteration 24, loss = 3.01776884\n",
      "Iteration 67, loss = 1.75438019\n",
      "Iteration 25, loss = 2.59802605\n",
      "Iteration 68, loss = 2.72968694\n",
      "Iteration 26, loss = 1.99022532\n",
      "Iteration 69, loss = 2.04528734\n",
      "Iteration 27, loss = 2.24007496\n",
      "Iteration 70, loss = 1.40733558\n",
      "Iteration 28, loss = 2.53008540\n",
      "Iteration 71, loss = 1.45656105\n",
      "Iteration 29, loss = 2.04726467\n",
      "Iteration 72, loss = 0.78929503\n",
      "Iteration 30, loss = 1.64046668\n",
      "Iteration 73, loss = 0.85314293\n",
      "Iteration 31, loss = 1.77986660\n",
      "Iteration 74, loss = 1.08580556\n",
      "Iteration 32, loss = 1.63246357\n",
      "Iteration 75, loss = 1.44100038\n",
      "Iteration 33, loss = 1.26118736\n",
      "Iteration 76, loss = 1.99931163\n",
      "Iteration 34, loss = 1.68019352\n",
      "Iteration 77, loss = 1.61051306\n",
      "Iteration 35, loss = 1.68938939\n",
      "Iteration 78, loss = 1.51933584\n",
      "Iteration 36, loss = 1.57558465\n",
      "Iteration 79, loss = 1.81425620\n",
      "Iteration 37, loss = 1.64815348\n",
      "Iteration 80, loss = 1.56084999\n",
      "Iteration 38, loss = 1.85967945\n",
      "Iteration 81, loss = 1.28386300\n",
      "Iteration 39, loss = 2.09788041\n",
      "Iteration 82, loss = 1.07648004\n",
      "Iteration 40, loss = 2.62648974\n",
      "Iteration 83, loss = 0.98843696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 1.95512165\n",
      "Iteration 42, loss = 1.62803344\n",
      "Iteration 43, loss = 1.99441863\n",
      "Iteration 44, loss = 1.66706471\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.20953553\n",
      "Iteration 2, loss = 12.55291662\n",
      "Iteration 3, loss = 8.52409124\n",
      "Iteration 4, loss = 7.39707408\n",
      "Iteration 1, loss = 19.53070836\n",
      "Iteration 5, loss = 10.22776619\n",
      "Iteration 2, loss = 14.01199915\n",
      "Iteration 6, loss = 10.70549149\n",
      "Iteration 3, loss = 11.20307682\n",
      "Iteration 7, loss = 11.22562652\n",
      "Iteration 4, loss = 9.15682877\n",
      "Iteration 8, loss = 10.79774039\n",
      "Iteration 5, loss = 6.14901548\n",
      "Iteration 9, loss = 7.15013878\n",
      "Iteration 6, loss = 6.13670163\n",
      "Iteration 10, loss = 6.57763497\n",
      "Iteration 7, loss = 6.07707181\n",
      "Iteration 11, loss = 5.11403613\n",
      "Iteration 8, loss = 4.48748454\n",
      "Iteration 12, loss = 5.97935209\n",
      "Iteration 9, loss = 3.38343093\n",
      "Iteration 13, loss = 6.50294902\n",
      "Iteration 10, loss = 3.38820441\n",
      "Iteration 14, loss = 3.69612739\n",
      "Iteration 11, loss = 3.25389047\n",
      "Iteration 15, loss = 4.04297998\n",
      "Iteration 12, loss = 2.81733235\n",
      "Iteration 16, loss = 4.13525915\n",
      "Iteration 13, loss = 2.32063339\n",
      "Iteration 17, loss = 4.29630729\n",
      "Iteration 14, loss = 3.99780507\n",
      "Iteration 18, loss = 6.48913487\n",
      "Iteration 15, loss = 5.08383836\n",
      "Iteration 19, loss = 5.64636438\n",
      "Iteration 16, loss = 3.36238032\n",
      "Iteration 20, loss = 4.13547996\n",
      "Iteration 17, loss = 3.58422698\n",
      "Iteration 21, loss = 3.62651856\n",
      "Iteration 18, loss = 3.22820215\n",
      "Iteration 22, loss = 3.03810846\n",
      "Iteration 19, loss = 2.79247274\n",
      "Iteration 23, loss = 3.12532909\n",
      "Iteration 20, loss = 2.44129585\n",
      "Iteration 24, loss = 3.34537460\n",
      "Iteration 21, loss = 2.58125403\n",
      "Iteration 25, loss = 2.79241784\n",
      "Iteration 22, loss = 2.40729251\n",
      "Iteration 26, loss = 2.47549714\n",
      "Iteration 23, loss = 2.78828198\n",
      "Iteration 27, loss = 2.87577373\n",
      "Iteration 24, loss = 2.08717525\n",
      "Iteration 28, loss = 2.74818751\n",
      "Iteration 25, loss = 2.01121773\n",
      "Iteration 29, loss = 2.46303923\n",
      "Iteration 30, loss = 2.87245945\n",
      "Iteration 26, loss = 1.66128360\n",
      "Iteration 31, loss = 2.60806654\n",
      "Iteration 27, loss = 1.08877944\n",
      "Iteration 32, loss = 2.78035069\n",
      "Iteration 28, loss = 1.30822280\n",
      "Iteration 33, loss = 2.43228649\n",
      "Iteration 29, loss = 1.09615591\n",
      "Iteration 34, loss = 2.56797343\n",
      "Iteration 30, loss = 1.30394372\n",
      "Iteration 35, loss = 2.78208868\n",
      "Iteration 31, loss = 1.03382122\n",
      "Iteration 36, loss = 2.28285732\n",
      "Iteration 32, loss = 0.95714877\n",
      "Iteration 37, loss = 2.52600652\n",
      "Iteration 33, loss = 0.86918077\n",
      "Iteration 38, loss = 1.93052632\n",
      "Iteration 34, loss = 0.94287206\n",
      "Iteration 39, loss = 1.74351267\n",
      "Iteration 35, loss = 1.23702507\n",
      "Iteration 40, loss = 1.63432121\n",
      "Iteration 36, loss = 1.00224602\n",
      "Iteration 41, loss = 1.39241670\n",
      "Iteration 37, loss = 1.69457479\n",
      "Iteration 42, loss = 1.61092630\n",
      "Iteration 38, loss = 1.82708273\n",
      "Iteration 43, loss = 1.86772473\n",
      "Iteration 39, loss = 1.35726352\n",
      "Iteration 44, loss = 1.83782228\n",
      "Iteration 40, loss = 1.15394265\n",
      "Iteration 45, loss = 1.70488428\n",
      "Iteration 41, loss = 0.97293531\n",
      "Iteration 46, loss = 2.50921919\n",
      "Iteration 42, loss = 1.18068103\n",
      "Iteration 47, loss = 2.49972828\n",
      "Iteration 43, loss = 1.54593164\n",
      "Iteration 48, loss = 2.52429507\n",
      "Iteration 44, loss = 1.46698697\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 2.55241781\n",
      "Iteration 50, loss = 2.55056034\n",
      "Iteration 51, loss = 2.07130820\n",
      "Iteration 52, loss = 2.11706807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.00171007\n",
      "Iteration 2, loss = 14.12990597\n",
      "Iteration 3, loss = 9.60822476\n",
      "Iteration 4, loss = 7.90886501\n",
      "Iteration 1, loss = 18.84520324\n",
      "Iteration 5, loss = 8.54396795\n",
      "Iteration 2, loss = 23.97252720\n",
      "Iteration 6, loss = 7.37454026\n",
      "Iteration 3, loss = 14.87946762\n",
      "Iteration 7, loss = 10.52502474\n",
      "Iteration 4, loss = 10.17991059\n",
      "Iteration 8, loss = 5.26942989\n",
      "Iteration 5, loss = 7.36193940\n",
      "Iteration 9, loss = 4.52770129\n",
      "Iteration 6, loss = 6.72881513\n",
      "Iteration 10, loss = 4.37432926\n",
      "Iteration 7, loss = 8.45906527\n",
      "Iteration 11, loss = 4.45623364\n",
      "Iteration 8, loss = 10.28219398\n",
      "Iteration 12, loss = 4.19201243\n",
      "Iteration 9, loss = 6.08809810\n",
      "Iteration 13, loss = 4.59342877\n",
      "Iteration 10, loss = 5.67082209\n",
      "Iteration 14, loss = 4.60231993\n",
      "Iteration 11, loss = 8.51951673\n",
      "Iteration 15, loss = 3.54860529\n",
      "Iteration 12, loss = 5.85860637\n",
      "Iteration 16, loss = 3.36651877\n",
      "Iteration 13, loss = 4.80391470\n",
      "Iteration 17, loss = 3.28532171\n",
      "Iteration 14, loss = 3.59862439\n",
      "Iteration 18, loss = 4.10061780\n",
      "Iteration 15, loss = 4.16687811\n",
      "Iteration 19, loss = 3.10081165\n",
      "Iteration 16, loss = 3.44108461\n",
      "Iteration 20, loss = 3.00224115\n",
      "Iteration 17, loss = 3.14220627\n",
      "Iteration 21, loss = 1.97914304\n",
      "Iteration 18, loss = 3.51366634\n",
      "Iteration 22, loss = 2.00707280\n",
      "Iteration 19, loss = 3.77541002\n",
      "Iteration 23, loss = 2.04933995\n",
      "Iteration 20, loss = 2.86661303\n",
      "Iteration 24, loss = 1.95651528\n",
      "Iteration 21, loss = 2.81092929\n",
      "Iteration 25, loss = 1.90730019\n",
      "Iteration 22, loss = 1.88067805\n",
      "Iteration 26, loss = 2.60901278\n",
      "Iteration 23, loss = 1.93304494\n",
      "Iteration 27, loss = 1.86351795\n",
      "Iteration 24, loss = 2.44638393\n",
      "Iteration 28, loss = 1.55371801\n",
      "Iteration 25, loss = 1.69293394\n",
      "Iteration 29, loss = 1.62774352\n",
      "Iteration 26, loss = 1.69396140\n",
      "Iteration 30, loss = 1.49602188\n",
      "Iteration 27, loss = 1.77387069\n",
      "Iteration 31, loss = 1.51987917\n",
      "Iteration 28, loss = 1.81392397\n",
      "Iteration 32, loss = 2.04974422\n",
      "Iteration 29, loss = 2.26055758\n",
      "Iteration 33, loss = 1.11501394\n",
      "Iteration 30, loss = 1.55890799\n",
      "Iteration 34, loss = 1.31426015\n",
      "Iteration 31, loss = 1.36051461\n",
      "Iteration 35, loss = 1.37124751\n",
      "Iteration 32, loss = 1.67106191\n",
      "Iteration 36, loss = 1.28736577\n",
      "Iteration 33, loss = 1.31297117\n",
      "Iteration 37, loss = 1.32489511\n",
      "Iteration 34, loss = 1.61096014\n",
      "Iteration 38, loss = 1.22893169\n",
      "Iteration 35, loss = 1.42876197\n",
      "Iteration 39, loss = 1.51934376\n",
      "Iteration 36, loss = 1.67917662\n",
      "Iteration 40, loss = 1.74990398\n",
      "Iteration 37, loss = 2.06121252\n",
      "Iteration 41, loss = 1.68283791\n",
      "Iteration 38, loss = 2.12407387\n",
      "Iteration 42, loss = 1.42213750\n",
      "Iteration 39, loss = 2.51773162\n",
      "Iteration 43, loss = 1.64134669\n",
      "Iteration 40, loss = 2.78588092\n",
      "Iteration 44, loss = 1.83029608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 2.39807851\n",
      "Iteration 42, loss = 2.70483660\n",
      "Iteration 43, loss = 2.70003259\n",
      "Iteration 44, loss = 1.97071255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.85586016\n",
      "Iteration 2, loss = 14.08222076\n",
      "Iteration 3, loss = 12.46677579\n",
      "Iteration 4, loss = 7.00938371\n",
      "Iteration 1, loss = 17.87179801\n",
      "Iteration 5, loss = 6.62570648\n",
      "Iteration 2, loss = 15.99992946\n",
      "Iteration 6, loss = 7.02600755\n",
      "Iteration 3, loss = 11.04008167\n",
      "Iteration 7, loss = 6.97192414\n",
      "Iteration 4, loss = 15.63557454\n",
      "Iteration 8, loss = 5.55887517\n",
      "Iteration 5, loss = 10.54209414\n",
      "Iteration 9, loss = 7.16192551\n",
      "Iteration 6, loss = 6.90773041\n",
      "Iteration 10, loss = 7.01829417\n",
      "Iteration 7, loss = 6.11898893\n",
      "Iteration 11, loss = 5.91556431\n",
      "Iteration 8, loss = 5.92255036\n",
      "Iteration 12, loss = 4.81742835\n",
      "Iteration 9, loss = 9.24826624\n",
      "Iteration 13, loss = 4.94251019\n",
      "Iteration 10, loss = 5.94737144\n",
      "Iteration 14, loss = 4.30868159\n",
      "Iteration 11, loss = 7.66738755\n",
      "Iteration 15, loss = 4.17166148\n",
      "Iteration 12, loss = 6.01720103\n",
      "Iteration 16, loss = 3.14083928\n",
      "Iteration 13, loss = 5.53590260\n",
      "Iteration 17, loss = 3.31204226\n",
      "Iteration 14, loss = 5.51600812\n",
      "Iteration 18, loss = 2.94617930\n",
      "Iteration 15, loss = 5.76826425\n",
      "Iteration 16, loss = 4.04648330Iteration 19, loss = 3.11747724\n",
      "\n",
      "Iteration 17, loss = 3.62292188\n",
      "Iteration 20, loss = 3.45778814\n",
      "Iteration 18, loss = 3.29888414\n",
      "Iteration 21, loss = 3.39517751\n",
      "Iteration 19, loss = 3.79444228\n",
      "Iteration 22, loss = 3.76603956\n",
      "Iteration 20, loss = 2.93391240\n",
      "Iteration 23, loss = 2.86339668\n",
      "Iteration 21, loss = 3.74205254\n",
      "Iteration 24, loss = 2.75257137\n",
      "Iteration 22, loss = 2.74251242\n",
      "Iteration 25, loss = 3.12259904\n",
      "Iteration 23, loss = 2.49741677\n",
      "Iteration 26, loss = 2.68829897\n",
      "Iteration 24, loss = 2.71556197\n",
      "Iteration 27, loss = 3.72531365\n",
      "Iteration 25, loss = 2.46217680\n",
      "Iteration 28, loss = 4.18406937\n",
      "Iteration 26, loss = 2.03917288\n",
      "Iteration 29, loss = 2.75897359\n",
      "Iteration 27, loss = 2.20361038\n",
      "Iteration 30, loss = 2.46194257\n",
      "Iteration 28, loss = 2.83287493\n",
      "Iteration 31, loss = 1.69210880\n",
      "Iteration 29, loss = 2.05350267\n",
      "Iteration 32, loss = 1.89295176\n",
      "Iteration 30, loss = 1.55942854\n",
      "Iteration 33, loss = 1.92399958\n",
      "Iteration 31, loss = 2.25887485\n",
      "Iteration 34, loss = 1.45524245\n",
      "Iteration 32, loss = 2.26646114\n",
      "Iteration 35, loss = 1.64945402\n",
      "Iteration 33, loss = 2.60680664\n",
      "Iteration 36, loss = 1.54418178\n",
      "Iteration 34, loss = 2.56274390\n",
      "Iteration 37, loss = 1.51599050\n",
      "Iteration 35, loss = 2.85265605\n",
      "Iteration 38, loss = 1.13361012\n",
      "Iteration 36, loss = 2.59038326\n",
      "Iteration 39, loss = 1.37810303\n",
      "Iteration 37, loss = 2.82650158\n",
      "Iteration 40, loss = 1.17489843\n",
      "Iteration 38, loss = 3.35031726\n",
      "Iteration 41, loss = 1.48078938\n",
      "Iteration 39, loss = 2.68437533\n",
      "Iteration 42, loss = 1.20777852\n",
      "Iteration 40, loss = 2.09637107\n",
      "Iteration 43, loss = 1.67970846\n",
      "Iteration 41, loss = 2.54611768\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 1.83712238\n",
      "Iteration 45, loss = 2.06304059\n",
      "Iteration 46, loss = 1.51627993\n",
      "Iteration 47, loss = 1.43578313\n",
      "Iteration 48, loss = 1.65995177\n",
      "Iteration 49, loss = 1.57514567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.48756240\n",
      "Iteration 2, loss = 14.48429211\n",
      "Iteration 3, loss = 11.43211112\n",
      "Iteration 4, loss = 7.20351713\n",
      "Iteration 5, loss = 11.33803949\n",
      "Iteration 6, loss = 7.76096055\n",
      "Iteration 1, loss = 14.76927314\n",
      "Iteration 7, loss = 8.05112204\n",
      "Iteration 2, loss = 11.40347409\n",
      "Iteration 8, loss = 6.57379460\n",
      "Iteration 3, loss = 10.88749206\n",
      "Iteration 9, loss = 5.63274411\n",
      "Iteration 4, loss = 7.22781579\n",
      "Iteration 10, loss = 5.49684285\n",
      "Iteration 5, loss = 8.22881113\n",
      "Iteration 11, loss = 5.21953288\n",
      "Iteration 6, loss = 7.32256901\n",
      "Iteration 12, loss = 6.07582347\n",
      "Iteration 7, loss = 8.50495373\n",
      "Iteration 13, loss = 4.40996494\n",
      "Iteration 8, loss = 5.58356229\n",
      "Iteration 14, loss = 4.09128692\n",
      "Iteration 9, loss = 5.37486602\n",
      "Iteration 15, loss = 4.15640554\n",
      "Iteration 10, loss = 5.70270694\n",
      "Iteration 16, loss = 3.84776942\n",
      "Iteration 11, loss = 5.06410956\n",
      "Iteration 17, loss = 3.41770974\n",
      "Iteration 12, loss = 5.44534140\n",
      "Iteration 18, loss = 3.09041352\n",
      "Iteration 13, loss = 5.02718864\n",
      "Iteration 19, loss = 2.53698665\n",
      "Iteration 14, loss = 3.96403487\n",
      "Iteration 20, loss = 2.72281216\n",
      "Iteration 15, loss = 3.94169061\n",
      "Iteration 21, loss = 2.62617338\n",
      "Iteration 16, loss = 5.28659177\n",
      "Iteration 22, loss = 3.15045707\n",
      "Iteration 17, loss = 3.72616037\n",
      "Iteration 18, loss = 2.87312039\n",
      "Iteration 23, loss = 2.36413131\n",
      "Iteration 19, loss = 2.88820858\n",
      "Iteration 24, loss = 1.95685106\n",
      "Iteration 20, loss = 4.16086557\n",
      "Iteration 25, loss = 1.77441783\n",
      "Iteration 21, loss = 3.00956656\n",
      "Iteration 26, loss = 1.55207071\n",
      "Iteration 22, loss = 3.62484166\n",
      "Iteration 27, loss = 1.56830871\n",
      "Iteration 23, loss = 2.23641090\n",
      "Iteration 28, loss = 1.71505887\n",
      "Iteration 24, loss = 3.04714029\n",
      "Iteration 29, loss = 2.46635942\n",
      "Iteration 25, loss = 3.05132662\n",
      "Iteration 30, loss = 1.74091299\n",
      "Iteration 26, loss = 2.76824610\n",
      "Iteration 31, loss = 2.31843574\n",
      "Iteration 27, loss = 2.84795839\n",
      "Iteration 32, loss = 1.85879860\n",
      "Iteration 28, loss = 2.32657142\n",
      "Iteration 33, loss = 1.24493006\n",
      "Iteration 29, loss = 2.78340556\n",
      "Iteration 34, loss = 1.58458045\n",
      "Iteration 30, loss = 2.51318942\n",
      "Iteration 35, loss = 1.40075560\n",
      "Iteration 31, loss = 2.47959351\n",
      "Iteration 36, loss = 1.63333963\n",
      "Iteration 32, loss = 2.30905204\n",
      "Iteration 37, loss = 1.78134609\n",
      "Iteration 33, loss = 2.35155108\n",
      "Iteration 38, loss = 1.70587322\n",
      "Iteration 34, loss = 1.88617061\n",
      "Iteration 39, loss = 1.70841802\n",
      "Iteration 35, loss = 1.93000972\n",
      "Iteration 40, loss = 2.32954161\n",
      "Iteration 36, loss = 1.98728339\n",
      "Iteration 41, loss = 1.56217904\n",
      "Iteration 37, loss = 1.85920731\n",
      "Iteration 42, loss = 1.97950609\n",
      "Iteration 38, loss = 2.17657180\n",
      "Iteration 43, loss = 1.91242398\n",
      "Iteration 39, loss = 2.51596845\n",
      "Iteration 44, loss = 2.82179310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 2.38068799\n",
      "Iteration 41, loss = 1.58522494\n",
      "Iteration 42, loss = 3.23571538\n",
      "Iteration 43, loss = 1.71357181\n",
      "Iteration 44, loss = 1.10167636\n",
      "Iteration 45, loss = 1.71691405\n",
      "Iteration 46, loss = 1.33277603\n",
      "Iteration 47, loss = 1.86124467\n",
      "Iteration 48, loss = 2.24311095\n",
      "Iteration 49, loss = 2.21981200\n",
      "Iteration 50, loss = 2.80478134\n",
      "Iteration 51, loss = 1.51360782\n",
      "Iteration 52, loss = 1.81355125\n",
      "Iteration 1, loss = 18.51076416\n",
      "Iteration 53, loss = 1.10006239\n",
      "Iteration 2, loss = 14.91297395\n",
      "Iteration 54, loss = 1.66668680\n",
      "Iteration 3, loss = 15.29075344\n",
      "Iteration 55, loss = 1.17203986\n",
      "Iteration 4, loss = 11.19959669\n",
      "Iteration 56, loss = 1.08484518\n",
      "Iteration 5, loss = 8.25359023\n",
      "Iteration 57, loss = 1.35465515\n",
      "Iteration 6, loss = 7.64245185\n",
      "Iteration 58, loss = 1.08481451\n",
      "Iteration 7, loss = 7.37360400\n",
      "Iteration 59, loss = 1.19516383\n",
      "Iteration 8, loss = 7.21069582\n",
      "Iteration 60, loss = 1.03978586\n",
      "Iteration 9, loss = 6.91232279\n",
      "Iteration 61, loss = 1.40727626\n",
      "Iteration 10, loss = 5.79488258\n",
      "Iteration 62, loss = 1.86126613\n",
      "Iteration 11, loss = 6.64181787\n",
      "Iteration 63, loss = 1.60842249\n",
      "Iteration 12, loss = 4.40139885\n",
      "Iteration 64, loss = 1.09415800\n",
      "Iteration 13, loss = 3.62288136\n",
      "Iteration 65, loss = 1.51138809\n",
      "Iteration 14, loss = 3.51540908\n",
      "Iteration 66, loss = 1.65731118\n",
      "Iteration 15, loss = 3.85472437\n",
      "Iteration 67, loss = 1.55727545\n",
      "Iteration 16, loss = 3.85171018\n",
      "Iteration 68, loss = 2.64280177\n",
      "Iteration 17, loss = 3.23116971\n",
      "Iteration 69, loss = 3.44354354\n",
      "Iteration 18, loss = 2.97178573\n",
      "Iteration 70, loss = 2.91833447\n",
      "Iteration 19, loss = 3.25587060\n",
      "Iteration 71, loss = 2.15243869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 3.38614065\n",
      "Iteration 21, loss = 2.45524624\n",
      "Iteration 22, loss = 2.35761540\n",
      "Iteration 23, loss = 2.66331762\n",
      "Iteration 24, loss = 2.76683460\n",
      "Iteration 25, loss = 2.58100693\n",
      "Iteration 26, loss = 2.43725132\n",
      "Iteration 27, loss = 2.05910017\n",
      "Iteration 28, loss = 2.54506029\n",
      "Iteration 29, loss = 1.90127739\n",
      "Iteration 30, loss = 1.93275530\n",
      "Iteration 31, loss = 1.98312006\n",
      "Iteration 32, loss = 2.82226264\n",
      "Iteration 1, loss = 19.30966883\n",
      "Iteration 33, loss = 2.51768382\n",
      "Iteration 2, loss = 16.56001209\n",
      "Iteration 34, loss = 2.43000871\n",
      "Iteration 3, loss = 11.05049722\n",
      "Iteration 35, loss = 2.28076543\n",
      "Iteration 4, loss = 7.17471083\n",
      "Iteration 36, loss = 1.59318287\n",
      "Iteration 5, loss = 6.48615829\n",
      "Iteration 37, loss = 2.19628420\n",
      "Iteration 6, loss = 6.46629086\n",
      "Iteration 38, loss = 2.54452045\n",
      "Iteration 7, loss = 7.64338043\n",
      "Iteration 39, loss = 2.73627013\n",
      "Iteration 8, loss = 8.03838195\n",
      "Iteration 40, loss = 2.94320098\n",
      "Iteration 9, loss = 5.85142102\n",
      "Iteration 41, loss = 2.59247541\n",
      "Iteration 10, loss = 5.58033657\n",
      "Iteration 42, loss = 3.02458296\n",
      "Iteration 11, loss = 5.13641868\n",
      "Iteration 43, loss = 2.36369872\n",
      "Iteration 12, loss = 4.30161931\n",
      "Iteration 44, loss = 1.79588375\n",
      "Iteration 13, loss = 5.35862392\n",
      "Iteration 45, loss = 2.74904803\n",
      "Iteration 14, loss = 4.12415827\n",
      "Iteration 46, loss = 2.11812423\n",
      "Iteration 15, loss = 4.18021359\n",
      "Iteration 47, loss = 2.10978947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 4.92317634\n",
      "Iteration 17, loss = 4.85357523\n",
      "Iteration 18, loss = 2.73457125\n",
      "Iteration 19, loss = 2.55491172\n",
      "Iteration 20, loss = 2.83886466\n",
      "Iteration 21, loss = 3.57916924\n",
      "Iteration 22, loss = 2.99481247\n",
      "Iteration 23, loss = 2.77086342\n",
      "Iteration 24, loss = 2.62404599\n",
      "Iteration 25, loss = 2.39036462\n",
      "Iteration 26, loss = 2.52335819\n",
      "Iteration 27, loss = 2.28191385\n",
      "Iteration 28, loss = 2.29626905\n",
      "Iteration 1, loss = 18.07633329\n",
      "Iteration 29, loss = 2.11010523\n",
      "Iteration 2, loss = 13.07052696\n",
      "Iteration 30, loss = 1.99209026\n",
      "Iteration 3, loss = 9.12965338\n",
      "Iteration 31, loss = 1.74073041\n",
      "Iteration 4, loss = 11.78902812\n",
      "Iteration 32, loss = 1.85115563\n",
      "Iteration 5, loss = 8.85470349\n",
      "Iteration 33, loss = 1.26520529\n",
      "Iteration 6, loss = 10.34936624\n",
      "Iteration 34, loss = 1.65145922\n",
      "Iteration 7, loss = 7.71807471\n",
      "Iteration 35, loss = 1.70442706\n",
      "Iteration 8, loss = 8.82674065\n",
      "Iteration 36, loss = 1.25425288\n",
      "Iteration 9, loss = 7.33796376\n",
      "Iteration 37, loss = 1.46318968\n",
      "Iteration 10, loss = 7.05757187\n",
      "Iteration 38, loss = 0.94463225\n",
      "Iteration 11, loss = 6.40900514\n",
      "Iteration 39, loss = 1.44942183\n",
      "Iteration 12, loss = 6.27132165\n",
      "Iteration 40, loss = 1.42136698\n",
      "Iteration 13, loss = 5.46434410\n",
      "Iteration 41, loss = 1.60939451\n",
      "Iteration 14, loss = 4.78751987\n",
      "Iteration 42, loss = 1.68713986\n",
      "Iteration 15, loss = 5.35239637\n",
      "Iteration 43, loss = 1.15523266\n",
      "Iteration 16, loss = 3.74185952\n",
      "Iteration 44, loss = 1.63813565\n",
      "Iteration 17, loss = 3.77665374\n",
      "Iteration 45, loss = 1.46448924\n",
      "Iteration 18, loss = 3.43796391\n",
      "Iteration 46, loss = 1.30012541\n",
      "Iteration 19, loss = 4.19723467\n",
      "Iteration 47, loss = 0.82728117\n",
      "Iteration 20, loss = 3.53687701\n",
      "Iteration 48, loss = 1.04312993\n",
      "Iteration 21, loss = 3.53407003\n",
      "Iteration 49, loss = 0.87067958\n",
      "Iteration 22, loss = 3.30596559\n",
      "Iteration 50, loss = 0.76780679\n",
      "Iteration 23, loss = 3.26323824\n",
      "Iteration 51, loss = 0.84438772\n",
      "Iteration 24, loss = 3.19553834\n",
      "Iteration 52, loss = 0.82934260\n",
      "Iteration 25, loss = 3.14945196\n",
      "Iteration 53, loss = 1.30494653\n",
      "Iteration 26, loss = 2.41771440\n",
      "Iteration 54, loss = 0.93934144\n",
      "Iteration 27, loss = 2.95684522\n",
      "Iteration 55, loss = 1.09936330\n",
      "Iteration 28, loss = 2.70498187\n",
      "Iteration 56, loss = 1.40725889\n",
      "Iteration 29, loss = 3.29480249\n",
      "Iteration 57, loss = 1.43627878\n",
      "Iteration 30, loss = 2.59506500\n",
      "Iteration 31, loss = 2.66013222\n",
      "Iteration 58, loss = 1.13524839\n",
      "Iteration 32, loss = 2.88957687\n",
      "Iteration 59, loss = 1.03459052\n",
      "Iteration 33, loss = 2.11187518\n",
      "Iteration 60, loss = 0.80516931\n",
      "Iteration 34, loss = 2.10628291\n",
      "Iteration 61, loss = 0.80100136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 1.52892382\n",
      "Iteration 36, loss = 1.41570625\n",
      "Iteration 37, loss = 1.91076960\n",
      "Iteration 38, loss = 1.98291207\n",
      "Iteration 39, loss = 1.86480701\n",
      "Iteration 40, loss = 1.38509716\n",
      "Iteration 41, loss = 1.65067005\n",
      "Iteration 42, loss = 1.89072618\n",
      "Iteration 43, loss = 1.76903662\n",
      "Iteration 44, loss = 1.34355726\n",
      "Iteration 45, loss = 1.18756761\n",
      "Iteration 46, loss = 1.27703727\n",
      "Iteration 47, loss = 1.57982073\n",
      "Iteration 1, loss = 16.98348071\n",
      "Iteration 48, loss = 1.73707651\n",
      "Iteration 2, loss = 14.67721393\n",
      "Iteration 49, loss = 1.34667859\n",
      "Iteration 3, loss = 11.54525717\n",
      "Iteration 50, loss = 1.24868385\n",
      "Iteration 4, loss = 9.19794962\n",
      "Iteration 51, loss = 0.88336885\n",
      "Iteration 5, loss = 6.51973596\n",
      "Iteration 6, loss = 5.71859279\n",
      "Iteration 52, loss = 0.95918807\n",
      "Iteration 7, loss = 4.62473101\n",
      "Iteration 53, loss = 0.96968800\n",
      "Iteration 8, loss = 3.83789137\n",
      "Iteration 54, loss = 0.93020457\n",
      "Iteration 9, loss = 4.37111096\n",
      "Iteration 55, loss = 0.97870765\n",
      "Iteration 10, loss = 4.21140057\n",
      "Iteration 56, loss = 1.24576221\n",
      "Iteration 11, loss = 3.74530382\n",
      "Iteration 57, loss = 0.75821656\n",
      "Iteration 12, loss = 4.27708899\n",
      "Iteration 58, loss = 1.06001753\n",
      "Iteration 13, loss = 3.29403366\n",
      "Iteration 59, loss = 1.54216171\n",
      "Iteration 14, loss = 3.32196848\n",
      "Iteration 60, loss = 1.13221105\n",
      "Iteration 15, loss = 2.70421789\n",
      "Iteration 61, loss = 0.73691915\n",
      "Iteration 16, loss = 2.03374842\n",
      "Iteration 62, loss = 1.16895688\n",
      "Iteration 17, loss = 2.48301545\n",
      "Iteration 63, loss = 1.21846304\n",
      "Iteration 18, loss = 2.50874355\n",
      "Iteration 64, loss = 1.31061993\n",
      "Iteration 19, loss = 2.15219656\n",
      "Iteration 65, loss = 1.38683839\n",
      "Iteration 20, loss = 1.81370338\n",
      "Iteration 66, loss = 1.34963460\n",
      "Iteration 21, loss = 2.44510694\n",
      "Iteration 67, loss = 0.95636056\n",
      "Iteration 22, loss = 1.94062523\n",
      "Iteration 68, loss = 1.34586434\n",
      "Iteration 23, loss = 1.86881501\n",
      "Iteration 69, loss = 0.82528898\n",
      "Iteration 24, loss = 1.33184413\n",
      "Iteration 70, loss = 0.84408068\n",
      "Iteration 25, loss = 1.76479108\n",
      "Iteration 71, loss = 0.72310927\n",
      "Iteration 26, loss = 1.74710545\n",
      "Iteration 72, loss = 0.53948034\n",
      "Iteration 27, loss = 1.53183952\n",
      "Iteration 73, loss = 0.61441803\n",
      "Iteration 28, loss = 1.64520948\n",
      "Iteration 74, loss = 0.58780027\n",
      "Iteration 29, loss = 1.47305627\n",
      "Iteration 75, loss = 0.50368713\n",
      "Iteration 30, loss = 1.74513449\n",
      "Iteration 76, loss = 0.34412059\n",
      "Iteration 31, loss = 1.59108104\n",
      "Iteration 77, loss = 0.52379336\n",
      "Iteration 32, loss = 2.71799480\n",
      "Iteration 78, loss = 0.43781657\n",
      "Iteration 33, loss = 2.06464368\n",
      "Iteration 79, loss = 0.49222464\n",
      "Iteration 34, loss = 1.04584365\n",
      "Iteration 80, loss = 0.50791713\n",
      "Iteration 35, loss = 1.47895082\n",
      "Iteration 81, loss = 0.60581860\n",
      "Iteration 36, loss = 1.52934850\n",
      "Iteration 82, loss = 0.56383384\n",
      "Iteration 37, loss = 1.50606522\n",
      "Iteration 83, loss = 0.70610918\n",
      "Iteration 38, loss = 1.53336712\n",
      "Iteration 84, loss = 0.82843039\n",
      "Iteration 39, loss = 1.74093104\n",
      "Iteration 85, loss = 0.73234621\n",
      "Iteration 40, loss = 0.96513640\n",
      "Iteration 86, loss = 0.48819910\n",
      "Iteration 41, loss = 1.62191223\n",
      "Iteration 87, loss = 0.36556234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 1.10858006\n",
      "Iteration 43, loss = 1.11535874\n",
      "Iteration 44, loss = 1.22597881\n",
      "Iteration 45, loss = 0.85502215\n",
      "Iteration 46, loss = 0.99319039\n",
      "Iteration 47, loss = 1.40007232\n",
      "Iteration 48, loss = 1.30804454\n",
      "Iteration 49, loss = 1.96355580\n",
      "Iteration 50, loss = 1.09171328\n",
      "Iteration 51, loss = 0.93843637\n",
      "Iteration 52, loss = 0.89135122\n",
      "Iteration 53, loss = 1.08644648\n",
      "Iteration 54, loss = 0.94459574\n",
      "Iteration 1, loss = 18.26195116\n",
      "Iteration 55, loss = 0.83440462\n",
      "Iteration 2, loss = 15.36466140\n",
      "Iteration 56, loss = 0.81979505\n",
      "Iteration 3, loss = 10.95846766\n",
      "Iteration 57, loss = 1.65008693\n",
      "Iteration 4, loss = 9.00375094\n",
      "Iteration 58, loss = 1.05162659\n",
      "Iteration 5, loss = 9.40252334\n",
      "Iteration 59, loss = 1.31113227\n",
      "Iteration 6, loss = 8.46701300\n",
      "Iteration 60, loss = 1.32631174\n",
      "Iteration 7, loss = 9.71109815\n",
      "Iteration 61, loss = 1.03553704\n",
      "Iteration 8, loss = 6.59405073\n",
      "Iteration 62, loss = 1.17707789\n",
      "Iteration 9, loss = 5.68418513\n",
      "Iteration 63, loss = 0.66184830\n",
      "Iteration 10, loss = 5.34568565\n",
      "Iteration 64, loss = 0.90193657\n",
      "Iteration 11, loss = 4.18920731\n",
      "Iteration 65, loss = 0.73505490\n",
      "Iteration 12, loss = 3.77179735\n",
      "Iteration 66, loss = 0.88634600\n",
      "Iteration 13, loss = 3.92509713\n",
      "Iteration 67, loss = 0.56250781\n",
      "Iteration 14, loss = 3.93794137\n",
      "Iteration 68, loss = 0.76093377\n",
      "Iteration 15, loss = 3.50667926\n",
      "Iteration 69, loss = 0.87540703\n",
      "Iteration 16, loss = 3.63662077\n",
      "Iteration 70, loss = 0.84419599\n",
      "Iteration 17, loss = 3.55341532\n",
      "Iteration 71, loss = 0.91939412\n",
      "Iteration 18, loss = 3.49881821\n",
      "Iteration 72, loss = 1.06827567\n",
      "Iteration 19, loss = 3.56921614\n",
      "Iteration 73, loss = 0.57091484\n",
      "Iteration 20, loss = 2.59102422\n",
      "Iteration 74, loss = 1.92622749\n",
      "Iteration 21, loss = 2.73049354\n",
      "Iteration 75, loss = 1.45474071\n",
      "Iteration 22, loss = 2.69317329\n",
      "Iteration 76, loss = 1.20071513\n",
      "Iteration 23, loss = 3.34794341\n",
      "Iteration 77, loss = 0.71945501\n",
      "Iteration 24, loss = 2.71712203\n",
      "Iteration 78, loss = 0.78338273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 2.58619846\n",
      "Iteration 26, loss = 3.92667662\n",
      "Iteration 27, loss = 2.78493480\n",
      "Iteration 28, loss = 2.43919743\n",
      "Iteration 29, loss = 2.23599407\n",
      "Iteration 30, loss = 1.68912327\n",
      "Iteration 31, loss = 2.01489609\n",
      "Iteration 32, loss = 1.46047236\n",
      "Iteration 33, loss = 1.62043437\n",
      "Iteration 34, loss = 1.46024838\n",
      "Iteration 35, loss = 1.56038915\n",
      "Iteration 36, loss = 2.10471512\n",
      "Iteration 37, loss = 1.71086520\n",
      "Iteration 1, loss = 15.63613985\n",
      "Iteration 38, loss = 1.33637431\n",
      "Iteration 2, loss = 16.44351798\n",
      "Iteration 39, loss = 1.10592384\n",
      "Iteration 3, loss = 11.87853016\n",
      "Iteration 40, loss = 1.21248892\n",
      "Iteration 4, loss = 8.85062921\n",
      "Iteration 41, loss = 1.17198168\n",
      "Iteration 5, loss = 8.39631696\n",
      "Iteration 42, loss = 1.73101254\n",
      "Iteration 6, loss = 6.49266648\n",
      "Iteration 43, loss = 2.57510397\n",
      "Iteration 7, loss = 9.03582827\n",
      "Iteration 44, loss = 2.49447007\n",
      "Iteration 8, loss = 8.35886329\n",
      "Iteration 45, loss = 2.12434974\n",
      "Iteration 9, loss = 7.17491736\n",
      "Iteration 46, loss = 2.35601545\n",
      "Iteration 10, loss = 5.24163141\n",
      "Iteration 47, loss = 2.81877191\n",
      "Iteration 11, loss = 3.73034929\n",
      "Iteration 48, loss = 3.88082721\n",
      "Iteration 12, loss = 4.03324999\n",
      "Iteration 49, loss = 3.23506417\n",
      "Iteration 13, loss = 4.78909771\n",
      "Iteration 50, loss = 2.27811140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 4.46200520\n",
      "Iteration 15, loss = 4.89791307\n",
      "Iteration 16, loss = 6.00220446\n",
      "Iteration 17, loss = 4.54841966\n",
      "Iteration 18, loss = 3.73840833\n",
      "Iteration 19, loss = 3.02445016\n",
      "Iteration 20, loss = 2.93939849\n",
      "Iteration 21, loss = 2.82711595\n",
      "Iteration 22, loss = 2.61318083\n",
      "Iteration 23, loss = 2.42064781\n",
      "Iteration 24, loss = 2.19194476\n",
      "Iteration 25, loss = 2.09416089\n",
      "Iteration 26, loss = 2.15686612\n",
      "Iteration 27, loss = 2.97939600\n",
      "Iteration 1, loss = 16.88593834\n",
      "Iteration 28, loss = 3.95180711\n",
      "Iteration 2, loss = 12.96351255\n",
      "Iteration 3, loss = 15.74581884\n",
      "Iteration 29, loss = 2.74907487\n",
      "Iteration 4, loss = 10.26285631\n",
      "Iteration 30, loss = 1.85240376\n",
      "Iteration 5, loss = 7.72103029\n",
      "Iteration 31, loss = 2.02065524\n",
      "Iteration 6, loss = 8.23757636\n",
      "Iteration 32, loss = 1.91895201\n",
      "Iteration 7, loss = 8.79399950\n",
      "Iteration 33, loss = 1.69000497\n",
      "Iteration 8, loss = 9.11331158\n",
      "Iteration 34, loss = 1.55635496\n",
      "Iteration 9, loss = 5.70252733\n",
      "Iteration 35, loss = 1.57152290\n",
      "Iteration 10, loss = 5.94389994\n",
      "Iteration 36, loss = 2.19461177\n",
      "Iteration 11, loss = 7.49682811\n",
      "Iteration 37, loss = 2.46189534\n",
      "Iteration 12, loss = 5.32193983\n",
      "Iteration 38, loss = 2.32760418\n",
      "Iteration 13, loss = 5.00359405\n",
      "Iteration 39, loss = 2.48074735\n",
      "Iteration 14, loss = 4.36983525\n",
      "Iteration 40, loss = 1.38340958\n",
      "Iteration 15, loss = 4.55632383\n",
      "Iteration 41, loss = 1.65473283\n",
      "Iteration 16, loss = 3.81666595\n",
      "Iteration 42, loss = 1.57973737\n",
      "Iteration 17, loss = 3.87143836\n",
      "Iteration 43, loss = 1.59980280\n",
      "Iteration 18, loss = 3.82381203\n",
      "Iteration 44, loss = 1.56657954\n",
      "Iteration 19, loss = 3.73899401\n",
      "Iteration 45, loss = 1.14828745\n",
      "Iteration 20, loss = 3.29568268\n",
      "Iteration 46, loss = 1.12148975\n",
      "Iteration 21, loss = 3.51241791\n",
      "Iteration 47, loss = 1.29627297\n",
      "Iteration 22, loss = 2.81725931\n",
      "Iteration 48, loss = 1.08294563\n",
      "Iteration 23, loss = 2.90109310\n",
      "Iteration 49, loss = 1.59062738\n",
      "Iteration 24, loss = 2.55970154\n",
      "Iteration 50, loss = 1.65296141\n",
      "Iteration 25, loss = 2.23853486\n",
      "Iteration 51, loss = 1.52759918\n",
      "Iteration 26, loss = 2.51198284\n",
      "Iteration 52, loss = 1.44145389\n",
      "Iteration 27, loss = 2.86620146\n",
      "Iteration 53, loss = 1.22935488\n",
      "Iteration 28, loss = 3.14546098\n",
      "Iteration 54, loss = 1.11749048\n",
      "Iteration 29, loss = 3.73695377\n",
      "Iteration 55, loss = 0.98139994\n",
      "Iteration 30, loss = 2.84213890\n",
      "Iteration 56, loss = 1.02119072\n",
      "Iteration 31, loss = 3.23436312\n",
      "Iteration 57, loss = 1.05064063\n",
      "Iteration 32, loss = 2.35314391\n",
      "Iteration 58, loss = 1.41866889\n",
      "Iteration 33, loss = 2.60521363\n",
      "Iteration 59, loss = 0.95190991\n",
      "Iteration 34, loss = 2.69407954\n",
      "Iteration 60, loss = 1.35952034\n",
      "Iteration 35, loss = 2.08091652\n",
      "Iteration 61, loss = 1.21514067\n",
      "Iteration 36, loss = 1.99971693\n",
      "Iteration 62, loss = 1.67029522\n",
      "Iteration 37, loss = 2.34903957\n",
      "Iteration 63, loss = 1.21904444\n",
      "Iteration 38, loss = 2.34670693\n",
      "Iteration 64, loss = 1.52425993\n",
      "Iteration 39, loss = 1.69614722\n",
      "Iteration 65, loss = 1.58606484\n",
      "Iteration 40, loss = 1.76884696\n",
      "Iteration 66, loss = 1.57502873\n",
      "Iteration 41, loss = 1.94162243\n",
      "Iteration 67, loss = 1.77804130\n",
      "Iteration 42, loss = 2.34451893\n",
      "Iteration 68, loss = 2.29597528\n",
      "Iteration 43, loss = 2.18192724\n",
      "Iteration 69, loss = 1.32813904\n",
      "Iteration 44, loss = 2.16629880\n",
      "Iteration 70, loss = 1.29892063\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 2.36675263\n",
      "Iteration 46, loss = 1.83492175\n",
      "Iteration 47, loss = 2.05707739\n",
      "Iteration 48, loss = 2.44797085\n",
      "Iteration 49, loss = 2.17051287\n",
      "Iteration 50, loss = 1.56875288\n",
      "Iteration 51, loss = 1.57370294\n",
      "Iteration 52, loss = 1.68308463\n",
      "Iteration 53, loss = 1.83586311\n",
      "Iteration 54, loss = 1.28939920\n",
      "Iteration 55, loss = 1.52867251\n",
      "Iteration 56, loss = 1.68001243\n",
      "Iteration 57, loss = 1.79043213\n",
      "Iteration 1, loss = 17.02075153\n",
      "Iteration 58, loss = 1.52840910\n",
      "Iteration 2, loss = 13.07127296\n",
      "Iteration 59, loss = 1.55772970\n",
      "Iteration 3, loss = 13.94887972\n",
      "Iteration 60, loss = 1.77327869\n",
      "Iteration 4, loss = 8.26025679\n",
      "Iteration 61, loss = 2.03479793\n",
      "Iteration 5, loss = 6.61369960\n",
      "Iteration 62, loss = 1.70916523\n",
      "Iteration 6, loss = 7.41055603\n",
      "Iteration 63, loss = 1.98480654\n",
      "Iteration 7, loss = 7.63873881\n",
      "Iteration 64, loss = 1.85911534\n",
      "Iteration 8, loss = 6.72086460\n",
      "Iteration 9, loss = 8.06579077\n",
      "Iteration 65, loss = 1.44061749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 8.53351577\n",
      "Iteration 11, loss = 5.23467792\n",
      "Iteration 12, loss = 4.98540856\n",
      "Iteration 13, loss = 7.27604471\n",
      "Iteration 14, loss = 5.55463425\n",
      "Iteration 15, loss = 4.10558009\n",
      "Iteration 16, loss = 3.41338819\n",
      "Iteration 17, loss = 3.74418819\n",
      "Iteration 18, loss = 3.01963093\n",
      "Iteration 19, loss = 2.92842788\n",
      "Iteration 20, loss = 3.21124880\n",
      "Iteration 21, loss = 3.54070147\n",
      "Iteration 1, loss = 16.86086733\n",
      "Iteration 22, loss = 2.50803394\n",
      "Iteration 2, loss = 15.53023556\n",
      "Iteration 23, loss = 2.64473066\n",
      "Iteration 3, loss = 17.90339409\n",
      "Iteration 24, loss = 3.62470390\n",
      "Iteration 4, loss = 14.61779399\n",
      "Iteration 25, loss = 3.66142194\n",
      "Iteration 5, loss = 9.79899807\n",
      "Iteration 26, loss = 3.15009459\n",
      "Iteration 6, loss = 8.11353841\n",
      "Iteration 27, loss = 3.48308369\n",
      "Iteration 7, loss = 6.27810734\n",
      "Iteration 28, loss = 2.63318267\n",
      "Iteration 8, loss = 7.91335962\n",
      "Iteration 29, loss = 2.78256445\n",
      "Iteration 9, loss = 5.79286709\n",
      "Iteration 30, loss = 1.92564080\n",
      "Iteration 10, loss = 5.03157875\n",
      "Iteration 31, loss = 2.15007253\n",
      "Iteration 11, loss = 5.60424504\n",
      "Iteration 32, loss = 1.99297914\n",
      "Iteration 12, loss = 7.36382595\n",
      "Iteration 33, loss = 1.82070360\n",
      "Iteration 13, loss = 4.82778487\n",
      "Iteration 34, loss = 1.99997897\n",
      "Iteration 14, loss = 4.34770613\n",
      "Iteration 35, loss = 2.01458398\n",
      "Iteration 15, loss = 3.50677531\n",
      "Iteration 36, loss = 1.80982331\n",
      "Iteration 16, loss = 3.30700370\n",
      "Iteration 37, loss = 1.39889209\n",
      "Iteration 17, loss = 3.43692521\n",
      "Iteration 38, loss = 1.41879538\n",
      "Iteration 18, loss = 3.38063920\n",
      "Iteration 39, loss = 1.77274903\n",
      "Iteration 19, loss = 5.60331485\n",
      "Iteration 40, loss = 1.65319708\n",
      "Iteration 20, loss = 4.31380608\n",
      "Iteration 41, loss = 2.22143093\n",
      "Iteration 21, loss = 2.78693116\n",
      "Iteration 42, loss = 2.49391910\n",
      "Iteration 22, loss = 2.99599971\n",
      "Iteration 43, loss = 2.33248174\n",
      "Iteration 23, loss = 2.15494472\n",
      "Iteration 44, loss = 3.34761332\n",
      "Iteration 24, loss = 2.79977293\n",
      "Iteration 45, loss = 2.29475221\n",
      "Iteration 25, loss = 2.23344984\n",
      "Iteration 46, loss = 1.90770730\n",
      "Iteration 26, loss = 2.20316533\n",
      "Iteration 47, loss = 1.57169141\n",
      "Iteration 27, loss = 1.60337115\n",
      "Iteration 48, loss = 2.06650048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 2.19510533\n",
      "Iteration 29, loss = 2.13565040\n",
      "Iteration 30, loss = 2.31129731\n",
      "Iteration 31, loss = 1.65799959\n",
      "Iteration 32, loss = 2.18023619\n",
      "Iteration 33, loss = 2.27700364\n",
      "Iteration 34, loss = 1.95330491\n",
      "Iteration 35, loss = 1.90728240\n",
      "Iteration 36, loss = 1.62747926\n",
      "Iteration 37, loss = 2.05637001\n",
      "Iteration 38, loss = 1.45887095\n",
      "Iteration 39, loss = 1.31648560\n",
      "Iteration 40, loss = 1.68417974\n",
      "Iteration 1, loss = 19.31297602\n",
      "Iteration 41, loss = 1.82348739\n",
      "Iteration 2, loss = 17.82357689\n",
      "Iteration 42, loss = 1.33395703\n",
      "Iteration 3, loss = 13.91682437\n",
      "Iteration 43, loss = 1.73873706\n",
      "Iteration 4, loss = 9.15812509\n",
      "Iteration 44, loss = 1.46936709\n",
      "Iteration 5, loss = 8.20035428\n",
      "Iteration 45, loss = 1.28158066\n",
      "Iteration 6, loss = 8.39305992\n",
      "Iteration 46, loss = 1.18175635\n",
      "Iteration 7, loss = 8.88499836\n",
      "Iteration 47, loss = 1.00882707\n",
      "Iteration 8, loss = 10.72033840\n",
      "Iteration 48, loss = 0.96610548\n",
      "Iteration 9, loss = 11.41704743\n",
      "Iteration 49, loss = 2.34487178\n",
      "Iteration 10, loss = 10.15231047\n",
      "Iteration 50, loss = 1.69338227\n",
      "Iteration 11, loss = 10.30734604\n",
      "Iteration 51, loss = 1.28910431\n",
      "Iteration 12, loss = 10.04223020\n",
      "Iteration 52, loss = 1.46046657\n",
      "Iteration 13, loss = 9.85846048\n",
      "Iteration 53, loss = 1.30877973\n",
      "Iteration 14, loss = 9.00716673\n",
      "Iteration 54, loss = 1.29353467\n",
      "Iteration 15, loss = 8.82635323\n",
      "Iteration 55, loss = 1.10962542\n",
      "Iteration 16, loss = 9.16047287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 1.13868229\n",
      "Iteration 57, loss = 1.40894015\n",
      "Iteration 58, loss = 1.52323872\n",
      "Iteration 59, loss = 2.00822941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.03878043\n",
      "Iteration 2, loss = 15.46565977\n",
      "Iteration 3, loss = 12.93917371\n",
      "Iteration 4, loss = 9.50553051\n",
      "Iteration 1, loss = 14.95380547\n",
      "Iteration 5, loss = 11.49883590\n",
      "Iteration 2, loss = 14.70909423\n",
      "Iteration 6, loss = 12.08014052\n",
      "Iteration 3, loss = 14.44160162\n",
      "Iteration 7, loss = 12.26140513\n",
      "Iteration 4, loss = 12.78247762\n",
      "Iteration 8, loss = 12.82096981\n",
      "Iteration 5, loss = 11.18026464\n",
      "Iteration 9, loss = 11.67772272\n",
      "Iteration 6, loss = 10.40612767\n",
      "Iteration 10, loss = 11.09166879\n",
      "Iteration 7, loss = 10.37254196\n",
      "Iteration 11, loss = 11.12802019\n",
      "Iteration 8, loss = 10.93285185\n",
      "Iteration 12, loss = 9.71029261\n",
      "Iteration 9, loss = 10.50819404\n",
      "Iteration 13, loss = 10.34806623\n",
      "Iteration 10, loss = 11.00905588\n",
      "Iteration 14, loss = 10.28641722\n",
      "Iteration 11, loss = 9.81231939\n",
      "Iteration 15, loss = 13.53715819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 10.79959250\n",
      "Iteration 13, loss = 11.57066337\n",
      "Iteration 14, loss = 10.92098637\n",
      "Iteration 15, loss = 11.27701722\n",
      "Iteration 16, loss = 10.18656605\n",
      "Iteration 17, loss = 9.82353145\n",
      "Iteration 18, loss = 13.43326753\n",
      "Iteration 19, loss = 12.49214276\n",
      "Iteration 20, loss = 14.46579517\n",
      "Iteration 21, loss = 14.77873046\n",
      "Iteration 22, loss = 13.03593689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.82261346\n",
      "Iteration 2, loss = 15.91553064\n",
      "Iteration 3, loss = 17.88684687\n",
      "Iteration 4, loss = 15.81170584\n",
      "Iteration 5, loss = 14.47355349\n",
      "Iteration 6, loss = 13.59120122\n",
      "Iteration 7, loss = 12.02709440\n",
      "Iteration 8, loss = 10.73970558\n",
      "Iteration 9, loss = 12.02547583\n",
      "Iteration 10, loss = 11.14434306\n",
      "Iteration 11, loss = 11.53577196\n",
      "Iteration 1, loss = 13.18414473\n",
      "Iteration 12, loss = 12.41055627\n",
      "Iteration 2, loss = 11.72108294\n",
      "Iteration 13, loss = 11.63939545\n",
      "Iteration 3, loss = 11.61956325\n",
      "Iteration 14, loss = 11.23253889\n",
      "Iteration 4, loss = 15.63213124\n",
      "Iteration 15, loss = 11.37638418\n",
      "Iteration 5, loss = 13.03443646\n",
      "Iteration 16, loss = 11.61131943\n",
      "Iteration 6, loss = 12.36020535\n",
      "Iteration 17, loss = 10.53184320\n",
      "Iteration 7, loss = 11.61834605\n",
      "Iteration 18, loss = 10.22528455\n",
      "Iteration 8, loss = 10.14122686\n",
      "Iteration 19, loss = 10.51417481\n",
      "Iteration 9, loss = 10.19453660\n",
      "Iteration 20, loss = 11.11431727\n",
      "Iteration 10, loss = 11.14605582\n",
      "Iteration 21, loss = 10.45906986\n",
      "Iteration 11, loss = 13.37534889\n",
      "Iteration 22, loss = 10.50224690\n",
      "Iteration 12, loss = 15.77959687\n",
      "Iteration 23, loss = 10.78185651\n",
      "Iteration 13, loss = 14.64807023\n",
      "Iteration 24, loss = 12.18076627\n",
      "Iteration 14, loss = 17.48567471\n",
      "Iteration 25, loss = 10.88178125\n",
      "Iteration 15, loss = 16.56287012\n",
      "Iteration 26, loss = 10.82223389\n",
      "Iteration 16, loss = 13.35006245\n",
      "Iteration 27, loss = 10.11215317\n",
      "Iteration 17, loss = 13.16266043\n",
      "Iteration 28, loss = 11.39620641\n",
      "Iteration 18, loss = 12.86924187\n",
      "Iteration 29, loss = 10.77276792\n",
      "Iteration 19, loss = 12.97453661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 10.70427022\n",
      "Iteration 31, loss = 10.42892805\n",
      "Iteration 32, loss = 10.61380903\n",
      "Iteration 33, loss = 10.43810908\n",
      "Iteration 34, loss = 10.52889681\n",
      "Iteration 35, loss = 10.10374034\n",
      "Iteration 36, loss = 11.59725589\n",
      "Iteration 37, loss = 11.32908114\n",
      "Iteration 38, loss = 10.98699671\n",
      "Iteration 39, loss = 10.92014141\n",
      "Iteration 40, loss = 10.69372549\n",
      "Iteration 41, loss = 11.69568654\n",
      "Iteration 42, loss = 10.59060680\n",
      "Iteration 1, loss = 15.04846094\n",
      "Iteration 43, loss = 10.96657277\n",
      "Iteration 2, loss = 12.88263624\n",
      "Iteration 44, loss = 11.34601815\n",
      "Iteration 3, loss = 11.00767021\n",
      "Iteration 45, loss = 11.11474752\n",
      "Iteration 4, loss = 15.01882621\n",
      "Iteration 46, loss = 10.72300250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 13.28810395\n",
      "Iteration 6, loss = 12.04448300\n",
      "Iteration 7, loss = 12.66294925\n",
      "Iteration 8, loss = 15.17179261\n",
      "Iteration 9, loss = 12.78825980\n",
      "Iteration 10, loss = 12.06505297\n",
      "Iteration 11, loss = 11.85229741\n",
      "Iteration 12, loss = 11.25873898\n",
      "Iteration 13, loss = 11.41401360\n",
      "Iteration 14, loss = 12.01738407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.17406990\n",
      "Iteration 2, loss = 15.71656204\n",
      "Iteration 3, loss = 18.50208984\n",
      "Iteration 4, loss = 16.20851967\n",
      "Iteration 5, loss = 13.97995560\n",
      "Iteration 6, loss = 13.17163994\n",
      "Iteration 7, loss = 13.08691372\n",
      "Iteration 8, loss = 11.97481625\n",
      "Iteration 9, loss = 11.75225138\n",
      "Iteration 10, loss = 11.55665969\n",
      "Iteration 11, loss = 15.25212415\n",
      "Iteration 1, loss = 19.30404582\n",
      "Iteration 12, loss = 12.99640702\n",
      "Iteration 2, loss = 18.31087612\n",
      "Iteration 13, loss = 12.22421767\n",
      "Iteration 3, loss = 12.14720114\n",
      "Iteration 14, loss = 11.19298288\n",
      "Iteration 4, loss = 12.38633490\n",
      "Iteration 15, loss = 12.02823451\n",
      "Iteration 5, loss = 14.80864311\n",
      "Iteration 16, loss = 11.10872146\n",
      "Iteration 6, loss = 14.42822934\n",
      "Iteration 17, loss = 12.81097762\n",
      "Iteration 7, loss = 15.93521995\n",
      "Iteration 18, loss = 11.12464070\n",
      "Iteration 8, loss = 14.59229083\n",
      "Iteration 19, loss = 11.85152742\n",
      "Iteration 9, loss = 13.31446003\n",
      "Iteration 20, loss = 11.37689518\n",
      "Iteration 10, loss = 13.89490204\n",
      "Iteration 21, loss = 11.57647230\n",
      "Iteration 11, loss = 12.82679457\n",
      "Iteration 22, loss = 10.66074693\n",
      "Iteration 12, loss = 12.65730522\n",
      "Iteration 23, loss = 10.84086448\n",
      "Iteration 13, loss = 12.74582575\n",
      "Iteration 24, loss = 11.92445168\n",
      "Iteration 14, loss = 15.51030720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 12.37971216\n",
      "Iteration 26, loss = 11.57647037\n",
      "Iteration 27, loss = 11.15761409\n",
      "Iteration 28, loss = 10.69332162\n",
      "Iteration 29, loss = 10.92676593\n",
      "Iteration 30, loss = 10.69551750\n",
      "Iteration 31, loss = 10.77466477\n",
      "Iteration 32, loss = 10.58318164\n",
      "Iteration 33, loss = 10.39001244\n",
      "Iteration 34, loss = 10.20093677\n",
      "Iteration 35, loss = 10.31691479\n",
      "Iteration 36, loss = 10.77934006\n",
      "Iteration 1, loss = 19.26606653\n",
      "Iteration 37, loss = 10.90345146\n",
      "Iteration 2, loss = 20.61618361\n",
      "Iteration 38, loss = 10.35991085\n",
      "Iteration 3, loss = 14.33971716\n",
      "Iteration 39, loss = 9.85187725\n",
      "Iteration 4, loss = 15.99047881\n",
      "Iteration 40, loss = 10.24270145\n",
      "Iteration 5, loss = 13.55482863\n",
      "Iteration 41, loss = 10.51768967\n",
      "Iteration 6, loss = 11.04497364\n",
      "Iteration 42, loss = 10.59962415\n",
      "Iteration 7, loss = 13.17557482\n",
      "Iteration 43, loss = 10.16844328\n",
      "Iteration 8, loss = 11.93570701\n",
      "Iteration 44, loss = 9.97078535\n",
      "Iteration 9, loss = 11.89157584\n",
      "Iteration 45, loss = 10.40383157\n",
      "Iteration 10, loss = 12.09406185\n",
      "Iteration 46, loss = 10.09170741\n",
      "Iteration 11, loss = 10.58559720\n",
      "Iteration 47, loss = 10.68181483\n",
      "Iteration 12, loss = 10.98836139\n",
      "Iteration 48, loss = 10.56920146\n",
      "Iteration 13, loss = 10.48500604\n",
      "Iteration 49, loss = 10.88583706\n",
      "Iteration 14, loss = 10.41830592\n",
      "Iteration 50, loss = 10.06039348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 11.02769629\n",
      "Iteration 16, loss = 10.44517855\n",
      "Iteration 17, loss = 11.35224178\n",
      "Iteration 18, loss = 10.26936541\n",
      "Iteration 19, loss = 9.97650095\n",
      "Iteration 20, loss = 12.10394044\n",
      "Iteration 21, loss = 9.95033125\n",
      "Iteration 22, loss = 9.59174003\n",
      "Iteration 23, loss = 11.18958145\n",
      "Iteration 24, loss = 10.58734943\n",
      "Iteration 25, loss = 9.82821717\n",
      "Iteration 26, loss = 8.76404370\n",
      "Iteration 27, loss = 8.96201713\n",
      "Iteration 1, loss = 18.45500308\n",
      "Iteration 28, loss = 9.36340035\n",
      "Iteration 2, loss = 17.60382761\n",
      "Iteration 29, loss = 9.64160167\n",
      "Iteration 3, loss = 19.11466116\n",
      "Iteration 30, loss = 10.57243859\n",
      "Iteration 4, loss = 13.67604636\n",
      "Iteration 31, loss = 8.63238355\n",
      "Iteration 5, loss = 15.56558164\n",
      "Iteration 32, loss = 9.23526528\n",
      "Iteration 6, loss = 10.22479786\n",
      "Iteration 33, loss = 8.81439057\n",
      "Iteration 7, loss = 9.47878452\n",
      "Iteration 34, loss = 8.42961056\n",
      "Iteration 8, loss = 10.42164444\n",
      "Iteration 35, loss = 8.83238314\n",
      "Iteration 9, loss = 11.66241906\n",
      "Iteration 36, loss = 9.40193512\n",
      "Iteration 10, loss = 10.86456561\n",
      "Iteration 37, loss = 8.74961576\n",
      "Iteration 11, loss = 11.56524727\n",
      "Iteration 38, loss = 8.95603497\n",
      "Iteration 12, loss = 10.43495773\n",
      "Iteration 39, loss = 8.93059787\n",
      "Iteration 13, loss = 10.56877061\n",
      "Iteration 40, loss = 9.92104277\n",
      "Iteration 14, loss = 12.10741005\n",
      "Iteration 41, loss = 8.51354785\n",
      "Iteration 15, loss = 11.10851690\n",
      "Iteration 42, loss = 8.04540343\n",
      "Iteration 16, loss = 11.30858557\n",
      "Iteration 43, loss = 8.12744339\n",
      "Iteration 17, loss = 11.77602473\n",
      "Iteration 44, loss = 8.13203882\n",
      "Iteration 18, loss = 11.35243513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 8.17281537\n",
      "Iteration 46, loss = 9.19374067\n",
      "Iteration 47, loss = 8.76055635\n",
      "Iteration 48, loss = 8.01100303\n",
      "Iteration 49, loss = 8.28192404\n",
      "Iteration 50, loss = 8.00565987\n",
      "Iteration 51, loss = 8.23853787\n",
      "Iteration 52, loss = 8.31474922\n",
      "Iteration 53, loss = 8.39135056\n",
      "Iteration 54, loss = 8.54719340\n",
      "Iteration 55, loss = 7.87891668\n",
      "Iteration 56, loss = 7.99098983\n",
      "Iteration 1, loss = 17.66928469\n",
      "Iteration 57, loss = 8.49670638\n",
      "Iteration 2, loss = 17.09697591\n",
      "Iteration 58, loss = 8.69160223\n",
      "Iteration 3, loss = 12.57599294\n",
      "Iteration 59, loss = 8.69101756\n",
      "Iteration 4, loss = 10.09014189\n",
      "Iteration 60, loss = 8.33662040\n",
      "Iteration 5, loss = 12.69179583\n",
      "Iteration 61, loss = 7.74617335\n",
      "Iteration 6, loss = 10.04084323\n",
      "Iteration 62, loss = 7.89992728\n",
      "Iteration 7, loss = 12.47687221\n",
      "Iteration 63, loss = 7.58356218\n",
      "Iteration 8, loss = 12.99507223\n",
      "Iteration 64, loss = 8.36770887\n",
      "Iteration 9, loss = 12.53802181\n",
      "Iteration 65, loss = 8.21571500\n",
      "Iteration 10, loss = 15.89422515\n",
      "Iteration 66, loss = 8.91526961\n",
      "Iteration 11, loss = 12.32753878\n",
      "Iteration 67, loss = 7.77596109\n",
      "Iteration 12, loss = 10.71288386\n",
      "Iteration 68, loss = 8.53617828\n",
      "Iteration 13, loss = 10.84076383\n",
      "Iteration 69, loss = 7.73571912\n",
      "Iteration 14, loss = 11.50538058\n",
      "Iteration 70, loss = 7.89452305\n",
      "Iteration 15, loss = 14.21012069\n",
      "Iteration 71, loss = 7.73604799\n",
      "Iteration 16, loss = 12.32330809\n",
      "Iteration 72, loss = 8.28271419\n",
      "Iteration 17, loss = 11.85384123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 73, loss = 8.27831257\n",
      "Iteration 74, loss = 8.23467594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.28510557\n",
      "Iteration 2, loss = 17.86584138\n",
      "Iteration 3, loss = 9.79102791\n",
      "Iteration 1, loss = 18.64139361\n",
      "Iteration 4, loss = 9.22300804\n",
      "Iteration 2, loss = 15.26026341\n",
      "Iteration 5, loss = 14.12092648\n",
      "Iteration 3, loss = 14.07106963\n",
      "Iteration 6, loss = 11.87978312\n",
      "Iteration 4, loss = 13.35616119\n",
      "Iteration 7, loss = 12.13921242\n",
      "Iteration 5, loss = 13.51736340\n",
      "Iteration 8, loss = 10.58354762\n",
      "Iteration 6, loss = 12.70936540\n",
      "Iteration 9, loss = 10.69059482\n",
      "Iteration 7, loss = 10.39319582\n",
      "Iteration 10, loss = 11.34614804\n",
      "Iteration 8, loss = 10.31485046\n",
      "Iteration 11, loss = 10.94765782\n",
      "Iteration 9, loss = 10.37526266\n",
      "Iteration 12, loss = 10.66599531\n",
      "Iteration 10, loss = 10.51298407\n",
      "Iteration 13, loss = 10.59795207\n",
      "Iteration 11, loss = 11.56969213\n",
      "Iteration 14, loss = 11.31512928\n",
      "Iteration 12, loss = 11.39384955\n",
      "Iteration 15, loss = 10.11830464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 12.15660534\n",
      "Iteration 14, loss = 12.55785770\n",
      "Iteration 15, loss = 11.95617464\n",
      "Iteration 16, loss = 11.93693959\n",
      "Iteration 17, loss = 13.74902670\n",
      "Iteration 18, loss = 12.18013699\n",
      "Iteration 19, loss = 11.69655206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.70687499\n",
      "Iteration 2, loss = 15.26779156\n",
      "Iteration 3, loss = 16.44160785\n",
      "Iteration 4, loss = 12.14488537\n",
      "Iteration 5, loss = 13.02798608\n",
      "Iteration 6, loss = 13.52532436\n",
      "Iteration 1, loss = 18.06499058\n",
      "Iteration 7, loss = 13.23290841\n",
      "Iteration 2, loss = 17.99844213\n",
      "Iteration 8, loss = 10.69186364\n",
      "Iteration 3, loss = 14.19142918\n",
      "Iteration 9, loss = 11.70898639\n",
      "Iteration 4, loss = 13.94848206\n",
      "Iteration 10, loss = 13.14753981\n",
      "Iteration 5, loss = 13.08461512\n",
      "Iteration 11, loss = 11.19011063\n",
      "Iteration 6, loss = 11.36129251\n",
      "Iteration 12, loss = 11.69349281\n",
      "Iteration 7, loss = 10.72986222\n",
      "Iteration 13, loss = 12.07590338\n",
      "Iteration 8, loss = 11.60494885\n",
      "Iteration 14, loss = 11.40572795\n",
      "Iteration 9, loss = 12.84475709\n",
      "Iteration 15, loss = 10.99582941\n",
      "Iteration 10, loss = 13.99947534\n",
      "Iteration 16, loss = 10.74813953\n",
      "Iteration 11, loss = 11.32350262\n",
      "Iteration 17, loss = 10.47452920\n",
      "Iteration 12, loss = 11.76378747\n",
      "Iteration 18, loss = 11.25700984\n",
      "Iteration 13, loss = 12.09815241\n",
      "Iteration 19, loss = 10.95416856\n",
      "Iteration 14, loss = 12.28838023\n",
      "Iteration 20, loss = 11.85645610\n",
      "Iteration 15, loss = 11.28017287\n",
      "Iteration 21, loss = 11.41339086\n",
      "Iteration 16, loss = 12.52511681\n",
      "Iteration 22, loss = 10.61603089\n",
      "Iteration 17, loss = 12.32810372\n",
      "Iteration 23, loss = 10.58546987\n",
      "Iteration 18, loss = 12.61410687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 10.31422330\n",
      "Iteration 25, loss = 10.31017523\n",
      "Iteration 26, loss = 9.74312370\n",
      "Iteration 27, loss = 10.19334396\n",
      "Iteration 28, loss = 10.33460937\n",
      "Iteration 29, loss = 9.91876827\n",
      "Iteration 30, loss = 10.32202239\n",
      "Iteration 31, loss = 9.58559136\n",
      "Iteration 32, loss = 9.66873258\n",
      "Iteration 33, loss = 9.82794132\n",
      "Iteration 34, loss = 9.90750237\n",
      "Iteration 35, loss = 10.26168545\n",
      "Iteration 36, loss = 9.99064877\n",
      "Iteration 1, loss = 18.02981403\n",
      "Iteration 37, loss = 10.30473505\n",
      "Iteration 2, loss = 17.31715751\n",
      "Iteration 38, loss = 11.09043410\n",
      "Iteration 3, loss = 10.41722342\n",
      "Iteration 39, loss = 10.53834277\n",
      "Iteration 4, loss = 10.52555068\n",
      "Iteration 40, loss = 9.59947356\n",
      "Iteration 5, loss = 10.74823284\n",
      "Iteration 41, loss = 10.07184295\n",
      "Iteration 6, loss = 10.44626123\n",
      "Iteration 42, loss = 9.83866403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 11.02854519\n",
      "Iteration 8, loss = 12.45578607\n",
      "Iteration 9, loss = 12.12235956\n",
      "Iteration 10, loss = 11.43553226\n",
      "Iteration 11, loss = 10.98128222\n",
      "Iteration 12, loss = 10.48890802\n",
      "Iteration 13, loss = 9.12508911\n",
      "Iteration 14, loss = 10.49544616\n",
      "Iteration 15, loss = 9.47647268\n",
      "Iteration 16, loss = 9.18265415\n",
      "Iteration 17, loss = 9.69974186\n",
      "Iteration 18, loss = 11.89382883\n",
      "Iteration 19, loss = 12.26482701\n",
      "Iteration 1, loss = 20.08964410\n",
      "Iteration 20, loss = 10.43472571\n",
      "Iteration 2, loss = 18.78874079\n",
      "Iteration 21, loss = 10.38634827\n",
      "Iteration 3, loss = 13.27979494\n",
      "Iteration 22, loss = 9.44079348\n",
      "Iteration 4, loss = 12.21854538\n",
      "Iteration 23, loss = 9.88465849\n",
      "Iteration 5, loss = 11.45199773\n",
      "Iteration 24, loss = 9.47403573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 9.87612855\n",
      "Iteration 7, loss = 11.12550526\n",
      "Iteration 8, loss = 10.41314498\n",
      "Iteration 9, loss = 10.79069627\n",
      "Iteration 10, loss = 11.00680997\n",
      "Iteration 11, loss = 10.85248165\n",
      "Iteration 12, loss = 12.12420102\n",
      "Iteration 13, loss = 13.48712712\n",
      "Iteration 14, loss = 10.54496929\n",
      "Iteration 15, loss = 9.99370401\n",
      "Iteration 16, loss = 9.66062730\n",
      "Iteration 17, loss = 10.13160535\n",
      "Iteration 18, loss = 11.07433180\n",
      "Iteration 1, loss = 19.46057753\n",
      "Iteration 19, loss = 11.70049115\n",
      "Iteration 2, loss = 18.09610865\n",
      "Iteration 20, loss = 11.15825437\n",
      "Iteration 3, loss = 12.39998337\n",
      "Iteration 21, loss = 12.99546314\n",
      "Iteration 4, loss = 13.75746968\n",
      "Iteration 22, loss = 11.71818880Iteration 5, loss = 12.51784161\n",
      "\n",
      "Iteration 6, loss = 11.89808812\n",
      "Iteration 23, loss = 10.94822516\n",
      "Iteration 7, loss = 12.14354076\n",
      "Iteration 24, loss = 11.14575494\n",
      "Iteration 25, loss = 10.03984814\n",
      "Iteration 8, loss = 10.28652303\n",
      "Iteration 26, loss = 10.29620795\n",
      "Iteration 9, loss = 11.19109187\n",
      "Iteration 27, loss = 9.55903299\n",
      "Iteration 10, loss = 10.29980238\n",
      "Iteration 11, loss = 13.27695367Iteration 28, loss = 10.41247734\n",
      "\n",
      "Iteration 12, loss = 12.12649662\n",
      "Iteration 29, loss = 10.09479789\n",
      "Iteration 13, loss = 11.50836223\n",
      "Iteration 30, loss = 10.40620496\n",
      "Iteration 14, loss = 12.27845103\n",
      "Iteration 31, loss = 9.46445960\n",
      "Iteration 15, loss = 11.64392999\n",
      "Iteration 32, loss = 9.15373242\n",
      "Iteration 16, loss = 10.51528896\n",
      "Iteration 33, loss = 9.90472478\n",
      "Iteration 34, loss = 9.40019591\n",
      "Iteration 17, loss = 10.04790515\n",
      "Iteration 35, loss = 9.91294466\n",
      "Iteration 18, loss = 10.32275571\n",
      "Iteration 36, loss = 10.22609554\n",
      "Iteration 19, loss = 9.79020179\n",
      "Iteration 20, loss = 9.81219421\n",
      "Iteration 37, loss = 10.45706340\n",
      "Iteration 21, loss = 10.41904435\n",
      "Iteration 38, loss = 9.23568588\n",
      "Iteration 22, loss = 9.95818887\n",
      "Iteration 39, loss = 10.02190733\n",
      "Iteration 23, loss = 9.10176840\n",
      "Iteration 40, loss = 9.51328140\n",
      "Iteration 24, loss = 10.05175312\n",
      "Iteration 41, loss = 9.35955033\n",
      "Iteration 25, loss = 9.19592955\n",
      "Iteration 42, loss = 9.16358208\n",
      "Iteration 43, loss = 8.96395061\n",
      "Iteration 26, loss = 9.27878930\n",
      "Iteration 44, loss = 9.58902422\n",
      "Iteration 27, loss = 8.85272420\n",
      "Iteration 45, loss = 10.21344813\n",
      "Iteration 28, loss = 9.05381712\n",
      "Iteration 46, loss = 9.46508955\n",
      "Iteration 29, loss = 9.99672227\n",
      "Iteration 47, loss = 9.92859953\n",
      "Iteration 30, loss = 9.95693027\n",
      "Iteration 48, loss = 9.80358778\n",
      "Iteration 31, loss = 9.92354170\n",
      "Iteration 49, loss = 9.92411658\n",
      "Iteration 32, loss = 8.87117816\n",
      "Iteration 50, loss = 9.65131338\n",
      "Iteration 33, loss = 9.31000875\n",
      "Iteration 51, loss = 9.84545132\n",
      "Iteration 34, loss = 9.59154766\n",
      "Iteration 52, loss = 9.80403281\n",
      "Iteration 35, loss = 9.20158747\n",
      "Iteration 53, loss = 9.72194011\n",
      "Iteration 36, loss = 9.52332305\n",
      "Iteration 54, loss = 9.83536947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 9.22154618\n",
      "Iteration 38, loss = 8.99621569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.05854497\n",
      "Iteration 2, loss = 13.04860063\n",
      "Iteration 1, loss = 19.48191980\n",
      "Iteration 3, loss = 11.41165789\n",
      "Iteration 2, loss = 21.36619295\n",
      "Iteration 4, loss = 13.25966497\n",
      "Iteration 3, loss = 15.52567328\n",
      "Iteration 5, loss = 11.69921138\n",
      "Iteration 4, loss = 15.10693406\n",
      "Iteration 6, loss = 11.14230126\n",
      "Iteration 5, loss = 10.00489634\n",
      "Iteration 7, loss = 11.19149317\n",
      "Iteration 6, loss = 10.86552794\n",
      "Iteration 8, loss = 10.34180648\n",
      "Iteration 7, loss = 13.16306019\n",
      "Iteration 9, loss = 9.83699849\n",
      "Iteration 8, loss = 15.41783244\n",
      "Iteration 10, loss = 10.73836505\n",
      "Iteration 9, loss = 12.18984296\n",
      "Iteration 11, loss = 9.63646799\n",
      "Iteration 10, loss = 11.81810979\n",
      "Iteration 12, loss = 11.64930817\n",
      "Iteration 11, loss = 12.88271303\n",
      "Iteration 13, loss = 10.52895920\n",
      "Iteration 12, loss = 12.14523838\n",
      "Iteration 14, loss = 10.05730890\n",
      "Iteration 13, loss = 11.71984976\n",
      "Iteration 15, loss = 9.69896464\n",
      "Iteration 14, loss = 10.27183891\n",
      "Iteration 16, loss = 10.24914976\n",
      "Iteration 15, loss = 11.63865098\n",
      "Iteration 17, loss = 9.70289066\n",
      "Iteration 16, loss = 10.44727412\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 9.86808820\n",
      "Iteration 19, loss = 9.97046638\n",
      "Iteration 20, loss = 10.44159982\n",
      "Iteration 21, loss = 10.75122045\n",
      "Iteration 22, loss = 10.22209102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.45727379\n",
      "Iteration 2, loss = 18.63660778\n",
      "Iteration 3, loss = 10.97088893\n",
      "Iteration 4, loss = 18.34778460\n",
      "Iteration 5, loss = 14.55151706\n",
      "Iteration 1, loss = 16.11246279\n",
      "Iteration 6, loss = 10.21318525\n",
      "Iteration 2, loss = 15.24602695\n",
      "Iteration 7, loss = 12.22658162\n",
      "Iteration 3, loss = 13.69451431\n",
      "Iteration 8, loss = 12.29044080\n",
      "Iteration 4, loss = 13.95480647\n",
      "Iteration 9, loss = 13.82543342\n",
      "Iteration 5, loss = 16.85972920\n",
      "Iteration 10, loss = 11.80096222\n",
      "Iteration 6, loss = 12.75330795\n",
      "Iteration 11, loss = 12.37899714\n",
      "Iteration 7, loss = 10.72568187\n",
      "Iteration 12, loss = 13.38799875\n",
      "Iteration 8, loss = 10.44616883\n",
      "Iteration 13, loss = 17.22437612\n",
      "Iteration 9, loss = 11.38993269\n",
      "Iteration 14, loss = 20.18468269\n",
      "Iteration 10, loss = 10.05940809\n",
      "Iteration 15, loss = 13.61433796\n",
      "Iteration 11, loss = 10.93984876\n",
      "Iteration 16, loss = 14.39037746\n",
      "Iteration 12, loss = 10.30320234\n",
      "Iteration 17, loss = 13.08195841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 11.61041790\n",
      "Iteration 14, loss = 12.73132553\n",
      "Iteration 15, loss = 12.16630489\n",
      "Iteration 16, loss = 11.49820273\n",
      "Iteration 17, loss = 10.14134272\n",
      "Iteration 18, loss = 10.23129784\n",
      "Iteration 19, loss = 10.04278551\n",
      "Iteration 20, loss = 9.57762751\n",
      "Iteration 21, loss = 9.41969608\n",
      "Iteration 22, loss = 10.62646679\n",
      "Iteration 23, loss = 10.18091968\n",
      "Iteration 24, loss = 9.53170771\n",
      "Iteration 25, loss = 9.38773897\n",
      "Iteration 1, loss = 16.72820067\n",
      "Iteration 26, loss = 10.10107538\n",
      "Iteration 2, loss = 13.95002224\n",
      "Iteration 27, loss = 10.22183370\n",
      "Iteration 3, loss = 13.57222393\n",
      "Iteration 28, loss = 10.65653945\n",
      "Iteration 4, loss = 11.69403197\n",
      "Iteration 29, loss = 10.50252999\n",
      "Iteration 5, loss = 10.05300650\n",
      "Iteration 30, loss = 10.50549211\n",
      "Iteration 6, loss = 11.75172162\n",
      "Iteration 31, loss = 9.84353962\n",
      "Iteration 7, loss = 12.40497803\n",
      "Iteration 32, loss = 9.38018603\n",
      "Iteration 8, loss = 12.21972452\n",
      "Iteration 33, loss = 9.66803868\n",
      "Iteration 9, loss = 11.42655090\n",
      "Iteration 34, loss = 9.28494630\n",
      "Iteration 10, loss = 11.26600248\n",
      "Iteration 35, loss = 9.64461951\n",
      "Iteration 11, loss = 12.58351661\n",
      "Iteration 36, loss = 8.98590506\n",
      "Iteration 12, loss = 10.62589036\n",
      "Iteration 37, loss = 10.28505252\n",
      "Iteration 13, loss = 11.73589274\n",
      "Iteration 38, loss = 9.38856822\n",
      "Iteration 14, loss = 10.44975516\n",
      "Iteration 39, loss = 9.82241207\n",
      "Iteration 15, loss = 10.31038622\n",
      "Iteration 40, loss = 9.15926115\n",
      "Iteration 16, loss = 9.88568222\n",
      "Iteration 41, loss = 9.28184441\n",
      "Iteration 17, loss = 10.04615578\n",
      "Iteration 42, loss = 8.88976036\n",
      "Iteration 18, loss = 10.07640431\n",
      "Iteration 43, loss = 9.32076374\n",
      "Iteration 19, loss = 9.31484780\n",
      "Iteration 44, loss = 9.36183145\n",
      "Iteration 20, loss = 9.88353280\n",
      "Iteration 45, loss = 9.67616173\n",
      "Iteration 46, loss = 9.36684328\n",
      "Iteration 21, loss = 9.54514416\n",
      "Iteration 47, loss = 8.89303661\n",
      "Iteration 22, loss = 9.28703095\n",
      "Iteration 48, loss = 8.57096505\n",
      "Iteration 23, loss = 9.41676778\n",
      "Iteration 49, loss = 8.84104417\n",
      "Iteration 24, loss = 9.23046420\n",
      "Iteration 50, loss = 8.72558085\n",
      "Iteration 25, loss = 9.12200308\n",
      "Iteration 51, loss = 9.08024429\n",
      "Iteration 26, loss = 9.16591632\n",
      "Iteration 52, loss = 9.39918111\n",
      "Iteration 27, loss = 9.24752385\n",
      "Iteration 53, loss = 8.65911892\n",
      "Iteration 28, loss = 9.72364769\n",
      "Iteration 54, loss = 8.89771578\n",
      "Iteration 29, loss = 8.63000698\n",
      "Iteration 55, loss = 9.05693898\n",
      "Iteration 30, loss = 8.32018372\n",
      "Iteration 56, loss = 8.31051588\n",
      "Iteration 31, loss = 8.59895837\n",
      "Iteration 57, loss = 8.66154723\n",
      "Iteration 32, loss = 9.34710224\n",
      "Iteration 58, loss = 8.38389473\n",
      "Iteration 33, loss = 9.07300511\n",
      "Iteration 59, loss = 9.12554813\n",
      "Iteration 34, loss = 8.76071616\n",
      "Iteration 60, loss = 9.00574141\n",
      "Iteration 35, loss = 8.72326158\n",
      "Iteration 61, loss = 9.04659304\n",
      "Iteration 36, loss = 10.21643073\n",
      "Iteration 62, loss = 8.69480339\n",
      "Iteration 37, loss = 9.39956125\n",
      "Iteration 63, loss = 8.53699070\n",
      "Iteration 38, loss = 9.28861540\n",
      "Iteration 64, loss = 8.53548423\n",
      "Iteration 39, loss = 8.70520375\n",
      "Iteration 65, loss = 8.68953801\n",
      "Iteration 40, loss = 9.02447296\n",
      "Iteration 66, loss = 8.72586859\n",
      "Iteration 41, loss = 8.99274337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 67, loss = 9.00261217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.85761799\n",
      "Iteration 2, loss = 25.19632676\n",
      "Iteration 3, loss = 17.74221818\n",
      "Iteration 1, loss = 20.05081289\n",
      "Iteration 4, loss = 13.16472735\n",
      "Iteration 2, loss = 16.29300282\n",
      "Iteration 5, loss = 12.99743937\n",
      "Iteration 3, loss = 13.90414308\n",
      "Iteration 6, loss = 9.18304593\n",
      "Iteration 4, loss = 9.50193150\n",
      "Iteration 7, loss = 8.34460761\n",
      "Iteration 5, loss = 11.10334930Iteration 8, loss = 8.81661050\n",
      "\n",
      "Iteration 9, loss = 9.21682568\n",
      "Iteration 6, loss = 12.93249942\n",
      "Iteration 10, loss = 14.04865345\n",
      "Iteration 7, loss = 10.24901039\n",
      "Iteration 11, loss = 13.97776922\n",
      "Iteration 8, loss = 10.49817408\n",
      "Iteration 12, loss = 11.41169556\n",
      "Iteration 9, loss = 11.53980079\n",
      "Iteration 13, loss = 12.63277313\n",
      "Iteration 10, loss = 12.80822779\n",
      "Iteration 14, loss = 10.74137599\n",
      "Iteration 11, loss = 13.68007916\n",
      "Iteration 15, loss = 9.54810745\n",
      "Iteration 12, loss = 11.85294546\n",
      "Iteration 16, loss = 9.52773044\n",
      "Iteration 13, loss = 10.95772497\n",
      "Iteration 17, loss = 10.21526609\n",
      "Iteration 14, loss = 10.47287294\n",
      "Iteration 18, loss = 9.46767998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 9.89901983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.98194720\n",
      "Iteration 1, loss = 19.35761404\n",
      "Iteration 2, loss = 24.05001490\n",
      "Iteration 2, loss = 12.69024854\n",
      "Iteration 3, loss = 13.48353230\n",
      "Iteration 3, loss = 10.82976966\n",
      "Iteration 4, loss = 8.94993906\n",
      "Iteration 4, loss = 13.29587339\n",
      "Iteration 5, loss = 8.77656915\n",
      "Iteration 5, loss = 13.77770063\n",
      "Iteration 6, loss = 11.48486400\n",
      "Iteration 6, loss = 15.21901153\n",
      "Iteration 7, loss = 12.39470159\n",
      "Iteration 7, loss = 12.33788305\n",
      "Iteration 8, loss = 9.21000890\n",
      "Iteration 8, loss = 10.80192667\n",
      "Iteration 9, loss = 9.56396652\n",
      "Iteration 9, loss = 11.22066907\n",
      "Iteration 10, loss = 11.03454385\n",
      "Iteration 10, loss = 11.00430185\n",
      "Iteration 11, loss = 13.26774849\n",
      "Iteration 11, loss = 12.13788756\n",
      "Iteration 12, loss = 12.24240986\n",
      "Iteration 12, loss = 10.67348869\n",
      "Iteration 13, loss = 9.94516538\n",
      "Iteration 13, loss = 11.54095900\n",
      "Iteration 14, loss = 10.57650968\n",
      "Iteration 14, loss = 11.79239848\n",
      "Iteration 15, loss = 10.73219478\n",
      "Iteration 15, loss = 12.22732039\n",
      "Iteration 16, loss = 11.53149236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 10.83950368\n",
      "Iteration 17, loss = 11.51846780\n",
      "Iteration 18, loss = 10.64501015\n",
      "Iteration 19, loss = 10.57295150\n",
      "Iteration 20, loss = 11.19647169\n",
      "Iteration 21, loss = 12.12847734\n",
      "Iteration 22, loss = 10.70167187\n",
      "Iteration 23, loss = 10.71820628\n",
      "Iteration 24, loss = 10.26533068\n",
      "Iteration 25, loss = 11.03484866\n",
      "Iteration 26, loss = 10.47179639\n",
      "Iteration 27, loss = 10.24416793\n",
      "Iteration 28, loss = 10.21789824\n",
      "Iteration 1, loss = 20.64814805\n",
      "Iteration 29, loss = 9.83505888\n",
      "Iteration 2, loss = 17.99085855\n",
      "Iteration 30, loss = 10.31609175\n",
      "Iteration 3, loss = 15.41121111\n",
      "Iteration 31, loss = 10.28211677\n",
      "Iteration 4, loss = 12.89878119\n",
      "Iteration 32, loss = 9.61943371\n",
      "Iteration 5, loss = 12.01038682\n",
      "Iteration 33, loss = 9.46352594\n",
      "Iteration 6, loss = 10.67758254\n",
      "Iteration 34, loss = 9.50097706\n",
      "Iteration 7, loss = 10.95685374\n",
      "Iteration 35, loss = 10.24573083\n",
      "Iteration 8, loss = 11.13807828\n",
      "Iteration 36, loss = 9.72729889\n",
      "Iteration 9, loss = 10.54644819\n",
      "Iteration 37, loss = 9.87559872\n",
      "Iteration 10, loss = 11.32235186\n",
      "Iteration 38, loss = 10.37852641\n",
      "Iteration 11, loss = 9.76545931\n",
      "Iteration 39, loss = 9.54663569\n",
      "Iteration 12, loss = 11.17113133\n",
      "Iteration 13, loss = 11.05559131\n",
      "Iteration 40, loss = 9.35907474\n",
      "Iteration 14, loss = 9.80607585\n",
      "Iteration 41, loss = 9.19210492\n",
      "Iteration 15, loss = 12.49457838\n",
      "Iteration 42, loss = 9.91493577\n",
      "Iteration 16, loss = 12.14309835\n",
      "Iteration 43, loss = 9.29532590\n",
      "Iteration 17, loss = 11.23164759\n",
      "Iteration 44, loss = 9.96546885\n",
      "Iteration 18, loss = 10.15574582\n",
      "Iteration 45, loss = 10.28072888\n",
      "Iteration 19, loss = 10.42031556\n",
      "Iteration 46, loss = 10.35742049\n",
      "Iteration 20, loss = 9.28087858\n",
      "Iteration 47, loss = 10.95128832\n",
      "Iteration 21, loss = 10.56849703\n",
      "Iteration 48, loss = 10.19089273\n",
      "Iteration 22, loss = 9.54040428\n",
      "Iteration 49, loss = 9.76892645\n",
      "Iteration 23, loss = 9.48538002\n",
      "Iteration 50, loss = 9.77042298\n",
      "Iteration 24, loss = 9.62031801\n",
      "Iteration 51, loss = 10.43929785\n",
      "Iteration 25, loss = 9.95065431\n",
      "Iteration 52, loss = 11.11129756\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 8.94303534\n",
      "Iteration 27, loss = 9.96993004\n",
      "Iteration 28, loss = 9.61450971\n",
      "Iteration 29, loss = 10.74131989\n",
      "Iteration 30, loss = 10.13100066\n",
      "Iteration 31, loss = 9.89483972\n",
      "Iteration 32, loss = 10.50824840\n",
      "Iteration 33, loss = 9.78055268\n",
      "Iteration 34, loss = 10.37855574\n",
      "Iteration 35, loss = 9.12607869\n",
      "Iteration 36, loss = 9.83575061\n",
      "Iteration 37, loss = 9.75894438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.19280387\n",
      "Iteration 2, loss = 18.05096829\n",
      "Iteration 3, loss = 17.44579078\n",
      "Iteration 4, loss = 11.27646714\n",
      "Iteration 5, loss = 12.23264450\n",
      "Iteration 6, loss = 15.26930669\n",
      "Iteration 7, loss = 14.36521947\n",
      "Iteration 8, loss = 13.78336078\n",
      "Iteration 9, loss = 12.41466694\n",
      "Iteration 10, loss = 11.00620775\n",
      "Iteration 11, loss = 10.61117870\n",
      "Iteration 12, loss = 11.36413775\n",
      "Iteration 13, loss = 10.35747661\n",
      "Iteration 1, loss = 15.01068220\n",
      "Iteration 14, loss = 10.56595374\n",
      "Iteration 2, loss = 16.99427655\n",
      "Iteration 15, loss = 12.28847088\n",
      "Iteration 3, loss = 13.52061249\n",
      "Iteration 16, loss = 10.74456675\n",
      "Iteration 4, loss = 10.55951068\n",
      "Iteration 17, loss = 10.08120687\n",
      "Iteration 5, loss = 11.11367702\n",
      "Iteration 6, loss = 10.56862799\n",
      "Iteration 18, loss = 11.13965678\n",
      "Iteration 7, loss = 11.20255546\n",
      "Iteration 19, loss = 11.78824668\n",
      "Iteration 8, loss = 11.02408713\n",
      "Iteration 20, loss = 10.69458007\n",
      "Iteration 9, loss = 10.24701560\n",
      "Iteration 21, loss = 9.72355700\n",
      "Iteration 10, loss = 9.41987604\n",
      "Iteration 22, loss = 10.76148016\n",
      "Iteration 11, loss = 11.36878345\n",
      "Iteration 23, loss = 10.80633526\n",
      "Iteration 12, loss = 10.82238945\n",
      "Iteration 24, loss = 11.07595698\n",
      "Iteration 13, loss = 11.39987426\n",
      "Iteration 25, loss = 10.27577578\n",
      "Iteration 14, loss = 13.88472717\n",
      "Iteration 26, loss = 11.68132415\n",
      "Iteration 15, loss = 14.00998124\n",
      "Iteration 27, loss = 11.67399558\n",
      "Iteration 16, loss = 10.22241308\n",
      "Iteration 28, loss = 10.34866445\n",
      "Iteration 17, loss = 10.08888796\n",
      "Iteration 29, loss = 9.83928259\n",
      "Iteration 18, loss = 9.48408197\n",
      "Iteration 30, loss = 10.23914137\n",
      "Iteration 19, loss = 9.37074885\n",
      "Iteration 31, loss = 9.56183926\n",
      "Iteration 20, loss = 10.03487874\n",
      "Iteration 32, loss = 9.65372127\n",
      "Iteration 21, loss = 12.20152465\n",
      "Iteration 33, loss = 9.37479424\n",
      "Iteration 22, loss = 9.52820639\n",
      "Iteration 34, loss = 9.51475470\n",
      "Iteration 23, loss = 9.54457705\n",
      "Iteration 35, loss = 9.88601926\n",
      "Iteration 36, loss = 9.78588092\n",
      "Iteration 24, loss = 9.62463519\n",
      "Iteration 37, loss = 9.05454468\n",
      "Iteration 25, loss = 9.56149719\n",
      "Iteration 26, loss = 9.56126815\n",
      "Iteration 38, loss = 9.26169402\n",
      "Iteration 27, loss = 9.62809687\n",
      "Iteration 39, loss = 8.91718220\n",
      "Iteration 28, loss = 9.09684992\n",
      "Iteration 40, loss = 9.19606476\n",
      "Iteration 29, loss = 8.95317303\n",
      "Iteration 41, loss = 9.93840089\n",
      "Iteration 42, loss = 10.24913397\n",
      "Iteration 30, loss = 10.02461442\n",
      "Iteration 43, loss = 9.50578122\n",
      "Iteration 31, loss = 9.83338178\n",
      "Iteration 44, loss = 8.88576009\n",
      "Iteration 32, loss = 9.48408983\n",
      "Iteration 45, loss = 9.00687970\n",
      "Iteration 33, loss = 9.56261291\n",
      "Iteration 46, loss = 9.04472900\n",
      "Iteration 34, loss = 9.36477748\n",
      "Iteration 35, loss = 9.20509484\n",
      "Iteration 47, loss = 9.39859501\n",
      "Iteration 36, loss = 9.32239081\n",
      "Iteration 48, loss = 8.89107008\n",
      "Iteration 37, loss = 9.12392733\n",
      "Iteration 49, loss = 8.80947966\n",
      "Iteration 38, loss = 8.61192946\n",
      "Iteration 50, loss = 8.45317622\n",
      "Iteration 39, loss = 9.27688338\n",
      "Iteration 51, loss = 8.65009875\n",
      "Iteration 40, loss = 9.19709155\n",
      "Iteration 52, loss = 8.53210145\n",
      "Iteration 41, loss = 10.10648691\n",
      "Iteration 53, loss = 8.49128591\n",
      "Iteration 42, loss = 8.82820353\n",
      "Iteration 54, loss = 8.53035180\n",
      "Iteration 43, loss = 9.27342075\n",
      "Iteration 55, loss = 8.72964966\n",
      "Iteration 44, loss = 8.81296308\n",
      "Iteration 56, loss = 8.84991932\n",
      "Iteration 45, loss = 8.70231299\n",
      "Iteration 57, loss = 9.00997864\n",
      "Iteration 46, loss = 9.09795455\n",
      "Iteration 58, loss = 8.81705716\n",
      "Iteration 47, loss = 9.23127564\n",
      "Iteration 59, loss = 8.58400838\n",
      "Iteration 48, loss = 9.87098124\n",
      "Iteration 60, loss = 8.66195405\n",
      "Iteration 49, loss = 9.43924101\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 61, loss = 8.85582127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.43934051\n",
      "Iteration 2, loss = 12.99745104\n",
      "Iteration 1, loss = 18.73900998\n",
      "Iteration 3, loss = 9.93251476\n",
      "Iteration 2, loss = 18.26732862\n",
      "Iteration 4, loss = 12.00349992\n",
      "Iteration 3, loss = 12.76050801\n",
      "Iteration 5, loss = 9.81310660\n",
      "Iteration 4, loss = 14.29962080\n",
      "Iteration 6, loss = 10.75197037\n",
      "Iteration 5, loss = 11.60135837\n",
      "Iteration 7, loss = 12.92631054\n",
      "Iteration 6, loss = 12.09979021\n",
      "Iteration 8, loss = 12.49742615\n",
      "Iteration 7, loss = 10.99089330\n",
      "Iteration 9, loss = 12.53797354\n",
      "Iteration 8, loss = 12.77063043\n",
      "Iteration 10, loss = 11.27587542\n",
      "Iteration 9, loss = 14.61425546\n",
      "Iteration 11, loss = 11.74045713\n",
      "Iteration 10, loss = 13.72190797\n",
      "Iteration 12, loss = 11.10089605\n",
      "Iteration 11, loss = 14.01034379\n",
      "Iteration 13, loss = 11.35811583\n",
      "Iteration 12, loss = 11.97582253\n",
      "Iteration 14, loss = 11.02442826\n",
      "Iteration 13, loss = 12.94381157\n",
      "Iteration 15, loss = 11.61405640\n",
      "Iteration 14, loss = 13.88113037\n",
      "Iteration 16, loss = 11.15318498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 12.92581672\n",
      "Iteration 16, loss = 13.10626051\n",
      "Iteration 17, loss = 12.60827138\n",
      "Iteration 18, loss = 13.60718660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.96301740\n",
      "Iteration 2, loss = 16.67692426\n",
      "Iteration 3, loss = 12.41415071\n",
      "Iteration 4, loss = 9.81020659\n",
      "Iteration 1, loss = 16.12479911\n",
      "Iteration 5, loss = 12.92095915\n",
      "Iteration 2, loss = 18.66037424\n",
      "Iteration 3, loss = 15.50898731\n",
      "Iteration 6, loss = 16.13489319\n",
      "Iteration 4, loss = 14.48583026\n",
      "Iteration 7, loss = 13.76486462\n",
      "Iteration 5, loss = 11.92105032\n",
      "Iteration 8, loss = 11.41180292\n",
      "Iteration 6, loss = 11.86925665\n",
      "Iteration 9, loss = 10.68308283\n",
      "Iteration 7, loss = 11.05800801\n",
      "Iteration 10, loss = 11.51794424\n",
      "Iteration 8, loss = 13.21138129\n",
      "Iteration 11, loss = 11.45246085\n",
      "Iteration 9, loss = 11.91463470\n",
      "Iteration 12, loss = 11.76728845\n",
      "Iteration 10, loss = 13.48579861\n",
      "Iteration 13, loss = 20.57175449\n",
      "Iteration 11, loss = 11.70108798\n",
      "Iteration 14, loss = 15.00685028\n",
      "Iteration 12, loss = 11.47572841\n",
      "Iteration 15, loss = 13.86238915\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 11.66800345\n",
      "Iteration 14, loss = 11.46793342\n",
      "Iteration 15, loss = 11.86924256\n",
      "Iteration 16, loss = 14.24575005\n",
      "Iteration 17, loss = 11.42521430\n",
      "Iteration 18, loss = 11.44947067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.71447890\n",
      "Iteration 2, loss = 17.77440135\n",
      "Iteration 3, loss = 12.44237221\n",
      "Iteration 4, loss = 9.65333837\n",
      "Iteration 5, loss = 8.78830129\n",
      "Iteration 6, loss = 10.58639512\n",
      "Iteration 7, loss = 11.23226658\n",
      "Iteration 1, loss = 16.58671047\n",
      "Iteration 8, loss = 10.50426872\n",
      "Iteration 2, loss = 14.83456121\n",
      "Iteration 9, loss = 9.90417402\n",
      "Iteration 3, loss = 14.83837222\n",
      "Iteration 10, loss = 10.96453288\n",
      "Iteration 4, loss = 11.55341181\n",
      "Iteration 11, loss = 10.78484109\n",
      "Iteration 5, loss = 10.92833467\n",
      "Iteration 12, loss = 10.64064372\n",
      "Iteration 6, loss = 11.72514547\n",
      "Iteration 13, loss = 10.84121253\n",
      "Iteration 7, loss = 14.81954550\n",
      "Iteration 14, loss = 11.06426850\n",
      "Iteration 8, loss = 12.10736438\n",
      "Iteration 15, loss = 10.07728401\n",
      "Iteration 9, loss = 12.77067960\n",
      "Iteration 16, loss = 10.23035030\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 16.91478235\n",
      "Iteration 11, loss = 10.60162172\n",
      "Iteration 12, loss = 12.87846176\n",
      "Iteration 13, loss = 14.10810353\n",
      "Iteration 14, loss = 16.07008557\n",
      "Iteration 15, loss = 12.56623727\n",
      "Iteration 16, loss = 11.36281780\n",
      "Iteration 17, loss = 16.96900695\n",
      "Iteration 18, loss = 12.13897175\n",
      "Iteration 19, loss = 13.64771212\n",
      "Iteration 20, loss = 17.91906614\n",
      "Iteration 21, loss = 16.03876919\n",
      "Iteration 22, loss = 13.67599134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.51275004\n",
      "Iteration 2, loss = 14.06209234\n",
      "Iteration 3, loss = 10.55031607\n",
      "Iteration 4, loss = 11.98289471\n",
      "Iteration 5, loss = 9.98379980\n",
      "Iteration 6, loss = 12.17931062\n",
      "Iteration 7, loss = 10.48324125\n",
      "Iteration 8, loss = 10.57645775\n",
      "Iteration 9, loss = 9.96761868\n",
      "Iteration 10, loss = 10.60192364\n",
      "Iteration 11, loss = 9.76754347\n",
      "Iteration 12, loss = 10.36591817\n",
      "Iteration 13, loss = 11.75912995\n",
      "Iteration 1, loss = 19.80312467\n",
      "Iteration 14, loss = 10.88109542\n",
      "Iteration 2, loss = 14.12579401\n",
      "Iteration 15, loss = 10.87127092\n",
      "Iteration 3, loss = 15.50891200\n",
      "Iteration 16, loss = 10.99770029\n",
      "Iteration 4, loss = 14.22262172\n",
      "Iteration 17, loss = 10.30960305\n",
      "Iteration 5, loss = 13.11684985\n",
      "Iteration 18, loss = 13.11414750\n",
      "Iteration 6, loss = 10.79016395\n",
      "Iteration 19, loss = 10.64273438\n",
      "Iteration 7, loss = 11.55283248\n",
      "Iteration 8, loss = 11.46753548\n",
      "Iteration 20, loss = 9.54571742\n",
      "Iteration 9, loss = 10.89771726\n",
      "Iteration 21, loss = 9.73472275\n",
      "Iteration 10, loss = 11.24611903\n",
      "Iteration 22, loss = 9.64728374\n",
      "Iteration 11, loss = 10.66880402\n",
      "Iteration 23, loss = 9.49100559\n",
      "Iteration 12, loss = 10.60860206\n",
      "Iteration 24, loss = 9.40733520\n",
      "Iteration 13, loss = 13.18333990\n",
      "Iteration 25, loss = 10.03798118\n",
      "Iteration 14, loss = 12.16985616\n",
      "Iteration 26, loss = 9.24537004\n",
      "Iteration 15, loss = 10.15912989\n",
      "Iteration 27, loss = 9.50253015\n",
      "Iteration 16, loss = 10.60950617\n",
      "Iteration 28, loss = 9.15806239\n",
      "Iteration 17, loss = 9.68056045\n",
      "Iteration 29, loss = 9.67406195\n",
      "Iteration 18, loss = 10.45045231\n",
      "Iteration 30, loss = 9.01640431\n",
      "Iteration 19, loss = 10.45140131\n",
      "Iteration 31, loss = 8.99098444\n",
      "Iteration 20, loss = 9.63710813\n",
      "Iteration 32, loss = 9.63057331\n",
      "Iteration 21, loss = 9.40791519Iteration 33, loss = 9.56054389\n",
      "\n",
      "Iteration 34, loss = 10.54975496\n",
      "Iteration 22, loss = 9.50403363\n",
      "Iteration 35, loss = 10.91104678\n",
      "Iteration 23, loss = 9.54555202\n",
      "Iteration 36, loss = 10.40054494\n",
      "Iteration 24, loss = 9.93118091\n",
      "Iteration 37, loss = 9.67918527\n",
      "Iteration 25, loss = 9.57147154\n",
      "Iteration 38, loss = 9.55326207\n",
      "Iteration 26, loss = 9.16320999\n",
      "Iteration 39, loss = 9.74615385\n",
      "Iteration 27, loss = 9.68980486\n",
      "Iteration 40, loss = 9.38826600\n",
      "Iteration 28, loss = 9.77688269\n",
      "Iteration 41, loss = 10.25176383\n",
      "Iteration 29, loss = 9.62183293\n",
      "Iteration 42, loss = 10.29694612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 8.91800624\n",
      "Iteration 31, loss = 8.96078536\n",
      "Iteration 32, loss = 9.52152267\n",
      "Iteration 33, loss = 9.78801398\n",
      "Iteration 34, loss = 9.83192216\n",
      "Iteration 35, loss = 9.95665265\n",
      "Iteration 36, loss = 8.78032222\n",
      "Iteration 37, loss = 9.01377595\n",
      "Iteration 38, loss = 9.64503712\n",
      "Iteration 39, loss = 9.65175038\n",
      "Iteration 40, loss = 9.88914907\n",
      "Iteration 41, loss = 10.16429653\n",
      "Iteration 42, loss = 10.05136967\n",
      "Iteration 1, loss = 18.15548287\n",
      "Iteration 43, loss = 9.23843078\n",
      "Iteration 2, loss = 13.52215987\n",
      "Iteration 44, loss = 9.28839624\n",
      "Iteration 3, loss = 12.78420132\n",
      "Iteration 45, loss = 9.52906232\n",
      "Iteration 4, loss = 9.96253375\n",
      "Iteration 46, loss = 9.37685927\n",
      "Iteration 5, loss = 13.15365130\n",
      "Iteration 47, loss = 8.51977137\n",
      "Iteration 6, loss = 11.98578403\n",
      "Iteration 48, loss = 8.45028260\n",
      "Iteration 7, loss = 11.79767353\n",
      "Iteration 49, loss = 8.45036559\n",
      "Iteration 8, loss = 10.06656203\n",
      "Iteration 50, loss = 9.19296063\n",
      "Iteration 9, loss = 11.05105486\n",
      "Iteration 51, loss = 9.42172052\n",
      "Iteration 10, loss = 10.01762785\n",
      "Iteration 52, loss = 8.67519674\n",
      "Iteration 11, loss = 11.38983536\n",
      "Iteration 53, loss = 8.64297452\n",
      "Iteration 12, loss = 11.16766677\n",
      "Iteration 54, loss = 8.45180688\n",
      "Iteration 13, loss = 11.85520612\n",
      "Iteration 55, loss = 8.37269910\n",
      "Iteration 14, loss = 11.57661192\n",
      "Iteration 56, loss = 9.27884377\n",
      "Iteration 15, loss = 10.63084148\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 9.00874771\n",
      "Iteration 58, loss = 8.77269018\n",
      "Iteration 59, loss = 8.85184450\n",
      "Iteration 60, loss = 8.49999230\n",
      "Iteration 61, loss = 8.70352629\n",
      "Iteration 62, loss = 8.94479668\n",
      "Iteration 63, loss = 8.51531084\n",
      "Iteration 64, loss = 8.20679740\n",
      "Iteration 65, loss = 8.36386887\n",
      "Iteration 66, loss = 8.36243499\n",
      "Iteration 67, loss = 8.71740431\n",
      "Iteration 68, loss = 8.60158228\n",
      "Iteration 1, loss = 17.27687251\n",
      "Iteration 69, loss = 9.15322588\n",
      "Iteration 2, loss = 14.44938888\n",
      "Iteration 70, loss = 8.21338624\n",
      "Iteration 3, loss = 12.71165374\n",
      "Iteration 71, loss = 8.33543521\n",
      "Iteration 4, loss = 11.19528147\n",
      "Iteration 72, loss = 8.25694883\n",
      "Iteration 5, loss = 14.68453070\n",
      "Iteration 73, loss = 8.13305854\n",
      "Iteration 6, loss = 10.98554005\n",
      "Iteration 74, loss = 8.28914221\n",
      "Iteration 7, loss = 11.06075092\n",
      "Iteration 75, loss = 8.09647640\n",
      "Iteration 8, loss = 12.16857861\n",
      "Iteration 76, loss = 7.78735638\n",
      "Iteration 9, loss = 11.85222811\n",
      "Iteration 77, loss = 8.02475411\n",
      "Iteration 10, loss = 14.02075245\n",
      "Iteration 78, loss = 8.85124887\n",
      "Iteration 11, loss = 13.14522094\n",
      "Iteration 79, loss = 8.30493273\n",
      "Iteration 12, loss = 13.32481182\n",
      "Iteration 80, loss = 8.03616938\n",
      "Iteration 13, loss = 13.48132111\n",
      "Iteration 81, loss = 8.07783183\n",
      "Iteration 14, loss = 12.16322952\n",
      "Iteration 82, loss = 8.07587366\n",
      "Iteration 15, loss = 11.58128637\n",
      "Iteration 83, loss = 8.31286693\n",
      "Iteration 16, loss = 11.04627371\n",
      "Iteration 84, loss = 8.54728595\n",
      "Iteration 17, loss = 11.42031274\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 85, loss = 8.23731750\n",
      "Iteration 86, loss = 8.44165359\n",
      "Iteration 87, loss = 8.56627991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03471461\n",
      "Iteration 2, loss = 0.46793099\n",
      "Iteration 1, loss = 1.25019969\n",
      "Iteration 3, loss = 0.39523032\n",
      "Iteration 2, loss = 0.42767227\n",
      "Iteration 4, loss = 0.34648651\n",
      "Iteration 3, loss = 0.37074869\n",
      "Iteration 5, loss = 0.29960345\n",
      "Iteration 4, loss = 0.32758375\n",
      "Iteration 5, loss = 0.29285556\n",
      "Iteration 6, loss = 0.25506184\n",
      "Iteration 6, loss = 0.28552561\n",
      "Iteration 7, loss = 0.26713507\n",
      "Iteration 7, loss = 0.25841975\n",
      "Iteration 8, loss = 0.25065146\n",
      "Iteration 9, loss = 0.21641175\n",
      "Iteration 8, loss = 0.25791712\n",
      "Iteration 9, loss = 0.22498035\n",
      "Iteration 10, loss = 0.20779009\n",
      "Iteration 10, loss = 0.21059463\n",
      "Iteration 11, loss = 0.21477220\n",
      "Iteration 11, loss = 0.18482865\n",
      "Iteration 12, loss = 0.18836825\n",
      "Iteration 13, loss = 0.17523289\n",
      "Iteration 12, loss = 0.20991327\n",
      "Iteration 13, loss = 0.18701175\n",
      "Iteration 14, loss = 0.15076553\n",
      "Iteration 14, loss = 0.17107189\n",
      "Iteration 15, loss = 0.17780048\n",
      "Iteration 15, loss = 0.14174543\n",
      "Iteration 16, loss = 0.13708012\n",
      "Iteration 17, loss = 0.15094554\n",
      "Iteration 16, loss = 0.14895883\n",
      "Iteration 18, loss = 0.14322840\n",
      "Iteration 19, loss = 0.13028966\n",
      "Iteration 20, loss = 0.13329124\n",
      "Iteration 17, loss = 0.15123451\n",
      "Iteration 21, loss = 0.14085469\n",
      "Iteration 18, loss = 0.16187537\n",
      "Iteration 22, loss = 0.15588561\n",
      "Iteration 19, loss = 0.17289421\n",
      "Iteration 23, loss = 0.18424503\n",
      "Iteration 24, loss = 0.13372123\n",
      "Iteration 20, loss = 0.13047078\n",
      "Iteration 21, loss = 0.12689101\n",
      "Iteration 25, loss = 0.10646947\n",
      "Iteration 22, loss = 0.13060978\n",
      "Iteration 26, loss = 0.10958127\n",
      "Iteration 27, loss = 0.10554796\n",
      "Iteration 23, loss = 0.13321481\n",
      "Iteration 28, loss = 0.09301067\n",
      "Iteration 24, loss = 0.15056299\n",
      "Iteration 25, loss = 0.16020059\n",
      "Iteration 29, loss = 0.08324638\n",
      "Iteration 26, loss = 0.14033358\n",
      "Iteration 30, loss = 0.10300813\n",
      "Iteration 31, loss = 0.09246138\n",
      "Iteration 27, loss = 0.12873483\n",
      "Iteration 28, loss = 0.11284331\n",
      "Iteration 32, loss = 0.11646995\n",
      "Iteration 29, loss = 0.12385566\n",
      "Iteration 30, loss = 0.10271774\n",
      "Iteration 33, loss = 0.08941319\n",
      "Iteration 34, loss = 0.08710847\n",
      "Iteration 31, loss = 0.09031141\n",
      "Iteration 35, loss = 0.07930567\n",
      "Iteration 36, loss = 0.07867325\n",
      "Iteration 32, loss = 0.09604031\n",
      "Iteration 37, loss = 0.08331936\n",
      "Iteration 33, loss = 0.09923643\n",
      "Iteration 34, loss = 0.10612657\n",
      "Iteration 38, loss = 0.09183258\n",
      "Iteration 35, loss = 0.10569584\n",
      "Iteration 39, loss = 0.09605654\n",
      "Iteration 40, loss = 0.12593411\n",
      "Iteration 41, loss = 0.10502701\n",
      "Iteration 36, loss = 0.08987422\n",
      "Iteration 42, loss = 0.07407931\n",
      "Iteration 43, loss = 0.07232445\n",
      "Iteration 44, loss = 0.08583548\n",
      "Iteration 45, loss = 0.06143550\n",
      "Iteration 37, loss = 0.08599184\n",
      "Iteration 46, loss = 0.06332441\n",
      "Iteration 38, loss = 0.08285449\n",
      "Iteration 47, loss = 0.06326247\n",
      "Iteration 39, loss = 0.05862774\n",
      "Iteration 40, loss = 0.07270818\n",
      "Iteration 48, loss = 0.05347435\n",
      "Iteration 41, loss = 0.06161896\n",
      "Iteration 49, loss = 0.05017786\n",
      "Iteration 42, loss = 0.06895639\n",
      "Iteration 50, loss = 0.05199330\n",
      "Iteration 43, loss = 0.05742277\n",
      "Iteration 51, loss = 0.05077851\n",
      "Iteration 44, loss = 0.07229622\n",
      "Iteration 52, loss = 0.07991528\n",
      "Iteration 45, loss = 0.06037333\n",
      "Iteration 53, loss = 0.07304194\n",
      "Iteration 46, loss = 0.04961360\n",
      "Iteration 54, loss = 0.08108479\n",
      "Iteration 55, loss = 0.06903169\n",
      "Iteration 47, loss = 0.05955894\n",
      "Iteration 56, loss = 0.07521211\n",
      "Iteration 48, loss = 0.06387896\n",
      "Iteration 49, loss = 0.06079036\n",
      "Iteration 50, loss = 0.05486310\n",
      "Iteration 57, loss = 0.08192312\n",
      "Iteration 51, loss = 0.08231745\n",
      "Iteration 58, loss = 0.09020417\n",
      "Iteration 52, loss = 0.06324469\n",
      "Iteration 59, loss = 0.08047770\n",
      "Iteration 53, loss = 0.07396454\n",
      "Iteration 60, loss = 0.10946860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.05928115\n",
      "Iteration 55, loss = 0.04713449\n",
      "Iteration 56, loss = 0.04015232\n",
      "Iteration 57, loss = 0.04019168\n",
      "Iteration 58, loss = 0.03870136\n",
      "Iteration 59, loss = 0.04473031\n",
      "Iteration 60, loss = 0.06870597\n",
      "Iteration 61, loss = 0.04846910\n",
      "Iteration 1, loss = 1.28692006\n",
      "Iteration 62, loss = 0.06317590\n",
      "Iteration 2, loss = 0.49674133\n",
      "Iteration 63, loss = 0.04058593\n",
      "Iteration 64, loss = 0.03461537\n",
      "Iteration 65, loss = 0.04043150\n",
      "Iteration 3, loss = 0.40658825\n",
      "Iteration 4, loss = 0.34682409\n",
      "Iteration 5, loss = 0.31423085\n",
      "Iteration 66, loss = 0.03198484\n",
      "Iteration 6, loss = 0.28641227\n",
      "Iteration 67, loss = 0.03341757\n",
      "Iteration 68, loss = 0.04165125\n",
      "Iteration 7, loss = 0.27088872\n",
      "Iteration 69, loss = 0.02739260\n",
      "Iteration 8, loss = 0.25119587\n",
      "Iteration 9, loss = 0.26358723\n",
      "Iteration 70, loss = 0.03921671\n",
      "Iteration 10, loss = 0.23177612\n",
      "Iteration 71, loss = 0.04498708\n",
      "Iteration 72, loss = 0.05148661\n",
      "Iteration 73, loss = 0.06008415\n",
      "Iteration 11, loss = 0.22528300\n",
      "Iteration 74, loss = 0.03628811\n",
      "Iteration 75, loss = 0.03318421\n",
      "Iteration 76, loss = 0.03054631\n",
      "Iteration 12, loss = 0.20875854\n",
      "Iteration 77, loss = 0.03051431\n",
      "Iteration 13, loss = 0.19542943\n",
      "Iteration 78, loss = 0.02754492\n",
      "Iteration 79, loss = 0.03346988\n",
      "Iteration 14, loss = 0.17601572\n",
      "Iteration 80, loss = 0.04851507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.15918208\n",
      "Iteration 16, loss = 0.14886519\n",
      "Iteration 17, loss = 0.14326455\n",
      "Iteration 18, loss = 0.13882888\n",
      "Iteration 19, loss = 0.12675905\n",
      "Iteration 1, loss = 1.08892725\n",
      "Iteration 20, loss = 0.12774059\n",
      "Iteration 2, loss = 0.55627679\n",
      "Iteration 21, loss = 0.12897498\n",
      "Iteration 3, loss = 0.40990785\n",
      "Iteration 22, loss = 0.12585815\n",
      "Iteration 4, loss = 0.35970049\n",
      "Iteration 23, loss = 0.11686617\n",
      "Iteration 24, loss = 0.11049736\n",
      "Iteration 5, loss = 0.31543412\n",
      "Iteration 6, loss = 0.28919467\n",
      "Iteration 25, loss = 0.10343627\n",
      "Iteration 7, loss = 0.27073369\n",
      "Iteration 26, loss = 0.10770862\n",
      "Iteration 27, loss = 0.10183587\n",
      "Iteration 28, loss = 0.10408963\n",
      "Iteration 8, loss = 0.26482619\n",
      "Iteration 9, loss = 0.23488643\n",
      "Iteration 29, loss = 0.11129427\n",
      "Iteration 10, loss = 0.21900764\n",
      "Iteration 11, loss = 0.20709796\n",
      "Iteration 30, loss = 0.09871973\n",
      "Iteration 31, loss = 0.09483392\n",
      "Iteration 12, loss = 0.19353550\n",
      "Iteration 32, loss = 0.11294363\n",
      "Iteration 33, loss = 0.12103858\n",
      "Iteration 13, loss = 0.17435295\n",
      "Iteration 14, loss = 0.18065635\n",
      "Iteration 34, loss = 0.11994816\n",
      "Iteration 15, loss = 0.18563722\n",
      "Iteration 35, loss = 0.16316829\n",
      "Iteration 16, loss = 0.18932338\n",
      "Iteration 17, loss = 0.16565392\n",
      "Iteration 36, loss = 0.16033330\n",
      "Iteration 37, loss = 0.14526626\n",
      "Iteration 18, loss = 0.14881847\n",
      "Iteration 38, loss = 0.13552880\n",
      "Iteration 19, loss = 0.12911998\n",
      "Iteration 39, loss = 0.10253849\n",
      "Iteration 20, loss = 0.13368203\n",
      "Iteration 21, loss = 0.11809346\n",
      "Iteration 40, loss = 0.12095300\n",
      "Iteration 22, loss = 0.11175962\n",
      "Iteration 41, loss = 0.11934516\n",
      "Iteration 42, loss = 0.17038522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.11567475\n",
      "Iteration 24, loss = 0.10107734\n",
      "Iteration 25, loss = 0.09896889\n",
      "Iteration 26, loss = 0.10173886\n",
      "Iteration 27, loss = 0.08443987\n",
      "Iteration 28, loss = 0.10006411\n",
      "Iteration 29, loss = 0.09581609\n",
      "Iteration 30, loss = 0.09763409\n",
      "Iteration 31, loss = 0.10694369\n",
      "Iteration 32, loss = 0.09916348\n",
      "Iteration 33, loss = 0.09677466\n",
      "Iteration 1, loss = 0.98837397\n",
      "Iteration 34, loss = 0.09377233\n",
      "Iteration 2, loss = 0.46247434\n",
      "Iteration 35, loss = 0.10041344\n",
      "Iteration 3, loss = 0.38541013\n",
      "Iteration 36, loss = 0.08537176\n",
      "Iteration 4, loss = 0.30636875\n",
      "Iteration 37, loss = 0.10381017\n",
      "Iteration 5, loss = 0.32027732\n",
      "Iteration 38, loss = 0.08737737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 0.31060217\n",
      "Iteration 7, loss = 0.28647987\n",
      "Iteration 8, loss = 0.23777931\n",
      "Iteration 9, loss = 0.21286155\n",
      "Iteration 10, loss = 0.20121095\n",
      "Iteration 11, loss = 0.17373427\n",
      "Iteration 12, loss = 0.15502308\n",
      "Iteration 13, loss = 0.14840517\n",
      "Iteration 14, loss = 0.13700796\n",
      "Iteration 15, loss = 0.12752328\n",
      "Iteration 16, loss = 0.11808680\n",
      "Iteration 17, loss = 0.13916977\n",
      "Iteration 18, loss = 0.10366257\n",
      "Iteration 1, loss = 1.00248276\n",
      "Iteration 19, loss = 0.10365127\n",
      "Iteration 2, loss = 0.54128166\n",
      "Iteration 3, loss = 0.45218737\n",
      "Iteration 4, loss = 0.39730321\n",
      "Iteration 20, loss = 0.09113293\n",
      "Iteration 5, loss = 0.31233982\n",
      "Iteration 21, loss = 0.09013664\n",
      "Iteration 6, loss = 0.28838388\n",
      "Iteration 22, loss = 0.10027159\n",
      "Iteration 7, loss = 0.26156554\n",
      "Iteration 23, loss = 0.08101352\n",
      "Iteration 24, loss = 0.08455318\n",
      "Iteration 25, loss = 0.06879719\n",
      "Iteration 8, loss = 0.24397198\n",
      "Iteration 26, loss = 0.07996002\n",
      "Iteration 27, loss = 0.06495113\n",
      "Iteration 28, loss = 0.06691246\n",
      "Iteration 29, loss = 0.06529105\n",
      "Iteration 9, loss = 0.22101939\n",
      "Iteration 30, loss = 0.06114668\n",
      "Iteration 31, loss = 0.07400876\n",
      "Iteration 32, loss = 0.11538532\n",
      "Iteration 10, loss = 0.21954005\n",
      "Iteration 33, loss = 0.17467435\n",
      "Iteration 34, loss = 0.10873703\n",
      "Iteration 35, loss = 0.09993669\n",
      "Iteration 11, loss = 0.19489073\n",
      "Iteration 36, loss = 0.10756402\n",
      "Iteration 12, loss = 0.18166758\n",
      "Iteration 13, loss = 0.18283170\n",
      "Iteration 37, loss = 0.11384847\n",
      "Iteration 14, loss = 0.17931676\n",
      "Iteration 38, loss = 0.14015733\n",
      "Iteration 15, loss = 0.15612248\n",
      "Iteration 39, loss = 0.11076076\n",
      "Iteration 16, loss = 0.16092734\n",
      "Iteration 40, loss = 0.11763667\n",
      "Iteration 17, loss = 0.14545807\n",
      "Iteration 41, loss = 0.08421322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.12761353\n",
      "Iteration 19, loss = 0.13971949\n",
      "Iteration 20, loss = 0.12410004\n",
      "Iteration 21, loss = 0.13489986\n",
      "Iteration 22, loss = 0.12876972\n",
      "Iteration 23, loss = 0.12529262\n",
      "Iteration 24, loss = 0.16434704\n",
      "Iteration 25, loss = 0.14522823\n",
      "Iteration 26, loss = 0.15053454\n",
      "Iteration 27, loss = 0.14412508\n",
      "Iteration 28, loss = 0.10919750\n",
      "Iteration 29, loss = 0.11646750\n",
      "Iteration 30, loss = 0.10066007\n",
      "Iteration 31, loss = 0.09065781\n",
      "Iteration 1, loss = 1.04824162\n",
      "Iteration 32, loss = 0.09181340\n",
      "Iteration 2, loss = 0.53632105\n",
      "Iteration 33, loss = 0.09420536\n",
      "Iteration 3, loss = 0.41236944\n",
      "Iteration 34, loss = 0.09611470\n",
      "Iteration 4, loss = 0.34395556\n",
      "Iteration 35, loss = 0.10330297\n",
      "Iteration 5, loss = 0.31253295\n",
      "Iteration 36, loss = 0.09111440\n",
      "Iteration 37, loss = 0.09179525\n",
      "Iteration 6, loss = 0.26364330\n",
      "Iteration 38, loss = 0.08309532\n",
      "Iteration 7, loss = 0.23940705\n",
      "Iteration 39, loss = 0.13362917\n",
      "Iteration 8, loss = 0.22623198\n",
      "Iteration 40, loss = 0.14044453\n",
      "Iteration 41, loss = 0.17944270\n",
      "Iteration 9, loss = 0.20957031\n",
      "Iteration 10, loss = 0.19610891\n",
      "Iteration 42, loss = 0.12379364\n",
      "Iteration 11, loss = 0.18670242\n",
      "Iteration 43, loss = 0.11044698\n",
      "Iteration 12, loss = 0.16839250\n",
      "Iteration 13, loss = 0.15103919\n",
      "Iteration 44, loss = 0.11403892\n",
      "Iteration 45, loss = 0.09556938\n",
      "Iteration 14, loss = 0.14989743\n",
      "Iteration 46, loss = 0.07306479\n",
      "Iteration 47, loss = 0.07357571\n",
      "Iteration 15, loss = 0.13751344\n",
      "Iteration 16, loss = 0.12581068\n",
      "Iteration 17, loss = 0.12311547\n",
      "Iteration 48, loss = 0.06534842\n",
      "Iteration 49, loss = 0.05858627\n",
      "Iteration 50, loss = 0.05515564\n",
      "Iteration 18, loss = 0.11040184\n",
      "Iteration 51, loss = 0.05479624\n",
      "Iteration 19, loss = 0.11411166\n",
      "Iteration 20, loss = 0.11180572\n",
      "Iteration 52, loss = 0.05687314\n",
      "Iteration 21, loss = 0.13921601\n",
      "Iteration 53, loss = 0.05198135\n",
      "Iteration 22, loss = 0.14974348\n",
      "Iteration 54, loss = 0.05127326\n",
      "Iteration 55, loss = 0.04695656\n",
      "Iteration 56, loss = 0.05164545\n",
      "Iteration 23, loss = 0.15731515\n",
      "Iteration 24, loss = 0.18125347\n",
      "Iteration 57, loss = 0.04920440\n",
      "Iteration 25, loss = 0.11961640\n",
      "Iteration 58, loss = 0.06251000\n",
      "Iteration 26, loss = 0.12412638\n",
      "Iteration 59, loss = 0.06671674\n",
      "Iteration 27, loss = 0.12999533\n",
      "Iteration 60, loss = 0.05730084\n",
      "Iteration 28, loss = 0.09914407\n",
      "Iteration 29, loss = 0.12289118\n",
      "Iteration 61, loss = 0.06197685\n",
      "Iteration 30, loss = 0.12760980\n",
      "Iteration 62, loss = 0.05793506\n",
      "Iteration 63, loss = 0.06893342\n",
      "Iteration 31, loss = 0.09791549\n",
      "Iteration 64, loss = 0.06269628\n",
      "Iteration 32, loss = 0.07841019\n",
      "Iteration 33, loss = 0.08289647\n",
      "Iteration 65, loss = 0.05509681\n",
      "Iteration 34, loss = 0.09322811\n",
      "Iteration 66, loss = 0.05298638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.07899829\n",
      "Iteration 36, loss = 0.08981316\n",
      "Iteration 37, loss = 0.10252075\n",
      "Iteration 38, loss = 0.11748353\n",
      "Iteration 39, loss = 0.10422575\n",
      "Iteration 40, loss = 0.09282979\n",
      "Iteration 1, loss = 0.99655049\n",
      "Iteration 2, loss = 0.48977514\n",
      "Iteration 41, loss = 0.06774547\n",
      "Iteration 3, loss = 0.41415829\n",
      "Iteration 42, loss = 0.07122998\n",
      "Iteration 4, loss = 0.35828629\n",
      "Iteration 5, loss = 0.30390293\n",
      "Iteration 6, loss = 0.28061997\n",
      "Iteration 43, loss = 0.06767254\n",
      "Iteration 7, loss = 0.25062977\n",
      "Iteration 8, loss = 0.23050925\n",
      "Iteration 44, loss = 0.06247771\n",
      "Iteration 9, loss = 0.21486276\n",
      "Iteration 45, loss = 0.05018303\n",
      "Iteration 10, loss = 0.19884018\n",
      "Iteration 46, loss = 0.05298610\n",
      "Iteration 11, loss = 0.18364413\n",
      "Iteration 47, loss = 0.05270461\n",
      "Iteration 12, loss = 0.18202854\n",
      "Iteration 48, loss = 0.05954310\n",
      "Iteration 13, loss = 0.16162955\n",
      "Iteration 49, loss = 0.06451768\n",
      "Iteration 14, loss = 0.15781483\n",
      "Iteration 50, loss = 0.05920497\n",
      "Iteration 15, loss = 0.14416921\n",
      "Iteration 16, loss = 0.13496149\n",
      "Iteration 51, loss = 0.05209753\n",
      "Iteration 17, loss = 0.13714585\n",
      "Iteration 52, loss = 0.04381451\n",
      "Iteration 18, loss = 0.13461510\n",
      "Iteration 53, loss = 0.03962349\n",
      "Iteration 19, loss = 0.12926054\n",
      "Iteration 54, loss = 0.03713051\n",
      "Iteration 20, loss = 0.12508552\n",
      "Iteration 55, loss = 0.03497140\n",
      "Iteration 21, loss = 0.12219796\n",
      "Iteration 56, loss = 0.03578331\n",
      "Iteration 22, loss = 0.10914345\n",
      "Iteration 23, loss = 0.09027633\n",
      "Iteration 57, loss = 0.03138631\n",
      "Iteration 24, loss = 0.08859759\n",
      "Iteration 58, loss = 0.03145956\n",
      "Iteration 25, loss = 0.08701465\n",
      "Iteration 59, loss = 0.03077068\n",
      "Iteration 60, loss = 0.03300845\n",
      "Iteration 26, loss = 0.09194416\n",
      "Iteration 61, loss = 0.03859370\n",
      "Iteration 27, loss = 0.08422246\n",
      "Iteration 62, loss = 0.04641188\n",
      "Iteration 28, loss = 0.09323826\n",
      "Iteration 29, loss = 0.08789487\n",
      "Iteration 30, loss = 0.08513007\n",
      "Iteration 63, loss = 0.04784567\n",
      "Iteration 64, loss = 0.05653438\n",
      "Iteration 31, loss = 0.11273591\n",
      "Iteration 65, loss = 0.05174555\n",
      "Iteration 66, loss = 0.04009276\n",
      "Iteration 32, loss = 0.12172928\n",
      "Iteration 33, loss = 0.11102465\n",
      "Iteration 67, loss = 0.03371396\n",
      "Iteration 34, loss = 0.09109181\n",
      "Iteration 68, loss = 0.05331983\n",
      "Iteration 35, loss = 0.09399203\n",
      "Iteration 69, loss = 0.03824025\n",
      "Iteration 70, loss = 0.03106212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.08223266\n",
      "Iteration 37, loss = 0.07540518\n",
      "Iteration 38, loss = 0.08788373\n",
      "Iteration 39, loss = 0.06555427\n",
      "Iteration 40, loss = 0.08794510\n",
      "Iteration 41, loss = 0.09224761\n",
      "Iteration 42, loss = 0.10738849\n",
      "Iteration 43, loss = 0.09119316\n",
      "Iteration 44, loss = 0.11569066\n",
      "Iteration 45, loss = 0.07803760\n",
      "Iteration 46, loss = 0.10265084\n",
      "Iteration 47, loss = 0.08747270\n",
      "Iteration 48, loss = 0.07326860\n",
      "Iteration 49, loss = 0.07764332\n",
      "Iteration 1, loss = 0.90980098\n",
      "Iteration 2, loss = 0.50038068\n",
      "Iteration 50, loss = 0.08478453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.39655000\n",
      "Iteration 4, loss = 0.34875247\n",
      "Iteration 5, loss = 0.33158284\n",
      "Iteration 6, loss = 0.28599579\n",
      "Iteration 7, loss = 0.25931329\n",
      "Iteration 8, loss = 0.23540332\n",
      "Iteration 9, loss = 0.22457131\n",
      "Iteration 10, loss = 0.22913364\n",
      "Iteration 11, loss = 0.20184921\n",
      "Iteration 1, loss = 0.96451309\n",
      "Iteration 12, loss = 0.19346382\n",
      "Iteration 2, loss = 0.46313273\n",
      "Iteration 13, loss = 0.17972432\n",
      "Iteration 3, loss = 0.40468132\n",
      "Iteration 14, loss = 0.15861725\n",
      "Iteration 4, loss = 0.35632248\n",
      "Iteration 15, loss = 0.14421057\n",
      "Iteration 5, loss = 0.29525706\n",
      "Iteration 16, loss = 0.13412691\n",
      "Iteration 6, loss = 0.26664961\n",
      "Iteration 17, loss = 0.12969740\n",
      "Iteration 7, loss = 0.24915169\n",
      "Iteration 18, loss = 0.12429489\n",
      "Iteration 8, loss = 0.23977690\n",
      "Iteration 19, loss = 0.12169446\n",
      "Iteration 9, loss = 0.22361996\n",
      "Iteration 20, loss = 0.11566284\n",
      "Iteration 10, loss = 0.20123030\n",
      "Iteration 21, loss = 0.11550053\n",
      "Iteration 11, loss = 0.18956579\n",
      "Iteration 12, loss = 0.19306172\n",
      "Iteration 22, loss = 0.11522638\n",
      "Iteration 13, loss = 0.19528270\n",
      "Iteration 23, loss = 0.10353897\n",
      "Iteration 14, loss = 0.17046179\n",
      "Iteration 24, loss = 0.10774513\n",
      "Iteration 15, loss = 0.16733908\n",
      "Iteration 25, loss = 0.12889486\n",
      "Iteration 16, loss = 0.15383469\n",
      "Iteration 26, loss = 0.12828159\n",
      "Iteration 17, loss = 0.14959062\n",
      "Iteration 27, loss = 0.13584348\n",
      "Iteration 18, loss = 0.15720460\n",
      "Iteration 19, loss = 0.12886863\n",
      "Iteration 28, loss = 0.16687343\n",
      "Iteration 20, loss = 0.12859290\n",
      "Iteration 29, loss = 0.15950647\n",
      "Iteration 21, loss = 0.11625067\n",
      "Iteration 30, loss = 0.15342194\n",
      "Iteration 31, loss = 0.14338661\n",
      "Iteration 22, loss = 0.12487731\n",
      "Iteration 23, loss = 0.10409446\n",
      "Iteration 32, loss = 0.13392813\n",
      "Iteration 24, loss = 0.12119295\n",
      "Iteration 25, loss = 0.17637472\n",
      "Iteration 26, loss = 0.12998233\n",
      "Iteration 33, loss = 0.13472239\n",
      "Iteration 27, loss = 0.10449089\n",
      "Iteration 34, loss = 0.12574894\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.10708282\n",
      "Iteration 29, loss = 0.11716892\n",
      "Iteration 30, loss = 0.10525894\n",
      "Iteration 31, loss = 0.09635237\n",
      "Iteration 32, loss = 0.08109958\n",
      "Iteration 33, loss = 0.10149101\n",
      "Iteration 34, loss = 0.09546484\n",
      "Iteration 35, loss = 0.09953367\n",
      "Iteration 36, loss = 0.06774045\n",
      "Iteration 1, loss = 1.01500433\n",
      "Iteration 2, loss = 0.57567466\n",
      "Iteration 37, loss = 0.06831097\n",
      "Iteration 3, loss = 0.42039192\n",
      "Iteration 4, loss = 0.36274283\n",
      "Iteration 38, loss = 0.06400722\n",
      "Iteration 39, loss = 0.06646354\n",
      "Iteration 40, loss = 0.06692023\n",
      "Iteration 5, loss = 0.35258018\n",
      "Iteration 41, loss = 0.08134023\n",
      "Iteration 6, loss = 0.29435043\n",
      "Iteration 7, loss = 0.26820398\n",
      "Iteration 42, loss = 0.06693327\n",
      "Iteration 8, loss = 0.24791767\n",
      "Iteration 43, loss = 0.08717303\n",
      "Iteration 44, loss = 0.08992458\n",
      "Iteration 45, loss = 0.07711045\n",
      "Iteration 9, loss = 0.24609859\n",
      "Iteration 10, loss = 0.22317274\n",
      "Iteration 11, loss = 0.21002421\n",
      "Iteration 46, loss = 0.05947936\n",
      "Iteration 47, loss = 0.05064669\n",
      "Iteration 12, loss = 0.21334276\n",
      "Iteration 48, loss = 0.05317256\n",
      "Iteration 49, loss = 0.06046216\n",
      "Iteration 13, loss = 0.20921565\n",
      "Iteration 14, loss = 0.16417191\n",
      "Iteration 15, loss = 0.16059530\n",
      "Iteration 50, loss = 0.06547092\n",
      "Iteration 51, loss = 0.04112186\n",
      "Iteration 52, loss = 0.04750509\n",
      "Iteration 16, loss = 0.15478258\n",
      "Iteration 53, loss = 0.03899214\n",
      "Iteration 17, loss = 0.15541425\n",
      "Iteration 54, loss = 0.04700839\n",
      "Iteration 18, loss = 0.14214247\n",
      "Iteration 55, loss = 0.06201006\n",
      "Iteration 19, loss = 0.13479090\n",
      "Iteration 56, loss = 0.06582942\n",
      "Iteration 57, loss = 0.09294819\n",
      "Iteration 20, loss = 0.13601394\n",
      "Iteration 58, loss = 0.09214284\n",
      "Iteration 59, loss = 0.08179112\n",
      "Iteration 60, loss = 0.07829021\n",
      "Iteration 21, loss = 0.12226962Iteration 61, loss = 0.07472867\n",
      "\n",
      "Iteration 62, loss = 0.08770841\n",
      "Iteration 63, loss = 0.10374317\n",
      "Iteration 22, loss = 0.11724027\n",
      "Iteration 64, loss = 0.09621345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.12156924\n",
      "Iteration 24, loss = 0.10367791\n",
      "Iteration 25, loss = 0.09782577\n",
      "Iteration 26, loss = 0.09962182\n",
      "Iteration 27, loss = 0.10415011\n",
      "Iteration 28, loss = 0.10516736\n",
      "Iteration 29, loss = 0.10055896\n",
      "Iteration 30, loss = 0.11859283\n",
      "Iteration 1, loss = 1.01806736\n",
      "Iteration 31, loss = 0.10187847\n",
      "Iteration 2, loss = 0.48648047\n",
      "Iteration 32, loss = 0.11053058\n",
      "Iteration 3, loss = 0.43852692\n",
      "Iteration 33, loss = 0.12860635\n",
      "Iteration 4, loss = 0.35574851\n",
      "Iteration 34, loss = 0.12886128\n",
      "Iteration 5, loss = 0.30959101\n",
      "Iteration 6, loss = 0.25936829\n",
      "Iteration 35, loss = 0.11351767\n",
      "Iteration 7, loss = 0.25336765\n",
      "Iteration 8, loss = 0.22976682\n",
      "Iteration 36, loss = 0.08753488\n",
      "Iteration 9, loss = 0.21296791\n",
      "Iteration 10, loss = 0.19679123\n",
      "Iteration 37, loss = 0.08347151\n",
      "Iteration 11, loss = 0.18606671\n",
      "Iteration 38, loss = 0.06659521\n",
      "Iteration 12, loss = 0.16939487\n",
      "Iteration 39, loss = 0.06554139\n",
      "Iteration 13, loss = 0.15755318\n",
      "Iteration 14, loss = 0.14763208\n",
      "Iteration 15, loss = 0.16697283\n",
      "Iteration 40, loss = 0.05930047\n",
      "Iteration 16, loss = 0.15078574\n",
      "Iteration 41, loss = 0.06194078\n",
      "Iteration 42, loss = 0.05322040\n",
      "Iteration 17, loss = 0.14084638\n",
      "Iteration 43, loss = 0.06163470\n",
      "Iteration 44, loss = 0.06187390\n",
      "Iteration 18, loss = 0.14094544\n",
      "Iteration 45, loss = 0.07129098\n",
      "Iteration 19, loss = 0.13312517\n",
      "Iteration 20, loss = 0.15953289\n",
      "Iteration 46, loss = 0.07661463\n",
      "Iteration 21, loss = 0.13864806\n",
      "Iteration 47, loss = 0.17736890\n",
      "Iteration 48, loss = 0.12540476\n",
      "Iteration 49, loss = 0.16638439\n",
      "Iteration 22, loss = 0.11983998\n",
      "Iteration 50, loss = 0.13070160\n",
      "Iteration 23, loss = 0.11333542\n",
      "Iteration 51, loss = 0.10858277\n",
      "Iteration 24, loss = 0.11218798\n",
      "Iteration 52, loss = 0.11994719\n",
      "Iteration 25, loss = 0.12969758\n",
      "Iteration 53, loss = 0.06977858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.12446287\n",
      "Iteration 27, loss = 0.11048264\n",
      "Iteration 28, loss = 0.16225871\n",
      "Iteration 29, loss = 0.13576901\n",
      "Iteration 30, loss = 0.14317707\n",
      "Iteration 1, loss = 1.12626404\n",
      "Iteration 31, loss = 0.15262171\n",
      "Iteration 32, loss = 0.12426271\n",
      "Iteration 2, loss = 0.50044890\n",
      "Iteration 33, loss = 0.10543207\n",
      "Iteration 3, loss = 0.38801378\n",
      "Iteration 34, loss = 0.09989424\n",
      "Iteration 4, loss = 0.35181692\n",
      "Iteration 5, loss = 0.30424306\n",
      "Iteration 35, loss = 0.09139206\n",
      "Iteration 6, loss = 0.25930815\n",
      "Iteration 36, loss = 0.09196219\n",
      "Iteration 37, loss = 0.07357676\n",
      "Iteration 7, loss = 0.24414682\n",
      "Iteration 38, loss = 0.07756661\n",
      "Iteration 8, loss = 0.22528135\n",
      "Iteration 9, loss = 0.21908048\n",
      "Iteration 39, loss = 0.07539790\n",
      "Iteration 10, loss = 0.19759819\n",
      "Iteration 11, loss = 0.18852728\n",
      "Iteration 40, loss = 0.07113682\n",
      "Iteration 12, loss = 0.19576015\n",
      "Iteration 41, loss = 0.07589512\n",
      "Iteration 42, loss = 0.07888652\n",
      "Iteration 43, loss = 0.07423601\n",
      "Iteration 13, loss = 0.19060434\n",
      "Iteration 44, loss = 0.06014432\n",
      "Iteration 45, loss = 0.06530303\n",
      "Iteration 14, loss = 0.17536397\n",
      "Iteration 15, loss = 0.17286971\n",
      "Iteration 16, loss = 0.16196040\n",
      "Iteration 46, loss = 0.05816071\n",
      "Iteration 17, loss = 0.17973042\n",
      "Iteration 47, loss = 0.05526895\n",
      "Iteration 18, loss = 0.16583774\n",
      "Iteration 48, loss = 0.06168960\n",
      "Iteration 49, loss = 0.04418583\n",
      "Iteration 19, loss = 0.13283395\n",
      "Iteration 50, loss = 0.07232939\n",
      "Iteration 20, loss = 0.12841328\n",
      "Iteration 51, loss = 0.07636171\n",
      "Iteration 21, loss = 0.11895023\n",
      "Iteration 52, loss = 0.16267836\n",
      "Iteration 22, loss = 0.10903581\n",
      "Iteration 53, loss = 0.11799067\n",
      "Iteration 23, loss = 0.09938215\n",
      "Iteration 54, loss = 0.12789751\n",
      "Iteration 24, loss = 0.11510412\n",
      "Iteration 25, loss = 0.10961453\n",
      "Iteration 55, loss = 0.12384439\n",
      "Iteration 26, loss = 0.10234878\n",
      "Iteration 56, loss = 0.14619405\n",
      "Iteration 27, loss = 0.09661756\n",
      "Iteration 57, loss = 0.15270629\n",
      "Iteration 28, loss = 0.08510802\n",
      "Iteration 58, loss = 0.16825376\n",
      "Iteration 29, loss = 0.08653969\n",
      "Iteration 59, loss = 0.13761372\n",
      "Iteration 60, loss = 0.13892905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.11130509\n",
      "Iteration 31, loss = 0.10243699\n",
      "Iteration 32, loss = 0.09112076\n",
      "Iteration 33, loss = 0.07648298\n",
      "Iteration 34, loss = 0.07772521\n",
      "Iteration 35, loss = 0.11866503\n",
      "Iteration 36, loss = 0.10397155\n",
      "Iteration 1, loss = 1.18830233\n",
      "Iteration 37, loss = 0.08445273\n",
      "Iteration 38, loss = 0.06830915\n",
      "Iteration 39, loss = 0.09065719\n",
      "Iteration 2, loss = 0.52764420\n",
      "Iteration 3, loss = 0.42116742\n",
      "Iteration 4, loss = 0.36523184\n",
      "Iteration 40, loss = 0.07932859\n",
      "Iteration 5, loss = 0.29079143\n",
      "Iteration 41, loss = 0.06881773\n",
      "Iteration 42, loss = 0.07568702\n",
      "Iteration 43, loss = 0.08072238\n",
      "Iteration 6, loss = 0.24334436\n",
      "Iteration 7, loss = 0.24880861\n",
      "Iteration 44, loss = 0.08907902\n",
      "Iteration 8, loss = 0.20339369\n",
      "Iteration 9, loss = 0.18659703\n",
      "Iteration 45, loss = 0.09762073\n",
      "Iteration 46, loss = 0.11686687\n",
      "Iteration 47, loss = 0.13952279\n",
      "Iteration 10, loss = 0.16661795\n",
      "Iteration 48, loss = 0.12160515\n",
      "Iteration 11, loss = 0.16149065\n",
      "Iteration 12, loss = 0.14947511\n",
      "Iteration 13, loss = 0.14659403\n",
      "Iteration 49, loss = 0.15417422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.12362500\n",
      "Iteration 15, loss = 0.11803268\n",
      "Iteration 16, loss = 0.11236803\n",
      "Iteration 17, loss = 0.10984231\n",
      "Iteration 18, loss = 0.11492645\n",
      "Iteration 19, loss = 0.12358628\n",
      "Iteration 20, loss = 0.13045017\n",
      "Iteration 21, loss = 0.11426948\n",
      "Iteration 22, loss = 0.10782304\n",
      "Iteration 23, loss = 0.08825780\n",
      "Iteration 24, loss = 0.07447031\n",
      "Iteration 25, loss = 0.07545718\n",
      "Iteration 1, loss = 0.92725220\n",
      "Iteration 26, loss = 0.08315597\n",
      "Iteration 2, loss = 0.52374882\n",
      "Iteration 27, loss = 0.07781834\n",
      "Iteration 3, loss = 0.43959469\n",
      "Iteration 28, loss = 0.07226536\n",
      "Iteration 4, loss = 0.40145813\n",
      "Iteration 29, loss = 0.07222384\n",
      "Iteration 5, loss = 0.34171686\n",
      "Iteration 30, loss = 0.06603091\n",
      "Iteration 31, loss = 0.07673265\n",
      "Iteration 6, loss = 0.30384909\n",
      "Iteration 32, loss = 0.05885403\n",
      "Iteration 7, loss = 0.26814998\n",
      "Iteration 8, loss = 0.24734092\n",
      "Iteration 9, loss = 0.24035690\n",
      "Iteration 33, loss = 0.07010360\n",
      "Iteration 34, loss = 0.09179020\n",
      "Iteration 10, loss = 0.21382742\n",
      "Iteration 35, loss = 0.09515801\n",
      "Iteration 36, loss = 0.11203074\n",
      "Iteration 11, loss = 0.20025174\n",
      "Iteration 37, loss = 0.13148435\n",
      "Iteration 38, loss = 0.11500888\n",
      "Iteration 39, loss = 0.11542143\n",
      "Iteration 12, loss = 0.19055292\n",
      "Iteration 40, loss = 0.09312946\n",
      "Iteration 41, loss = 0.10099360\n",
      "Iteration 42, loss = 0.09055586\n",
      "Iteration 13, loss = 0.17369278\n",
      "Iteration 43, loss = 0.11089609\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.16942585\n",
      "Iteration 15, loss = 0.16479534\n",
      "Iteration 16, loss = 0.15426847\n",
      "Iteration 17, loss = 0.13578152\n",
      "Iteration 18, loss = 0.13302052\n",
      "Iteration 1, loss = 1.04965618\n",
      "Iteration 19, loss = 0.14144484\n",
      "Iteration 20, loss = 0.13662629\n",
      "Iteration 2, loss = 0.46889175\n",
      "Iteration 21, loss = 0.12835302\n",
      "Iteration 22, loss = 0.13394043\n",
      "Iteration 3, loss = 0.40463460\n",
      "Iteration 23, loss = 0.14439152\n",
      "Iteration 4, loss = 0.35353488\n",
      "Iteration 24, loss = 0.12898735\n",
      "Iteration 5, loss = 0.30212042\n",
      "Iteration 25, loss = 0.14293165Iteration 6, loss = 0.28648742\n",
      "\n",
      "Iteration 7, loss = 0.23555126\n",
      "Iteration 8, loss = 0.21871898\n",
      "Iteration 9, loss = 0.19516485\n",
      "Iteration 26, loss = 0.11751321\n",
      "Iteration 27, loss = 0.13431236\n",
      "Iteration 10, loss = 0.19449117\n",
      "Iteration 28, loss = 0.15151278\n",
      "Iteration 29, loss = 0.13586220\n",
      "Iteration 11, loss = 0.18220051\n",
      "Iteration 12, loss = 0.16296331\n",
      "Iteration 30, loss = 0.12636118\n",
      "Iteration 13, loss = 0.15071063\n",
      "Iteration 14, loss = 0.13693137\n",
      "Iteration 31, loss = 0.10981117\n",
      "Iteration 32, loss = 0.11565369\n",
      "Iteration 15, loss = 0.13846867\n",
      "Iteration 33, loss = 0.09804267\n",
      "Iteration 34, loss = 0.10350599\n",
      "Iteration 16, loss = 0.12508861\n",
      "Iteration 17, loss = 0.11995266\n",
      "Iteration 18, loss = 0.12257878\n",
      "Iteration 19, loss = 0.10494157\n",
      "Iteration 35, loss = 0.08306793\n",
      "Iteration 36, loss = 0.08287601\n",
      "Iteration 37, loss = 0.08446641\n",
      "Iteration 20, loss = 0.12027641\n",
      "Iteration 21, loss = 0.15668565\n",
      "Iteration 38, loss = 0.08505079\n",
      "Iteration 22, loss = 0.12848006\n",
      "Iteration 23, loss = 0.13931444\n",
      "Iteration 39, loss = 0.07542589\n",
      "Iteration 24, loss = 0.12311164\n",
      "Iteration 25, loss = 0.09571371\n",
      "Iteration 26, loss = 0.10977333\n",
      "Iteration 40, loss = 0.09091831\n",
      "Iteration 27, loss = 0.10463363\n",
      "Iteration 41, loss = 0.06751463\n",
      "Iteration 42, loss = 0.06528977\n",
      "Iteration 43, loss = 0.06015186\n",
      "Iteration 28, loss = 0.09898813\n",
      "Iteration 44, loss = 0.06707095\n",
      "Iteration 45, loss = 0.05769980\n",
      "Iteration 29, loss = 0.10431566\n",
      "Iteration 46, loss = 0.04843740\n",
      "Iteration 30, loss = 0.11256999\n",
      "Iteration 47, loss = 0.05235697\n",
      "Iteration 31, loss = 0.10647493\n",
      "Iteration 48, loss = 0.05815743\n",
      "Iteration 32, loss = 0.09833303\n",
      "Iteration 49, loss = 0.05555850\n",
      "Iteration 33, loss = 0.11026170\n",
      "Iteration 50, loss = 0.05042648\n",
      "Iteration 34, loss = 0.08878224\n",
      "Iteration 35, loss = 0.09434744\n",
      "Iteration 36, loss = 0.08923158\n",
      "Iteration 51, loss = 0.04723544\n",
      "Iteration 37, loss = 0.11750854\n",
      "Iteration 38, loss = 0.10781621\n",
      "Iteration 52, loss = 0.04517660\n",
      "Iteration 39, loss = 0.08532695\n",
      "Iteration 53, loss = 0.05476229\n",
      "Iteration 54, loss = 0.04840745\n",
      "Iteration 40, loss = 0.07441782\n",
      "Iteration 55, loss = 0.05093129\n",
      "Iteration 41, loss = 0.08547662\n",
      "Iteration 56, loss = 0.05078369\n",
      "Iteration 42, loss = 0.07970121\n",
      "Iteration 57, loss = 0.05071081\n",
      "Iteration 43, loss = 0.07823967\n",
      "Iteration 58, loss = 0.07487502\n",
      "Iteration 44, loss = 0.10912632\n",
      "Iteration 59, loss = 0.14185781\n",
      "Iteration 45, loss = 0.08969760\n",
      "Iteration 60, loss = 0.10396354\n",
      "Iteration 46, loss = 0.05935604\n",
      "Iteration 47, loss = 0.07133941\n",
      "Iteration 61, loss = 0.10002313\n",
      "Iteration 48, loss = 0.08852216\n",
      "Iteration 62, loss = 0.13974372\n",
      "Iteration 49, loss = 0.08250892\n",
      "Iteration 63, loss = 0.16010511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.09878237\n",
      "Iteration 51, loss = 0.08317023\n",
      "Iteration 52, loss = 0.06848246\n",
      "Iteration 53, loss = 0.07232465\n",
      "Iteration 54, loss = 0.05612628\n",
      "Iteration 55, loss = 0.06384383\n",
      "Iteration 56, loss = 0.04957162\n",
      "Iteration 57, loss = 0.04167867\n",
      "Iteration 58, loss = 0.03796385\n",
      "Iteration 59, loss = 0.04048379\n",
      "Iteration 60, loss = 0.03577184\n",
      "Iteration 61, loss = 0.04491099\n",
      "Iteration 62, loss = 0.03633409\n",
      "Iteration 63, loss = 0.04304393\n",
      "Iteration 1, loss = 1.08402620\n",
      "Iteration 2, loss = 0.55119632\n",
      "Iteration 64, loss = 0.03971511\n",
      "Iteration 65, loss = 0.04274327\n",
      "Iteration 3, loss = 0.43866596\n",
      "Iteration 66, loss = 0.04081151\n",
      "Iteration 67, loss = 0.06669266\n",
      "Iteration 4, loss = 0.37022617\n",
      "Iteration 5, loss = 0.32272885\n",
      "Iteration 6, loss = 0.30151656\n",
      "Iteration 68, loss = 0.04408809\n",
      "Iteration 69, loss = 0.03607031\n",
      "Iteration 7, loss = 0.27299099\n",
      "Iteration 70, loss = 0.04834813\n",
      "Iteration 71, loss = 0.04917593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.25492881\n",
      "Iteration 9, loss = 0.21870788\n",
      "Iteration 10, loss = 0.20963913\n",
      "Iteration 11, loss = 0.20797805\n",
      "Iteration 12, loss = 0.19056316\n",
      "Iteration 13, loss = 0.18465400\n",
      "Iteration 14, loss = 0.19074459\n",
      "Iteration 15, loss = 0.16362741\n",
      "Iteration 16, loss = 0.15223949\n",
      "Iteration 17, loss = 0.13995759\n",
      "Iteration 18, loss = 0.13515000\n",
      "Iteration 19, loss = 0.13490562\n",
      "Iteration 20, loss = 0.12326303\n",
      "Iteration 1, loss = 1.27750455\n",
      "Iteration 21, loss = 0.12683046\n",
      "Iteration 2, loss = 0.56083152\n",
      "Iteration 22, loss = 0.13888188\n",
      "Iteration 3, loss = 0.48414791\n",
      "Iteration 4, loss = 0.38883272\n",
      "Iteration 5, loss = 0.35195708\n",
      "Iteration 23, loss = 0.11975944\n",
      "Iteration 24, loss = 0.12262572\n",
      "Iteration 25, loss = 0.12691695\n",
      "Iteration 6, loss = 0.31259228\n",
      "Iteration 26, loss = 0.16227291\n",
      "Iteration 7, loss = 0.28673209\n",
      "Iteration 8, loss = 0.26838934\n",
      "Iteration 9, loss = 0.24518511\n",
      "Iteration 10, loss = 0.23423513\n",
      "Iteration 27, loss = 0.10930201\n",
      "Iteration 11, loss = 0.20859841\n",
      "Iteration 28, loss = 0.10773302\n",
      "Iteration 12, loss = 0.20542482\n",
      "Iteration 29, loss = 0.10421537\n",
      "Iteration 13, loss = 0.20955139\n",
      "Iteration 30, loss = 0.09929284\n",
      "Iteration 14, loss = 0.20557948\n",
      "Iteration 15, loss = 0.17364017\n",
      "Iteration 16, loss = 0.16327866\n",
      "Iteration 17, loss = 0.15237175\n",
      "Iteration 31, loss = 0.09998434\n",
      "Iteration 18, loss = 0.15859314\n",
      "Iteration 19, loss = 0.15107803\n",
      "Iteration 20, loss = 0.14496774\n",
      "Iteration 32, loss = 0.11658684\n",
      "Iteration 21, loss = 0.13990270\n",
      "Iteration 22, loss = 0.14312040\n",
      "Iteration 33, loss = 0.11815074\n",
      "Iteration 23, loss = 0.15778713\n",
      "Iteration 34, loss = 0.10251602\n",
      "Iteration 24, loss = 0.15867241\n",
      "Iteration 35, loss = 0.12753831\n",
      "Iteration 25, loss = 0.16030778\n",
      "Iteration 36, loss = 0.11293549\n",
      "Iteration 26, loss = 0.19577189\n",
      "Iteration 37, loss = 0.10491000\n",
      "Iteration 27, loss = 0.15309090\n",
      "Iteration 38, loss = 0.09774624\n",
      "Iteration 28, loss = 0.15469951\n",
      "Iteration 39, loss = 0.08769094\n",
      "Iteration 29, loss = 0.15428805\n",
      "Iteration 30, loss = 0.13291634\n",
      "Iteration 31, loss = 0.12086222\n",
      "Iteration 40, loss = 0.07060520\n",
      "Iteration 32, loss = 0.10064085\n",
      "Iteration 33, loss = 0.11076391\n",
      "Iteration 41, loss = 0.06991680\n",
      "Iteration 34, loss = 0.10334995\n",
      "Iteration 42, loss = 0.06752415\n",
      "Iteration 35, loss = 0.09575649\n",
      "Iteration 36, loss = 0.09325716\n",
      "Iteration 43, loss = 0.06559219\n",
      "Iteration 37, loss = 0.07977256\n",
      "Iteration 38, loss = 0.09672782\n",
      "Iteration 44, loss = 0.08536137\n",
      "Iteration 39, loss = 0.09287776\n",
      "Iteration 40, loss = 0.08519212\n",
      "Iteration 45, loss = 0.16186729\n",
      "Iteration 41, loss = 0.09469763\n",
      "Iteration 46, loss = 0.10570553\n",
      "Iteration 42, loss = 0.08913431\n",
      "Iteration 43, loss = 0.08593235\n",
      "Iteration 47, loss = 0.12955674\n",
      "Iteration 44, loss = 0.08107226\n",
      "Iteration 45, loss = 0.07517267\n",
      "Iteration 46, loss = 0.06932354\n",
      "Iteration 48, loss = 0.13918679\n",
      "Iteration 47, loss = 0.06784235\n",
      "Iteration 48, loss = 0.06632053\n",
      "Iteration 49, loss = 0.06584425\n",
      "Iteration 50, loss = 0.07743632\n",
      "Iteration 49, loss = 0.13093609\n",
      "Iteration 51, loss = 0.09899792\n",
      "Iteration 52, loss = 0.13136009\n",
      "Iteration 50, loss = 0.18045298\n",
      "Iteration 53, loss = 0.11927410\n",
      "Iteration 54, loss = 0.11426183\n",
      "Iteration 55, loss = 0.11466070\n",
      "Iteration 51, loss = 0.16865586\n",
      "Iteration 56, loss = 0.09698398\n",
      "Iteration 57, loss = 0.14540903\n",
      "Iteration 58, loss = 0.11001823\n",
      "Iteration 59, loss = 0.09080786\n",
      "Iteration 52, loss = 0.13035632\n",
      "Iteration 60, loss = 0.10615715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.10418739\n",
      "Iteration 54, loss = 0.10371691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.45968491\n",
      "Iteration 1, loss = 1.12729313\n",
      "Iteration 2, loss = 0.61593331\n",
      "Iteration 2, loss = 0.61657772\n",
      "Iteration 3, loss = 0.43448227\n",
      "Iteration 4, loss = 0.40151408\n",
      "Iteration 3, loss = 0.50302093\n",
      "Iteration 5, loss = 0.32239820\n",
      "Iteration 4, loss = 0.42403871\n",
      "Iteration 6, loss = 0.28545107\n",
      "Iteration 5, loss = 0.35332771\n",
      "Iteration 6, loss = 0.32898228\n",
      "Iteration 7, loss = 0.27248364\n",
      "Iteration 8, loss = 0.25864735\n",
      "Iteration 7, loss = 0.29958743\n",
      "Iteration 9, loss = 0.22906962\n",
      "Iteration 10, loss = 0.21520675\n",
      "Iteration 8, loss = 0.27997652\n",
      "Iteration 11, loss = 0.20102663\n",
      "Iteration 9, loss = 0.26578608\n",
      "Iteration 12, loss = 0.19401358\n",
      "Iteration 10, loss = 0.23152887\n",
      "Iteration 13, loss = 0.18258683\n",
      "Iteration 11, loss = 0.22015023\n",
      "Iteration 12, loss = 0.21451012\n",
      "Iteration 14, loss = 0.16016483\n",
      "Iteration 13, loss = 0.18929629\n",
      "Iteration 14, loss = 0.17859307\n",
      "Iteration 15, loss = 0.15822764\n",
      "Iteration 15, loss = 0.16857449\n",
      "Iteration 16, loss = 0.16112450\n",
      "Iteration 17, loss = 0.15158070\n",
      "Iteration 16, loss = 0.15210966\n",
      "Iteration 18, loss = 0.13145789\n",
      "Iteration 17, loss = 0.15682056\n",
      "Iteration 19, loss = 0.13104962\n",
      "Iteration 18, loss = 0.12856359\n",
      "Iteration 20, loss = 0.13638780\n",
      "Iteration 19, loss = 0.13093265\n",
      "Iteration 21, loss = 0.12791487\n",
      "Iteration 20, loss = 0.15935505\n",
      "Iteration 22, loss = 0.13790114\n",
      "Iteration 21, loss = 0.11662320\n",
      "Iteration 23, loss = 0.13020269\n",
      "Iteration 22, loss = 0.10956315\n",
      "Iteration 23, loss = 0.10083305\n",
      "Iteration 24, loss = 0.19527266\n",
      "Iteration 24, loss = 0.11992738\n",
      "Iteration 25, loss = 0.16214876\n",
      "Iteration 25, loss = 0.09750879\n",
      "Iteration 26, loss = 0.15022807\n",
      "Iteration 27, loss = 0.18037896\n",
      "Iteration 26, loss = 0.10153785\n",
      "Iteration 28, loss = 0.14317266\n",
      "Iteration 27, loss = 0.11358576\n",
      "Iteration 28, loss = 0.12112987\n",
      "Iteration 29, loss = 0.12807779\n",
      "Iteration 30, loss = 0.12755405\n",
      "Iteration 29, loss = 0.12073421\n",
      "Iteration 31, loss = 0.15075043\n",
      "Iteration 30, loss = 0.13181905\n",
      "Iteration 32, loss = 0.12822095\n",
      "Iteration 31, loss = 0.11741366\n",
      "Iteration 33, loss = 0.10890463\n",
      "Iteration 34, loss = 0.13799158\n",
      "Iteration 32, loss = 0.11938353\n",
      "Iteration 33, loss = 0.09946809\n",
      "Iteration 35, loss = 0.11248112\n",
      "Iteration 34, loss = 0.08969920\n",
      "Iteration 36, loss = 0.09882419\n",
      "Iteration 35, loss = 0.07445853\n",
      "Iteration 37, loss = 0.09771875\n",
      "Iteration 36, loss = 0.05934316\n",
      "Iteration 38, loss = 0.08254785\n",
      "Iteration 37, loss = 0.06961933\n",
      "Iteration 39, loss = 0.07978404\n",
      "Iteration 38, loss = 0.07379932\n",
      "Iteration 40, loss = 0.07580300\n",
      "Iteration 39, loss = 0.07054668\n",
      "Iteration 41, loss = 0.07315489\n",
      "Iteration 42, loss = 0.07282746\n",
      "Iteration 40, loss = 0.07078108\n",
      "Iteration 43, loss = 0.07982793\n",
      "Iteration 41, loss = 0.06381710\n",
      "Iteration 44, loss = 0.08038232\n",
      "Iteration 42, loss = 0.06986022\n",
      "Iteration 45, loss = 0.07722644\n",
      "Iteration 46, loss = 0.07519472\n",
      "Iteration 47, loss = 0.08335532\n",
      "Iteration 48, loss = 0.05811808\n",
      "Iteration 43, loss = 0.04958050\n",
      "Iteration 44, loss = 0.09196251\n",
      "Iteration 45, loss = 0.07865542\n",
      "Iteration 49, loss = 0.05647932\n",
      "Iteration 46, loss = 0.13711978\n",
      "Iteration 50, loss = 0.05609950\n",
      "Iteration 51, loss = 0.05618260\n",
      "Iteration 52, loss = 0.05334472\n",
      "Iteration 47, loss = 0.12930518\n",
      "Iteration 53, loss = 0.06507909\n",
      "Iteration 48, loss = 0.09103724\n",
      "Iteration 49, loss = 0.11552290\n",
      "Iteration 54, loss = 0.06574810\n",
      "Iteration 55, loss = 0.04542574\n",
      "Iteration 50, loss = 0.05153369\n",
      "Iteration 56, loss = 0.04827677\n",
      "Iteration 57, loss = 0.04906262\n",
      "Iteration 51, loss = 0.06327389\n",
      "Iteration 52, loss = 0.04557015\n",
      "Iteration 58, loss = 0.04793571\n",
      "Iteration 53, loss = 0.03969631\n",
      "Iteration 54, loss = 0.03302075\n",
      "Iteration 59, loss = 0.04162585\n",
      "Iteration 55, loss = 0.04921681\n",
      "Iteration 60, loss = 0.05708886\n",
      "Iteration 56, loss = 0.05425044\n",
      "Iteration 61, loss = 0.05914945\n",
      "Iteration 57, loss = 0.03863215\n",
      "Iteration 58, loss = 0.03446194\n",
      "Iteration 62, loss = 0.08258789\n",
      "Iteration 59, loss = 0.06286611\n",
      "Iteration 60, loss = 0.06473600\n",
      "Iteration 63, loss = 0.06120670\n",
      "Iteration 61, loss = 0.09815398\n",
      "Iteration 64, loss = 0.06898567\n",
      "Iteration 62, loss = 0.12672839\n",
      "Iteration 65, loss = 0.05642101\n",
      "Iteration 63, loss = 0.06075190\n",
      "Iteration 66, loss = 0.05687903\n",
      "Iteration 64, loss = 0.07437335\n",
      "Iteration 67, loss = 0.06715488\n",
      "Iteration 65, loss = 0.05416548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 68, loss = 0.08768734\n",
      "Iteration 69, loss = 0.07186427\n",
      "Iteration 70, loss = 0.08878094\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95675826\n",
      "Iteration 2, loss = 0.49653781\n",
      "Iteration 3, loss = 0.39508980\n",
      "Iteration 4, loss = 0.35248771\n",
      "Iteration 5, loss = 0.30560753\n",
      "Iteration 6, loss = 0.27714006\n",
      "Iteration 7, loss = 0.26011291\n",
      "Iteration 8, loss = 0.23368737\n",
      "Iteration 9, loss = 0.21447908\n",
      "Iteration 10, loss = 0.19975862\n",
      "Iteration 1, loss = 1.27053799\n",
      "Iteration 11, loss = 0.18842178\n",
      "Iteration 2, loss = 0.55823206\n",
      "Iteration 12, loss = 0.18780923\n",
      "Iteration 3, loss = 0.48177773\n",
      "Iteration 13, loss = 0.17081413\n",
      "Iteration 14, loss = 0.17335419\n",
      "Iteration 4, loss = 0.38286432\n",
      "Iteration 15, loss = 0.17910534\n",
      "Iteration 16, loss = 0.14683780\n",
      "Iteration 5, loss = 0.34606770\n",
      "Iteration 17, loss = 0.15795887\n",
      "Iteration 6, loss = 0.31874765\n",
      "Iteration 7, loss = 0.27239980\n",
      "Iteration 18, loss = 0.13821505\n",
      "Iteration 19, loss = 0.17301276\n",
      "Iteration 20, loss = 0.14330306\n",
      "Iteration 8, loss = 0.24767250\n",
      "Iteration 21, loss = 0.12011894\n",
      "Iteration 22, loss = 0.10660532\n",
      "Iteration 9, loss = 0.22423237\n",
      "Iteration 23, loss = 0.12360382\n",
      "Iteration 24, loss = 0.14695639\n",
      "Iteration 25, loss = 0.13610625\n",
      "Iteration 10, loss = 0.21122160\n",
      "Iteration 26, loss = 0.12642528\n",
      "Iteration 27, loss = 0.18686575\n",
      "Iteration 11, loss = 0.19759639\n",
      "Iteration 28, loss = 0.18309546\n",
      "Iteration 12, loss = 0.18939424\n",
      "Iteration 29, loss = 0.15249222\n",
      "Iteration 13, loss = 0.17234985\n",
      "Iteration 30, loss = 0.12328191\n",
      "Iteration 14, loss = 0.16524935\n",
      "Iteration 31, loss = 0.10270543\n",
      "Iteration 32, loss = 0.10477039\n",
      "Iteration 33, loss = 0.08746524\n",
      "Iteration 15, loss = 0.17360267\n",
      "Iteration 34, loss = 0.08584659\n",
      "Iteration 16, loss = 0.16562710\n",
      "Iteration 35, loss = 0.07575700\n",
      "Iteration 17, loss = 0.14277020\n",
      "Iteration 36, loss = 0.07849626\n",
      "Iteration 18, loss = 0.14085995\n",
      "Iteration 37, loss = 0.07582566\n",
      "Iteration 19, loss = 0.13342785\n",
      "Iteration 38, loss = 0.07646810\n",
      "Iteration 20, loss = 0.13644075\n",
      "Iteration 39, loss = 0.09048582\n",
      "Iteration 40, loss = 0.09179757\n",
      "Iteration 41, loss = 0.06058275\n",
      "Iteration 21, loss = 0.12658380\n",
      "Iteration 22, loss = 0.13054835\n",
      "Iteration 42, loss = 0.05976509\n",
      "Iteration 23, loss = 0.13250431\n",
      "Iteration 43, loss = 0.06735581\n",
      "Iteration 24, loss = 0.12429793\n",
      "Iteration 44, loss = 0.06529679\n",
      "Iteration 25, loss = 0.10118096\n",
      "Iteration 45, loss = 0.06895019\n",
      "Iteration 26, loss = 0.10315763\n",
      "Iteration 46, loss = 0.08818421\n",
      "Iteration 27, loss = 0.09060963\n",
      "Iteration 47, loss = 0.07106693\n",
      "Iteration 28, loss = 0.09039096\n",
      "Iteration 48, loss = 0.05037624\n",
      "Iteration 29, loss = 0.08130751\n",
      "Iteration 49, loss = 0.07892826\n",
      "Iteration 30, loss = 0.08228167\n",
      "Iteration 50, loss = 0.06637707\n",
      "Iteration 51, loss = 0.06674625\n",
      "Iteration 31, loss = 0.09177356\n",
      "Iteration 52, loss = 0.08633522\n",
      "Iteration 32, loss = 0.06177968\n",
      "Iteration 33, loss = 0.06627720\n",
      "Iteration 53, loss = 0.05508212\n",
      "Iteration 34, loss = 0.07285940\n",
      "Iteration 54, loss = 0.05795064\n",
      "Iteration 35, loss = 0.08465208\n",
      "Iteration 55, loss = 0.07315893\n",
      "Iteration 36, loss = 0.07532384\n",
      "Iteration 56, loss = 0.07373099\n",
      "Iteration 37, loss = 0.07637279\n",
      "Iteration 57, loss = 0.11217242\n",
      "Iteration 38, loss = 0.07908674\n",
      "Iteration 58, loss = 0.06190107\n",
      "Iteration 39, loss = 0.10344334\n",
      "Iteration 59, loss = 0.08358941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.09639837\n",
      "Iteration 41, loss = 0.15380974\n",
      "Iteration 42, loss = 0.13372825\n",
      "Iteration 43, loss = 0.07694084\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99640242\n",
      "Iteration 2, loss = 0.46380538\n",
      "Iteration 3, loss = 0.38954363\n",
      "Iteration 4, loss = 0.32975601\n",
      "Iteration 5, loss = 0.29650649\n",
      "Iteration 1, loss = 1.24265385\n",
      "Iteration 6, loss = 0.28675140\n",
      "Iteration 2, loss = 0.58478699\n",
      "Iteration 7, loss = 0.25730435\n",
      "Iteration 8, loss = 0.23734576\n",
      "Iteration 3, loss = 0.44071007\n",
      "Iteration 9, loss = 0.20933413\n",
      "Iteration 4, loss = 0.38329811\n",
      "Iteration 10, loss = 0.18887496\n",
      "Iteration 5, loss = 0.34483352\n",
      "Iteration 11, loss = 0.18313448\n",
      "Iteration 6, loss = 0.30791557\n",
      "Iteration 12, loss = 0.17257405\n",
      "Iteration 7, loss = 0.28338717\n",
      "Iteration 13, loss = 0.18258358\n",
      "Iteration 8, loss = 0.25459402\n",
      "Iteration 14, loss = 0.18730679\n",
      "Iteration 9, loss = 0.23721576\n",
      "Iteration 15, loss = 0.18076736\n",
      "Iteration 16, loss = 0.17767450\n",
      "Iteration 10, loss = 0.23535665\n",
      "Iteration 17, loss = 0.18871725\n",
      "Iteration 18, loss = 0.17121485\n",
      "Iteration 19, loss = 0.17015167\n",
      "Iteration 11, loss = 0.21394882\n",
      "Iteration 20, loss = 0.13356869\n",
      "Iteration 21, loss = 0.12726121\n",
      "Iteration 22, loss = 0.12757693\n",
      "Iteration 12, loss = 0.18988657\n",
      "Iteration 13, loss = 0.17827995\n",
      "Iteration 14, loss = 0.17450228\n",
      "Iteration 15, loss = 0.17697829\n",
      "Iteration 23, loss = 0.12842688\n",
      "Iteration 24, loss = 0.10583038\n",
      "Iteration 16, loss = 0.15249082\n",
      "Iteration 25, loss = 0.10450535\n",
      "Iteration 26, loss = 0.11447968\n",
      "Iteration 27, loss = 0.12710788\n",
      "Iteration 17, loss = 0.16477547\n",
      "Iteration 28, loss = 0.10889702\n",
      "Iteration 29, loss = 0.11109035\n",
      "Iteration 30, loss = 0.10409970\n",
      "Iteration 18, loss = 0.14137436\n",
      "Iteration 31, loss = 0.13377968\n",
      "Iteration 19, loss = 0.13327141\n",
      "Iteration 32, loss = 0.11060543\n",
      "Iteration 20, loss = 0.13520705\n",
      "Iteration 33, loss = 0.12429272\n",
      "Iteration 21, loss = 0.13226352\n",
      "Iteration 22, loss = 0.14146442\n",
      "Iteration 34, loss = 0.10287735\n",
      "Iteration 23, loss = 0.15097347\n",
      "Iteration 35, loss = 0.08561806\n",
      "Iteration 24, loss = 0.13402869\n",
      "Iteration 36, loss = 0.09170041\n",
      "Iteration 25, loss = 0.14397534\n",
      "Iteration 37, loss = 0.08546677\n",
      "Iteration 26, loss = 0.13238882\n",
      "Iteration 38, loss = 0.10962989\n",
      "Iteration 27, loss = 0.14617691\n",
      "Iteration 39, loss = 0.08566693\n",
      "Iteration 28, loss = 0.16838294\n",
      "Iteration 40, loss = 0.09660918\n",
      "Iteration 29, loss = 0.15123764\n",
      "Iteration 41, loss = 0.08300552\n",
      "Iteration 30, loss = 0.14652473\n",
      "Iteration 42, loss = 0.07425108\n",
      "Iteration 31, loss = 0.12328618\n",
      "Iteration 43, loss = 0.06983972\n",
      "Iteration 32, loss = 0.12687948\n",
      "Iteration 44, loss = 0.08136540\n",
      "Iteration 33, loss = 0.10893941\n",
      "Iteration 45, loss = 0.09842167\n",
      "Iteration 34, loss = 0.10601432\n",
      "Iteration 35, loss = 0.09573336\n",
      "Iteration 46, loss = 0.13165632\n",
      "Iteration 36, loss = 0.08174208\n",
      "Iteration 37, loss = 0.11294520\n",
      "Iteration 38, loss = 0.12450328\n",
      "Iteration 47, loss = 0.08527468\n",
      "Iteration 48, loss = 0.10590966\n",
      "Iteration 39, loss = 0.11178685\n",
      "Iteration 49, loss = 0.08229197\n",
      "Iteration 40, loss = 0.10133655\n",
      "Iteration 50, loss = 0.07111143\n",
      "Iteration 41, loss = 0.07503844\n",
      "Iteration 42, loss = 0.07787717\n",
      "Iteration 51, loss = 0.08063398\n",
      "Iteration 43, loss = 0.06915343\n",
      "Iteration 44, loss = 0.06503675\n",
      "Iteration 45, loss = 0.06528061\n",
      "Iteration 52, loss = 0.10752953\n",
      "Iteration 53, loss = 0.16774868\n",
      "Iteration 46, loss = 0.07628643\n",
      "Iteration 54, loss = 0.16492987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.06653255\n",
      "Iteration 48, loss = 0.05595304\n",
      "Iteration 49, loss = 0.06080613\n",
      "Iteration 50, loss = 0.05689979\n",
      "Iteration 51, loss = 0.05324645\n",
      "Iteration 52, loss = 0.06239802\n",
      "Iteration 53, loss = 0.05180188\n",
      "Iteration 54, loss = 0.05467522\n",
      "Iteration 55, loss = 0.05964000\n",
      "Iteration 56, loss = 0.08097555\n",
      "Iteration 1, loss = 0.98180380\n",
      "Iteration 57, loss = 0.10913041\n",
      "Iteration 2, loss = 0.45755308\n",
      "Iteration 58, loss = 0.10711777\n",
      "Iteration 3, loss = 0.38125946\n",
      "Iteration 59, loss = 0.16531177\n",
      "Iteration 4, loss = 0.31589243\n",
      "Iteration 60, loss = 0.08980098\n",
      "Iteration 5, loss = 0.26905546\n",
      "Iteration 61, loss = 0.11622222\n",
      "Iteration 6, loss = 0.27683801\n",
      "Iteration 62, loss = 0.12017838\n",
      "Iteration 63, loss = 0.10274594\n",
      "Iteration 7, loss = 0.24828913\n",
      "Iteration 64, loss = 0.06685767\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.20195015\n",
      "Iteration 9, loss = 0.19091068\n",
      "Iteration 10, loss = 0.16440333\n",
      "Iteration 11, loss = 0.17057126\n",
      "Iteration 12, loss = 0.17085323\n",
      "Iteration 13, loss = 0.14405008\n",
      "Iteration 14, loss = 0.13917105\n",
      "Iteration 15, loss = 0.14604122\n",
      "Iteration 16, loss = 0.14209392\n",
      "Iteration 17, loss = 0.12553281\n",
      "Iteration 18, loss = 0.12228088\n",
      "Iteration 1, loss = 0.96189637\n",
      "Iteration 19, loss = 0.12754211\n",
      "Iteration 2, loss = 0.52066289\n",
      "Iteration 3, loss = 0.44522091\n",
      "Iteration 4, loss = 0.34911045\n",
      "Iteration 5, loss = 0.32336692\n",
      "Iteration 20, loss = 0.16261598\n",
      "Iteration 6, loss = 0.29773092\n",
      "Iteration 21, loss = 0.13585148\n",
      "Iteration 7, loss = 0.30556381\n",
      "Iteration 22, loss = 0.16658734\n",
      "Iteration 8, loss = 0.31053656\n",
      "Iteration 9, loss = 0.26190473\n",
      "Iteration 23, loss = 0.11995483\n",
      "Iteration 24, loss = 0.11339391\n",
      "Iteration 25, loss = 0.10470758\n",
      "Iteration 10, loss = 0.21782640\n",
      "Iteration 26, loss = 0.09439483\n",
      "Iteration 11, loss = 0.20222384\n",
      "Iteration 27, loss = 0.08131506\n",
      "Iteration 12, loss = 0.18853218\n",
      "Iteration 28, loss = 0.09248042\n",
      "Iteration 13, loss = 0.17912132\n",
      "Iteration 14, loss = 0.16535324\n",
      "Iteration 29, loss = 0.08086384\n",
      "Iteration 15, loss = 0.15597486\n",
      "Iteration 30, loss = 0.07998750\n",
      "Iteration 31, loss = 0.08778611\n",
      "Iteration 16, loss = 0.14910643\n",
      "Iteration 32, loss = 0.07353903\n",
      "Iteration 17, loss = 0.14468304\n",
      "Iteration 33, loss = 0.07572118\n",
      "Iteration 18, loss = 0.13273598\n",
      "Iteration 34, loss = 0.07258574\n",
      "Iteration 19, loss = 0.12591734\n",
      "Iteration 35, loss = 0.07021267\n",
      "Iteration 20, loss = 0.12294694\n",
      "Iteration 36, loss = 0.07690846\n",
      "Iteration 21, loss = 0.10875610\n",
      "Iteration 22, loss = 0.10542260\n",
      "Iteration 23, loss = 0.10126125\n",
      "Iteration 24, loss = 0.10542026\n",
      "Iteration 37, loss = 0.07515961\n",
      "Iteration 25, loss = 0.12476247\n",
      "Iteration 38, loss = 0.06769438\n",
      "Iteration 26, loss = 0.13216386\n",
      "Iteration 39, loss = 0.06593933\n",
      "Iteration 27, loss = 0.12823695\n",
      "Iteration 40, loss = 0.06147510\n",
      "Iteration 28, loss = 0.17633342\n",
      "Iteration 29, loss = 0.20841443\n",
      "Iteration 41, loss = 0.04539293\n",
      "Iteration 30, loss = 0.13813210\n",
      "Iteration 31, loss = 0.11932849\n",
      "Iteration 42, loss = 0.05145879\n",
      "Iteration 43, loss = 0.05206160\n",
      "Iteration 32, loss = 0.11047571\n",
      "Iteration 44, loss = 0.04875825\n",
      "Iteration 33, loss = 0.12067957\n",
      "Iteration 45, loss = 0.04582082\n",
      "Iteration 34, loss = 0.10756638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.05687223\n",
      "Iteration 47, loss = 0.06181555\n",
      "Iteration 48, loss = 0.05948406\n",
      "Iteration 49, loss = 0.05382581\n",
      "Iteration 50, loss = 0.06417027\n",
      "Iteration 51, loss = 0.10743036\n",
      "Iteration 52, loss = 0.09114973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01160533\n",
      "Iteration 2, loss = 0.45836729\n",
      "Iteration 3, loss = 0.43991237\n",
      "Iteration 4, loss = 0.36980631\n",
      "Iteration 5, loss = 0.32669297\n",
      "Iteration 6, loss = 0.30604916\n",
      "Iteration 7, loss = 0.26370677\n",
      "Iteration 1, loss = 0.96477282\n",
      "Iteration 8, loss = 0.24861736\n",
      "Iteration 2, loss = 0.50478118\n",
      "Iteration 9, loss = 0.23478391\n",
      "Iteration 3, loss = 0.40357004\n",
      "Iteration 10, loss = 0.20889483\n",
      "Iteration 11, loss = 0.19791436\n",
      "Iteration 4, loss = 0.31344452\n",
      "Iteration 5, loss = 0.27891051\n",
      "Iteration 12, loss = 0.19519104\n",
      "Iteration 6, loss = 0.26148248\n",
      "Iteration 13, loss = 0.18039859\n",
      "Iteration 7, loss = 0.23934755\n",
      "Iteration 8, loss = 0.22247163\n",
      "Iteration 14, loss = 0.16845895\n",
      "Iteration 15, loss = 0.15472732\n",
      "Iteration 9, loss = 0.19767682\n",
      "Iteration 16, loss = 0.13611874\n",
      "Iteration 10, loss = 0.18525843\n",
      "Iteration 17, loss = 0.14827342\n",
      "Iteration 11, loss = 0.17712651\n",
      "Iteration 12, loss = 0.16011447\n",
      "Iteration 18, loss = 0.13347593\n",
      "Iteration 13, loss = 0.14225037\n",
      "Iteration 19, loss = 0.11505449\n",
      "Iteration 14, loss = 0.13731171\n",
      "Iteration 20, loss = 0.10331347\n",
      "Iteration 15, loss = 0.13793892\n",
      "Iteration 21, loss = 0.10878208\n",
      "Iteration 16, loss = 0.14171717\n",
      "Iteration 22, loss = 0.10435061\n",
      "Iteration 17, loss = 0.12747102\n",
      "Iteration 23, loss = 0.11250621\n",
      "Iteration 18, loss = 0.11748888\n",
      "Iteration 24, loss = 0.14955193\n",
      "Iteration 19, loss = 0.10809883\n",
      "Iteration 20, loss = 0.10170388\n",
      "Iteration 25, loss = 0.12282405\n",
      "Iteration 26, loss = 0.08714151\n",
      "Iteration 21, loss = 0.10340717\n",
      "Iteration 22, loss = 0.08948494\n",
      "Iteration 27, loss = 0.11789056\n",
      "Iteration 28, loss = 0.11984258\n",
      "Iteration 23, loss = 0.10266309\n",
      "Iteration 24, loss = 0.09498209\n",
      "Iteration 29, loss = 0.08923141\n",
      "Iteration 25, loss = 0.09695916\n",
      "Iteration 30, loss = 0.09368201\n",
      "Iteration 31, loss = 0.07237836\n",
      "Iteration 26, loss = 0.10900365\n",
      "Iteration 32, loss = 0.06556029\n",
      "Iteration 33, loss = 0.06068782\n",
      "Iteration 27, loss = 0.09795552\n",
      "Iteration 34, loss = 0.05792953\n",
      "Iteration 28, loss = 0.10652522\n",
      "Iteration 35, loss = 0.06353806\n",
      "Iteration 29, loss = 0.09343337\n",
      "Iteration 36, loss = 0.05750937\n",
      "Iteration 37, loss = 0.07250473\n",
      "Iteration 30, loss = 0.10216542\n",
      "Iteration 38, loss = 0.08143781\n",
      "Iteration 39, loss = 0.08267887\n",
      "Iteration 31, loss = 0.10093484\n",
      "Iteration 40, loss = 0.09945727\n",
      "Iteration 32, loss = 0.09246578\n",
      "Iteration 33, loss = 0.09837604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.07592130\n",
      "Iteration 42, loss = 0.05587231\n",
      "Iteration 43, loss = 0.06339740\n",
      "Iteration 44, loss = 0.11536472\n",
      "Iteration 45, loss = 0.08557894\n",
      "Iteration 46, loss = 0.09662937\n",
      "Iteration 47, loss = 0.05845393\n",
      "Iteration 48, loss = 0.08085437\n",
      "Iteration 49, loss = 0.08867514\n",
      "Iteration 50, loss = 0.15048517\n",
      "Iteration 1, loss = 1.02852940\n",
      "Iteration 51, loss = 0.09589566\n",
      "Iteration 2, loss = 0.47692103\n",
      "Iteration 52, loss = 0.10989249\n",
      "Iteration 3, loss = 0.38743444\n",
      "Iteration 53, loss = 0.13069035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.33935792\n",
      "Iteration 5, loss = 0.31340586\n",
      "Iteration 6, loss = 0.27678095\n",
      "Iteration 7, loss = 0.26166838\n",
      "Iteration 8, loss = 0.23600495\n",
      "Iteration 9, loss = 0.20861026\n",
      "Iteration 10, loss = 0.19240686\n",
      "Iteration 11, loss = 0.18015216\n",
      "Iteration 12, loss = 0.16306061\n",
      "Iteration 13, loss = 0.15190209\n",
      "Iteration 14, loss = 0.14763694\n",
      "Iteration 15, loss = 0.13661482\n",
      "Iteration 16, loss = 0.13691492\n",
      "Iteration 17, loss = 0.12947760\n",
      "Iteration 1, loss = 0.92564080\n",
      "Iteration 18, loss = 0.12740525\n",
      "Iteration 2, loss = 0.49544793\n",
      "Iteration 3, loss = 0.44416575\n",
      "Iteration 4, loss = 0.35920635\n",
      "Iteration 19, loss = 0.15241372\n",
      "Iteration 5, loss = 0.31483284\n",
      "Iteration 6, loss = 0.29077546\n",
      "Iteration 20, loss = 0.21078167\n",
      "Iteration 21, loss = 0.18464410\n",
      "Iteration 7, loss = 0.25427529\n",
      "Iteration 22, loss = 0.14510505\n",
      "Iteration 8, loss = 0.25280406\n",
      "Iteration 23, loss = 0.13981114\n",
      "Iteration 9, loss = 0.23783606\n",
      "Iteration 24, loss = 0.13883481\n",
      "Iteration 10, loss = 0.22432504\n",
      "Iteration 11, loss = 0.19822864\n",
      "Iteration 25, loss = 0.11766133\n",
      "Iteration 12, loss = 0.17656653\n",
      "Iteration 26, loss = 0.10706714\n",
      "Iteration 13, loss = 0.15917792\n",
      "Iteration 27, loss = 0.10751578\n",
      "Iteration 14, loss = 0.15595178\n",
      "Iteration 28, loss = 0.10181710\n",
      "Iteration 15, loss = 0.15429722\n",
      "Iteration 16, loss = 0.13401847\n",
      "Iteration 29, loss = 0.10879075\n",
      "Iteration 17, loss = 0.12703277\n",
      "Iteration 30, loss = 0.11826339\n",
      "Iteration 31, loss = 0.12453197\n",
      "Iteration 18, loss = 0.13421869\n",
      "Iteration 32, loss = 0.12835031\n",
      "Iteration 19, loss = 0.13532929\n",
      "Iteration 20, loss = 0.14533459\n",
      "Iteration 33, loss = 0.12413630\n",
      "Iteration 21, loss = 0.14736599\n",
      "Iteration 34, loss = 0.15495540\n",
      "Iteration 35, loss = 0.13482468\n",
      "Iteration 36, loss = 0.15609903\n",
      "Iteration 22, loss = 0.13478346\n",
      "Iteration 37, loss = 0.11438041\n",
      "Iteration 23, loss = 0.12327294\n",
      "Iteration 24, loss = 0.14482877\n",
      "Iteration 25, loss = 0.17623902\n",
      "Iteration 38, loss = 0.08997031\n",
      "Iteration 26, loss = 0.16673681\n",
      "Iteration 39, loss = 0.10058173\n",
      "Iteration 40, loss = 0.08252282\n",
      "Iteration 27, loss = 0.14168360\n",
      "Iteration 41, loss = 0.07822519\n",
      "Iteration 28, loss = 0.11582316\n",
      "Iteration 42, loss = 0.09035828\n",
      "Iteration 29, loss = 0.13227101\n",
      "Iteration 43, loss = 0.10532017\n",
      "Iteration 30, loss = 0.11527269\n",
      "Iteration 44, loss = 0.10138972\n",
      "Iteration 31, loss = 0.09827558\n",
      "Iteration 32, loss = 0.09938421\n",
      "Iteration 45, loss = 0.08918943\n",
      "Iteration 46, loss = 0.08218112\n",
      "Iteration 33, loss = 0.09963285\n",
      "Iteration 34, loss = 0.10297047\n",
      "Iteration 47, loss = 0.07549961\n",
      "Iteration 35, loss = 0.09842580\n",
      "Iteration 48, loss = 0.06871015\n",
      "Iteration 49, loss = 0.08907835\n",
      "Iteration 36, loss = 0.08139878\n",
      "Iteration 50, loss = 0.09097189\n",
      "Iteration 37, loss = 0.07745343\n",
      "Iteration 38, loss = 0.08092856\n",
      "Iteration 51, loss = 0.08637285\n",
      "Iteration 39, loss = 0.08708925\n",
      "Iteration 52, loss = 0.13542681\n",
      "Iteration 53, loss = 0.10119790\n",
      "Iteration 40, loss = 0.07943169\n",
      "Iteration 54, loss = 0.09663078\n",
      "Iteration 41, loss = 0.08229077\n",
      "Iteration 42, loss = 0.06904547\n",
      "Iteration 55, loss = 0.08825867\n",
      "Iteration 43, loss = 0.06245441\n",
      "Iteration 56, loss = 0.10217622\n",
      "Iteration 57, loss = 0.06667285\n",
      "Iteration 44, loss = 0.06711036\n",
      "Iteration 58, loss = 0.06496085\n",
      "Iteration 45, loss = 0.06492668\n",
      "Iteration 59, loss = 0.06753581\n",
      "Iteration 46, loss = 0.07320808\n",
      "Iteration 47, loss = 0.06309799\n",
      "Iteration 60, loss = 0.07184875\n",
      "Iteration 61, loss = 0.09068451\n",
      "Iteration 62, loss = 0.10310148\n",
      "Iteration 48, loss = 0.05856358\n",
      "Iteration 63, loss = 0.06711519\n",
      "Iteration 49, loss = 0.07914346\n",
      "Iteration 50, loss = 0.08021838\n",
      "Iteration 51, loss = 0.07654976\n",
      "Iteration 64, loss = 0.06569912\n",
      "Iteration 65, loss = 0.06975146\n",
      "Iteration 52, loss = 0.07140282\n",
      "Iteration 66, loss = 0.07864332\n",
      "Iteration 67, loss = 0.05986272\n",
      "Iteration 68, loss = 0.08828745\n",
      "Iteration 53, loss = 0.07338384\n",
      "Iteration 69, loss = 0.06979163\n",
      "Iteration 70, loss = 0.07548098\n",
      "Iteration 71, loss = 0.05300802\n",
      "Iteration 72, loss = 0.05128186\n",
      "Iteration 54, loss = 0.05592123\n",
      "Iteration 73, loss = 0.05376818\n",
      "Iteration 74, loss = 0.05923966\n",
      "Iteration 55, loss = 0.05451564\n",
      "Iteration 56, loss = 0.05605204\n",
      "Iteration 57, loss = 0.05484133\n",
      "Iteration 75, loss = 0.05113802\n",
      "Iteration 76, loss = 0.06602555\n",
      "Iteration 58, loss = 0.06925722\n",
      "Iteration 59, loss = 0.05973244\n",
      "Iteration 60, loss = 0.04758544\n",
      "Iteration 77, loss = 0.04477900\n",
      "Iteration 78, loss = 0.03644661\n",
      "Iteration 61, loss = 0.07706962\n",
      "Iteration 79, loss = 0.06125723\n",
      "Iteration 62, loss = 0.05156903\n",
      "Iteration 80, loss = 0.07456287\n",
      "Iteration 63, loss = 0.04146737\n",
      "Iteration 64, loss = 0.04785401\n",
      "Iteration 81, loss = 0.09141334\n",
      "Iteration 65, loss = 0.05639186\n",
      "Iteration 82, loss = 0.07913320\n",
      "Iteration 66, loss = 0.06816006\n",
      "Iteration 67, loss = 0.11056110\n",
      "Iteration 83, loss = 0.08108289\n",
      "Iteration 84, loss = 0.10987521\n",
      "Iteration 68, loss = 0.09415066\n",
      "Iteration 69, loss = 0.09826687\n",
      "Iteration 85, loss = 0.07582113\n",
      "Iteration 70, loss = 0.06462426\n",
      "Iteration 86, loss = 0.06587362\n",
      "Iteration 71, loss = 0.04969243\n",
      "Iteration 87, loss = 0.05038134\n",
      "Iteration 88, loss = 0.05530951\n",
      "Iteration 72, loss = 0.07858193\n",
      "Iteration 73, loss = 0.10359292\n",
      "Iteration 89, loss = 0.07844126\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 74, loss = 0.08157697\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97063693\n",
      "Iteration 2, loss = 0.45537223\n",
      "Iteration 1, loss = 1.20762262\n",
      "Iteration 3, loss = 0.38519651\n",
      "Iteration 2, loss = 0.52711020\n",
      "Iteration 3, loss = 0.43936098\n",
      "Iteration 4, loss = 0.33955121\n",
      "Iteration 5, loss = 0.31281431\n",
      "Iteration 6, loss = 0.27578897\n",
      "Iteration 4, loss = 0.35784687\n",
      "Iteration 5, loss = 0.31705701\n",
      "Iteration 7, loss = 0.25826803\n",
      "Iteration 8, loss = 0.27187697\n",
      "Iteration 6, loss = 0.28199365\n",
      "Iteration 9, loss = 0.22741275\n",
      "Iteration 7, loss = 0.24942459\n",
      "Iteration 8, loss = 0.24128945\n",
      "Iteration 10, loss = 0.28301475\n",
      "Iteration 9, loss = 0.22136934\n",
      "Iteration 11, loss = 0.20997461\n",
      "Iteration 10, loss = 0.21855977\n",
      "Iteration 12, loss = 0.19013779\n",
      "Iteration 13, loss = 0.17488240\n",
      "Iteration 11, loss = 0.21337183\n",
      "Iteration 14, loss = 0.17775317\n",
      "Iteration 12, loss = 0.18889289\n",
      "Iteration 15, loss = 0.15049922\n",
      "Iteration 13, loss = 0.16543599\n",
      "Iteration 16, loss = 0.15108105\n",
      "Iteration 14, loss = 0.17275467\n",
      "Iteration 17, loss = 0.15219707\n",
      "Iteration 15, loss = 0.17026599\n",
      "Iteration 18, loss = 0.13273442\n",
      "Iteration 16, loss = 0.16014901\n",
      "Iteration 19, loss = 0.13558776\n",
      "Iteration 17, loss = 0.14709944\n",
      "Iteration 20, loss = 0.11461192\n",
      "Iteration 21, loss = 0.11148548\n",
      "Iteration 22, loss = 0.10528664\n",
      "Iteration 23, loss = 0.11943330\n",
      "Iteration 18, loss = 0.17434351\n",
      "Iteration 19, loss = 0.13985758\n",
      "Iteration 20, loss = 0.13983740\n",
      "Iteration 24, loss = 0.10833899\n",
      "Iteration 21, loss = 0.12475419\n",
      "Iteration 25, loss = 0.12206426\n",
      "Iteration 22, loss = 0.12011330\n",
      "Iteration 26, loss = 0.12249149\n",
      "Iteration 27, loss = 0.12169161\n",
      "Iteration 23, loss = 0.11948374\n",
      "Iteration 24, loss = 0.11281632\n",
      "Iteration 28, loss = 0.11578594\n",
      "Iteration 25, loss = 0.12244350\n",
      "Iteration 29, loss = 0.14004445\n",
      "Iteration 26, loss = 0.11726148\n",
      "Iteration 30, loss = 0.13739332\n",
      "Iteration 31, loss = 0.16886457\n",
      "Iteration 27, loss = 0.10690448\n",
      "Iteration 28, loss = 0.11061743\n",
      "Iteration 29, loss = 0.12476487\n",
      "Iteration 32, loss = 0.13396474\n",
      "Iteration 30, loss = 0.09920305\n",
      "Iteration 33, loss = 0.14121181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.12105955\n",
      "Iteration 32, loss = 0.09111875\n",
      "Iteration 33, loss = 0.09159927\n",
      "Iteration 34, loss = 0.08764780\n",
      "Iteration 35, loss = 0.08934487\n",
      "Iteration 36, loss = 0.09543161\n",
      "Iteration 37, loss = 0.10181619\n",
      "Iteration 38, loss = 0.10704722\n",
      "Iteration 39, loss = 0.09959249\n",
      "Iteration 40, loss = 0.12781506\n",
      "Iteration 1, loss = 1.37686576\n",
      "Iteration 41, loss = 0.12884471\n",
      "Iteration 2, loss = 0.50756118\n",
      "Iteration 3, loss = 0.41009433\n",
      "Iteration 42, loss = 0.09713807\n",
      "Iteration 4, loss = 0.32942569\n",
      "Iteration 43, loss = 0.10501219\n",
      "Iteration 44, loss = 0.15123847\n",
      "Iteration 5, loss = 0.31155725\n",
      "Iteration 6, loss = 0.28128730\n",
      "Iteration 7, loss = 0.25653843\n",
      "Iteration 45, loss = 0.14700428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.24465030\n",
      "Iteration 9, loss = 0.22857072\n",
      "Iteration 10, loss = 0.22347595\n",
      "Iteration 11, loss = 0.21640838\n",
      "Iteration 12, loss = 0.20760985\n",
      "Iteration 13, loss = 0.19173371\n",
      "Iteration 14, loss = 0.17561915\n",
      "Iteration 15, loss = 0.16018886\n",
      "Iteration 16, loss = 0.14267505\n",
      "Iteration 17, loss = 0.13794178\n",
      "Iteration 18, loss = 0.13859128\n",
      "Iteration 19, loss = 0.14180947\n",
      "Iteration 1, loss = 0.97791517\n",
      "Iteration 20, loss = 0.13278434\n",
      "Iteration 2, loss = 0.45082255\n",
      "Iteration 21, loss = 0.11838859\n",
      "Iteration 22, loss = 0.11166331\n",
      "Iteration 23, loss = 0.11702256\n",
      "Iteration 3, loss = 0.37542436\n",
      "Iteration 4, loss = 0.32708237\n",
      "Iteration 24, loss = 0.13641570\n",
      "Iteration 5, loss = 0.27300136\n",
      "Iteration 6, loss = 0.25439898\n",
      "Iteration 25, loss = 0.13796278\n",
      "Iteration 7, loss = 0.23730249\n",
      "Iteration 26, loss = 0.13913578\n",
      "Iteration 8, loss = 0.24196299\n",
      "Iteration 27, loss = 0.12269052\n",
      "Iteration 9, loss = 0.19954031\n",
      "Iteration 28, loss = 0.10215762\n",
      "Iteration 10, loss = 0.16790207\n",
      "Iteration 29, loss = 0.09519778\n",
      "Iteration 11, loss = 0.16170424\n",
      "Iteration 30, loss = 0.11241701\n",
      "Iteration 12, loss = 0.13959055\n",
      "Iteration 31, loss = 0.09282464\n",
      "Iteration 13, loss = 0.14924850\n",
      "Iteration 32, loss = 0.07861995\n",
      "Iteration 14, loss = 0.13416325\n",
      "Iteration 33, loss = 0.07466444\n",
      "Iteration 15, loss = 0.11664799\n",
      "Iteration 34, loss = 0.07184636\n",
      "Iteration 16, loss = 0.12691135\n",
      "Iteration 35, loss = 0.07622684\n",
      "Iteration 36, loss = 0.06671008\n",
      "Iteration 17, loss = 0.12885726\n",
      "Iteration 37, loss = 0.06562177\n",
      "Iteration 38, loss = 0.06094253\n",
      "Iteration 18, loss = 0.14723135\n",
      "Iteration 39, loss = 0.05398977\n",
      "Iteration 40, loss = 0.05815358\n",
      "Iteration 19, loss = 0.14235730\n",
      "Iteration 20, loss = 0.11293607\n",
      "Iteration 41, loss = 0.07906006\n",
      "Iteration 21, loss = 0.09371160\n",
      "Iteration 22, loss = 0.09000823\n",
      "Iteration 42, loss = 0.06130327\n",
      "Iteration 43, loss = 0.09331050\n",
      "Iteration 23, loss = 0.09371292\n",
      "Iteration 44, loss = 0.06930646\n",
      "Iteration 24, loss = 0.09482213\n",
      "Iteration 45, loss = 0.07965081\n",
      "Iteration 25, loss = 0.09962441\n",
      "Iteration 26, loss = 0.09993699\n",
      "Iteration 46, loss = 0.07090100\n",
      "Iteration 47, loss = 0.05313554\n",
      "Iteration 27, loss = 0.08233623\n",
      "Iteration 48, loss = 0.05257131\n",
      "Iteration 49, loss = 0.05067874\n",
      "Iteration 28, loss = 0.08058850\n",
      "Iteration 29, loss = 0.07323894\n",
      "Iteration 50, loss = 0.05206902\n",
      "Iteration 30, loss = 0.06707444\n",
      "Iteration 51, loss = 0.04347423\n",
      "Iteration 31, loss = 0.06137919\n",
      "Iteration 52, loss = 0.04390983\n",
      "Iteration 53, loss = 0.04691388\n",
      "Iteration 32, loss = 0.08231265\n",
      "Iteration 33, loss = 0.09036620\n",
      "Iteration 34, loss = 0.09151933\n",
      "Iteration 54, loss = 0.06414788\n",
      "Iteration 35, loss = 0.07111014\n",
      "Iteration 55, loss = 0.10629669\n",
      "Iteration 56, loss = 0.10174628\n",
      "Iteration 36, loss = 0.08100690\n",
      "Iteration 57, loss = 0.14270066\n",
      "Iteration 37, loss = 0.07726053\n",
      "Iteration 38, loss = 0.07185165\n",
      "Iteration 39, loss = 0.06396631\n",
      "Iteration 58, loss = 0.16680521\n",
      "Iteration 59, loss = 0.06597201\n",
      "Iteration 60, loss = 0.06976540\n",
      "Iteration 40, loss = 0.06962537\n",
      "Iteration 61, loss = 0.06743034\n",
      "Iteration 62, loss = 0.05421362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.07405331\n",
      "Iteration 42, loss = 0.07540973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50976668\n",
      "Iteration 1, loss = 0.95745199\n",
      "Iteration 2, loss = 0.55705502\n",
      "Iteration 3, loss = 0.44663976\n",
      "Iteration 2, loss = 0.47436632\n",
      "Iteration 4, loss = 0.37033183\n",
      "Iteration 5, loss = 0.32081041\n",
      "Iteration 6, loss = 0.31019142\n",
      "Iteration 3, loss = 0.40617324\n",
      "Iteration 4, loss = 0.34253521\n",
      "Iteration 7, loss = 0.28931338\n",
      "Iteration 5, loss = 0.31903119\n",
      "Iteration 6, loss = 0.28235412\n",
      "Iteration 8, loss = 0.25472013\n",
      "Iteration 7, loss = 0.25980977\n",
      "Iteration 9, loss = 0.20869717\n",
      "Iteration 10, loss = 0.20419822\n",
      "Iteration 8, loss = 0.23836565\n",
      "Iteration 11, loss = 0.19494434\n",
      "Iteration 9, loss = 0.21262352\n",
      "Iteration 10, loss = 0.19383098\n",
      "Iteration 12, loss = 0.17614283\n",
      "Iteration 13, loss = 0.16047991\n",
      "Iteration 11, loss = 0.20137518\n",
      "Iteration 14, loss = 0.15288568\n",
      "Iteration 15, loss = 0.14324191\n",
      "Iteration 12, loss = 0.19802842\n",
      "Iteration 16, loss = 0.14021222\n",
      "Iteration 13, loss = 0.20441966\n",
      "Iteration 17, loss = 0.15541559\n",
      "Iteration 14, loss = 0.20467477\n",
      "Iteration 18, loss = 0.14010309\n",
      "Iteration 15, loss = 0.17943416\n",
      "Iteration 19, loss = 0.13454569\n",
      "Iteration 16, loss = 0.15333308\n",
      "Iteration 20, loss = 0.13257107\n",
      "Iteration 17, loss = 0.15451869\n",
      "Iteration 21, loss = 0.13175058\n",
      "Iteration 18, loss = 0.14451536\n",
      "Iteration 22, loss = 0.12421629\n",
      "Iteration 19, loss = 0.14141687\n",
      "Iteration 23, loss = 0.11575226\n",
      "Iteration 20, loss = 0.11824506\n",
      "Iteration 24, loss = 0.13938515\n",
      "Iteration 21, loss = 0.12169640\n",
      "Iteration 22, loss = 0.11574334\n",
      "Iteration 25, loss = 0.14696981\n",
      "Iteration 23, loss = 0.10410486\n",
      "Iteration 26, loss = 0.12045782\n",
      "Iteration 24, loss = 0.09688674\n",
      "Iteration 27, loss = 0.10633437\n",
      "Iteration 25, loss = 0.10054537\n",
      "Iteration 28, loss = 0.10579015\n",
      "Iteration 29, loss = 0.09743272\n",
      "Iteration 26, loss = 0.10411582\n",
      "Iteration 30, loss = 0.09120981\n",
      "Iteration 27, loss = 0.08451961\n",
      "Iteration 28, loss = 0.08293133\n",
      "Iteration 31, loss = 0.08793505\n",
      "Iteration 29, loss = 0.10077850\n",
      "Iteration 30, loss = 0.13093055\n",
      "Iteration 32, loss = 0.08953801\n",
      "Iteration 31, loss = 0.15169496\n",
      "Iteration 33, loss = 0.09140636\n",
      "Iteration 32, loss = 0.16767651\n",
      "Iteration 34, loss = 0.10022157\n",
      "Iteration 33, loss = 0.13837114\n",
      "Iteration 35, loss = 0.10640651\n",
      "Iteration 34, loss = 0.13626893\n",
      "Iteration 36, loss = 0.11751645\n",
      "Iteration 35, loss = 0.10400632\n",
      "Iteration 36, loss = 0.10222444\n",
      "Iteration 37, loss = 0.10660968\n",
      "Iteration 37, loss = 0.09670697\n",
      "Iteration 38, loss = 0.09236193\n",
      "Iteration 39, loss = 0.11668358\n",
      "Iteration 38, loss = 0.14631223\n",
      "Iteration 40, loss = 0.09188422\n",
      "Iteration 41, loss = 0.12521926\n",
      "Iteration 42, loss = 0.14083722\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.10541297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93888753\n",
      "Iteration 2, loss = 0.52750862\n",
      "Iteration 3, loss = 0.44090020\n",
      "Iteration 4, loss = 0.37006970\n",
      "Iteration 5, loss = 0.32612048\n",
      "Iteration 6, loss = 0.29488309\n",
      "Iteration 1, loss = 1.16722914\n",
      "Iteration 7, loss = 0.26970999\n",
      "Iteration 2, loss = 0.52252146\n",
      "Iteration 3, loss = 0.39163823\n",
      "Iteration 8, loss = 0.24607138\n",
      "Iteration 4, loss = 0.34334515\n",
      "Iteration 9, loss = 0.23684053\n",
      "Iteration 10, loss = 0.20820048\n",
      "Iteration 5, loss = 0.31236746\n",
      "Iteration 11, loss = 0.20524793\n",
      "Iteration 6, loss = 0.27947563\n",
      "Iteration 7, loss = 0.26669938\n",
      "Iteration 12, loss = 0.19189506\n",
      "Iteration 8, loss = 0.23733946\n",
      "Iteration 13, loss = 0.19142040\n",
      "Iteration 14, loss = 0.18033096\n",
      "Iteration 15, loss = 0.16297346\n",
      "Iteration 9, loss = 0.21307731\n",
      "Iteration 10, loss = 0.20355663\n",
      "Iteration 11, loss = 0.18666902\n",
      "Iteration 16, loss = 0.16564193\n",
      "Iteration 12, loss = 0.19396028\n",
      "Iteration 17, loss = 0.15983607\n",
      "Iteration 13, loss = 0.18101689\n",
      "Iteration 18, loss = 0.16170370\n",
      "Iteration 19, loss = 0.14682850\n",
      "Iteration 14, loss = 0.15157884\n",
      "Iteration 15, loss = 0.15296972\n",
      "Iteration 20, loss = 0.13776862\n",
      "Iteration 21, loss = 0.13292588\n",
      "Iteration 16, loss = 0.15435625\n",
      "Iteration 22, loss = 0.10789999\n",
      "Iteration 17, loss = 0.14782928\n",
      "Iteration 23, loss = 0.11056124\n",
      "Iteration 18, loss = 0.13191312\n",
      "Iteration 24, loss = 0.11417426\n",
      "Iteration 19, loss = 0.12774601\n",
      "Iteration 20, loss = 0.13532294\n",
      "Iteration 25, loss = 0.10631585\n",
      "Iteration 26, loss = 0.10155450\n",
      "Iteration 27, loss = 0.13578964\n",
      "Iteration 21, loss = 0.19461803\n",
      "Iteration 28, loss = 0.10015040\n",
      "Iteration 29, loss = 0.12306858\n",
      "Iteration 30, loss = 0.11592691\n",
      "Iteration 22, loss = 0.13750258\n",
      "Iteration 31, loss = 0.11714641\n",
      "Iteration 23, loss = 0.13435453\n",
      "Iteration 32, loss = 0.10495829\n",
      "Iteration 24, loss = 0.10689236\n",
      "Iteration 33, loss = 0.09290956\n",
      "Iteration 25, loss = 0.12335321\n",
      "Iteration 26, loss = 0.10103611\n",
      "Iteration 34, loss = 0.10299358\n",
      "Iteration 27, loss = 0.11143420\n",
      "Iteration 28, loss = 0.10959453\n",
      "Iteration 35, loss = 0.09957086\n",
      "Iteration 29, loss = 0.09963165\n",
      "Iteration 36, loss = 0.10801495\n",
      "Iteration 30, loss = 0.09458370\n",
      "Iteration 37, loss = 0.12651738\n",
      "Iteration 38, loss = 0.15872859\n",
      "Iteration 39, loss = 0.11804845\n",
      "Iteration 31, loss = 0.08230394\n",
      "Iteration 40, loss = 0.13377328\n",
      "Iteration 32, loss = 0.09025921\n",
      "Iteration 41, loss = 0.15233052\n",
      "Iteration 33, loss = 0.07795052\n",
      "Iteration 42, loss = 0.15096032\n",
      "Iteration 34, loss = 0.07918299\n",
      "Iteration 43, loss = 0.12186019\n",
      "Iteration 44, loss = 0.09406961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.08713532\n",
      "Iteration 36, loss = 0.09945645\n",
      "Iteration 37, loss = 0.11716510\n",
      "Iteration 38, loss = 0.19899691\n",
      "Iteration 39, loss = 0.17609578\n",
      "Iteration 1, loss = 1.07680268\n",
      "Iteration 2, loss = 0.58565080\n",
      "Iteration 40, loss = 0.13307298\n",
      "Iteration 3, loss = 0.39197417\n",
      "Iteration 4, loss = 0.37493530\n",
      "Iteration 5, loss = 0.31708581\n",
      "Iteration 41, loss = 0.12200800\n",
      "Iteration 6, loss = 0.28843053\n",
      "Iteration 7, loss = 0.25406989\n",
      "Iteration 8, loss = 0.25433187\n",
      "Iteration 42, loss = 0.10326385\n",
      "Iteration 9, loss = 0.22812461\n",
      "Iteration 10, loss = 0.20650622\n",
      "Iteration 11, loss = 0.18823427\n",
      "Iteration 43, loss = 0.11019065\n",
      "Iteration 44, loss = 0.09810754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.17411143\n",
      "Iteration 13, loss = 0.16040630\n",
      "Iteration 14, loss = 0.15708764\n",
      "Iteration 15, loss = 0.14343266\n",
      "Iteration 16, loss = 0.12510966\n",
      "Iteration 17, loss = 0.13977921\n",
      "Iteration 18, loss = 0.12465679\n",
      "Iteration 19, loss = 0.12093610\n",
      "Iteration 1, loss = 0.95168783\n",
      "Iteration 20, loss = 0.11810979\n",
      "Iteration 21, loss = 0.10290525\n",
      "Iteration 2, loss = 0.46544632\n",
      "Iteration 22, loss = 0.11351883\n",
      "Iteration 23, loss = 0.11393758\n",
      "Iteration 3, loss = 0.40916379\n",
      "Iteration 4, loss = 0.35442957\n",
      "Iteration 24, loss = 0.10190907\n",
      "Iteration 5, loss = 0.32517718\n",
      "Iteration 25, loss = 0.08611104\n",
      "Iteration 26, loss = 0.09276879\n",
      "Iteration 6, loss = 0.27667704\n",
      "Iteration 27, loss = 0.09002593\n",
      "Iteration 7, loss = 0.26407195\n",
      "Iteration 28, loss = 0.06848040\n",
      "Iteration 8, loss = 0.22426211\n",
      "Iteration 9, loss = 0.21227478\n",
      "Iteration 29, loss = 0.07398415\n",
      "Iteration 30, loss = 0.07633146\n",
      "Iteration 10, loss = 0.20007339\n",
      "Iteration 31, loss = 0.08153487\n",
      "Iteration 32, loss = 0.10454319\n",
      "Iteration 11, loss = 0.19307317\n",
      "Iteration 12, loss = 0.17995480\n",
      "Iteration 13, loss = 0.17364610\n",
      "Iteration 33, loss = 0.18609314\n",
      "Iteration 14, loss = 0.14865218\n",
      "Iteration 34, loss = 0.11313731\n",
      "Iteration 15, loss = 0.14744705\n",
      "Iteration 35, loss = 0.08640963\n",
      "Iteration 16, loss = 0.14637060\n",
      "Iteration 36, loss = 0.07819843\n",
      "Iteration 17, loss = 0.13081840\n",
      "Iteration 18, loss = 0.14638604\n",
      "Iteration 19, loss = 0.13314953\n",
      "Iteration 37, loss = 0.08359822\n",
      "Iteration 20, loss = 0.16320332\n",
      "Iteration 38, loss = 0.09086732\n",
      "Iteration 21, loss = 0.16324427\n",
      "Iteration 39, loss = 0.07962790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 0.15530504\n",
      "Iteration 23, loss = 0.15190987\n",
      "Iteration 24, loss = 0.13001542\n",
      "Iteration 25, loss = 0.12519575\n",
      "Iteration 26, loss = 0.14974804\n",
      "Iteration 27, loss = 0.11530435\n",
      "Iteration 1, loss = 1.14611304\n",
      "Iteration 28, loss = 0.10094022\n",
      "Iteration 2, loss = 0.81771114\n",
      "Iteration 3, loss = 0.64407627\n",
      "Iteration 4, loss = 0.55237619\n",
      "Iteration 29, loss = 0.09201110\n",
      "Iteration 5, loss = 0.49084991\n",
      "Iteration 30, loss = 0.08545931\n",
      "Iteration 31, loss = 0.08328494\n",
      "Iteration 6, loss = 0.45259951\n",
      "Iteration 32, loss = 0.07471138\n",
      "Iteration 7, loss = 0.42153219\n",
      "Iteration 8, loss = 0.40122815\n",
      "Iteration 9, loss = 0.38307809\n",
      "Iteration 33, loss = 0.07780676\n",
      "Iteration 10, loss = 0.36873337\n",
      "Iteration 34, loss = 0.07642093\n",
      "Iteration 11, loss = 0.35734581\n",
      "Iteration 35, loss = 0.07737084\n",
      "Iteration 12, loss = 0.34553595\n",
      "Iteration 13, loss = 0.33321116\n",
      "Iteration 36, loss = 0.08364400\n",
      "Iteration 14, loss = 0.32292532\n",
      "Iteration 15, loss = 0.31651951\n",
      "Iteration 37, loss = 0.08571441\n",
      "Iteration 16, loss = 0.30631659\n",
      "Iteration 17, loss = 0.29983221\n",
      "Iteration 38, loss = 0.11255719\n",
      "Iteration 18, loss = 0.29025368\n",
      "Iteration 39, loss = 0.08156632\n",
      "Iteration 19, loss = 0.28279002\n",
      "Iteration 20, loss = 0.27757929\n",
      "Iteration 21, loss = 0.27090279\n",
      "Iteration 22, loss = 0.26358355\n",
      "Iteration 40, loss = 0.09031472\n",
      "Iteration 41, loss = 0.07561161\n",
      "Iteration 23, loss = 0.25616366\n",
      "Iteration 42, loss = 0.08102786\n",
      "Iteration 24, loss = 0.25293077\n",
      "Iteration 43, loss = 0.09639506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 0.24502421\n",
      "Iteration 26, loss = 0.23997426\n",
      "Iteration 27, loss = 0.23465515\n",
      "Iteration 28, loss = 0.23093387\n",
      "Iteration 29, loss = 0.22630399\n",
      "Iteration 30, loss = 0.21979347\n",
      "Iteration 31, loss = 0.21716131\n",
      "Iteration 32, loss = 0.20923837\n",
      "Iteration 33, loss = 0.20808461\n",
      "Iteration 34, loss = 0.20300016\n",
      "Iteration 35, loss = 0.19797817\n",
      "Iteration 36, loss = 0.19508473\n",
      "Iteration 37, loss = 0.19005219\n",
      "Iteration 38, loss = 0.18962605\n",
      "Iteration 39, loss = 0.18384104\n",
      "Iteration 1, loss = 1.06329663\n",
      "Iteration 40, loss = 0.18026335\n",
      "Iteration 2, loss = 0.72119095\n",
      "Iteration 3, loss = 0.55263770\n",
      "Iteration 41, loss = 0.17584758\n",
      "Iteration 4, loss = 0.48058519\n",
      "Iteration 42, loss = 0.17413919\n",
      "Iteration 5, loss = 0.43709722\n",
      "Iteration 43, loss = 0.16950911\n",
      "Iteration 6, loss = 0.40726950\n",
      "Iteration 44, loss = 0.16676158\n",
      "Iteration 7, loss = 0.38254529\n",
      "Iteration 45, loss = 0.16288543\n",
      "Iteration 8, loss = 0.36613405\n",
      "Iteration 9, loss = 0.34991564\n",
      "Iteration 46, loss = 0.16048173\n",
      "Iteration 10, loss = 0.33806735\n",
      "Iteration 47, loss = 0.15706834\n",
      "Iteration 11, loss = 0.32711883\n",
      "Iteration 48, loss = 0.15537963\n",
      "Iteration 12, loss = 0.31679441\n",
      "Iteration 49, loss = 0.15344692\n",
      "Iteration 13, loss = 0.30662949\n",
      "Iteration 14, loss = 0.29711702\n",
      "Iteration 50, loss = 0.15032226\n",
      "Iteration 51, loss = 0.14865440\n",
      "Iteration 15, loss = 0.29000284\n",
      "Iteration 52, loss = 0.14614537\n",
      "Iteration 53, loss = 0.14509949\n",
      "Iteration 16, loss = 0.28197605\n",
      "Iteration 17, loss = 0.27536272\n",
      "Iteration 54, loss = 0.13967590\n",
      "Iteration 18, loss = 0.26952294\n",
      "Iteration 55, loss = 0.13965976\n",
      "Iteration 19, loss = 0.26209580\n",
      "Iteration 56, loss = 0.13770600\n",
      "Iteration 20, loss = 0.25752493\n",
      "Iteration 21, loss = 0.25111359\n",
      "Iteration 57, loss = 0.13408967\n",
      "Iteration 58, loss = 0.13295141\n",
      "Iteration 22, loss = 0.24466647\n",
      "Iteration 59, loss = 0.12876408\n",
      "Iteration 23, loss = 0.23900341\n",
      "Iteration 24, loss = 0.23390450\n",
      "Iteration 60, loss = 0.12823116\n",
      "Iteration 25, loss = 0.22994593\n",
      "Iteration 61, loss = 0.12583207\n",
      "Iteration 26, loss = 0.22541629\n",
      "Iteration 62, loss = 0.13126182\n",
      "Iteration 63, loss = 0.12931595\n",
      "Iteration 27, loss = 0.22069743\n",
      "Iteration 64, loss = 0.12342387\n",
      "Iteration 28, loss = 0.21956275\n",
      "Iteration 65, loss = 0.12245201\n",
      "Iteration 66, loss = 0.12023717\n",
      "Iteration 67, loss = 0.11685523\n",
      "Iteration 68, loss = 0.11912936\n",
      "Iteration 29, loss = 0.21365251\n",
      "Iteration 69, loss = 0.11672765\n",
      "Iteration 70, loss = 0.11295146\n",
      "Iteration 71, loss = 0.11032430\n",
      "Iteration 30, loss = 0.20889767\n",
      "Iteration 72, loss = 0.10856935\n",
      "Iteration 73, loss = 0.10689102\n",
      "Iteration 74, loss = 0.10774237\n",
      "Iteration 31, loss = 0.20357983\n",
      "Iteration 75, loss = 0.10982541\n",
      "Iteration 32, loss = 0.19947198\n",
      "Iteration 76, loss = 0.10601306\n",
      "Iteration 33, loss = 0.19747637\n",
      "Iteration 34, loss = 0.19233872\n",
      "Iteration 77, loss = 0.10945865\n",
      "Iteration 78, loss = 0.10786812\n",
      "Iteration 79, loss = 0.10173132\n",
      "Iteration 35, loss = 0.19038709\n",
      "Iteration 80, loss = 0.09822965\n",
      "Iteration 81, loss = 0.09633429\n",
      "Iteration 82, loss = 0.09819671\n",
      "Iteration 36, loss = 0.18571663\n",
      "Iteration 83, loss = 0.09494695\n",
      "Iteration 84, loss = 0.09440448\n",
      "Iteration 37, loss = 0.18056096\n",
      "Iteration 38, loss = 0.17837812\n",
      "Iteration 85, loss = 0.09603826\n",
      "Iteration 39, loss = 0.17415617\n",
      "Iteration 40, loss = 0.17058700\n",
      "Iteration 86, loss = 0.09292325\n",
      "Iteration 41, loss = 0.16749495\n",
      "Iteration 42, loss = 0.16353759\n",
      "Iteration 87, loss = 0.09236080\n",
      "Iteration 43, loss = 0.16285283\n",
      "Iteration 88, loss = 0.08768428\n",
      "Iteration 89, loss = 0.08967172\n",
      "Iteration 90, loss = 0.08882086\n",
      "Iteration 44, loss = 0.15885949\n",
      "Iteration 91, loss = 0.08547267\n",
      "Iteration 92, loss = 0.08298581\n",
      "Iteration 45, loss = 0.15535742\n",
      "Iteration 93, loss = 0.08187709\n",
      "Iteration 46, loss = 0.15527316\n",
      "Iteration 47, loss = 0.15172055\n",
      "Iteration 94, loss = 0.08387845\n",
      "Iteration 48, loss = 0.14780597\n",
      "Iteration 95, loss = 0.08583907\n",
      "Iteration 96, loss = 0.08198745\n",
      "Iteration 97, loss = 0.08394308\n",
      "Iteration 49, loss = 0.14559257\n",
      "Iteration 98, loss = 0.08037505\n",
      "Iteration 99, loss = 0.07652546\n",
      "Iteration 50, loss = 0.14608224\n",
      "Iteration 100, loss = 0.08000075\n",
      "Iteration 51, loss = 0.14252218\n",
      "Iteration 52, loss = 0.14150283\n",
      "Iteration 53, loss = 0.13652400\n",
      "Iteration 54, loss = 0.13581164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 0.13145081\n",
      "Iteration 56, loss = 0.13158696\n",
      "Iteration 57, loss = 0.12707665\n",
      "Iteration 58, loss = 0.12555636\n",
      "Iteration 59, loss = 0.12511777\n",
      "Iteration 60, loss = 0.12377275\n",
      "Iteration 1, loss = 1.11259898\n",
      "Iteration 61, loss = 0.11977824\n",
      "Iteration 62, loss = 0.11991317\n",
      "Iteration 2, loss = 0.76936993\n",
      "Iteration 3, loss = 0.60130797\n",
      "Iteration 4, loss = 0.51674615\n",
      "Iteration 5, loss = 0.47146290\n",
      "Iteration 63, loss = 0.11975060\n",
      "Iteration 64, loss = 0.12254637\n",
      "Iteration 65, loss = 0.11625109\n",
      "Iteration 6, loss = 0.43585916\n",
      "Iteration 66, loss = 0.11188290\n",
      "Iteration 7, loss = 0.41204585\n",
      "Iteration 8, loss = 0.38792645\n",
      "Iteration 67, loss = 0.10989381\n",
      "Iteration 9, loss = 0.37195622\n",
      "Iteration 68, loss = 0.10928493\n",
      "Iteration 69, loss = 0.10558972\n",
      "Iteration 10, loss = 0.35649412\n",
      "Iteration 70, loss = 0.10424538\n",
      "Iteration 11, loss = 0.34402223\n",
      "Iteration 12, loss = 0.33272386\n",
      "Iteration 71, loss = 0.10214772\n",
      "Iteration 13, loss = 0.32282508\n",
      "Iteration 72, loss = 0.10114560\n",
      "Iteration 14, loss = 0.31084421\n",
      "Iteration 73, loss = 0.10179139\n",
      "Iteration 15, loss = 0.30428266\n",
      "Iteration 74, loss = 0.10000764\n",
      "Iteration 16, loss = 0.29409119\n",
      "Iteration 75, loss = 0.09678752\n",
      "Iteration 17, loss = 0.28747226\n",
      "Iteration 76, loss = 0.09454750\n",
      "Iteration 18, loss = 0.27775845\n",
      "Iteration 77, loss = 0.09269508\n",
      "Iteration 19, loss = 0.27479799\n",
      "Iteration 20, loss = 0.26374898\n",
      "Iteration 21, loss = 0.25999205\n",
      "Iteration 22, loss = 0.25268819\n",
      "Iteration 78, loss = 0.09306973\n",
      "Iteration 23, loss = 0.24605860\n",
      "Iteration 79, loss = 0.09059868\n",
      "Iteration 24, loss = 0.24108919\n",
      "Iteration 25, loss = 0.23781551\n",
      "Iteration 80, loss = 0.08935756\n",
      "Iteration 26, loss = 0.22824412\n",
      "Iteration 81, loss = 0.08950352\n",
      "Iteration 82, loss = 0.08998175\n",
      "Iteration 27, loss = 0.22661463\n",
      "Iteration 83, loss = 0.09166776\n",
      "Iteration 84, loss = 0.08853867\n",
      "Iteration 28, loss = 0.21903279\n",
      "Iteration 85, loss = 0.08480571\n",
      "Iteration 86, loss = 0.08132333\n",
      "Iteration 87, loss = 0.07879035\n",
      "Iteration 29, loss = 0.21478636\n",
      "Iteration 88, loss = 0.08003197\n",
      "Iteration 89, loss = 0.07851476\n",
      "Iteration 30, loss = 0.21031008\n",
      "Iteration 31, loss = 0.20617361\n",
      "Iteration 90, loss = 0.07759691\n",
      "Iteration 32, loss = 0.20229038\n",
      "Iteration 33, loss = 0.20182707\n",
      "Iteration 34, loss = 0.19633228\n",
      "Iteration 91, loss = 0.07399252\n",
      "Iteration 35, loss = 0.19148160\n",
      "Iteration 92, loss = 0.07314002\n",
      "Iteration 36, loss = 0.18887442\n",
      "Iteration 93, loss = 0.07333585\n",
      "Iteration 94, loss = 0.07188296\n",
      "Iteration 37, loss = 0.18581593\n",
      "Iteration 38, loss = 0.18108657\n",
      "Iteration 95, loss = 0.06905771\n",
      "Iteration 39, loss = 0.17931812\n",
      "Iteration 96, loss = 0.06878629\n",
      "Iteration 40, loss = 0.17652417\n",
      "Iteration 97, loss = 0.07195580\n",
      "Iteration 98, loss = 0.06878036\n",
      "Iteration 41, loss = 0.17179754\n",
      "Iteration 42, loss = 0.17237517\n",
      "Iteration 99, loss = 0.06904031\n",
      "Iteration 43, loss = 0.16623207\n",
      "Iteration 100, loss = 0.07239699\n",
      "Iteration 44, loss = 0.16369290\n",
      "Iteration 45, loss = 0.16131045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.15863668\n",
      "Iteration 47, loss = 0.15561099\n",
      "Iteration 48, loss = 0.15429871\n",
      "Iteration 49, loss = 0.15216699\n",
      "Iteration 50, loss = 0.15512390\n",
      "Iteration 51, loss = 0.15227112\n",
      "Iteration 52, loss = 0.14757394\n",
      "Iteration 53, loss = 0.14392366\n",
      "Iteration 54, loss = 0.14518226\n",
      "Iteration 55, loss = 0.14216669\n",
      "Iteration 56, loss = 0.13753612\n",
      "Iteration 57, loss = 0.13340329\n",
      "Iteration 58, loss = 0.13306451\n",
      "Iteration 59, loss = 0.13300063\n",
      "Iteration 60, loss = 0.13168607\n",
      "Iteration 61, loss = 0.12862600\n",
      "Iteration 62, loss = 0.13096414\n",
      "Iteration 63, loss = 0.12339017\n",
      "Iteration 64, loss = 0.12291894\n",
      "Iteration 65, loss = 0.12035185\n",
      "Iteration 66, loss = 0.11899148\n",
      "Iteration 67, loss = 0.11902440\n",
      "Iteration 68, loss = 0.11642148\n",
      "Iteration 1, loss = 1.08295903\n",
      "Iteration 69, loss = 0.11616002\n",
      "Iteration 70, loss = 0.11879811\n",
      "Iteration 2, loss = 0.75705496\n",
      "Iteration 71, loss = 0.11067900\n",
      "Iteration 72, loss = 0.11172635\n",
      "Iteration 3, loss = 0.60846850\n",
      "Iteration 73, loss = 0.10922712\n",
      "Iteration 4, loss = 0.53027154\n",
      "Iteration 74, loss = 0.10928863\n",
      "Iteration 5, loss = 0.48444329\n",
      "Iteration 6, loss = 0.44897232\n",
      "Iteration 75, loss = 0.10679096\n",
      "Iteration 7, loss = 0.42340966\n",
      "Iteration 76, loss = 0.10736845\n",
      "Iteration 77, loss = 0.10267352\n",
      "Iteration 78, loss = 0.10295921\n",
      "Iteration 79, loss = 0.10114133\n",
      "Iteration 8, loss = 0.40257250\n",
      "Iteration 80, loss = 0.10207038\n",
      "Iteration 81, loss = 0.10560959\n",
      "Iteration 9, loss = 0.38809284\n",
      "Iteration 82, loss = 0.10462846\n",
      "Iteration 83, loss = 0.09965183\n",
      "Iteration 84, loss = 0.09440449\n",
      "Iteration 85, loss = 0.10262989\n",
      "Iteration 10, loss = 0.37153062\n",
      "Iteration 86, loss = 0.10000965\n",
      "Iteration 11, loss = 0.35946548\n",
      "Iteration 87, loss = 0.09102148\n",
      "Iteration 88, loss = 0.09172147\n",
      "Iteration 12, loss = 0.34808099\n",
      "Iteration 89, loss = 0.09307399\n",
      "Iteration 90, loss = 0.08942880\n",
      "Iteration 13, loss = 0.33643356\n",
      "Iteration 91, loss = 0.08931658\n",
      "Iteration 14, loss = 0.32875927Iteration 92, loss = 0.08689828\n",
      "\n",
      "Iteration 93, loss = 0.08497386\n",
      "Iteration 94, loss = 0.08544869\n",
      "Iteration 95, loss = 0.08402298\n",
      "Iteration 96, loss = 0.08356915\n",
      "Iteration 15, loss = 0.31803815\n",
      "Iteration 97, loss = 0.08183363\n",
      "Iteration 98, loss = 0.08033655\n",
      "Iteration 99, loss = 0.08339201\n",
      "Iteration 16, loss = 0.30972997\n",
      "Iteration 100, loss = 0.08093605\n",
      "Iteration 17, loss = 0.30058498\n",
      "Iteration 18, loss = 0.29353865\n",
      "Iteration 19, loss = 0.28504758\n",
      "Iteration 20, loss = 0.27769503\n",
      "Iteration 21, loss = 0.27138028\n",
      "Iteration 22, loss = 0.26548112\n",
      "Iteration 23, loss = 0.25875238\n",
      "Iteration 24, loss = 0.25887191\n",
      "Iteration 25, loss = 0.25069018\n",
      "Iteration 26, loss = 0.24177116\n",
      "Iteration 27, loss = 0.23736086\n",
      "Iteration 28, loss = 0.23009996\n",
      "Iteration 29, loss = 0.22509276\n",
      "Iteration 30, loss = 0.21971393\n",
      "Iteration 31, loss = 0.21689414\n",
      "Iteration 32, loss = 0.21175130\n",
      "Iteration 33, loss = 0.20616694\n",
      "Iteration 34, loss = 0.20488127\n",
      "Iteration 1, loss = 1.27989245\n",
      "Iteration 35, loss = 0.19974070\n",
      "Iteration 2, loss = 0.79993685\n",
      "Iteration 36, loss = 0.19993875\n",
      "Iteration 3, loss = 0.60276477\n",
      "Iteration 4, loss = 0.51190839\n",
      "Iteration 5, loss = 0.46142859\n",
      "Iteration 37, loss = 0.19107034\n",
      "Iteration 38, loss = 0.18606410\n",
      "Iteration 6, loss = 0.42743649\n",
      "Iteration 39, loss = 0.18739231\n",
      "Iteration 7, loss = 0.40274784\n",
      "Iteration 40, loss = 0.17965158\n",
      "Iteration 8, loss = 0.37914300\n",
      "Iteration 9, loss = 0.36117853\n",
      "Iteration 41, loss = 0.17711941\n",
      "Iteration 10, loss = 0.34865789\n",
      "Iteration 42, loss = 0.17330926\n",
      "Iteration 11, loss = 0.33550199\n",
      "Iteration 43, loss = 0.16891890\n",
      "Iteration 12, loss = 0.32507949\n",
      "Iteration 44, loss = 0.16736682\n",
      "Iteration 45, loss = 0.16241986\n",
      "Iteration 13, loss = 0.31346114\n",
      "Iteration 46, loss = 0.16017475\n",
      "Iteration 47, loss = 0.15784554\n",
      "Iteration 48, loss = 0.15441118\n",
      "Iteration 14, loss = 0.30670301\n",
      "Iteration 49, loss = 0.15439367\n",
      "Iteration 50, loss = 0.15093340\n",
      "Iteration 51, loss = 0.14777511\n",
      "Iteration 52, loss = 0.14582525\n",
      "Iteration 15, loss = 0.29442704\n",
      "Iteration 53, loss = 0.14129167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, loss = 0.14100889\n",
      "Iteration 55, loss = 0.13697958\n",
      "Iteration 16, loss = 0.28671791\n",
      "Iteration 56, loss = 0.13588140\n",
      "Iteration 57, loss = 0.13345490\n",
      "Iteration 58, loss = 0.13160385\n",
      "Iteration 59, loss = 0.13057191\n",
      "Iteration 60, loss = 0.12649894\n",
      "Iteration 17, loss = 0.27856158\n",
      "Iteration 61, loss = 0.12579504\n",
      "Iteration 18, loss = 0.27010213\n",
      "Iteration 62, loss = 0.12394387\n",
      "Iteration 19, loss = 0.26207452\n",
      "Iteration 63, loss = 0.12194651\n",
      "Iteration 20, loss = 0.25597198\n",
      "Iteration 64, loss = 0.11960032\n",
      "Iteration 65, loss = 0.11763181\n",
      "Iteration 66, loss = 0.11690993\n",
      "Iteration 67, loss = 0.11412975\n",
      "Iteration 21, loss = 0.24836608\n",
      "Iteration 68, loss = 0.11293139\n",
      "Iteration 22, loss = 0.24221137\n",
      "Iteration 69, loss = 0.11389678\n",
      "Iteration 70, loss = 0.11306601\n",
      "Iteration 23, loss = 0.23648897\n",
      "Iteration 71, loss = 0.10887008\n",
      "Iteration 72, loss = 0.10720178\n",
      "Iteration 73, loss = 0.10544027\n",
      "Iteration 24, loss = 0.22777638\n",
      "Iteration 74, loss = 0.10616184\n",
      "Iteration 25, loss = 0.22391272\n",
      "Iteration 26, loss = 0.21870233\n",
      "Iteration 27, loss = 0.21233582\n",
      "Iteration 75, loss = 0.10662028\n",
      "Iteration 28, loss = 0.20827013\n",
      "Iteration 76, loss = 0.10115404\n",
      "Iteration 29, loss = 0.20433190\n",
      "Iteration 77, loss = 0.09837441\n",
      "Iteration 30, loss = 0.19671782\n",
      "Iteration 78, loss = 0.09719136\n",
      "Iteration 31, loss = 0.19143519\n",
      "Iteration 79, loss = 0.09770399\n",
      "Iteration 32, loss = 0.18667700\n",
      "Iteration 80, loss = 0.09606791\n",
      "Iteration 81, loss = 0.09714962\n",
      "Iteration 33, loss = 0.18275502\n",
      "Iteration 82, loss = 0.09519490\n",
      "Iteration 83, loss = 0.09922761\n",
      "Iteration 34, loss = 0.17917731\n",
      "Iteration 84, loss = 0.09472468\n",
      "Iteration 85, loss = 0.08926920\n",
      "Iteration 86, loss = 0.08552766\n",
      "Iteration 35, loss = 0.17379111\n",
      "Iteration 36, loss = 0.17030734\n",
      "Iteration 87, loss = 0.08733294\n",
      "Iteration 37, loss = 0.16733704\n",
      "Iteration 88, loss = 0.08999839\n",
      "Iteration 38, loss = 0.16212935\n",
      "Iteration 89, loss = 0.08946623\n",
      "Iteration 90, loss = 0.09172103\n",
      "Iteration 91, loss = 0.08481631\n",
      "Iteration 39, loss = 0.16151294\n",
      "Iteration 92, loss = 0.08126083\n",
      "Iteration 40, loss = 0.15614325\n",
      "Iteration 41, loss = 0.15086278\n",
      "Iteration 42, loss = 0.15133280\n",
      "Iteration 93, loss = 0.07946777\n",
      "Iteration 94, loss = 0.07789480\n",
      "Iteration 43, loss = 0.14608979\n",
      "Iteration 95, loss = 0.08085635\n",
      "Iteration 96, loss = 0.07998841\n",
      "Iteration 44, loss = 0.14268556\n",
      "Iteration 45, loss = 0.14055279\n",
      "Iteration 46, loss = 0.13929434\n",
      "Iteration 97, loss = 0.07985464\n",
      "Iteration 47, loss = 0.13354713\n",
      "Iteration 98, loss = 0.07998417\n",
      "Iteration 48, loss = 0.13229624\n",
      "Iteration 99, loss = 0.07654832\n",
      "Iteration 49, loss = 0.12901599\n",
      "Iteration 100, loss = 0.07971683\n",
      "Iteration 50, loss = 0.12706701\n",
      "Iteration 51, loss = 0.12802647\n",
      "Iteration 52, loss = 0.12275124\n",
      "Iteration 53, loss = 0.11864472\n",
      "Iteration 54, loss = 0.11622444\n",
      "Iteration 55, loss = 0.11663618\n",
      "Iteration 56, loss = 0.11365750\n",
      "Iteration 57, loss = 0.11374863\n",
      "Iteration 58, loss = 0.11052942\n",
      "Iteration 59, loss = 0.10612130\n",
      "Iteration 1, loss = 1.06346570\n",
      "Iteration 2, loss = 0.76818132\n",
      "Iteration 60, loss = 0.10546053\n",
      "Iteration 3, loss = 0.61612093\n",
      "Iteration 61, loss = 0.10641384\n",
      "Iteration 4, loss = 0.54123027\n",
      "Iteration 62, loss = 0.10642108\n",
      "Iteration 5, loss = 0.49066257\n",
      "Iteration 63, loss = 0.10128544\n",
      "Iteration 6, loss = 0.45357105\n",
      "Iteration 64, loss = 0.09863644\n",
      "Iteration 7, loss = 0.42972837\n",
      "Iteration 65, loss = 0.09730086\n",
      "Iteration 66, loss = 0.09908162\n",
      "Iteration 67, loss = 0.09445533\n",
      "Iteration 68, loss = 0.09649948\n",
      "Iteration 8, loss = 0.40803013\n",
      "Iteration 9, loss = 0.39008668\n",
      "Iteration 69, loss = 0.09134716\n",
      "Iteration 70, loss = 0.08936810\n",
      "Iteration 71, loss = 0.08754067\n",
      "Iteration 10, loss = 0.37479798\n",
      "Iteration 72, loss = 0.08517164\n",
      "Iteration 73, loss = 0.08445272\n",
      "Iteration 11, loss = 0.36116208\n",
      "Iteration 74, loss = 0.08557860\n",
      "Iteration 75, loss = 0.08157750\n",
      "Iteration 12, loss = 0.35043723\n",
      "Iteration 13, loss = 0.33862753\n",
      "Iteration 76, loss = 0.08059370\n",
      "Iteration 77, loss = 0.07964743\n",
      "Iteration 14, loss = 0.32824220\n",
      "Iteration 78, loss = 0.07741390\n",
      "Iteration 79, loss = 0.07741462\n",
      "Iteration 80, loss = 0.07804278\n",
      "Iteration 15, loss = 0.31901207\n",
      "Iteration 81, loss = 0.07681293\n",
      "Iteration 16, loss = 0.31102097\n",
      "Iteration 82, loss = 0.07541621\n",
      "Iteration 17, loss = 0.30308257\n",
      "Iteration 83, loss = 0.07385424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 84, loss = 0.07424601\n",
      "Iteration 18, loss = 0.29421573\n",
      "Iteration 85, loss = 0.07225368\n",
      "Iteration 19, loss = 0.28667473\n",
      "Iteration 20, loss = 0.27915707\n",
      "Iteration 86, loss = 0.07237032\n",
      "Iteration 21, loss = 0.27380815\n",
      "Iteration 87, loss = 0.06867236\n",
      "Iteration 22, loss = 0.26647929\n",
      "Iteration 88, loss = 0.07057026\n",
      "Iteration 23, loss = 0.25928789\n",
      "Iteration 89, loss = 0.06768443\n",
      "Iteration 24, loss = 0.25436058\n",
      "Iteration 25, loss = 0.24709074\n",
      "Iteration 90, loss = 0.06518322\n",
      "Iteration 91, loss = 0.06439350\n",
      "Iteration 92, loss = 0.06568095\n",
      "Iteration 26, loss = 0.24216643\n",
      "Iteration 93, loss = 0.06417385\n",
      "Iteration 27, loss = 0.23643876\n",
      "Iteration 28, loss = 0.23211595\n",
      "Iteration 94, loss = 0.06509552\n",
      "Iteration 29, loss = 0.22520670\n",
      "Iteration 30, loss = 0.21956798\n",
      "Iteration 95, loss = 0.06267242\n",
      "Iteration 96, loss = 0.06251837\n",
      "Iteration 31, loss = 0.21528714\n",
      "Iteration 32, loss = 0.21075269\n",
      "Iteration 97, loss = 0.06066266\n",
      "Iteration 33, loss = 0.20709378\n",
      "Iteration 34, loss = 0.20365391\n",
      "Iteration 98, loss = 0.05907624\n",
      "Iteration 35, loss = 0.20044858\n",
      "Iteration 99, loss = 0.06001584\n",
      "Iteration 36, loss = 0.19593422\n",
      "Iteration 37, loss = 0.19121145\n",
      "Iteration 38, loss = 0.18660309\n",
      "Iteration 100, loss = 0.06145959\n",
      "Iteration 39, loss = 0.18158320\n",
      "Iteration 40, loss = 0.17864934\n",
      "Iteration 41, loss = 0.17695072\n",
      "Iteration 42, loss = 0.17091757\n",
      "Iteration 43, loss = 0.16941924\n",
      "Iteration 44, loss = 0.16641796\n",
      "Iteration 45, loss = 0.16299054\n",
      "Iteration 1, loss = 0.99554990\n",
      "Iteration 46, loss = 0.15845767\n",
      "Iteration 2, loss = 0.70853649\n",
      "Iteration 47, loss = 0.16066174\n",
      "Iteration 3, loss = 0.56728983\n",
      "Iteration 48, loss = 0.15558860\n",
      "Iteration 4, loss = 0.49552447\n",
      "Iteration 49, loss = 0.15353959\n",
      "Iteration 5, loss = 0.44686017\n",
      "Iteration 50, loss = 0.14927957\n",
      "Iteration 6, loss = 0.41598926\n",
      "Iteration 51, loss = 0.14835879\n",
      "Iteration 52, loss = 0.14806974\n",
      "Iteration 53, loss = 0.14063377\n",
      "Iteration 7, loss = 0.38824476\n",
      "Iteration 54, loss = 0.13925361\n",
      "Iteration 8, loss = 0.37026145\n",
      "Iteration 55, loss = 0.13694542\n",
      "Iteration 9, loss = 0.35227647\n",
      "Iteration 56, loss = 0.13573354\n",
      "Iteration 10, loss = 0.34088774\n",
      "Iteration 57, loss = 0.13535401\n",
      "Iteration 11, loss = 0.32685980\n",
      "Iteration 12, loss = 0.31429498\n",
      "Iteration 13, loss = 0.30274411\n",
      "Iteration 58, loss = 0.13793518\n",
      "Iteration 59, loss = 0.13286699\n",
      "Iteration 60, loss = 0.12976058\n",
      "Iteration 14, loss = 0.29338323\n",
      "Iteration 61, loss = 0.12533858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.28368798\n",
      "Iteration 62, loss = 0.12357944\n",
      "Iteration 16, loss = 0.27630761\n",
      "Iteration 17, loss = 0.26734607\n",
      "Iteration 63, loss = 0.12397447\n",
      "Iteration 64, loss = 0.11889975\n",
      "Iteration 18, loss = 0.25923147\n",
      "Iteration 65, loss = 0.11767484\n",
      "Iteration 66, loss = 0.11551418\n",
      "Iteration 19, loss = 0.25430445\n",
      "Iteration 20, loss = 0.24889077\n",
      "Iteration 67, loss = 0.11369346\n",
      "Iteration 21, loss = 0.24147106\n",
      "Iteration 68, loss = 0.11108127\n",
      "Iteration 22, loss = 0.23511016\n",
      "Iteration 69, loss = 0.11039484\n",
      "Iteration 70, loss = 0.10907818\n",
      "Iteration 23, loss = 0.22909852\n",
      "Iteration 71, loss = 0.10857508\n",
      "Iteration 24, loss = 0.22315890\n",
      "Iteration 25, loss = 0.21892801\n",
      "Iteration 26, loss = 0.21399869\n",
      "Iteration 72, loss = 0.10462739\n",
      "Iteration 73, loss = 0.10522203\n",
      "Iteration 74, loss = 0.10736818\n",
      "Iteration 27, loss = 0.20942326\n",
      "Iteration 28, loss = 0.20582609\n",
      "Iteration 29, loss = 0.19987651\n",
      "Iteration 30, loss = 0.20004670\n",
      "Iteration 75, loss = 0.10355182\n",
      "Iteration 76, loss = 0.10516874\n",
      "Iteration 31, loss = 0.19164951\n",
      "Iteration 77, loss = 0.10273812\n",
      "Iteration 78, loss = 0.10338270\n",
      "Iteration 32, loss = 0.19114948\n",
      "Iteration 33, loss = 0.18474494\n",
      "Iteration 34, loss = 0.18125010\n",
      "Iteration 79, loss = 0.09639616\n",
      "Iteration 35, loss = 0.17758223\n",
      "Iteration 80, loss = 0.09548568\n",
      "Iteration 81, loss = 0.09185202\n",
      "Iteration 82, loss = 0.09007523\n",
      "Iteration 36, loss = 0.17532612\n",
      "Iteration 83, loss = 0.08956250\n",
      "Iteration 84, loss = 0.08929988\n",
      "Iteration 37, loss = 0.17055044\n",
      "Iteration 85, loss = 0.08695381\n",
      "Iteration 86, loss = 0.08544958\n",
      "Iteration 38, loss = 0.16724058\n",
      "Iteration 87, loss = 0.08456082\n",
      "Iteration 88, loss = 0.08229559\n",
      "Iteration 39, loss = 0.16616624\n",
      "Iteration 40, loss = 0.16017125\n",
      "Iteration 41, loss = 0.16060692\n",
      "Iteration 89, loss = 0.08321578\n",
      "Iteration 90, loss = 0.08197762\n",
      "Iteration 42, loss = 0.15572670\n",
      "Iteration 91, loss = 0.08276113\n",
      "Iteration 43, loss = 0.15280404\n",
      "Iteration 92, loss = 0.08226808\n",
      "Iteration 44, loss = 0.14906847\n",
      "Iteration 45, loss = 0.14650693\n",
      "Iteration 46, loss = 0.14591202\n",
      "Iteration 93, loss = 0.07899537\n",
      "Iteration 47, loss = 0.14129841\n",
      "Iteration 94, loss = 0.08158446\n",
      "Iteration 48, loss = 0.14041314\n",
      "Iteration 95, loss = 0.07631547\n",
      "Iteration 49, loss = 0.14087516\n",
      "Iteration 96, loss = 0.07747015\n",
      "Iteration 50, loss = 0.13568719\n",
      "Iteration 51, loss = 0.13131340\n",
      "Iteration 97, loss = 0.07685414\n",
      "Iteration 52, loss = 0.13042371\n",
      "Iteration 53, loss = 0.13154969\n",
      "Iteration 98, loss = 0.07331566\n",
      "Iteration 99, loss = 0.07404331\n",
      "Iteration 100, loss = 0.07136981\n",
      "Iteration 54, loss = 0.12784709\n",
      "Iteration 55, loss = 0.12464522\n",
      "Iteration 56, loss = 0.12390156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57, loss = 0.12189455\n",
      "Iteration 58, loss = 0.11884948\n",
      "Iteration 59, loss = 0.11715443\n",
      "Iteration 1, loss = 0.99397754\n",
      "Iteration 60, loss = 0.11283321\n",
      "Iteration 2, loss = 0.70657626\n",
      "Iteration 61, loss = 0.11530417\n",
      "Iteration 3, loss = 0.57842208\n",
      "Iteration 4, loss = 0.50399013\n",
      "Iteration 62, loss = 0.11674014\n",
      "Iteration 63, loss = 0.11322977\n",
      "Iteration 5, loss = 0.46104380\n",
      "Iteration 64, loss = 0.11232029\n",
      "Iteration 6, loss = 0.43031935\n",
      "Iteration 65, loss = 0.10550111\n",
      "Iteration 7, loss = 0.40048578\n",
      "Iteration 66, loss = 0.11200600\n",
      "Iteration 8, loss = 0.38346213\n",
      "Iteration 67, loss = 0.10666204\n",
      "Iteration 9, loss = 0.36721214\n",
      "Iteration 68, loss = 0.10357279\n",
      "Iteration 10, loss = 0.35243987\n",
      "Iteration 69, loss = 0.10341110\n",
      "Iteration 11, loss = 0.33949527\n",
      "Iteration 70, loss = 0.09918597\n",
      "Iteration 12, loss = 0.32931599\n",
      "Iteration 71, loss = 0.09584792\n",
      "Iteration 13, loss = 0.31828498\n",
      "Iteration 72, loss = 0.09851957\n",
      "Iteration 14, loss = 0.30958468\n",
      "Iteration 73, loss = 0.09407658\n",
      "Iteration 15, loss = 0.30020571\n",
      "Iteration 74, loss = 0.09220701\n",
      "Iteration 16, loss = 0.29125457\n",
      "Iteration 17, loss = 0.28186444\n",
      "Iteration 75, loss = 0.09221972\n",
      "Iteration 18, loss = 0.27542168\n",
      "Iteration 76, loss = 0.09015029\n",
      "Iteration 77, loss = 0.08890864\n",
      "Iteration 19, loss = 0.26541012\n",
      "Iteration 78, loss = 0.08818890\n",
      "Iteration 20, loss = 0.25985811\n",
      "Iteration 21, loss = 0.25274251\n",
      "Iteration 79, loss = 0.09088768\n",
      "Iteration 80, loss = 0.09181457\n",
      "Iteration 22, loss = 0.24592113\n",
      "Iteration 81, loss = 0.08829436\n",
      "Iteration 23, loss = 0.23932341\n",
      "Iteration 24, loss = 0.23371947\n",
      "Iteration 25, loss = 0.23279339\n",
      "Iteration 82, loss = 0.08874779\n",
      "Iteration 26, loss = 0.22246887\n",
      "Iteration 83, loss = 0.08262916\n",
      "Iteration 84, loss = 0.08444028\n",
      "Iteration 27, loss = 0.22083601\n",
      "Iteration 85, loss = 0.08381798\n",
      "Iteration 28, loss = 0.21421529\n",
      "Iteration 86, loss = 0.07938049\n",
      "Iteration 87, loss = 0.07770788\n",
      "Iteration 29, loss = 0.20687572\n",
      "Iteration 88, loss = 0.07972002\n",
      "Iteration 30, loss = 0.20526082\n",
      "Iteration 89, loss = 0.07972437\n",
      "Iteration 31, loss = 0.19909551\n",
      "Iteration 90, loss = 0.07748474\n",
      "Iteration 32, loss = 0.19408789\n",
      "Iteration 91, loss = 0.07668844\n",
      "Iteration 33, loss = 0.18992952\n",
      "Iteration 92, loss = 0.07102331\n",
      "Iteration 34, loss = 0.18728970\n",
      "Iteration 93, loss = 0.07270977\n",
      "Iteration 35, loss = 0.18267195\n",
      "Iteration 94, loss = 0.07139297\n",
      "Iteration 36, loss = 0.17856241\n",
      "Iteration 95, loss = 0.07090626\n",
      "Iteration 37, loss = 0.17425526\n",
      "Iteration 38, loss = 0.17142226\n",
      "Iteration 96, loss = 0.07064380\n",
      "Iteration 39, loss = 0.17061343\n",
      "Iteration 40, loss = 0.16626740\n",
      "Iteration 97, loss = 0.06754792\n",
      "Iteration 41, loss = 0.15924441\n",
      "Iteration 42, loss = 0.15975682\n",
      "Iteration 98, loss = 0.06633706\n",
      "Iteration 43, loss = 0.15289128\n",
      "Iteration 99, loss = 0.06649231\n",
      "Iteration 100, loss = 0.06626704\n",
      "Iteration 44, loss = 0.15166390\n",
      "Iteration 45, loss = 0.14687475\n",
      "Iteration 46, loss = 0.14612653\n",
      "Iteration 47, loss = 0.14350826\n",
      "Iteration 48, loss = 0.13898073\n",
      "Iteration 49, loss = 0.13644976\n",
      "Iteration 50, loss = 0.13346271\n",
      "Iteration 1, loss = 1.04227445\n",
      "Iteration 51, loss = 0.13063411\n",
      "Iteration 2, loss = 0.75795507\n",
      "Iteration 52, loss = 0.12924175\n",
      "Iteration 3, loss = 0.59377730\n",
      "Iteration 53, loss = 0.12716046\n",
      "Iteration 4, loss = 0.52199605\n",
      "Iteration 54, loss = 0.12478964\n",
      "Iteration 5, loss = 0.47243228\n",
      "Iteration 55, loss = 0.12386553\n",
      "Iteration 6, loss = 0.44268331\n",
      "Iteration 56, loss = 0.12345175\n",
      "Iteration 7, loss = 0.41846605\n",
      "Iteration 57, loss = 0.12476458\n",
      "Iteration 8, loss = 0.39922074\n",
      "Iteration 58, loss = 0.11980889\n",
      "Iteration 9, loss = 0.38515927\n",
      "Iteration 10, loss = 0.37129795\n",
      "Iteration 59, loss = 0.11282020\n",
      "Iteration 60, loss = 0.11351899\n",
      "Iteration 11, loss = 0.35717529\n",
      "Iteration 61, loss = 0.11215773\n",
      "Iteration 62, loss = 0.11256925\n",
      "Iteration 12, loss = 0.34607590\n",
      "Iteration 63, loss = 0.11048863\n",
      "Iteration 64, loss = 0.10475748\n",
      "Iteration 65, loss = 0.10430216\n",
      "Iteration 13, loss = 0.33513954\n",
      "Iteration 14, loss = 0.32624702\n",
      "Iteration 66, loss = 0.10488607\n",
      "Iteration 67, loss = 0.10512888\n",
      "Iteration 15, loss = 0.31726533\n",
      "Iteration 68, loss = 0.10374408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, loss = 0.31030369\n",
      "Iteration 69, loss = 0.10117482\n",
      "Iteration 17, loss = 0.29963004\n",
      "Iteration 18, loss = 0.29479095\n",
      "Iteration 19, loss = 0.28629333\n",
      "Iteration 70, loss = 0.09341153\n",
      "Iteration 20, loss = 0.27882277\n",
      "Iteration 71, loss = 0.09553918\n",
      "Iteration 21, loss = 0.27115234\n",
      "Iteration 72, loss = 0.08984069\n",
      "Iteration 22, loss = 0.26507027\n",
      "Iteration 23, loss = 0.25832199\n",
      "Iteration 24, loss = 0.25346411\n",
      "Iteration 73, loss = 0.09010906\n",
      "Iteration 25, loss = 0.24970473\n",
      "Iteration 74, loss = 0.09884307\n",
      "Iteration 75, loss = 0.09180449\n",
      "Iteration 76, loss = 0.08736444\n",
      "Iteration 26, loss = 0.23955775\n",
      "Iteration 27, loss = 0.23694995\n",
      "Iteration 77, loss = 0.08384596\n",
      "Iteration 28, loss = 0.23795999\n",
      "Iteration 29, loss = 0.22418416\n",
      "Iteration 78, loss = 0.08171867\n",
      "Iteration 30, loss = 0.22093210\n",
      "Iteration 79, loss = 0.08200873\n",
      "Iteration 31, loss = 0.21670712\n",
      "Iteration 80, loss = 0.08102430\n",
      "Iteration 32, loss = 0.21226957\n",
      "Iteration 81, loss = 0.08252937\n",
      "Iteration 33, loss = 0.20618612\n",
      "Iteration 82, loss = 0.07705756\n",
      "Iteration 34, loss = 0.20225299\n",
      "Iteration 83, loss = 0.07394697\n",
      "Iteration 35, loss = 0.19667606\n",
      "Iteration 36, loss = 0.19287261\n",
      "Iteration 84, loss = 0.07335087\n",
      "Iteration 37, loss = 0.18930807\n",
      "Iteration 85, loss = 0.07285736\n",
      "Iteration 86, loss = 0.07284707\n",
      "Iteration 87, loss = 0.07217027\n",
      "Iteration 38, loss = 0.18436120\n",
      "Iteration 39, loss = 0.18045524\n",
      "Iteration 40, loss = 0.17799674\n",
      "Iteration 88, loss = 0.07197587\n",
      "Iteration 41, loss = 0.17872324\n",
      "Iteration 89, loss = 0.07176606\n",
      "Iteration 42, loss = 0.17239152\n",
      "Iteration 90, loss = 0.06864079\n",
      "Iteration 43, loss = 0.17146634\n",
      "Iteration 44, loss = 0.16897824\n",
      "Iteration 45, loss = 0.16974567\n",
      "Iteration 91, loss = 0.07202245\n",
      "Iteration 46, loss = 0.15904517\n",
      "Iteration 92, loss = 0.06582761\n",
      "Iteration 47, loss = 0.16126952\n",
      "Iteration 93, loss = 0.06567744\n",
      "Iteration 48, loss = 0.15495221\n",
      "Iteration 94, loss = 0.06764774\n",
      "Iteration 49, loss = 0.15292704\n",
      "Iteration 95, loss = 0.06350935\n",
      "Iteration 50, loss = 0.15155797\n",
      "Iteration 51, loss = 0.15077215\n",
      "Iteration 96, loss = 0.06200041\n",
      "Iteration 97, loss = 0.05846248\n",
      "Iteration 98, loss = 0.05933586\n",
      "Iteration 52, loss = 0.14499775\n",
      "Iteration 99, loss = 0.05660933\n",
      "Iteration 53, loss = 0.14184597\n",
      "Iteration 54, loss = 0.14267737\n",
      "Iteration 55, loss = 0.13747215\n",
      "Iteration 100, loss = 0.05756020\n",
      "Iteration 56, loss = 0.13849120\n",
      "Iteration 57, loss = 0.13470839\n",
      "Iteration 58, loss = 0.13190344\n",
      "Iteration 59, loss = 0.13180190\n",
      "Iteration 60, loss = 0.12889362\n",
      "Iteration 61, loss = 0.12769390\n",
      "Iteration 62, loss = 0.12634948\n",
      "Iteration 63, loss = 0.12419118\n",
      "Iteration 64, loss = 0.12380046\n",
      "Iteration 65, loss = 0.12524710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, loss = 0.12198648\n",
      "Iteration 67, loss = 0.12284434\n",
      "Iteration 68, loss = 0.11947715\n",
      "Iteration 69, loss = 0.11782707\n",
      "Iteration 70, loss = 0.11572330\n",
      "Iteration 71, loss = 0.11389518\n",
      "Iteration 72, loss = 0.11305182\n",
      "Iteration 73, loss = 0.11077721\n",
      "Iteration 74, loss = 0.10856985\n",
      "Iteration 75, loss = 0.10727130\n",
      "Iteration 76, loss = 0.10645909\n",
      "Iteration 77, loss = 0.10668113\n",
      "Iteration 1, loss = 1.09400205\n",
      "Iteration 78, loss = 0.10586287\n",
      "Iteration 2, loss = 0.82329375\n",
      "Iteration 3, loss = 0.61127400\n",
      "Iteration 4, loss = 0.55547614\n",
      "Iteration 79, loss = 0.10206983\n",
      "Iteration 80, loss = 0.10327965\n",
      "Iteration 81, loss = 0.10021519\n",
      "Iteration 5, loss = 0.48509946\n",
      "Iteration 82, loss = 0.10243366\n",
      "Iteration 6, loss = 0.45237936\n",
      "Iteration 7, loss = 0.42579968\n",
      "Iteration 83, loss = 0.10214998\n",
      "Iteration 8, loss = 0.40331122\n",
      "Iteration 84, loss = 0.09831443\n",
      "Iteration 85, loss = 0.09645097\n",
      "Iteration 9, loss = 0.38898416\n",
      "Iteration 86, loss = 0.09737466\n",
      "Iteration 10, loss = 0.37495447\n",
      "Iteration 11, loss = 0.36216522\n",
      "Iteration 87, loss = 0.09802819\n",
      "Iteration 12, loss = 0.35108996\n",
      "Iteration 88, loss = 0.09593109\n",
      "Iteration 13, loss = 0.33998264\n",
      "Iteration 89, loss = 0.09424308\n",
      "Iteration 90, loss = 0.09420100\n",
      "Iteration 14, loss = 0.33110267\n",
      "Iteration 91, loss = 0.09185997\n",
      "Iteration 15, loss = 0.32233306\n",
      "Iteration 16, loss = 0.31325251\n",
      "Iteration 17, loss = 0.30546075\n",
      "Iteration 92, loss = 0.09374134\n",
      "Iteration 18, loss = 0.29791120\n",
      "Iteration 19, loss = 0.29120591\n",
      "Iteration 20, loss = 0.28470624\n",
      "Iteration 93, loss = 0.09236272\n",
      "Iteration 94, loss = 0.08892880\n",
      "Iteration 95, loss = 0.08795182\n",
      "Iteration 96, loss = 0.08607178\n",
      "Iteration 21, loss = 0.27733542\n",
      "Iteration 22, loss = 0.27106120\n",
      "Iteration 23, loss = 0.26441537\n",
      "Iteration 97, loss = 0.08729110\n",
      "Iteration 24, loss = 0.25914095\n",
      "Iteration 25, loss = 0.25218390\n",
      "Iteration 26, loss = 0.24724380\n",
      "Iteration 98, loss = 0.08346981\n",
      "Iteration 27, loss = 0.24171363\n",
      "Iteration 28, loss = 0.23797872\n",
      "Iteration 99, loss = 0.08173705\n",
      "Iteration 29, loss = 0.23298016\n",
      "Iteration 100, loss = 0.08078072\n",
      "Iteration 30, loss = 0.22809551\n",
      "Iteration 31, loss = 0.22538330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.21832922\n",
      "Iteration 33, loss = 0.21482536\n",
      "Iteration 34, loss = 0.20850284\n",
      "Iteration 35, loss = 0.20554846\n",
      "Iteration 1, loss = 1.12722330\n",
      "Iteration 2, loss = 0.79089748\n",
      "Iteration 3, loss = 0.63006421\n",
      "Iteration 36, loss = 0.19961689\n",
      "Iteration 4, loss = 0.55102154\n",
      "Iteration 5, loss = 0.49458375\n",
      "Iteration 6, loss = 0.45165204\n",
      "Iteration 7, loss = 0.42513921\n",
      "Iteration 8, loss = 0.39981572\n",
      "Iteration 37, loss = 0.19601116\n",
      "Iteration 9, loss = 0.38256781\n",
      "Iteration 38, loss = 0.19284988\n",
      "Iteration 10, loss = 0.36695482\n",
      "Iteration 39, loss = 0.18937555\n",
      "Iteration 11, loss = 0.35328321\n",
      "Iteration 40, loss = 0.18521499\n",
      "Iteration 41, loss = 0.18123390\n",
      "Iteration 42, loss = 0.17856472Iteration 12, loss = 0.34185759\n",
      "\n",
      "Iteration 13, loss = 0.33103573\n",
      "Iteration 14, loss = 0.32132790\n",
      "Iteration 43, loss = 0.17725268\n",
      "Iteration 15, loss = 0.31235477\n",
      "Iteration 16, loss = 0.30503415\n",
      "Iteration 44, loss = 0.17211599\n",
      "Iteration 45, loss = 0.16764066\n",
      "Iteration 17, loss = 0.29604795\n",
      "Iteration 46, loss = 0.16571659\n",
      "Iteration 18, loss = 0.28806338\n",
      "Iteration 47, loss = 0.16044085\n",
      "Iteration 19, loss = 0.28072635\n",
      "Iteration 20, loss = 0.27319916\n",
      "Iteration 48, loss = 0.15941225\n",
      "Iteration 21, loss = 0.26752034\n",
      "Iteration 49, loss = 0.15625064\n",
      "Iteration 50, loss = 0.15343081\n",
      "Iteration 22, loss = 0.26068757\n",
      "Iteration 51, loss = 0.15041928\n",
      "Iteration 23, loss = 0.25502168\n",
      "Iteration 24, loss = 0.24700181\n",
      "Iteration 52, loss = 0.14964057\n",
      "Iteration 25, loss = 0.24081417\n",
      "Iteration 53, loss = 0.14720880\n",
      "Iteration 26, loss = 0.23550689\n",
      "Iteration 54, loss = 0.14316975\n",
      "Iteration 27, loss = 0.23056627\n",
      "Iteration 55, loss = 0.14293152\n",
      "Iteration 28, loss = 0.22350685\n",
      "Iteration 29, loss = 0.21852933\n",
      "Iteration 56, loss = 0.14328059\n",
      "Iteration 57, loss = 0.13847556\n",
      "Iteration 30, loss = 0.21346647\n",
      "Iteration 58, loss = 0.13608104\n",
      "Iteration 31, loss = 0.20934164\n",
      "Iteration 59, loss = 0.13488543\n",
      "Iteration 32, loss = 0.20741112\n",
      "Iteration 33, loss = 0.19999637\n",
      "Iteration 60, loss = 0.13103736\n",
      "Iteration 34, loss = 0.19686112\n",
      "Iteration 61, loss = 0.12693156\n",
      "Iteration 35, loss = 0.19541424\n",
      "Iteration 62, loss = 0.12845581\n",
      "Iteration 63, loss = 0.12481412\n",
      "Iteration 36, loss = 0.19288626\n",
      "Iteration 64, loss = 0.12294553\n",
      "Iteration 37, loss = 0.18435243\n",
      "Iteration 65, loss = 0.12234078\n",
      "Iteration 38, loss = 0.18269023\n",
      "Iteration 66, loss = 0.11958234\n",
      "Iteration 39, loss = 0.17728237\n",
      "Iteration 67, loss = 0.11928318\n",
      "Iteration 68, loss = 0.11789609\n",
      "Iteration 69, loss = 0.11635855\n",
      "Iteration 40, loss = 0.17244059\n",
      "Iteration 70, loss = 0.11351727\n",
      "Iteration 41, loss = 0.16911492\n",
      "Iteration 42, loss = 0.16728950\n",
      "Iteration 71, loss = 0.11242881\n",
      "Iteration 43, loss = 0.16397200\n",
      "Iteration 44, loss = 0.15931632\n",
      "Iteration 72, loss = 0.11142869\n",
      "Iteration 45, loss = 0.15591835\n",
      "Iteration 73, loss = 0.11148813\n",
      "Iteration 46, loss = 0.15585625\n",
      "Iteration 47, loss = 0.14982766\n",
      "Iteration 74, loss = 0.10722486\n",
      "Iteration 48, loss = 0.14743508\n",
      "Iteration 75, loss = 0.10462290\n",
      "Iteration 49, loss = 0.14756098\n",
      "Iteration 50, loss = 0.14074797\n",
      "Iteration 76, loss = 0.10303999\n",
      "Iteration 51, loss = 0.14029172\n",
      "Iteration 52, loss = 0.14213074\n",
      "Iteration 77, loss = 0.10151946\n",
      "Iteration 78, loss = 0.10208040\n",
      "Iteration 53, loss = 0.14407808\n",
      "Iteration 79, loss = 0.09945556\n",
      "Iteration 80, loss = 0.09932477\n",
      "Iteration 54, loss = 0.13219243\n",
      "Iteration 55, loss = 0.13301436\n",
      "Iteration 56, loss = 0.13133045\n",
      "Iteration 81, loss = 0.09837543\n",
      "Iteration 57, loss = 0.13078825\n",
      "Iteration 58, loss = 0.12431278\n",
      "Iteration 82, loss = 0.09542346\n",
      "Iteration 59, loss = 0.12348344\n",
      "Iteration 83, loss = 0.09768689\n",
      "Iteration 60, loss = 0.12357538\n",
      "Iteration 84, loss = 0.09360659\n",
      "Iteration 61, loss = 0.11879096\n",
      "Iteration 62, loss = 0.11750577\n",
      "Iteration 85, loss = 0.09309199\n",
      "Iteration 63, loss = 0.11502237\n",
      "Iteration 64, loss = 0.11356179\n",
      "Iteration 65, loss = 0.11370115\n",
      "Iteration 86, loss = 0.09052272\n",
      "Iteration 66, loss = 0.11244139\n",
      "Iteration 67, loss = 0.10932142\n",
      "Iteration 87, loss = 0.09151899\n",
      "Iteration 68, loss = 0.10902067\n",
      "Iteration 88, loss = 0.08932274\n",
      "Iteration 69, loss = 0.10848444\n",
      "Iteration 89, loss = 0.08714003\n",
      "Iteration 70, loss = 0.10734032\n",
      "Iteration 71, loss = 0.10473511\n",
      "Iteration 72, loss = 0.10045862\n",
      "Iteration 90, loss = 0.09094297\n",
      "Iteration 73, loss = 0.10306878\n",
      "Iteration 74, loss = 0.10017232\n",
      "Iteration 75, loss = 0.09560252\n",
      "Iteration 91, loss = 0.08438722\n",
      "Iteration 76, loss = 0.09504553\n",
      "Iteration 92, loss = 0.08452226\n",
      "Iteration 77, loss = 0.09392019\n",
      "Iteration 93, loss = 0.08278026\n",
      "Iteration 78, loss = 0.09231542\n",
      "Iteration 79, loss = 0.09215206\n",
      "Iteration 94, loss = 0.08097634\n",
      "Iteration 80, loss = 0.09043532\n",
      "Iteration 81, loss = 0.08999254\n",
      "Iteration 95, loss = 0.08073672\n",
      "Iteration 82, loss = 0.08847990\n",
      "Iteration 96, loss = 0.08235823\n",
      "Iteration 83, loss = 0.08642286\n",
      "Iteration 97, loss = 0.08055297\n",
      "Iteration 84, loss = 0.08539000\n",
      "Iteration 85, loss = 0.08659879\n",
      "Iteration 98, loss = 0.07697157\n",
      "Iteration 86, loss = 0.08528467\n",
      "Iteration 87, loss = 0.08320361\n",
      "Iteration 99, loss = 0.07832047\n",
      "Iteration 88, loss = 0.08049691\n",
      "Iteration 100, loss = 0.07748971\n",
      "Iteration 89, loss = 0.08269194\n",
      "Iteration 101, loss = 0.07554469\n",
      "Iteration 90, loss = 0.07951597\n",
      "Iteration 91, loss = 0.07769894\n",
      "Iteration 102, loss = 0.07396940\n",
      "Iteration 92, loss = 0.07650310\n",
      "Iteration 93, loss = 0.07489367\n",
      "Iteration 94, loss = 0.07447309\n",
      "Iteration 103, loss = 0.07399121\n",
      "Iteration 95, loss = 0.07315703\n",
      "Iteration 104, loss = 0.07144529\n",
      "Iteration 96, loss = 0.07241400\n",
      "Iteration 105, loss = 0.07217581\n",
      "Iteration 97, loss = 0.07107619\n",
      "Iteration 106, loss = 0.07424369\n",
      "Iteration 98, loss = 0.07323829\n",
      "Iteration 99, loss = 0.07290024\n",
      "Iteration 100, loss = 0.06920689\n",
      "Iteration 107, loss = 0.07370901\n",
      "Iteration 108, loss = 0.07380019\n",
      "Iteration 109, loss = 0.06903582\n",
      "Iteration 110, loss = 0.06709437\n",
      "Iteration 111, loss = 0.06483573\n",
      "Iteration 112, loss = 0.06753352\n",
      "Iteration 113, loss = 0.06709902\n",
      "Iteration 114, loss = 0.06687728\n",
      "Iteration 115, loss = 0.06530364\n",
      "Iteration 1, loss = 0.94746996\n",
      "Iteration 116, loss = 0.06211884\n",
      "Iteration 117, loss = 0.06957311\n",
      "Iteration 2, loss = 0.67967778\n",
      "Iteration 3, loss = 0.54818217\n",
      "Iteration 118, loss = 0.06571697\n",
      "Iteration 4, loss = 0.47542311\n",
      "Iteration 119, loss = 0.06201142\n",
      "Iteration 5, loss = 0.42743812\n",
      "Iteration 120, loss = 0.05975351\n",
      "Iteration 121, loss = 0.05974236\n",
      "Iteration 6, loss = 0.39684886\n",
      "Iteration 122, loss = 0.06078386\n",
      "Iteration 7, loss = 0.37413188\n",
      "Iteration 123, loss = 0.05915983\n",
      "Iteration 8, loss = 0.35537445\n",
      "Iteration 124, loss = 0.05670411\n",
      "Iteration 9, loss = 0.34508226\n",
      "Iteration 125, loss = 0.05560677\n",
      "Iteration 10, loss = 0.32955381\n",
      "Iteration 126, loss = 0.05652227\n",
      "Iteration 11, loss = 0.31712921\n",
      "Iteration 127, loss = 0.05616298\n",
      "Iteration 12, loss = 0.30658109\n",
      "Iteration 13, loss = 0.29805361\n",
      "Iteration 128, loss = 0.05447124\n",
      "Iteration 14, loss = 0.28887023\n",
      "Iteration 129, loss = 0.05349438\n",
      "Iteration 15, loss = 0.28106436\n",
      "Iteration 130, loss = 0.05326586\n",
      "Iteration 131, loss = 0.05771170\n",
      "Iteration 16, loss = 0.27312240\n",
      "Iteration 132, loss = 0.05380637\n",
      "Iteration 17, loss = 0.26548830\n",
      "Iteration 133, loss = 0.05825566\n",
      "Iteration 134, loss = 0.05325031\n",
      "Iteration 18, loss = 0.25902647\n",
      "Iteration 135, loss = 0.05254548\n",
      "Iteration 19, loss = 0.25346227\n",
      "Iteration 136, loss = 0.05258654\n",
      "Iteration 20, loss = 0.24736004\n",
      "Iteration 21, loss = 0.24116644\n",
      "Iteration 137, loss = 0.04901534\n",
      "Iteration 22, loss = 0.23575760\n",
      "Iteration 138, loss = 0.04878800\n",
      "Iteration 23, loss = 0.23037797\n",
      "Iteration 139, loss = 0.04796825\n",
      "Iteration 24, loss = 0.22733700\n",
      "Iteration 140, loss = 0.04773870\n",
      "Iteration 25, loss = 0.22153833\n",
      "Iteration 141, loss = 0.04982518\n",
      "Iteration 26, loss = 0.21667924\n",
      "Iteration 142, loss = 0.05158063\n",
      "Iteration 143, loss = 0.05096675\n",
      "Iteration 27, loss = 0.21043298\n",
      "Iteration 28, loss = 0.20590460\n",
      "Iteration 144, loss = 0.05291186\n",
      "Iteration 29, loss = 0.20291459\n",
      "Iteration 30, loss = 0.19952590\n",
      "Iteration 145, loss = 0.05074274\n",
      "Iteration 31, loss = 0.19386219\n",
      "Iteration 146, loss = 0.04573406\n",
      "Iteration 32, loss = 0.19069191\n",
      "Iteration 147, loss = 0.04550773\n",
      "Iteration 33, loss = 0.18777095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 148, loss = 0.04520831\n",
      "Iteration 34, loss = 0.18254754\n",
      "Iteration 35, loss = 0.17998422\n",
      "Iteration 36, loss = 0.17661749\n",
      "Iteration 149, loss = 0.04324618\n",
      "Iteration 37, loss = 0.17429163\n",
      "Iteration 150, loss = 0.04221995\n",
      "Iteration 38, loss = 0.16985372\n",
      "Iteration 151, loss = 0.04326967\n",
      "Iteration 152, loss = 0.04426437\n",
      "Iteration 39, loss = 0.16861694\n",
      "Iteration 40, loss = 0.16557647\n",
      "Iteration 41, loss = 0.16074615\n",
      "Iteration 153, loss = 0.04221159\n",
      "Iteration 42, loss = 0.15915824\n",
      "Iteration 154, loss = 0.04154325\n",
      "Iteration 43, loss = 0.15637266\n",
      "Iteration 155, loss = 0.04204417\n",
      "Iteration 44, loss = 0.15273943\n",
      "Iteration 156, loss = 0.04302237\n",
      "Iteration 45, loss = 0.15060721\n",
      "Iteration 46, loss = 0.14797255\n",
      "Iteration 157, loss = 0.04304230\n",
      "Iteration 47, loss = 0.15010702\n",
      "Iteration 48, loss = 0.14307914\n",
      "Iteration 158, loss = 0.04381829\n",
      "Iteration 49, loss = 0.14162590\n",
      "Iteration 159, loss = 0.04524411\n",
      "Iteration 50, loss = 0.13804586\n",
      "Iteration 160, loss = 0.04317871\n",
      "Iteration 51, loss = 0.13894404\n",
      "Iteration 161, loss = 0.04320405\n",
      "Iteration 52, loss = 0.13338159\n",
      "Iteration 53, loss = 0.13186454\n",
      "Iteration 54, loss = 0.13286264\n",
      "Iteration 162, loss = 0.04127089\n",
      "Iteration 163, loss = 0.03957073\n",
      "Iteration 164, loss = 0.03953469\n",
      "Iteration 55, loss = 0.13169277\n",
      "Iteration 165, loss = 0.03712796\n",
      "Iteration 56, loss = 0.12647605\n",
      "Iteration 57, loss = 0.12663432\n",
      "Iteration 166, loss = 0.03694305\n",
      "Iteration 58, loss = 0.12364615\n",
      "Iteration 59, loss = 0.12131730\n",
      "Iteration 60, loss = 0.11924287\n",
      "Iteration 167, loss = 0.04000138\n",
      "Iteration 61, loss = 0.11690712\n",
      "Iteration 62, loss = 0.11633239\n",
      "Iteration 168, loss = 0.03934272\n",
      "Iteration 63, loss = 0.11477885\n",
      "Iteration 169, loss = 0.03773162\n",
      "Iteration 64, loss = 0.11218170\n",
      "Iteration 170, loss = 0.03933817\n",
      "Iteration 65, loss = 0.11129165\n",
      "Iteration 171, loss = 0.03720219\n",
      "Iteration 66, loss = 0.10903999\n",
      "Iteration 172, loss = 0.03766595\n",
      "Iteration 173, loss = 0.03566622\n",
      "Iteration 67, loss = 0.10865767\n",
      "Iteration 174, loss = 0.03437899\n",
      "Iteration 68, loss = 0.10729976\n",
      "Iteration 175, loss = 0.03353003\n",
      "Iteration 69, loss = 0.10587662\n",
      "Iteration 176, loss = 0.03368993\n",
      "Iteration 70, loss = 0.10328201\n",
      "Iteration 177, loss = 0.03553399\n",
      "Iteration 71, loss = 0.10285856\n",
      "Iteration 178, loss = 0.03486363\n",
      "Iteration 72, loss = 0.09923361\n",
      "Iteration 179, loss = 0.03336958\n",
      "Iteration 73, loss = 0.09810579\n",
      "Iteration 180, loss = 0.03581275\n",
      "Iteration 74, loss = 0.09940357\n",
      "Iteration 75, loss = 0.09696860\n",
      "Iteration 181, loss = 0.03323282\n",
      "Iteration 76, loss = 0.09474698\n",
      "Iteration 77, loss = 0.09291638\n",
      "Iteration 78, loss = 0.09098523\n",
      "Iteration 182, loss = 0.03223476\n",
      "Iteration 79, loss = 0.08971722\n",
      "Iteration 80, loss = 0.08859196\n",
      "Iteration 183, loss = 0.03591823\n",
      "Iteration 81, loss = 0.08801153\n",
      "Iteration 82, loss = 0.08683464\n",
      "Iteration 184, loss = 0.03624413\n",
      "Iteration 83, loss = 0.08358639\n",
      "Iteration 84, loss = 0.08140277\n",
      "Iteration 185, loss = 0.03676380\n",
      "Iteration 85, loss = 0.08328452\n",
      "Iteration 186, loss = 0.03478484\n",
      "Iteration 86, loss = 0.08066980\n",
      "Iteration 187, loss = 0.03129656\n",
      "Iteration 87, loss = 0.07864221\n",
      "Iteration 88, loss = 0.07748241\n",
      "Iteration 188, loss = 0.03318900\n",
      "Iteration 89, loss = 0.07601845\n",
      "Iteration 189, loss = 0.03068056\n",
      "Iteration 190, loss = 0.03246679\n",
      "Iteration 191, loss = 0.03122078\n",
      "Iteration 90, loss = 0.07711103\n",
      "Iteration 91, loss = 0.07444245\n",
      "Iteration 192, loss = 0.02960771\n",
      "Iteration 92, loss = 0.07391911\n",
      "Iteration 93, loss = 0.07600471\n",
      "Iteration 193, loss = 0.03040967\n",
      "Iteration 194, loss = 0.02953633\n",
      "Iteration 94, loss = 0.07041002\n",
      "Iteration 195, loss = 0.03284237\n",
      "Iteration 95, loss = 0.06890458\n",
      "Iteration 196, loss = 0.03324706\n",
      "Iteration 96, loss = 0.06843064\n",
      "Iteration 97, loss = 0.06790741\n",
      "Iteration 197, loss = 0.03373951\n",
      "Iteration 98, loss = 0.06594169\n",
      "Iteration 198, loss = 0.03146580\n",
      "Iteration 199, loss = 0.02946148\n",
      "Iteration 99, loss = 0.06469172\n",
      "Iteration 200, loss = 0.03173278\n",
      "Iteration 100, loss = 0.06369836\n",
      "Iteration 101, loss = 0.06368514\n",
      "Iteration 102, loss = 0.06327880\n",
      "Iteration 103, loss = 0.06391032\n",
      "Iteration 104, loss = 0.06441845\n",
      "Iteration 105, loss = 0.06112598\n",
      "Iteration 106, loss = 0.05867677\n",
      "Iteration 107, loss = 0.06271493\n",
      "Iteration 108, loss = 0.06243007\n",
      "Iteration 1, loss = 1.19692055\n",
      "Iteration 109, loss = 0.05521543\n",
      "Iteration 2, loss = 0.77665435\n",
      "Iteration 110, loss = 0.06062506\n",
      "Iteration 111, loss = 0.05566544\n",
      "Iteration 3, loss = 0.61802382\n",
      "Iteration 112, loss = 0.06087937\n",
      "Iteration 113, loss = 0.05843189\n",
      "Iteration 4, loss = 0.52224531\n",
      "Iteration 5, loss = 0.47086119\n",
      "Iteration 6, loss = 0.43383440\n",
      "Iteration 114, loss = 0.05789865\n",
      "Iteration 115, loss = 0.06001110\n",
      "Iteration 7, loss = 0.40572069\n",
      "Iteration 116, loss = 0.06193334\n",
      "Iteration 8, loss = 0.38356718\n",
      "Iteration 117, loss = 0.05462294\n",
      "Iteration 9, loss = 0.36869524\n",
      "Iteration 10, loss = 0.35406369\n",
      "Iteration 118, loss = 0.05181291\n",
      "Iteration 11, loss = 0.34029834\n",
      "Iteration 119, loss = 0.05399109\n",
      "Iteration 12, loss = 0.32869587\n",
      "Iteration 13, loss = 0.31868946\n",
      "Iteration 120, loss = 0.04954711\n",
      "Iteration 14, loss = 0.30920045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 121, loss = 0.05030572\n",
      "Iteration 15, loss = 0.30006272\n",
      "Iteration 122, loss = 0.05088660\n",
      "Iteration 123, loss = 0.04917901\n",
      "Iteration 16, loss = 0.29114461\n",
      "Iteration 124, loss = 0.05193559\n",
      "Iteration 17, loss = 0.28421510\n",
      "Iteration 125, loss = 0.04981151\n",
      "Iteration 126, loss = 0.04820699\n",
      "Iteration 18, loss = 0.27902544\n",
      "Iteration 19, loss = 0.26875123\n",
      "Iteration 127, loss = 0.05044434\n",
      "Iteration 20, loss = 0.26385849\n",
      "Iteration 21, loss = 0.25575908\n",
      "Iteration 128, loss = 0.04992995\n",
      "Iteration 22, loss = 0.25183300\n",
      "Iteration 23, loss = 0.24508064\n",
      "Iteration 129, loss = 0.04871433\n",
      "Iteration 130, loss = 0.04584910\n",
      "Iteration 24, loss = 0.23900723\n",
      "Iteration 131, loss = 0.04757560\n",
      "Iteration 132, loss = 0.04450679\n",
      "Iteration 25, loss = 0.23275606\n",
      "Iteration 133, loss = 0.04452802\n",
      "Iteration 134, loss = 0.04529744\n",
      "Iteration 26, loss = 0.22840189\n",
      "Iteration 135, loss = 0.04300294\n",
      "Iteration 27, loss = 0.22130081\n",
      "Iteration 136, loss = 0.04227823\n",
      "Iteration 28, loss = 0.21853598\n",
      "Iteration 137, loss = 0.04135122\n",
      "Iteration 29, loss = 0.21387153\n",
      "Iteration 30, loss = 0.21373159\n",
      "Iteration 138, loss = 0.04372748\n",
      "Iteration 31, loss = 0.20823433\n",
      "Iteration 139, loss = 0.04368488\n",
      "Iteration 32, loss = 0.20093055\n",
      "Iteration 140, loss = 0.04231814\n",
      "Iteration 33, loss = 0.19757294\n",
      "Iteration 141, loss = 0.04116517\n",
      "Iteration 142, loss = 0.04105820\n",
      "Iteration 34, loss = 0.19324683\n",
      "Iteration 143, loss = 0.04355713\n",
      "Iteration 35, loss = 0.19073273\n",
      "Iteration 144, loss = 0.04448078\n",
      "Iteration 36, loss = 0.18663401\n",
      "Iteration 37, loss = 0.18613032\n",
      "Iteration 145, loss = 0.04438864\n",
      "Iteration 38, loss = 0.18121587\n",
      "Iteration 146, loss = 0.04634379\n",
      "Iteration 39, loss = 0.17638604\n",
      "Iteration 147, loss = 0.04732647\n",
      "Iteration 40, loss = 0.17222988\n",
      "Iteration 148, loss = 0.03955902\n",
      "Iteration 41, loss = 0.17156492\n",
      "Iteration 42, loss = 0.16764908\n",
      "Iteration 149, loss = 0.03961406\n",
      "Iteration 43, loss = 0.16407854\n",
      "Iteration 150, loss = 0.03763024\n",
      "Iteration 44, loss = 0.16411085\n",
      "Iteration 151, loss = 0.04061490\n",
      "Iteration 45, loss = 0.15905189\n",
      "Iteration 46, loss = 0.15916942\n",
      "Iteration 47, loss = 0.15379837\n",
      "Iteration 152, loss = 0.03911154\n",
      "Iteration 48, loss = 0.15419181\n",
      "Iteration 153, loss = 0.03729552\n",
      "Iteration 49, loss = 0.15120423\n",
      "Iteration 50, loss = 0.14895174\n",
      "Iteration 154, loss = 0.03664577\n",
      "Iteration 51, loss = 0.14598021\n",
      "Iteration 52, loss = 0.14782331\n",
      "Iteration 155, loss = 0.03596695\n",
      "Iteration 53, loss = 0.14353459\n",
      "Iteration 54, loss = 0.14294287\n",
      "Iteration 156, loss = 0.03560447\n",
      "Iteration 55, loss = 0.13903649\n",
      "Iteration 157, loss = 0.03569203\n",
      "Iteration 56, loss = 0.14025174\n",
      "Iteration 158, loss = 0.03494973\n",
      "Iteration 57, loss = 0.13799641\n",
      "Iteration 159, loss = 0.03534602\n",
      "Iteration 58, loss = 0.14280624\n",
      "Iteration 59, loss = 0.12984777\n",
      "Iteration 160, loss = 0.03432423\n",
      "Iteration 60, loss = 0.13185366\n",
      "Iteration 61, loss = 0.12891033\n",
      "Iteration 161, loss = 0.03536311\n",
      "Iteration 162, loss = 0.03678259\n",
      "Iteration 62, loss = 0.12930590\n",
      "Iteration 163, loss = 0.03511088\n",
      "Iteration 164, loss = 0.03377252\n",
      "Iteration 63, loss = 0.12466049\n",
      "Iteration 165, loss = 0.03295445\n",
      "Iteration 64, loss = 0.12314519\n",
      "Iteration 166, loss = 0.03394846\n",
      "Iteration 65, loss = 0.12171645\n",
      "Iteration 167, loss = 0.03215894\n",
      "Iteration 66, loss = 0.12071827\n",
      "Iteration 168, loss = 0.03160428\n",
      "Iteration 67, loss = 0.11802492\n",
      "Iteration 169, loss = 0.03289159\n",
      "Iteration 68, loss = 0.11708588\n",
      "Iteration 170, loss = 0.03276387\n",
      "Iteration 69, loss = 0.11530392\n",
      "Iteration 171, loss = 0.03163115\n",
      "Iteration 70, loss = 0.11372384\n",
      "Iteration 172, loss = 0.03170163\n",
      "Iteration 71, loss = 0.11549040\n",
      "Iteration 173, loss = 0.03275273\n",
      "Iteration 72, loss = 0.11348176\n",
      "Iteration 174, loss = 0.03277042\n",
      "Iteration 73, loss = 0.11066593\n",
      "Iteration 175, loss = 0.03137606\n",
      "Iteration 74, loss = 0.10853545\n",
      "Iteration 75, loss = 0.10932150\n",
      "Iteration 176, loss = 0.03150224\n",
      "Iteration 177, loss = 0.03366713\n",
      "Iteration 76, loss = 0.10908593\n",
      "Iteration 77, loss = 0.10782993\n",
      "Iteration 178, loss = 0.02933570\n",
      "Iteration 78, loss = 0.10713518\n",
      "Iteration 79, loss = 0.10484232\n",
      "Iteration 80, loss = 0.10246861\n",
      "Iteration 179, loss = 0.03282338\n",
      "Iteration 81, loss = 0.10365514\n",
      "Iteration 180, loss = 0.02999303\n",
      "Iteration 82, loss = 0.10568083\n",
      "Iteration 83, loss = 0.10568269\n",
      "Iteration 84, loss = 0.10160880\n",
      "Iteration 181, loss = 0.03557473\n",
      "Iteration 182, loss = 0.02912887\n",
      "Iteration 85, loss = 0.09920684\n",
      "Iteration 183, loss = 0.03691221\n",
      "Iteration 184, loss = 0.03864607\n",
      "Iteration 185, loss = 0.04110282\n",
      "Iteration 86, loss = 0.09943406\n",
      "Iteration 186, loss = 0.05365200\n",
      "Iteration 187, loss = 0.04778615\n",
      "Iteration 87, loss = 0.09322747\n",
      "Iteration 188, loss = 0.04430421\n",
      "Iteration 88, loss = 0.09307658\n",
      "Iteration 189, loss = 0.03380932\n",
      "Iteration 89, loss = 0.09311511\n",
      "Iteration 190, loss = 0.03152982\n",
      "Iteration 191, loss = 0.02849498\n",
      "Iteration 90, loss = 0.09120571\n",
      "Iteration 192, loss = 0.02745347\n",
      "Iteration 193, loss = 0.02750467\n",
      "Iteration 194, loss = 0.02732986\n",
      "Iteration 91, loss = 0.08935342\n",
      "Iteration 195, loss = 0.02693256\n",
      "Iteration 196, loss = 0.02716972\n",
      "Iteration 92, loss = 0.08771570\n",
      "Iteration 197, loss = 0.02576739\n",
      "Iteration 198, loss = 0.02812351\n",
      "Iteration 199, loss = 0.02600180\n",
      "Iteration 200, loss = 0.02580770\n",
      "Iteration 93, loss = 0.08748966\n",
      "Iteration 94, loss = 0.08695567\n",
      "Iteration 95, loss = 0.08647465\n",
      "Iteration 96, loss = 0.09371392\n",
      "Iteration 97, loss = 0.08760859\n",
      "Iteration 98, loss = 0.08339818\n",
      "Iteration 99, loss = 0.08195119\n",
      "Iteration 100, loss = 0.08056676\n",
      "Iteration 1, loss = 1.13896433\n",
      "Iteration 101, loss = 0.08220180\n",
      "Iteration 102, loss = 0.08240461\n",
      "Iteration 103, loss = 0.08150974\n",
      "Iteration 2, loss = 0.76151759\n",
      "Iteration 3, loss = 0.60780910\n",
      "Iteration 4, loss = 0.52397947\n",
      "Iteration 104, loss = 0.07666435\n",
      "Iteration 5, loss = 0.47406781\n",
      "Iteration 105, loss = 0.07905193\n",
      "Iteration 106, loss = 0.07405720\n",
      "Iteration 6, loss = 0.44199533\n",
      "Iteration 107, loss = 0.07453621\n",
      "Iteration 7, loss = 0.41793087\n",
      "Iteration 108, loss = 0.07452288\n",
      "Iteration 8, loss = 0.39945801\n",
      "Iteration 109, loss = 0.07575987\n",
      "Iteration 9, loss = 0.38268375\n",
      "Iteration 110, loss = 0.07743716\n",
      "Iteration 10, loss = 0.36857071\n",
      "Iteration 11, loss = 0.35623896\n",
      "Iteration 111, loss = 0.07649620\n",
      "Iteration 12, loss = 0.34425812\n",
      "Iteration 112, loss = 0.07147253\n",
      "Iteration 13, loss = 0.33483342\n",
      "Iteration 113, loss = 0.06862990\n",
      "Iteration 14, loss = 0.32547247\n",
      "Iteration 114, loss = 0.06938923\n",
      "Iteration 15, loss = 0.31652020\n",
      "Iteration 16, loss = 0.30826246\n",
      "Iteration 115, loss = 0.06970857\n",
      "Iteration 17, loss = 0.29783293\n",
      "Iteration 116, loss = 0.06722899\n",
      "Iteration 117, loss = 0.06900786\n",
      "Iteration 118, loss = 0.06560323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.29323713\n",
      "Iteration 19, loss = 0.28266585\n",
      "Iteration 119, loss = 0.06568032\n",
      "Iteration 20, loss = 0.27957040\n",
      "Iteration 21, loss = 0.26911709\n",
      "Iteration 120, loss = 0.06619794\n",
      "Iteration 121, loss = 0.06415995\n",
      "Iteration 22, loss = 0.26399580\n",
      "Iteration 122, loss = 0.06347231\n",
      "Iteration 123, loss = 0.06172318\n",
      "Iteration 23, loss = 0.25403249\n",
      "Iteration 124, loss = 0.06064457\n",
      "Iteration 125, loss = 0.06132955\n",
      "Iteration 24, loss = 0.25038098\n",
      "Iteration 25, loss = 0.24565451\n",
      "Iteration 26, loss = 0.23928668\n",
      "Iteration 126, loss = 0.06097557\n",
      "Iteration 27, loss = 0.23161369\n",
      "Iteration 127, loss = 0.05993952\n",
      "Iteration 128, loss = 0.06128572\n",
      "Iteration 28, loss = 0.23259788\n",
      "Iteration 129, loss = 0.06192147\n",
      "Iteration 29, loss = 0.22758410\n",
      "Iteration 30, loss = 0.22230777\n",
      "Iteration 130, loss = 0.06090503\n",
      "Iteration 131, loss = 0.06144946\n",
      "Iteration 132, loss = 0.05833746\n",
      "Iteration 31, loss = 0.21310167\n",
      "Iteration 133, loss = 0.05596693\n",
      "Iteration 134, loss = 0.05647739\n",
      "Iteration 135, loss = 0.05773104\n",
      "Iteration 32, loss = 0.20999363\n",
      "Iteration 33, loss = 0.20634377\n",
      "Iteration 136, loss = 0.05835848\n",
      "Iteration 34, loss = 0.20059456\n",
      "Iteration 35, loss = 0.19828187\n",
      "Iteration 36, loss = 0.19428862\n",
      "Iteration 137, loss = 0.05800415\n",
      "Iteration 37, loss = 0.19090798\n",
      "Iteration 38, loss = 0.18877838\n",
      "Iteration 39, loss = 0.18458078\n",
      "Iteration 138, loss = 0.05610989\n",
      "Iteration 139, loss = 0.06070719\n",
      "Iteration 40, loss = 0.17988095\n",
      "Iteration 140, loss = 0.06312921\n",
      "Iteration 41, loss = 0.17710670\n",
      "Iteration 141, loss = 0.06397291\n",
      "Iteration 142, loss = 0.05374870\n",
      "Iteration 42, loss = 0.17625176\n",
      "Iteration 143, loss = 0.05097464\n",
      "Iteration 144, loss = 0.05017756\n",
      "Iteration 43, loss = 0.17217211\n",
      "Iteration 145, loss = 0.05270095\n",
      "Iteration 146, loss = 0.04983195\n",
      "Iteration 147, loss = 0.05243117\n",
      "Iteration 44, loss = 0.16892191\n",
      "Iteration 148, loss = 0.05516489\n",
      "Iteration 45, loss = 0.16638044\n",
      "Iteration 46, loss = 0.16325387\n",
      "Iteration 149, loss = 0.05916482\n",
      "Iteration 47, loss = 0.16129328\n",
      "Iteration 150, loss = 0.05078312\n",
      "Iteration 151, loss = 0.04687992\n",
      "Iteration 48, loss = 0.15839876\n",
      "Iteration 152, loss = 0.04786011\n",
      "Iteration 49, loss = 0.15377127\n",
      "Iteration 50, loss = 0.15395208\n",
      "Iteration 153, loss = 0.04818218Iteration 51, loss = 0.14814093\n",
      "\n",
      "Iteration 154, loss = 0.04980893\n",
      "Iteration 52, loss = 0.14725069\n",
      "Iteration 155, loss = 0.04789494\n",
      "Iteration 53, loss = 0.14695580\n",
      "Iteration 156, loss = 0.04714968\n",
      "Iteration 157, loss = 0.04430167\n",
      "Iteration 54, loss = 0.14224139\n",
      "Iteration 55, loss = 0.14147520\n",
      "Iteration 158, loss = 0.04633589\n",
      "Iteration 56, loss = 0.13965617\n",
      "Iteration 159, loss = 0.04331971\n",
      "Iteration 57, loss = 0.13925672\n",
      "Iteration 160, loss = 0.04389745\n",
      "Iteration 58, loss = 0.14084357\n",
      "Iteration 161, loss = 0.04280720\n",
      "Iteration 59, loss = 0.13554994\n",
      "Iteration 60, loss = 0.13022177\n",
      "Iteration 162, loss = 0.04422417\n",
      "Iteration 163, loss = 0.04447065\n",
      "Iteration 61, loss = 0.12799688\n",
      "Iteration 164, loss = 0.04619858\n",
      "Iteration 62, loss = 0.12793636\n",
      "Iteration 165, loss = 0.04535555\n",
      "Iteration 63, loss = 0.12427636\n",
      "Iteration 64, loss = 0.12283946\n",
      "Iteration 166, loss = 0.04302914\n",
      "Iteration 167, loss = 0.04594402\n",
      "Iteration 65, loss = 0.12157506\n",
      "Iteration 168, loss = 0.04480535\n",
      "Iteration 66, loss = 0.11985329\n",
      "Iteration 169, loss = 0.04164646\n",
      "Iteration 67, loss = 0.12311697\n",
      "Iteration 170, loss = 0.04086018\n",
      "Iteration 171, loss = 0.04073204\n",
      "Iteration 68, loss = 0.11632077\n",
      "Iteration 69, loss = 0.11471445\n",
      "Iteration 172, loss = 0.04081587\n",
      "Iteration 70, loss = 0.11419124\n",
      "Iteration 71, loss = 0.11279476\n",
      "Iteration 72, loss = 0.11864973\n",
      "Iteration 173, loss = 0.04248782\n",
      "Iteration 174, loss = 0.04130015\n",
      "Iteration 73, loss = 0.11130869\n",
      "Iteration 175, loss = 0.03852948\n",
      "Iteration 74, loss = 0.10927230\n",
      "Iteration 176, loss = 0.03712991\n",
      "Iteration 75, loss = 0.11398839\n",
      "Iteration 177, loss = 0.03702412\n",
      "Iteration 76, loss = 0.11009946\n",
      "Iteration 178, loss = 0.03746685\n",
      "Iteration 179, loss = 0.03831354\n",
      "Iteration 77, loss = 0.10497962\n",
      "Iteration 180, loss = 0.04159360\n",
      "Iteration 78, loss = 0.11063554\n",
      "Iteration 79, loss = 0.10125838\n",
      "Iteration 181, loss = 0.04035774\n",
      "Iteration 80, loss = 0.10109496\n",
      "Iteration 182, loss = 0.04014104\n",
      "Iteration 81, loss = 0.09985040\n",
      "Iteration 183, loss = 0.03582266\n",
      "Iteration 82, loss = 0.10526766\n",
      "Iteration 184, loss = 0.03672835\n",
      "Iteration 83, loss = 0.10268705\n",
      "Iteration 84, loss = 0.10066179\n",
      "Iteration 185, loss = 0.03653828\n",
      "Iteration 85, loss = 0.09497080\n",
      "Iteration 86, loss = 0.09585422\n",
      "Iteration 186, loss = 0.03477943\n",
      "Iteration 87, loss = 0.09572036\n",
      "Iteration 187, loss = 0.03468012\n",
      "Iteration 188, loss = 0.03506779\n",
      "Iteration 189, loss = 0.03364005\n",
      "Iteration 88, loss = 0.09579760\n",
      "Iteration 89, loss = 0.09039525\n",
      "Iteration 190, loss = 0.03382942\n",
      "Iteration 90, loss = 0.08712254\n",
      "Iteration 91, loss = 0.09388950\n",
      "Iteration 191, loss = 0.03331248\n",
      "Iteration 192, loss = 0.03724706\n",
      "Iteration 193, loss = 0.03423460\n",
      "Iteration 92, loss = 0.08650036\n",
      "Iteration 194, loss = 0.03394431\n",
      "Iteration 93, loss = 0.08373494\n",
      "Iteration 94, loss = 0.08633266\n",
      "Iteration 195, loss = 0.03350419\n",
      "Iteration 95, loss = 0.08300951\n",
      "Iteration 196, loss = 0.03315030\n",
      "Iteration 96, loss = 0.08137336\n",
      "Iteration 97, loss = 0.07944151\n",
      "Iteration 197, loss = 0.03227864\n",
      "Iteration 198, loss = 0.03426704\n",
      "Iteration 98, loss = 0.07898332\n",
      "Iteration 199, loss = 0.03548991\n",
      "Iteration 99, loss = 0.07928827\n",
      "Iteration 100, loss = 0.07974741\n",
      "Iteration 200, loss = 0.03551548\n",
      "Iteration 101, loss = 0.07776370\n",
      "Iteration 102, loss = 0.07680025\n",
      "Iteration 103, loss = 0.07634354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 104, loss = 0.07536659\n",
      "Iteration 105, loss = 0.07436878\n",
      "Iteration 106, loss = 0.07409899\n",
      "Iteration 107, loss = 0.07217502\n",
      "Iteration 108, loss = 0.07041367\n",
      "Iteration 109, loss = 0.06866054\n",
      "Iteration 110, loss = 0.07111985\n",
      "Iteration 111, loss = 0.06739564\n",
      "Iteration 112, loss = 0.07093777\n",
      "Iteration 1, loss = 1.38387923\n",
      "Iteration 113, loss = 0.06690936\n",
      "Iteration 2, loss = 0.83097197\n",
      "Iteration 114, loss = 0.06706079\n",
      "Iteration 3, loss = 0.63137835\n",
      "Iteration 4, loss = 0.51428621\n",
      "Iteration 5, loss = 0.46656574\n",
      "Iteration 6, loss = 0.42414445\n",
      "Iteration 115, loss = 0.06905238\n",
      "Iteration 116, loss = 0.06836427\n",
      "Iteration 117, loss = 0.07112751\n",
      "Iteration 7, loss = 0.39417468\n",
      "Iteration 8, loss = 0.37415685\n",
      "Iteration 118, loss = 0.07603091\n",
      "Iteration 9, loss = 0.35402240\n",
      "Iteration 119, loss = 0.07061058\n",
      "Iteration 10, loss = 0.33963694\n",
      "Iteration 120, loss = 0.06617820\n",
      "Iteration 11, loss = 0.32724701\n",
      "Iteration 12, loss = 0.31582860\n",
      "Iteration 13, loss = 0.30618942\n",
      "Iteration 14, loss = 0.29694632\n",
      "Iteration 121, loss = 0.06109502\n",
      "Iteration 15, loss = 0.28705382\n",
      "Iteration 16, loss = 0.28011688\n",
      "Iteration 17, loss = 0.27269560\n",
      "Iteration 18, loss = 0.26538345\n",
      "Iteration 122, loss = 0.06358941\n",
      "Iteration 19, loss = 0.25660389\n",
      "Iteration 123, loss = 0.05881783\n",
      "Iteration 20, loss = 0.25093799\n",
      "Iteration 124, loss = 0.05933891\n",
      "Iteration 21, loss = 0.24334399\n",
      "Iteration 22, loss = 0.23526910\n",
      "Iteration 125, loss = 0.06264424\n",
      "Iteration 23, loss = 0.23063304\n",
      "Iteration 24, loss = 0.22455109\n",
      "Iteration 25, loss = 0.21922181\n",
      "Iteration 126, loss = 0.05745884\n",
      "Iteration 26, loss = 0.21245501\n",
      "Iteration 27, loss = 0.20884149\n",
      "Iteration 127, loss = 0.05765699\n",
      "Iteration 28, loss = 0.20143474\n",
      "Iteration 29, loss = 0.19743522\n",
      "Iteration 30, loss = 0.19197246\n",
      "Iteration 128, loss = 0.05786532\n",
      "Iteration 31, loss = 0.18827383\n",
      "Iteration 32, loss = 0.18266226\n",
      "Iteration 33, loss = 0.17905587\n",
      "Iteration 34, loss = 0.17578084\n",
      "Iteration 129, loss = 0.05704960\n",
      "Iteration 35, loss = 0.17023589\n",
      "Iteration 36, loss = 0.16740896\n",
      "Iteration 37, loss = 0.16365698\n",
      "Iteration 38, loss = 0.15912289\n",
      "Iteration 130, loss = 0.05389661\n",
      "Iteration 39, loss = 0.15635214\n",
      "Iteration 131, loss = 0.05343139\n",
      "Iteration 40, loss = 0.15516490\n",
      "Iteration 132, loss = 0.05715713\n",
      "Iteration 41, loss = 0.15237041\n",
      "Iteration 133, loss = 0.05614302\n",
      "Iteration 42, loss = 0.14620314\n",
      "Iteration 134, loss = 0.05352485\n",
      "Iteration 135, loss = 0.05821763\n",
      "Iteration 43, loss = 0.14389368\n",
      "Iteration 136, loss = 0.05672227\n",
      "Iteration 137, loss = 0.05665393\n",
      "Iteration 44, loss = 0.14242002\n",
      "Iteration 45, loss = 0.13825341\n",
      "Iteration 46, loss = 0.13441442\n",
      "Iteration 138, loss = 0.05338265\n",
      "Iteration 47, loss = 0.13149729\n",
      "Iteration 139, loss = 0.05565262\n",
      "Iteration 140, loss = 0.05708873\n",
      "Iteration 141, loss = 0.04969322\n",
      "Iteration 48, loss = 0.13044549\n",
      "Iteration 142, loss = 0.04908138\n",
      "Iteration 49, loss = 0.12740993\n",
      "Iteration 50, loss = 0.12361694\n",
      "Iteration 51, loss = 0.12168863\n",
      "Iteration 143, loss = 0.05243051\n",
      "Iteration 144, loss = 0.05179894\n",
      "Iteration 52, loss = 0.12231487\n",
      "Iteration 145, loss = 0.04927447\n",
      "Iteration 146, loss = 0.04672154\n",
      "Iteration 53, loss = 0.11616102\n",
      "Iteration 54, loss = 0.11558909\n",
      "Iteration 147, loss = 0.04566213\n",
      "Iteration 55, loss = 0.11252507\n",
      "Iteration 56, loss = 0.11074347\n",
      "Iteration 148, loss = 0.04589800\n",
      "Iteration 149, loss = 0.04579793\n",
      "Iteration 150, loss = 0.04713534\n",
      "Iteration 57, loss = 0.10942682\n",
      "Iteration 151, loss = 0.04477135\n",
      "Iteration 152, loss = 0.04387291\n",
      "Iteration 58, loss = 0.10573767\n",
      "Iteration 153, loss = 0.04307175\n",
      "Iteration 59, loss = 0.10899289\n",
      "Iteration 60, loss = 0.10470409\n",
      "Iteration 154, loss = 0.04331032\n",
      "Iteration 61, loss = 0.10112280\n",
      "Iteration 62, loss = 0.10095509\n",
      "Iteration 63, loss = 0.09815273\n",
      "Iteration 155, loss = 0.04377491\n",
      "Iteration 64, loss = 0.09781708\n",
      "Iteration 156, loss = 0.04320929\n",
      "Iteration 65, loss = 0.09447435\n",
      "Iteration 157, loss = 0.04112717\n",
      "Iteration 66, loss = 0.09733321\n",
      "Iteration 67, loss = 0.09560524\n",
      "Iteration 158, loss = 0.04104340\n",
      "Iteration 68, loss = 0.09540015\n",
      "Iteration 159, loss = 0.04132881\n",
      "Iteration 160, loss = 0.04330331\n",
      "Iteration 161, loss = 0.04531266\n",
      "Iteration 69, loss = 0.08931611\n",
      "Iteration 162, loss = 0.04356354\n",
      "Iteration 70, loss = 0.08869196\n",
      "Iteration 71, loss = 0.08738277\n",
      "Iteration 163, loss = 0.04013067\n",
      "Iteration 72, loss = 0.08412373\n",
      "Iteration 164, loss = 0.03890280\n",
      "Iteration 165, loss = 0.04098959\n",
      "Iteration 73, loss = 0.08311359\n",
      "Iteration 74, loss = 0.08179251\n",
      "Iteration 75, loss = 0.08005714\n",
      "Iteration 166, loss = 0.03875365\n",
      "Iteration 76, loss = 0.07904433\n",
      "Iteration 77, loss = 0.07801643\n",
      "Iteration 167, loss = 0.04164145\n",
      "Iteration 78, loss = 0.08062311\n",
      "Iteration 168, loss = 0.03866555\n",
      "Iteration 79, loss = 0.07760124\n",
      "Iteration 169, loss = 0.04028010\n",
      "Iteration 170, loss = 0.04027514\n",
      "Iteration 80, loss = 0.07496685\n",
      "Iteration 171, loss = 0.03727487\n",
      "Iteration 81, loss = 0.07744362\n",
      "Iteration 172, loss = 0.03847315\n",
      "Iteration 82, loss = 0.07280839\n",
      "Iteration 173, loss = 0.03786327\n",
      "Iteration 174, loss = 0.03778005\n",
      "Iteration 175, loss = 0.03622778\n",
      "Iteration 83, loss = 0.07191981\n",
      "Iteration 176, loss = 0.03788305\n",
      "Iteration 177, loss = 0.03708163\n",
      "Iteration 84, loss = 0.07000035\n",
      "Iteration 85, loss = 0.06883682\n",
      "Iteration 178, loss = 0.03472898\n",
      "Iteration 86, loss = 0.06949066\n",
      "Iteration 179, loss = 0.03550723\n",
      "Iteration 180, loss = 0.03395779\n",
      "Iteration 87, loss = 0.06791668\n",
      "Iteration 181, loss = 0.03368692\n",
      "Iteration 88, loss = 0.06661772\n",
      "Iteration 182, loss = 0.03476872\n",
      "Iteration 89, loss = 0.06635672\n",
      "Iteration 90, loss = 0.06417647\n",
      "Iteration 91, loss = 0.06421973\n",
      "Iteration 183, loss = 0.03558052\n",
      "Iteration 92, loss = 0.06386328\n",
      "Iteration 184, loss = 0.03611673\n",
      "Iteration 185, loss = 0.03516855\n",
      "Iteration 93, loss = 0.06229158\n",
      "Iteration 94, loss = 0.06120289\n",
      "Iteration 186, loss = 0.03296545\n",
      "Iteration 95, loss = 0.06084279\n",
      "Iteration 96, loss = 0.05993106\n",
      "Iteration 187, loss = 0.03392204\n",
      "Iteration 188, loss = 0.03305497\n",
      "Iteration 189, loss = 0.03173067\n",
      "Iteration 190, loss = 0.03141232\n",
      "Iteration 97, loss = 0.06036149\n",
      "Iteration 98, loss = 0.05883243\n",
      "Iteration 191, loss = 0.03182208\n",
      "Iteration 99, loss = 0.05742243\n",
      "Iteration 192, loss = 0.03157813\n",
      "Iteration 100, loss = 0.05640891\n",
      "Iteration 193, loss = 0.03001566\n",
      "Iteration 101, loss = 0.05574491\n",
      "Iteration 194, loss = 0.03022121\n",
      "Iteration 102, loss = 0.05486917\n",
      "Iteration 195, loss = 0.03329928\n",
      "Iteration 196, loss = 0.03197870\n",
      "Iteration 197, loss = 0.03590782\n",
      "Iteration 103, loss = 0.05383062\n",
      "Iteration 198, loss = 0.03317812\n",
      "Iteration 104, loss = 0.05480613\n",
      "Iteration 199, loss = 0.03293887\n",
      "Iteration 105, loss = 0.05374927\n",
      "Iteration 200, loss = 0.03217814\n",
      "Iteration 106, loss = 0.05738698\n",
      "Iteration 107, loss = 0.05324159\n",
      "Iteration 108, loss = 0.05007233\n",
      "Iteration 109, loss = 0.05299275\n",
      "Iteration 110, loss = 0.05092073\n",
      "Iteration 111, loss = 0.05351993\n",
      "Iteration 112, loss = 0.04960133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 113, loss = 0.04877436\n",
      "Iteration 114, loss = 0.04760155\n",
      "Iteration 115, loss = 0.04683003\n",
      "Iteration 116, loss = 0.04724735\n",
      "Iteration 117, loss = 0.04635445\n",
      "Iteration 1, loss = 0.93602802\n",
      "Iteration 118, loss = 0.04699133\n",
      "Iteration 2, loss = 0.71551827\n",
      "Iteration 119, loss = 0.04519216\n",
      "Iteration 120, loss = 0.04409102\n",
      "Iteration 3, loss = 0.59220883\n",
      "Iteration 121, loss = 0.04458712\n",
      "Iteration 4, loss = 0.52350728\n",
      "Iteration 122, loss = 0.04434212\n",
      "Iteration 5, loss = 0.47493279\n",
      "Iteration 123, loss = 0.04298773\n",
      "Iteration 6, loss = 0.44302676\n",
      "Iteration 124, loss = 0.04204107\n",
      "Iteration 7, loss = 0.41616870\n",
      "Iteration 125, loss = 0.04307312\n",
      "Iteration 8, loss = 0.39823079\n",
      "Iteration 126, loss = 0.04317279\n",
      "Iteration 9, loss = 0.38115399\n",
      "Iteration 127, loss = 0.04305752\n",
      "Iteration 128, loss = 0.04155984\n",
      "Iteration 129, loss = 0.04014942\n",
      "Iteration 130, loss = 0.03982738\n",
      "Iteration 10, loss = 0.36639488\n",
      "Iteration 131, loss = 0.04108726\n",
      "Iteration 11, loss = 0.35354763\n",
      "Iteration 132, loss = 0.03958368\n",
      "Iteration 12, loss = 0.34234597\n",
      "Iteration 13, loss = 0.33189890\n",
      "Iteration 133, loss = 0.04108500\n",
      "Iteration 134, loss = 0.04148873\n",
      "Iteration 14, loss = 0.32187205\n",
      "Iteration 135, loss = 0.03932504\n",
      "Iteration 136, loss = 0.03801467\n",
      "Iteration 15, loss = 0.31213228\n",
      "Iteration 16, loss = 0.30330814\n",
      "Iteration 137, loss = 0.03729100\n",
      "Iteration 17, loss = 0.29572374\n",
      "Iteration 138, loss = 0.03950468\n",
      "Iteration 139, loss = 0.03638281\n",
      "Iteration 18, loss = 0.28698288\n",
      "Iteration 140, loss = 0.03718074\n",
      "Iteration 19, loss = 0.28626672\n",
      "Iteration 20, loss = 0.27137434\n",
      "Iteration 141, loss = 0.03743291\n",
      "Iteration 21, loss = 0.26716179\n",
      "Iteration 22, loss = 0.26085116\n",
      "Iteration 142, loss = 0.03526676\n",
      "Iteration 143, loss = 0.03492965\n",
      "Iteration 23, loss = 0.25212582\n",
      "Iteration 144, loss = 0.03376219\n",
      "Iteration 24, loss = 0.24777933\n",
      "Iteration 145, loss = 0.03413999\n",
      "Iteration 25, loss = 0.24064847\n",
      "Iteration 26, loss = 0.23937263\n",
      "Iteration 146, loss = 0.03711687\n",
      "Iteration 27, loss = 0.23299419\n",
      "Iteration 147, loss = 0.03615783\n",
      "Iteration 28, loss = 0.22323500\n",
      "Iteration 29, loss = 0.22029231\n",
      "Iteration 30, loss = 0.21396173\n",
      "Iteration 31, loss = 0.20779499\n",
      "Iteration 148, loss = 0.03542214\n",
      "Iteration 32, loss = 0.20419682\n",
      "Iteration 149, loss = 0.03408146\n",
      "Iteration 33, loss = 0.20266901\n",
      "Iteration 150, loss = 0.03529468\n",
      "Iteration 34, loss = 0.19683523\n",
      "Iteration 151, loss = 0.03202217\n",
      "Iteration 35, loss = 0.19195762\n",
      "Iteration 152, loss = 0.03112681\n",
      "Iteration 153, loss = 0.03188231\n",
      "Iteration 36, loss = 0.19132413\n",
      "Iteration 154, loss = 0.03106144\n",
      "Iteration 155, loss = 0.03058536\n",
      "Iteration 156, loss = 0.02932468\n",
      "Iteration 37, loss = 0.18542288\n",
      "Iteration 38, loss = 0.18321119\n",
      "Iteration 157, loss = 0.03170747\n",
      "Iteration 39, loss = 0.17589553\n",
      "Iteration 158, loss = 0.03039980\n",
      "Iteration 159, loss = 0.03202465\n",
      "Iteration 40, loss = 0.17382991\n",
      "Iteration 160, loss = 0.03143992\n",
      "Iteration 41, loss = 0.17176855\n",
      "Iteration 161, loss = 0.02970407\n",
      "Iteration 42, loss = 0.16766310\n",
      "Iteration 162, loss = 0.03019096\n",
      "Iteration 163, loss = 0.03034364\n",
      "Iteration 164, loss = 0.02800377\n",
      "Iteration 165, loss = 0.02748612\n",
      "Iteration 166, loss = 0.02811793\n",
      "Iteration 167, loss = 0.02838424\n",
      "Iteration 168, loss = 0.02746411\n",
      "Iteration 43, loss = 0.16488834\n",
      "Iteration 169, loss = 0.02762631\n",
      "Iteration 170, loss = 0.02693847\n",
      "Iteration 44, loss = 0.16289942\n",
      "Iteration 171, loss = 0.02643703\n",
      "Iteration 45, loss = 0.16717102\n",
      "Iteration 172, loss = 0.02872297\n",
      "Iteration 46, loss = 0.16277112\n",
      "Iteration 173, loss = 0.02634855\n",
      "Iteration 47, loss = 0.15796377\n",
      "Iteration 174, loss = 0.02637466\n",
      "Iteration 48, loss = 0.15619353\n",
      "Iteration 175, loss = 0.02582948\n",
      "Iteration 49, loss = 0.15494374\n",
      "Iteration 176, loss = 0.02631559\n",
      "Iteration 50, loss = 0.15193980\n",
      "Iteration 177, loss = 0.02486517\n",
      "Iteration 178, loss = 0.02471109\n",
      "Iteration 179, loss = 0.02448578\n",
      "Iteration 180, loss = 0.02399423\n",
      "Iteration 51, loss = 0.14726915\n",
      "Iteration 52, loss = 0.14656830\n",
      "Iteration 181, loss = 0.02465549\n",
      "Iteration 182, loss = 0.02593106\n",
      "Iteration 183, loss = 0.02304270\n",
      "Iteration 53, loss = 0.13655775\n",
      "Iteration 184, loss = 0.02621634\n",
      "Iteration 54, loss = 0.13634427\n",
      "Iteration 55, loss = 0.13557551\n",
      "Iteration 185, loss = 0.02430046\n",
      "Iteration 56, loss = 0.13150745\n",
      "Iteration 186, loss = 0.02466610\n",
      "Iteration 57, loss = 0.12993235\n",
      "Iteration 187, loss = 0.02678803\n",
      "Iteration 58, loss = 0.12829875\n",
      "Iteration 188, loss = 0.02718155\n",
      "Iteration 189, loss = 0.02767311\n",
      "Iteration 59, loss = 0.12538607\n",
      "Iteration 190, loss = 0.02989377\n",
      "Iteration 60, loss = 0.12390926\n",
      "Iteration 191, loss = 0.02447025\n",
      "Iteration 61, loss = 0.12174202\n",
      "Iteration 192, loss = 0.02227601\n",
      "Iteration 62, loss = 0.12129672\n",
      "Iteration 193, loss = 0.02220618\n",
      "Iteration 194, loss = 0.02146554\n",
      "Iteration 195, loss = 0.02239217\n",
      "Iteration 196, loss = 0.02197751\n",
      "Iteration 63, loss = 0.11799130\n",
      "Iteration 64, loss = 0.11650965\n",
      "Iteration 197, loss = 0.02073495\n",
      "Iteration 198, loss = 0.02096527\n",
      "Iteration 65, loss = 0.11590820\n",
      "Iteration 199, loss = 0.02090658\n",
      "Iteration 200, loss = 0.02383514\n",
      "Iteration 66, loss = 0.11272518\n",
      "Iteration 67, loss = 0.11182381\n",
      "Iteration 68, loss = 0.11182638\n",
      "Iteration 69, loss = 0.11022291\n",
      "Iteration 70, loss = 0.10888477\n",
      "Iteration 71, loss = 0.10544091\n",
      "Iteration 72, loss = 0.10368653\n",
      "Iteration 73, loss = 0.10193926\n",
      "Iteration 74, loss = 0.10212166\n",
      "Iteration 75, loss = 0.10355727\n",
      "Iteration 76, loss = 0.09967958\n",
      "Iteration 77, loss = 0.09648812\n",
      "Iteration 78, loss = 0.10068119\n",
      "Iteration 79, loss = 0.09250858\n",
      "Iteration 80, loss = 0.09175008\n",
      "Iteration 1, loss = 1.06864352\n",
      "Iteration 81, loss = 0.09317344\n",
      "Iteration 2, loss = 0.73123964\n",
      "Iteration 82, loss = 0.09841096\n",
      "Iteration 3, loss = 0.57980406\n",
      "Iteration 83, loss = 0.09427642\n",
      "Iteration 4, loss = 0.50260497\n",
      "Iteration 84, loss = 0.09330785\n",
      "Iteration 5, loss = 0.45187211\n",
      "Iteration 85, loss = 0.09627260\n",
      "Iteration 6, loss = 0.41867326\n",
      "Iteration 86, loss = 0.09183529\n",
      "Iteration 7, loss = 0.39507873\n",
      "Iteration 87, loss = 0.08553639\n",
      "Iteration 8, loss = 0.37434217\n",
      "Iteration 88, loss = 0.08598585\n",
      "Iteration 9, loss = 0.35721639\n",
      "Iteration 89, loss = 0.08360240\n",
      "Iteration 10, loss = 0.34477505\n",
      "Iteration 90, loss = 0.08366928\n",
      "Iteration 11, loss = 0.33201986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91, loss = 0.08280582\n",
      "Iteration 12, loss = 0.31806344\n",
      "Iteration 92, loss = 0.07949893\n",
      "Iteration 13, loss = 0.30777266\n",
      "Iteration 93, loss = 0.07821928\n",
      "Iteration 94, loss = 0.07463994\n",
      "Iteration 14, loss = 0.29772976\n",
      "Iteration 95, loss = 0.07267500\n",
      "Iteration 15, loss = 0.28731259\n",
      "Iteration 96, loss = 0.08432824\n",
      "Iteration 16, loss = 0.27974219\n",
      "Iteration 97, loss = 0.08169617\n",
      "Iteration 17, loss = 0.26996548\n",
      "Iteration 98, loss = 0.07747121\n",
      "Iteration 18, loss = 0.26328178\n",
      "Iteration 99, loss = 0.07202961\n",
      "Iteration 19, loss = 0.25525572\n",
      "Iteration 100, loss = 0.07390008\n",
      "Iteration 20, loss = 0.24947831\n",
      "Iteration 101, loss = 0.06875930\n",
      "Iteration 102, loss = 0.06840767\n",
      "Iteration 21, loss = 0.24304762\n",
      "Iteration 22, loss = 0.23653669\n",
      "Iteration 23, loss = 0.23240853\n",
      "Iteration 103, loss = 0.06706297\n",
      "Iteration 24, loss = 0.22607310\n",
      "Iteration 25, loss = 0.22088631\n",
      "Iteration 104, loss = 0.07006478\n",
      "Iteration 26, loss = 0.21801898\n",
      "Iteration 105, loss = 0.06662242\n",
      "Iteration 27, loss = 0.21140092\n",
      "Iteration 106, loss = 0.06397840\n",
      "Iteration 28, loss = 0.20809373\n",
      "Iteration 107, loss = 0.06196458\n",
      "Iteration 108, loss = 0.06224168\n",
      "Iteration 109, loss = 0.06148749\n",
      "Iteration 29, loss = 0.20461927\n",
      "Iteration 110, loss = 0.06029252\n",
      "Iteration 30, loss = 0.20279199\n",
      "Iteration 111, loss = 0.06115111\n",
      "Iteration 31, loss = 0.19513302\n",
      "Iteration 32, loss = 0.19204852\n",
      "Iteration 112, loss = 0.06124235\n",
      "Iteration 113, loss = 0.06054714\n",
      "Iteration 33, loss = 0.18695123\n",
      "Iteration 114, loss = 0.05749999\n",
      "Iteration 34, loss = 0.18378404\n",
      "Iteration 115, loss = 0.05857843\n",
      "Iteration 35, loss = 0.17953977\n",
      "Iteration 36, loss = 0.17630128\n",
      "Iteration 116, loss = 0.05610799\n",
      "Iteration 117, loss = 0.05581552\n",
      "Iteration 37, loss = 0.17138637\n",
      "Iteration 38, loss = 0.16932983\n",
      "Iteration 118, loss = 0.05460043\n",
      "Iteration 39, loss = 0.16526234\n",
      "Iteration 40, loss = 0.16373269\n",
      "Iteration 119, loss = 0.05436008\n",
      "Iteration 120, loss = 0.05432472\n",
      "Iteration 121, loss = 0.05661277\n",
      "Iteration 41, loss = 0.15757208\n",
      "Iteration 42, loss = 0.15734128\n",
      "Iteration 122, loss = 0.05680285\n",
      "Iteration 43, loss = 0.15528492\n",
      "Iteration 123, loss = 0.05315818\n",
      "Iteration 44, loss = 0.14957905\n",
      "Iteration 124, loss = 0.05766736\n",
      "Iteration 45, loss = 0.14915488\n",
      "Iteration 125, loss = 0.05124634\n",
      "Iteration 46, loss = 0.14901137\n",
      "Iteration 126, loss = 0.05008207\n",
      "Iteration 47, loss = 0.14599151\n",
      "Iteration 127, loss = 0.04963316\n",
      "Iteration 48, loss = 0.14214592\n",
      "Iteration 128, loss = 0.05020581\n",
      "Iteration 49, loss = 0.13775713\n",
      "Iteration 50, loss = 0.13589217\n",
      "Iteration 129, loss = 0.05014302\n",
      "Iteration 51, loss = 0.13312832\n",
      "Iteration 52, loss = 0.13281362\n",
      "Iteration 130, loss = 0.05419562\n",
      "Iteration 131, loss = 0.05021073\n",
      "Iteration 53, loss = 0.13030553\n",
      "Iteration 132, loss = 0.04867337\n",
      "Iteration 133, loss = 0.04788166\n",
      "Iteration 134, loss = 0.05197381\n",
      "Iteration 54, loss = 0.12733426\n",
      "Iteration 135, loss = 0.04894100\n",
      "Iteration 136, loss = 0.05063515\n",
      "Iteration 137, loss = 0.04902243\n",
      "Iteration 55, loss = 0.12582576\n",
      "Iteration 138, loss = 0.04774566\n",
      "Iteration 56, loss = 0.12401265\n",
      "Iteration 57, loss = 0.12248721\n",
      "Iteration 58, loss = 0.12527357\n",
      "Iteration 139, loss = 0.04642173\n",
      "Iteration 140, loss = 0.04643951\n",
      "Iteration 59, loss = 0.11948614\n",
      "Iteration 141, loss = 0.04525899\n",
      "Iteration 60, loss = 0.11403984\n",
      "Iteration 142, loss = 0.04580041\n",
      "Iteration 61, loss = 0.11558294\n",
      "Iteration 62, loss = 0.11302186\n",
      "Iteration 143, loss = 0.04681219\n",
      "Iteration 144, loss = 0.04378681\n",
      "Iteration 63, loss = 0.11129790\n",
      "Iteration 64, loss = 0.10841503\n",
      "Iteration 145, loss = 0.04293617\n",
      "Iteration 146, loss = 0.04208150\n",
      "Iteration 65, loss = 0.10752914\n",
      "Iteration 147, loss = 0.04063169\n",
      "Iteration 66, loss = 0.10523478\n",
      "Iteration 148, loss = 0.04042493\n",
      "Iteration 67, loss = 0.10422466\n",
      "Iteration 68, loss = 0.10201827\n",
      "Iteration 69, loss = 0.10598645\n",
      "Iteration 149, loss = 0.03980840\n",
      "Iteration 70, loss = 0.10583551\n",
      "Iteration 150, loss = 0.03949425\n",
      "Iteration 71, loss = 0.10311495\n",
      "Iteration 151, loss = 0.03810958\n",
      "Iteration 152, loss = 0.03997344\n",
      "Iteration 72, loss = 0.10210980\n",
      "Iteration 153, loss = 0.03976089\n",
      "Iteration 73, loss = 0.09831100\n",
      "Iteration 74, loss = 0.09659158\n",
      "Iteration 75, loss = 0.09214348\n",
      "Iteration 154, loss = 0.04046171\n",
      "Iteration 155, loss = 0.03780849\n",
      "Iteration 156, loss = 0.03757377\n",
      "Iteration 76, loss = 0.09248505\n",
      "Iteration 157, loss = 0.03680274\n",
      "Iteration 77, loss = 0.09473228\n",
      "Iteration 78, loss = 0.09023490\n",
      "Iteration 158, loss = 0.03655864\n",
      "Iteration 79, loss = 0.08793582\n",
      "Iteration 159, loss = 0.03707583\n",
      "Iteration 80, loss = 0.08799730\n",
      "Iteration 160, loss = 0.03602160\n",
      "Iteration 161, loss = 0.03547712\n",
      "Iteration 81, loss = 0.09088345\n",
      "Iteration 82, loss = 0.08466767\n",
      "Iteration 83, loss = 0.08358857\n",
      "Iteration 162, loss = 0.03427073\n",
      "Iteration 163, loss = 0.03631883\n",
      "Iteration 164, loss = 0.03652624\n",
      "Iteration 165, loss = 0.03566478\n",
      "Iteration 84, loss = 0.08260667\n",
      "Iteration 166, loss = 0.03529341\n",
      "Iteration 85, loss = 0.08290397\n",
      "Iteration 167, loss = 0.03487587\n",
      "Iteration 86, loss = 0.08379864\n",
      "Iteration 87, loss = 0.08351348\n",
      "Iteration 88, loss = 0.08011450\n",
      "Iteration 168, loss = 0.03738794\n",
      "Iteration 89, loss = 0.07798262\n",
      "Iteration 169, loss = 0.03934508\n",
      "Iteration 90, loss = 0.07576001\n",
      "Iteration 170, loss = 0.04378031\n",
      "Iteration 91, loss = 0.07784865\n",
      "Iteration 171, loss = 0.04756812\n",
      "Iteration 92, loss = 0.07700480\n",
      "Iteration 172, loss = 0.04026545\n",
      "Iteration 93, loss = 0.07306644\n",
      "Iteration 173, loss = 0.03466936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 94, loss = 0.07049657\n",
      "Iteration 95, loss = 0.07151579\n",
      "Iteration 96, loss = 0.06891890\n",
      "Iteration 97, loss = 0.07103761\n",
      "Iteration 98, loss = 0.07004873\n",
      "Iteration 99, loss = 0.06960853\n",
      "Iteration 1, loss = 0.97058691\n",
      "Iteration 100, loss = 0.06630697\n",
      "Iteration 2, loss = 0.67507192\n",
      "Iteration 3, loss = 0.55188398\n",
      "Iteration 4, loss = 0.48918211\n",
      "Iteration 101, loss = 0.06548032\n",
      "Iteration 5, loss = 0.44810555\n",
      "Iteration 6, loss = 0.42093422\n",
      "Iteration 7, loss = 0.39737920\n",
      "Iteration 102, loss = 0.06452691\n",
      "Iteration 8, loss = 0.37916264\n",
      "Iteration 9, loss = 0.36391183\n",
      "Iteration 10, loss = 0.35175219\n",
      "Iteration 11, loss = 0.33870455\n",
      "Iteration 103, loss = 0.06540694\n",
      "Iteration 12, loss = 0.32859920\n",
      "Iteration 13, loss = 0.31668188\n",
      "Iteration 14, loss = 0.30879082\n",
      "Iteration 104, loss = 0.06515074\n",
      "Iteration 15, loss = 0.30216611\n",
      "Iteration 16, loss = 0.28890369\n",
      "Iteration 105, loss = 0.06283806\n",
      "Iteration 17, loss = 0.28561581\n",
      "Iteration 106, loss = 0.06295227\n",
      "Iteration 18, loss = 0.27652524\n",
      "Iteration 107, loss = 0.07011733\n",
      "Iteration 108, loss = 0.07150645\n",
      "Iteration 19, loss = 0.26895368\n",
      "Iteration 20, loss = 0.26137839\n",
      "Iteration 21, loss = 0.25455662\n",
      "Iteration 109, loss = 0.07004950\n",
      "Iteration 22, loss = 0.24704491\n",
      "Iteration 110, loss = 0.06039198\n",
      "Iteration 111, loss = 0.05819257\n",
      "Iteration 23, loss = 0.24175001\n",
      "Iteration 24, loss = 0.23587620\n",
      "Iteration 112, loss = 0.05599491\n",
      "Iteration 25, loss = 0.23066660\n",
      "Iteration 113, loss = 0.05611917\n",
      "Iteration 114, loss = 0.05615487\n",
      "Iteration 26, loss = 0.22715307\n",
      "Iteration 115, loss = 0.05413082\n",
      "Iteration 116, loss = 0.05462908\n",
      "Iteration 117, loss = 0.05389605\n",
      "Iteration 27, loss = 0.22006535\n",
      "Iteration 118, loss = 0.05367189\n",
      "Iteration 119, loss = 0.05361434\n",
      "Iteration 28, loss = 0.21448577\n",
      "Iteration 120, loss = 0.05145985\n",
      "Iteration 121, loss = 0.05030038\n",
      "Iteration 29, loss = 0.21168721\n",
      "Iteration 122, loss = 0.04947313\n",
      "Iteration 123, loss = 0.04919181\n",
      "Iteration 30, loss = 0.20559401\n",
      "Iteration 31, loss = 0.20184199\n",
      "Iteration 124, loss = 0.04824038\n",
      "Iteration 32, loss = 0.19760082\n",
      "Iteration 33, loss = 0.19306606\n",
      "Iteration 125, loss = 0.04737090\n",
      "Iteration 34, loss = 0.18806866\n",
      "Iteration 126, loss = 0.05135736\n",
      "Iteration 127, loss = 0.05169753\n",
      "Iteration 35, loss = 0.18530938\n",
      "Iteration 128, loss = 0.04679346\n",
      "Iteration 36, loss = 0.18091755\n",
      "Iteration 129, loss = 0.04579248\n",
      "Iteration 37, loss = 0.17747898\n",
      "Iteration 130, loss = 0.04673679\n",
      "Iteration 38, loss = 0.17238280\n",
      "Iteration 131, loss = 0.04701291\n",
      "Iteration 39, loss = 0.16872866\n",
      "Iteration 132, loss = 0.04409458\n",
      "Iteration 40, loss = 0.16596875\n",
      "Iteration 133, loss = 0.04355890\n",
      "Iteration 41, loss = 0.16304200\n",
      "Iteration 134, loss = 0.04451215\n",
      "Iteration 42, loss = 0.16187277\n",
      "Iteration 135, loss = 0.04250365\n",
      "Iteration 43, loss = 0.15903736\n",
      "Iteration 136, loss = 0.04216278\n",
      "Iteration 44, loss = 0.15523656\n",
      "Iteration 137, loss = 0.04240301\n",
      "Iteration 45, loss = 0.15213534\n",
      "Iteration 138, loss = 0.04279658\n",
      "Iteration 46, loss = 0.15634754\n",
      "Iteration 139, loss = 0.04409465\n",
      "Iteration 47, loss = 0.15307676\n",
      "Iteration 140, loss = 0.04128605\n",
      "Iteration 48, loss = 0.14269030\n",
      "Iteration 141, loss = 0.04112931\n",
      "Iteration 49, loss = 0.14208027\n",
      "Iteration 142, loss = 0.04003337\n",
      "Iteration 50, loss = 0.13823058\n",
      "Iteration 143, loss = 0.04167383\n",
      "Iteration 51, loss = 0.13577155\n",
      "Iteration 144, loss = 0.04051280\n",
      "Iteration 52, loss = 0.13336347\n",
      "Iteration 145, loss = 0.03930269\n",
      "Iteration 53, loss = 0.13054693\n",
      "Iteration 146, loss = 0.03983616\n",
      "Iteration 54, loss = 0.12620459\n",
      "Iteration 55, loss = 0.12341831\n",
      "Iteration 147, loss = 0.03956207\n",
      "Iteration 56, loss = 0.12478602\n",
      "Iteration 148, loss = 0.04045375\n",
      "Iteration 57, loss = 0.12683703\n",
      "Iteration 149, loss = 0.03755534\n",
      "Iteration 150, loss = 0.03889267\n",
      "Iteration 58, loss = 0.12471994\n",
      "Iteration 59, loss = 0.11734139\n",
      "Iteration 151, loss = 0.03732450\n",
      "Iteration 60, loss = 0.11348964\n",
      "Iteration 152, loss = 0.03722227\n",
      "Iteration 61, loss = 0.11351668\n",
      "Iteration 153, loss = 0.03869981\n",
      "Iteration 62, loss = 0.11078634\n",
      "Iteration 154, loss = 0.04277418\n",
      "Iteration 63, loss = 0.10834143\n",
      "Iteration 64, loss = 0.10680324\n",
      "Iteration 65, loss = 0.10488280\n",
      "Iteration 155, loss = 0.04092284\n",
      "Iteration 66, loss = 0.10146777\n",
      "Iteration 67, loss = 0.10034498\n",
      "Iteration 156, loss = 0.04569409\n",
      "Iteration 68, loss = 0.09835689\n",
      "Iteration 157, loss = 0.03850375\n",
      "Iteration 158, loss = 0.04501672\n",
      "Iteration 69, loss = 0.09609128\n",
      "Iteration 159, loss = 0.04257209\n",
      "Iteration 70, loss = 0.09514521\n",
      "Iteration 71, loss = 0.09331081\n",
      "Iteration 160, loss = 0.03684537\n",
      "Iteration 72, loss = 0.09205159\n",
      "Iteration 161, loss = 0.03893101\n",
      "Iteration 73, loss = 0.09026003\n",
      "Iteration 74, loss = 0.08951368\n",
      "Iteration 162, loss = 0.03612055\n",
      "Iteration 75, loss = 0.08714921\n",
      "Iteration 163, loss = 0.03869032\n",
      "Iteration 76, loss = 0.08572942\n",
      "Iteration 77, loss = 0.08857785\n",
      "Iteration 78, loss = 0.08567701\n",
      "Iteration 164, loss = 0.03822762\n",
      "Iteration 79, loss = 0.08087055\n",
      "Iteration 165, loss = 0.03237599\n",
      "Iteration 80, loss = 0.07833670\n",
      "Iteration 166, loss = 0.03312783\n",
      "Iteration 167, loss = 0.03210939\n",
      "Iteration 81, loss = 0.08046535\n",
      "Iteration 82, loss = 0.07889743\n",
      "Iteration 83, loss = 0.07845124\n",
      "Iteration 168, loss = 0.03422172\n",
      "Iteration 84, loss = 0.08062603\n",
      "Iteration 169, loss = 0.03376469\n",
      "Iteration 170, loss = 0.03428562\n",
      "Iteration 171, loss = 0.03497061\n",
      "Iteration 85, loss = 0.08207668\n",
      "Iteration 86, loss = 0.07548478\n",
      "Iteration 87, loss = 0.07239035\n",
      "Iteration 172, loss = 0.03249160\n",
      "Iteration 88, loss = 0.07003126\n",
      "Iteration 173, loss = 0.03597309\n",
      "Iteration 174, loss = 0.04006549\n",
      "Iteration 89, loss = 0.06945514\n",
      "Iteration 175, loss = 0.03766247\n",
      "Iteration 90, loss = 0.06882195\n",
      "Iteration 176, loss = 0.03554630\n",
      "Iteration 91, loss = 0.06569027\n",
      "Iteration 92, loss = 0.07004513\n",
      "Iteration 177, loss = 0.03417189\n",
      "Iteration 178, loss = 0.03325748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 93, loss = 0.06745355\n",
      "Iteration 94, loss = 0.06304595\n",
      "Iteration 95, loss = 0.06746182\n",
      "Iteration 96, loss = 0.07358816\n",
      "Iteration 97, loss = 0.06201518\n",
      "Iteration 98, loss = 0.05728862\n",
      "Iteration 99, loss = 0.05848712\n",
      "Iteration 100, loss = 0.06226022\n",
      "Iteration 101, loss = 0.05818188\n",
      "Iteration 102, loss = 0.05486243\n",
      "Iteration 103, loss = 0.05639874\n",
      "Iteration 104, loss = 0.05609151\n",
      "Iteration 105, loss = 0.05661990\n",
      "Iteration 106, loss = 0.05509184\n",
      "Iteration 107, loss = 0.05213123\n",
      "Iteration 108, loss = 0.04803717\n",
      "Iteration 109, loss = 0.04799096\n",
      "Iteration 110, loss = 0.04989580\n",
      "Iteration 1, loss = 1.33241254\n",
      "Iteration 111, loss = 0.05109481\n",
      "Iteration 112, loss = 0.05350386\n",
      "Iteration 113, loss = 0.05084034\n",
      "Iteration 2, loss = 0.83652003\n",
      "Iteration 114, loss = 0.04736985\n",
      "Iteration 3, loss = 0.65401332\n",
      "Iteration 115, loss = 0.04341439\n",
      "Iteration 4, loss = 0.55680793\n",
      "Iteration 5, loss = 0.49902674\n",
      "Iteration 116, loss = 0.04372339\n",
      "Iteration 6, loss = 0.45699937\n",
      "Iteration 7, loss = 0.43177987\n",
      "Iteration 117, loss = 0.04598387\n",
      "Iteration 118, loss = 0.04423799\n",
      "Iteration 8, loss = 0.40727359\n",
      "Iteration 119, loss = 0.04245038\n",
      "Iteration 120, loss = 0.04184040\n",
      "Iteration 121, loss = 0.04091666\n",
      "Iteration 9, loss = 0.38971331\n",
      "Iteration 122, loss = 0.04002879\n",
      "Iteration 10, loss = 0.37363469\n",
      "Iteration 123, loss = 0.03873856\n",
      "Iteration 11, loss = 0.36049201\n",
      "Iteration 12, loss = 0.34910798\n",
      "Iteration 13, loss = 0.33879099\n",
      "Iteration 124, loss = 0.03794780\n",
      "Iteration 14, loss = 0.32977859\n",
      "Iteration 125, loss = 0.03849498\n",
      "Iteration 15, loss = 0.32014945\n",
      "Iteration 126, loss = 0.03703339\n",
      "Iteration 16, loss = 0.31304266\n",
      "Iteration 127, loss = 0.03785635\n",
      "Iteration 17, loss = 0.30419552\n",
      "Iteration 128, loss = 0.03595863\n",
      "Iteration 129, loss = 0.03576752\n",
      "Iteration 18, loss = 0.29694502\n",
      "Iteration 130, loss = 0.03446177\n",
      "Iteration 131, loss = 0.03415224\n",
      "Iteration 19, loss = 0.28983284\n",
      "Iteration 132, loss = 0.03310478\n",
      "Iteration 20, loss = 0.28338811\n",
      "Iteration 21, loss = 0.27625855\n",
      "Iteration 133, loss = 0.03313665\n",
      "Iteration 22, loss = 0.27101566\n",
      "Iteration 134, loss = 0.03609230\n",
      "Iteration 23, loss = 0.26434588\n",
      "Iteration 24, loss = 0.25959373\n",
      "Iteration 135, loss = 0.03357577\n",
      "Iteration 25, loss = 0.25438694\n",
      "Iteration 136, loss = 0.03617413\n",
      "Iteration 137, loss = 0.03542261\n",
      "Iteration 138, loss = 0.03187562\n",
      "Iteration 26, loss = 0.24791892\n",
      "Iteration 139, loss = 0.03063507\n",
      "Iteration 27, loss = 0.24255798\n",
      "Iteration 28, loss = 0.23600017\n",
      "Iteration 140, loss = 0.03021805\n",
      "Iteration 29, loss = 0.23325599\n",
      "Iteration 141, loss = 0.03064278\n",
      "Iteration 142, loss = 0.03268208\n",
      "Iteration 143, loss = 0.03098245\n",
      "Iteration 30, loss = 0.22686128\n",
      "Iteration 144, loss = 0.02850806\n",
      "Iteration 31, loss = 0.22248905\n",
      "Iteration 145, loss = 0.02828602\n",
      "Iteration 32, loss = 0.21826022\n",
      "Iteration 146, loss = 0.02767495\n",
      "Iteration 33, loss = 0.21393041\n",
      "Iteration 147, loss = 0.02956011\n",
      "Iteration 148, loss = 0.02718719\n",
      "Iteration 34, loss = 0.21461364\n",
      "Iteration 149, loss = 0.02715351\n",
      "Iteration 35, loss = 0.20765897\n",
      "Iteration 150, loss = 0.02899343\n",
      "Iteration 36, loss = 0.20139871\n",
      "Iteration 151, loss = 0.03213775\n",
      "Iteration 152, loss = 0.03621248\n",
      "Iteration 37, loss = 0.19944716\n",
      "Iteration 153, loss = 0.04014420\n",
      "Iteration 38, loss = 0.19473243\n",
      "Iteration 154, loss = 0.03868086\n",
      "Iteration 39, loss = 0.19196665\n",
      "Iteration 40, loss = 0.18701746\n",
      "Iteration 155, loss = 0.03069758\n",
      "Iteration 41, loss = 0.18391434\n",
      "Iteration 156, loss = 0.02813661\n",
      "Iteration 42, loss = 0.17940312\n",
      "Iteration 157, loss = 0.02744358\n",
      "Iteration 43, loss = 0.17576476\n",
      "Iteration 158, loss = 0.02343824\n",
      "Iteration 44, loss = 0.17192105\n",
      "Iteration 159, loss = 0.02374904\n",
      "Iteration 160, loss = 0.02326414\n",
      "Iteration 45, loss = 0.16949699\n",
      "Iteration 161, loss = 0.02260504\n",
      "Iteration 46, loss = 0.16584236\n",
      "Iteration 162, loss = 0.02366741\n",
      "Iteration 47, loss = 0.16593222\n",
      "Iteration 163, loss = 0.02208200\n",
      "Iteration 48, loss = 0.16087461\n",
      "Iteration 164, loss = 0.02162068\n",
      "Iteration 165, loss = 0.02185362\n",
      "Iteration 49, loss = 0.15836534\n",
      "Iteration 50, loss = 0.15713250\n",
      "Iteration 166, loss = 0.02131106\n",
      "Iteration 51, loss = 0.15582742\n",
      "Iteration 167, loss = 0.02057997\n",
      "Iteration 52, loss = 0.15296177\n",
      "Iteration 168, loss = 0.02068983\n",
      "Iteration 169, loss = 0.02022929\n",
      "Iteration 53, loss = 0.14692156\n",
      "Iteration 54, loss = 0.14659106\n",
      "Iteration 170, loss = 0.02018995\n",
      "Iteration 55, loss = 0.14475891\n",
      "Iteration 56, loss = 0.14512616\n",
      "Iteration 171, loss = 0.02022119\n",
      "Iteration 172, loss = 0.02010489\n",
      "Iteration 57, loss = 0.14150045\n",
      "Iteration 173, loss = 0.01997479\n",
      "Iteration 174, loss = 0.02056435\n",
      "Iteration 58, loss = 0.13849504\n",
      "Iteration 59, loss = 0.13879746\n",
      "Iteration 60, loss = 0.13672642\n",
      "Iteration 175, loss = 0.02249931\n",
      "Iteration 176, loss = 0.02366191\n",
      "Iteration 61, loss = 0.13196196\n",
      "Iteration 177, loss = 0.01995506\n",
      "Iteration 178, loss = 0.02011112\n",
      "Iteration 62, loss = 0.13005227\n",
      "Iteration 63, loss = 0.12893034\n",
      "Iteration 179, loss = 0.01878013\n",
      "Iteration 64, loss = 0.12816593\n",
      "Iteration 180, loss = 0.02016802\n",
      "Iteration 181, loss = 0.01858786\n",
      "Iteration 65, loss = 0.12814037\n",
      "Iteration 182, loss = 0.02109659\n",
      "Iteration 66, loss = 0.12363131\n",
      "Iteration 67, loss = 0.12414105\n",
      "Iteration 183, loss = 0.02070460\n",
      "Iteration 68, loss = 0.12292671\n",
      "Iteration 184, loss = 0.01996161\n",
      "Iteration 69, loss = 0.11805016\n",
      "Iteration 70, loss = 0.11962154\n",
      "Iteration 185, loss = 0.01769518\n",
      "Iteration 71, loss = 0.11701969\n",
      "Iteration 186, loss = 0.01840719\n",
      "Iteration 187, loss = 0.01727097\n",
      "Iteration 188, loss = 0.01865086\n",
      "Iteration 72, loss = 0.11532735\n",
      "Iteration 73, loss = 0.11459983\n",
      "Iteration 189, loss = 0.01702890\n",
      "Iteration 74, loss = 0.11247416\n",
      "Iteration 75, loss = 0.11096503\n",
      "Iteration 190, loss = 0.01653062\n",
      "Iteration 191, loss = 0.01570914\n",
      "Iteration 76, loss = 0.11053114\n",
      "Iteration 192, loss = 0.01548025\n",
      "Iteration 193, loss = 0.01537708\n",
      "Iteration 194, loss = 0.01529517\n",
      "Iteration 77, loss = 0.11055870\n",
      "Iteration 195, loss = 0.01523524\n",
      "Iteration 78, loss = 0.10701351\n",
      "Iteration 196, loss = 0.01547790\n",
      "Iteration 79, loss = 0.10660967\n",
      "Iteration 80, loss = 0.10653329\n",
      "Iteration 197, loss = 0.01458862\n",
      "Iteration 198, loss = 0.01493851\n",
      "Iteration 81, loss = 0.10560256\n",
      "Iteration 199, loss = 0.01729837\n",
      "Iteration 82, loss = 0.10543237\n",
      "Iteration 200, loss = 0.01774516\n",
      "Iteration 83, loss = 0.10107879\n",
      "Iteration 84, loss = 0.10265922\n",
      "Iteration 85, loss = 0.09889150\n",
      "Iteration 86, loss = 0.09796937\n",
      "Iteration 87, loss = 0.10012982\n",
      "Iteration 88, loss = 0.09971224\n",
      "Iteration 89, loss = 0.09661259\n",
      "Iteration 90, loss = 0.09673683\n",
      "Iteration 91, loss = 0.09434596\n",
      "Iteration 92, loss = 0.09327270\n",
      "Iteration 93, loss = 0.09433579\n",
      "Iteration 1, loss = 1.12927946\n",
      "Iteration 2, loss = 0.78839525\n",
      "Iteration 94, loss = 0.09096693\n",
      "Iteration 95, loss = 0.08935504\n",
      "Iteration 3, loss = 0.60051537\n",
      "Iteration 96, loss = 0.08752703\n",
      "Iteration 97, loss = 0.08885392\n",
      "Iteration 4, loss = 0.52484701\n",
      "Iteration 5, loss = 0.46773298\n",
      "Iteration 6, loss = 0.43397903\n",
      "Iteration 98, loss = 0.08620381\n",
      "Iteration 99, loss = 0.08504714\n",
      "Iteration 7, loss = 0.40579699\n",
      "Iteration 100, loss = 0.08971722\n",
      "Iteration 101, loss = 0.09272424\n",
      "Iteration 8, loss = 0.38562526\n",
      "Iteration 9, loss = 0.36820773\n",
      "Iteration 10, loss = 0.35633024\n",
      "Iteration 102, loss = 0.08968068\n",
      "Iteration 103, loss = 0.09004008\n",
      "Iteration 11, loss = 0.34346524\n",
      "Iteration 104, loss = 0.09709451\n",
      "Iteration 105, loss = 0.08677499\n",
      "Iteration 106, loss = 0.08448212\n",
      "Iteration 12, loss = 0.33529895\n",
      "Iteration 107, loss = 0.08028199\n",
      "Iteration 108, loss = 0.07782040\n",
      "Iteration 13, loss = 0.32385680\n",
      "Iteration 14, loss = 0.31346406\n",
      "Iteration 15, loss = 0.30618589\n",
      "Iteration 109, loss = 0.07770163\n",
      "Iteration 110, loss = 0.07604416\n",
      "Iteration 16, loss = 0.29759608\n",
      "Iteration 111, loss = 0.07937462\n",
      "Iteration 112, loss = 0.07400435\n",
      "Iteration 113, loss = 0.07380085\n",
      "Iteration 17, loss = 0.29037089\n",
      "Iteration 18, loss = 0.28267147\n",
      "Iteration 19, loss = 0.27673895\n",
      "Iteration 114, loss = 0.07286499\n",
      "Iteration 20, loss = 0.26998273\n",
      "Iteration 115, loss = 0.07296677\n",
      "Iteration 21, loss = 0.26276622\n",
      "Iteration 116, loss = 0.07802308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.25717668\n",
      "Iteration 117, loss = 0.07850997\n",
      "Iteration 23, loss = 0.25121698\n",
      "Iteration 24, loss = 0.24553306\n",
      "Iteration 118, loss = 0.08655986\n",
      "Iteration 25, loss = 0.23946887\n",
      "Iteration 119, loss = 0.07509938\n",
      "Iteration 26, loss = 0.23301492\n",
      "Iteration 120, loss = 0.06871382\n",
      "Iteration 27, loss = 0.22849046\n",
      "Iteration 121, loss = 0.06950374\n",
      "Iteration 28, loss = 0.22429531\n",
      "Iteration 122, loss = 0.06698761\n",
      "Iteration 29, loss = 0.21944454\n",
      "Iteration 123, loss = 0.06793305\n",
      "Iteration 30, loss = 0.21561938\n",
      "Iteration 124, loss = 0.06407093\n",
      "Iteration 31, loss = 0.21035953\n",
      "Iteration 125, loss = 0.06389571\n",
      "Iteration 32, loss = 0.20398736\n",
      "Iteration 33, loss = 0.20215448\n",
      "Iteration 34, loss = 0.20068701\n",
      "Iteration 126, loss = 0.06658345\n",
      "Iteration 127, loss = 0.07516970\n",
      "Iteration 128, loss = 0.07589114\n",
      "Iteration 35, loss = 0.19219027\n",
      "Iteration 129, loss = 0.08102172\n",
      "Iteration 36, loss = 0.18871810\n",
      "Iteration 37, loss = 0.18499355\n",
      "Iteration 38, loss = 0.18593132\n",
      "Iteration 130, loss = 0.07141691\n",
      "Iteration 39, loss = 0.17737492\n",
      "Iteration 131, loss = 0.06700027\n",
      "Iteration 40, loss = 0.17609851\n",
      "Iteration 132, loss = 0.06453176\n",
      "Iteration 133, loss = 0.06173400\n",
      "Iteration 41, loss = 0.17179825\n",
      "Iteration 134, loss = 0.06007046\n",
      "Iteration 135, loss = 0.06369366\n",
      "Iteration 42, loss = 0.16903165\n",
      "Iteration 43, loss = 0.16510270\n",
      "Iteration 44, loss = 0.16263504\n",
      "Iteration 136, loss = 0.06295643\n",
      "Iteration 45, loss = 0.15939567\n",
      "Iteration 137, loss = 0.05907993\n",
      "Iteration 46, loss = 0.15607447\n",
      "Iteration 138, loss = 0.05789053\n",
      "Iteration 47, loss = 0.15416788\n",
      "Iteration 139, loss = 0.05572776\n",
      "Iteration 48, loss = 0.14935975\n",
      "Iteration 49, loss = 0.14750024\n",
      "Iteration 140, loss = 0.06096027\n",
      "Iteration 50, loss = 0.14614630\n",
      "Iteration 51, loss = 0.14159019\n",
      "Iteration 141, loss = 0.05838615\n",
      "Iteration 52, loss = 0.14009532\n",
      "Iteration 53, loss = 0.13900726\n",
      "Iteration 142, loss = 0.05636033\n",
      "Iteration 54, loss = 0.13472375\n",
      "Iteration 143, loss = 0.05411913\n",
      "Iteration 144, loss = 0.05682402\n",
      "Iteration 55, loss = 0.13239788\n",
      "Iteration 56, loss = 0.13130040\n",
      "Iteration 145, loss = 0.05470077\n",
      "Iteration 57, loss = 0.13155562\n",
      "Iteration 146, loss = 0.05259611\n",
      "Iteration 58, loss = 0.12702952\n",
      "Iteration 59, loss = 0.12370504\n",
      "Iteration 147, loss = 0.05174690\n",
      "Iteration 60, loss = 0.12525288\n",
      "Iteration 61, loss = 0.12445483\n",
      "Iteration 62, loss = 0.12079209\n",
      "Iteration 148, loss = 0.05084912\n",
      "Iteration 149, loss = 0.05114620\n",
      "Iteration 150, loss = 0.05112396\n",
      "Iteration 63, loss = 0.11978321\n",
      "Iteration 151, loss = 0.04920140\n",
      "Iteration 64, loss = 0.11557917\n",
      "Iteration 65, loss = 0.11461492\n",
      "Iteration 66, loss = 0.11759629\n",
      "Iteration 152, loss = 0.04963577\n",
      "Iteration 67, loss = 0.11022451Iteration 153, loss = 0.04835016\n",
      "\n",
      "Iteration 154, loss = 0.04787710\n",
      "Iteration 155, loss = 0.04831988\n",
      "Iteration 68, loss = 0.10984571\n",
      "Iteration 69, loss = 0.10918023\n",
      "Iteration 70, loss = 0.10703166\n",
      "Iteration 156, loss = 0.04688294\n",
      "Iteration 71, loss = 0.10486207\n",
      "Iteration 72, loss = 0.10330014\n",
      "Iteration 157, loss = 0.04650643\n",
      "Iteration 73, loss = 0.10281254\n",
      "Iteration 74, loss = 0.09958451\n",
      "Iteration 158, loss = 0.04681033\n",
      "Iteration 75, loss = 0.09992192\n",
      "Iteration 159, loss = 0.04690168\n",
      "Iteration 160, loss = 0.04915189\n",
      "Iteration 76, loss = 0.09866015\n",
      "Iteration 161, loss = 0.04918396\n",
      "Iteration 77, loss = 0.09695826\n",
      "Iteration 78, loss = 0.09584988\n",
      "Iteration 79, loss = 0.09401145\n",
      "Iteration 162, loss = 0.04705415\n",
      "Iteration 80, loss = 0.09553320\n",
      "Iteration 163, loss = 0.04964769\n",
      "Iteration 164, loss = 0.05292624\n",
      "Iteration 81, loss = 0.09004589\n",
      "Iteration 165, loss = 0.04885362\n",
      "Iteration 82, loss = 0.09041699\n",
      "Iteration 83, loss = 0.09882109\n",
      "Iteration 166, loss = 0.04523949\n",
      "Iteration 84, loss = 0.10052942\n",
      "Iteration 167, loss = 0.04689918\n",
      "Iteration 168, loss = 0.04874374\n",
      "Iteration 85, loss = 0.09987316\n",
      "Iteration 169, loss = 0.04376716\n",
      "Iteration 86, loss = 0.09174363\n",
      "Iteration 170, loss = 0.04198806\n",
      "Iteration 87, loss = 0.08716619\n",
      "Iteration 171, loss = 0.04299522\n",
      "Iteration 88, loss = 0.08728583\n",
      "Iteration 172, loss = 0.04251313\n",
      "Iteration 173, loss = 0.04020682\n",
      "Iteration 89, loss = 0.08652063\n",
      "Iteration 174, loss = 0.04333524\n",
      "Iteration 175, loss = 0.04371307\n",
      "Iteration 90, loss = 0.09138551\n",
      "Iteration 176, loss = 0.03872994\n",
      "Iteration 177, loss = 0.04393304\n",
      "Iteration 91, loss = 0.09606355\n",
      "Iteration 178, loss = 0.05574303\n",
      "Iteration 179, loss = 0.04916767\n",
      "Iteration 92, loss = 0.08787971\n",
      "Iteration 180, loss = 0.05116109\n",
      "Iteration 93, loss = 0.07803210\n",
      "Iteration 181, loss = 0.04082894\n",
      "Iteration 182, loss = 0.04015770\n",
      "Iteration 94, loss = 0.07982171\n",
      "Iteration 95, loss = 0.07758040\n",
      "Iteration 183, loss = 0.04069982\n",
      "Iteration 96, loss = 0.07477717\n",
      "Iteration 97, loss = 0.07391253\n",
      "Iteration 184, loss = 0.03605395\n",
      "Iteration 98, loss = 0.07742657\n",
      "Iteration 185, loss = 0.03811628\n",
      "Iteration 99, loss = 0.07236830\n",
      "Iteration 186, loss = 0.03820466\n",
      "Iteration 100, loss = 0.07058778\n",
      "Iteration 101, loss = 0.07310742\n",
      "Iteration 187, loss = 0.04115892\n",
      "Iteration 102, loss = 0.06957493\n",
      "Iteration 103, loss = 0.06782109\n",
      "Iteration 188, loss = 0.04103004\n",
      "Iteration 104, loss = 0.06759419\n",
      "Iteration 105, loss = 0.06693995\n",
      "Iteration 106, loss = 0.06552075\n",
      "Iteration 189, loss = 0.03863631\n",
      "Iteration 107, loss = 0.06533628\n",
      "Iteration 108, loss = 0.06680333\n",
      "Iteration 190, loss = 0.04251701\n",
      "Iteration 109, loss = 0.06455393\n",
      "Iteration 191, loss = 0.03784856\n",
      "Iteration 192, loss = 0.03628509\n",
      "Iteration 110, loss = 0.06226390\n",
      "Iteration 193, loss = 0.03438430\n",
      "Iteration 111, loss = 0.06565476\n",
      "Iteration 112, loss = 0.06406882\n",
      "Iteration 194, loss = 0.03658914\n",
      "Iteration 113, loss = 0.06168853\n",
      "Iteration 195, loss = 0.03722961\n",
      "Iteration 114, loss = 0.05953129\n",
      "Iteration 196, loss = 0.03457360\n",
      "Iteration 197, loss = 0.03600756\n",
      "Iteration 198, loss = 0.03572199\n",
      "Iteration 115, loss = 0.05858556\n",
      "Iteration 199, loss = 0.03477367\n",
      "Iteration 116, loss = 0.05835099\n",
      "Iteration 117, loss = 0.05709557\n",
      "Iteration 200, loss = 0.04028510\n",
      "Iteration 118, loss = 0.05620295\n",
      "Iteration 119, loss = 0.05737748\n",
      "Iteration 120, loss = 0.05483819\n",
      "Iteration 121, loss = 0.05510864\n",
      "Iteration 122, loss = 0.05360740\n",
      "Iteration 123, loss = 0.05444171\n",
      "Iteration 124, loss = 0.05211728\n",
      "Iteration 1, loss = 1.03096269\n",
      "Iteration 2, loss = 0.69034945\n",
      "Iteration 3, loss = 0.55942788\n",
      "Iteration 4, loss = 0.48672892\n",
      "Iteration 125, loss = 0.05202741\n",
      "Iteration 5, loss = 0.44400524\n",
      "Iteration 126, loss = 0.05308769\n",
      "Iteration 6, loss = 0.41400410\n",
      "Iteration 7, loss = 0.39238067\n",
      "Iteration 127, loss = 0.05142746\n",
      "Iteration 128, loss = 0.05137534\n",
      "Iteration 8, loss = 0.37586967\n",
      "Iteration 129, loss = 0.05176686\n",
      "Iteration 9, loss = 0.36128262\n",
      "Iteration 130, loss = 0.05329028\n",
      "Iteration 10, loss = 0.34909370\n",
      "Iteration 11, loss = 0.33820831\n",
      "Iteration 131, loss = 0.04847576\n",
      "Iteration 132, loss = 0.04942397\n",
      "Iteration 12, loss = 0.32954667\n",
      "Iteration 133, loss = 0.04859840\n",
      "Iteration 134, loss = 0.04777600\n",
      "Iteration 13, loss = 0.31753532\n",
      "Iteration 14, loss = 0.30912898\n",
      "Iteration 135, loss = 0.04732732\n",
      "Iteration 15, loss = 0.30035499\n",
      "Iteration 136, loss = 0.04672970\n",
      "Iteration 16, loss = 0.29372401\n",
      "Iteration 137, loss = 0.04608764\n",
      "Iteration 17, loss = 0.28641663\n",
      "Iteration 138, loss = 0.04757225\n",
      "Iteration 139, loss = 0.05328847\n",
      "Iteration 18, loss = 0.27786757\n",
      "Iteration 140, loss = 0.04694336\n",
      "Iteration 19, loss = 0.27130905\n",
      "Iteration 141, loss = 0.04603510\n",
      "Iteration 142, loss = 0.05222683\n",
      "Iteration 143, loss = 0.04732974\n",
      "Iteration 20, loss = 0.26401430\n",
      "Iteration 144, loss = 0.04830972\n",
      "Iteration 21, loss = 0.25871534\n",
      "Iteration 145, loss = 0.05294468\n",
      "Iteration 22, loss = 0.25282698\n",
      "Iteration 146, loss = 0.04835926\n",
      "Iteration 23, loss = 0.24415355\n",
      "Iteration 147, loss = 0.04388692\n",
      "Iteration 24, loss = 0.23962351\n",
      "Iteration 148, loss = 0.04612982\n",
      "Iteration 25, loss = 0.23481993\n",
      "Iteration 149, loss = 0.04592141\n",
      "Iteration 26, loss = 0.22805010\n",
      "Iteration 150, loss = 0.04652039\n",
      "Iteration 27, loss = 0.22236671\n",
      "Iteration 151, loss = 0.04432512\n",
      "Iteration 28, loss = 0.21885713\n",
      "Iteration 152, loss = 0.04002509\n",
      "Iteration 29, loss = 0.21712405\n",
      "Iteration 30, loss = 0.20912274\n",
      "Iteration 153, loss = 0.04010010\n",
      "Iteration 31, loss = 0.20578224\n",
      "Iteration 154, loss = 0.04190066\n",
      "Iteration 32, loss = 0.20299154\n",
      "Iteration 33, loss = 0.19381070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 155, loss = 0.03893867\n",
      "Iteration 34, loss = 0.19108671\n",
      "Iteration 156, loss = 0.03831919\n",
      "Iteration 157, loss = 0.03990350\n",
      "Iteration 35, loss = 0.18791312\n",
      "Iteration 158, loss = 0.03675742\n",
      "Iteration 36, loss = 0.18179918\n",
      "Iteration 159, loss = 0.03599247\n",
      "Iteration 37, loss = 0.18103905\n",
      "Iteration 38, loss = 0.17717314\n",
      "Iteration 39, loss = 0.17546293\n",
      "Iteration 160, loss = 0.03535039\n",
      "Iteration 161, loss = 0.03501559\n",
      "Iteration 40, loss = 0.16847029\n",
      "Iteration 41, loss = 0.16590334\n",
      "Iteration 162, loss = 0.03553140\n",
      "Iteration 42, loss = 0.16179691\n",
      "Iteration 163, loss = 0.03540265\n",
      "Iteration 164, loss = 0.03451955\n",
      "Iteration 43, loss = 0.15874799\n",
      "Iteration 165, loss = 0.03364474\n",
      "Iteration 44, loss = 0.15515188\n",
      "Iteration 166, loss = 0.03534504\n",
      "Iteration 45, loss = 0.15195275\n",
      "Iteration 167, loss = 0.03405153\n",
      "Iteration 46, loss = 0.15213166\n",
      "Iteration 47, loss = 0.14906999\n",
      "Iteration 168, loss = 0.03367324\n",
      "Iteration 48, loss = 0.14618910\n",
      "Iteration 169, loss = 0.03388031\n",
      "Iteration 49, loss = 0.14245990\n",
      "Iteration 170, loss = 0.03618638\n",
      "Iteration 171, loss = 0.03179493\n",
      "Iteration 50, loss = 0.13909175\n",
      "Iteration 172, loss = 0.03352248\n",
      "Iteration 51, loss = 0.13710870\n",
      "Iteration 173, loss = 0.03268827\n",
      "Iteration 52, loss = 0.13518534\n",
      "Iteration 174, loss = 0.03258614\n",
      "Iteration 53, loss = 0.13461427\n",
      "Iteration 175, loss = 0.03109106\n",
      "Iteration 176, loss = 0.03320197\n",
      "Iteration 54, loss = 0.13224941\n",
      "Iteration 177, loss = 0.03127409\n",
      "Iteration 178, loss = 0.03067428\n",
      "Iteration 55, loss = 0.12841156\n",
      "Iteration 56, loss = 0.12970484\n",
      "Iteration 179, loss = 0.02941620\n",
      "Iteration 57, loss = 0.12513653\n",
      "Iteration 180, loss = 0.03052382\n",
      "Iteration 58, loss = 0.12460100\n",
      "Iteration 181, loss = 0.02948142\n",
      "Iteration 59, loss = 0.12279311\n",
      "Iteration 60, loss = 0.11903830\n",
      "Iteration 182, loss = 0.03492122\n",
      "Iteration 183, loss = 0.03153447\n",
      "Iteration 61, loss = 0.11908090\n",
      "Iteration 184, loss = 0.03209805\n",
      "Iteration 185, loss = 0.02827474\n",
      "Iteration 62, loss = 0.12066305\n",
      "Iteration 186, loss = 0.02785214\n",
      "Iteration 63, loss = 0.12207085\n",
      "Iteration 187, loss = 0.02670568\n",
      "Iteration 64, loss = 0.12001478\n",
      "Iteration 188, loss = 0.02746708\n",
      "Iteration 65, loss = 0.11841876\n",
      "Iteration 189, loss = 0.02818503\n",
      "Iteration 66, loss = 0.11311502\n",
      "Iteration 190, loss = 0.02842365\n",
      "Iteration 67, loss = 0.10955248\n",
      "Iteration 191, loss = 0.02680464\n",
      "Iteration 68, loss = 0.10850880\n",
      "Iteration 192, loss = 0.02685593\n",
      "Iteration 69, loss = 0.10548636\n",
      "Iteration 193, loss = 0.02965788\n",
      "Iteration 70, loss = 0.10392431\n",
      "Iteration 194, loss = 0.02816416\n",
      "Iteration 71, loss = 0.10160455\n",
      "Iteration 195, loss = 0.02629112\n",
      "Iteration 72, loss = 0.10038480\n",
      "Iteration 196, loss = 0.02686151\n",
      "Iteration 73, loss = 0.10308749\n",
      "Iteration 197, loss = 0.02513761\n",
      "Iteration 74, loss = 0.09798639\n",
      "Iteration 198, loss = 0.02511394\n",
      "Iteration 75, loss = 0.09677753\n",
      "Iteration 199, loss = 0.02503380\n",
      "Iteration 76, loss = 0.09468786\n",
      "Iteration 200, loss = 0.02405856\n",
      "Iteration 77, loss = 0.09366527\n",
      "Iteration 78, loss = 0.09407849\n",
      "Iteration 79, loss = 0.09580783\n",
      "Iteration 80, loss = 0.08866733\n",
      "Iteration 81, loss = 0.08702498\n",
      "Iteration 82, loss = 0.09242195\n",
      "Iteration 83, loss = 0.08857475\n",
      "Iteration 84, loss = 0.08856435\n",
      "Iteration 85, loss = 0.08676223\n",
      "Iteration 86, loss = 0.08849977\n",
      "Iteration 1, loss = 0.98414867\n",
      "Iteration 87, loss = 0.08470204\n",
      "Iteration 2, loss = 0.71403633\n",
      "Iteration 88, loss = 0.08191790\n",
      "Iteration 3, loss = 0.57907758\n",
      "Iteration 89, loss = 0.08471976\n",
      "Iteration 4, loss = 0.50959143\n",
      "Iteration 5, loss = 0.45855215\n",
      "Iteration 6, loss = 0.42575397\n",
      "Iteration 90, loss = 0.08024393\n",
      "Iteration 91, loss = 0.07595468\n",
      "Iteration 92, loss = 0.07664225\n",
      "Iteration 7, loss = 0.40260043\n",
      "Iteration 93, loss = 0.07390844\n",
      "Iteration 94, loss = 0.07883152\n",
      "Iteration 8, loss = 0.38171409\n",
      "Iteration 9, loss = 0.36594853\n",
      "Iteration 10, loss = 0.35133327\n",
      "Iteration 95, loss = 0.08176232\n",
      "Iteration 96, loss = 0.08976948\n",
      "Iteration 11, loss = 0.33845717\n",
      "Iteration 97, loss = 0.08533122\n",
      "Iteration 98, loss = 0.08006621\n",
      "Iteration 12, loss = 0.32688068\n",
      "Iteration 13, loss = 0.31729737\n",
      "Iteration 99, loss = 0.07058327\n",
      "Iteration 100, loss = 0.07567327\n",
      "Iteration 101, loss = 0.07120915\n",
      "Iteration 14, loss = 0.30753235\n",
      "Iteration 102, loss = 0.07010206\n",
      "Iteration 103, loss = 0.06634011\n",
      "Iteration 15, loss = 0.29953636\n",
      "Iteration 104, loss = 0.06588068\n",
      "Iteration 16, loss = 0.29117223\n",
      "Iteration 17, loss = 0.28213076\n",
      "Iteration 105, loss = 0.06351093\n",
      "Iteration 18, loss = 0.27332496\n",
      "Iteration 106, loss = 0.06469245\n",
      "Iteration 107, loss = 0.06281418\n",
      "Iteration 108, loss = 0.06188128\n",
      "Iteration 19, loss = 0.26753356\n",
      "Iteration 109, loss = 0.06230394\n",
      "Iteration 20, loss = 0.25985469\n",
      "Iteration 110, loss = 0.06135668\n",
      "Iteration 111, loss = 0.05986994\n",
      "Iteration 112, loss = 0.05926481\n",
      "Iteration 21, loss = 0.25138212\n",
      "Iteration 113, loss = 0.05836043\n",
      "Iteration 22, loss = 0.24650518\n",
      "Iteration 114, loss = 0.05804386\n",
      "Iteration 115, loss = 0.05726608\n",
      "Iteration 116, loss = 0.05804581\n",
      "Iteration 23, loss = 0.23804705\n",
      "Iteration 117, loss = 0.05618910\n",
      "Iteration 24, loss = 0.23245220\n",
      "Iteration 118, loss = 0.05681796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 119, loss = 0.05540546\n",
      "Iteration 25, loss = 0.22772321\n",
      "Iteration 120, loss = 0.05494572\n",
      "Iteration 121, loss = 0.05385075\n",
      "Iteration 122, loss = 0.05255836\n",
      "Iteration 26, loss = 0.22160664\n",
      "Iteration 123, loss = 0.05215935\n",
      "Iteration 27, loss = 0.21865144\n",
      "Iteration 28, loss = 0.21008826\n",
      "Iteration 124, loss = 0.05227362\n",
      "Iteration 29, loss = 0.20695679\n",
      "Iteration 30, loss = 0.20273960\n",
      "Iteration 31, loss = 0.19621107\n",
      "Iteration 125, loss = 0.05248520\n",
      "Iteration 32, loss = 0.19292802\n",
      "Iteration 126, loss = 0.05338326\n",
      "Iteration 127, loss = 0.05207495\n",
      "Iteration 33, loss = 0.18936859\n",
      "Iteration 128, loss = 0.04955538\n",
      "Iteration 34, loss = 0.18614718\n",
      "Iteration 129, loss = 0.04863301\n",
      "Iteration 35, loss = 0.18286753\n",
      "Iteration 36, loss = 0.17893548\n",
      "Iteration 130, loss = 0.04827011\n",
      "Iteration 131, loss = 0.05127669\n",
      "Iteration 37, loss = 0.17498188\n",
      "Iteration 132, loss = 0.05019192\n",
      "Iteration 133, loss = 0.05474006\n",
      "Iteration 38, loss = 0.17238790\n",
      "Iteration 39, loss = 0.16835355\n",
      "Iteration 40, loss = 0.16937433\n",
      "Iteration 134, loss = 0.05133893\n",
      "Iteration 41, loss = 0.16201673\n",
      "Iteration 135, loss = 0.05013394\n",
      "Iteration 136, loss = 0.05246703\n",
      "Iteration 42, loss = 0.15998569\n",
      "Iteration 137, loss = 0.05476833\n",
      "Iteration 43, loss = 0.15802400\n",
      "Iteration 44, loss = 0.15365215\n",
      "Iteration 45, loss = 0.15206287\n",
      "Iteration 138, loss = 0.05347626\n",
      "Iteration 139, loss = 0.05473011\n",
      "Iteration 140, loss = 0.05453591\n",
      "Iteration 46, loss = 0.15151933\n",
      "Iteration 141, loss = 0.04956360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.14831321\n",
      "Iteration 48, loss = 0.14686734\n",
      "Iteration 49, loss = 0.14428465\n",
      "Iteration 50, loss = 0.14048303\n",
      "Iteration 51, loss = 0.14060019\n",
      "Iteration 52, loss = 0.13609069\n",
      "Iteration 53, loss = 0.13435643\n",
      "Iteration 54, loss = 0.13577543\n",
      "Iteration 55, loss = 0.13084090\n",
      "Iteration 56, loss = 0.13010875\n",
      "Iteration 57, loss = 0.12865189\n",
      "Iteration 58, loss = 0.12752866\n",
      "Iteration 59, loss = 0.12625606\n",
      "Iteration 60, loss = 0.12258524\n",
      "Iteration 1, loss = 1.25854091\n",
      "Iteration 61, loss = 0.12094374\n",
      "Iteration 2, loss = 0.81077871\n",
      "Iteration 62, loss = 0.11944147\n",
      "Iteration 3, loss = 0.60393658\n",
      "Iteration 63, loss = 0.11870421\n",
      "Iteration 4, loss = 0.51960124\n",
      "Iteration 64, loss = 0.11615284\n",
      "Iteration 5, loss = 0.46707604\n",
      "Iteration 6, loss = 0.43011882\n",
      "Iteration 65, loss = 0.11590016\n",
      "Iteration 66, loss = 0.11377462\n",
      "Iteration 7, loss = 0.40191258\n",
      "Iteration 67, loss = 0.11112058\n",
      "Iteration 68, loss = 0.10980153\n",
      "Iteration 8, loss = 0.38415196\n",
      "Iteration 69, loss = 0.11010807\n",
      "Iteration 9, loss = 0.36716435\n",
      "Iteration 70, loss = 0.10872997\n",
      "Iteration 10, loss = 0.35468262\n",
      "Iteration 71, loss = 0.10897794\n",
      "Iteration 72, loss = 0.10888850\n",
      "Iteration 73, loss = 0.10413444\n",
      "Iteration 11, loss = 0.34107433\n",
      "Iteration 74, loss = 0.10730337\n",
      "Iteration 75, loss = 0.10423110\n",
      "Iteration 76, loss = 0.09980165\n",
      "Iteration 12, loss = 0.33159249\n",
      "Iteration 77, loss = 0.09823685\n",
      "Iteration 78, loss = 0.09778583\n",
      "Iteration 13, loss = 0.32083158\n",
      "Iteration 79, loss = 0.09608447\n",
      "Iteration 14, loss = 0.31203083\n",
      "Iteration 80, loss = 0.09387386\n",
      "Iteration 15, loss = 0.30361460\n",
      "Iteration 81, loss = 0.09920942\n",
      "Iteration 16, loss = 0.29979585\n",
      "Iteration 82, loss = 0.09559563\n",
      "Iteration 17, loss = 0.28891288\n",
      "Iteration 83, loss = 0.09567220\n",
      "Iteration 18, loss = 0.28344187\n",
      "Iteration 84, loss = 0.09224893\n",
      "Iteration 19, loss = 0.27531226\n",
      "Iteration 85, loss = 0.08890883\n",
      "Iteration 20, loss = 0.27070207\n",
      "Iteration 86, loss = 0.08800189\n",
      "Iteration 21, loss = 0.26306974\n",
      "Iteration 87, loss = 0.08598269\n",
      "Iteration 22, loss = 0.25925210\n",
      "Iteration 88, loss = 0.08468003\n",
      "Iteration 23, loss = 0.25206599\n",
      "Iteration 89, loss = 0.08672214\n",
      "Iteration 24, loss = 0.24844076\n",
      "Iteration 90, loss = 0.08658236\n",
      "Iteration 25, loss = 0.24220176\n",
      "Iteration 26, loss = 0.23836172\n",
      "Iteration 91, loss = 0.08476105\n",
      "Iteration 92, loss = 0.08445846\n",
      "Iteration 27, loss = 0.23278043\n",
      "Iteration 28, loss = 0.23063211\n",
      "Iteration 93, loss = 0.07986407\n",
      "Iteration 29, loss = 0.22546179\n",
      "Iteration 94, loss = 0.07998179\n",
      "Iteration 30, loss = 0.22084842\n",
      "Iteration 95, loss = 0.07864934\n",
      "Iteration 31, loss = 0.21525712\n",
      "Iteration 96, loss = 0.07688078\n",
      "Iteration 32, loss = 0.21130683\n",
      "Iteration 97, loss = 0.07648852\n",
      "Iteration 33, loss = 0.20821581\n",
      "Iteration 98, loss = 0.07419706\n",
      "Iteration 34, loss = 0.20361386\n",
      "Iteration 99, loss = 0.07486700\n",
      "Iteration 35, loss = 0.19980170\n",
      "Iteration 100, loss = 0.07353419\n",
      "Iteration 36, loss = 0.19628982\n",
      "Iteration 101, loss = 0.07410840\n",
      "Iteration 37, loss = 0.19616336\n",
      "Iteration 102, loss = 0.07217103\n",
      "Iteration 38, loss = 0.19032913\n",
      "Iteration 103, loss = 0.07093324\n",
      "Iteration 39, loss = 0.18668214\n",
      "Iteration 104, loss = 0.06964146\n",
      "Iteration 40, loss = 0.18675058\n",
      "Iteration 105, loss = 0.06937501\n",
      "Iteration 41, loss = 0.17770037\n",
      "Iteration 106, loss = 0.06871953\n",
      "Iteration 42, loss = 0.17741787\n",
      "Iteration 107, loss = 0.06695873\n",
      "Iteration 108, loss = 0.06755187\n",
      "Iteration 43, loss = 0.17231380\n",
      "Iteration 109, loss = 0.06572524\n",
      "Iteration 44, loss = 0.16954283\n",
      "Iteration 45, loss = 0.16749167\n",
      "Iteration 110, loss = 0.06515863\n",
      "Iteration 46, loss = 0.16449757\n",
      "Iteration 111, loss = 0.06735442\n",
      "Iteration 112, loss = 0.06642945\n",
      "Iteration 47, loss = 0.16068704\n",
      "Iteration 113, loss = 0.06554941\n",
      "Iteration 48, loss = 0.15911015\n",
      "Iteration 114, loss = 0.06381749\n",
      "Iteration 49, loss = 0.15481942\n",
      "Iteration 115, loss = 0.06414228\n",
      "Iteration 50, loss = 0.15326645\n",
      "Iteration 116, loss = 0.06713067\n",
      "Iteration 117, loss = 0.06538632\n",
      "Iteration 51, loss = 0.15060061\n",
      "Iteration 52, loss = 0.14835196\n",
      "Iteration 118, loss = 0.06314431\n",
      "Iteration 53, loss = 0.14669894\n",
      "Iteration 119, loss = 0.06557327\n",
      "Iteration 54, loss = 0.14637798\n",
      "Iteration 120, loss = 0.05881680\n",
      "Iteration 55, loss = 0.14303219\n",
      "Iteration 121, loss = 0.05912406\n",
      "Iteration 56, loss = 0.14098830\n",
      "Iteration 122, loss = 0.06463809\n",
      "Iteration 123, loss = 0.05781363\n",
      "Iteration 57, loss = 0.13740183\n",
      "Iteration 58, loss = 0.13417619\n",
      "Iteration 124, loss = 0.05626582\n",
      "Iteration 59, loss = 0.13308065\n",
      "Iteration 125, loss = 0.05548572\n",
      "Iteration 60, loss = 0.13080972\n",
      "Iteration 126, loss = 0.05276580\n",
      "Iteration 127, loss = 0.05574331\n",
      "Iteration 61, loss = 0.12966362\n",
      "Iteration 128, loss = 0.05630918\n",
      "Iteration 62, loss = 0.12575657\n",
      "Iteration 63, loss = 0.12417614\n",
      "Iteration 129, loss = 0.05872556\n",
      "Iteration 64, loss = 0.12325445\n",
      "Iteration 130, loss = 0.05490409\n",
      "Iteration 65, loss = 0.12347314\n",
      "Iteration 131, loss = 0.05203800\n",
      "Iteration 132, loss = 0.05112339\n",
      "Iteration 133, loss = 0.05195288\n",
      "Iteration 66, loss = 0.12655165\n",
      "Iteration 134, loss = 0.05042419\n",
      "Iteration 135, loss = 0.05468772\n",
      "Iteration 67, loss = 0.12202551\n",
      "Iteration 136, loss = 0.05272117\n",
      "Iteration 68, loss = 0.11420358\n",
      "Iteration 69, loss = 0.11784226\n",
      "Iteration 137, loss = 0.05427912\n",
      "Iteration 70, loss = 0.11285386\n",
      "Iteration 138, loss = 0.05283611\n",
      "Iteration 139, loss = 0.05057038\n",
      "Iteration 71, loss = 0.11530427\n",
      "Iteration 140, loss = 0.05142342\n",
      "Iteration 141, loss = 0.04848106\n",
      "Iteration 142, loss = 0.04786572\n",
      "Iteration 72, loss = 0.11056789\n",
      "Iteration 143, loss = 0.04576385\n",
      "Iteration 144, loss = 0.04443884\n",
      "Iteration 73, loss = 0.11016252\n",
      "Iteration 74, loss = 0.10527306\n",
      "Iteration 145, loss = 0.04696322\n",
      "Iteration 75, loss = 0.10535329\n",
      "Iteration 76, loss = 0.10471381\n",
      "Iteration 146, loss = 0.04509404\n",
      "Iteration 147, loss = 0.04738074\n",
      "Iteration 77, loss = 0.10385535\n",
      "Iteration 148, loss = 0.05203622\n",
      "Iteration 78, loss = 0.09858592\n",
      "Iteration 79, loss = 0.09875800\n",
      "Iteration 149, loss = 0.04667445\n",
      "Iteration 80, loss = 0.10024762\n",
      "Iteration 81, loss = 0.09846405\n",
      "Iteration 150, loss = 0.04307938\n",
      "Iteration 82, loss = 0.09369763\n",
      "Iteration 151, loss = 0.04261847\n",
      "Iteration 83, loss = 0.09128620\n",
      "Iteration 152, loss = 0.04629130\n",
      "Iteration 153, loss = 0.04895736\n",
      "Iteration 84, loss = 0.08970431\n",
      "Iteration 85, loss = 0.08733390\n",
      "Iteration 86, loss = 0.08492707\n",
      "Iteration 154, loss = 0.04796787\n",
      "Iteration 87, loss = 0.08613355\n",
      "Iteration 155, loss = 0.04762186\n",
      "Iteration 88, loss = 0.08340167\n",
      "Iteration 156, loss = 0.04232202\n",
      "Iteration 157, loss = 0.03992000\n",
      "Iteration 89, loss = 0.08138407\n",
      "Iteration 158, loss = 0.04350757\n",
      "Iteration 90, loss = 0.08482294\n",
      "Iteration 159, loss = 0.04738259\n",
      "Iteration 91, loss = 0.08359884\n",
      "Iteration 160, loss = 0.04472137\n",
      "Iteration 161, loss = 0.03905719\n",
      "Iteration 92, loss = 0.07698236\n",
      "Iteration 162, loss = 0.03824667\n",
      "Iteration 93, loss = 0.07712665\n",
      "Iteration 94, loss = 0.07593580\n",
      "Iteration 95, loss = 0.07896049\n",
      "Iteration 163, loss = 0.03851383\n",
      "Iteration 164, loss = 0.03668048\n",
      "Iteration 96, loss = 0.07909749\n",
      "Iteration 165, loss = 0.03715319\n",
      "Iteration 166, loss = 0.03853884\n",
      "Iteration 97, loss = 0.07561866\n",
      "Iteration 98, loss = 0.07157962\n",
      "Iteration 99, loss = 0.07143146\n",
      "Iteration 167, loss = 0.03858333\n",
      "Iteration 100, loss = 0.06904353\n",
      "Iteration 168, loss = 0.03847256\n",
      "Iteration 101, loss = 0.06904465\n",
      "Iteration 169, loss = 0.03952722\n",
      "Iteration 102, loss = 0.06770838\n",
      "Iteration 170, loss = 0.04043415\n",
      "Iteration 103, loss = 0.06618752\n",
      "Iteration 171, loss = 0.04426489\n",
      "Iteration 104, loss = 0.06605851\n",
      "Iteration 172, loss = 0.03878883\n",
      "Iteration 105, loss = 0.06432846\n",
      "Iteration 106, loss = 0.06636014\n",
      "Iteration 107, loss = 0.06464668\n",
      "Iteration 173, loss = 0.03794195\n",
      "Iteration 174, loss = 0.03625659\n",
      "Iteration 108, loss = 0.06208729\n",
      "Iteration 175, loss = 0.03534886\n",
      "Iteration 109, loss = 0.06147179\n",
      "Iteration 176, loss = 0.03440009\n",
      "Iteration 110, loss = 0.05975433\n",
      "Iteration 177, loss = 0.03588451\n",
      "Iteration 111, loss = 0.06125134\n",
      "Iteration 178, loss = 0.03432508\n",
      "Iteration 112, loss = 0.06224325\n",
      "Iteration 179, loss = 0.03724805\n",
      "Iteration 113, loss = 0.05838414\n",
      "Iteration 180, loss = 0.03707184\n",
      "Iteration 114, loss = 0.05709339\n",
      "Iteration 181, loss = 0.04723803\n",
      "Iteration 115, loss = 0.05641485\n",
      "Iteration 182, loss = 0.04709406\n",
      "Iteration 116, loss = 0.05551157\n",
      "Iteration 183, loss = 0.03834347\n",
      "Iteration 117, loss = 0.06000968\n",
      "Iteration 184, loss = 0.03415653\n",
      "Iteration 118, loss = 0.05946910\n",
      "Iteration 185, loss = 0.03338260\n",
      "Iteration 119, loss = 0.05595450\n",
      "Iteration 186, loss = 0.03104781\n",
      "Iteration 120, loss = 0.05214129\n",
      "Iteration 121, loss = 0.05220738\n",
      "Iteration 187, loss = 0.03248837\n",
      "Iteration 122, loss = 0.05546367\n",
      "Iteration 123, loss = 0.05439879\n",
      "Iteration 188, loss = 0.03159901\n",
      "Iteration 189, loss = 0.02993775\n",
      "Iteration 124, loss = 0.05639083\n",
      "Iteration 190, loss = 0.03080040\n",
      "Iteration 125, loss = 0.05822096\n",
      "Iteration 191, loss = 0.03059832\n",
      "Iteration 126, loss = 0.05865371\n",
      "Iteration 192, loss = 0.03130855\n",
      "Iteration 127, loss = 0.06080055\n",
      "Iteration 193, loss = 0.03138662\n",
      "Iteration 128, loss = 0.05990524\n",
      "Iteration 194, loss = 0.03143416\n",
      "Iteration 195, loss = 0.03263930\n",
      "Iteration 196, loss = 0.03264725\n",
      "Iteration 197, loss = 0.03419512\n",
      "Iteration 129, loss = 0.06576934\n",
      "Iteration 198, loss = 0.03120108\n",
      "Iteration 130, loss = 0.05682907\n",
      "Iteration 199, loss = 0.03128043\n",
      "Iteration 200, loss = 0.02933725\n",
      "Iteration 201, loss = 0.02885640\n",
      "Iteration 131, loss = 0.05656819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 202, loss = 0.02925267\n",
      "Iteration 203, loss = 0.02816289\n",
      "Iteration 204, loss = 0.03193710\n",
      "Iteration 205, loss = 0.02813340\n",
      "Iteration 206, loss = 0.03021256\n",
      "Iteration 207, loss = 0.02693619\n",
      "Iteration 208, loss = 0.02789556\n",
      "Iteration 1, loss = 1.04945427\n",
      "Iteration 2, loss = 0.69231427\n",
      "Iteration 209, loss = 0.02791670\n",
      "Iteration 3, loss = 0.56273614\n",
      "Iteration 210, loss = 0.02621163\n",
      "Iteration 4, loss = 0.49407470\n",
      "Iteration 5, loss = 0.45276756\n",
      "Iteration 211, loss = 0.02536077\n",
      "Iteration 6, loss = 0.42548130\n",
      "Iteration 212, loss = 0.02798253\n",
      "Iteration 213, loss = 0.02872545\n",
      "Iteration 7, loss = 0.40460325\n",
      "Iteration 214, loss = 0.03147198\n",
      "Iteration 8, loss = 0.38744735\n",
      "Iteration 9, loss = 0.37475267\n",
      "Iteration 10, loss = 0.36265175\n",
      "Iteration 215, loss = 0.03712348\n",
      "Iteration 11, loss = 0.35157002\n",
      "Iteration 216, loss = 0.05215102\n",
      "Iteration 12, loss = 0.34046221\n",
      "Iteration 217, loss = 0.05336607\n",
      "Iteration 218, loss = 0.07206161\n",
      "Iteration 13, loss = 0.33032226\n",
      "Iteration 14, loss = 0.32319845\n",
      "Iteration 15, loss = 0.31191794\n",
      "Iteration 219, loss = 0.06353785\n",
      "Iteration 16, loss = 0.30463753\n",
      "Iteration 17, loss = 0.29843455\n",
      "Iteration 18, loss = 0.28928511\n",
      "Iteration 220, loss = 0.04603182\n",
      "Iteration 221, loss = 0.04132351\n",
      "Iteration 222, loss = 0.03993941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.28340436\n",
      "Iteration 20, loss = 0.27643175\n",
      "Iteration 21, loss = 0.26742606\n",
      "Iteration 22, loss = 0.26451944\n",
      "Iteration 23, loss = 0.25444266\n",
      "Iteration 24, loss = 0.25101617\n",
      "Iteration 25, loss = 0.24615831\n",
      "Iteration 26, loss = 0.24089481\n",
      "Iteration 27, loss = 0.23339861\n",
      "Iteration 28, loss = 0.22900230\n",
      "Iteration 29, loss = 0.22444621\n",
      "Iteration 30, loss = 0.21888379\n",
      "Iteration 31, loss = 0.21570285\n",
      "Iteration 1, loss = 1.24458501\n",
      "Iteration 32, loss = 0.21282927\n",
      "Iteration 2, loss = 0.77324197\n",
      "Iteration 33, loss = 0.20794251\n",
      "Iteration 3, loss = 0.59462260\n",
      "Iteration 34, loss = 0.20235011\n",
      "Iteration 4, loss = 0.50223753\n",
      "Iteration 35, loss = 0.19945754\n",
      "Iteration 5, loss = 0.45188779\n",
      "Iteration 6, loss = 0.41739269\n",
      "Iteration 7, loss = 0.39123881\n",
      "Iteration 36, loss = 0.19438744\n",
      "Iteration 8, loss = 0.37297473\n",
      "Iteration 9, loss = 0.35679105\n",
      "Iteration 10, loss = 0.34348801\n",
      "Iteration 37, loss = 0.19012502\n",
      "Iteration 11, loss = 0.33039156\n",
      "Iteration 38, loss = 0.18666849\n",
      "Iteration 39, loss = 0.18269573\n",
      "Iteration 12, loss = 0.31990291\n",
      "Iteration 40, loss = 0.17931218\n",
      "Iteration 41, loss = 0.17594581\n",
      "Iteration 42, loss = 0.17463830\n",
      "Iteration 43, loss = 0.17799167\n",
      "Iteration 13, loss = 0.31023574\n",
      "Iteration 14, loss = 0.29967209\n",
      "Iteration 15, loss = 0.29282446\n",
      "Iteration 44, loss = 0.17308074\n",
      "Iteration 16, loss = 0.28289441\n",
      "Iteration 45, loss = 0.16336565\n",
      "Iteration 46, loss = 0.16805662\n",
      "Iteration 17, loss = 0.27692986\n",
      "Iteration 47, loss = 0.15999972\n",
      "Iteration 48, loss = 0.15855441\n",
      "Iteration 18, loss = 0.26717832\n",
      "Iteration 49, loss = 0.16406563\n",
      "Iteration 19, loss = 0.26002733\n",
      "Iteration 50, loss = 0.15681731\n",
      "Iteration 20, loss = 0.25333176\n",
      "Iteration 21, loss = 0.24619243\n",
      "Iteration 51, loss = 0.15425714\n",
      "Iteration 52, loss = 0.15064729\n",
      "Iteration 53, loss = 0.14549691\n",
      "Iteration 22, loss = 0.24253742\n",
      "Iteration 23, loss = 0.23785741\n",
      "Iteration 54, loss = 0.14393439\n",
      "Iteration 24, loss = 0.22660582\n",
      "Iteration 25, loss = 0.22385295\n",
      "Iteration 55, loss = 0.14129245\n",
      "Iteration 56, loss = 0.13845878\n",
      "Iteration 26, loss = 0.21395928\n",
      "Iteration 57, loss = 0.13800126\n",
      "Iteration 58, loss = 0.13850261\n",
      "Iteration 27, loss = 0.20892398\n",
      "Iteration 28, loss = 0.20256566\n",
      "Iteration 29, loss = 0.20033016\n",
      "Iteration 59, loss = 0.13575978\n",
      "Iteration 60, loss = 0.13062723\n",
      "Iteration 30, loss = 0.19461980\n",
      "Iteration 61, loss = 0.13316832\n",
      "Iteration 31, loss = 0.18826711\n",
      "Iteration 32, loss = 0.18271612\n",
      "Iteration 62, loss = 0.12786357\n",
      "Iteration 33, loss = 0.17842614\n",
      "Iteration 34, loss = 0.17375821\n",
      "Iteration 63, loss = 0.12598887\n",
      "Iteration 64, loss = 0.12442078\n",
      "Iteration 65, loss = 0.12108987\n",
      "Iteration 35, loss = 0.16975450\n",
      "Iteration 66, loss = 0.11973929\n",
      "Iteration 67, loss = 0.11987518\n",
      "Iteration 36, loss = 0.16623421\n",
      "Iteration 37, loss = 0.16389018\n",
      "Iteration 68, loss = 0.11711631\n",
      "Iteration 38, loss = 0.15760694\n",
      "Iteration 69, loss = 0.11382562\n",
      "Iteration 70, loss = 0.11482786\n",
      "Iteration 39, loss = 0.15538012\n",
      "Iteration 71, loss = 0.11698009\n",
      "Iteration 40, loss = 0.15586162Iteration 72, loss = 0.11678054\n",
      "\n",
      "Iteration 73, loss = 0.11033440\n",
      "Iteration 74, loss = 0.10696230\n",
      "Iteration 75, loss = 0.10948372\n",
      "Iteration 41, loss = 0.15262640\n",
      "Iteration 42, loss = 0.14693475\n",
      "Iteration 76, loss = 0.10711127\n",
      "Iteration 43, loss = 0.14441043\n",
      "Iteration 44, loss = 0.13952170\n",
      "Iteration 77, loss = 0.10749962\n",
      "Iteration 45, loss = 0.13660756\n",
      "Iteration 78, loss = 0.10617910\n",
      "Iteration 46, loss = 0.13273073\n",
      "Iteration 79, loss = 0.10292756\n",
      "Iteration 47, loss = 0.12965145\n",
      "Iteration 80, loss = 0.10139739\n",
      "Iteration 48, loss = 0.12677701\n",
      "Iteration 81, loss = 0.09931154\n",
      "Iteration 82, loss = 0.09694755\n",
      "Iteration 83, loss = 0.09390967\n",
      "Iteration 49, loss = 0.12583895\n",
      "Iteration 84, loss = 0.09319534\n",
      "Iteration 85, loss = 0.09261081\n",
      "Iteration 50, loss = 0.12245129\n",
      "Iteration 51, loss = 0.11942171\n",
      "Iteration 52, loss = 0.11839762\n",
      "Iteration 86, loss = 0.09213864\n",
      "Iteration 53, loss = 0.11999235\n",
      "Iteration 87, loss = 0.09218823\n",
      "Iteration 54, loss = 0.11472739\n",
      "Iteration 55, loss = 0.11375379\n",
      "Iteration 88, loss = 0.08888848\n",
      "Iteration 56, loss = 0.10955351\n",
      "Iteration 89, loss = 0.08724274\n",
      "Iteration 90, loss = 0.09025192\n",
      "Iteration 57, loss = 0.10832555\n",
      "Iteration 58, loss = 0.10546840\n",
      "Iteration 91, loss = 0.08609841\n",
      "Iteration 59, loss = 0.10448335\n",
      "Iteration 60, loss = 0.10180804\n",
      "Iteration 92, loss = 0.08774248\n",
      "Iteration 61, loss = 0.09865631\n",
      "Iteration 93, loss = 0.09051041\n",
      "Iteration 62, loss = 0.09762593\n",
      "Iteration 94, loss = 0.08387283\n",
      "Iteration 63, loss = 0.09575934\n",
      "Iteration 95, loss = 0.08103932\n",
      "Iteration 64, loss = 0.09529447\n",
      "Iteration 96, loss = 0.08031619\n",
      "Iteration 97, loss = 0.07884443\n",
      "Iteration 65, loss = 0.09210707\n",
      "Iteration 66, loss = 0.09121182\n",
      "Iteration 98, loss = 0.08189802\n",
      "Iteration 67, loss = 0.09105281\n",
      "Iteration 99, loss = 0.07705543\n",
      "Iteration 68, loss = 0.08866070\n",
      "Iteration 100, loss = 0.07729131\n",
      "Iteration 69, loss = 0.09231870\n",
      "Iteration 101, loss = 0.07541914\n",
      "Iteration 70, loss = 0.08904589\n",
      "Iteration 71, loss = 0.08544980\n",
      "Iteration 72, loss = 0.08309531\n",
      "Iteration 102, loss = 0.07529178\n",
      "Iteration 73, loss = 0.08320160\n",
      "Iteration 103, loss = 0.07448288\n",
      "Iteration 74, loss = 0.08233883\n",
      "Iteration 104, loss = 0.07186746\n",
      "Iteration 105, loss = 0.07321975\n",
      "Iteration 75, loss = 0.07933624\n",
      "Iteration 106, loss = 0.07291613\n",
      "Iteration 76, loss = 0.07913490\n",
      "Iteration 107, loss = 0.07236827\n",
      "Iteration 77, loss = 0.07916987\n",
      "Iteration 108, loss = 0.07208468\n",
      "Iteration 78, loss = 0.07660285\n",
      "Iteration 109, loss = 0.07266790\n",
      "Iteration 79, loss = 0.07584094\n",
      "Iteration 80, loss = 0.07372209\n",
      "Iteration 110, loss = 0.06870308\n",
      "Iteration 81, loss = 0.07252848\n",
      "Iteration 82, loss = 0.07071814\n",
      "Iteration 111, loss = 0.06912926\n",
      "Iteration 83, loss = 0.07004826\n",
      "Iteration 112, loss = 0.07147026\n",
      "Iteration 84, loss = 0.06927443\n",
      "Iteration 85, loss = 0.06779189\n",
      "Iteration 86, loss = 0.06738556\n",
      "Iteration 113, loss = 0.06534459\n",
      "Iteration 87, loss = 0.06661120\n",
      "Iteration 114, loss = 0.06501147\n",
      "Iteration 88, loss = 0.06551194\n",
      "Iteration 115, loss = 0.06351650\n",
      "Iteration 89, loss = 0.06351374\n",
      "Iteration 116, loss = 0.06291848\n",
      "Iteration 90, loss = 0.06701238\n",
      "Iteration 91, loss = 0.06496837\n",
      "Iteration 117, loss = 0.06375250\n",
      "Iteration 92, loss = 0.06338905\n",
      "Iteration 93, loss = 0.06169313\n",
      "Iteration 118, loss = 0.06275034\n",
      "Iteration 119, loss = 0.06385757\n",
      "Iteration 94, loss = 0.06111428\n",
      "Iteration 120, loss = 0.06335822\n",
      "Iteration 95, loss = 0.05951986\n",
      "Iteration 96, loss = 0.05884589\n",
      "Iteration 97, loss = 0.05965823\n",
      "Iteration 121, loss = 0.06041434\n",
      "Iteration 122, loss = 0.06187643\n",
      "Iteration 98, loss = 0.05774868\n",
      "Iteration 123, loss = 0.06617706\n",
      "Iteration 124, loss = 0.07080346\n",
      "Iteration 125, loss = 0.06238619\n",
      "Iteration 99, loss = 0.05610716\n",
      "Iteration 126, loss = 0.06312093\n",
      "Iteration 127, loss = 0.05895857\n",
      "Iteration 100, loss = 0.05636542\n",
      "Iteration 128, loss = 0.05689249\n",
      "Iteration 101, loss = 0.05615255\n",
      "Iteration 129, loss = 0.05592681\n",
      "Iteration 102, loss = 0.05944547\n",
      "Iteration 103, loss = 0.05575187\n",
      "Iteration 130, loss = 0.05464480\n",
      "Iteration 104, loss = 0.05483685\n",
      "Iteration 131, loss = 0.05442828\n",
      "Iteration 132, loss = 0.05350398\n",
      "Iteration 133, loss = 0.05275585\n",
      "Iteration 105, loss = 0.05513074\n",
      "Iteration 106, loss = 0.05242284\n",
      "Iteration 107, loss = 0.05164564\n",
      "Iteration 134, loss = 0.05304768\n",
      "Iteration 108, loss = 0.05036889\n",
      "Iteration 135, loss = 0.05256077\n",
      "Iteration 109, loss = 0.05096446\n",
      "Iteration 136, loss = 0.05230101\n",
      "Iteration 137, loss = 0.05133878\n",
      "Iteration 110, loss = 0.05007453\n",
      "Iteration 138, loss = 0.04895577\n",
      "Iteration 111, loss = 0.05155199\n",
      "Iteration 139, loss = 0.05037725\n",
      "Iteration 112, loss = 0.05539070\n",
      "Iteration 140, loss = 0.04862787\n",
      "Iteration 113, loss = 0.04893294\n",
      "Iteration 141, loss = 0.05017756\n",
      "Iteration 114, loss = 0.04718027\n",
      "Iteration 142, loss = 0.05144160\n",
      "Iteration 143, loss = 0.04758836\n",
      "Iteration 115, loss = 0.04801575\n",
      "Iteration 116, loss = 0.04939136\n",
      "Iteration 144, loss = 0.04872696\n",
      "Iteration 117, loss = 0.04884712\n",
      "Iteration 145, loss = 0.04707695\n",
      "Iteration 118, loss = 0.04474386\n",
      "Iteration 146, loss = 0.04689592\n",
      "Iteration 147, loss = 0.04536161\n",
      "Iteration 119, loss = 0.04644062\n",
      "Iteration 148, loss = 0.04596561\n",
      "Iteration 120, loss = 0.04755678\n",
      "Iteration 121, loss = 0.04447748\n",
      "Iteration 149, loss = 0.04462226\n",
      "Iteration 122, loss = 0.04198820\n",
      "Iteration 150, loss = 0.04455699\n",
      "Iteration 123, loss = 0.04359847\n",
      "Iteration 151, loss = 0.04403853\n",
      "Iteration 124, loss = 0.04170876\n",
      "Iteration 152, loss = 0.04401388\n",
      "Iteration 125, loss = 0.04128666\n",
      "Iteration 153, loss = 0.04292334\n",
      "Iteration 154, loss = 0.04394309\n",
      "Iteration 126, loss = 0.04065247\n",
      "Iteration 155, loss = 0.04263192\n",
      "Iteration 127, loss = 0.03994484\n",
      "Iteration 128, loss = 0.04150512\n",
      "Iteration 156, loss = 0.04262810\n",
      "Iteration 129, loss = 0.04146917\n",
      "Iteration 157, loss = 0.04106073\n",
      "Iteration 130, loss = 0.03928811\n",
      "Iteration 158, loss = 0.04176916\n",
      "Iteration 131, loss = 0.03820810\n",
      "Iteration 159, loss = 0.04072018\n",
      "Iteration 132, loss = 0.03720015\n",
      "Iteration 133, loss = 0.03764472\n",
      "Iteration 160, loss = 0.04052227\n",
      "Iteration 134, loss = 0.03662908\n",
      "Iteration 161, loss = 0.03934714\n",
      "Iteration 162, loss = 0.04016771\n",
      "Iteration 135, loss = 0.03782494\n",
      "Iteration 163, loss = 0.04021904\n",
      "Iteration 164, loss = 0.03932420\n",
      "Iteration 136, loss = 0.03727066\n",
      "Iteration 137, loss = 0.03715671\n",
      "Iteration 138, loss = 0.03594295\n",
      "Iteration 165, loss = 0.03931226\n",
      "Iteration 139, loss = 0.03592161\n",
      "Iteration 166, loss = 0.03841647\n",
      "Iteration 140, loss = 0.03486375\n",
      "Iteration 167, loss = 0.04040455\n",
      "Iteration 141, loss = 0.03401863\n",
      "Iteration 142, loss = 0.03371146\n",
      "Iteration 168, loss = 0.04072761\n",
      "Iteration 143, loss = 0.03331480\n",
      "Iteration 169, loss = 0.04327864\n",
      "Iteration 170, loss = 0.05529089\n",
      "Iteration 144, loss = 0.03320067\n",
      "Iteration 171, loss = 0.05169612\n",
      "Iteration 145, loss = 0.03458893\n",
      "Iteration 146, loss = 0.03468023\n",
      "Iteration 147, loss = 0.03465143\n",
      "Iteration 172, loss = 0.04586206\n",
      "Iteration 148, loss = 0.03550215\n",
      "Iteration 173, loss = 0.04413227\n",
      "Iteration 174, loss = 0.04128162\n",
      "Iteration 149, loss = 0.03348378\n",
      "Iteration 175, loss = 0.04196046\n",
      "Iteration 150, loss = 0.03243133\n",
      "Iteration 176, loss = 0.03732759\n",
      "Iteration 151, loss = 0.03512887\n",
      "Iteration 152, loss = 0.03464474\n",
      "Iteration 177, loss = 0.03497624\n",
      "Iteration 153, loss = 0.03110029\n",
      "Iteration 154, loss = 0.03177959\n",
      "Iteration 155, loss = 0.03135807\n",
      "Iteration 178, loss = 0.03464902\n",
      "Iteration 156, loss = 0.03084317\n",
      "Iteration 157, loss = 0.03065634\n",
      "Iteration 179, loss = 0.03378386\n",
      "Iteration 158, loss = 0.02998300\n",
      "Iteration 159, loss = 0.02924343\n",
      "Iteration 180, loss = 0.03458590\n",
      "Iteration 160, loss = 0.02933060\n",
      "Iteration 181, loss = 0.03391676\n",
      "Iteration 161, loss = 0.03034918\n",
      "Iteration 182, loss = 0.03526575\n",
      "Iteration 162, loss = 0.03038886\n",
      "Iteration 183, loss = 0.03353931\n",
      "Iteration 163, loss = 0.02895147\n",
      "Iteration 184, loss = 0.03638774\n",
      "Iteration 164, loss = 0.02885635\n",
      "Iteration 165, loss = 0.03078404\n",
      "Iteration 185, loss = 0.03562013\n",
      "Iteration 166, loss = 0.03116729\n",
      "Iteration 167, loss = 0.03064693\n",
      "Iteration 186, loss = 0.03452678\n",
      "Iteration 168, loss = 0.02870842\n",
      "Iteration 187, loss = 0.03317805\n",
      "Iteration 169, loss = 0.02763740\n",
      "Iteration 188, loss = 0.03351291\n",
      "Iteration 170, loss = 0.02697890\n",
      "Iteration 189, loss = 0.03185085\n",
      "Iteration 171, loss = 0.03027296\n",
      "Iteration 190, loss = 0.03147990\n",
      "Iteration 172, loss = 0.02942933\n",
      "Iteration 191, loss = 0.03138723\n",
      "Iteration 173, loss = 0.02802981\n",
      "Iteration 192, loss = 0.03219204\n",
      "Iteration 174, loss = 0.02673889\n",
      "Iteration 193, loss = 0.03052021\n",
      "Iteration 175, loss = 0.02659609\n",
      "Iteration 194, loss = 0.02988719\n",
      "Iteration 176, loss = 0.02850903\n",
      "Iteration 195, loss = 0.03015804\n",
      "Iteration 177, loss = 0.02684645\n",
      "Iteration 196, loss = 0.03028845\n",
      "Iteration 178, loss = 0.03733349\n",
      "Iteration 197, loss = 0.03123197\n",
      "Iteration 179, loss = 0.03096097\n",
      "Iteration 180, loss = 0.03449011\n",
      "Iteration 198, loss = 0.03023915\n",
      "Iteration 181, loss = 0.03058295\n",
      "Iteration 182, loss = 0.02662452\n",
      "Iteration 183, loss = 0.02401339\n",
      "Iteration 184, loss = 0.02236797\n",
      "Iteration 199, loss = 0.03029701\n",
      "Iteration 185, loss = 0.02438450\n",
      "Iteration 186, loss = 0.02398402\n",
      "Iteration 187, loss = 0.02231985\n",
      "Iteration 200, loss = 0.02843784\n",
      "Iteration 201, loss = 0.02958544\n",
      "Iteration 188, loss = 0.02279000\n",
      "Iteration 202, loss = 0.02876910\n",
      "Iteration 203, loss = 0.02857922\n",
      "Iteration 189, loss = 0.02218042\n",
      "Iteration 190, loss = 0.02201762\n",
      "Iteration 191, loss = 0.02328746\n",
      "Iteration 204, loss = 0.02861128\n",
      "Iteration 192, loss = 0.02127673\n",
      "Iteration 205, loss = 0.02725264\n",
      "Iteration 193, loss = 0.02077461\n",
      "Iteration 206, loss = 0.02797512\n",
      "Iteration 194, loss = 0.02079873\n",
      "Iteration 207, loss = 0.02739540\n",
      "Iteration 195, loss = 0.02070719\n",
      "Iteration 196, loss = 0.02012667\n",
      "Iteration 197, loss = 0.02083165\n",
      "Iteration 208, loss = 0.02676453\n",
      "Iteration 198, loss = 0.02139597\n",
      "Iteration 199, loss = 0.02026376\n",
      "Iteration 209, loss = 0.02686352\n",
      "Iteration 210, loss = 0.02806286\n",
      "Iteration 200, loss = 0.02010373\n",
      "Iteration 211, loss = 0.03025227\n",
      "Iteration 201, loss = 0.02061757\n",
      "Iteration 202, loss = 0.02585686\n",
      "Iteration 212, loss = 0.02930215\n",
      "Iteration 203, loss = 0.03110012\n",
      "Iteration 204, loss = 0.02824988\n",
      "Iteration 213, loss = 0.02675154\n",
      "Iteration 205, loss = 0.03300456\n",
      "Iteration 214, loss = 0.02973828\n",
      "Iteration 206, loss = 0.02666646\n",
      "Iteration 215, loss = 0.02745190\n",
      "Iteration 207, loss = 0.02288254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 216, loss = 0.03326299\n",
      "Iteration 217, loss = 0.04242300\n",
      "Iteration 218, loss = 0.05655032\n",
      "Iteration 219, loss = 0.04045651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88293252\n",
      "Iteration 2, loss = 0.66150623\n",
      "Iteration 3, loss = 0.54241449\n",
      "Iteration 4, loss = 0.47876807\n",
      "Iteration 5, loss = 0.43733952\n",
      "Iteration 6, loss = 0.40686303\n",
      "Iteration 7, loss = 0.38293777\n",
      "Iteration 1, loss = 1.49142183\n",
      "Iteration 8, loss = 0.36457475\n",
      "Iteration 2, loss = 0.95437382\n",
      "Iteration 9, loss = 0.34964424\n",
      "Iteration 10, loss = 0.33590590\n",
      "Iteration 3, loss = 0.75634023\n",
      "Iteration 11, loss = 0.32295072\n",
      "Iteration 4, loss = 0.61878190\n",
      "Iteration 12, loss = 0.31084556\n",
      "Iteration 13, loss = 0.30064117\n",
      "Iteration 14, loss = 0.29037338\n",
      "Iteration 5, loss = 0.54933688\n",
      "Iteration 15, loss = 0.28124486\n",
      "Iteration 6, loss = 0.49523429\n",
      "Iteration 16, loss = 0.27115191\n",
      "Iteration 7, loss = 0.46242067\n",
      "Iteration 17, loss = 0.26314913\n",
      "Iteration 8, loss = 0.43589968\n",
      "Iteration 18, loss = 0.25604948\n",
      "Iteration 9, loss = 0.41645553\n",
      "Iteration 19, loss = 0.24755978\n",
      "Iteration 10, loss = 0.40093007\n",
      "Iteration 11, loss = 0.38630933\n",
      "Iteration 12, loss = 0.37329800\n",
      "Iteration 20, loss = 0.24084232\n",
      "Iteration 13, loss = 0.36142553\n",
      "Iteration 14, loss = 0.35181718\n",
      "Iteration 21, loss = 0.23441401\n",
      "Iteration 15, loss = 0.34151649\n",
      "Iteration 22, loss = 0.22948082\n",
      "Iteration 16, loss = 0.33224575\n",
      "Iteration 23, loss = 0.22478631\n",
      "Iteration 17, loss = 0.32346712\n",
      "Iteration 24, loss = 0.21998989\n",
      "Iteration 25, loss = 0.21239602\n",
      "Iteration 18, loss = 0.31483632\n",
      "Iteration 26, loss = 0.20883418\n",
      "Iteration 19, loss = 0.30799322\n",
      "Iteration 27, loss = 0.20407353\n",
      "Iteration 20, loss = 0.29993466\n",
      "Iteration 28, loss = 0.19964280\n",
      "Iteration 21, loss = 0.29266170\n",
      "Iteration 29, loss = 0.19766811\n",
      "Iteration 30, loss = 0.19008030\n",
      "Iteration 31, loss = 0.18629584\n",
      "Iteration 22, loss = 0.28657707\n",
      "Iteration 23, loss = 0.28050778\n",
      "Iteration 32, loss = 0.18205067\n",
      "Iteration 24, loss = 0.27379679\n",
      "Iteration 33, loss = 0.17753906\n",
      "Iteration 25, loss = 0.26779254\n",
      "Iteration 34, loss = 0.17437761\n",
      "Iteration 35, loss = 0.17087102\n",
      "Iteration 26, loss = 0.26060145\n",
      "Iteration 36, loss = 0.16703107\n",
      "Iteration 27, loss = 0.25529405\n",
      "Iteration 28, loss = 0.24952529\n",
      "Iteration 37, loss = 0.16374069\n",
      "Iteration 29, loss = 0.24398028\n",
      "Iteration 38, loss = 0.15980890\n",
      "Iteration 30, loss = 0.23917844\n",
      "Iteration 39, loss = 0.15791724\n",
      "Iteration 31, loss = 0.23408939\n",
      "Iteration 32, loss = 0.23080014\n",
      "Iteration 40, loss = 0.15454832\n",
      "Iteration 33, loss = 0.22674746\n",
      "Iteration 34, loss = 0.22417932\n",
      "Iteration 35, loss = 0.21692094\n",
      "Iteration 41, loss = 0.15127842\n",
      "Iteration 36, loss = 0.21318274\n",
      "Iteration 37, loss = 0.20813138\n",
      "Iteration 42, loss = 0.14832181\n",
      "Iteration 38, loss = 0.20470483\n",
      "Iteration 43, loss = 0.14787955\n",
      "Iteration 39, loss = 0.20228680\n",
      "Iteration 44, loss = 0.14274761\n",
      "Iteration 40, loss = 0.19840906\n",
      "Iteration 45, loss = 0.14278964\n",
      "Iteration 41, loss = 0.19430130\n",
      "Iteration 46, loss = 0.14068906\n",
      "Iteration 42, loss = 0.19000589\n",
      "Iteration 47, loss = 0.13869252\n",
      "Iteration 48, loss = 0.13064319\n",
      "Iteration 43, loss = 0.18616028\n",
      "Iteration 49, loss = 0.13223715\n",
      "Iteration 44, loss = 0.18303672\n",
      "Iteration 50, loss = 0.13076034\n",
      "Iteration 45, loss = 0.17927087\n",
      "Iteration 51, loss = 0.12783191\n",
      "Iteration 46, loss = 0.17679179\n",
      "Iteration 52, loss = 0.12376161\n",
      "Iteration 47, loss = 0.17314887\n",
      "Iteration 48, loss = 0.17105024\n",
      "Iteration 53, loss = 0.12124098\n",
      "Iteration 49, loss = 0.17201488\n",
      "Iteration 54, loss = 0.11986116\n",
      "Iteration 55, loss = 0.11746276\n",
      "Iteration 50, loss = 0.16533675\n",
      "Iteration 56, loss = 0.11448801\n",
      "Iteration 51, loss = 0.16390865\n",
      "Iteration 57, loss = 0.11527845\n",
      "Iteration 52, loss = 0.15996416\n",
      "Iteration 58, loss = 0.11254323\n",
      "Iteration 53, loss = 0.15566575\n",
      "Iteration 59, loss = 0.11336099\n",
      "Iteration 54, loss = 0.15713900\n",
      "Iteration 60, loss = 0.10939019\n",
      "Iteration 55, loss = 0.15144815\n",
      "Iteration 61, loss = 0.10851893\n",
      "Iteration 56, loss = 0.14894633\n",
      "Iteration 62, loss = 0.11140657\n",
      "Iteration 57, loss = 0.14794931\n",
      "Iteration 63, loss = 0.10621241\n",
      "Iteration 58, loss = 0.14561480\n",
      "Iteration 64, loss = 0.10606756\n",
      "Iteration 59, loss = 0.14351819\n",
      "Iteration 65, loss = 0.10104483\n",
      "Iteration 60, loss = 0.14311362\n",
      "Iteration 66, loss = 0.09832021\n",
      "Iteration 61, loss = 0.14016540\n",
      "Iteration 67, loss = 0.09919994\n",
      "Iteration 62, loss = 0.13821378\n",
      "Iteration 68, loss = 0.09541685\n",
      "Iteration 69, loss = 0.09581269\n",
      "Iteration 63, loss = 0.13632738\n",
      "Iteration 64, loss = 0.13316372\n",
      "Iteration 70, loss = 0.09561626\n",
      "Iteration 65, loss = 0.12971855\n",
      "Iteration 71, loss = 0.09286444\n",
      "Iteration 66, loss = 0.12823522\n",
      "Iteration 72, loss = 0.09153564\n",
      "Iteration 67, loss = 0.12732748\n",
      "Iteration 73, loss = 0.08926303\n",
      "Iteration 68, loss = 0.12515611\n",
      "Iteration 74, loss = 0.08932043\n",
      "Iteration 69, loss = 0.12386500\n",
      "Iteration 75, loss = 0.08793640\n",
      "Iteration 76, loss = 0.08475917\n",
      "Iteration 70, loss = 0.12068158\n",
      "Iteration 77, loss = 0.08272179\n",
      "Iteration 71, loss = 0.12031529\n",
      "Iteration 78, loss = 0.08303284\n",
      "Iteration 72, loss = 0.12077243\n",
      "Iteration 79, loss = 0.08381106\n",
      "Iteration 73, loss = 0.11875728\n",
      "Iteration 80, loss = 0.08132626\n",
      "Iteration 74, loss = 0.11451352\n",
      "Iteration 81, loss = 0.08036320\n",
      "Iteration 75, loss = 0.11429458\n",
      "Iteration 82, loss = 0.08370829\n",
      "Iteration 76, loss = 0.11245407\n",
      "Iteration 83, loss = 0.08851221\n",
      "Iteration 77, loss = 0.11029362\n",
      "Iteration 78, loss = 0.10909331\n",
      "Iteration 84, loss = 0.08126521\n",
      "Iteration 79, loss = 0.10961742\n",
      "Iteration 85, loss = 0.07550725\n",
      "Iteration 80, loss = 0.10758513\n",
      "Iteration 86, loss = 0.07378576\n",
      "Iteration 81, loss = 0.10461835\n",
      "Iteration 87, loss = 0.07379855\n",
      "Iteration 82, loss = 0.10378538\n",
      "Iteration 88, loss = 0.06987129\n",
      "Iteration 89, loss = 0.06943583\n",
      "Iteration 90, loss = 0.06798431\n",
      "Iteration 83, loss = 0.10455126\n",
      "Iteration 91, loss = 0.06736007\n",
      "Iteration 84, loss = 0.09953306\n",
      "Iteration 92, loss = 0.06872566\n",
      "Iteration 85, loss = 0.09894601\n",
      "Iteration 93, loss = 0.06593035\n",
      "Iteration 86, loss = 0.09775970\n",
      "Iteration 94, loss = 0.06566512\n",
      "Iteration 87, loss = 0.09672126\n",
      "Iteration 95, loss = 0.06440440\n",
      "Iteration 88, loss = 0.09417408\n",
      "Iteration 96, loss = 0.06389556\n",
      "Iteration 97, loss = 0.06430381\n",
      "Iteration 89, loss = 0.09471910\n",
      "Iteration 98, loss = 0.06467241\n",
      "Iteration 90, loss = 0.09333080\n",
      "Iteration 99, loss = 0.07413416\n",
      "Iteration 91, loss = 0.09090078\n",
      "Iteration 100, loss = 0.07383415\n",
      "Iteration 92, loss = 0.09059848\n",
      "Iteration 101, loss = 0.06835407\n",
      "Iteration 102, loss = 0.05922678\n",
      "Iteration 93, loss = 0.09137597\n",
      "Iteration 103, loss = 0.05924724\n",
      "Iteration 104, loss = 0.05631329\n",
      "Iteration 94, loss = 0.08780414\n",
      "Iteration 95, loss = 0.08693064\n",
      "Iteration 96, loss = 0.08538000\n",
      "Iteration 105, loss = 0.05511063\n",
      "Iteration 97, loss = 0.08396554\n",
      "Iteration 106, loss = 0.05559718\n",
      "Iteration 107, loss = 0.05482598\n",
      "Iteration 98, loss = 0.08490657\n",
      "Iteration 108, loss = 0.05383402\n",
      "Iteration 99, loss = 0.08228576\n",
      "Iteration 100, loss = 0.08092156\n",
      "Iteration 101, loss = 0.08128514\n",
      "Iteration 109, loss = 0.05267943\n",
      "Iteration 110, loss = 0.05350330\n",
      "Iteration 102, loss = 0.08120358Iteration 111, loss = 0.05690230\n",
      "\n",
      "Iteration 103, loss = 0.07821207\n",
      "Iteration 112, loss = 0.05559525\n",
      "Iteration 104, loss = 0.07733175\n",
      "Iteration 105, loss = 0.07613396\n",
      "Iteration 113, loss = 0.05442715\n",
      "Iteration 106, loss = 0.07761461\n",
      "Iteration 114, loss = 0.05426086\n",
      "Iteration 115, loss = 0.05307836\n",
      "Iteration 107, loss = 0.07803545\n",
      "Iteration 116, loss = 0.04970523\n",
      "Iteration 108, loss = 0.07791457\n",
      "Iteration 109, loss = 0.07307818\n",
      "Iteration 110, loss = 0.07229787\n",
      "Iteration 117, loss = 0.05115138\n",
      "Iteration 118, loss = 0.04735437\n",
      "Iteration 119, loss = 0.04702118\n",
      "Iteration 111, loss = 0.06926497Iteration 120, loss = 0.04694838\n",
      "\n",
      "Iteration 112, loss = 0.06995516\n",
      "Iteration 113, loss = 0.06870502\n",
      "Iteration 121, loss = 0.04851414\n",
      "Iteration 114, loss = 0.06835533\n",
      "Iteration 122, loss = 0.04530962\n",
      "Iteration 123, loss = 0.04506103\n",
      "Iteration 124, loss = 0.04817587\n",
      "Iteration 115, loss = 0.06699267\n",
      "Iteration 116, loss = 0.06554618\n",
      "Iteration 125, loss = 0.04790914\n",
      "Iteration 117, loss = 0.06555063\n",
      "Iteration 118, loss = 0.06756272\n",
      "Iteration 126, loss = 0.04990438\n",
      "Iteration 127, loss = 0.04557819\n",
      "Iteration 119, loss = 0.06333414\n",
      "Iteration 128, loss = 0.04462934\n",
      "Iteration 129, loss = 0.04422499\n",
      "Iteration 130, loss = 0.04079069Iteration 120, loss = 0.06245086\n",
      "\n",
      "Iteration 131, loss = 0.04234716\n",
      "Iteration 121, loss = 0.06391778\n",
      "Iteration 132, loss = 0.03986079\n",
      "Iteration 133, loss = 0.04007073\n",
      "Iteration 122, loss = 0.06623073\n",
      "Iteration 134, loss = 0.04128154\n",
      "Iteration 123, loss = 0.06713889\n",
      "Iteration 124, loss = 0.06346260\n",
      "Iteration 135, loss = 0.04092833\n",
      "Iteration 125, loss = 0.06160864\n",
      "Iteration 126, loss = 0.06028174\n",
      "Iteration 136, loss = 0.04448303\n",
      "Iteration 127, loss = 0.06046442\n",
      "Iteration 137, loss = 0.04441717\n",
      "Iteration 128, loss = 0.05841144\n",
      "Iteration 138, loss = 0.04299043\n",
      "Iteration 139, loss = 0.04008246\n",
      "Iteration 129, loss = 0.05774889\n",
      "Iteration 130, loss = 0.05603197\n",
      "Iteration 140, loss = 0.03970568\n",
      "Iteration 131, loss = 0.05739242\n",
      "Iteration 141, loss = 0.04289786\n",
      "Iteration 132, loss = 0.05479832\n",
      "Iteration 142, loss = 0.04078537\n",
      "Iteration 143, loss = 0.03839985\n",
      "Iteration 133, loss = 0.05422115\n",
      "Iteration 144, loss = 0.03658356\n",
      "Iteration 134, loss = 0.05411974\n",
      "Iteration 145, loss = 0.03489898\n",
      "Iteration 135, loss = 0.05305486\n",
      "Iteration 146, loss = 0.03447319\n",
      "Iteration 136, loss = 0.05307791\n",
      "Iteration 137, loss = 0.05364078\n",
      "Iteration 147, loss = 0.03452765\n",
      "Iteration 148, loss = 0.03490494\n",
      "Iteration 138, loss = 0.05151750\n",
      "Iteration 139, loss = 0.05133028\n",
      "Iteration 149, loss = 0.03320358\n",
      "Iteration 140, loss = 0.05103088\n",
      "Iteration 150, loss = 0.03455166\n",
      "Iteration 141, loss = 0.04952124\n",
      "Iteration 151, loss = 0.03631046\n",
      "Iteration 142, loss = 0.04947715\n",
      "Iteration 143, loss = 0.05054334\n",
      "Iteration 152, loss = 0.03427046\n",
      "Iteration 144, loss = 0.04810487\n",
      "Iteration 145, loss = 0.04817695\n",
      "Iteration 153, loss = 0.03516665\n",
      "Iteration 154, loss = 0.03650619\n",
      "Iteration 155, loss = 0.03760043\n",
      "Iteration 146, loss = 0.04722604\n",
      "Iteration 156, loss = 0.03466215\n",
      "Iteration 147, loss = 0.04717922\n",
      "Iteration 157, loss = 0.03362969\n",
      "Iteration 148, loss = 0.04837021\n",
      "Iteration 158, loss = 0.03171544\n",
      "Iteration 149, loss = 0.04860835\n",
      "Iteration 159, loss = 0.03358017\n",
      "Iteration 160, loss = 0.03229499\n",
      "Iteration 150, loss = 0.04632845\n",
      "Iteration 161, loss = 0.03238095\n",
      "Iteration 151, loss = 0.04552914\n",
      "Iteration 152, loss = 0.04794448\n",
      "Iteration 162, loss = 0.03143015\n",
      "Iteration 153, loss = 0.04685386\n",
      "Iteration 163, loss = 0.03432963\n",
      "Iteration 164, loss = 0.03205615\n",
      "Iteration 154, loss = 0.04612554\n",
      "Iteration 165, loss = 0.02945693\n",
      "Iteration 155, loss = 0.04449946\n",
      "Iteration 156, loss = 0.04481895\n",
      "Iteration 166, loss = 0.02949766\n",
      "Iteration 157, loss = 0.04289650\n",
      "Iteration 167, loss = 0.03033003\n",
      "Iteration 158, loss = 0.04416528\n",
      "Iteration 168, loss = 0.03688668\n",
      "Iteration 169, loss = 0.03755351\n",
      "Iteration 159, loss = 0.04185605\n",
      "Iteration 170, loss = 0.04337885\n",
      "Iteration 160, loss = 0.04189554\n",
      "Iteration 171, loss = 0.03428358\n",
      "Iteration 161, loss = 0.04333237\n",
      "Iteration 162, loss = 0.04325503\n",
      "Iteration 172, loss = 0.02759476\n",
      "Iteration 163, loss = 0.04124933\n",
      "Iteration 173, loss = 0.02764026\n",
      "Iteration 164, loss = 0.04034155\n",
      "Iteration 174, loss = 0.02764073\n",
      "Iteration 165, loss = 0.04161281\n",
      "Iteration 175, loss = 0.02806337\n",
      "Iteration 166, loss = 0.04048008\n",
      "Iteration 176, loss = 0.02720552\n",
      "Iteration 177, loss = 0.03392453\n",
      "Iteration 167, loss = 0.04134274\n",
      "Iteration 168, loss = 0.04006502\n",
      "Iteration 178, loss = 0.03110487\n",
      "Iteration 169, loss = 0.03787560\n",
      "Iteration 170, loss = 0.04131640\n",
      "Iteration 179, loss = 0.02974109\n",
      "Iteration 180, loss = 0.03144728\n",
      "Iteration 171, loss = 0.04240734\n",
      "Iteration 172, loss = 0.03968337\n",
      "Iteration 181, loss = 0.03264666\n",
      "Iteration 173, loss = 0.03807453\n",
      "Iteration 174, loss = 0.03794931\n",
      "Iteration 175, loss = 0.03721272\n",
      "Iteration 182, loss = 0.02894016\n",
      "Iteration 183, loss = 0.02824026\n",
      "Iteration 176, loss = 0.03779019\n",
      "Iteration 184, loss = 0.02651504\n",
      "Iteration 177, loss = 0.03866376\n",
      "Iteration 178, loss = 0.04004559\n",
      "Iteration 185, loss = 0.02640327\n",
      "Iteration 179, loss = 0.04002597\n",
      "Iteration 180, loss = 0.04275210\n",
      "Iteration 186, loss = 0.02438570\n",
      "Iteration 181, loss = 0.04008991\n",
      "Iteration 187, loss = 0.02814236\n",
      "Iteration 188, loss = 0.02533116\n",
      "Iteration 189, loss = 0.02476693\n",
      "Iteration 182, loss = 0.03781186\n",
      "Iteration 183, loss = 0.03578864\n",
      "Iteration 190, loss = 0.02516409\n",
      "Iteration 184, loss = 0.03682344\n",
      "Iteration 185, loss = 0.03418864\n",
      "Iteration 191, loss = 0.02811423\n",
      "Iteration 192, loss = 0.02854089\n",
      "Iteration 193, loss = 0.02667163\n",
      "Iteration 186, loss = 0.03450476\n",
      "Iteration 194, loss = 0.02680805\n",
      "Iteration 187, loss = 0.03587998\n",
      "Iteration 188, loss = 0.03467986\n",
      "Iteration 189, loss = 0.03341918\n",
      "Iteration 195, loss = 0.02621820\n",
      "Iteration 196, loss = 0.02439355\n",
      "Iteration 190, loss = 0.03272498\n",
      "Iteration 197, loss = 0.02322397\n",
      "Iteration 198, loss = 0.02471707\n",
      "Iteration 199, loss = 0.02486106\n",
      "Iteration 200, loss = 0.02411983\n",
      "Iteration 191, loss = 0.03317127\n",
      "Iteration 192, loss = 0.03445253\n",
      "Iteration 201, loss = 0.02708340\n",
      "Iteration 193, loss = 0.03272130\n",
      "Iteration 194, loss = 0.03196463\n",
      "Iteration 202, loss = 0.02300101\n",
      "Iteration 195, loss = 0.03456627\n",
      "Iteration 203, loss = 0.02612037\n",
      "Iteration 204, loss = 0.02428353\n",
      "Iteration 196, loss = 0.03558328\n",
      "Iteration 205, loss = 0.02069850\n",
      "Iteration 197, loss = 0.04250742\n",
      "Iteration 206, loss = 0.02854856\n",
      "Iteration 198, loss = 0.03811590\n",
      "Iteration 207, loss = 0.03027273\n",
      "Iteration 199, loss = 0.03491439\n",
      "Iteration 208, loss = 0.03708997\n",
      "Iteration 200, loss = 0.03553292\n",
      "Iteration 201, loss = 0.03294031\n",
      "Iteration 209, loss = 0.02560032\n",
      "Iteration 202, loss = 0.03021987\n",
      "Iteration 210, loss = 0.02956975\n",
      "Iteration 203, loss = 0.03047496\n",
      "Iteration 211, loss = 0.02187335\n",
      "Iteration 204, loss = 0.03108134\n",
      "Iteration 212, loss = 0.02888290\n",
      "Iteration 205, loss = 0.03013385\n",
      "Iteration 213, loss = 0.03018914\n",
      "Iteration 206, loss = 0.02966008\n",
      "Iteration 207, loss = 0.02911247\n",
      "Iteration 214, loss = 0.03282891\n",
      "Iteration 208, loss = 0.02903380\n",
      "Iteration 215, loss = 0.02742197\n",
      "Iteration 216, loss = 0.02492452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 209, loss = 0.02919603\n",
      "Iteration 210, loss = 0.02851421\n",
      "Iteration 211, loss = 0.02908691\n",
      "Iteration 212, loss = 0.02864166\n",
      "Iteration 213, loss = 0.02872650\n",
      "Iteration 214, loss = 0.02853081\n",
      "Iteration 215, loss = 0.02905920\n",
      "Iteration 1, loss = 1.22883022\n",
      "Iteration 2, loss = 0.78152436\n",
      "Iteration 3, loss = 0.59852876\n",
      "Iteration 216, loss = 0.02812770\n",
      "Iteration 4, loss = 0.51310160\n",
      "Iteration 217, loss = 0.02808747\n",
      "Iteration 218, loss = 0.02769487\n",
      "Iteration 219, loss = 0.02915984\n",
      "Iteration 220, loss = 0.02796632\n",
      "Iteration 5, loss = 0.47258249\n",
      "Iteration 221, loss = 0.02779537\n",
      "Iteration 222, loss = 0.02714464\n",
      "Iteration 6, loss = 0.44407376\n",
      "Iteration 223, loss = 0.02688365\n",
      "Iteration 7, loss = 0.41830284\n",
      "Iteration 8, loss = 0.39779596\n",
      "Iteration 224, loss = 0.02660476\n",
      "Iteration 9, loss = 0.38247067\n",
      "Iteration 10, loss = 0.36850805\n",
      "Iteration 11, loss = 0.35574310\n",
      "Iteration 12, loss = 0.34524703\n",
      "Iteration 225, loss = 0.02610836\n",
      "Iteration 13, loss = 0.33278442\n",
      "Iteration 14, loss = 0.32468328\n",
      "Iteration 15, loss = 0.31228773\n",
      "Iteration 16, loss = 0.30846684\n",
      "Iteration 226, loss = 0.02726222\n",
      "Iteration 17, loss = 0.29560288\n",
      "Iteration 18, loss = 0.29084031\n",
      "Iteration 19, loss = 0.28035346\n",
      "Iteration 227, loss = 0.02783690\n",
      "Iteration 20, loss = 0.27120750\n",
      "Iteration 21, loss = 0.26531839\n",
      "Iteration 228, loss = 0.02798247\n",
      "Iteration 22, loss = 0.25769893\n",
      "Iteration 229, loss = 0.02691038\n",
      "Iteration 23, loss = 0.25228971\n",
      "Iteration 230, loss = 0.02572075\n",
      "Iteration 24, loss = 0.24414733\n",
      "Iteration 231, loss = 0.02589951\n",
      "Iteration 25, loss = 0.24003690\n",
      "Iteration 232, loss = 0.02614883\n",
      "Iteration 26, loss = 0.23573074\n",
      "Iteration 233, loss = 0.02556620\n",
      "Iteration 27, loss = 0.23024360\n",
      "Iteration 234, loss = 0.02610274\n",
      "Iteration 235, loss = 0.02867032\n",
      "Iteration 28, loss = 0.22316775\n",
      "Iteration 236, loss = 0.02949738\n",
      "Iteration 29, loss = 0.21962962\n",
      "Iteration 237, loss = 0.02675257\n",
      "Iteration 30, loss = 0.21410782\n",
      "Iteration 238, loss = 0.02724482\n",
      "Iteration 31, loss = 0.20711761\n",
      "Iteration 32, loss = 0.20438256\n",
      "Iteration 239, loss = 0.02436280\n",
      "Iteration 240, loss = 0.02637790\n",
      "Iteration 33, loss = 0.20045832\n",
      "Iteration 241, loss = 0.02592035\n",
      "Iteration 34, loss = 0.19478764\n",
      "Iteration 35, loss = 0.19298794\n",
      "Iteration 242, loss = 0.02433111\n",
      "Iteration 36, loss = 0.19008880\n",
      "Iteration 243, loss = 0.02563922\n",
      "Iteration 37, loss = 0.18390717\n",
      "Iteration 244, loss = 0.02495257\n",
      "Iteration 38, loss = 0.17889422\n",
      "Iteration 245, loss = 0.02601579\n",
      "Iteration 39, loss = 0.17687898\n",
      "Iteration 246, loss = 0.02613320\n",
      "Iteration 40, loss = 0.17278376\n",
      "Iteration 247, loss = 0.02663653\n",
      "Iteration 41, loss = 0.16823499\n",
      "Iteration 248, loss = 0.02705332\n",
      "Iteration 42, loss = 0.16488229\n",
      "Iteration 249, loss = 0.02365000\n",
      "Iteration 43, loss = 0.16250015\n",
      "Iteration 44, loss = 0.15893797\n",
      "Iteration 250, loss = 0.02515788\n",
      "Iteration 45, loss = 0.15683875\n",
      "Iteration 251, loss = 0.02730781\n",
      "Iteration 252, loss = 0.02807124\n",
      "Iteration 46, loss = 0.15209338\n",
      "Iteration 253, loss = 0.02449098\n",
      "Iteration 47, loss = 0.15063771\n",
      "Iteration 48, loss = 0.14934710\n",
      "Iteration 254, loss = 0.02397734\n",
      "Iteration 49, loss = 0.14588699\n",
      "Iteration 255, loss = 0.02256987\n",
      "Iteration 50, loss = 0.14191894\n",
      "Iteration 256, loss = 0.02293061\n",
      "Iteration 51, loss = 0.13956512\n",
      "Iteration 257, loss = 0.02307071\n",
      "Iteration 52, loss = 0.13990713\n",
      "Iteration 258, loss = 0.02279880\n",
      "Iteration 53, loss = 0.13468289\n",
      "Iteration 259, loss = 0.02369296\n",
      "Iteration 54, loss = 0.13167008\n",
      "Iteration 55, loss = 0.13096237\n",
      "Iteration 260, loss = 0.02401355\n",
      "Iteration 261, loss = 0.02252301\n",
      "Iteration 56, loss = 0.13067637\n",
      "Iteration 262, loss = 0.02187853\n",
      "Iteration 57, loss = 0.12567627\n",
      "Iteration 263, loss = 0.02281285\n",
      "Iteration 58, loss = 0.12284416\n",
      "Iteration 264, loss = 0.02504088\n",
      "Iteration 265, loss = 0.02331458\n",
      "Iteration 266, loss = 0.02179998\n",
      "Iteration 59, loss = 0.12107624\n",
      "Iteration 267, loss = 0.02244904\n",
      "Iteration 268, loss = 0.02196958\n",
      "Iteration 269, loss = 0.02087590\n",
      "Iteration 60, loss = 0.11854000\n",
      "Iteration 270, loss = 0.02159298\n",
      "Iteration 61, loss = 0.11847000\n",
      "Iteration 62, loss = 0.11464411\n",
      "Iteration 63, loss = 0.11142835\n",
      "Iteration 271, loss = 0.02202268\n",
      "Iteration 64, loss = 0.11033326\n",
      "Iteration 272, loss = 0.02159092\n",
      "Iteration 65, loss = 0.10897726\n",
      "Iteration 273, loss = 0.02069168\n",
      "Iteration 66, loss = 0.10675112\n",
      "Iteration 274, loss = 0.02103440\n",
      "Iteration 275, loss = 0.02088104\n",
      "Iteration 67, loss = 0.10751937\n",
      "Iteration 276, loss = 0.02085548\n",
      "Iteration 277, loss = 0.02236206\n",
      "Iteration 68, loss = 0.10566959\n",
      "Iteration 278, loss = 0.02152629\n",
      "Iteration 69, loss = 0.10126995\n",
      "Iteration 279, loss = 0.02044074\n",
      "Iteration 70, loss = 0.09990212\n",
      "Iteration 71, loss = 0.09939034\n",
      "Iteration 72, loss = 0.09895526\n",
      "Iteration 73, loss = 0.09840810\n",
      "Iteration 280, loss = 0.01972767\n",
      "Iteration 74, loss = 0.09512901\n",
      "Iteration 281, loss = 0.02096050\n",
      "Iteration 282, loss = 0.02015965\n",
      "Iteration 75, loss = 0.09193169\n",
      "Iteration 283, loss = 0.02046791\n",
      "Iteration 76, loss = 0.09122900\n",
      "Iteration 77, loss = 0.09153148\n",
      "Iteration 78, loss = 0.08749094\n",
      "Iteration 284, loss = 0.01995654\n",
      "Iteration 285, loss = 0.01966552\n",
      "Iteration 79, loss = 0.08686054\n",
      "Iteration 286, loss = 0.02171099\n",
      "Iteration 287, loss = 0.02298866\n",
      "Iteration 80, loss = 0.08613427\n",
      "Iteration 288, loss = 0.02375549\n",
      "Iteration 81, loss = 0.08312084\n",
      "Iteration 82, loss = 0.08179445\n",
      "Iteration 289, loss = 0.02970226\n",
      "Iteration 83, loss = 0.08257335\n",
      "Iteration 290, loss = 0.02458517\n",
      "Iteration 291, loss = 0.02637757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 84, loss = 0.07889947\n",
      "Iteration 85, loss = 0.07597235\n",
      "Iteration 86, loss = 0.07626614\n",
      "Iteration 87, loss = 0.07681836\n",
      "Iteration 88, loss = 0.07206560\n",
      "Iteration 89, loss = 0.07275077\n",
      "Iteration 90, loss = 0.07095239\n",
      "Iteration 91, loss = 0.06938449\n",
      "Iteration 92, loss = 0.07116778\n",
      "Iteration 1, loss = 0.97321445\n",
      "Iteration 2, loss = 0.68497939\n",
      "Iteration 93, loss = 0.06972179\n",
      "Iteration 94, loss = 0.06547757\n",
      "Iteration 3, loss = 0.56267838\n",
      "Iteration 95, loss = 0.06569115\n",
      "Iteration 96, loss = 0.06612073\n",
      "Iteration 4, loss = 0.49685673\n",
      "Iteration 97, loss = 0.06488180\n",
      "Iteration 5, loss = 0.45250287\n",
      "Iteration 98, loss = 0.06281962\n",
      "Iteration 6, loss = 0.42221179\n",
      "Iteration 99, loss = 0.06088857\n",
      "Iteration 7, loss = 0.39875792\n",
      "Iteration 8, loss = 0.38186305\n",
      "Iteration 9, loss = 0.36253484\n",
      "Iteration 100, loss = 0.06006063\n",
      "Iteration 10, loss = 0.34790036\n",
      "Iteration 101, loss = 0.06097963\n",
      "Iteration 11, loss = 0.33761380\n",
      "Iteration 102, loss = 0.06162585\n",
      "Iteration 12, loss = 0.32466000\n",
      "Iteration 103, loss = 0.05959055\n",
      "Iteration 13, loss = 0.31481557\n",
      "Iteration 104, loss = 0.06148681\n",
      "Iteration 14, loss = 0.30492598\n",
      "Iteration 105, loss = 0.05500988\n",
      "Iteration 106, loss = 0.05411220\n",
      "Iteration 15, loss = 0.29537500\n",
      "Iteration 16, loss = 0.28747658\n",
      "Iteration 107, loss = 0.05434233\n",
      "Iteration 17, loss = 0.28117132\n",
      "Iteration 18, loss = 0.27166039\n",
      "Iteration 19, loss = 0.26524207\n",
      "Iteration 20, loss = 0.25850671\n",
      "Iteration 108, loss = 0.05772678\n",
      "Iteration 21, loss = 0.25037271\n",
      "Iteration 22, loss = 0.24329974\n",
      "Iteration 109, loss = 0.05448295\n",
      "Iteration 23, loss = 0.23780964\n",
      "Iteration 24, loss = 0.23286781\n",
      "Iteration 110, loss = 0.05582265\n",
      "Iteration 25, loss = 0.22565683\n",
      "Iteration 111, loss = 0.05166554\n",
      "Iteration 26, loss = 0.22179736\n",
      "Iteration 112, loss = 0.04866008\n",
      "Iteration 113, loss = 0.04866362\n",
      "Iteration 27, loss = 0.21574396\n",
      "Iteration 114, loss = 0.05220462\n",
      "Iteration 28, loss = 0.21248572\n",
      "Iteration 115, loss = 0.05084936\n",
      "Iteration 29, loss = 0.20627130\n",
      "Iteration 30, loss = 0.20233210\n",
      "Iteration 31, loss = 0.19757908\n",
      "Iteration 116, loss = 0.04885691\n",
      "Iteration 32, loss = 0.19335542\n",
      "Iteration 33, loss = 0.19388911\n",
      "Iteration 34, loss = 0.19231154\n",
      "Iteration 35, loss = 0.18172426\n",
      "Iteration 117, loss = 0.04820997\n",
      "Iteration 118, loss = 0.04642816\n",
      "Iteration 36, loss = 0.18109962\n",
      "Iteration 119, loss = 0.04439771\n",
      "Iteration 120, loss = 0.04286903\n",
      "Iteration 121, loss = 0.04333661\n",
      "Iteration 37, loss = 0.17753685\n",
      "Iteration 122, loss = 0.04183882\n",
      "Iteration 123, loss = 0.04138433\n",
      "Iteration 124, loss = 0.04113323\n",
      "Iteration 38, loss = 0.17560871\n",
      "Iteration 125, loss = 0.04007043\n",
      "Iteration 126, loss = 0.04374737\n",
      "Iteration 127, loss = 0.04347719\n",
      "Iteration 39, loss = 0.17058526\n",
      "Iteration 40, loss = 0.16584751\n",
      "Iteration 128, loss = 0.04037228\n",
      "Iteration 41, loss = 0.16662913\n",
      "Iteration 42, loss = 0.16323353\n",
      "Iteration 129, loss = 0.04060516\n",
      "Iteration 130, loss = 0.04237386\n",
      "Iteration 43, loss = 0.16020379\n",
      "Iteration 131, loss = 0.03858083\n",
      "Iteration 44, loss = 0.15617120\n",
      "Iteration 45, loss = 0.15203372\n",
      "Iteration 132, loss = 0.03571203\n",
      "Iteration 46, loss = 0.15050638\n",
      "Iteration 133, loss = 0.03534242\n",
      "Iteration 47, loss = 0.14744989\n",
      "Iteration 134, loss = 0.03474592\n",
      "Iteration 48, loss = 0.14487833\n",
      "Iteration 135, loss = 0.03606167\n",
      "Iteration 49, loss = 0.14538224\n",
      "Iteration 136, loss = 0.03509775\n",
      "Iteration 50, loss = 0.14315222\n",
      "Iteration 137, loss = 0.03347355\n",
      "Iteration 51, loss = 0.14130031\n",
      "Iteration 138, loss = 0.03406416\n",
      "Iteration 52, loss = 0.13974364\n",
      "Iteration 139, loss = 0.03588203\n",
      "Iteration 140, loss = 0.03429488\n",
      "Iteration 141, loss = 0.03370057\n",
      "Iteration 53, loss = 0.13501007\n",
      "Iteration 142, loss = 0.03340570\n",
      "Iteration 143, loss = 0.03272302\n",
      "Iteration 144, loss = 0.03055192\n",
      "Iteration 145, loss = 0.03145610\n",
      "Iteration 54, loss = 0.13481695\n",
      "Iteration 146, loss = 0.03191828\n",
      "Iteration 147, loss = 0.03060935\n",
      "Iteration 148, loss = 0.02920958\n",
      "Iteration 55, loss = 0.13138687\n",
      "Iteration 149, loss = 0.02961378\n",
      "Iteration 56, loss = 0.13060501\n",
      "Iteration 150, loss = 0.02878725\n",
      "Iteration 57, loss = 0.12849750\n",
      "Iteration 151, loss = 0.02855384\n",
      "Iteration 58, loss = 0.13060406\n",
      "Iteration 59, loss = 0.12672914\n",
      "Iteration 60, loss = 0.12125481\n",
      "Iteration 152, loss = 0.02751513\n",
      "Iteration 61, loss = 0.12479675\n",
      "Iteration 153, loss = 0.02845154\n",
      "Iteration 154, loss = 0.02915956\n",
      "Iteration 155, loss = 0.02983717\n",
      "Iteration 62, loss = 0.12057638\n",
      "Iteration 63, loss = 0.12329472\n",
      "Iteration 156, loss = 0.03631356\n",
      "Iteration 64, loss = 0.11720132\n",
      "Iteration 65, loss = 0.11371866\n",
      "Iteration 157, loss = 0.03455756\n",
      "Iteration 158, loss = 0.03377376\n",
      "Iteration 66, loss = 0.11714568\n",
      "Iteration 159, loss = 0.02961881\n",
      "Iteration 67, loss = 0.11388036\n",
      "Iteration 160, loss = 0.03143193\n",
      "Iteration 68, loss = 0.11164073\n",
      "Iteration 69, loss = 0.11220222\n",
      "Iteration 161, loss = 0.03064780\n",
      "Iteration 162, loss = 0.02828292\n",
      "Iteration 70, loss = 0.10773128\n",
      "Iteration 163, loss = 0.02533493\n",
      "Iteration 164, loss = 0.02440922\n",
      "Iteration 71, loss = 0.10896840\n",
      "Iteration 72, loss = 0.10700383\n",
      "Iteration 165, loss = 0.03047413\n",
      "Iteration 73, loss = 0.10589403\n",
      "Iteration 166, loss = 0.02555990\n",
      "Iteration 74, loss = 0.10334872\n",
      "Iteration 167, loss = 0.02259616\n",
      "Iteration 168, loss = 0.02495737\n",
      "Iteration 75, loss = 0.10274165\n",
      "Iteration 169, loss = 0.02640280\n",
      "Iteration 76, loss = 0.10216542\n",
      "Iteration 77, loss = 0.10168726\n",
      "Iteration 170, loss = 0.02489564\n",
      "Iteration 171, loss = 0.02511942\n",
      "Iteration 78, loss = 0.10127547\n",
      "Iteration 172, loss = 0.02234027\n",
      "Iteration 173, loss = 0.02244643\n",
      "Iteration 79, loss = 0.09727008\n",
      "Iteration 80, loss = 0.09861067\n",
      "Iteration 81, loss = 0.09711796\n",
      "Iteration 174, loss = 0.02234224\n",
      "Iteration 82, loss = 0.09552052\n",
      "Iteration 83, loss = 0.09586097\n",
      "Iteration 84, loss = 0.09486485\n",
      "Iteration 175, loss = 0.02278345\n",
      "Iteration 85, loss = 0.09228240\n",
      "Iteration 176, loss = 0.02128228\n",
      "Iteration 177, loss = 0.02048339\n",
      "Iteration 86, loss = 0.09159598\n",
      "Iteration 178, loss = 0.01982208\n",
      "Iteration 87, loss = 0.09052051\n",
      "Iteration 88, loss = 0.08846825\n",
      "Iteration 179, loss = 0.02016980\n",
      "Iteration 89, loss = 0.08633141\n",
      "Iteration 180, loss = 0.02073045\n",
      "Iteration 90, loss = 0.08597383\n",
      "Iteration 181, loss = 0.02425320\n",
      "Iteration 91, loss = 0.08602895\n",
      "Iteration 92, loss = 0.08427558\n",
      "Iteration 182, loss = 0.01942221\n",
      "Iteration 93, loss = 0.08590457\n",
      "Iteration 183, loss = 0.02001263\n",
      "Iteration 94, loss = 0.09472479\n",
      "Iteration 184, loss = 0.01896201\n",
      "Iteration 185, loss = 0.01907881Iteration 95, loss = 0.09410056\n",
      "\n",
      "Iteration 96, loss = 0.08656132\n",
      "Iteration 186, loss = 0.01986537\n",
      "Iteration 97, loss = 0.07779665\n",
      "Iteration 98, loss = 0.08119113\n",
      "Iteration 99, loss = 0.08732242\n",
      "Iteration 187, loss = 0.01804811\n",
      "Iteration 100, loss = 0.08219179\n",
      "Iteration 188, loss = 0.01877209\n",
      "Iteration 101, loss = 0.07745291\n",
      "Iteration 189, loss = 0.02025031\n",
      "Iteration 190, loss = 0.01964727\n",
      "Iteration 191, loss = 0.01860968\n",
      "Iteration 192, loss = 0.01715050\n",
      "Iteration 102, loss = 0.07615532\n",
      "Iteration 193, loss = 0.01708165\n",
      "Iteration 103, loss = 0.07711576\n",
      "Iteration 104, loss = 0.07497322\n",
      "Iteration 194, loss = 0.01674278\n",
      "Iteration 105, loss = 0.07562498\n",
      "Iteration 106, loss = 0.07709770\n",
      "Iteration 195, loss = 0.01727339\n",
      "Iteration 196, loss = 0.01851083\n",
      "Iteration 197, loss = 0.02022238\n",
      "Iteration 107, loss = 0.08274544\n",
      "Iteration 198, loss = 0.01824373\n",
      "Iteration 199, loss = 0.01594231\n",
      "Iteration 108, loss = 0.07449723\n",
      "Iteration 200, loss = 0.01579869\n",
      "Iteration 109, loss = 0.07235669\n",
      "Iteration 201, loss = 0.01558337\n",
      "Iteration 110, loss = 0.06838645\n",
      "Iteration 202, loss = 0.01525724\n",
      "Iteration 111, loss = 0.06799808\n",
      "Iteration 203, loss = 0.01619705\n",
      "Iteration 112, loss = 0.07017234\n",
      "Iteration 204, loss = 0.01692721\n",
      "Iteration 205, loss = 0.01658555\n",
      "Iteration 113, loss = 0.06710892\n",
      "Iteration 114, loss = 0.06680840\n",
      "Iteration 206, loss = 0.01667056\n",
      "Iteration 115, loss = 0.06591748\n",
      "Iteration 116, loss = 0.06466695\n",
      "Iteration 207, loss = 0.01468523\n",
      "Iteration 117, loss = 0.06548981\n",
      "Iteration 208, loss = 0.01482419\n",
      "Iteration 118, loss = 0.06588279\n",
      "Iteration 209, loss = 0.01496136\n",
      "Iteration 119, loss = 0.06933454\n",
      "Iteration 210, loss = 0.01614954\n",
      "Iteration 120, loss = 0.06865024\n",
      "Iteration 121, loss = 0.06326028\n",
      "Iteration 211, loss = 0.02119042\n",
      "Iteration 122, loss = 0.06151903\n",
      "Iteration 212, loss = 0.02258058\n",
      "Iteration 213, loss = 0.02610999\n",
      "Iteration 123, loss = 0.06257117\n",
      "Iteration 214, loss = 0.02364122\n",
      "Iteration 124, loss = 0.06023896\n",
      "Iteration 215, loss = 0.02147673\n",
      "Iteration 125, loss = 0.06198577\n",
      "Iteration 126, loss = 0.06180290\n",
      "Iteration 127, loss = 0.06292184\n",
      "Iteration 216, loss = 0.02316509\n",
      "Iteration 217, loss = 0.02490061\n",
      "Iteration 128, loss = 0.07096369\n",
      "Iteration 218, loss = 0.01581359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 129, loss = 0.07389481\n",
      "Iteration 130, loss = 0.06840804\n",
      "Iteration 131, loss = 0.06361278\n",
      "Iteration 132, loss = 0.05950110\n",
      "Iteration 133, loss = 0.05645677\n",
      "Iteration 134, loss = 0.05483137\n",
      "Iteration 1, loss = 1.44040483\n",
      "Iteration 135, loss = 0.05341153\n",
      "Iteration 2, loss = 0.95382901\n",
      "Iteration 3, loss = 0.72957979\n",
      "Iteration 4, loss = 0.61222619\n",
      "Iteration 136, loss = 0.05492006\n",
      "Iteration 5, loss = 0.53502910\n",
      "Iteration 137, loss = 0.05621953\n",
      "Iteration 138, loss = 0.05789914\n",
      "Iteration 139, loss = 0.05553568\n",
      "Iteration 6, loss = 0.49064972\n",
      "Iteration 140, loss = 0.05454137\n",
      "Iteration 141, loss = 0.05324063\n",
      "Iteration 7, loss = 0.45305285\n",
      "Iteration 142, loss = 0.05241603\n",
      "Iteration 8, loss = 0.42855698\n",
      "Iteration 9, loss = 0.40504196\n",
      "Iteration 143, loss = 0.05279906\n",
      "Iteration 144, loss = 0.05094721\n",
      "Iteration 10, loss = 0.38783237\n",
      "Iteration 145, loss = 0.04958726\n",
      "Iteration 11, loss = 0.37297747\n",
      "Iteration 146, loss = 0.05175124\n",
      "Iteration 12, loss = 0.35951871\n",
      "Iteration 147, loss = 0.04811073\n",
      "Iteration 13, loss = 0.34695028\n",
      "Iteration 148, loss = 0.04642911\n",
      "Iteration 14, loss = 0.33493460\n",
      "Iteration 15, loss = 0.32575973\n",
      "Iteration 16, loss = 0.31582809\n",
      "Iteration 149, loss = 0.04617617\n",
      "Iteration 150, loss = 0.04846975\n",
      "Iteration 151, loss = 0.04741771\n",
      "Iteration 17, loss = 0.30616627\n",
      "Iteration 152, loss = 0.05086294\n",
      "Iteration 18, loss = 0.29805558\n",
      "Iteration 19, loss = 0.29053915\n",
      "Iteration 153, loss = 0.05889201\n",
      "Iteration 20, loss = 0.28267060\n",
      "Iteration 154, loss = 0.05471070\n",
      "Iteration 21, loss = 0.27441570\n",
      "Iteration 155, loss = 0.05012974\n",
      "Iteration 156, loss = 0.05238149\n",
      "Iteration 22, loss = 0.26734085\n",
      "Iteration 157, loss = 0.05614352\n",
      "Iteration 23, loss = 0.26033956\n",
      "Iteration 24, loss = 0.25455474\n",
      "Iteration 25, loss = 0.24765595\n",
      "Iteration 158, loss = 0.05547262\n",
      "Iteration 159, loss = 0.04672414\n",
      "Iteration 160, loss = 0.04334422\n",
      "Iteration 161, loss = 0.04361696\n",
      "Iteration 26, loss = 0.24099226\n",
      "Iteration 27, loss = 0.23533757\n",
      "Iteration 28, loss = 0.23116244\n",
      "Iteration 162, loss = 0.04121187\n",
      "Iteration 29, loss = 0.22648628\n",
      "Iteration 163, loss = 0.04283896\n",
      "Iteration 164, loss = 0.04501685\n",
      "Iteration 30, loss = 0.22036788\n",
      "Iteration 165, loss = 0.04586481\n",
      "Iteration 31, loss = 0.21558875\n",
      "Iteration 32, loss = 0.21244966\n",
      "Iteration 166, loss = 0.05040265\n",
      "Iteration 33, loss = 0.20977883\n",
      "Iteration 167, loss = 0.04957701\n",
      "Iteration 34, loss = 0.20403488\n",
      "Iteration 168, loss = 0.04706565\n",
      "Iteration 169, loss = 0.03802298\n",
      "Iteration 35, loss = 0.19967787\n",
      "Iteration 170, loss = 0.03955482\n",
      "Iteration 36, loss = 0.19588558\n",
      "Iteration 37, loss = 0.19259243\n",
      "Iteration 171, loss = 0.03803907\n",
      "Iteration 172, loss = 0.03841200\n",
      "Iteration 38, loss = 0.18890518\n",
      "Iteration 173, loss = 0.03950130\n",
      "Iteration 39, loss = 0.18457869\n",
      "Iteration 174, loss = 0.03898485\n",
      "Iteration 40, loss = 0.18068760\n",
      "Iteration 175, loss = 0.03771364\n",
      "Iteration 41, loss = 0.17793035\n",
      "Iteration 176, loss = 0.03937078\n",
      "Iteration 42, loss = 0.17361644\n",
      "Iteration 177, loss = 0.03852577\n",
      "Iteration 43, loss = 0.16998694\n",
      "Iteration 178, loss = 0.03630176\n",
      "Iteration 44, loss = 0.16788173\n",
      "Iteration 179, loss = 0.03817331\n",
      "Iteration 180, loss = 0.03643455\n",
      "Iteration 45, loss = 0.16247253\n",
      "Iteration 46, loss = 0.16049779\n",
      "Iteration 181, loss = 0.03584532\n",
      "Iteration 47, loss = 0.15877320\n",
      "Iteration 182, loss = 0.03574020\n",
      "Iteration 48, loss = 0.15571705\n",
      "Iteration 183, loss = 0.03518841\n",
      "Iteration 184, loss = 0.03517722\n",
      "Iteration 185, loss = 0.03448053\n",
      "Iteration 49, loss = 0.15336312\n",
      "Iteration 50, loss = 0.15123971\n",
      "Iteration 51, loss = 0.14781811\n",
      "Iteration 52, loss = 0.14651699\n",
      "Iteration 186, loss = 0.03593098\n",
      "Iteration 187, loss = 0.03581549\n",
      "Iteration 188, loss = 0.03621704\n",
      "Iteration 53, loss = 0.14201115\n",
      "Iteration 189, loss = 0.03366818\n",
      "Iteration 54, loss = 0.14057253\n",
      "Iteration 55, loss = 0.13829761\n",
      "Iteration 56, loss = 0.13711480\n",
      "Iteration 190, loss = 0.03505014\n",
      "Iteration 191, loss = 0.03511887\n",
      "Iteration 57, loss = 0.13441683\n",
      "Iteration 192, loss = 0.03444437\n",
      "Iteration 193, loss = 0.03318798\n",
      "Iteration 194, loss = 0.03157179\n",
      "Iteration 58, loss = 0.13836106\n",
      "Iteration 195, loss = 0.03220053\n",
      "Iteration 196, loss = 0.03274067\n",
      "Iteration 197, loss = 0.03212425\n",
      "Iteration 198, loss = 0.03101011\n",
      "Iteration 59, loss = 0.13389912\n",
      "Iteration 199, loss = 0.03085207\n",
      "Iteration 200, loss = 0.03018654\n",
      "Iteration 60, loss = 0.13381487\n",
      "Iteration 201, loss = 0.03033498\n",
      "Iteration 202, loss = 0.03469814\n",
      "Iteration 203, loss = 0.03991393\n",
      "Iteration 61, loss = 0.12754999\n",
      "Iteration 204, loss = 0.04178643\n",
      "Iteration 62, loss = 0.12344546\n",
      "Iteration 63, loss = 0.12766835\n",
      "Iteration 205, loss = 0.04460871\n",
      "Iteration 64, loss = 0.12185437\n",
      "Iteration 65, loss = 0.11886667\n",
      "Iteration 206, loss = 0.04691096\n",
      "Iteration 66, loss = 0.11959950\n",
      "Iteration 67, loss = 0.11799251\n",
      "Iteration 68, loss = 0.11391108\n",
      "Iteration 69, loss = 0.11353315\n",
      "Iteration 207, loss = 0.03890861\n",
      "Iteration 70, loss = 0.11193631\n",
      "Iteration 71, loss = 0.11068553\n",
      "Iteration 208, loss = 0.03649352\n",
      "Iteration 209, loss = 0.03373318\n",
      "Iteration 72, loss = 0.10817491\n",
      "Iteration 210, loss = 0.03266122\n",
      "Iteration 73, loss = 0.10616739\n",
      "Iteration 74, loss = 0.10561860\n",
      "Iteration 211, loss = 0.02995381\n",
      "Iteration 75, loss = 0.10682061\n",
      "Iteration 212, loss = 0.03182659\n",
      "Iteration 213, loss = 0.03032675\n",
      "Iteration 76, loss = 0.10633686\n",
      "Iteration 214, loss = 0.03134976\n",
      "Iteration 77, loss = 0.10709500\n",
      "Iteration 215, loss = 0.02751546\n",
      "Iteration 78, loss = 0.10138212\n",
      "Iteration 79, loss = 0.10119479\n",
      "Iteration 216, loss = 0.02784005\n",
      "Iteration 80, loss = 0.09891520\n",
      "Iteration 217, loss = 0.02852536\n",
      "Iteration 81, loss = 0.09674508\n",
      "Iteration 218, loss = 0.02738184\n",
      "Iteration 219, loss = 0.02815141\n",
      "Iteration 82, loss = 0.09552520\n",
      "Iteration 83, loss = 0.09399856\n",
      "Iteration 220, loss = 0.02717316\n",
      "Iteration 84, loss = 0.09237612\n",
      "Iteration 221, loss = 0.02709475\n",
      "Iteration 85, loss = 0.09197761\n",
      "Iteration 222, loss = 0.02945058\n",
      "Iteration 223, loss = 0.03156143\n",
      "Iteration 86, loss = 0.09101880\n",
      "Iteration 224, loss = 0.03375454\n",
      "Iteration 87, loss = 0.09149876\n",
      "Iteration 88, loss = 0.08854434\n",
      "Iteration 225, loss = 0.03139950\n",
      "Iteration 89, loss = 0.08727696\n",
      "Iteration 226, loss = 0.02569488\n",
      "Iteration 90, loss = 0.08567944\n",
      "Iteration 227, loss = 0.02971710\n",
      "Iteration 91, loss = 0.08467884\n",
      "Iteration 228, loss = 0.03024060\n",
      "Iteration 229, loss = 0.02716283\n",
      "Iteration 92, loss = 0.08363453\n",
      "Iteration 230, loss = 0.02615673\n",
      "Iteration 231, loss = 0.02507135\n",
      "Iteration 93, loss = 0.08265997\n",
      "Iteration 94, loss = 0.08192497\n",
      "Iteration 232, loss = 0.02534649\n",
      "Iteration 95, loss = 0.08204358\n",
      "Iteration 96, loss = 0.08288207\n",
      "Iteration 233, loss = 0.02562116\n",
      "Iteration 97, loss = 0.08368079\n",
      "Iteration 234, loss = 0.02532743\n",
      "Iteration 98, loss = 0.07941460\n",
      "Iteration 235, loss = 0.02371538\n",
      "Iteration 99, loss = 0.07713211\n",
      "Iteration 100, loss = 0.07853747\n",
      "Iteration 101, loss = 0.08005783\n",
      "Iteration 236, loss = 0.02456407\n",
      "Iteration 102, loss = 0.08389030\n",
      "Iteration 103, loss = 0.07929456\n",
      "Iteration 237, loss = 0.02424785\n",
      "Iteration 104, loss = 0.07453987\n",
      "Iteration 105, loss = 0.07469829\n",
      "Iteration 238, loss = 0.02457848\n",
      "Iteration 106, loss = 0.07137222\n",
      "Iteration 239, loss = 0.02519874\n",
      "Iteration 240, loss = 0.02500866\n",
      "Iteration 107, loss = 0.06915566\n",
      "Iteration 241, loss = 0.02436101\n",
      "Iteration 108, loss = 0.07086249\n",
      "Iteration 109, loss = 0.06819692\n",
      "Iteration 242, loss = 0.02717648\n",
      "Iteration 110, loss = 0.06799016\n",
      "Iteration 111, loss = 0.06651054\n",
      "Iteration 243, loss = 0.02776119\n",
      "Iteration 244, loss = 0.03405756\n",
      "Iteration 245, loss = 0.02476577\n",
      "Iteration 112, loss = 0.06569695\n",
      "Iteration 246, loss = 0.02448107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 113, loss = 0.06694719\n",
      "Iteration 114, loss = 0.06365484\n",
      "Iteration 115, loss = 0.06277357\n",
      "Iteration 116, loss = 0.06218056\n",
      "Iteration 117, loss = 0.06228940\n",
      "Iteration 118, loss = 0.06421921\n",
      "Iteration 119, loss = 0.06526600\n",
      "Iteration 120, loss = 0.06857018\n",
      "Iteration 121, loss = 0.06239380\n",
      "Iteration 122, loss = 0.06071016\n",
      "Iteration 123, loss = 0.05843414\n",
      "Iteration 1, loss = 1.17985213\n",
      "Iteration 124, loss = 0.05860316\n",
      "Iteration 2, loss = 0.77595129\n",
      "Iteration 125, loss = 0.05943415\n",
      "Iteration 3, loss = 0.60219616\n",
      "Iteration 126, loss = 0.05591133\n",
      "Iteration 4, loss = 0.52118726\n",
      "Iteration 5, loss = 0.47258617\n",
      "Iteration 127, loss = 0.05663457\n",
      "Iteration 128, loss = 0.05749293\n",
      "Iteration 6, loss = 0.43715165\n",
      "Iteration 129, loss = 0.05851246\n",
      "Iteration 7, loss = 0.41343616\n",
      "Iteration 130, loss = 0.05698677\n",
      "Iteration 8, loss = 0.39395010\n",
      "Iteration 9, loss = 0.37750510\n",
      "Iteration 131, loss = 0.05580636\n",
      "Iteration 10, loss = 0.36479216\n",
      "Iteration 132, loss = 0.06573818\n",
      "Iteration 133, loss = 0.05680792\n",
      "Iteration 11, loss = 0.35105282\n",
      "Iteration 134, loss = 0.05214041\n",
      "Iteration 12, loss = 0.34105329\n",
      "Iteration 13, loss = 0.33276747\n",
      "Iteration 135, loss = 0.05002630\n",
      "Iteration 14, loss = 0.32242524\n",
      "Iteration 136, loss = 0.04957888\n",
      "Iteration 137, loss = 0.05031722\n",
      "Iteration 15, loss = 0.31552634\n",
      "Iteration 138, loss = 0.05646796\n",
      "Iteration 16, loss = 0.30569853\n",
      "Iteration 139, loss = 0.05643147\n",
      "Iteration 17, loss = 0.29578835\n",
      "Iteration 18, loss = 0.28842027\n",
      "Iteration 140, loss = 0.05286021\n",
      "Iteration 141, loss = 0.04730932\n",
      "Iteration 19, loss = 0.28093570\n",
      "Iteration 142, loss = 0.04556441\n",
      "Iteration 143, loss = 0.04725933\n",
      "Iteration 20, loss = 0.27401781\n",
      "Iteration 144, loss = 0.04887721\n",
      "Iteration 21, loss = 0.26705468\n",
      "Iteration 145, loss = 0.05361867\n",
      "Iteration 22, loss = 0.26018122\n",
      "Iteration 146, loss = 0.06394932\n",
      "Iteration 23, loss = 0.25544114\n",
      "Iteration 147, loss = 0.05286277\n",
      "Iteration 148, loss = 0.05225427\n",
      "Iteration 24, loss = 0.24772852\n",
      "Iteration 149, loss = 0.05300051\n",
      "Iteration 150, loss = 0.05603425\n",
      "Iteration 25, loss = 0.24281722\n",
      "Iteration 151, loss = 0.05063418\n",
      "Iteration 26, loss = 0.23685330\n",
      "Iteration 152, loss = 0.04337187\n",
      "Iteration 27, loss = 0.23225511\n",
      "Iteration 28, loss = 0.22712281\n",
      "Iteration 153, loss = 0.04664174\n",
      "Iteration 154, loss = 0.04304217\n",
      "Iteration 155, loss = 0.04521914\n",
      "Iteration 29, loss = 0.22199863\n",
      "Iteration 156, loss = 0.04716215\n",
      "Iteration 30, loss = 0.21671703\n",
      "Iteration 157, loss = 0.04980171\n",
      "Iteration 31, loss = 0.21314438\n",
      "Iteration 158, loss = 0.05215154\n",
      "Iteration 32, loss = 0.20801414\n",
      "Iteration 159, loss = 0.04875228\n",
      "Iteration 33, loss = 0.20390635\n",
      "Iteration 34, loss = 0.19960327\n",
      "Iteration 160, loss = 0.04233858\n",
      "Iteration 35, loss = 0.19556337\n",
      "Iteration 161, loss = 0.04220153\n",
      "Iteration 36, loss = 0.19129119\n",
      "Iteration 162, loss = 0.04377569\n",
      "Iteration 37, loss = 0.18930734\n",
      "Iteration 163, loss = 0.04341233\n",
      "Iteration 38, loss = 0.18662089\n",
      "Iteration 164, loss = 0.04514254\n",
      "Iteration 165, loss = 0.04277557\n",
      "Iteration 166, loss = 0.03724605\n",
      "Iteration 39, loss = 0.17978319\n",
      "Iteration 40, loss = 0.17915383\n",
      "Iteration 167, loss = 0.03806988\n",
      "Iteration 41, loss = 0.17405881\n",
      "Iteration 168, loss = 0.03695063\n",
      "Iteration 42, loss = 0.16984639\n",
      "Iteration 169, loss = 0.03564969\n",
      "Iteration 170, loss = 0.03620944\n",
      "Iteration 171, loss = 0.03823755\n",
      "Iteration 43, loss = 0.16761842\n",
      "Iteration 44, loss = 0.17000335\n",
      "Iteration 172, loss = 0.03644266\n",
      "Iteration 45, loss = 0.16229769\n",
      "Iteration 46, loss = 0.16332308\n",
      "Iteration 173, loss = 0.03565582\n",
      "Iteration 174, loss = 0.03465672\n",
      "Iteration 175, loss = 0.03500074\n",
      "Iteration 47, loss = 0.15640080\n",
      "Iteration 48, loss = 0.15395958\n",
      "Iteration 176, loss = 0.03551253\n",
      "Iteration 177, loss = 0.03318374\n",
      "Iteration 49, loss = 0.15169952\n",
      "Iteration 178, loss = 0.03289087\n",
      "Iteration 179, loss = 0.03255914\n",
      "Iteration 50, loss = 0.15103512\n",
      "Iteration 180, loss = 0.03292557\n",
      "Iteration 51, loss = 0.14891900\n",
      "Iteration 52, loss = 0.14484478\n",
      "Iteration 181, loss = 0.03570049\n",
      "Iteration 182, loss = 0.03543984\n",
      "Iteration 183, loss = 0.03150446\n",
      "Iteration 184, loss = 0.03197995\n",
      "Iteration 185, loss = 0.03326417\n",
      "Iteration 53, loss = 0.14508585\n",
      "Iteration 186, loss = 0.03152352\n",
      "Iteration 187, loss = 0.03096003\n",
      "Iteration 188, loss = 0.03007166\n",
      "Iteration 54, loss = 0.14757428\n",
      "Iteration 55, loss = 0.13990289\n",
      "Iteration 56, loss = 0.13966147\n",
      "Iteration 189, loss = 0.03041537\n",
      "Iteration 57, loss = 0.13544727\n",
      "Iteration 190, loss = 0.02970950\n",
      "Iteration 191, loss = 0.02916560\n",
      "Iteration 192, loss = 0.02932036\n",
      "Iteration 58, loss = 0.13344892\n",
      "Iteration 59, loss = 0.12845934\n",
      "Iteration 193, loss = 0.02905267\n",
      "Iteration 60, loss = 0.12685806\n",
      "Iteration 194, loss = 0.02862408\n",
      "Iteration 61, loss = 0.12527413\n",
      "Iteration 195, loss = 0.03235039\n",
      "Iteration 196, loss = 0.03002647\n",
      "Iteration 197, loss = 0.03061166\n",
      "Iteration 62, loss = 0.12649794\n",
      "Iteration 63, loss = 0.12181621\n",
      "Iteration 198, loss = 0.02861447\n",
      "Iteration 64, loss = 0.12034420\n",
      "Iteration 199, loss = 0.02877318\n",
      "Iteration 65, loss = 0.11873278\n",
      "Iteration 200, loss = 0.02746315\n",
      "Iteration 201, loss = 0.02992477\n",
      "Iteration 66, loss = 0.11647533\n",
      "Iteration 202, loss = 0.03083441\n",
      "Iteration 67, loss = 0.11653420\n",
      "Iteration 203, loss = 0.03385153\n",
      "Iteration 68, loss = 0.11701736\n",
      "Iteration 204, loss = 0.03098082\n",
      "Iteration 69, loss = 0.11456043\n",
      "Iteration 205, loss = 0.03112490\n",
      "Iteration 206, loss = 0.03069071\n",
      "Iteration 70, loss = 0.11283582\n",
      "Iteration 71, loss = 0.11654450\n",
      "Iteration 207, loss = 0.03151155\n",
      "Iteration 72, loss = 0.11206459\n",
      "Iteration 73, loss = 0.11357394\n",
      "Iteration 208, loss = 0.03544773\n",
      "Iteration 209, loss = 0.03216372\n",
      "Iteration 74, loss = 0.11286259\n",
      "Iteration 210, loss = 0.02893225\n",
      "Iteration 75, loss = 0.10744028\n",
      "Iteration 76, loss = 0.10645964\n",
      "Iteration 211, loss = 0.02716129\n",
      "Iteration 77, loss = 0.10131032\n",
      "Iteration 212, loss = 0.02601178\n",
      "Iteration 213, loss = 0.02696441\n",
      "Iteration 78, loss = 0.10330294\n",
      "Iteration 214, loss = 0.02641143\n",
      "Iteration 215, loss = 0.02354558\n",
      "Iteration 216, loss = 0.02397745\n",
      "Iteration 79, loss = 0.10408425\n",
      "Iteration 217, loss = 0.02323362\n",
      "Iteration 80, loss = 0.10345434\n",
      "Iteration 218, loss = 0.02326712\n",
      "Iteration 81, loss = 0.09928587\n",
      "Iteration 82, loss = 0.09805786\n",
      "Iteration 83, loss = 0.09952301\n",
      "Iteration 219, loss = 0.02289139\n",
      "Iteration 220, loss = 0.02405123\n",
      "Iteration 84, loss = 0.09943506\n",
      "Iteration 85, loss = 0.10058383\n",
      "Iteration 221, loss = 0.02455660\n",
      "Iteration 86, loss = 0.09299520\n",
      "Iteration 87, loss = 0.09307093\n",
      "Iteration 222, loss = 0.02269434\n",
      "Iteration 223, loss = 0.02236791\n",
      "Iteration 224, loss = 0.02271852\n",
      "Iteration 88, loss = 0.08858706\n",
      "Iteration 225, loss = 0.02143516\n",
      "Iteration 89, loss = 0.08640215\n",
      "Iteration 90, loss = 0.08640496\n",
      "Iteration 226, loss = 0.02159230\n",
      "Iteration 227, loss = 0.02234461\n",
      "Iteration 91, loss = 0.08630187\n",
      "Iteration 228, loss = 0.02586129\n",
      "Iteration 229, loss = 0.02621885\n",
      "Iteration 230, loss = 0.02232731\n",
      "Iteration 92, loss = 0.08593537\n",
      "Iteration 231, loss = 0.02120751\n",
      "Iteration 232, loss = 0.02268739\n",
      "Iteration 233, loss = 0.02212466\n",
      "Iteration 93, loss = 0.08317457\n",
      "Iteration 234, loss = 0.02110360\n",
      "Iteration 235, loss = 0.02054108\n",
      "Iteration 94, loss = 0.08205091\n",
      "Iteration 236, loss = 0.01998926\n",
      "Iteration 95, loss = 0.08106442\n",
      "Iteration 237, loss = 0.02259114\n",
      "Iteration 96, loss = 0.08090222\n",
      "Iteration 238, loss = 0.02105492\n",
      "Iteration 97, loss = 0.07973638\n",
      "Iteration 239, loss = 0.02433042\n",
      "Iteration 98, loss = 0.07920678\n",
      "Iteration 240, loss = 0.02247762\n",
      "Iteration 241, loss = 0.02278927\n",
      "Iteration 99, loss = 0.07731335\n",
      "Iteration 100, loss = 0.07754062\n",
      "Iteration 242, loss = 0.01897245\n",
      "Iteration 101, loss = 0.07508542\n",
      "Iteration 102, loss = 0.07427320\n",
      "Iteration 243, loss = 0.02283957\n",
      "Iteration 103, loss = 0.07279659\n",
      "Iteration 244, loss = 0.02319649\n",
      "Iteration 104, loss = 0.07382669\n",
      "Iteration 245, loss = 0.01847671\n",
      "Iteration 105, loss = 0.07027267\n",
      "Iteration 246, loss = 0.01949534\n",
      "Iteration 106, loss = 0.07173304\n",
      "Iteration 247, loss = 0.01896349\n",
      "Iteration 107, loss = 0.07340725\n",
      "Iteration 108, loss = 0.06818015\n",
      "Iteration 248, loss = 0.01855153\n",
      "Iteration 109, loss = 0.06997861\n",
      "Iteration 249, loss = 0.01921314\n",
      "Iteration 250, loss = 0.01944851\n",
      "Iteration 110, loss = 0.07054905\n",
      "Iteration 251, loss = 0.01890085\n",
      "Iteration 111, loss = 0.07006460\n",
      "Iteration 112, loss = 0.07113283\n",
      "Iteration 252, loss = 0.01822064\n",
      "Iteration 113, loss = 0.07224818\n",
      "Iteration 253, loss = 0.01987379\n",
      "Iteration 114, loss = 0.06913803\n",
      "Iteration 254, loss = 0.01696347\n",
      "Iteration 115, loss = 0.06875421\n",
      "Iteration 255, loss = 0.01778784\n",
      "Iteration 116, loss = 0.06851678\n",
      "Iteration 256, loss = 0.01905575\n",
      "Iteration 117, loss = 0.06413879\n",
      "Iteration 257, loss = 0.01885864\n",
      "Iteration 118, loss = 0.06352117\n",
      "Iteration 258, loss = 0.01837562\n",
      "Iteration 259, loss = 0.02053993\n",
      "Iteration 119, loss = 0.06340717\n",
      "Iteration 120, loss = 0.06012256\n",
      "Iteration 260, loss = 0.01778505\n",
      "Iteration 121, loss = 0.05989645\n",
      "Iteration 261, loss = 0.01695280\n",
      "Iteration 122, loss = 0.06114929\n",
      "Iteration 262, loss = 0.01719614\n",
      "Iteration 263, loss = 0.01771777\n",
      "Iteration 123, loss = 0.05988815\n",
      "Iteration 264, loss = 0.01932319\n",
      "Iteration 124, loss = 0.05931522\n",
      "Iteration 265, loss = 0.02108315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.05667811\n",
      "Iteration 126, loss = 0.05770638\n",
      "Iteration 127, loss = 0.05550914\n",
      "Iteration 128, loss = 0.05634923\n",
      "Iteration 129, loss = 0.05831886\n",
      "Iteration 130, loss = 0.05422409\n",
      "Iteration 131, loss = 0.05400998\n",
      "Iteration 132, loss = 0.05473396\n",
      "Iteration 133, loss = 0.05606511\n",
      "Iteration 134, loss = 0.05309385\n",
      "Iteration 135, loss = 0.05348799\n",
      "Iteration 136, loss = 0.04987876\n",
      "Iteration 137, loss = 0.04986562\n",
      "Iteration 138, loss = 0.05044192\n",
      "Iteration 139, loss = 0.04862790\n",
      "Iteration 140, loss = 0.05074864\n",
      "Iteration 141, loss = 0.04975658\n",
      "Iteration 142, loss = 0.05500553\n",
      "Iteration 1, loss = 1.06799546\n",
      "Iteration 143, loss = 0.05549958\n",
      "Iteration 144, loss = 0.05217084\n",
      "Iteration 145, loss = 0.04567519\n",
      "Iteration 2, loss = 0.76348275\n",
      "Iteration 3, loss = 0.58974488\n",
      "Iteration 4, loss = 0.50049466\n",
      "Iteration 146, loss = 0.04791424\n",
      "Iteration 5, loss = 0.46123523\n",
      "Iteration 147, loss = 0.04565992\n",
      "Iteration 148, loss = 0.04475831\n",
      "Iteration 149, loss = 0.04445709\n",
      "Iteration 6, loss = 0.41577075\n",
      "Iteration 7, loss = 0.39761312\n",
      "Iteration 150, loss = 0.04495450\n",
      "Iteration 8, loss = 0.37711607\n",
      "Iteration 9, loss = 0.35938361\n",
      "Iteration 151, loss = 0.04548797\n",
      "Iteration 10, loss = 0.34750315\n",
      "Iteration 152, loss = 0.04543295\n",
      "Iteration 153, loss = 0.04840402\n",
      "Iteration 11, loss = 0.33634214\n",
      "Iteration 12, loss = 0.32623972\n",
      "Iteration 154, loss = 0.04940132\n",
      "Iteration 13, loss = 0.31896529\n",
      "Iteration 14, loss = 0.30775811\n",
      "Iteration 155, loss = 0.05125276\n",
      "Iteration 156, loss = 0.04835091\n",
      "Iteration 157, loss = 0.05158015\n",
      "Iteration 158, loss = 0.04716272\n",
      "Iteration 15, loss = 0.29982402\n",
      "Iteration 159, loss = 0.04194245\n",
      "Iteration 16, loss = 0.29116061\n",
      "Iteration 160, loss = 0.04297405\n",
      "Iteration 17, loss = 0.28406203\n",
      "Iteration 161, loss = 0.04115891\n",
      "Iteration 18, loss = 0.27774625\n",
      "Iteration 162, loss = 0.04162482\n",
      "Iteration 163, loss = 0.04363846\n",
      "Iteration 19, loss = 0.27157235\n",
      "Iteration 20, loss = 0.26425980\n",
      "Iteration 21, loss = 0.25972350\n",
      "Iteration 164, loss = 0.03923654\n",
      "Iteration 22, loss = 0.25429614\n",
      "Iteration 23, loss = 0.24686242\n",
      "Iteration 165, loss = 0.03915250\n",
      "Iteration 24, loss = 0.24296642\n",
      "Iteration 166, loss = 0.03914566\n",
      "Iteration 25, loss = 0.23670421Iteration 167, loss = 0.03746093\n",
      "\n",
      "Iteration 168, loss = 0.03730911\n",
      "Iteration 26, loss = 0.23218115\n",
      "Iteration 27, loss = 0.22696441\n",
      "Iteration 169, loss = 0.03727955\n",
      "Iteration 28, loss = 0.22166760\n",
      "Iteration 29, loss = 0.22039241\n",
      "Iteration 170, loss = 0.03718103\n",
      "Iteration 171, loss = 0.03694955\n",
      "Iteration 30, loss = 0.21983209\n",
      "Iteration 31, loss = 0.21171343\n",
      "Iteration 172, loss = 0.03674686\n",
      "Iteration 32, loss = 0.20712553\n",
      "Iteration 173, loss = 0.03608493\n",
      "Iteration 33, loss = 0.20597396\n",
      "Iteration 34, loss = 0.19981330\n",
      "Iteration 35, loss = 0.19499857\n",
      "Iteration 36, loss = 0.19123180\n",
      "Iteration 174, loss = 0.03634840\n",
      "Iteration 37, loss = 0.18766706\n",
      "Iteration 175, loss = 0.04046207\n",
      "Iteration 38, loss = 0.18467107\n",
      "Iteration 176, loss = 0.03661338\n",
      "Iteration 39, loss = 0.18074946\n",
      "Iteration 177, loss = 0.04195700\n",
      "Iteration 40, loss = 0.17752073\n",
      "Iteration 178, loss = 0.03992961\n",
      "Iteration 179, loss = 0.03934447\n",
      "Iteration 41, loss = 0.17535237\n",
      "Iteration 180, loss = 0.03605424\n",
      "Iteration 42, loss = 0.17269523\n",
      "Iteration 43, loss = 0.17055971\n",
      "Iteration 181, loss = 0.03451261\n",
      "Iteration 44, loss = 0.16796734\n",
      "Iteration 45, loss = 0.16633533\n",
      "Iteration 46, loss = 0.16324648\n",
      "Iteration 182, loss = 0.03316055\n",
      "Iteration 47, loss = 0.15982859\n",
      "Iteration 183, loss = 0.03341919\n",
      "Iteration 184, loss = 0.03439338\n",
      "Iteration 185, loss = 0.03304945\n",
      "Iteration 48, loss = 0.15693586\n",
      "Iteration 49, loss = 0.15513444\n",
      "Iteration 186, loss = 0.03327093\n",
      "Iteration 50, loss = 0.15165376\n",
      "Iteration 187, loss = 0.03334166\n",
      "Iteration 188, loss = 0.03291366\n",
      "Iteration 51, loss = 0.15100939\n",
      "Iteration 189, loss = 0.03256735\n",
      "Iteration 52, loss = 0.14887171\n",
      "Iteration 190, loss = 0.03142301\n",
      "Iteration 53, loss = 0.14739630\n",
      "Iteration 54, loss = 0.14743090\n",
      "Iteration 55, loss = 0.14295089\n",
      "Iteration 191, loss = 0.03120429\n",
      "Iteration 192, loss = 0.03035928\n",
      "Iteration 193, loss = 0.03090501\n",
      "Iteration 56, loss = 0.14135139\n",
      "Iteration 194, loss = 0.03056280\n",
      "Iteration 57, loss = 0.13861416\n",
      "Iteration 58, loss = 0.13645716\n",
      "Iteration 195, loss = 0.03064252\n",
      "Iteration 59, loss = 0.13393155\n",
      "Iteration 196, loss = 0.02963520\n",
      "Iteration 197, loss = 0.03029046\n",
      "Iteration 60, loss = 0.13407022\n",
      "Iteration 198, loss = 0.02970427\n",
      "Iteration 61, loss = 0.13088783\n",
      "Iteration 199, loss = 0.02893349\n",
      "Iteration 62, loss = 0.12832384\n",
      "Iteration 200, loss = 0.02877897\n",
      "Iteration 63, loss = 0.12959218\n",
      "Iteration 201, loss = 0.02970095\n",
      "Iteration 64, loss = 0.12660814\n",
      "Iteration 202, loss = 0.02917391\n",
      "Iteration 65, loss = 0.12495904\n",
      "Iteration 203, loss = 0.02853174\n",
      "Iteration 66, loss = 0.12285067\n",
      "Iteration 204, loss = 0.02907282\n",
      "Iteration 205, loss = 0.02899445\n",
      "Iteration 67, loss = 0.12058426\n",
      "Iteration 206, loss = 0.02775655\n",
      "Iteration 68, loss = 0.12190147\n",
      "Iteration 69, loss = 0.12679020\n",
      "Iteration 207, loss = 0.02798616\n",
      "Iteration 70, loss = 0.12293648\n",
      "Iteration 208, loss = 0.02749728\n",
      "Iteration 209, loss = 0.02959213\n",
      "Iteration 71, loss = 0.11871798\n",
      "Iteration 210, loss = 0.03178135\n",
      "Iteration 72, loss = 0.11489188\n",
      "Iteration 211, loss = 0.02872806\n",
      "Iteration 73, loss = 0.11487349\n",
      "Iteration 212, loss = 0.02736683\n",
      "Iteration 74, loss = 0.11128341\n",
      "Iteration 213, loss = 0.02627355\n",
      "Iteration 214, loss = 0.02633746\n",
      "Iteration 75, loss = 0.10884019\n",
      "Iteration 215, loss = 0.02558330\n",
      "Iteration 76, loss = 0.10833166\n",
      "Iteration 216, loss = 0.02654321\n",
      "Iteration 77, loss = 0.10703420\n",
      "Iteration 217, loss = 0.02754362\n",
      "Iteration 78, loss = 0.10501022\n",
      "Iteration 79, loss = 0.10347320\n",
      "Iteration 218, loss = 0.02701719\n",
      "Iteration 80, loss = 0.10617485\n",
      "Iteration 81, loss = 0.10005111\n",
      "Iteration 82, loss = 0.10148791\n",
      "Iteration 219, loss = 0.02642080\n",
      "Iteration 83, loss = 0.10343688\n",
      "Iteration 220, loss = 0.02625990\n",
      "Iteration 84, loss = 0.09929583\n",
      "Iteration 221, loss = 0.02573396\n",
      "Iteration 85, loss = 0.09808634\n",
      "Iteration 86, loss = 0.09646286\n",
      "Iteration 222, loss = 0.02435274\n",
      "Iteration 87, loss = 0.09551485\n",
      "Iteration 223, loss = 0.02900865\n",
      "Iteration 88, loss = 0.09284234\n",
      "Iteration 224, loss = 0.02955647\n",
      "Iteration 225, loss = 0.02654552\n",
      "Iteration 89, loss = 0.09370962\n",
      "Iteration 226, loss = 0.02511910\n",
      "Iteration 227, loss = 0.02412113\n",
      "Iteration 90, loss = 0.09197871\n",
      "Iteration 228, loss = 0.02426615\n",
      "Iteration 229, loss = 0.02638428\n",
      "Iteration 91, loss = 0.09587066\n",
      "Iteration 230, loss = 0.02364447\n",
      "Iteration 92, loss = 0.08947040\n",
      "Iteration 231, loss = 0.02258447\n",
      "Iteration 93, loss = 0.08957358\n",
      "Iteration 94, loss = 0.08776601\n",
      "Iteration 232, loss = 0.02441588\n",
      "Iteration 95, loss = 0.08847971\n",
      "Iteration 233, loss = 0.02582876\n",
      "Iteration 96, loss = 0.08900613\n",
      "Iteration 234, loss = 0.02721181\n",
      "Iteration 97, loss = 0.09112141\n",
      "Iteration 235, loss = 0.02304977\n",
      "Iteration 98, loss = 0.09110990\n",
      "Iteration 236, loss = 0.02242428\n",
      "Iteration 99, loss = 0.08713448\n",
      "Iteration 237, loss = 0.02337384\n",
      "Iteration 100, loss = 0.09231464\n",
      "Iteration 238, loss = 0.02202085\n",
      "Iteration 101, loss = 0.09979976\n",
      "Iteration 239, loss = 0.02250242\n",
      "Iteration 102, loss = 0.09603010\n",
      "Iteration 103, loss = 0.10129132\n",
      "Iteration 240, loss = 0.02226468\n",
      "Iteration 241, loss = 0.02288502\n",
      "Iteration 104, loss = 0.08966364\n",
      "Iteration 105, loss = 0.07834591\n",
      "Iteration 242, loss = 0.02478275\n",
      "Iteration 243, loss = 0.02553113\n",
      "Iteration 106, loss = 0.08281370\n",
      "Iteration 244, loss = 0.02382212\n",
      "Iteration 107, loss = 0.07799991\n",
      "Iteration 245, loss = 0.02407394\n",
      "Iteration 108, loss = 0.07478834\n",
      "Iteration 109, loss = 0.07401111\n",
      "Iteration 110, loss = 0.07121200\n",
      "Iteration 246, loss = 0.02270199\n",
      "Iteration 247, loss = 0.02123442\n",
      "Iteration 111, loss = 0.07229485\n",
      "Iteration 248, loss = 0.02282583\n",
      "Iteration 249, loss = 0.02260365\n",
      "Iteration 112, loss = 0.07336955\n",
      "Iteration 113, loss = 0.06952879\n",
      "Iteration 114, loss = 0.06862607\n",
      "Iteration 250, loss = 0.02400542\n",
      "Iteration 115, loss = 0.06866217\n",
      "Iteration 251, loss = 0.02322427\n",
      "Iteration 252, loss = 0.02364790\n",
      "Iteration 116, loss = 0.06744856\n",
      "Iteration 253, loss = 0.02175141\n",
      "Iteration 117, loss = 0.06579351\n",
      "Iteration 118, loss = 0.06509422\n",
      "Iteration 119, loss = 0.06659579\n",
      "Iteration 254, loss = 0.02211718\n",
      "Iteration 255, loss = 0.02288874\n",
      "Iteration 120, loss = 0.06447448\n",
      "Iteration 256, loss = 0.01996893\n",
      "Iteration 257, loss = 0.01983869\n",
      "Iteration 121, loss = 0.06500234\n",
      "Iteration 122, loss = 0.06430614\n",
      "Iteration 258, loss = 0.01994220\n",
      "Iteration 123, loss = 0.06660469\n",
      "Iteration 259, loss = 0.01883878\n",
      "Iteration 124, loss = 0.06562787\n",
      "Iteration 260, loss = 0.01920675\n",
      "Iteration 125, loss = 0.06530504\n",
      "Iteration 126, loss = 0.06690372\n",
      "Iteration 261, loss = 0.01928161\n",
      "Iteration 127, loss = 0.06923502\n",
      "Iteration 262, loss = 0.01913930\n",
      "Iteration 128, loss = 0.06811373\n",
      "Iteration 263, loss = 0.01827808\n",
      "Iteration 129, loss = 0.06316432\n",
      "Iteration 264, loss = 0.02105335\n",
      "Iteration 130, loss = 0.05803662\n",
      "Iteration 131, loss = 0.05805772\n",
      "Iteration 265, loss = 0.01971955\n",
      "Iteration 132, loss = 0.05863754\n",
      "Iteration 266, loss = 0.01983502\n",
      "Iteration 133, loss = 0.05800338\n",
      "Iteration 267, loss = 0.02105824\n",
      "Iteration 268, loss = 0.02417194\n",
      "Iteration 134, loss = 0.05550498\n",
      "Iteration 135, loss = 0.05580551\n",
      "Iteration 269, loss = 0.02261647\n",
      "Iteration 136, loss = 0.05581199\n",
      "Iteration 270, loss = 0.01952428\n",
      "Iteration 137, loss = 0.05608733\n",
      "Iteration 271, loss = 0.01952571\n",
      "Iteration 138, loss = 0.05245769\n",
      "Iteration 272, loss = 0.01824955\n",
      "Iteration 139, loss = 0.05272798\n",
      "Iteration 273, loss = 0.01846905\n",
      "Iteration 140, loss = 0.05259637\n",
      "Iteration 274, loss = 0.01831285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 141, loss = 0.05237592\n",
      "Iteration 142, loss = 0.05309632\n",
      "Iteration 143, loss = 0.05537373\n",
      "Iteration 144, loss = 0.05190501\n",
      "Iteration 145, loss = 0.05280637\n",
      "Iteration 146, loss = 0.05189814\n",
      "Iteration 147, loss = 0.05254474\n",
      "Iteration 148, loss = 0.05080850\n",
      "Iteration 149, loss = 0.04692214\n",
      "Iteration 150, loss = 0.04769454\n",
      "Iteration 151, loss = 0.04800591\n",
      "Iteration 152, loss = 0.04885360\n",
      "Iteration 1, loss = 1.41046112\n",
      "Iteration 2, loss = 0.86749410\n",
      "Iteration 153, loss = 0.06024455\n",
      "Iteration 154, loss = 0.05564660\n",
      "Iteration 3, loss = 0.65000104\n",
      "Iteration 155, loss = 0.04708032\n",
      "Iteration 4, loss = 0.55056912\n",
      "Iteration 156, loss = 0.04556184\n",
      "Iteration 5, loss = 0.48662298\n",
      "Iteration 6, loss = 0.44462368\n",
      "Iteration 157, loss = 0.04390469\n",
      "Iteration 7, loss = 0.41636842\n",
      "Iteration 158, loss = 0.04392449\n",
      "Iteration 8, loss = 0.39236875\n",
      "Iteration 9, loss = 0.37760039\n",
      "Iteration 159, loss = 0.04497065\n",
      "Iteration 10, loss = 0.36218398\n",
      "Iteration 160, loss = 0.04264923\n",
      "Iteration 11, loss = 0.34857980\n",
      "Iteration 161, loss = 0.04259352\n",
      "Iteration 12, loss = 0.33736562\n",
      "Iteration 13, loss = 0.32654052\n",
      "Iteration 14, loss = 0.31540421\n",
      "Iteration 162, loss = 0.04264108\n",
      "Iteration 15, loss = 0.30767690\n",
      "Iteration 16, loss = 0.29925774\n",
      "Iteration 163, loss = 0.04293269\n",
      "Iteration 17, loss = 0.29133964\n",
      "Iteration 164, loss = 0.04308165\n",
      "Iteration 18, loss = 0.28414135\n",
      "Iteration 19, loss = 0.27550281\n",
      "Iteration 165, loss = 0.04292105\n",
      "Iteration 166, loss = 0.04158433\n",
      "Iteration 20, loss = 0.26898276\n",
      "Iteration 167, loss = 0.04460332\n",
      "Iteration 168, loss = 0.04357707\n",
      "Iteration 21, loss = 0.26304453\n",
      "Iteration 169, loss = 0.04512996\n",
      "Iteration 170, loss = 0.04214445\n",
      "Iteration 22, loss = 0.25561184\n",
      "Iteration 171, loss = 0.04071227\n",
      "Iteration 23, loss = 0.25088367\n",
      "Iteration 172, loss = 0.04092423\n",
      "Iteration 173, loss = 0.03811753\n",
      "Iteration 24, loss = 0.24449178\n",
      "Iteration 25, loss = 0.23988694\n",
      "Iteration 174, loss = 0.03941768\n",
      "Iteration 26, loss = 0.23524542\n",
      "Iteration 175, loss = 0.03970214\n",
      "Iteration 176, loss = 0.03722478\n",
      "Iteration 177, loss = 0.03708384\n",
      "Iteration 27, loss = 0.23505738\n",
      "Iteration 178, loss = 0.04224602\n",
      "Iteration 28, loss = 0.22560768\n",
      "Iteration 29, loss = 0.22115197\n",
      "Iteration 179, loss = 0.05205244\n",
      "Iteration 30, loss = 0.21646286\n",
      "Iteration 180, loss = 0.04435938\n",
      "Iteration 181, loss = 0.04099214\n",
      "Iteration 31, loss = 0.21152795\n",
      "Iteration 182, loss = 0.04480769\n",
      "Iteration 32, loss = 0.20696386\n",
      "Iteration 33, loss = 0.20383767\n",
      "Iteration 183, loss = 0.03917732\n",
      "Iteration 34, loss = 0.19995905\n",
      "Iteration 184, loss = 0.03628974\n",
      "Iteration 35, loss = 0.19574673\n",
      "Iteration 185, loss = 0.03562795\n",
      "Iteration 36, loss = 0.19152462\n",
      "Iteration 186, loss = 0.03564940\n",
      "Iteration 37, loss = 0.18902962\n",
      "Iteration 187, loss = 0.03549765\n",
      "Iteration 188, loss = 0.03697818\n",
      "Iteration 38, loss = 0.18514685\n",
      "Iteration 39, loss = 0.18186045\n",
      "Iteration 189, loss = 0.03371181\n",
      "Iteration 40, loss = 0.17790985\n",
      "Iteration 190, loss = 0.03849733\n",
      "Iteration 41, loss = 0.17802101\n",
      "Iteration 191, loss = 0.04245410\n",
      "Iteration 192, loss = 0.05221604\n",
      "Iteration 42, loss = 0.17403774\n",
      "Iteration 43, loss = 0.17130873\n",
      "Iteration 193, loss = 0.05104808\n",
      "Iteration 44, loss = 0.16572259\n",
      "Iteration 194, loss = 0.05155221\n",
      "Iteration 45, loss = 0.16685174\n",
      "Iteration 195, loss = 0.04495923\n",
      "Iteration 196, loss = 0.04364828\n",
      "Iteration 46, loss = 0.16269781\n",
      "Iteration 197, loss = 0.03180594\n",
      "Iteration 47, loss = 0.15941863\n",
      "Iteration 198, loss = 0.03341345\n",
      "Iteration 199, loss = 0.03197315\n",
      "Iteration 200, loss = 0.03198130\n",
      "Iteration 48, loss = 0.15636081\n",
      "Iteration 201, loss = 0.03056952\n",
      "Iteration 49, loss = 0.15498838\n",
      "Iteration 50, loss = 0.15208906\n",
      "Iteration 202, loss = 0.03164253\n",
      "Iteration 203, loss = 0.02995972\n",
      "Iteration 51, loss = 0.14894518\n",
      "Iteration 204, loss = 0.03049389\n",
      "Iteration 205, loss = 0.03009822\n",
      "Iteration 52, loss = 0.14804551\n",
      "Iteration 206, loss = 0.03085318\n",
      "Iteration 53, loss = 0.14347690\n",
      "Iteration 207, loss = 0.02988333\n",
      "Iteration 208, loss = 0.02937500\n",
      "Iteration 54, loss = 0.14136175\n",
      "Iteration 209, loss = 0.02971095\n",
      "Iteration 55, loss = 0.14068412\n",
      "Iteration 210, loss = 0.02931684\n",
      "Iteration 56, loss = 0.13690711\n",
      "Iteration 211, loss = 0.03013064\n",
      "Iteration 212, loss = 0.02811762\n",
      "Iteration 213, loss = 0.02889641\n",
      "Iteration 57, loss = 0.13650361\n",
      "Iteration 58, loss = 0.13438776\n",
      "Iteration 214, loss = 0.02877339\n",
      "Iteration 59, loss = 0.13101519\n",
      "Iteration 60, loss = 0.12987596\n",
      "Iteration 215, loss = 0.02896039\n",
      "Iteration 216, loss = 0.02795723\n",
      "Iteration 61, loss = 0.12833456\n",
      "Iteration 217, loss = 0.02892748\n",
      "Iteration 218, loss = 0.03123712\n",
      "Iteration 62, loss = 0.12502546\n",
      "Iteration 219, loss = 0.03601019\n",
      "Iteration 220, loss = 0.04016977\n",
      "Iteration 63, loss = 0.12548513\n",
      "Iteration 221, loss = 0.04788365\n",
      "Iteration 64, loss = 0.12438027\n",
      "Iteration 65, loss = 0.12052779\n",
      "Iteration 66, loss = 0.11971028\n",
      "Iteration 222, loss = 0.03892452\n",
      "Iteration 67, loss = 0.11873168\n",
      "Iteration 223, loss = 0.03428038\n",
      "Iteration 68, loss = 0.11691515\n",
      "Iteration 224, loss = 0.03577696\n",
      "Iteration 225, loss = 0.03291800\n",
      "Iteration 69, loss = 0.11452859\n",
      "Iteration 70, loss = 0.11466136\n",
      "Iteration 226, loss = 0.03690957\n",
      "Iteration 71, loss = 0.10942619\n",
      "Iteration 227, loss = 0.03110003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 72, loss = 0.11033210\n",
      "Iteration 73, loss = 0.10783023\n",
      "Iteration 74, loss = 0.10628591\n",
      "Iteration 75, loss = 0.10326564\n",
      "Iteration 76, loss = 0.10275947\n",
      "Iteration 77, loss = 0.10119117\n",
      "Iteration 78, loss = 0.09906527\n",
      "Iteration 79, loss = 0.09754309\n",
      "Iteration 80, loss = 0.09984164\n",
      "Iteration 1, loss = 1.04076060\n",
      "Iteration 81, loss = 0.09614874\n",
      "Iteration 2, loss = 0.69063428\n",
      "Iteration 82, loss = 0.09789132\n",
      "Iteration 83, loss = 0.09350974\n",
      "Iteration 3, loss = 0.55619655\n",
      "Iteration 4, loss = 0.48907251\n",
      "Iteration 84, loss = 0.09400718\n",
      "Iteration 5, loss = 0.44967246\n",
      "Iteration 6, loss = 0.42282920\n",
      "Iteration 85, loss = 0.09094007\n",
      "Iteration 86, loss = 0.08937711\n",
      "Iteration 7, loss = 0.40351484\n",
      "Iteration 87, loss = 0.08907494\n",
      "Iteration 8, loss = 0.38736073\n",
      "Iteration 88, loss = 0.08590081\n",
      "Iteration 9, loss = 0.37428380\n",
      "Iteration 10, loss = 0.36248732\n",
      "Iteration 89, loss = 0.08528119\n",
      "Iteration 11, loss = 0.35109358\n",
      "Iteration 90, loss = 0.08465679\n",
      "Iteration 91, loss = 0.08221284\n",
      "Iteration 92, loss = 0.08160560\n",
      "Iteration 93, loss = 0.07847786\n",
      "Iteration 12, loss = 0.34136841\n",
      "Iteration 94, loss = 0.08198311\n",
      "Iteration 95, loss = 0.07821680\n",
      "Iteration 96, loss = 0.07699122\n",
      "Iteration 13, loss = 0.33182969\n",
      "Iteration 97, loss = 0.07547332\n",
      "Iteration 14, loss = 0.32503196\n",
      "Iteration 98, loss = 0.07431309\n",
      "Iteration 15, loss = 0.31438577\n",
      "Iteration 99, loss = 0.07413494\n",
      "Iteration 16, loss = 0.30620962\n",
      "Iteration 100, loss = 0.07063928\n",
      "Iteration 17, loss = 0.29747199\n",
      "Iteration 101, loss = 0.07255446\n",
      "Iteration 102, loss = 0.07169497\n",
      "Iteration 18, loss = 0.29008207\n",
      "Iteration 103, loss = 0.06978342\n",
      "Iteration 19, loss = 0.28356120\n",
      "Iteration 104, loss = 0.06691730\n",
      "Iteration 20, loss = 0.27611223\n",
      "Iteration 105, loss = 0.06612833\n",
      "Iteration 21, loss = 0.26922681\n",
      "Iteration 106, loss = 0.06723461\n",
      "Iteration 107, loss = 0.06509155\n",
      "Iteration 22, loss = 0.26656775\n",
      "Iteration 108, loss = 0.06444976\n",
      "Iteration 109, loss = 0.06478660\n",
      "Iteration 23, loss = 0.25614857\n",
      "Iteration 110, loss = 0.06337055\n",
      "Iteration 24, loss = 0.25164446\n",
      "Iteration 111, loss = 0.06274942\n",
      "Iteration 112, loss = 0.06038982\n",
      "Iteration 25, loss = 0.24452350\n",
      "Iteration 113, loss = 0.06162119\n",
      "Iteration 114, loss = 0.05980117\n",
      "Iteration 26, loss = 0.23951516\n",
      "Iteration 115, loss = 0.06046371\n",
      "Iteration 116, loss = 0.06388293\n",
      "Iteration 27, loss = 0.23506794\n",
      "Iteration 117, loss = 0.06372363\n",
      "Iteration 28, loss = 0.23042693\n",
      "Iteration 118, loss = 0.06197801\n",
      "Iteration 29, loss = 0.22493386\n",
      "Iteration 119, loss = 0.05905211\n",
      "Iteration 120, loss = 0.05672549\n",
      "Iteration 30, loss = 0.21778957\n",
      "Iteration 121, loss = 0.05458249\n",
      "Iteration 122, loss = 0.05562144\n",
      "Iteration 31, loss = 0.21408744\n",
      "Iteration 123, loss = 0.05411609\n",
      "Iteration 32, loss = 0.20970813\n",
      "Iteration 124, loss = 0.05290705\n",
      "Iteration 33, loss = 0.20804759\n",
      "Iteration 125, loss = 0.05322480\n",
      "Iteration 126, loss = 0.05263865\n",
      "Iteration 127, loss = 0.05043788\n",
      "Iteration 34, loss = 0.20299728\n",
      "Iteration 128, loss = 0.05068769\n",
      "Iteration 129, loss = 0.05131489\n",
      "Iteration 130, loss = 0.05189512\n",
      "Iteration 35, loss = 0.19588936\n",
      "Iteration 131, loss = 0.05216412\n",
      "Iteration 132, loss = 0.05294383\n",
      "Iteration 36, loss = 0.19567547\n",
      "Iteration 133, loss = 0.04847495\n",
      "Iteration 37, loss = 0.18822680\n",
      "Iteration 134, loss = 0.04820381\n",
      "Iteration 38, loss = 0.18643574\n",
      "Iteration 135, loss = 0.04738542\n",
      "Iteration 39, loss = 0.18240979\n",
      "Iteration 136, loss = 0.04684143\n",
      "Iteration 137, loss = 0.04870252\n",
      "Iteration 40, loss = 0.17680382\n",
      "Iteration 41, loss = 0.17570713\n",
      "Iteration 42, loss = 0.17099676\n",
      "Iteration 138, loss = 0.04815501\n",
      "Iteration 43, loss = 0.16800082\n",
      "Iteration 44, loss = 0.16548181\n",
      "Iteration 139, loss = 0.05139646\n",
      "Iteration 45, loss = 0.16212488\n",
      "Iteration 46, loss = 0.15801098\n",
      "Iteration 47, loss = 0.16097086\n",
      "Iteration 140, loss = 0.04624322\n",
      "Iteration 48, loss = 0.15809893\n",
      "Iteration 49, loss = 0.15350255\n",
      "Iteration 50, loss = 0.15286250\n",
      "Iteration 51, loss = 0.14604562\n",
      "Iteration 141, loss = 0.04403177\n",
      "Iteration 52, loss = 0.14604240\n",
      "Iteration 53, loss = 0.14315786\n",
      "Iteration 142, loss = 0.04363892\n",
      "Iteration 143, loss = 0.04303665\n",
      "Iteration 144, loss = 0.04430973\n",
      "Iteration 54, loss = 0.14247020\n",
      "Iteration 145, loss = 0.04390291\n",
      "Iteration 55, loss = 0.13877910\n",
      "Iteration 56, loss = 0.13625938\n",
      "Iteration 57, loss = 0.13379715\n",
      "Iteration 146, loss = 0.04380617\n",
      "Iteration 58, loss = 0.13171019\n",
      "Iteration 147, loss = 0.04786536\n",
      "Iteration 148, loss = 0.04572160\n",
      "Iteration 59, loss = 0.12909024\n",
      "Iteration 60, loss = 0.13254224\n",
      "Iteration 149, loss = 0.04203105\n",
      "Iteration 150, loss = 0.04451746\n",
      "Iteration 61, loss = 0.13228763\n",
      "Iteration 151, loss = 0.04147553\n",
      "Iteration 152, loss = 0.04128637\n",
      "Iteration 62, loss = 0.13205816\n",
      "Iteration 153, loss = 0.04290654\n",
      "Iteration 63, loss = 0.12853074\n",
      "Iteration 64, loss = 0.12192642\n",
      "Iteration 154, loss = 0.04177169\n",
      "Iteration 65, loss = 0.12641908\n",
      "Iteration 155, loss = 0.04066408\n",
      "Iteration 66, loss = 0.12358452\n",
      "Iteration 156, loss = 0.03872564\n",
      "Iteration 67, loss = 0.12130771\n",
      "Iteration 157, loss = 0.03832689\n",
      "Iteration 68, loss = 0.11911267\n",
      "Iteration 158, loss = 0.03897820\n",
      "Iteration 159, loss = 0.04050971\n",
      "Iteration 69, loss = 0.12225691\n",
      "Iteration 160, loss = 0.04817329\n",
      "Iteration 70, loss = 0.12393420\n",
      "Iteration 71, loss = 0.11716485\n",
      "Iteration 72, loss = 0.10705276\n",
      "Iteration 161, loss = 0.04949412\n",
      "Iteration 73, loss = 0.11439699\n",
      "Iteration 74, loss = 0.11633307\n",
      "Iteration 75, loss = 0.10517017\n",
      "Iteration 162, loss = 0.04470928\n",
      "Iteration 76, loss = 0.10697376\n",
      "Iteration 163, loss = 0.04477598\n",
      "Iteration 77, loss = 0.10527618\n",
      "Iteration 164, loss = 0.04027308\n",
      "Iteration 78, loss = 0.10535070\n",
      "Iteration 165, loss = 0.03851518\n",
      "Iteration 79, loss = 0.10111598\n",
      "Iteration 166, loss = 0.03878829\n",
      "Iteration 80, loss = 0.10121899\n",
      "Iteration 167, loss = 0.03617985\n",
      "Iteration 81, loss = 0.09704735\n",
      "Iteration 168, loss = 0.03505832\n",
      "Iteration 82, loss = 0.09401167\n",
      "Iteration 169, loss = 0.03408447\n",
      "Iteration 83, loss = 0.09490536\n",
      "Iteration 170, loss = 0.03623715\n",
      "Iteration 171, loss = 0.03433063\n",
      "Iteration 84, loss = 0.09117562\n",
      "Iteration 172, loss = 0.03442537\n",
      "Iteration 173, loss = 0.03724510\n",
      "Iteration 174, loss = 0.03465482\n",
      "Iteration 85, loss = 0.08986311\n",
      "Iteration 175, loss = 0.03349969\n",
      "Iteration 86, loss = 0.08996890\n",
      "Iteration 87, loss = 0.08856815\n",
      "Iteration 88, loss = 0.08608211\n",
      "Iteration 176, loss = 0.03453197\n",
      "Iteration 89, loss = 0.08560891\n",
      "Iteration 177, loss = 0.03302808\n",
      "Iteration 178, loss = 0.03298670\n",
      "Iteration 90, loss = 0.08473536\n",
      "Iteration 179, loss = 0.03530312\n",
      "Iteration 91, loss = 0.08341136\n",
      "Iteration 92, loss = 0.08204558\n",
      "Iteration 180, loss = 0.03562116\n",
      "Iteration 93, loss = 0.08136458\n",
      "Iteration 181, loss = 0.03523099\n",
      "Iteration 182, loss = 0.03291461\n",
      "Iteration 94, loss = 0.08162531\n",
      "Iteration 183, loss = 0.03378903\n",
      "Iteration 95, loss = 0.08057308\n",
      "Iteration 184, loss = 0.03193914\n",
      "Iteration 96, loss = 0.07889209\n",
      "Iteration 97, loss = 0.07972857\n",
      "Iteration 185, loss = 0.03119242\n",
      "Iteration 186, loss = 0.03181065\n",
      "Iteration 187, loss = 0.02983153\n",
      "Iteration 98, loss = 0.07676479\n",
      "Iteration 99, loss = 0.07420879\n",
      "Iteration 188, loss = 0.03024663\n",
      "Iteration 100, loss = 0.07427400\n",
      "Iteration 101, loss = 0.07560787\n",
      "Iteration 189, loss = 0.03056937\n",
      "Iteration 102, loss = 0.07362730\n",
      "Iteration 190, loss = 0.03161461\n",
      "Iteration 191, loss = 0.03098497\n",
      "Iteration 103, loss = 0.07266894\n",
      "Iteration 192, loss = 0.03167101\n",
      "Iteration 104, loss = 0.07114611\n",
      "Iteration 105, loss = 0.06935424\n",
      "Iteration 193, loss = 0.02936301\n",
      "Iteration 106, loss = 0.07011201\n",
      "Iteration 194, loss = 0.02979645\n",
      "Iteration 195, loss = 0.03114378\n",
      "Iteration 196, loss = 0.03192767\n",
      "Iteration 107, loss = 0.06961023\n",
      "Iteration 108, loss = 0.06568855\n",
      "Iteration 197, loss = 0.03121758\n",
      "Iteration 109, loss = 0.06779578\n",
      "Iteration 110, loss = 0.06627725\n",
      "Iteration 198, loss = 0.02854869\n",
      "Iteration 111, loss = 0.06638598\n",
      "Iteration 199, loss = 0.02804393\n",
      "Iteration 112, loss = 0.07002448\n",
      "Iteration 200, loss = 0.02768825\n",
      "Iteration 113, loss = 0.06803516\n",
      "Iteration 114, loss = 0.06554901\n",
      "Iteration 115, loss = 0.06329638\n",
      "Iteration 201, loss = 0.02783159\n",
      "Iteration 116, loss = 0.06364909\n",
      "Iteration 202, loss = 0.02743262\n",
      "Iteration 203, loss = 0.02742632\n",
      "Iteration 204, loss = 0.02840800\n",
      "Iteration 117, loss = 0.06358501\n",
      "Iteration 205, loss = 0.02761265\n",
      "Iteration 118, loss = 0.06508111\n",
      "Iteration 119, loss = 0.05816523\n",
      "Iteration 206, loss = 0.02687629\n",
      "Iteration 120, loss = 0.05961231\n",
      "Iteration 121, loss = 0.06172142\n",
      "Iteration 207, loss = 0.02859128\n",
      "Iteration 122, loss = 0.06466696\n",
      "Iteration 208, loss = 0.03140245\n",
      "Iteration 123, loss = 0.06363498\n",
      "Iteration 209, loss = 0.03141959\n",
      "Iteration 210, loss = 0.02639096\n",
      "Iteration 124, loss = 0.06373784\n",
      "Iteration 125, loss = 0.05810402\n",
      "Iteration 211, loss = 0.02593581\n",
      "Iteration 126, loss = 0.05823559\n",
      "Iteration 212, loss = 0.02897722\n",
      "Iteration 127, loss = 0.05842837\n",
      "Iteration 213, loss = 0.03483516\n",
      "Iteration 128, loss = 0.05792541\n",
      "Iteration 214, loss = 0.02959427\n",
      "Iteration 129, loss = 0.05634041\n",
      "Iteration 215, loss = 0.03603337\n",
      "Iteration 130, loss = 0.05567110\n",
      "Iteration 216, loss = 0.04057524\n",
      "Iteration 131, loss = 0.05462805\n",
      "Iteration 217, loss = 0.04418069\n",
      "Iteration 218, loss = 0.02903567\n",
      "Iteration 132, loss = 0.05107703\n",
      "Iteration 219, loss = 0.03364860\n",
      "Iteration 133, loss = 0.05099221\n",
      "Iteration 220, loss = 0.03591286\n",
      "Iteration 134, loss = 0.04845399\n",
      "Iteration 221, loss = 0.02870312\n",
      "Iteration 135, loss = 0.04942576\n",
      "Iteration 222, loss = 0.03099871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 136, loss = 0.04850454\n",
      "Iteration 137, loss = 0.05065871\n",
      "Iteration 138, loss = 0.04929479\n",
      "Iteration 139, loss = 0.04773316\n",
      "Iteration 140, loss = 0.04822269\n",
      "Iteration 141, loss = 0.04840999\n",
      "Iteration 142, loss = 0.04983782\n",
      "Iteration 143, loss = 0.04733086\n",
      "Iteration 1, loss = 1.68099450\n",
      "Iteration 144, loss = 0.04507960\n",
      "Iteration 2, loss = 0.93742080\n",
      "Iteration 145, loss = 0.04677518\n",
      "Iteration 146, loss = 0.04323834\n",
      "Iteration 3, loss = 0.70657803\n",
      "Iteration 4, loss = 0.54170174\n",
      "Iteration 147, loss = 0.04392298\n",
      "Iteration 5, loss = 0.49394096\n",
      "Iteration 148, loss = 0.04332506\n",
      "Iteration 6, loss = 0.44529132\n",
      "Iteration 149, loss = 0.04287986\n",
      "Iteration 150, loss = 0.04242917\n",
      "Iteration 7, loss = 0.41257699\n",
      "Iteration 151, loss = 0.04199880\n",
      "Iteration 152, loss = 0.04164584\n",
      "Iteration 153, loss = 0.04257727\n",
      "Iteration 8, loss = 0.38991894\n",
      "Iteration 154, loss = 0.04281165\n",
      "Iteration 9, loss = 0.37042909\n",
      "Iteration 155, loss = 0.04312954\n",
      "Iteration 10, loss = 0.35547891\n",
      "Iteration 156, loss = 0.04228358\n",
      "Iteration 157, loss = 0.04309984\n",
      "Iteration 11, loss = 0.34076300\n",
      "Iteration 158, loss = 0.04380572\n",
      "Iteration 159, loss = 0.04010230\n",
      "Iteration 160, loss = 0.03847494\n",
      "Iteration 12, loss = 0.32944472\n",
      "Iteration 161, loss = 0.04495787\n",
      "Iteration 13, loss = 0.31697666\n",
      "Iteration 162, loss = 0.03969386\n",
      "Iteration 14, loss = 0.30835888\n",
      "Iteration 163, loss = 0.04262317\n",
      "Iteration 164, loss = 0.04073656\n",
      "Iteration 15, loss = 0.29909367\n",
      "Iteration 165, loss = 0.03929767\n",
      "Iteration 16, loss = 0.28999964\n",
      "Iteration 17, loss = 0.28219653\n",
      "Iteration 166, loss = 0.03978606\n",
      "Iteration 18, loss = 0.27418514\n",
      "Iteration 19, loss = 0.26823658\n",
      "Iteration 167, loss = 0.03566287\n",
      "Iteration 168, loss = 0.03480326\n",
      "Iteration 169, loss = 0.03506937\n",
      "Iteration 20, loss = 0.26031611\n",
      "Iteration 21, loss = 0.25405599\n",
      "Iteration 170, loss = 0.03420894\n",
      "Iteration 22, loss = 0.24729286\n",
      "Iteration 23, loss = 0.24359912\n",
      "Iteration 171, loss = 0.03436074\n",
      "Iteration 172, loss = 0.03623806\n",
      "Iteration 173, loss = 0.03648524\n",
      "Iteration 24, loss = 0.23670331\n",
      "Iteration 174, loss = 0.03638835\n",
      "Iteration 25, loss = 0.23158732\n",
      "Iteration 26, loss = 0.22481283\n",
      "Iteration 27, loss = 0.22020431\n",
      "Iteration 175, loss = 0.03642000\n",
      "Iteration 176, loss = 0.03634261\n",
      "Iteration 177, loss = 0.03673184\n",
      "Iteration 28, loss = 0.21588920\n",
      "Iteration 178, loss = 0.03506155\n",
      "Iteration 29, loss = 0.21232462\n",
      "Iteration 30, loss = 0.20529715\n",
      "Iteration 179, loss = 0.03953641\n",
      "Iteration 31, loss = 0.20184772\n",
      "Iteration 180, loss = 0.03242433\n",
      "Iteration 181, loss = 0.03231548\n",
      "Iteration 32, loss = 0.20273696\n",
      "Iteration 182, loss = 0.03448120\n",
      "Iteration 33, loss = 0.19161219\n",
      "Iteration 34, loss = 0.19004141\n",
      "Iteration 183, loss = 0.03345554\n",
      "Iteration 35, loss = 0.18559959\n",
      "Iteration 36, loss = 0.18093681\n",
      "Iteration 184, loss = 0.03433463\n",
      "Iteration 37, loss = 0.17818927\n",
      "Iteration 38, loss = 0.17437040\n",
      "Iteration 185, loss = 0.03480362\n",
      "Iteration 39, loss = 0.17001935\n",
      "Iteration 186, loss = 0.03147824\n",
      "Iteration 187, loss = 0.02968704\n",
      "Iteration 188, loss = 0.02956770\n",
      "Iteration 40, loss = 0.16928115\n",
      "Iteration 189, loss = 0.02912486\n",
      "Iteration 41, loss = 0.16375392\n",
      "Iteration 190, loss = 0.03080946\n",
      "Iteration 191, loss = 0.03288572\n",
      "Iteration 192, loss = 0.03197405\n",
      "Iteration 42, loss = 0.16290953\n",
      "Iteration 193, loss = 0.02885368\n",
      "Iteration 43, loss = 0.15693852\n",
      "Iteration 194, loss = 0.03019198\n",
      "Iteration 44, loss = 0.15740618\n",
      "Iteration 195, loss = 0.03194147\n",
      "Iteration 45, loss = 0.15176261\n",
      "Iteration 196, loss = 0.03466421\n",
      "Iteration 46, loss = 0.14782099\n",
      "Iteration 47, loss = 0.14548502\n",
      "Iteration 48, loss = 0.14423110\n",
      "Iteration 197, loss = 0.03880380\n",
      "Iteration 49, loss = 0.14307785\n",
      "Iteration 198, loss = 0.03688595\n",
      "Iteration 50, loss = 0.13674674\n",
      "Iteration 199, loss = 0.03087239\n",
      "Iteration 51, loss = 0.13641449\n",
      "Iteration 200, loss = 0.03093823\n",
      "Iteration 52, loss = 0.13269865\n",
      "Iteration 201, loss = 0.02980296\n",
      "Iteration 53, loss = 0.13086585\n",
      "Iteration 202, loss = 0.03123531\n",
      "Iteration 54, loss = 0.12918742\n",
      "Iteration 203, loss = 0.03029630\n",
      "Iteration 55, loss = 0.12811421\n",
      "Iteration 56, loss = 0.12393903\n",
      "Iteration 204, loss = 0.03020753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.12384956\n",
      "Iteration 58, loss = 0.12085488\n",
      "Iteration 59, loss = 0.11825540\n",
      "Iteration 60, loss = 0.11601554\n",
      "Iteration 61, loss = 0.11532901\n",
      "Iteration 62, loss = 0.11155119\n",
      "Iteration 63, loss = 0.10994081\n",
      "Iteration 64, loss = 0.10775501\n",
      "Iteration 65, loss = 0.10585284\n",
      "Iteration 1, loss = 1.23667304\n",
      "Iteration 66, loss = 0.10415779\n",
      "Iteration 2, loss = 0.80406811\n",
      "Iteration 67, loss = 0.10307092\n",
      "Iteration 3, loss = 0.63048489\n",
      "Iteration 68, loss = 0.10376078\n",
      "Iteration 4, loss = 0.54388939\n",
      "Iteration 69, loss = 0.09956552\n",
      "Iteration 70, loss = 0.09746204\n",
      "Iteration 71, loss = 0.09746429\n",
      "Iteration 5, loss = 0.48309408\n",
      "Iteration 6, loss = 0.44273986\n",
      "Iteration 72, loss = 0.09527021\n",
      "Iteration 7, loss = 0.41681261\n",
      "Iteration 8, loss = 0.39093128\n",
      "Iteration 73, loss = 0.09378194\n",
      "Iteration 9, loss = 0.37359171\n",
      "Iteration 74, loss = 0.09410989\n",
      "Iteration 75, loss = 0.09151937\n",
      "Iteration 76, loss = 0.08957483\n",
      "Iteration 10, loss = 0.35789724\n",
      "Iteration 11, loss = 0.34849157\n",
      "Iteration 12, loss = 0.33397260\n",
      "Iteration 13, loss = 0.32280405\n",
      "Iteration 77, loss = 0.08884130\n",
      "Iteration 78, loss = 0.08781172\n",
      "Iteration 79, loss = 0.08549889\n",
      "Iteration 14, loss = 0.31121277\n",
      "Iteration 80, loss = 0.08425139\n",
      "Iteration 15, loss = 0.30161102\n",
      "Iteration 16, loss = 0.29367211\n",
      "Iteration 81, loss = 0.08376568\n",
      "Iteration 17, loss = 0.28558356\n",
      "Iteration 82, loss = 0.08351143\n",
      "Iteration 18, loss = 0.27924196\n",
      "Iteration 83, loss = 0.08122477\n",
      "Iteration 84, loss = 0.07981843\n",
      "Iteration 85, loss = 0.07898309\n",
      "Iteration 19, loss = 0.27046514\n",
      "Iteration 20, loss = 0.26408072\n",
      "Iteration 21, loss = 0.25828626\n",
      "Iteration 22, loss = 0.25226008\n",
      "Iteration 86, loss = 0.07765251\n",
      "Iteration 87, loss = 0.07826307\n",
      "Iteration 88, loss = 0.07524470\n",
      "Iteration 23, loss = 0.24676333\n",
      "Iteration 89, loss = 0.07609831\n",
      "Iteration 24, loss = 0.24212481\n",
      "Iteration 90, loss = 0.07481112\n",
      "Iteration 25, loss = 0.23552092\n",
      "Iteration 26, loss = 0.23352612\n",
      "Iteration 91, loss = 0.07624748\n",
      "Iteration 27, loss = 0.22843400\n",
      "Iteration 92, loss = 0.07195618\n",
      "Iteration 93, loss = 0.07219563\n",
      "Iteration 94, loss = 0.07010199\n",
      "Iteration 28, loss = 0.22230829\n",
      "Iteration 95, loss = 0.06844817\n",
      "Iteration 29, loss = 0.22003147\n",
      "Iteration 96, loss = 0.06789785\n",
      "Iteration 30, loss = 0.21434123\n",
      "Iteration 97, loss = 0.06741991\n",
      "Iteration 98, loss = 0.06634042\n",
      "Iteration 99, loss = 0.06638028\n",
      "Iteration 31, loss = 0.21051802\n",
      "Iteration 100, loss = 0.06458632\n",
      "Iteration 32, loss = 0.20484931\n",
      "Iteration 33, loss = 0.20160179\n",
      "Iteration 34, loss = 0.19917569\n",
      "Iteration 101, loss = 0.06397818\n",
      "Iteration 102, loss = 0.06350823\n",
      "Iteration 103, loss = 0.06226734\n",
      "Iteration 35, loss = 0.19418063\n",
      "Iteration 104, loss = 0.06354632\n",
      "Iteration 36, loss = 0.18985240\n",
      "Iteration 37, loss = 0.18912626\n",
      "Iteration 38, loss = 0.18375613\n",
      "Iteration 105, loss = 0.06230125\n",
      "Iteration 106, loss = 0.05931938\n",
      "Iteration 39, loss = 0.18001547\n",
      "Iteration 107, loss = 0.05960233\n",
      "Iteration 108, loss = 0.05865228\n",
      "Iteration 40, loss = 0.17830484\n",
      "Iteration 41, loss = 0.17526235\n",
      "Iteration 109, loss = 0.05748600\n",
      "Iteration 42, loss = 0.17521601\n",
      "Iteration 43, loss = 0.17135512\n",
      "Iteration 110, loss = 0.05812841\n",
      "Iteration 111, loss = 0.05712327\n",
      "Iteration 112, loss = 0.05528745\n",
      "Iteration 44, loss = 0.16586870\n",
      "Iteration 45, loss = 0.16328304\n",
      "Iteration 113, loss = 0.05552447\n",
      "Iteration 46, loss = 0.16000915\n",
      "Iteration 47, loss = 0.15647368\n",
      "Iteration 114, loss = 0.05805208\n",
      "Iteration 115, loss = 0.05882021\n",
      "Iteration 48, loss = 0.15613774\n",
      "Iteration 116, loss = 0.05424420\n",
      "Iteration 49, loss = 0.15234524\n",
      "Iteration 117, loss = 0.05787985\n",
      "Iteration 50, loss = 0.15052663\n",
      "Iteration 51, loss = 0.14806133\n",
      "Iteration 52, loss = 0.14655296\n",
      "Iteration 118, loss = 0.05441244\n",
      "Iteration 119, loss = 0.05360929\n",
      "Iteration 120, loss = 0.05538989\n",
      "Iteration 121, loss = 0.05571462\n",
      "Iteration 53, loss = 0.14173693\n",
      "Iteration 54, loss = 0.14067151\n",
      "Iteration 122, loss = 0.05117047\n",
      "Iteration 55, loss = 0.13960397\n",
      "Iteration 123, loss = 0.05027222\n",
      "Iteration 124, loss = 0.05059627\n",
      "Iteration 125, loss = 0.05075174\n",
      "Iteration 56, loss = 0.13961185\n",
      "Iteration 126, loss = 0.04881565\n",
      "Iteration 57, loss = 0.13611458\n",
      "Iteration 58, loss = 0.13452502\n",
      "Iteration 127, loss = 0.04783826\n",
      "Iteration 128, loss = 0.04760262\n",
      "Iteration 129, loss = 0.04622973\n",
      "Iteration 59, loss = 0.12849644\n",
      "Iteration 130, loss = 0.04687016\n",
      "Iteration 131, loss = 0.04814831\n",
      "Iteration 60, loss = 0.13046462\n",
      "Iteration 132, loss = 0.05081527\n",
      "Iteration 61, loss = 0.12681379\n",
      "Iteration 133, loss = 0.04939902\n",
      "Iteration 62, loss = 0.12470628\n",
      "Iteration 134, loss = 0.04808569\n",
      "Iteration 63, loss = 0.12272630\n",
      "Iteration 135, loss = 0.04307037\n",
      "Iteration 64, loss = 0.11991450\n",
      "Iteration 136, loss = 0.04426764\n",
      "Iteration 65, loss = 0.11760071\n",
      "Iteration 137, loss = 0.04337474\n",
      "Iteration 66, loss = 0.11621774\n",
      "Iteration 67, loss = 0.11437758\n",
      "Iteration 138, loss = 0.04484705\n",
      "Iteration 68, loss = 0.11278383\n",
      "Iteration 69, loss = 0.11321977\n",
      "Iteration 139, loss = 0.04608036\n",
      "Iteration 140, loss = 0.04587010\n",
      "Iteration 141, loss = 0.04078293\n",
      "Iteration 142, loss = 0.04146713\n",
      "Iteration 70, loss = 0.11063010\n",
      "Iteration 143, loss = 0.04058764\n",
      "Iteration 71, loss = 0.10862999\n",
      "Iteration 144, loss = 0.04202208\n",
      "Iteration 72, loss = 0.10726580\n",
      "Iteration 145, loss = 0.04294527\n",
      "Iteration 73, loss = 0.10602208\n",
      "Iteration 74, loss = 0.10589270\n",
      "Iteration 75, loss = 0.10665479\n",
      "Iteration 76, loss = 0.10339789\n",
      "Iteration 146, loss = 0.04150791\n",
      "Iteration 77, loss = 0.10931592\n",
      "Iteration 78, loss = 0.10398693\n",
      "Iteration 147, loss = 0.04004898\n",
      "Iteration 79, loss = 0.11131802\n",
      "Iteration 80, loss = 0.11034578\n",
      "Iteration 81, loss = 0.10631988\n",
      "Iteration 148, loss = 0.03842332\n",
      "Iteration 82, loss = 0.09638834\n",
      "Iteration 83, loss = 0.10324291\n",
      "Iteration 149, loss = 0.04097150\n",
      "Iteration 84, loss = 0.09560380\n",
      "Iteration 150, loss = 0.04305034\n",
      "Iteration 85, loss = 0.09231288\n",
      "Iteration 151, loss = 0.04514979\n",
      "Iteration 86, loss = 0.09419325\n",
      "Iteration 87, loss = 0.08970988\n",
      "Iteration 152, loss = 0.03993128\n",
      "Iteration 88, loss = 0.08810571\n",
      "Iteration 153, loss = 0.03751412\n",
      "Iteration 89, loss = 0.08668010\n",
      "Iteration 154, loss = 0.03787304\n",
      "Iteration 90, loss = 0.08760762\n",
      "Iteration 155, loss = 0.03760126\n",
      "Iteration 91, loss = 0.08315892\n",
      "Iteration 92, loss = 0.08263630\n",
      "Iteration 156, loss = 0.03651534\n",
      "Iteration 93, loss = 0.08152764\n",
      "Iteration 94, loss = 0.07896137\n",
      "Iteration 95, loss = 0.07980281\n",
      "Iteration 157, loss = 0.03684997\n",
      "Iteration 96, loss = 0.07864824\n",
      "Iteration 97, loss = 0.07664533\n",
      "Iteration 158, loss = 0.03787520\n",
      "Iteration 98, loss = 0.07582758\n",
      "Iteration 159, loss = 0.03447924\n",
      "Iteration 160, loss = 0.03467021\n",
      "Iteration 99, loss = 0.07605004\n",
      "Iteration 161, loss = 0.03403563\n",
      "Iteration 100, loss = 0.07378472\n",
      "Iteration 101, loss = 0.07375434\n",
      "Iteration 162, loss = 0.03485746\n",
      "Iteration 102, loss = 0.07288434\n",
      "Iteration 163, loss = 0.03376611\n",
      "Iteration 103, loss = 0.07200751\n",
      "Iteration 164, loss = 0.03320380\n",
      "Iteration 104, loss = 0.07072727\n",
      "Iteration 165, loss = 0.03364634\n",
      "Iteration 105, loss = 0.06823989\n",
      "Iteration 166, loss = 0.03399644\n",
      "Iteration 106, loss = 0.06884903\n",
      "Iteration 167, loss = 0.03315008\n",
      "Iteration 107, loss = 0.07041848\n",
      "Iteration 168, loss = 0.03238619\n",
      "Iteration 108, loss = 0.07007941\n",
      "Iteration 169, loss = 0.03332818\n",
      "Iteration 109, loss = 0.06988489\n",
      "Iteration 170, loss = 0.03088616\n",
      "Iteration 110, loss = 0.07319939\n",
      "Iteration 111, loss = 0.06707439\n",
      "Iteration 171, loss = 0.03391142\n",
      "Iteration 112, loss = 0.06438102\n",
      "Iteration 113, loss = 0.06139687\n",
      "Iteration 172, loss = 0.03031804\n",
      "Iteration 114, loss = 0.06169952\n",
      "Iteration 173, loss = 0.03104652\n",
      "Iteration 115, loss = 0.06264267\n",
      "Iteration 174, loss = 0.03065532\n",
      "Iteration 116, loss = 0.05995154\n",
      "Iteration 117, loss = 0.06091644\n",
      "Iteration 175, loss = 0.03149029\n",
      "Iteration 118, loss = 0.05998894\n",
      "Iteration 119, loss = 0.05853255Iteration 176, loss = 0.03357875\n",
      "\n",
      "Iteration 177, loss = 0.03620335\n",
      "Iteration 120, loss = 0.05762390\n",
      "Iteration 178, loss = 0.03213140\n",
      "Iteration 121, loss = 0.06299530\n",
      "Iteration 122, loss = 0.06449254\n",
      "Iteration 123, loss = 0.05729353\n",
      "Iteration 179, loss = 0.02842949\n",
      "Iteration 124, loss = 0.05503398\n",
      "Iteration 180, loss = 0.03087275\n",
      "Iteration 125, loss = 0.05277505\n",
      "Iteration 181, loss = 0.02912866\n",
      "Iteration 126, loss = 0.05416673\n",
      "Iteration 182, loss = 0.02876593\n",
      "Iteration 127, loss = 0.05206117\n",
      "Iteration 183, loss = 0.02998819\n",
      "Iteration 128, loss = 0.05198096\n",
      "Iteration 184, loss = 0.02899093\n",
      "Iteration 185, loss = 0.02723691\n",
      "Iteration 186, loss = 0.02758823\n",
      "Iteration 129, loss = 0.05199805\n",
      "Iteration 187, loss = 0.02713235\n",
      "Iteration 130, loss = 0.05144113\n",
      "Iteration 188, loss = 0.02904487\n",
      "Iteration 131, loss = 0.04990406\n",
      "Iteration 189, loss = 0.03194081\n",
      "Iteration 190, loss = 0.02898890\n",
      "Iteration 132, loss = 0.04949471\n",
      "Iteration 191, loss = 0.03231573\n",
      "Iteration 192, loss = 0.02851201\n",
      "Iteration 133, loss = 0.04924879\n",
      "Iteration 193, loss = 0.02502001\n",
      "Iteration 134, loss = 0.04779986\n",
      "Iteration 135, loss = 0.04888605\n",
      "Iteration 194, loss = 0.02608178\n",
      "Iteration 136, loss = 0.04751676\n",
      "Iteration 195, loss = 0.02500232\n",
      "Iteration 137, loss = 0.04695867\n",
      "Iteration 196, loss = 0.02467347\n",
      "Iteration 197, loss = 0.02478392\n",
      "Iteration 138, loss = 0.04834837\n",
      "Iteration 139, loss = 0.04545939\n",
      "Iteration 198, loss = 0.02460544\n",
      "Iteration 140, loss = 0.04506551\n",
      "Iteration 141, loss = 0.04548106\n",
      "Iteration 199, loss = 0.02471814\n",
      "Iteration 142, loss = 0.05423551\n",
      "Iteration 200, loss = 0.02388174\n",
      "Iteration 201, loss = 0.02411454\n",
      "Iteration 143, loss = 0.05166504\n",
      "Iteration 144, loss = 0.05150707\n",
      "Iteration 202, loss = 0.02573494\n",
      "Iteration 145, loss = 0.04965717\n",
      "Iteration 146, loss = 0.04840060\n",
      "Iteration 203, loss = 0.02340054\n",
      "Iteration 204, loss = 0.02321348\n",
      "Iteration 147, loss = 0.04238879\n",
      "Iteration 205, loss = 0.02357955\n",
      "Iteration 206, loss = 0.02363722\n",
      "Iteration 148, loss = 0.04218359\n",
      "Iteration 149, loss = 0.04140585\n",
      "Iteration 207, loss = 0.02322116\n",
      "Iteration 208, loss = 0.02372264\n",
      "Iteration 209, loss = 0.02777214\n",
      "Iteration 150, loss = 0.04188117\n",
      "Iteration 210, loss = 0.02579588\n",
      "Iteration 151, loss = 0.04072569\n",
      "Iteration 211, loss = 0.02439654\n",
      "Iteration 212, loss = 0.02695897\n",
      "Iteration 152, loss = 0.03995837\n",
      "Iteration 213, loss = 0.02448365\n",
      "Iteration 153, loss = 0.03977324\n",
      "Iteration 214, loss = 0.02365261\n",
      "Iteration 215, loss = 0.02493379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 154, loss = 0.04095978\n",
      "Iteration 155, loss = 0.04114631\n",
      "Iteration 156, loss = 0.03835294\n",
      "Iteration 157, loss = 0.03867055\n",
      "Iteration 158, loss = 0.03741934\n",
      "Iteration 159, loss = 0.03743587\n",
      "Iteration 1, loss = 1.11546392\n",
      "Iteration 160, loss = 0.03688386\n",
      "Iteration 2, loss = 0.70512300\n",
      "Iteration 161, loss = 0.03658177\n",
      "Iteration 162, loss = 0.03618955\n",
      "Iteration 3, loss = 0.55761945\n",
      "Iteration 163, loss = 0.03570158\n",
      "Iteration 4, loss = 0.49507682\n",
      "Iteration 164, loss = 0.03527464\n",
      "Iteration 5, loss = 0.45304607\n",
      "Iteration 6, loss = 0.42322252\n",
      "Iteration 7, loss = 0.40247637\n",
      "Iteration 165, loss = 0.03500418\n",
      "Iteration 166, loss = 0.03623390\n",
      "Iteration 167, loss = 0.03531360\n",
      "Iteration 168, loss = 0.03568976\n",
      "Iteration 8, loss = 0.38501099\n",
      "Iteration 9, loss = 0.37123369\n",
      "Iteration 10, loss = 0.35690007\n",
      "Iteration 169, loss = 0.03394022\n",
      "Iteration 11, loss = 0.34558709\n",
      "Iteration 170, loss = 0.03589890\n",
      "Iteration 12, loss = 0.33521061\n",
      "Iteration 171, loss = 0.03378664\n",
      "Iteration 172, loss = 0.03333841\n",
      "Iteration 13, loss = 0.32511346\n",
      "Iteration 14, loss = 0.31489074\n",
      "Iteration 173, loss = 0.03333971\n",
      "Iteration 15, loss = 0.30706337\n",
      "Iteration 16, loss = 0.29894793\n",
      "Iteration 174, loss = 0.03398459\n",
      "Iteration 175, loss = 0.03391408\n",
      "Iteration 17, loss = 0.29057201\n",
      "Iteration 176, loss = 0.03232966\n",
      "Iteration 18, loss = 0.28628541\n",
      "Iteration 177, loss = 0.03311135\n",
      "Iteration 19, loss = 0.27605563\n",
      "Iteration 20, loss = 0.27167157\n",
      "Iteration 178, loss = 0.03200101\n",
      "Iteration 21, loss = 0.26242547\n",
      "Iteration 179, loss = 0.03306098\n",
      "Iteration 180, loss = 0.03274461\n",
      "Iteration 22, loss = 0.25575753\n",
      "Iteration 23, loss = 0.24919969\n",
      "Iteration 181, loss = 0.03308423\n",
      "Iteration 24, loss = 0.24241315\n",
      "Iteration 25, loss = 0.23803698\n",
      "Iteration 26, loss = 0.23261186\n",
      "Iteration 182, loss = 0.03382824\n",
      "Iteration 27, loss = 0.22632075\n",
      "Iteration 183, loss = 0.03576390\n",
      "Iteration 28, loss = 0.21986879\n",
      "Iteration 184, loss = 0.03132162\n",
      "Iteration 185, loss = 0.03152885\n",
      "Iteration 29, loss = 0.21751354\n",
      "Iteration 186, loss = 0.03246623\n",
      "Iteration 30, loss = 0.21069113\n",
      "Iteration 187, loss = 0.03439302\n",
      "Iteration 31, loss = 0.20689322\n",
      "Iteration 188, loss = 0.02996346\n",
      "Iteration 32, loss = 0.20242633\n",
      "Iteration 33, loss = 0.19933814\n",
      "Iteration 189, loss = 0.03258618\n",
      "Iteration 34, loss = 0.19378894\n",
      "Iteration 35, loss = 0.19111759\n",
      "Iteration 36, loss = 0.19119091\n",
      "Iteration 190, loss = 0.02893165\n",
      "Iteration 37, loss = 0.18706258\n",
      "Iteration 38, loss = 0.18481505\n",
      "Iteration 39, loss = 0.17887982\n",
      "Iteration 40, loss = 0.17263188\n",
      "Iteration 191, loss = 0.02852605\n",
      "Iteration 41, loss = 0.17103529\n",
      "Iteration 42, loss = 0.16614859\n",
      "Iteration 43, loss = 0.16267762\n",
      "Iteration 192, loss = 0.03161266\n",
      "Iteration 44, loss = 0.16100475\n",
      "Iteration 193, loss = 0.04007070\n",
      "Iteration 194, loss = 0.03142049\n",
      "Iteration 195, loss = 0.02957917\n",
      "Iteration 45, loss = 0.15785980\n",
      "Iteration 196, loss = 0.02949680\n",
      "Iteration 197, loss = 0.02801265\n",
      "Iteration 46, loss = 0.15436399\n",
      "Iteration 198, loss = 0.02974942\n",
      "Iteration 47, loss = 0.15284131\n",
      "Iteration 199, loss = 0.02801143\n",
      "Iteration 48, loss = 0.15111591\n",
      "Iteration 200, loss = 0.02783453\n",
      "Iteration 49, loss = 0.14756937\n",
      "Iteration 50, loss = 0.14410886\n",
      "Iteration 201, loss = 0.02810001\n",
      "Iteration 51, loss = 0.14592846\n",
      "Iteration 202, loss = 0.02729845\n",
      "Iteration 52, loss = 0.14273798\n",
      "Iteration 203, loss = 0.02584785\n",
      "Iteration 53, loss = 0.14019631\n",
      "Iteration 54, loss = 0.13756116\n",
      "Iteration 204, loss = 0.02782027\n",
      "Iteration 55, loss = 0.13569109\n",
      "Iteration 205, loss = 0.03253883\n",
      "Iteration 56, loss = 0.13336543\n",
      "Iteration 206, loss = 0.03445941\n",
      "Iteration 57, loss = 0.12932754\n",
      "Iteration 207, loss = 0.02849112\n",
      "Iteration 208, loss = 0.03006884\n",
      "Iteration 58, loss = 0.12729506\n",
      "Iteration 59, loss = 0.12625278\n",
      "Iteration 209, loss = 0.04206495\n",
      "Iteration 210, loss = 0.04834434\n",
      "Iteration 60, loss = 0.12267264\n",
      "Iteration 211, loss = 0.04912370\n",
      "Iteration 61, loss = 0.12132987\n",
      "Iteration 212, loss = 0.05163326\n",
      "Iteration 62, loss = 0.11943720\n",
      "Iteration 213, loss = 0.05354813\n",
      "Iteration 63, loss = 0.11732965\n",
      "Iteration 214, loss = 0.04143825\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.11541867\n",
      "Iteration 65, loss = 0.11662243\n",
      "Iteration 66, loss = 0.11512729\n",
      "Iteration 67, loss = 0.10986910\n",
      "Iteration 68, loss = 0.10945347\n",
      "Iteration 69, loss = 0.11004878\n",
      "Iteration 70, loss = 0.11145996\n",
      "Iteration 71, loss = 0.11012558\n",
      "Iteration 72, loss = 0.10802997\n",
      "Iteration 1, loss = 1.21071904\n",
      "Iteration 2, loss = 0.76947401\n",
      "Iteration 73, loss = 0.10086309\n",
      "Iteration 3, loss = 0.58283051\n",
      "Iteration 74, loss = 0.10413488\n",
      "Iteration 4, loss = 0.50570147\n",
      "Iteration 75, loss = 0.10073778\n",
      "Iteration 5, loss = 0.46458247\n",
      "Iteration 6, loss = 0.43747658\n",
      "Iteration 76, loss = 0.09895773\n",
      "Iteration 77, loss = 0.09645147\n",
      "Iteration 7, loss = 0.41637917\n",
      "Iteration 78, loss = 0.09641021\n",
      "Iteration 8, loss = 0.39737663\n",
      "Iteration 79, loss = 0.09367913\n",
      "Iteration 9, loss = 0.38247727\n",
      "Iteration 10, loss = 0.36834907\n",
      "Iteration 80, loss = 0.09354246\n",
      "Iteration 81, loss = 0.09017725\n",
      "Iteration 11, loss = 0.35699038\n",
      "Iteration 82, loss = 0.09195880\n",
      "Iteration 12, loss = 0.34508439\n",
      "Iteration 83, loss = 0.09120136\n",
      "Iteration 13, loss = 0.33449259\n",
      "Iteration 14, loss = 0.32409974\n",
      "Iteration 84, loss = 0.08961864\n",
      "Iteration 15, loss = 0.31452477\n",
      "Iteration 16, loss = 0.30661669\n",
      "Iteration 85, loss = 0.09022854\n",
      "Iteration 17, loss = 0.29819574\n",
      "Iteration 18, loss = 0.29007357\n",
      "Iteration 86, loss = 0.08783174\n",
      "Iteration 87, loss = 0.08554301\n",
      "Iteration 19, loss = 0.28205278\n",
      "Iteration 88, loss = 0.08712443\n",
      "Iteration 89, loss = 0.08514672\n",
      "Iteration 20, loss = 0.27400980\n",
      "Iteration 90, loss = 0.08407327\n",
      "Iteration 91, loss = 0.08404269\n",
      "Iteration 21, loss = 0.27269903\n",
      "Iteration 92, loss = 0.08106030\n",
      "Iteration 22, loss = 0.26244033\n",
      "Iteration 93, loss = 0.07875296\n",
      "Iteration 23, loss = 0.25462871\n",
      "Iteration 94, loss = 0.07897900\n",
      "Iteration 24, loss = 0.25041174\n",
      "Iteration 25, loss = 0.24240090\n",
      "Iteration 26, loss = 0.23650065\n",
      "Iteration 95, loss = 0.07987654\n",
      "Iteration 27, loss = 0.23197008\n",
      "Iteration 96, loss = 0.08454205\n",
      "Iteration 28, loss = 0.22429830\n",
      "Iteration 29, loss = 0.22125475\n",
      "Iteration 30, loss = 0.21488955\n",
      "Iteration 97, loss = 0.07910602\n",
      "Iteration 31, loss = 0.21211422\n",
      "Iteration 98, loss = 0.07512031\n",
      "Iteration 32, loss = 0.20608176\n",
      "Iteration 99, loss = 0.07585768\n",
      "Iteration 33, loss = 0.20356367\n",
      "Iteration 100, loss = 0.07536422\n",
      "Iteration 34, loss = 0.19740579\n",
      "Iteration 35, loss = 0.19395156\n",
      "Iteration 101, loss = 0.07210875\n",
      "Iteration 36, loss = 0.18776713\n",
      "Iteration 37, loss = 0.18594518\n",
      "Iteration 102, loss = 0.07000031\n",
      "Iteration 103, loss = 0.07037401\n",
      "Iteration 104, loss = 0.07271167\n",
      "Iteration 105, loss = 0.07290258\n",
      "Iteration 38, loss = 0.18467872\n",
      "Iteration 106, loss = 0.07109737\n",
      "Iteration 39, loss = 0.18176911\n",
      "Iteration 107, loss = 0.06840735\n",
      "Iteration 108, loss = 0.06570302\n",
      "Iteration 109, loss = 0.06311174\n",
      "Iteration 110, loss = 0.06130157\n",
      "Iteration 40, loss = 0.17842985\n",
      "Iteration 41, loss = 0.16922646\n",
      "Iteration 111, loss = 0.06078116\n",
      "Iteration 42, loss = 0.16897033\n",
      "Iteration 43, loss = 0.16687023\n",
      "Iteration 112, loss = 0.06146703\n",
      "Iteration 44, loss = 0.16386551\n",
      "Iteration 113, loss = 0.06335910\n",
      "Iteration 114, loss = 0.06239188\n",
      "Iteration 45, loss = 0.15742844\n",
      "Iteration 115, loss = 0.05740081\n",
      "Iteration 46, loss = 0.15278484\n",
      "Iteration 47, loss = 0.15262531\n",
      "Iteration 48, loss = 0.14755758\n",
      "Iteration 49, loss = 0.14470875\n",
      "Iteration 116, loss = 0.05798938\n",
      "Iteration 50, loss = 0.14484907\n",
      "Iteration 51, loss = 0.14094241\n",
      "Iteration 52, loss = 0.13981770\n",
      "Iteration 117, loss = 0.05888503\n",
      "Iteration 53, loss = 0.13665592\n",
      "Iteration 54, loss = 0.13371959\n",
      "Iteration 118, loss = 0.05989135\n",
      "Iteration 55, loss = 0.13058521\n",
      "Iteration 119, loss = 0.06250581\n",
      "Iteration 120, loss = 0.06240097\n",
      "Iteration 56, loss = 0.12995331\n",
      "Iteration 121, loss = 0.06755752\n",
      "Iteration 57, loss = 0.12449968\n",
      "Iteration 58, loss = 0.12484554\n",
      "Iteration 122, loss = 0.06947956\n",
      "Iteration 59, loss = 0.12198346\n",
      "Iteration 123, loss = 0.06339351\n",
      "Iteration 124, loss = 0.05365910\n",
      "Iteration 60, loss = 0.12202887\n",
      "Iteration 125, loss = 0.05538264\n",
      "Iteration 61, loss = 0.11760145\n",
      "Iteration 62, loss = 0.11592479\n",
      "Iteration 63, loss = 0.11811199\n",
      "Iteration 126, loss = 0.05526764\n",
      "Iteration 127, loss = 0.05734086\n",
      "Iteration 64, loss = 0.11165231\n",
      "Iteration 128, loss = 0.05372707\n",
      "Iteration 129, loss = 0.05248417\n",
      "Iteration 65, loss = 0.11090762\n",
      "Iteration 66, loss = 0.11085375\n",
      "Iteration 67, loss = 0.10644638\n",
      "Iteration 130, loss = 0.05012617\n",
      "Iteration 68, loss = 0.10681846\n",
      "Iteration 69, loss = 0.10815401\n",
      "Iteration 131, loss = 0.05058523\n",
      "Iteration 70, loss = 0.10597800\n",
      "Iteration 132, loss = 0.05029107\n",
      "Iteration 71, loss = 0.10513984\n",
      "Iteration 133, loss = 0.04870551\n",
      "Iteration 72, loss = 0.10344981\n",
      "Iteration 73, loss = 0.09608775\n",
      "Iteration 74, loss = 0.09695214\n",
      "Iteration 134, loss = 0.04768318\n",
      "Iteration 75, loss = 0.09088667\n",
      "Iteration 135, loss = 0.04681047\n",
      "Iteration 76, loss = 0.09068245\n",
      "Iteration 136, loss = 0.04833019\n",
      "Iteration 137, loss = 0.04561192\n",
      "Iteration 77, loss = 0.09012028\n",
      "Iteration 78, loss = 0.08782565\n",
      "Iteration 79, loss = 0.08876370\n",
      "Iteration 138, loss = 0.04540871\n",
      "Iteration 139, loss = 0.04605444\n",
      "Iteration 80, loss = 0.08533604\n",
      "Iteration 140, loss = 0.04814262\n",
      "Iteration 141, loss = 0.04436704\n",
      "Iteration 81, loss = 0.08417340\n",
      "Iteration 142, loss = 0.04490751\n",
      "Iteration 82, loss = 0.08227570\n",
      "Iteration 83, loss = 0.08085235\n",
      "Iteration 84, loss = 0.07962917\n",
      "Iteration 143, loss = 0.04351926\n",
      "Iteration 85, loss = 0.07895401\n",
      "Iteration 144, loss = 0.04427881\n",
      "Iteration 86, loss = 0.07703906\n",
      "Iteration 145, loss = 0.04601932\n",
      "Iteration 146, loss = 0.04549359\n",
      "Iteration 87, loss = 0.07302834\n",
      "Iteration 147, loss = 0.04226158\n",
      "Iteration 88, loss = 0.07465618\n",
      "Iteration 89, loss = 0.07489737\n",
      "Iteration 148, loss = 0.04196037\n",
      "Iteration 149, loss = 0.04092450\n",
      "Iteration 90, loss = 0.07616033\n",
      "Iteration 150, loss = 0.04072221\n",
      "Iteration 91, loss = 0.07522960\n",
      "Iteration 151, loss = 0.04011795\n",
      "Iteration 92, loss = 0.06938702\n",
      "Iteration 152, loss = 0.04080111\n",
      "Iteration 93, loss = 0.06829283\n",
      "Iteration 153, loss = 0.04008292\n",
      "Iteration 94, loss = 0.07137123\n",
      "Iteration 154, loss = 0.03971247\n",
      "Iteration 95, loss = 0.06546966\n",
      "Iteration 155, loss = 0.04020096\n",
      "Iteration 96, loss = 0.06376462\n",
      "Iteration 97, loss = 0.06250335\n",
      "Iteration 98, loss = 0.06245183\n",
      "Iteration 156, loss = 0.03874857\n",
      "Iteration 99, loss = 0.06002553\n",
      "Iteration 100, loss = 0.06058964\n",
      "Iteration 157, loss = 0.03794866\n",
      "Iteration 101, loss = 0.05866400\n",
      "Iteration 102, loss = 0.05719115\n",
      "Iteration 103, loss = 0.05752749\n",
      "Iteration 158, loss = 0.03717239\n",
      "Iteration 159, loss = 0.03754142\n",
      "Iteration 104, loss = 0.05729064\n",
      "Iteration 160, loss = 0.03835020\n",
      "Iteration 105, loss = 0.05503142\n",
      "Iteration 161, loss = 0.03857390\n",
      "Iteration 106, loss = 0.05441350\n",
      "Iteration 162, loss = 0.04066566\n",
      "Iteration 163, loss = 0.03856712\n",
      "Iteration 107, loss = 0.05557579\n",
      "Iteration 164, loss = 0.03953523\n",
      "Iteration 108, loss = 0.05337579\n",
      "Iteration 165, loss = 0.03584468\n",
      "Iteration 109, loss = 0.05458845\n",
      "Iteration 166, loss = 0.03539568\n",
      "Iteration 110, loss = 0.05727317\n",
      "Iteration 167, loss = 0.03606777\n",
      "Iteration 168, loss = 0.03520267\n",
      "Iteration 169, loss = 0.03591548\n",
      "Iteration 111, loss = 0.05157186\n",
      "Iteration 170, loss = 0.03475120\n",
      "Iteration 112, loss = 0.04937986\n",
      "Iteration 171, loss = 0.03605519\n",
      "Iteration 113, loss = 0.04952951\n",
      "Iteration 172, loss = 0.03549223\n",
      "Iteration 114, loss = 0.04797369\n",
      "Iteration 173, loss = 0.03404081\n",
      "Iteration 115, loss = 0.04846947\n",
      "Iteration 174, loss = 0.03498661\n",
      "Iteration 116, loss = 0.04697793\n",
      "Iteration 175, loss = 0.03387834\n",
      "Iteration 176, loss = 0.03345153\n",
      "Iteration 117, loss = 0.04692489\n",
      "Iteration 177, loss = 0.03445660\n",
      "Iteration 178, loss = 0.03555230\n",
      "Iteration 118, loss = 0.04578374\n",
      "Iteration 179, loss = 0.03870375\n",
      "Iteration 119, loss = 0.04410608\n",
      "Iteration 180, loss = 0.03681105\n",
      "Iteration 120, loss = 0.04378640\n",
      "Iteration 181, loss = 0.04096819\n",
      "Iteration 121, loss = 0.04195822\n",
      "Iteration 182, loss = 0.03844807\n",
      "Iteration 183, loss = 0.03625670\n",
      "Iteration 122, loss = 0.04403926\n",
      "Iteration 184, loss = 0.03218817\n",
      "Iteration 123, loss = 0.04160115\n",
      "Iteration 185, loss = 0.03104201\n",
      "Iteration 186, loss = 0.03230788\n",
      "Iteration 124, loss = 0.04183339\n",
      "Iteration 187, loss = 0.03110973\n",
      "Iteration 188, loss = 0.03005599\n",
      "Iteration 125, loss = 0.04059037\n",
      "Iteration 126, loss = 0.04237468\n",
      "Iteration 127, loss = 0.04137806\n",
      "Iteration 189, loss = 0.03778343\n",
      "Iteration 128, loss = 0.04432889\n",
      "Iteration 190, loss = 0.03705830\n",
      "Iteration 191, loss = 0.03359220\n",
      "Iteration 129, loss = 0.04355680\n",
      "Iteration 192, loss = 0.03250254\n",
      "Iteration 130, loss = 0.04336877\n",
      "Iteration 131, loss = 0.03929234\n",
      "Iteration 193, loss = 0.03124917\n",
      "Iteration 132, loss = 0.03544752\n",
      "Iteration 194, loss = 0.03166299\n",
      "Iteration 195, loss = 0.02961990\n",
      "Iteration 133, loss = 0.03553694\n",
      "Iteration 196, loss = 0.03004549\n",
      "Iteration 197, loss = 0.02949805\n",
      "Iteration 134, loss = 0.03663715\n",
      "Iteration 135, loss = 0.03527761\n",
      "Iteration 198, loss = 0.02896540\n",
      "Iteration 136, loss = 0.03605464\n",
      "Iteration 199, loss = 0.02820868\n",
      "Iteration 137, loss = 0.03361271\n",
      "Iteration 200, loss = 0.02820213\n",
      "Iteration 201, loss = 0.02873433\n",
      "Iteration 138, loss = 0.03480518\n",
      "Iteration 202, loss = 0.02926738\n",
      "Iteration 139, loss = 0.03660039\n",
      "Iteration 140, loss = 0.03510347\n",
      "Iteration 203, loss = 0.02974361\n",
      "Iteration 141, loss = 0.03614501\n",
      "Iteration 204, loss = 0.02777954\n",
      "Iteration 205, loss = 0.03014807\n",
      "Iteration 142, loss = 0.03510302\n",
      "Iteration 206, loss = 0.03044250\n",
      "Iteration 143, loss = 0.03630399\n",
      "Iteration 144, loss = 0.03837978\n",
      "Iteration 145, loss = 0.03031282\n",
      "Iteration 207, loss = 0.03535358\n",
      "Iteration 208, loss = 0.03501768\n",
      "Iteration 146, loss = 0.02930293\n",
      "Iteration 209, loss = 0.03096964\n",
      "Iteration 210, loss = 0.02972251\n",
      "Iteration 147, loss = 0.03092316\n",
      "Iteration 211, loss = 0.03187469\n",
      "Iteration 148, loss = 0.02818519\n",
      "Iteration 149, loss = 0.02791187\n",
      "Iteration 212, loss = 0.02710783\n",
      "Iteration 150, loss = 0.02763890\n",
      "Iteration 213, loss = 0.02748172\n",
      "Iteration 214, loss = 0.02804839\n",
      "Iteration 151, loss = 0.02721134\n",
      "Iteration 215, loss = 0.02720491\n",
      "Iteration 152, loss = 0.02697940\n",
      "Iteration 153, loss = 0.02685531\n",
      "Iteration 216, loss = 0.02453144\n",
      "Iteration 154, loss = 0.02600171\n",
      "Iteration 217, loss = 0.02640995\n",
      "Iteration 218, loss = 0.02800004\n",
      "Iteration 219, loss = 0.02538821\n",
      "Iteration 155, loss = 0.02764490\n",
      "Iteration 220, loss = 0.02737038\n",
      "Iteration 156, loss = 0.02637466\n",
      "Iteration 221, loss = 0.02756713\n",
      "Iteration 222, loss = 0.02952420\n",
      "Iteration 157, loss = 0.02637433\n",
      "Iteration 223, loss = 0.02918896\n",
      "Iteration 158, loss = 0.03067125\n",
      "Iteration 159, loss = 0.03191276\n",
      "Iteration 224, loss = 0.02820511\n",
      "Iteration 225, loss = 0.02557708\n",
      "Iteration 160, loss = 0.02536814\n",
      "Iteration 226, loss = 0.02439618\n",
      "Iteration 227, loss = 0.02391227\n",
      "Iteration 161, loss = 0.02473091\n",
      "Iteration 228, loss = 0.02436075\n",
      "Iteration 229, loss = 0.02375075\n",
      "Iteration 162, loss = 0.02413808\n",
      "Iteration 230, loss = 0.02643308\n",
      "Iteration 163, loss = 0.02500332\n",
      "Iteration 231, loss = 0.03099747\n",
      "Iteration 164, loss = 0.02482647\n",
      "Iteration 165, loss = 0.02443725\n",
      "Iteration 232, loss = 0.02544235\n",
      "Iteration 166, loss = 0.02427253\n",
      "Iteration 167, loss = 0.02233446\n",
      "Iteration 233, loss = 0.02311111\n",
      "Iteration 234, loss = 0.02383656\n",
      "Iteration 168, loss = 0.02188787\n",
      "Iteration 235, loss = 0.02362048\n",
      "Iteration 169, loss = 0.02176050\n",
      "Iteration 170, loss = 0.02087467\n",
      "Iteration 236, loss = 0.02451141\n",
      "Iteration 171, loss = 0.02108812\n",
      "Iteration 237, loss = 0.02310146\n",
      "Iteration 172, loss = 0.02152294\n",
      "Iteration 238, loss = 0.02233705\n",
      "Iteration 239, loss = 0.02306597\n",
      "Iteration 173, loss = 0.02211020\n",
      "Iteration 240, loss = 0.02263016\n",
      "Iteration 174, loss = 0.02126974\n",
      "Iteration 241, loss = 0.02272576\n",
      "Iteration 242, loss = 0.02385617\n",
      "Iteration 243, loss = 0.02230467\n",
      "Iteration 175, loss = 0.02069579\n",
      "Iteration 176, loss = 0.01999297\n",
      "Iteration 177, loss = 0.02008438\n",
      "Iteration 244, loss = 0.02643208\n",
      "Iteration 178, loss = 0.01938432\n",
      "Iteration 245, loss = 0.02989844\n",
      "Iteration 246, loss = 0.02698447\n",
      "Iteration 179, loss = 0.01939938\n",
      "Iteration 247, loss = 0.02806792\n",
      "Iteration 180, loss = 0.01875404\n",
      "Iteration 248, loss = 0.02438240\n",
      "Iteration 181, loss = 0.01897785\n",
      "Iteration 182, loss = 0.01886653\n",
      "Iteration 249, loss = 0.02472814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 183, loss = 0.01847640\n",
      "Iteration 184, loss = 0.01811314\n",
      "Iteration 185, loss = 0.01814294\n",
      "Iteration 186, loss = 0.01797985\n",
      "Iteration 187, loss = 0.01928453\n",
      "Iteration 188, loss = 0.01787328\n",
      "Iteration 189, loss = 0.01732434\n",
      "Iteration 190, loss = 0.01806548\n",
      "Iteration 191, loss = 0.01996873\n",
      "Iteration 1, loss = 1.15863762\n",
      "Iteration 2, loss = 0.77239793\n",
      "Iteration 192, loss = 0.02288471\n",
      "Iteration 3, loss = 0.61466223\n",
      "Iteration 193, loss = 0.01866161\n",
      "Iteration 194, loss = 0.01801480\n",
      "Iteration 4, loss = 0.52834789\n",
      "Iteration 195, loss = 0.01928603\n",
      "Iteration 5, loss = 0.48282392\n",
      "Iteration 6, loss = 0.44223515\n",
      "Iteration 7, loss = 0.41830498\n",
      "Iteration 196, loss = 0.01796699\n",
      "Iteration 8, loss = 0.39774531\n",
      "Iteration 9, loss = 0.38182924\n",
      "Iteration 197, loss = 0.01795208\n",
      "Iteration 198, loss = 0.01653288\n",
      "Iteration 10, loss = 0.36962171\n",
      "Iteration 199, loss = 0.01671061\n",
      "Iteration 11, loss = 0.35600582\n",
      "Iteration 12, loss = 0.34710985\n",
      "Iteration 200, loss = 0.01523445\n",
      "Iteration 13, loss = 0.33553164\n",
      "Iteration 14, loss = 0.32602174\n",
      "Iteration 201, loss = 0.01729964\n",
      "Iteration 15, loss = 0.31919018\n",
      "Iteration 16, loss = 0.30846920\n",
      "Iteration 202, loss = 0.01658220\n",
      "Iteration 17, loss = 0.30167867\n",
      "Iteration 18, loss = 0.29374653\n",
      "Iteration 203, loss = 0.01646753\n",
      "Iteration 19, loss = 0.28538726\n",
      "Iteration 20, loss = 0.28008068\n",
      "Iteration 21, loss = 0.27255697\n",
      "Iteration 204, loss = 0.01495932\n",
      "Iteration 22, loss = 0.26606926\n",
      "Iteration 205, loss = 0.01589896\n",
      "Iteration 23, loss = 0.26002251\n",
      "Iteration 24, loss = 0.25392559\n",
      "Iteration 206, loss = 0.01574598\n",
      "Iteration 25, loss = 0.24753618\n",
      "Iteration 26, loss = 0.24341603\n",
      "Iteration 207, loss = 0.01434006\n",
      "Iteration 27, loss = 0.23687870\n",
      "Iteration 28, loss = 0.23273840\n",
      "Iteration 208, loss = 0.01494278\n",
      "Iteration 29, loss = 0.22805974\n",
      "Iteration 30, loss = 0.22239669\n",
      "Iteration 209, loss = 0.01394731\n",
      "Iteration 31, loss = 0.21854182\n",
      "Iteration 210, loss = 0.01356665\n",
      "Iteration 32, loss = 0.21303464\n",
      "Iteration 33, loss = 0.20916881\n",
      "Iteration 211, loss = 0.01617329\n",
      "Iteration 212, loss = 0.01629468\n",
      "Iteration 34, loss = 0.20450991\n",
      "Iteration 35, loss = 0.19924392\n",
      "Iteration 213, loss = 0.01741949\n",
      "Iteration 36, loss = 0.19650423\n",
      "Iteration 214, loss = 0.01445141\n",
      "Iteration 215, loss = 0.01365175\n",
      "Iteration 37, loss = 0.19134090\n",
      "Iteration 216, loss = 0.01343460\n",
      "Iteration 38, loss = 0.18817316\n",
      "Iteration 217, loss = 0.01396102\n",
      "Iteration 39, loss = 0.18394759\n",
      "Iteration 40, loss = 0.18102339\n",
      "Iteration 218, loss = 0.01279840\n",
      "Iteration 219, loss = 0.01417320\n",
      "Iteration 41, loss = 0.17830751\n",
      "Iteration 220, loss = 0.01468699\n",
      "Iteration 42, loss = 0.17356706\n",
      "Iteration 221, loss = 0.01317810\n",
      "Iteration 43, loss = 0.17197087\n",
      "Iteration 222, loss = 0.01228706\n",
      "Iteration 44, loss = 0.16814710\n",
      "Iteration 223, loss = 0.01227793\n",
      "Iteration 224, loss = 0.01204303\n",
      "Iteration 45, loss = 0.16441547\n",
      "Iteration 225, loss = 0.01205963\n",
      "Iteration 46, loss = 0.16250975\n",
      "Iteration 226, loss = 0.01171110\n",
      "Iteration 47, loss = 0.15823632\n",
      "Iteration 227, loss = 0.01158401\n",
      "Iteration 48, loss = 0.15754987\n",
      "Iteration 228, loss = 0.01177082\n",
      "Iteration 49, loss = 0.15590615\n",
      "Iteration 229, loss = 0.01128707\n",
      "Iteration 50, loss = 0.15394026\n",
      "Iteration 230, loss = 0.01116627\n",
      "Iteration 51, loss = 0.15087290\n",
      "Iteration 231, loss = 0.01157005\n",
      "Iteration 52, loss = 0.14650237\n",
      "Iteration 232, loss = 0.01174582\n",
      "Iteration 53, loss = 0.14376176\n",
      "Iteration 233, loss = 0.01076486\n",
      "Iteration 54, loss = 0.14229498\n",
      "Iteration 234, loss = 0.01062375\n",
      "Iteration 55, loss = 0.14214907\n",
      "Iteration 235, loss = 0.01053318\n",
      "Iteration 56, loss = 0.14089119\n",
      "Iteration 57, loss = 0.13747967\n",
      "Iteration 236, loss = 0.01079628\n",
      "Iteration 237, loss = 0.01137390\n",
      "Iteration 58, loss = 0.13439324\n",
      "Iteration 59, loss = 0.13319290\n",
      "Iteration 60, loss = 0.13269232\n",
      "Iteration 238, loss = 0.01195612\n",
      "Iteration 61, loss = 0.12852923\n",
      "Iteration 62, loss = 0.12671324\n",
      "Iteration 63, loss = 0.12644562\n",
      "Iteration 239, loss = 0.01116294\n",
      "Iteration 64, loss = 0.12598585\n",
      "Iteration 65, loss = 0.12210181\n",
      "Iteration 66, loss = 0.12036919\n",
      "Iteration 240, loss = 0.01029170\n",
      "Iteration 67, loss = 0.12062728\n",
      "Iteration 68, loss = 0.11999021\n",
      "Iteration 69, loss = 0.11980320\n",
      "Iteration 241, loss = 0.01048825\n",
      "Iteration 242, loss = 0.00996930\n",
      "Iteration 243, loss = 0.01033769\n",
      "Iteration 244, loss = 0.00990583\n",
      "Iteration 70, loss = 0.11802723\n",
      "Iteration 71, loss = 0.11783155\n",
      "Iteration 245, loss = 0.01089833\n",
      "Iteration 72, loss = 0.11430443\n",
      "Iteration 73, loss = 0.11218133\n",
      "Iteration 246, loss = 0.01138491\n",
      "Iteration 74, loss = 0.11238211\n",
      "Iteration 75, loss = 0.11095326\n",
      "Iteration 247, loss = 0.00983699\n",
      "Iteration 76, loss = 0.11312642\n",
      "Iteration 248, loss = 0.01063450\n",
      "Iteration 77, loss = 0.10863287\n",
      "Iteration 249, loss = 0.00983905\n",
      "Iteration 78, loss = 0.10436729\n",
      "Iteration 250, loss = 0.00924871\n",
      "Iteration 251, loss = 0.00923853\n",
      "Iteration 252, loss = 0.00931535\n",
      "Iteration 253, loss = 0.00906586\n",
      "Iteration 79, loss = 0.10822937\n",
      "Iteration 254, loss = 0.00927477\n",
      "Iteration 255, loss = 0.00879326\n",
      "Iteration 256, loss = 0.01208019\n",
      "Iteration 80, loss = 0.10243710\n",
      "Iteration 257, loss = 0.01099891\n",
      "Iteration 258, loss = 0.00963182\n",
      "Iteration 81, loss = 0.10164490\n",
      "Iteration 259, loss = 0.00905040\n",
      "Iteration 260, loss = 0.00893243\n",
      "Iteration 82, loss = 0.09977203\n",
      "Iteration 261, loss = 0.00968052\n",
      "Iteration 262, loss = 0.00935319\n",
      "Iteration 83, loss = 0.09919938\n",
      "Iteration 84, loss = 0.09700111\n",
      "Iteration 263, loss = 0.00900797\n",
      "Iteration 85, loss = 0.09603972\n",
      "Iteration 264, loss = 0.00975922\n",
      "Iteration 86, loss = 0.09814660\n",
      "Iteration 265, loss = 0.01033898\n",
      "Iteration 266, loss = 0.00848268\n",
      "Iteration 87, loss = 0.09776967\n",
      "Iteration 88, loss = 0.09688226\n",
      "Iteration 89, loss = 0.09530668\n",
      "Iteration 267, loss = 0.00985055\n",
      "Iteration 90, loss = 0.09508981\n",
      "Iteration 268, loss = 0.00838719\n",
      "Iteration 269, loss = 0.00978564\n",
      "Iteration 91, loss = 0.09323257\n",
      "Iteration 270, loss = 0.00896253\n",
      "Iteration 92, loss = 0.08901637\n",
      "Iteration 271, loss = 0.00799982\n",
      "Iteration 93, loss = 0.08930573\n",
      "Iteration 94, loss = 0.08849018\n",
      "Iteration 272, loss = 0.00800415\n",
      "Iteration 273, loss = 0.00803694\n",
      "Iteration 274, loss = 0.00783910\n",
      "Iteration 95, loss = 0.08916626\n",
      "Iteration 275, loss = 0.00790956\n",
      "Iteration 96, loss = 0.08473126\n",
      "Iteration 97, loss = 0.08563438\n",
      "Iteration 276, loss = 0.00764987\n",
      "Iteration 98, loss = 0.08366825\n",
      "Iteration 277, loss = 0.00715747\n",
      "Iteration 99, loss = 0.08422501\n",
      "Iteration 100, loss = 0.08304796\n",
      "Iteration 278, loss = 0.00722767\n",
      "Iteration 279, loss = 0.00749778\n",
      "Iteration 101, loss = 0.08295888\n",
      "Iteration 280, loss = 0.00730322\n",
      "Iteration 281, loss = 0.00743692\n",
      "Iteration 102, loss = 0.07987322\n",
      "Iteration 103, loss = 0.08096049\n",
      "Iteration 104, loss = 0.07859403\n",
      "Iteration 282, loss = 0.00700069\n",
      "Iteration 283, loss = 0.00673298\n",
      "Iteration 105, loss = 0.07691899\n",
      "Iteration 284, loss = 0.00704551\n",
      "Iteration 285, loss = 0.00687111\n",
      "Iteration 106, loss = 0.07683622\n",
      "Iteration 107, loss = 0.08013734\n",
      "Iteration 286, loss = 0.00712461\n",
      "Iteration 108, loss = 0.07669336\n",
      "Iteration 287, loss = 0.00662627\n",
      "Iteration 288, loss = 0.00680714\n",
      "Iteration 289, loss = 0.00782878\n",
      "Iteration 290, loss = 0.00694571\n",
      "Iteration 109, loss = 0.07486895\n",
      "Iteration 110, loss = 0.07521612\n",
      "Iteration 291, loss = 0.00741231\n",
      "Iteration 111, loss = 0.07249010\n",
      "Iteration 112, loss = 0.07280246\n",
      "Iteration 292, loss = 0.00834981\n",
      "Iteration 113, loss = 0.07947242\n",
      "Iteration 114, loss = 0.07245460\n",
      "Iteration 293, loss = 0.00880343\n",
      "Iteration 115, loss = 0.06993506\n",
      "Iteration 116, loss = 0.07345201\n",
      "Iteration 294, loss = 0.01082234\n",
      "Iteration 295, loss = 0.01025182\n",
      "Iteration 296, loss = 0.02191674\n",
      "Iteration 117, loss = 0.07693466\n",
      "Iteration 118, loss = 0.08448344\n",
      "Iteration 119, loss = 0.07471392\n",
      "Iteration 120, loss = 0.07747967\n",
      "Iteration 297, loss = 0.01776600\n",
      "Iteration 298, loss = 0.01383925\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 121, loss = 0.06631198\n",
      "Iteration 122, loss = 0.07208854\n",
      "Iteration 123, loss = 0.06929300\n",
      "Iteration 124, loss = 0.06857610\n",
      "Iteration 125, loss = 0.06651499\n",
      "Iteration 126, loss = 0.06973590\n",
      "Iteration 127, loss = 0.06884990\n",
      "Iteration 128, loss = 0.06401332\n",
      "Iteration 1, loss = 0.95034099\n",
      "Iteration 129, loss = 0.06150521\n",
      "Iteration 130, loss = 0.06143872\n",
      "Iteration 2, loss = 0.65421084\n",
      "Iteration 131, loss = 0.06174654\n",
      "Iteration 3, loss = 0.53310636\n",
      "Iteration 132, loss = 0.05906047\n",
      "Iteration 133, loss = 0.05695860\n",
      "Iteration 134, loss = 0.05789809\n",
      "Iteration 4, loss = 0.47642098\n",
      "Iteration 5, loss = 0.43423157\n",
      "Iteration 135, loss = 0.05757551\n",
      "Iteration 6, loss = 0.40727331\n",
      "Iteration 136, loss = 0.05576148\n",
      "Iteration 7, loss = 0.38770662\n",
      "Iteration 137, loss = 0.05817921\n",
      "Iteration 8, loss = 0.37236928\n",
      "Iteration 9, loss = 0.35802445\n",
      "Iteration 138, loss = 0.05356037\n",
      "Iteration 139, loss = 0.05415618\n",
      "Iteration 10, loss = 0.34693875\n",
      "Iteration 140, loss = 0.05411069\n",
      "Iteration 141, loss = 0.05358860\n",
      "Iteration 11, loss = 0.33554792\n",
      "Iteration 142, loss = 0.05300760\n",
      "Iteration 12, loss = 0.32695030\n",
      "Iteration 13, loss = 0.31596705\n",
      "Iteration 143, loss = 0.05179386\n",
      "Iteration 144, loss = 0.05154022\n",
      "Iteration 14, loss = 0.30807563\n",
      "Iteration 145, loss = 0.05107678\n",
      "Iteration 146, loss = 0.05375983\n",
      "Iteration 15, loss = 0.29951326\n",
      "Iteration 147, loss = 0.05232359\n",
      "Iteration 16, loss = 0.29062080\n",
      "Iteration 148, loss = 0.05381163\n",
      "Iteration 17, loss = 0.28242699\n",
      "Iteration 149, loss = 0.05106270\n",
      "Iteration 150, loss = 0.04771459\n",
      "Iteration 151, loss = 0.04985591\n",
      "Iteration 18, loss = 0.27586638\n",
      "Iteration 152, loss = 0.04823849\n",
      "Iteration 19, loss = 0.26654857\n",
      "Iteration 153, loss = 0.05338312\n",
      "Iteration 20, loss = 0.26025681\n",
      "Iteration 154, loss = 0.05562486\n",
      "Iteration 155, loss = 0.04851661\n",
      "Iteration 21, loss = 0.25255244\n",
      "Iteration 156, loss = 0.04637567\n",
      "Iteration 22, loss = 0.24867238\n",
      "Iteration 157, loss = 0.04489975\n",
      "Iteration 23, loss = 0.23991738\n",
      "Iteration 24, loss = 0.23419246\n",
      "Iteration 158, loss = 0.04511495\n",
      "Iteration 25, loss = 0.22764694\n",
      "Iteration 159, loss = 0.04651297\n",
      "Iteration 26, loss = 0.22218811\n",
      "Iteration 160, loss = 0.04724947\n",
      "Iteration 27, loss = 0.21457964\n",
      "Iteration 161, loss = 0.04777242\n",
      "Iteration 28, loss = 0.21137696\n",
      "Iteration 162, loss = 0.04379130\n",
      "Iteration 29, loss = 0.21026640\n",
      "Iteration 163, loss = 0.04676782\n",
      "Iteration 30, loss = 0.20641623\n",
      "Iteration 164, loss = 0.04456078\n",
      "Iteration 165, loss = 0.04576230\n",
      "Iteration 31, loss = 0.19805565\n",
      "Iteration 166, loss = 0.04448321\n",
      "Iteration 32, loss = 0.19251485\n",
      "Iteration 167, loss = 0.04307184\n",
      "Iteration 33, loss = 0.18835969\n",
      "Iteration 34, loss = 0.18214349\n",
      "Iteration 168, loss = 0.04594172\n",
      "Iteration 35, loss = 0.18004336\n",
      "Iteration 169, loss = 0.04826053\n",
      "Iteration 36, loss = 0.17277607\n",
      "Iteration 170, loss = 0.04327595\n",
      "Iteration 37, loss = 0.17151825\n",
      "Iteration 171, loss = 0.04269769\n",
      "Iteration 38, loss = 0.16760319\n",
      "Iteration 172, loss = 0.03873631\n",
      "Iteration 39, loss = 0.16307659\n",
      "Iteration 173, loss = 0.04247453\n",
      "Iteration 40, loss = 0.16178479\n",
      "Iteration 174, loss = 0.03937399\n",
      "Iteration 41, loss = 0.15858724\n",
      "Iteration 175, loss = 0.03953091\n",
      "Iteration 42, loss = 0.15432809\n",
      "Iteration 176, loss = 0.03809932\n",
      "Iteration 43, loss = 0.15182086\n",
      "Iteration 177, loss = 0.03812052\n",
      "Iteration 44, loss = 0.14886661\n",
      "Iteration 45, loss = 0.14507232\n",
      "Iteration 178, loss = 0.03789286\n",
      "Iteration 46, loss = 0.14182320\n",
      "Iteration 179, loss = 0.03713049\n",
      "Iteration 180, loss = 0.03701969\n",
      "Iteration 181, loss = 0.03718252\n",
      "Iteration 182, loss = 0.03651159\n",
      "Iteration 47, loss = 0.14108054\n",
      "Iteration 183, loss = 0.03751227\n",
      "Iteration 184, loss = 0.03665221\n",
      "Iteration 48, loss = 0.13809168\n",
      "Iteration 185, loss = 0.03656056\n",
      "Iteration 186, loss = 0.03654473\n",
      "Iteration 49, loss = 0.13427745\n",
      "Iteration 187, loss = 0.03730672\n",
      "Iteration 50, loss = 0.13299568\n",
      "Iteration 188, loss = 0.03568942\n",
      "Iteration 51, loss = 0.12974103\n",
      "Iteration 189, loss = 0.03487997\n",
      "Iteration 190, loss = 0.03505008\n",
      "Iteration 191, loss = 0.03492848\n",
      "Iteration 52, loss = 0.12771826\n",
      "Iteration 192, loss = 0.03602255\n",
      "Iteration 193, loss = 0.03772740\n",
      "Iteration 194, loss = 0.03485958\n",
      "Iteration 53, loss = 0.12648275\n",
      "Iteration 195, loss = 0.03575445\n",
      "Iteration 196, loss = 0.03234444\n",
      "Iteration 54, loss = 0.12510852\n",
      "Iteration 197, loss = 0.03241881\n",
      "Iteration 55, loss = 0.12196049\n",
      "Iteration 198, loss = 0.03300361\n",
      "Iteration 56, loss = 0.12162726\n",
      "Iteration 199, loss = 0.03274087\n",
      "Iteration 200, loss = 0.03389592\n",
      "Iteration 201, loss = 0.03430097\n",
      "Iteration 57, loss = 0.11731189\n",
      "Iteration 202, loss = 0.03325014\n",
      "Iteration 203, loss = 0.03023811\n",
      "Iteration 58, loss = 0.11601177\n",
      "Iteration 204, loss = 0.03277679\n",
      "Iteration 59, loss = 0.11389990\n",
      "Iteration 205, loss = 0.03463949\n",
      "Iteration 60, loss = 0.11125744\n",
      "Iteration 206, loss = 0.03181018\n",
      "Iteration 207, loss = 0.03028857\n",
      "Iteration 208, loss = 0.03057008\n",
      "Iteration 61, loss = 0.11148345\n",
      "Iteration 209, loss = 0.03318900\n",
      "Iteration 62, loss = 0.11248108\n",
      "Iteration 210, loss = 0.03178904\n",
      "Iteration 63, loss = 0.10876032\n",
      "Iteration 211, loss = 0.03120154\n",
      "Iteration 64, loss = 0.10569545\n",
      "Iteration 65, loss = 0.10312643\n",
      "Iteration 212, loss = 0.03168612\n",
      "Iteration 66, loss = 0.10345853\n",
      "Iteration 67, loss = 0.10070586\n",
      "Iteration 213, loss = 0.02849899\n",
      "Iteration 214, loss = 0.02883233\n",
      "Iteration 215, loss = 0.02817138\n",
      "Iteration 216, loss = 0.02857749\n",
      "Iteration 68, loss = 0.09965048\n",
      "Iteration 69, loss = 0.09862061\n",
      "Iteration 217, loss = 0.02917393\n",
      "Iteration 70, loss = 0.09689662\n",
      "Iteration 218, loss = 0.03205148\n",
      "Iteration 219, loss = 0.03314593\n",
      "Iteration 71, loss = 0.09750619\n",
      "Iteration 220, loss = 0.03288827\n",
      "Iteration 72, loss = 0.09719972\n",
      "Iteration 221, loss = 0.03330209\n",
      "Iteration 222, loss = 0.02885415\n",
      "Iteration 73, loss = 0.09408004\n",
      "Iteration 223, loss = 0.02797318\n",
      "Iteration 74, loss = 0.09273716\n",
      "Iteration 224, loss = 0.02859230\n",
      "Iteration 75, loss = 0.09155018\n",
      "Iteration 76, loss = 0.08825534\n",
      "Iteration 225, loss = 0.02811018\n",
      "Iteration 77, loss = 0.08854980\n",
      "Iteration 226, loss = 0.03304806\n",
      "Iteration 227, loss = 0.03555923\n",
      "Iteration 78, loss = 0.08483894\n",
      "Iteration 228, loss = 0.03242781\n",
      "Iteration 79, loss = 0.08479510\n",
      "Iteration 229, loss = 0.02889087\n",
      "Iteration 80, loss = 0.08327049\n",
      "Iteration 230, loss = 0.02721138\n",
      "Iteration 231, loss = 0.02486993\n",
      "Iteration 81, loss = 0.08528499\n",
      "Iteration 82, loss = 0.08450708\n",
      "Iteration 232, loss = 0.02793296\n",
      "Iteration 83, loss = 0.08206304\n",
      "Iteration 84, loss = 0.08124292\n",
      "Iteration 233, loss = 0.02540445\n",
      "Iteration 234, loss = 0.02649858\n",
      "Iteration 85, loss = 0.08255583\n",
      "Iteration 235, loss = 0.02531954\n",
      "Iteration 236, loss = 0.02464687Iteration 86, loss = 0.08225063\n",
      "\n",
      "Iteration 87, loss = 0.07605616\n",
      "Iteration 88, loss = 0.07650161\n",
      "Iteration 237, loss = 0.02454197\n",
      "Iteration 238, loss = 0.02439119\n",
      "Iteration 89, loss = 0.07312428\n",
      "Iteration 239, loss = 0.02872629\n",
      "Iteration 240, loss = 0.02655794\n",
      "Iteration 90, loss = 0.07352067\n",
      "Iteration 91, loss = 0.07334730\n",
      "Iteration 92, loss = 0.07207540\n",
      "Iteration 241, loss = 0.02708889\n",
      "Iteration 93, loss = 0.06971158\n",
      "Iteration 94, loss = 0.07033653\n",
      "Iteration 242, loss = 0.02719819\n",
      "Iteration 95, loss = 0.06722739\n",
      "Iteration 243, loss = 0.02811778\n",
      "Iteration 96, loss = 0.06768414\n",
      "Iteration 244, loss = 0.03292149\n",
      "Iteration 97, loss = 0.06838731\n",
      "Iteration 98, loss = 0.07014655\n",
      "Iteration 99, loss = 0.07108835\n",
      "Iteration 100, loss = 0.06468483\n",
      "Iteration 245, loss = 0.03751908\n",
      "Iteration 101, loss = 0.06471778\n",
      "Iteration 246, loss = 0.04940630\n",
      "Iteration 102, loss = 0.06101116\n",
      "Iteration 247, loss = 0.03191769\n",
      "Iteration 248, loss = 0.02964286\n",
      "Iteration 103, loss = 0.06045236\n",
      "Iteration 104, loss = 0.06399290\n",
      "Iteration 249, loss = 0.02750648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 105, loss = 0.06400535\n",
      "Iteration 106, loss = 0.06311841\n",
      "Iteration 107, loss = 0.06239766\n",
      "Iteration 108, loss = 0.06170516\n",
      "Iteration 109, loss = 0.05966756\n",
      "Iteration 110, loss = 0.05904046\n",
      "Iteration 111, loss = 0.06077915\n",
      "Iteration 112, loss = 0.05608440\n",
      "Iteration 113, loss = 0.05592228\n",
      "Iteration 114, loss = 0.05529454\n",
      "Iteration 115, loss = 0.05404573\n",
      "Iteration 116, loss = 0.05300168\n",
      "Iteration 1, loss = 14.24428367\n",
      "Iteration 117, loss = 0.05096627\n",
      "Iteration 2, loss = 13.44522006\n",
      "Iteration 118, loss = 0.05053281\n",
      "Iteration 3, loss = 9.80449901\n",
      "Iteration 119, loss = 0.05032348\n",
      "Iteration 120, loss = 0.04963030\n",
      "Iteration 121, loss = 0.04991391\n",
      "Iteration 4, loss = 7.10683597\n",
      "Iteration 122, loss = 0.04803732\n",
      "Iteration 5, loss = 6.59162818\n",
      "Iteration 6, loss = 5.24945845\n",
      "Iteration 7, loss = 9.41784214\n",
      "Iteration 123, loss = 0.04841955\n",
      "Iteration 8, loss = 7.92927988\n",
      "Iteration 9, loss = 7.23039257\n",
      "Iteration 124, loss = 0.04774413\n",
      "Iteration 10, loss = 5.63584492\n",
      "Iteration 11, loss = 5.57295154\n",
      "Iteration 12, loss = 5.01173200\n",
      "Iteration 125, loss = 0.04681793\n",
      "Iteration 126, loss = 0.04714487\n",
      "Iteration 13, loss = 3.63963001\n",
      "Iteration 127, loss = 0.04625062\n",
      "Iteration 128, loss = 0.04523191\n",
      "Iteration 14, loss = 7.41146634\n",
      "Iteration 15, loss = 4.93404532\n",
      "Iteration 129, loss = 0.04422398\n",
      "Iteration 16, loss = 3.86241935\n",
      "Iteration 130, loss = 0.04469183\n",
      "Iteration 17, loss = 4.20380298\n",
      "Iteration 131, loss = 0.04608682\n",
      "Iteration 132, loss = 0.04366868\n",
      "Iteration 18, loss = 4.44182085\n",
      "Iteration 133, loss = 0.04321745\n",
      "Iteration 19, loss = 3.68881211\n",
      "Iteration 20, loss = 3.54299624\n",
      "Iteration 21, loss = 2.28783513\n",
      "Iteration 134, loss = 0.04961416\n",
      "Iteration 135, loss = 0.04818532\n",
      "Iteration 136, loss = 0.05526722\n",
      "Iteration 22, loss = 3.40294983\n",
      "Iteration 137, loss = 0.06075131\n",
      "Iteration 23, loss = 2.33831228\n",
      "Iteration 24, loss = 2.89165650\n",
      "Iteration 25, loss = 2.83261986\n",
      "Iteration 138, loss = 0.06297123\n",
      "Iteration 139, loss = 0.05458033\n",
      "Iteration 26, loss = 1.99355063\n",
      "Iteration 140, loss = 0.05376632\n",
      "Iteration 141, loss = 0.04470626\n",
      "Iteration 27, loss = 2.27340632\n",
      "Iteration 28, loss = 1.88727931\n",
      "Iteration 29, loss = 1.73787769\n",
      "Iteration 142, loss = 0.04412541\n",
      "Iteration 30, loss = 3.01473273\n",
      "Iteration 143, loss = 0.04064172\n",
      "Iteration 144, loss = 0.03934689\n",
      "Iteration 31, loss = 1.91254522\n",
      "Iteration 145, loss = 0.03824279\n",
      "Iteration 146, loss = 0.03947202\n",
      "Iteration 32, loss = 1.70587475\n",
      "Iteration 33, loss = 1.69681914\n",
      "Iteration 34, loss = 1.86842082\n",
      "Iteration 147, loss = 0.03981958\n",
      "Iteration 35, loss = 1.90686004\n",
      "Iteration 148, loss = 0.03823944\n",
      "Iteration 149, loss = 0.03608845\n",
      "Iteration 150, loss = 0.03568686\n",
      "Iteration 36, loss = 1.85382403\n",
      "Iteration 151, loss = 0.03487946\n",
      "Iteration 37, loss = 0.98583709\n",
      "Iteration 38, loss = 1.84440678\n",
      "Iteration 152, loss = 0.03526928\n",
      "Iteration 39, loss = 1.70365760\n",
      "Iteration 153, loss = 0.03549355\n",
      "Iteration 40, loss = 1.76048099\n",
      "Iteration 154, loss = 0.03422503\n",
      "Iteration 41, loss = 1.38281283\n",
      "Iteration 42, loss = 1.94750648\n",
      "Iteration 155, loss = 0.03417923\n",
      "Iteration 43, loss = 2.93573502\n",
      "Iteration 156, loss = 0.03382707\n",
      "Iteration 44, loss = 2.90688342\n",
      "Iteration 157, loss = 0.03464414\n",
      "Iteration 45, loss = 2.53686750\n",
      "Iteration 158, loss = 0.03428894\n",
      "Iteration 159, loss = 0.03408026\n",
      "Iteration 46, loss = 1.80125240\n",
      "Iteration 47, loss = 2.91764246\n",
      "Iteration 160, loss = 0.03433144\n",
      "Iteration 48, loss = 2.06678509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 161, loss = 0.03378415\n",
      "Iteration 162, loss = 0.03293562\n",
      "Iteration 163, loss = 0.03138880\n",
      "Iteration 164, loss = 0.03079537\n",
      "Iteration 165, loss = 0.03104034\n",
      "Iteration 166, loss = 0.03162220\n",
      "Iteration 167, loss = 0.03330123\n",
      "Iteration 168, loss = 0.03409289\n",
      "Iteration 169, loss = 0.03191391\n",
      "Iteration 170, loss = 0.03190435\n",
      "Iteration 171, loss = 0.02988663\n",
      "Iteration 1, loss = 15.31019818\n",
      "Iteration 172, loss = 0.02947689\n",
      "Iteration 2, loss = 14.53215038\n",
      "Iteration 3, loss = 12.46609089\n",
      "Iteration 173, loss = 0.02954792\n",
      "Iteration 4, loss = 9.80941083\n",
      "Iteration 174, loss = 0.02912649\n",
      "Iteration 5, loss = 7.54493475\n",
      "Iteration 175, loss = 0.03088658\n",
      "Iteration 176, loss = 0.02796187\n",
      "Iteration 6, loss = 6.43986442\n",
      "Iteration 7, loss = 5.52752013\n",
      "Iteration 177, loss = 0.02802106\n",
      "Iteration 8, loss = 5.22300516\n",
      "Iteration 178, loss = 0.02919647\n",
      "Iteration 9, loss = 4.28203262\n",
      "Iteration 179, loss = 0.03330680\n",
      "Iteration 180, loss = 0.02838033\n",
      "Iteration 10, loss = 4.56263279\n",
      "Iteration 181, loss = 0.02895555\n",
      "Iteration 11, loss = 4.30066436\n",
      "Iteration 12, loss = 5.67898843\n",
      "Iteration 13, loss = 6.15733362\n",
      "Iteration 182, loss = 0.03125831\n",
      "Iteration 14, loss = 5.38512942\n",
      "Iteration 183, loss = 0.03469121\n",
      "Iteration 15, loss = 3.76534697\n",
      "Iteration 184, loss = 0.02963232\n",
      "Iteration 16, loss = 4.02166222\n",
      "Iteration 185, loss = 0.03268614\n",
      "Iteration 17, loss = 3.71825803\n",
      "Iteration 186, loss = 0.02912098\n",
      "Iteration 18, loss = 3.64941918\n",
      "Iteration 19, loss = 2.83716858\n",
      "Iteration 187, loss = 0.02725035\n",
      "Iteration 20, loss = 3.80214686\n",
      "Iteration 188, loss = 0.02602739\n",
      "Iteration 21, loss = 3.01983662\n",
      "Iteration 189, loss = 0.02626768\n",
      "Iteration 22, loss = 3.48236419\n",
      "Iteration 190, loss = 0.02834343\n",
      "Iteration 23, loss = 3.37368597\n",
      "Iteration 191, loss = 0.02548679\n",
      "Iteration 24, loss = 2.34003455\n",
      "Iteration 192, loss = 0.02417783\n",
      "Iteration 25, loss = 3.04438721\n",
      "Iteration 193, loss = 0.02410522\n",
      "Iteration 26, loss = 2.81613378\n",
      "Iteration 194, loss = 0.02375999\n",
      "Iteration 195, loss = 0.02294614\n",
      "Iteration 27, loss = 2.13023188\n",
      "Iteration 196, loss = 0.02334932\n",
      "Iteration 197, loss = 0.02266702\n",
      "Iteration 28, loss = 2.13061290\n",
      "Iteration 198, loss = 0.02446632\n",
      "Iteration 29, loss = 2.86667453\n",
      "Iteration 199, loss = 0.02218839\n",
      "Iteration 30, loss = 2.78815563\n",
      "Iteration 200, loss = 0.02416620\n",
      "Iteration 31, loss = 3.10526252\n",
      "Iteration 201, loss = 0.02792685\n",
      "Iteration 202, loss = 0.02457118\n",
      "Iteration 203, loss = 0.03011704\n",
      "Iteration 32, loss = 2.85671502\n",
      "Iteration 204, loss = 0.02816043\n",
      "Iteration 205, loss = 0.02244520\n",
      "Iteration 206, loss = 0.02160631\n",
      "Iteration 207, loss = 0.02244870\n",
      "Iteration 33, loss = 3.02486813\n",
      "Iteration 34, loss = 2.42749350\n",
      "Iteration 208, loss = 0.02224614\n",
      "Iteration 35, loss = 2.33769193\n",
      "Iteration 36, loss = 2.82893397\n",
      "Iteration 209, loss = 0.02136428\n",
      "Iteration 210, loss = 0.02110294\n",
      "Iteration 37, loss = 2.78153427\n",
      "Iteration 211, loss = 0.02144736\n",
      "Iteration 38, loss = 2.17751978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 212, loss = 0.02070902\n",
      "Iteration 213, loss = 0.02002712\n",
      "Iteration 214, loss = 0.02071389\n",
      "Iteration 215, loss = 0.02102273\n",
      "Iteration 216, loss = 0.02107584\n",
      "Iteration 217, loss = 0.02151959\n",
      "Iteration 218, loss = 0.01968718\n",
      "Iteration 219, loss = 0.02003184\n",
      "Iteration 220, loss = 0.01916043\n",
      "Iteration 1, loss = 13.41075315\n",
      "Iteration 2, loss = 14.25468832\n",
      "Iteration 221, loss = 0.02023829\n",
      "Iteration 222, loss = 0.01855663\n",
      "Iteration 223, loss = 0.01837945\n",
      "Iteration 3, loss = 10.93164504\n",
      "Iteration 4, loss = 12.34715286\n",
      "Iteration 224, loss = 0.01781399\n",
      "Iteration 5, loss = 9.34737149\n",
      "Iteration 6, loss = 8.65338923\n",
      "Iteration 225, loss = 0.01830562\n",
      "Iteration 7, loss = 9.33716248\n",
      "Iteration 226, loss = 0.01754520\n",
      "Iteration 227, loss = 0.01776483\n",
      "Iteration 8, loss = 8.98699570\n",
      "Iteration 9, loss = 7.46442421\n",
      "Iteration 10, loss = 8.53383546\n",
      "Iteration 228, loss = 0.01885650\n",
      "Iteration 11, loss = 7.32568220\n",
      "Iteration 229, loss = 0.01813581\n",
      "Iteration 230, loss = 0.01752462\n",
      "Iteration 12, loss = 5.35576661\n",
      "Iteration 231, loss = 0.01872339\n",
      "Iteration 13, loss = 6.03865512\n",
      "Iteration 232, loss = 0.02232820\n",
      "Iteration 14, loss = 4.79710276\n",
      "Iteration 233, loss = 0.02092964\n",
      "Iteration 15, loss = 7.36925292\n",
      "Iteration 16, loss = 4.98891548\n",
      "Iteration 234, loss = 0.01690514\n",
      "Iteration 17, loss = 3.90826161\n",
      "Iteration 18, loss = 4.30931474\n",
      "Iteration 235, loss = 0.01655977\n",
      "Iteration 19, loss = 3.15235444\n",
      "Iteration 20, loss = 5.33950817\n",
      "Iteration 236, loss = 0.01780964\n",
      "Iteration 21, loss = 6.04974906\n",
      "Iteration 22, loss = 5.78181078\n",
      "Iteration 237, loss = 0.01703149\n",
      "Iteration 238, loss = 0.01808114\n",
      "Iteration 239, loss = 0.01698873\n",
      "Iteration 23, loss = 5.06878394\n",
      "Iteration 24, loss = 5.23335565\n",
      "Iteration 25, loss = 3.46893600\n",
      "Iteration 240, loss = 0.01804728\n",
      "Iteration 26, loss = 4.02693828\n",
      "Iteration 241, loss = 0.01602886\n",
      "Iteration 27, loss = 3.05423482\n",
      "Iteration 242, loss = 0.01839344\n",
      "Iteration 243, loss = 0.01890556\n",
      "Iteration 244, loss = 0.01631414\n",
      "Iteration 28, loss = 3.01427195\n",
      "Iteration 245, loss = 0.01717201\n",
      "Iteration 246, loss = 0.01524505\n",
      "Iteration 247, loss = 0.01564193\n",
      "Iteration 29, loss = 5.62834401\n",
      "Iteration 248, loss = 0.01708133\n",
      "Iteration 30, loss = 3.48935746\n",
      "Iteration 249, loss = 0.01569885\n",
      "Iteration 31, loss = 2.65644571\n",
      "Iteration 250, loss = 0.01970268\n",
      "Iteration 32, loss = 2.15642816\n",
      "Iteration 33, loss = 2.16072859\n",
      "Iteration 251, loss = 0.01852818\n",
      "Iteration 34, loss = 2.28400168\n",
      "Iteration 252, loss = 0.01944882\n",
      "Iteration 35, loss = 3.00014902\n",
      "Iteration 253, loss = 0.01817257\n",
      "Iteration 36, loss = 2.03853109\n",
      "Iteration 254, loss = 0.01536897\n",
      "Iteration 37, loss = 1.89645088\n",
      "Iteration 255, loss = 0.01564044\n",
      "Iteration 38, loss = 2.68189618\n",
      "Iteration 256, loss = 0.01337419\n",
      "Iteration 257, loss = 0.01358822\n",
      "Iteration 258, loss = 0.01351263\n",
      "Iteration 39, loss = 2.50881345\n",
      "Iteration 259, loss = 0.01336899\n",
      "Iteration 40, loss = 2.62634372\n",
      "Iteration 260, loss = 0.01332295\n",
      "Iteration 41, loss = 2.47137551\n",
      "Iteration 261, loss = 0.01363425\n",
      "Iteration 262, loss = 0.01319812\n",
      "Iteration 42, loss = 2.13005434\n",
      "Iteration 263, loss = 0.01260049\n",
      "Iteration 264, loss = 0.01319969\n",
      "Iteration 43, loss = 1.65012977\n",
      "Iteration 265, loss = 0.01377193\n",
      "Iteration 266, loss = 0.01477983\n",
      "Iteration 267, loss = 0.01181299\n",
      "Iteration 44, loss = 1.54339380\n",
      "Iteration 268, loss = 0.01519546\n",
      "Iteration 45, loss = 1.98184303\n",
      "Iteration 269, loss = 0.01338276\n",
      "Iteration 270, loss = 0.02077000\n",
      "Iteration 46, loss = 1.85051028\n",
      "Iteration 271, loss = 0.01408267\n",
      "Iteration 272, loss = 0.01311163\n",
      "Iteration 47, loss = 1.80702155\n",
      "Iteration 273, loss = 0.01247817\n",
      "Iteration 48, loss = 2.24325706\n",
      "Iteration 274, loss = 0.01352048\n",
      "Iteration 49, loss = 2.39780243\n",
      "Iteration 50, loss = 1.56478004\n",
      "Iteration 275, loss = 0.01299316\n",
      "Iteration 51, loss = 1.57579023\n",
      "Iteration 52, loss = 1.79461033\n",
      "Iteration 276, loss = 0.01266122\n",
      "Iteration 277, loss = 0.01418185\n",
      "Iteration 278, loss = 0.01343533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 1.81049220\n",
      "Iteration 54, loss = 1.84470241\n",
      "Iteration 55, loss = 1.98000277\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.38927223\n",
      "Iteration 1, loss = 19.38681760\n",
      "Iteration 2, loss = 13.09773350\n",
      "Iteration 2, loss = 14.21395368\n",
      "Iteration 3, loss = 9.17169354\n",
      "Iteration 3, loss = 8.21207122\n",
      "Iteration 4, loss = 7.65735596\n",
      "Iteration 4, loss = 11.26314614\n",
      "Iteration 5, loss = 8.17165481\n",
      "Iteration 5, loss = 7.86277869\n",
      "Iteration 6, loss = 6.46240086\n",
      "Iteration 7, loss = 5.68876504\n",
      "Iteration 6, loss = 9.02743946\n",
      "Iteration 7, loss = 10.61722014\n",
      "Iteration 8, loss = 9.40682264\n",
      "Iteration 8, loss = 6.24936337\n",
      "Iteration 9, loss = 6.91111007\n",
      "Iteration 9, loss = 6.33856503\n",
      "Iteration 10, loss = 5.56349965\n",
      "Iteration 11, loss = 4.95903900\n",
      "Iteration 10, loss = 5.50307916\n",
      "Iteration 12, loss = 4.90039637\n",
      "Iteration 13, loss = 4.98757780\n",
      "Iteration 14, loss = 5.41023794\n",
      "Iteration 11, loss = 5.34656477\n",
      "Iteration 15, loss = 5.07250083\n",
      "Iteration 16, loss = 3.53807906\n",
      "Iteration 12, loss = 4.46162926\n",
      "Iteration 17, loss = 4.23574621\n",
      "Iteration 18, loss = 2.65702186\n",
      "Iteration 19, loss = 3.36182653\n",
      "Iteration 20, loss = 2.50322938\n",
      "Iteration 13, loss = 3.23665560\n",
      "Iteration 21, loss = 2.57564311\n",
      "Iteration 14, loss = 3.08746405\n",
      "Iteration 22, loss = 2.33013192\n",
      "Iteration 15, loss = 4.50485232\n",
      "Iteration 23, loss = 2.63482097\n",
      "Iteration 16, loss = 3.69555012\n",
      "Iteration 24, loss = 2.28587235\n",
      "Iteration 25, loss = 3.28295918\n",
      "Iteration 17, loss = 2.66125960\n",
      "Iteration 26, loss = 3.00585954\n",
      "Iteration 27, loss = 2.50374302\n",
      "Iteration 18, loss = 2.43263605\n",
      "Iteration 28, loss = 3.41218961\n",
      "Iteration 19, loss = 2.46447933\n",
      "Iteration 29, loss = 3.00296907\n",
      "Iteration 20, loss = 1.49648294\n",
      "Iteration 21, loss = 1.96875737\n",
      "Iteration 30, loss = 3.16870022\n",
      "Iteration 22, loss = 3.33087718\n",
      "Iteration 31, loss = 2.97049088\n",
      "Iteration 32, loss = 2.72641587\n",
      "Iteration 33, loss = 2.25374770\n",
      "Iteration 23, loss = 3.36424682\n",
      "Iteration 34, loss = 3.40315233\n",
      "Iteration 24, loss = 2.62824581\n",
      "Iteration 35, loss = 3.10144242\n",
      "Iteration 25, loss = 2.94185240\n",
      "Iteration 36, loss = 2.07008411\n",
      "Iteration 26, loss = 2.64667417\n",
      "Iteration 37, loss = 3.12302602\n",
      "Iteration 38, loss = 2.11517233\n",
      "Iteration 27, loss = 2.54894260\n",
      "Iteration 39, loss = 2.52121608\n",
      "Iteration 40, loss = 2.01883810\n",
      "Iteration 28, loss = 2.20117939\n",
      "Iteration 41, loss = 2.30649782\n",
      "Iteration 29, loss = 2.34559965\n",
      "Iteration 42, loss = 1.94496384\n",
      "Iteration 30, loss = 1.92767429\n",
      "Iteration 43, loss = 2.07333394\n",
      "Iteration 31, loss = 1.99609584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 2.21609351\n",
      "Iteration 45, loss = 2.54702680\n",
      "Iteration 46, loss = 3.09639748\n",
      "Iteration 47, loss = 2.86857510\n",
      "Iteration 48, loss = 2.25170963\n",
      "Iteration 49, loss = 1.90427014\n",
      "Iteration 50, loss = 1.78184706\n",
      "Iteration 51, loss = 1.86343915\n",
      "Iteration 52, loss = 1.94914411\n",
      "Iteration 53, loss = 2.25185224\n",
      "Iteration 54, loss = 1.73321912\n",
      "Iteration 55, loss = 1.89688273\n",
      "Iteration 56, loss = 1.32628619\n",
      "Iteration 57, loss = 2.03514416\n",
      "Iteration 58, loss = 1.71046172\n",
      "Iteration 59, loss = 1.94372379\n",
      "Iteration 60, loss = 2.17602460\n",
      "Iteration 61, loss = 2.20090131\n",
      "Iteration 62, loss = 1.51613599\n",
      "Iteration 63, loss = 2.00291437\n",
      "Iteration 64, loss = 3.04934325\n",
      "Iteration 1, loss = 16.43272988\n",
      "Iteration 65, loss = 2.18096466\n",
      "Iteration 66, loss = 2.30916915\n",
      "Iteration 2, loss = 13.97140290\n",
      "Iteration 67, loss = 2.38554245\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 9.44520065\n",
      "Iteration 4, loss = 9.22532114\n",
      "Iteration 5, loss = 9.45879738\n",
      "Iteration 6, loss = 5.56646175\n",
      "Iteration 7, loss = 4.91196518\n",
      "Iteration 1, loss = 14.50017875\n",
      "Iteration 8, loss = 7.23367466\n",
      "Iteration 2, loss = 13.70704259\n",
      "Iteration 3, loss = 10.54156076\n",
      "Iteration 9, loss = 9.32327354\n",
      "Iteration 4, loss = 7.84787454\n",
      "Iteration 10, loss = 5.54133766\n",
      "Iteration 5, loss = 6.91938646\n",
      "Iteration 11, loss = 5.49102321\n",
      "Iteration 6, loss = 5.79466333\n",
      "Iteration 12, loss = 5.41652963\n",
      "Iteration 7, loss = 9.47437061\n",
      "Iteration 13, loss = 4.81657021\n",
      "Iteration 8, loss = 9.25650641\n",
      "Iteration 14, loss = 4.22594742\n",
      "Iteration 9, loss = 7.27128662\n",
      "Iteration 15, loss = 3.46329971\n",
      "Iteration 16, loss = 3.97469131\n",
      "Iteration 10, loss = 6.17937187\n",
      "Iteration 11, loss = 6.48362816\n",
      "Iteration 17, loss = 4.78768051\n",
      "Iteration 12, loss = 4.37805269\n",
      "Iteration 18, loss = 4.38124518\n",
      "Iteration 19, loss = 4.66098538\n",
      "Iteration 13, loss = 5.23009899\n",
      "Iteration 20, loss = 3.35483826\n",
      "Iteration 14, loss = 4.49283097\n",
      "Iteration 21, loss = 3.21476811\n",
      "Iteration 15, loss = 4.35946174\n",
      "Iteration 22, loss = 2.90236228\n",
      "Iteration 16, loss = 3.73552575\n",
      "Iteration 23, loss = 2.63169665\n",
      "Iteration 24, loss = 2.53281642\n",
      "Iteration 17, loss = 4.28356000\n",
      "Iteration 25, loss = 2.53448174\n",
      "Iteration 18, loss = 4.67069826\n",
      "Iteration 26, loss = 2.24814530\n",
      "Iteration 19, loss = 3.89774357\n",
      "Iteration 27, loss = 2.25133606\n",
      "Iteration 20, loss = 3.75766531\n",
      "Iteration 28, loss = 1.99289709\n",
      "Iteration 21, loss = 3.65671594\n",
      "Iteration 29, loss = 1.92379795\n",
      "Iteration 22, loss = 3.13689139\n",
      "Iteration 30, loss = 1.72561698\n",
      "Iteration 23, loss = 2.67956184\n",
      "Iteration 24, loss = 2.54860286\n",
      "Iteration 25, loss = 2.27081994\n",
      "Iteration 31, loss = 1.69204656\n",
      "Iteration 26, loss = 2.36122804\n",
      "Iteration 32, loss = 2.29575630\n",
      "Iteration 33, loss = 2.20580746\n",
      "Iteration 27, loss = 2.50796276\n",
      "Iteration 34, loss = 1.85781533\n",
      "Iteration 28, loss = 2.21308496\n",
      "Iteration 35, loss = 1.86866910\n",
      "Iteration 29, loss = 2.18065092\n",
      "Iteration 30, loss = 1.53390084\n",
      "Iteration 31, loss = 2.49476715\n",
      "Iteration 36, loss = 1.98181273\n",
      "Iteration 32, loss = 2.39532715\n",
      "Iteration 37, loss = 2.09857191\n",
      "Iteration 33, loss = 2.56674121\n",
      "Iteration 38, loss = 1.98126229\n",
      "Iteration 34, loss = 2.11026606\n",
      "Iteration 39, loss = 1.38792537\n",
      "Iteration 35, loss = 2.40054626\n",
      "Iteration 36, loss = 1.99415189\n",
      "Iteration 37, loss = 2.21828044\n",
      "Iteration 40, loss = 1.24021628\n",
      "Iteration 38, loss = 2.06505579\n",
      "Iteration 41, loss = 1.80708252\n",
      "Iteration 39, loss = 1.23750593\n",
      "Iteration 42, loss = 1.65122767\n",
      "Iteration 40, loss = 1.31984188\n",
      "Iteration 43, loss = 1.33633380\n",
      "Iteration 41, loss = 1.38588865\n",
      "Iteration 42, loss = 1.61382821\n",
      "Iteration 44, loss = 1.73935724\n",
      "Iteration 43, loss = 1.60393041\n",
      "Iteration 45, loss = 1.57120051\n",
      "Iteration 46, loss = 1.45058088\n",
      "Iteration 44, loss = 2.65411132\n",
      "Iteration 47, loss = 2.14712046\n",
      "Iteration 45, loss = 2.52090184\n",
      "Iteration 48, loss = 2.27828109\n",
      "Iteration 46, loss = 2.38113038\n",
      "Iteration 47, loss = 2.22924915\n",
      "Iteration 49, loss = 2.30249800\n",
      "Iteration 48, loss = 1.91040760\n",
      "Iteration 50, loss = 2.01215299\n",
      "Iteration 51, loss = 1.75354740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 1.70852430\n",
      "Iteration 50, loss = 1.64538896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.55485110\n",
      "Iteration 1, loss = 16.77179253\n",
      "Iteration 2, loss = 19.27334671\n",
      "Iteration 2, loss = 10.64566435\n",
      "Iteration 3, loss = 11.94106453\n",
      "Iteration 4, loss = 10.12652754\n",
      "Iteration 3, loss = 9.47742087\n",
      "Iteration 4, loss = 9.01980925\n",
      "Iteration 5, loss = 7.17049703\n",
      "Iteration 5, loss = 9.34578101\n",
      "Iteration 6, loss = 6.64915917\n",
      "Iteration 6, loss = 5.60224562\n",
      "Iteration 7, loss = 6.78734263\n",
      "Iteration 7, loss = 7.55983820\n",
      "Iteration 8, loss = 5.77605237\n",
      "Iteration 9, loss = 4.85273217\n",
      "Iteration 8, loss = 5.02860713\n",
      "Iteration 9, loss = 5.94462900\n",
      "Iteration 10, loss = 5.16797455\n",
      "Iteration 10, loss = 8.77181740\n",
      "Iteration 11, loss = 5.77531020\n",
      "Iteration 11, loss = 4.43888261\n",
      "Iteration 12, loss = 4.80464979\n",
      "Iteration 13, loss = 3.79938279\n",
      "Iteration 14, loss = 3.56365660\n",
      "Iteration 12, loss = 4.63487607\n",
      "Iteration 13, loss = 4.66735069\n",
      "Iteration 14, loss = 4.63269704\n",
      "Iteration 15, loss = 4.52984287\n",
      "Iteration 15, loss = 4.20438693\n",
      "Iteration 16, loss = 3.88915411\n",
      "Iteration 16, loss = 4.17818534\n",
      "Iteration 17, loss = 4.42091746\n",
      "Iteration 17, loss = 4.53867367\n",
      "Iteration 18, loss = 3.51439505\n",
      "Iteration 18, loss = 4.97098489\n",
      "Iteration 19, loss = 3.01748799\n",
      "Iteration 20, loss = 3.97462966\n",
      "Iteration 19, loss = 4.16304025\n",
      "Iteration 21, loss = 3.56748989\n",
      "Iteration 22, loss = 3.22438819\n",
      "Iteration 20, loss = 2.96477815\n",
      "Iteration 21, loss = 3.45554500\n",
      "Iteration 22, loss = 2.37780190\n",
      "Iteration 23, loss = 3.02839428\n",
      "Iteration 23, loss = 2.34506198\n",
      "Iteration 24, loss = 3.43996400\n",
      "Iteration 25, loss = 3.73319340\n",
      "Iteration 26, loss = 2.80135059\n",
      "Iteration 24, loss = 1.66360582\n",
      "Iteration 27, loss = 2.08687576\n",
      "Iteration 25, loss = 2.45639858\n",
      "Iteration 28, loss = 1.97628059\n",
      "Iteration 26, loss = 2.06609038\n",
      "Iteration 29, loss = 2.52074215\n",
      "Iteration 30, loss = 2.22797338\n",
      "Iteration 27, loss = 2.60751227\n",
      "Iteration 31, loss = 2.24126628\n",
      "Iteration 28, loss = 2.05294373\n",
      "Iteration 32, loss = 2.36913787\n",
      "Iteration 33, loss = 2.48278647\n",
      "Iteration 34, loss = 2.61447653\n",
      "Iteration 29, loss = 2.06320539\n",
      "Iteration 35, loss = 2.65674126\n",
      "Iteration 30, loss = 1.89340346\n",
      "Iteration 36, loss = 2.33115338\n",
      "Iteration 31, loss = 1.88190293\n",
      "Iteration 37, loss = 2.20160087\n",
      "Iteration 32, loss = 1.58591744\n",
      "Iteration 38, loss = 1.84734175\n",
      "Iteration 33, loss = 1.63761245\n",
      "Iteration 39, loss = 2.35854820\n",
      "Iteration 40, loss = 2.06517208\n",
      "Iteration 34, loss = 1.57254738\n",
      "Iteration 41, loss = 2.51385838\n",
      "Iteration 35, loss = 3.90209898\n",
      "Iteration 42, loss = 2.16792332\n",
      "Iteration 36, loss = 2.58825120\n",
      "Iteration 37, loss = 2.21281995\n",
      "Iteration 43, loss = 2.50806459\n",
      "Iteration 44, loss = 2.06458016\n",
      "Iteration 38, loss = 1.33968045\n",
      "Iteration 45, loss = 2.19931610\n",
      "Iteration 39, loss = 1.93469290\n",
      "Iteration 46, loss = 1.85386999\n",
      "Iteration 40, loss = 1.57866934\n",
      "Iteration 47, loss = 1.71615811\n",
      "Iteration 41, loss = 1.54215375\n",
      "Iteration 48, loss = 2.56641475\n",
      "Iteration 42, loss = 1.59708195\n",
      "Iteration 49, loss = 3.32372754\n",
      "Iteration 43, loss = 1.42625251\n",
      "Iteration 50, loss = 2.50640914\n",
      "Iteration 44, loss = 2.03429118\n",
      "Iteration 51, loss = 2.38953739\n",
      "Iteration 45, loss = 2.09660630\n",
      "Iteration 46, loss = 1.28599021\n",
      "Iteration 52, loss = 2.49839701\n",
      "Iteration 53, loss = 2.39200986\n",
      "Iteration 47, loss = 1.31146014\n",
      "Iteration 54, loss = 2.13482417\n",
      "Iteration 48, loss = 1.35569927\n",
      "Iteration 55, loss = 1.41010235\n",
      "Iteration 49, loss = 1.06112642\n",
      "Iteration 56, loss = 1.35387275\n",
      "Iteration 50, loss = 1.34393287\n",
      "Iteration 51, loss = 1.80795668\n",
      "Iteration 57, loss = 1.69792308\n",
      "Iteration 52, loss = 1.21713964\n",
      "Iteration 58, loss = 1.90737376\n",
      "Iteration 53, loss = 1.48888931\n",
      "Iteration 59, loss = 1.63410383\n",
      "Iteration 54, loss = 1.40216134\n",
      "Iteration 55, loss = 0.95986889\n",
      "Iteration 56, loss = 0.80577741\n",
      "Iteration 57, loss = 1.04593221\n",
      "Iteration 60, loss = 1.79965831\n",
      "Iteration 58, loss = 1.05182891\n",
      "Iteration 61, loss = 2.84746025\n",
      "Iteration 62, loss = 2.02756798\n",
      "Iteration 59, loss = 1.57773194\n",
      "Iteration 60, loss = 3.20773668\n",
      "Iteration 63, loss = 1.55094112\n",
      "Iteration 61, loss = 3.48442079\n",
      "Iteration 64, loss = 1.45967424\n",
      "Iteration 62, loss = 2.41684115\n",
      "Iteration 65, loss = 1.41944046\n",
      "Iteration 66, loss = 1.71106732\n",
      "Iteration 63, loss = 1.23883422\n",
      "Iteration 64, loss = 1.78534111\n",
      "Iteration 67, loss = 1.30968737\n",
      "Iteration 65, loss = 1.66773136\n",
      "Iteration 68, loss = 1.83784751\n",
      "Iteration 66, loss = 1.77755161\n",
      "Iteration 69, loss = 1.70404271\n",
      "Iteration 67, loss = 2.00873797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 70, loss = 1.64180439\n",
      "Iteration 71, loss = 1.75855017\n",
      "Iteration 72, loss = 1.68661915\n",
      "Iteration 73, loss = 1.64985695\n",
      "Iteration 74, loss = 1.57914107\n",
      "Iteration 75, loss = 1.58242324\n",
      "Iteration 76, loss = 1.39553149\n",
      "Iteration 77, loss = 1.81666552\n",
      "Iteration 78, loss = 1.74727777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.41915499\n",
      "Iteration 1, loss = 18.06446616\n",
      "Iteration 2, loss = 14.72532384\n",
      "Iteration 2, loss = 16.26982050\n",
      "Iteration 3, loss = 14.63436729\n",
      "Iteration 3, loss = 14.51303224\n",
      "Iteration 4, loss = 12.39378798\n",
      "Iteration 4, loss = 10.01302787\n",
      "Iteration 5, loss = 10.21561128\n",
      "Iteration 5, loss = 8.35689264\n",
      "Iteration 6, loss = 7.87841649\n",
      "Iteration 6, loss = 7.98231266\n",
      "Iteration 7, loss = 9.78871129\n",
      "Iteration 7, loss = 7.87083693\n",
      "Iteration 8, loss = 5.97706993\n",
      "Iteration 8, loss = 6.50701681\n",
      "Iteration 9, loss = 8.95674625\n",
      "Iteration 9, loss = 6.32330652\n",
      "Iteration 10, loss = 7.45379036\n",
      "Iteration 10, loss = 5.58061891\n",
      "Iteration 11, loss = 6.03035460\n",
      "Iteration 11, loss = 6.18710439\n",
      "Iteration 12, loss = 6.29191414\n",
      "Iteration 13, loss = 7.50460481\n",
      "Iteration 12, loss = 7.54910806\n",
      "Iteration 14, loss = 5.07077711\n",
      "Iteration 15, loss = 4.41802605\n",
      "Iteration 16, loss = 3.62063874\n",
      "Iteration 13, loss = 7.24914614\n",
      "Iteration 17, loss = 4.43405928\n",
      "Iteration 14, loss = 10.99816713\n",
      "Iteration 18, loss = 3.37302772\n",
      "Iteration 15, loss = 6.20258391\n",
      "Iteration 19, loss = 8.99937369\n",
      "Iteration 16, loss = 4.67869358\n",
      "Iteration 20, loss = 5.05946019\n",
      "Iteration 17, loss = 4.41897504\n",
      "Iteration 21, loss = 4.29782453\n",
      "Iteration 18, loss = 3.70643532\n",
      "Iteration 22, loss = 4.51414307\n",
      "Iteration 23, loss = 4.78294817\n",
      "Iteration 19, loss = 3.82146534\n",
      "Iteration 20, loss = 4.51950723\n",
      "Iteration 24, loss = 3.34387991\n",
      "Iteration 21, loss = 3.72744118\n",
      "Iteration 25, loss = 5.07164760\n",
      "Iteration 22, loss = 3.28842247\n",
      "Iteration 26, loss = 4.29315911\n",
      "Iteration 27, loss = 2.94678628\n",
      "Iteration 23, loss = 5.82916962\n",
      "Iteration 28, loss = 3.61220559\n",
      "Iteration 24, loss = 8.40411654\n",
      "Iteration 29, loss = 3.46407091\n",
      "Iteration 25, loss = 5.06332952\n",
      "Iteration 26, loss = 4.17421964\n",
      "Iteration 30, loss = 3.94855261\n",
      "Iteration 27, loss = 3.48910932\n",
      "Iteration 31, loss = 4.01423016\n",
      "Iteration 28, loss = 2.48760070\n",
      "Iteration 32, loss = 3.71948314\n",
      "Iteration 29, loss = 3.46002714\n",
      "Iteration 33, loss = 3.12264633\n",
      "Iteration 30, loss = 2.64592606\n",
      "Iteration 34, loss = 4.62935266\n",
      "Iteration 31, loss = 3.26682713\n",
      "Iteration 35, loss = 3.90472220\n",
      "Iteration 32, loss = 2.50786145\n",
      "Iteration 36, loss = 3.20079769\n",
      "Iteration 33, loss = 2.04380800\n",
      "Iteration 37, loss = 2.38565166\n",
      "Iteration 34, loss = 2.03504764\n",
      "Iteration 38, loss = 2.49798776\n",
      "Iteration 35, loss = 1.68402194\n",
      "Iteration 36, loss = 2.52353826\n",
      "Iteration 39, loss = 1.87948245\n",
      "Iteration 37, loss = 2.52957302\n",
      "Iteration 40, loss = 2.44141850\n",
      "Iteration 38, loss = 1.56363839\n",
      "Iteration 41, loss = 1.83479445\n",
      "Iteration 39, loss = 1.76309931\n",
      "Iteration 42, loss = 1.74936022\n",
      "Iteration 40, loss = 2.01852237\n",
      "Iteration 43, loss = 1.91752240\n",
      "Iteration 44, loss = 1.70238822\n",
      "Iteration 41, loss = 1.89395324\n",
      "Iteration 42, loss = 2.50710273\n",
      "Iteration 43, loss = 2.62505322\n",
      "Iteration 44, loss = 1.98010872\n",
      "Iteration 45, loss = 1.44512309\n",
      "Iteration 46, loss = 1.51964640\n",
      "Iteration 45, loss = 1.47644872\n",
      "Iteration 47, loss = 1.48679080\n",
      "Iteration 46, loss = 1.72301589\n",
      "Iteration 48, loss = 2.50506690\n",
      "Iteration 47, loss = 1.39113303\n",
      "Iteration 48, loss = 1.29619098\n",
      "Iteration 49, loss = 1.23787934\n",
      "Iteration 49, loss = 1.87826334\n",
      "Iteration 50, loss = 1.44451885\n",
      "Iteration 51, loss = 1.46079030\n",
      "Iteration 50, loss = 1.44210101\n",
      "Iteration 52, loss = 1.43002147\n",
      "Iteration 53, loss = 1.35766183\n",
      "Iteration 51, loss = 1.50532363\n",
      "Iteration 54, loss = 1.33710229\n",
      "Iteration 52, loss = 1.53654917\n",
      "Iteration 55, loss = 1.22986432\n",
      "Iteration 53, loss = 1.31412677\n",
      "Iteration 56, loss = 1.68050884\n",
      "Iteration 54, loss = 0.95924240\n",
      "Iteration 57, loss = 1.79580600\n",
      "Iteration 55, loss = 1.21482207\n",
      "Iteration 58, loss = 1.74030161\n",
      "Iteration 56, loss = 1.15047138\n",
      "Iteration 59, loss = 2.16597631\n",
      "Iteration 57, loss = 1.16321156\n",
      "Iteration 60, loss = 1.24079663\n",
      "Iteration 58, loss = 1.35100326\n",
      "Iteration 61, loss = 1.41390017\n",
      "Iteration 59, loss = 0.96093957\n",
      "Iteration 60, loss = 1.27540084\n",
      "Iteration 61, loss = 1.11329713\n",
      "Iteration 62, loss = 1.39881992\n",
      "Iteration 62, loss = 1.39453432\n",
      "Iteration 63, loss = 1.12964176\n",
      "Iteration 63, loss = 1.87114501\n",
      "Iteration 64, loss = 1.23197356\n",
      "Iteration 64, loss = 1.68929871\n",
      "Iteration 65, loss = 1.57632235\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 65, loss = 0.93746682\n",
      "Iteration 66, loss = 1.01447238\n",
      "Iteration 67, loss = 1.95577923\n",
      "Iteration 68, loss = 2.35228333\n",
      "Iteration 69, loss = 2.64163038\n",
      "Iteration 70, loss = 2.08991765\n",
      "Iteration 71, loss = 2.69105039\n",
      "Iteration 72, loss = 2.57469949\n",
      "Iteration 73, loss = 1.65953672\n",
      "Iteration 1, loss = 20.39898150\n",
      "Iteration 2, loss = 14.91874593\n",
      "Iteration 74, loss = 1.92351109\n",
      "Iteration 3, loss = 15.52726958\n",
      "Iteration 75, loss = 1.32801436\n",
      "Iteration 4, loss = 13.96500161\n",
      "Iteration 5, loss = 13.93402700\n",
      "Iteration 76, loss = 1.37276534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 10.70270765\n",
      "Iteration 7, loss = 9.18915962\n",
      "Iteration 8, loss = 8.70089352\n",
      "Iteration 9, loss = 7.56762657\n",
      "Iteration 10, loss = 5.79570506\n",
      "Iteration 11, loss = 5.38550825\n",
      "Iteration 12, loss = 10.61617860\n",
      "Iteration 13, loss = 5.66729214\n",
      "Iteration 14, loss = 4.92530788\n",
      "Iteration 15, loss = 4.57348288\n",
      "Iteration 16, loss = 3.54003779\n",
      "Iteration 17, loss = 3.67222537\n",
      "Iteration 18, loss = 3.58548544\n",
      "Iteration 1, loss = 19.23504276\n",
      "Iteration 2, loss = 21.41557559\n",
      "Iteration 19, loss = 3.90432979\n",
      "Iteration 3, loss = 9.04955491\n",
      "Iteration 20, loss = 3.95080491\n",
      "Iteration 21, loss = 3.95688819\n",
      "Iteration 4, loss = 10.55436015\n",
      "Iteration 22, loss = 4.20206712\n",
      "Iteration 23, loss = 3.25490817\n",
      "Iteration 5, loss = 8.23204098\n",
      "Iteration 24, loss = 2.95720232\n",
      "Iteration 25, loss = 2.11168254\n",
      "Iteration 6, loss = 8.06938332\n",
      "Iteration 26, loss = 2.86215692\n",
      "Iteration 27, loss = 2.14337551\n",
      "Iteration 7, loss = 6.63118721\n",
      "Iteration 8, loss = 7.70274140\n",
      "Iteration 28, loss = 2.76365226\n",
      "Iteration 9, loss = 4.81253032\n",
      "Iteration 29, loss = 2.90526850\n",
      "Iteration 10, loss = 4.66540354\n",
      "Iteration 30, loss = 3.43798641\n",
      "Iteration 31, loss = 2.60262466\n",
      "Iteration 11, loss = 3.65248656\n",
      "Iteration 12, loss = 5.56543578\n",
      "Iteration 32, loss = 2.99114290\n",
      "Iteration 13, loss = 4.58768397\n",
      "Iteration 14, loss = 4.88060856\n",
      "Iteration 33, loss = 2.59395563\n",
      "Iteration 34, loss = 3.06453174\n",
      "Iteration 35, loss = 3.36152300\n",
      "Iteration 15, loss = 5.07781653\n",
      "Iteration 36, loss = 3.10423571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 4.26357281\n",
      "Iteration 17, loss = 3.20008195\n",
      "Iteration 18, loss = 4.14952552\n",
      "Iteration 19, loss = 3.39819568\n",
      "Iteration 20, loss = 2.75566254\n",
      "Iteration 21, loss = 3.86995292\n",
      "Iteration 22, loss = 2.44724976\n",
      "Iteration 23, loss = 2.30613017\n",
      "Iteration 24, loss = 2.91953837\n",
      "Iteration 25, loss = 2.24141803\n",
      "Iteration 26, loss = 3.39959849\n",
      "Iteration 1, loss = 17.53714692\n",
      "Iteration 27, loss = 1.98610688\n",
      "Iteration 2, loss = 18.64312254\n",
      "Iteration 28, loss = 2.10692336\n",
      "Iteration 3, loss = 11.17303558\n",
      "Iteration 29, loss = 2.06124122\n",
      "Iteration 30, loss = 2.56532385\n",
      "Iteration 31, loss = 2.21715472\n",
      "Iteration 4, loss = 8.39231843\n",
      "Iteration 32, loss = 2.74574845\n",
      "Iteration 33, loss = 5.08678397\n",
      "Iteration 5, loss = 9.31439540\n",
      "Iteration 34, loss = 3.48093462\n",
      "Iteration 6, loss = 7.72192658\n",
      "Iteration 35, loss = 2.54023221\n",
      "Iteration 7, loss = 6.24178768\n",
      "Iteration 36, loss = 2.37762930\n",
      "Iteration 8, loss = 8.58032711\n",
      "Iteration 37, loss = 2.07301159\n",
      "Iteration 38, loss = 3.12008919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 6.30378812\n",
      "Iteration 10, loss = 5.34012108\n",
      "Iteration 11, loss = 6.61778609\n",
      "Iteration 12, loss = 5.32149891\n",
      "Iteration 13, loss = 4.55851241\n",
      "Iteration 14, loss = 6.20223288\n",
      "Iteration 15, loss = 5.17945588\n",
      "Iteration 16, loss = 3.67037494\n",
      "Iteration 17, loss = 4.18974785\n",
      "Iteration 18, loss = 3.91550078\n",
      "Iteration 19, loss = 3.28896410\n",
      "Iteration 20, loss = 5.00994601\n",
      "Iteration 1, loss = 15.07492529\n",
      "Iteration 21, loss = 4.16611154\n",
      "Iteration 2, loss = 15.15879432\n",
      "Iteration 22, loss = 4.88352834\n",
      "Iteration 3, loss = 10.37842948\n",
      "Iteration 23, loss = 5.79293796\n",
      "Iteration 4, loss = 10.00030111\n",
      "Iteration 24, loss = 6.33326172\n",
      "Iteration 5, loss = 10.68344301\n",
      "Iteration 25, loss = 5.84592395\n",
      "Iteration 6, loss = 8.30346129\n",
      "Iteration 7, loss = 6.29752089\n",
      "Iteration 8, loss = 7.10079439\n",
      "Iteration 26, loss = 4.13922566\n",
      "Iteration 9, loss = 4.47992777\n",
      "Iteration 27, loss = 5.74084809\n",
      "Iteration 28, loss = 3.92651969\n",
      "Iteration 29, loss = 2.93573971\n",
      "Iteration 10, loss = 5.33543660\n",
      "Iteration 30, loss = 2.86698265\n",
      "Iteration 11, loss = 5.59320166\n",
      "Iteration 12, loss = 3.75374765\n",
      "Iteration 13, loss = 3.54537629\n",
      "Iteration 31, loss = 3.66023746\n",
      "Iteration 32, loss = 3.72599017\n",
      "Iteration 14, loss = 3.66370053Iteration 33, loss = 3.37988938\n",
      "\n",
      "Iteration 34, loss = 4.41866297\n",
      "Iteration 15, loss = 3.83219691\n",
      "Iteration 16, loss = 3.75390159\n",
      "Iteration 17, loss = 4.27531385\n",
      "Iteration 35, loss = 4.61226576\n",
      "Iteration 18, loss = 3.12469763\n",
      "Iteration 36, loss = 2.71395907\n",
      "Iteration 37, loss = 3.16419364\n",
      "Iteration 38, loss = 2.06962749\n",
      "Iteration 19, loss = 2.65830029\n",
      "Iteration 20, loss = 2.87960452\n",
      "Iteration 39, loss = 2.15137004\n",
      "Iteration 21, loss = 2.55538902\n",
      "Iteration 22, loss = 3.52341805\n",
      "Iteration 40, loss = 2.32851842\n",
      "Iteration 41, loss = 2.22203293\n",
      "Iteration 23, loss = 3.03946900\n",
      "Iteration 42, loss = 2.06955669\n",
      "Iteration 43, loss = 1.76641299\n",
      "Iteration 24, loss = 2.09392640\n",
      "Iteration 25, loss = 2.20090996\n",
      "Iteration 26, loss = 1.77798792\n",
      "Iteration 44, loss = 2.66449695\n",
      "Iteration 45, loss = 2.69012413\n",
      "Iteration 27, loss = 1.71560705\n",
      "Iteration 46, loss = 2.84714720\n",
      "Iteration 47, loss = 2.46944839\n",
      "Iteration 28, loss = 1.69094877\n",
      "Iteration 29, loss = 2.04442258\n",
      "Iteration 48, loss = 2.92614086\n",
      "Iteration 30, loss = 1.62433981\n",
      "Iteration 31, loss = 2.40321060\n",
      "Iteration 49, loss = 2.96643513\n",
      "Iteration 32, loss = 1.96741282\n",
      "Iteration 50, loss = 2.85198542\n",
      "Iteration 33, loss = 1.96892729\n",
      "Iteration 51, loss = 3.04558514\n",
      "Iteration 34, loss = 1.48378092\n",
      "Iteration 52, loss = 2.22041247\n",
      "Iteration 35, loss = 1.43678558\n",
      "Iteration 53, loss = 1.73773649\n",
      "Iteration 36, loss = 1.48414366\n",
      "Iteration 54, loss = 2.26001847\n",
      "Iteration 55, loss = 2.28708228\n",
      "Iteration 37, loss = 1.37419343\n",
      "Iteration 56, loss = 1.79889789\n",
      "Iteration 38, loss = 1.08175755\n",
      "Iteration 57, loss = 2.19506741\n",
      "Iteration 39, loss = 1.30587302\n",
      "Iteration 58, loss = 2.01202324\n",
      "Iteration 40, loss = 1.28974841\n",
      "Iteration 59, loss = 2.06860188\n",
      "Iteration 41, loss = 1.22264365\n",
      "Iteration 60, loss = 1.35884932\n",
      "Iteration 42, loss = 1.22673100\n",
      "Iteration 61, loss = 1.48729322\n",
      "Iteration 43, loss = 1.67320596\n",
      "Iteration 62, loss = 1.51261394\n",
      "Iteration 44, loss = 3.14738333\n",
      "Iteration 63, loss = 1.91343804\n",
      "Iteration 45, loss = 2.55324376\n",
      "Iteration 64, loss = 2.02232185\n",
      "Iteration 46, loss = 2.53192146\n",
      "Iteration 65, loss = 2.47149301\n",
      "Iteration 66, loss = 2.25073969\n",
      "Iteration 67, loss = 1.71737422\n",
      "Iteration 47, loss = 1.90791119\n",
      "Iteration 68, loss = 1.42408600\n",
      "Iteration 48, loss = 1.32184276\n",
      "Iteration 49, loss = 1.67826734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 69, loss = 1.27256044\n",
      "Iteration 70, loss = 1.93851015\n",
      "Iteration 71, loss = 2.14377235\n",
      "Iteration 72, loss = 1.74940496\n",
      "Iteration 73, loss = 2.08702144\n",
      "Iteration 74, loss = 1.18611963\n",
      "Iteration 75, loss = 1.67575345\n",
      "Iteration 76, loss = 1.43786780\n",
      "Iteration 77, loss = 1.70538054\n",
      "Iteration 78, loss = 1.69975982\n",
      "Iteration 1, loss = 14.35773033\n",
      "Iteration 79, loss = 1.11508646\n",
      "Iteration 2, loss = 12.33419487\n",
      "Iteration 80, loss = 1.67903156\n",
      "Iteration 3, loss = 14.11031837\n",
      "Iteration 4, loss = 11.41710286\n",
      "Iteration 81, loss = 1.37648318\n",
      "Iteration 5, loss = 8.37256723\n",
      "Iteration 82, loss = 1.55029333\n",
      "Iteration 6, loss = 7.92931535\n",
      "Iteration 7, loss = 8.26197176\n",
      "Iteration 83, loss = 1.37795784\n",
      "Iteration 8, loss = 8.49313518\n",
      "Iteration 9, loss = 5.35186976\n",
      "Iteration 84, loss = 1.34973152\n",
      "Iteration 85, loss = 1.39280618\n",
      "Iteration 86, loss = 1.40694483\n",
      "Iteration 10, loss = 6.18668373\n",
      "Iteration 11, loss = 7.77132178\n",
      "Iteration 12, loss = 4.47221655\n",
      "Iteration 87, loss = 1.09567955\n",
      "Iteration 13, loss = 4.89379537\n",
      "Iteration 88, loss = 1.41640290\n",
      "Iteration 89, loss = 1.35716958\n",
      "Iteration 14, loss = 3.84098748\n",
      "Iteration 90, loss = 1.04732685\n",
      "Iteration 15, loss = 2.95165649\n",
      "Iteration 16, loss = 3.68092211\n",
      "Iteration 91, loss = 0.77660118\n",
      "Iteration 17, loss = 2.90065959\n",
      "Iteration 92, loss = 0.81594758\n",
      "Iteration 18, loss = 2.85592202\n",
      "Iteration 93, loss = 1.15751960\n",
      "Iteration 94, loss = 1.33348772\n",
      "Iteration 19, loss = 4.34292881\n",
      "Iteration 20, loss = 3.16685968\n",
      "Iteration 95, loss = 0.94742869Iteration 21, loss = 3.06527605\n",
      "\n",
      "Iteration 96, loss = 0.79607893\n",
      "Iteration 22, loss = 3.30765563\n",
      "Iteration 97, loss = 0.68491884\n",
      "Iteration 98, loss = 1.39323281\n",
      "Iteration 23, loss = 3.12274283\n",
      "Iteration 24, loss = 2.57635714\n",
      "Iteration 99, loss = 1.34657606\n",
      "Iteration 25, loss = 3.29925462\n",
      "Iteration 26, loss = 2.50455918\n",
      "Iteration 100, loss = 1.67190353\n",
      "Iteration 27, loss = 2.93623857\n",
      "Iteration 101, loss = 2.05151887\n",
      "Iteration 102, loss = 1.30307768\n",
      "Iteration 103, loss = 1.05307777\n",
      "Iteration 28, loss = 2.71302202\n",
      "Iteration 29, loss = 2.98306743\n",
      "Iteration 30, loss = 3.18612466\n",
      "Iteration 104, loss = 1.07263568\n",
      "Iteration 31, loss = 2.34485335\n",
      "Iteration 105, loss = 1.44851492\n",
      "Iteration 32, loss = 2.74642867\n",
      "Iteration 106, loss = 1.87662370\n",
      "Iteration 107, loss = 1.65581748\n",
      "Iteration 33, loss = 2.17415677\n",
      "Iteration 108, loss = 1.42032116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 2.19043402\n",
      "Iteration 35, loss = 1.88190447\n",
      "Iteration 36, loss = 1.83449122\n",
      "Iteration 37, loss = 1.69791988\n",
      "Iteration 38, loss = 2.11989679\n",
      "Iteration 39, loss = 2.23622097\n",
      "Iteration 40, loss = 1.78862426\n",
      "Iteration 41, loss = 2.33071315\n",
      "Iteration 42, loss = 1.90951792\n",
      "Iteration 1, loss = 13.80806876\n",
      "Iteration 43, loss = 1.96193536\n",
      "Iteration 2, loss = 13.03997984\n",
      "Iteration 3, loss = 13.36818561\n",
      "Iteration 44, loss = 1.84418774\n",
      "Iteration 4, loss = 8.32283364\n",
      "Iteration 5, loss = 5.90480598\n",
      "Iteration 45, loss = 2.21388941\n",
      "Iteration 46, loss = 1.74853906\n",
      "Iteration 47, loss = 1.72477327\n",
      "Iteration 6, loss = 5.84024563\n",
      "Iteration 48, loss = 1.31173963\n",
      "Iteration 7, loss = 5.34240429\n",
      "Iteration 8, loss = 6.21684804\n",
      "Iteration 9, loss = 7.67628626\n",
      "Iteration 49, loss = 1.53898330\n",
      "Iteration 50, loss = 1.74809015\n",
      "Iteration 10, loss = 6.97679090\n",
      "Iteration 51, loss = 1.71191999\n",
      "Iteration 52, loss = 1.89698112\n",
      "Iteration 11, loss = 6.62976550\n",
      "Iteration 53, loss = 1.82127257\n",
      "Iteration 12, loss = 5.19782836\n",
      "Iteration 13, loss = 4.65626791\n",
      "Iteration 54, loss = 1.94267992\n",
      "Iteration 14, loss = 3.95640229\n",
      "Iteration 15, loss = 3.93630387\n",
      "Iteration 55, loss = 2.41904346Iteration 16, loss = 3.22509512\n",
      "\n",
      "Iteration 17, loss = 2.83913300\n",
      "Iteration 56, loss = 2.12604302\n",
      "Iteration 18, loss = 2.94839515\n",
      "Iteration 57, loss = 1.89288823\n",
      "Iteration 19, loss = 2.77219496\n",
      "Iteration 58, loss = 1.90060967\n",
      "Iteration 20, loss = 1.98478597\n",
      "Iteration 59, loss = 1.84299732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 2.86252021\n",
      "Iteration 22, loss = 3.44795359\n",
      "Iteration 23, loss = 2.25663522\n",
      "Iteration 24, loss = 2.81408002\n",
      "Iteration 25, loss = 2.73025432\n",
      "Iteration 26, loss = 2.89869768\n",
      "Iteration 27, loss = 2.67534898\n",
      "Iteration 28, loss = 2.50321654\n",
      "Iteration 29, loss = 2.59123806\n",
      "Iteration 30, loss = 2.00387497\n",
      "Iteration 1, loss = 16.54515910\n",
      "Iteration 2, loss = 12.95934921\n",
      "Iteration 31, loss = 2.40924895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 14.01825512\n",
      "Iteration 4, loss = 8.69418773\n",
      "Iteration 5, loss = 8.21235877\n",
      "Iteration 6, loss = 7.32306488\n",
      "Iteration 7, loss = 7.72672879\n",
      "Iteration 1, loss = 19.20406498\n",
      "Iteration 8, loss = 6.11364907\n",
      "Iteration 2, loss = 20.80291027\n",
      "Iteration 9, loss = 6.48719052\n",
      "Iteration 3, loss = 15.16459062\n",
      "Iteration 10, loss = 5.44006654\n",
      "Iteration 4, loss = 12.74068248\n",
      "Iteration 5, loss = 9.37876775\n",
      "Iteration 11, loss = 5.38895694\n",
      "Iteration 6, loss = 10.02260557\n",
      "Iteration 12, loss = 4.70587914\n",
      "Iteration 7, loss = 9.57677433\n",
      "Iteration 13, loss = 5.92910116\n",
      "Iteration 14, loss = 4.51723833\n",
      "Iteration 8, loss = 10.10676606\n",
      "Iteration 15, loss = 5.24041875\n",
      "Iteration 9, loss = 5.96204547\n",
      "Iteration 16, loss = 4.46971202\n",
      "Iteration 17, loss = 3.89671242\n",
      "Iteration 10, loss = 5.53587829\n",
      "Iteration 18, loss = 4.19554451\n",
      "Iteration 11, loss = 6.52976910\n",
      "Iteration 19, loss = 3.13769640\n",
      "Iteration 12, loss = 7.24502758\n",
      "Iteration 20, loss = 4.96782485\n",
      "Iteration 21, loss = 3.58937919\n",
      "Iteration 22, loss = 2.64933032\n",
      "Iteration 13, loss = 5.40287978\n",
      "Iteration 23, loss = 4.39843513\n",
      "Iteration 24, loss = 4.81764817\n",
      "Iteration 14, loss = 5.62074388\n",
      "Iteration 15, loss = 4.66105984\n",
      "Iteration 16, loss = 4.26150023\n",
      "Iteration 25, loss = 3.35247305\n",
      "Iteration 17, loss = 4.98169338\n",
      "Iteration 26, loss = 2.88203300\n",
      "Iteration 27, loss = 2.75586975\n",
      "Iteration 28, loss = 3.52720753\n",
      "Iteration 18, loss = 4.85826285\n",
      "Iteration 29, loss = 2.51323866\n",
      "Iteration 19, loss = 3.87080586\n",
      "Iteration 20, loss = 3.29322924\n",
      "Iteration 30, loss = 1.95694415\n",
      "Iteration 21, loss = 2.63292796\n",
      "Iteration 31, loss = 2.12711549\n",
      "Iteration 32, loss = 1.79020079\n",
      "Iteration 33, loss = 2.41119246\n",
      "Iteration 22, loss = 3.33086310\n",
      "Iteration 23, loss = 3.73578659\n",
      "Iteration 24, loss = 3.46626756\n",
      "Iteration 34, loss = 2.66789045\n",
      "Iteration 25, loss = 2.96447179\n",
      "Iteration 35, loss = 2.72733472\n",
      "Iteration 36, loss = 2.37020927\n",
      "Iteration 37, loss = 1.82237092\n",
      "Iteration 26, loss = 3.07841278\n",
      "Iteration 27, loss = 2.70474539\n",
      "Iteration 28, loss = 2.56055348\n",
      "Iteration 38, loss = 2.54166611\n",
      "Iteration 29, loss = 3.46110121\n",
      "Iteration 30, loss = 2.49845585\n",
      "Iteration 39, loss = 2.30047755\n",
      "Iteration 40, loss = 2.55735468\n",
      "Iteration 41, loss = 1.95317142\n",
      "Iteration 31, loss = 2.16913098\n",
      "Iteration 32, loss = 2.75770624\n",
      "Iteration 42, loss = 2.44733000\n",
      "Iteration 33, loss = 2.92785755\n",
      "Iteration 34, loss = 2.74090861\n",
      "Iteration 43, loss = 2.56488130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 2.67705473\n",
      "Iteration 36, loss = 3.63468528\n",
      "Iteration 37, loss = 2.82511224\n",
      "Iteration 38, loss = 2.95080960\n",
      "Iteration 39, loss = 2.31560167\n",
      "Iteration 40, loss = 2.13188411\n",
      "Iteration 41, loss = 2.08081501\n",
      "Iteration 42, loss = 2.17334285\n",
      "Iteration 43, loss = 2.24173526\n",
      "Iteration 1, loss = 18.34755230\n",
      "Iteration 44, loss = 2.38477685\n",
      "Iteration 2, loss = 15.59831468\n",
      "Iteration 3, loss = 13.71415988\n",
      "Iteration 45, loss = 3.15419777\n",
      "Iteration 4, loss = 12.97904713\n",
      "Iteration 46, loss = 2.79584984\n",
      "Iteration 5, loss = 12.28421264\n",
      "Iteration 47, loss = 3.23195980\n",
      "Iteration 48, loss = 2.76872943\n",
      "Iteration 6, loss = 9.52473023\n",
      "Iteration 7, loss = 7.72982029\n",
      "Iteration 49, loss = 3.40265602\n",
      "Iteration 8, loss = 8.34181076\n",
      "Iteration 50, loss = 3.14589393\n",
      "Iteration 9, loss = 5.93364835\n",
      "Iteration 51, loss = 2.01705129\n",
      "Iteration 52, loss = 2.06048703\n",
      "Iteration 10, loss = 4.90268458\n",
      "Iteration 53, loss = 1.74495309\n",
      "Iteration 11, loss = 4.58098968\n",
      "Iteration 12, loss = 4.96315708\n",
      "Iteration 54, loss = 2.09639582\n",
      "Iteration 13, loss = 7.51612233\n",
      "Iteration 55, loss = 2.59223341\n",
      "Iteration 56, loss = 3.44525908\n",
      "Iteration 14, loss = 4.53574127\n",
      "Iteration 57, loss = 2.29637792\n",
      "Iteration 15, loss = 4.24856176\n",
      "Iteration 58, loss = 1.86205355\n",
      "Iteration 16, loss = 4.67614313\n",
      "Iteration 59, loss = 1.73666094\n",
      "Iteration 17, loss = 4.56888341\n",
      "Iteration 60, loss = 1.96720983\n",
      "Iteration 18, loss = 3.89590526\n",
      "Iteration 61, loss = 1.79065636\n",
      "Iteration 19, loss = 3.01090352\n",
      "Iteration 20, loss = 2.75456066\n",
      "Iteration 62, loss = 1.48185630\n",
      "Iteration 21, loss = 2.59353217\n",
      "Iteration 63, loss = 2.24855210\n",
      "Iteration 22, loss = 2.53756112\n",
      "Iteration 64, loss = 2.11739726\n",
      "Iteration 65, loss = 1.90580078\n",
      "Iteration 23, loss = 2.99677606\n",
      "Iteration 24, loss = 3.01211441\n",
      "Iteration 66, loss = 1.65346092\n",
      "Iteration 25, loss = 2.76527321\n",
      "Iteration 67, loss = 1.45432500\n",
      "Iteration 26, loss = 2.16422856\n",
      "Iteration 68, loss = 2.48066088\n",
      "Iteration 27, loss = 2.60165893\n",
      "Iteration 28, loss = 2.45456845\n",
      "Iteration 69, loss = 2.37510607\n",
      "Iteration 29, loss = 1.94868476\n",
      "Iteration 30, loss = 2.37050322\n",
      "Iteration 70, loss = 1.81826596\n",
      "Iteration 31, loss = 2.71916711\n",
      "Iteration 71, loss = 1.96858151\n",
      "Iteration 32, loss = 1.99094037\n",
      "Iteration 72, loss = 1.90043670\n",
      "Iteration 33, loss = 1.63303425\n",
      "Iteration 34, loss = 1.87179037\n",
      "Iteration 73, loss = 1.88479993\n",
      "Iteration 35, loss = 1.59404064\n",
      "Iteration 36, loss = 1.25717467\n",
      "Iteration 74, loss = 2.07484981\n",
      "Iteration 75, loss = 2.27409060\n",
      "Iteration 76, loss = 2.83956836\n",
      "Iteration 37, loss = 1.74188943\n",
      "Iteration 38, loss = 3.16675132\n",
      "Iteration 39, loss = 3.18257252\n",
      "Iteration 77, loss = 2.11031535\n",
      "Iteration 40, loss = 1.96675807\n",
      "Iteration 78, loss = 1.99530950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 2.05279720\n",
      "Iteration 42, loss = 2.15288926\n",
      "Iteration 43, loss = 2.70749476\n",
      "Iteration 44, loss = 1.77389837\n",
      "Iteration 45, loss = 2.19318840\n",
      "Iteration 46, loss = 1.97450124\n",
      "Iteration 47, loss = 1.98446514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.50017740\n",
      "Iteration 2, loss = 14.47032762\n",
      "Iteration 3, loss = 15.22327874\n",
      "Iteration 4, loss = 9.38562420\n",
      "Iteration 5, loss = 11.67547413\n",
      "Iteration 1, loss = 19.31319823\n",
      "Iteration 6, loss = 9.44847563\n",
      "Iteration 7, loss = 7.73133371\n",
      "Iteration 2, loss = 16.84365581\n",
      "Iteration 8, loss = 6.75942571\n",
      "Iteration 3, loss = 14.77443552\n",
      "Iteration 9, loss = 5.00083657\n",
      "Iteration 4, loss = 14.61065691\n",
      "Iteration 5, loss = 12.63771752\n",
      "Iteration 10, loss = 5.41704217\n",
      "Iteration 6, loss = 7.04760669\n",
      "Iteration 11, loss = 4.65142642\n",
      "Iteration 12, loss = 4.41303501\n",
      "Iteration 7, loss = 7.00014847\n",
      "Iteration 8, loss = 5.67764848\n",
      "Iteration 13, loss = 6.84182169\n",
      "Iteration 9, loss = 5.88736137\n",
      "Iteration 10, loss = 5.18396099\n",
      "Iteration 14, loss = 6.93041260\n",
      "Iteration 11, loss = 6.71320723\n",
      "Iteration 15, loss = 4.26592818\n",
      "Iteration 12, loss = 6.05172226\n",
      "Iteration 16, loss = 3.94623592\n",
      "Iteration 13, loss = 5.77680356\n",
      "Iteration 14, loss = 5.00557831\n",
      "Iteration 17, loss = 4.23089447\n",
      "Iteration 18, loss = 3.55455476\n",
      "Iteration 15, loss = 5.24968834\n",
      "Iteration 19, loss = 3.56740468\n",
      "Iteration 16, loss = 6.18663331\n",
      "Iteration 20, loss = 3.06976601\n",
      "Iteration 17, loss = 6.10626436\n",
      "Iteration 18, loss = 5.07631521\n",
      "Iteration 21, loss = 3.10921682\n",
      "Iteration 19, loss = 4.19470892\n",
      "Iteration 22, loss = 3.33238124\n",
      "Iteration 20, loss = 3.37589759\n",
      "Iteration 23, loss = 2.95038983\n",
      "Iteration 21, loss = 2.82620089\n",
      "Iteration 24, loss = 2.23084582\n",
      "Iteration 25, loss = 3.12584938\n",
      "Iteration 26, loss = 3.46266506\n",
      "Iteration 22, loss = 2.55905620\n",
      "Iteration 23, loss = 2.47902972\n",
      "Iteration 24, loss = 2.29963896\n",
      "Iteration 27, loss = 2.76644257\n",
      "Iteration 25, loss = 2.21933944\n",
      "Iteration 26, loss = 2.27051107\n",
      "Iteration 28, loss = 2.90405415\n",
      "Iteration 29, loss = 2.20742347\n",
      "Iteration 30, loss = 2.61871991\n",
      "Iteration 27, loss = 2.72144001\n",
      "Iteration 31, loss = 2.47199539\n",
      "Iteration 32, loss = 2.08540496\n",
      "Iteration 28, loss = 3.58311577\n",
      "Iteration 33, loss = 2.29671795\n",
      "Iteration 29, loss = 3.00417846\n",
      "Iteration 30, loss = 3.00007006\n",
      "Iteration 34, loss = 1.62934104\n",
      "Iteration 35, loss = 2.07563088\n",
      "Iteration 31, loss = 2.58279555\n",
      "Iteration 36, loss = 1.98157442\n",
      "Iteration 32, loss = 2.07715386\n",
      "Iteration 37, loss = 2.68726978\n",
      "Iteration 33, loss = 2.40796309\n",
      "Iteration 34, loss = 2.38166574\n",
      "Iteration 38, loss = 1.95756265\n",
      "Iteration 35, loss = 2.08673406\n",
      "Iteration 39, loss = 2.61298595\n",
      "Iteration 36, loss = 2.25169292\n",
      "Iteration 40, loss = 2.17389017\n",
      "Iteration 37, loss = 2.23256797\n",
      "Iteration 41, loss = 1.87728141\n",
      "Iteration 38, loss = 2.73965865\n",
      "Iteration 39, loss = 2.35333363\n",
      "Iteration 40, loss = 2.52399223\n",
      "Iteration 42, loss = 1.39804972\n",
      "Iteration 41, loss = 1.82865147\n",
      "Iteration 43, loss = 2.36705723\n",
      "Iteration 42, loss = 1.90230333\n",
      "Iteration 44, loss = 1.89078598\n",
      "Iteration 43, loss = 1.90216127\n",
      "Iteration 45, loss = 2.24452131\n",
      "Iteration 44, loss = 1.72523466\n",
      "Iteration 46, loss = 1.63677035\n",
      "Iteration 45, loss = 1.43079736\n",
      "Iteration 47, loss = 1.66957602\n",
      "Iteration 46, loss = 1.41416358\n",
      "Iteration 48, loss = 1.81888212\n",
      "Iteration 47, loss = 1.94634815\n",
      "Iteration 49, loss = 1.89668393\n",
      "Iteration 48, loss = 2.09394241\n",
      "Iteration 49, loss = 2.01142232\n",
      "Iteration 50, loss = 1.82859109\n",
      "Iteration 50, loss = 2.82879463\n",
      "Iteration 51, loss = 2.15217140\n",
      "Iteration 52, loss = 1.71246097\n",
      "Iteration 51, loss = 2.09665677\n",
      "Iteration 53, loss = 2.23000959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 2.15810779\n",
      "Iteration 53, loss = 1.56238927\n",
      "Iteration 54, loss = 1.33391535\n",
      "Iteration 55, loss = 1.52450283\n",
      "Iteration 56, loss = 2.41313048\n",
      "Iteration 57, loss = 2.02200207\n",
      "Iteration 58, loss = 1.75020115\n",
      "Iteration 59, loss = 2.29165034\n",
      "Iteration 60, loss = 2.61696859\n",
      "Iteration 61, loss = 1.98229500\n",
      "Iteration 1, loss = 20.02621869\n",
      "Iteration 62, loss = 1.78575637\n",
      "Iteration 2, loss = 16.14945546\n",
      "Iteration 63, loss = 2.02373040\n",
      "Iteration 64, loss = 1.29941453\n",
      "Iteration 3, loss = 7.16493826\n",
      "Iteration 65, loss = 1.76630167\n",
      "Iteration 4, loss = 7.45269724\n",
      "Iteration 5, loss = 10.10485496\n",
      "Iteration 66, loss = 1.21257763\n",
      "Iteration 6, loss = 7.55164098\n",
      "Iteration 67, loss = 1.37432407\n",
      "Iteration 68, loss = 1.33882822\n",
      "Iteration 69, loss = 1.64479217\n",
      "Iteration 7, loss = 6.20993103\n",
      "Iteration 70, loss = 1.07367852\n",
      "Iteration 71, loss = 1.22722525\n",
      "Iteration 8, loss = 5.51674831\n",
      "Iteration 72, loss = 0.87180950\n",
      "Iteration 9, loss = 5.30791900\n",
      "Iteration 10, loss = 5.33540002\n",
      "Iteration 73, loss = 1.11996763\n",
      "Iteration 11, loss = 5.99713174\n",
      "Iteration 74, loss = 1.33628805\n",
      "Iteration 75, loss = 1.16075693\n",
      "Iteration 12, loss = 4.89326983\n",
      "Iteration 76, loss = 1.17293941\n",
      "Iteration 13, loss = 4.00515239\n",
      "Iteration 14, loss = 4.29927661\n",
      "Iteration 77, loss = 1.43261733\n",
      "Iteration 15, loss = 4.39895834\n",
      "Iteration 16, loss = 3.02741637\n",
      "Iteration 78, loss = 1.92262217\n",
      "Iteration 79, loss = 2.47289775\n",
      "Iteration 80, loss = 2.43363225\n",
      "Iteration 17, loss = 3.58959145\n",
      "Iteration 18, loss = 3.28935321\n",
      "Iteration 19, loss = 4.03752999\n",
      "Iteration 81, loss = 2.02177980\n",
      "Iteration 20, loss = 3.08984877\n",
      "Iteration 82, loss = 2.09114652\n",
      "Iteration 83, loss = 2.25592236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 2.70928308\n",
      "Iteration 22, loss = 2.55479885\n",
      "Iteration 23, loss = 2.71800654\n",
      "Iteration 24, loss = 2.66802913\n",
      "Iteration 25, loss = 3.23757342\n",
      "Iteration 26, loss = 3.09116683\n",
      "Iteration 27, loss = 2.20661034\n",
      "Iteration 28, loss = 2.90156442\n",
      "Iteration 29, loss = 3.02372272\n",
      "Iteration 30, loss = 2.42276767\n",
      "Iteration 1, loss = 19.21945105\n",
      "Iteration 31, loss = 2.21917992\n",
      "Iteration 2, loss = 19.85947739\n",
      "Iteration 32, loss = 2.51859271\n",
      "Iteration 3, loss = 13.28397703\n",
      "Iteration 33, loss = 2.78172250\n",
      "Iteration 34, loss = 3.06695034\n",
      "Iteration 4, loss = 9.96145950\n",
      "Iteration 5, loss = 8.95033422\n",
      "Iteration 35, loss = 2.94540784\n",
      "Iteration 6, loss = 8.71918418\n",
      "Iteration 7, loss = 7.52307526\n",
      "Iteration 36, loss = 2.93134665\n",
      "Iteration 37, loss = 2.34752376\n",
      "Iteration 8, loss = 8.86703089\n",
      "Iteration 38, loss = 1.93105335\n",
      "Iteration 9, loss = 8.47041571\n",
      "Iteration 39, loss = 2.06653304\n",
      "Iteration 10, loss = 9.02578259\n",
      "Iteration 11, loss = 6.06405113\n",
      "Iteration 40, loss = 1.90507064\n",
      "Iteration 41, loss = 2.48577969\n",
      "Iteration 12, loss = 4.94128031\n",
      "Iteration 42, loss = 2.55163124\n",
      "Iteration 43, loss = 2.06632120\n",
      "Iteration 13, loss = 5.22378556\n",
      "Iteration 14, loss = 4.73046967\n",
      "Iteration 15, loss = 4.78281817\n",
      "Iteration 44, loss = 1.95909793\n",
      "Iteration 45, loss = 1.46791901\n",
      "Iteration 16, loss = 5.64327502\n",
      "Iteration 46, loss = 1.74604740\n",
      "Iteration 47, loss = 1.92534812\n",
      "Iteration 48, loss = 2.13363423\n",
      "Iteration 17, loss = 4.53458079\n",
      "Iteration 18, loss = 3.92960738\n",
      "Iteration 19, loss = 5.87696124\n",
      "Iteration 49, loss = 1.90131482\n",
      "Iteration 20, loss = 5.09051865\n",
      "Iteration 50, loss = 2.06720306\n",
      "Iteration 21, loss = 5.26284198Iteration 51, loss = 1.96377106\n",
      "\n",
      "Iteration 22, loss = 4.35779866\n",
      "Iteration 52, loss = 2.20526700\n",
      "Iteration 23, loss = 4.56700011\n",
      "Iteration 53, loss = 1.51084219\n",
      "Iteration 24, loss = 3.46907701\n",
      "Iteration 54, loss = 2.34641169\n",
      "Iteration 25, loss = 3.49134551\n",
      "Iteration 55, loss = 2.51636371\n",
      "Iteration 26, loss = 3.94972465\n",
      "Iteration 56, loss = 2.01658182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 3.34778011\n",
      "Iteration 28, loss = 3.73974505\n",
      "Iteration 29, loss = 2.53977663\n",
      "Iteration 30, loss = 2.71803666\n",
      "Iteration 31, loss = 3.54894370\n",
      "Iteration 32, loss = 4.16679832\n",
      "Iteration 33, loss = 3.73130960\n",
      "Iteration 34, loss = 3.32014525\n",
      "Iteration 35, loss = 3.03674443\n",
      "Iteration 1, loss = 17.70939185\n",
      "Iteration 2, loss = 15.41700776\n",
      "Iteration 36, loss = 2.90845682\n",
      "Iteration 37, loss = 2.85211364\n",
      "Iteration 3, loss = 10.43399011\n",
      "Iteration 38, loss = 2.41654352\n",
      "Iteration 4, loss = 8.23814857\n",
      "Iteration 5, loss = 7.69991410\n",
      "Iteration 39, loss = 3.02024285\n",
      "Iteration 6, loss = 5.14786401\n",
      "Iteration 40, loss = 2.90549089\n",
      "Iteration 7, loss = 6.22782724\n",
      "Iteration 41, loss = 2.40109967\n",
      "Iteration 42, loss = 2.23909354\n",
      "Iteration 43, loss = 3.33769856\n",
      "Iteration 8, loss = 6.43058681\n",
      "Iteration 9, loss = 6.43597407\n",
      "Iteration 10, loss = 4.08739815\n",
      "Iteration 44, loss = 4.17711868\n",
      "Iteration 45, loss = 4.71793775\n",
      "Iteration 46, loss = 4.02246603\n",
      "Iteration 11, loss = 3.41605326\n",
      "Iteration 47, loss = 2.66619855\n",
      "Iteration 48, loss = 2.33775637\n",
      "Iteration 12, loss = 3.62518411\n",
      "Iteration 49, loss = 3.47380772\n",
      "Iteration 50, loss = 2.55672301\n",
      "Iteration 51, loss = 2.72583081\n",
      "Iteration 13, loss = 3.13651513\n",
      "Iteration 52, loss = 2.82700236\n",
      "Iteration 53, loss = 3.01836225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 2.49340127\n",
      "Iteration 15, loss = 3.17174368\n",
      "Iteration 16, loss = 4.19913115\n",
      "Iteration 17, loss = 2.73179876\n",
      "Iteration 18, loss = 2.34448306\n",
      "Iteration 19, loss = 2.73699918\n",
      "Iteration 20, loss = 4.61016708\n",
      "Iteration 21, loss = 3.85371274\n",
      "Iteration 22, loss = 3.54703807\n",
      "Iteration 23, loss = 3.12679295\n",
      "Iteration 24, loss = 2.55167338\n",
      "Iteration 1, loss = 15.86144512\n",
      "Iteration 2, loss = 11.88055699\n",
      "Iteration 25, loss = 2.18018450\n",
      "Iteration 26, loss = 1.88579204\n",
      "Iteration 3, loss = 8.65948659\n",
      "Iteration 27, loss = 1.42699866\n",
      "Iteration 28, loss = 1.82305188\n",
      "Iteration 4, loss = 6.51444946\n",
      "Iteration 5, loss = 5.23302114\n",
      "Iteration 6, loss = 6.27322903\n",
      "Iteration 29, loss = 1.60025993\n",
      "Iteration 7, loss = 6.42654041\n",
      "Iteration 8, loss = 12.25573336\n",
      "Iteration 9, loss = 9.34550460\n",
      "Iteration 30, loss = 1.54324484\n",
      "Iteration 10, loss = 6.33091451\n",
      "Iteration 11, loss = 6.49359589\n",
      "Iteration 31, loss = 1.62813135\n",
      "Iteration 12, loss = 5.67622568\n",
      "Iteration 32, loss = 1.16769372\n",
      "Iteration 13, loss = 5.69003122\n",
      "Iteration 33, loss = 1.54012697\n",
      "Iteration 14, loss = 7.44100012\n",
      "Iteration 34, loss = 0.89896224\n",
      "Iteration 15, loss = 5.56701062\n",
      "Iteration 35, loss = 1.49542334\n",
      "Iteration 16, loss = 4.64815332\n",
      "Iteration 36, loss = 1.30521298\n",
      "Iteration 17, loss = 3.04425176\n",
      "Iteration 37, loss = 1.03001131\n",
      "Iteration 38, loss = 1.00973809\n",
      "Iteration 18, loss = 2.95539912\n",
      "Iteration 39, loss = 1.03192638\n",
      "Iteration 19, loss = 2.94811476\n",
      "Iteration 40, loss = 0.74922978\n",
      "Iteration 41, loss = 1.27235745\n",
      "Iteration 20, loss = 3.88595077\n",
      "Iteration 42, loss = 1.10523714\n",
      "Iteration 21, loss = 2.88738735\n",
      "Iteration 43, loss = 1.32348162\n",
      "Iteration 22, loss = 2.90986495\n",
      "Iteration 44, loss = 1.50387211\n",
      "Iteration 23, loss = 2.60735019\n",
      "Iteration 45, loss = 0.98760405\n",
      "Iteration 24, loss = 2.68667292\n",
      "Iteration 46, loss = 1.25816029\n",
      "Iteration 47, loss = 1.11476788\n",
      "Iteration 25, loss = 2.44848345\n",
      "Iteration 48, loss = 0.66658980\n",
      "Iteration 49, loss = 1.05255299\n",
      "Iteration 26, loss = 2.42073310\n",
      "Iteration 50, loss = 1.29041711\n",
      "Iteration 27, loss = 2.61837225\n",
      "Iteration 28, loss = 1.90206011\n",
      "Iteration 51, loss = 0.92253329\n",
      "Iteration 52, loss = 1.78196814\n",
      "Iteration 53, loss = 1.38431189\n",
      "Iteration 54, loss = 1.35238037\n",
      "Iteration 29, loss = 2.06419552\n",
      "Iteration 30, loss = 2.63568050\n",
      "Iteration 31, loss = 2.89109911\n",
      "Iteration 55, loss = 1.74435829\n",
      "Iteration 32, loss = 2.12148386\n",
      "Iteration 56, loss = 1.13260069\n",
      "Iteration 57, loss = 1.17185126\n",
      "Iteration 33, loss = 2.45291616\n",
      "Iteration 58, loss = 2.34939525\n",
      "Iteration 34, loss = 2.37167747\n",
      "Iteration 35, loss = 2.73031148\n",
      "Iteration 59, loss = 1.94005753\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 2.52645535\n",
      "Iteration 37, loss = 1.93255931\n",
      "Iteration 38, loss = 1.84673532\n",
      "Iteration 39, loss = 2.38985907\n",
      "Iteration 40, loss = 2.30776000\n",
      "Iteration 41, loss = 3.07010204\n",
      "Iteration 42, loss = 2.90417455\n",
      "Iteration 43, loss = 2.96558755\n",
      "Iteration 44, loss = 1.81440826\n",
      "Iteration 45, loss = 1.70978059\n",
      "Iteration 46, loss = 1.75954230\n",
      "Iteration 47, loss = 2.16992745\n",
      "Iteration 48, loss = 2.73223443\n",
      "Iteration 49, loss = 3.16091518\n",
      "Iteration 50, loss = 2.02168214\n",
      "Iteration 1, loss = 18.44884889\n",
      "Iteration 51, loss = 1.91600444\n",
      "Iteration 2, loss = 14.68965157\n",
      "Iteration 3, loss = 9.10198578\n",
      "Iteration 4, loss = 10.91124439\n",
      "Iteration 52, loss = 2.12043135\n",
      "Iteration 5, loss = 7.52831052\n",
      "Iteration 6, loss = 6.89389149\n",
      "Iteration 53, loss = 2.46196269\n",
      "Iteration 7, loss = 5.56951572\n",
      "Iteration 54, loss = 2.22914578\n",
      "Iteration 8, loss = 5.65615372\n",
      "Iteration 55, loss = 2.64303254\n",
      "Iteration 9, loss = 5.42737256\n",
      "Iteration 56, loss = 2.36049755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 6.25444397\n",
      "Iteration 11, loss = 7.71783166\n",
      "Iteration 12, loss = 4.86928396\n",
      "Iteration 13, loss = 4.82388218\n",
      "Iteration 14, loss = 3.41926505\n",
      "Iteration 15, loss = 3.46287245\n",
      "Iteration 16, loss = 3.81484403\n",
      "Iteration 17, loss = 4.19513285\n",
      "Iteration 18, loss = 3.33202136\n",
      "Iteration 19, loss = 3.31097872\n",
      "Iteration 20, loss = 3.15466334\n",
      "Iteration 1, loss = 19.87888318\n",
      "Iteration 2, loss = 17.07341150\n",
      "Iteration 21, loss = 3.15719449\n",
      "Iteration 3, loss = 10.45127830\n",
      "Iteration 4, loss = 8.19221225\n",
      "Iteration 22, loss = 2.75958815\n",
      "Iteration 5, loss = 7.91684454\n",
      "Iteration 23, loss = 2.03332654\n",
      "Iteration 6, loss = 7.81805776\n",
      "Iteration 24, loss = 3.91897272\n",
      "Iteration 25, loss = 2.97862578\n",
      "Iteration 7, loss = 8.10023986\n",
      "Iteration 8, loss = 7.20452452\n",
      "Iteration 26, loss = 2.02410174\n",
      "Iteration 9, loss = 6.23942972\n",
      "Iteration 27, loss = 2.03226006\n",
      "Iteration 10, loss = 5.47264628\n",
      "Iteration 11, loss = 7.16182086\n",
      "Iteration 28, loss = 2.16027214\n",
      "Iteration 29, loss = 2.14349656\n",
      "Iteration 12, loss = 6.97854599\n",
      "Iteration 13, loss = 4.59465674\n",
      "Iteration 30, loss = 3.35094944\n",
      "Iteration 31, loss = 3.78184522\n",
      "Iteration 14, loss = 3.92417592\n",
      "Iteration 32, loss = 2.66810821\n",
      "Iteration 15, loss = 3.88212723\n",
      "Iteration 33, loss = 2.50552258\n",
      "Iteration 16, loss = 5.88390227\n",
      "Iteration 34, loss = 2.48413694\n",
      "Iteration 17, loss = 5.83719357\n",
      "Iteration 35, loss = 2.08729291\n",
      "Iteration 18, loss = 10.15964526\n",
      "Iteration 36, loss = 1.89369869\n",
      "Iteration 19, loss = 9.52442204\n",
      "Iteration 37, loss = 3.62256649\n",
      "Iteration 38, loss = 2.62688904\n",
      "Iteration 20, loss = 9.39398733\n",
      "Iteration 21, loss = 8.66268428\n",
      "Iteration 39, loss = 2.73250685\n",
      "Iteration 22, loss = 5.17188336\n",
      "Iteration 40, loss = 3.07326458\n",
      "Iteration 23, loss = 4.78862673\n",
      "Iteration 41, loss = 2.62752704\n",
      "Iteration 42, loss = 2.51081428\n",
      "Iteration 43, loss = 1.85501031\n",
      "Iteration 24, loss = 5.14840138\n",
      "Iteration 44, loss = 2.00428699\n",
      "Iteration 25, loss = 4.78687538\n",
      "Iteration 45, loss = 2.18474214\n",
      "Iteration 26, loss = 5.11105895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 1.68029702\n",
      "Iteration 47, loss = 1.72498904\n",
      "Iteration 48, loss = 2.12564083\n",
      "Iteration 49, loss = 2.09483700\n",
      "Iteration 50, loss = 1.51368713\n",
      "Iteration 51, loss = 2.78001268\n",
      "Iteration 52, loss = 1.90780977\n",
      "Iteration 53, loss = 1.57856435\n",
      "Iteration 54, loss = 1.88125109\n",
      "Iteration 55, loss = 1.72069746\n",
      "Iteration 56, loss = 1.40760215\n",
      "Iteration 1, loss = 17.75833911\n",
      "Iteration 57, loss = 1.09314710\n",
      "Iteration 2, loss = 16.03281660\n",
      "Iteration 58, loss = 1.43281472\n",
      "Iteration 3, loss = 13.28743884\n",
      "Iteration 59, loss = 1.17844514\n",
      "Iteration 60, loss = 1.67691826\n",
      "Iteration 4, loss = 7.80857144\n",
      "Iteration 61, loss = 1.72169728\n",
      "Iteration 5, loss = 9.70230206\n",
      "Iteration 6, loss = 9.79129723\n",
      "Iteration 7, loss = 7.36885062\n",
      "Iteration 62, loss = 2.58845002\n",
      "Iteration 63, loss = 2.44870726\n",
      "Iteration 8, loss = 7.46464565\n",
      "Iteration 64, loss = 2.35573607\n",
      "Iteration 65, loss = 1.77671425\n",
      "Iteration 9, loss = 6.45070998\n",
      "Iteration 10, loss = 6.96161320\n",
      "Iteration 11, loss = 5.44737644\n",
      "Iteration 66, loss = 1.41889811\n",
      "Iteration 12, loss = 4.80092964\n",
      "Iteration 67, loss = 1.95180901\n",
      "Iteration 13, loss = 5.93668916\n",
      "Iteration 68, loss = 2.82482338\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 4.83149165\n",
      "Iteration 15, loss = 5.70223051\n",
      "Iteration 16, loss = 5.64373422\n",
      "Iteration 17, loss = 6.64348490\n",
      "Iteration 18, loss = 7.93490048\n",
      "Iteration 19, loss = 4.77736477\n",
      "Iteration 20, loss = 3.54861456\n",
      "Iteration 21, loss = 3.88872010\n",
      "Iteration 1, loss = 18.72338201\n",
      "Iteration 22, loss = 6.00784446\n",
      "Iteration 23, loss = 4.03570689\n",
      "Iteration 2, loss = 13.74363392\n",
      "Iteration 24, loss = 3.93459166\n",
      "Iteration 25, loss = 3.73799209\n",
      "Iteration 3, loss = 11.36004014\n",
      "Iteration 26, loss = 4.25311116\n",
      "Iteration 27, loss = 3.60391106\n",
      "Iteration 28, loss = 3.57984609\n",
      "Iteration 29, loss = 2.97337484\n",
      "Iteration 4, loss = 10.37782730\n",
      "Iteration 30, loss = 2.73570765\n",
      "Iteration 31, loss = 2.51432788\n",
      "Iteration 5, loss = 7.47365384\n",
      "Iteration 32, loss = 2.95153260\n",
      "Iteration 6, loss = 8.87728957\n",
      "Iteration 33, loss = 3.66520128\n",
      "Iteration 7, loss = 7.58464575\n",
      "Iteration 34, loss = 2.67764467\n",
      "Iteration 35, loss = 2.50072882\n",
      "Iteration 36, loss = 2.20136805\n",
      "Iteration 8, loss = 5.30793089\n",
      "Iteration 37, loss = 2.70948204\n",
      "Iteration 38, loss = 1.95760253\n",
      "Iteration 39, loss = 2.42205176\n",
      "Iteration 9, loss = 6.22513073\n",
      "Iteration 10, loss = 6.21912453\n",
      "Iteration 40, loss = 2.24974608\n",
      "Iteration 11, loss = 4.74560248\n",
      "Iteration 12, loss = 3.98392940\n",
      "Iteration 41, loss = 2.58423198\n",
      "Iteration 13, loss = 3.68529998\n",
      "Iteration 42, loss = 3.91713439\n",
      "Iteration 14, loss = 3.58965653\n",
      "Iteration 43, loss = 2.50060628\n",
      "Iteration 15, loss = 3.76376466\n",
      "Iteration 44, loss = 3.02308224\n",
      "Iteration 45, loss = 2.62072981\n",
      "Iteration 16, loss = 5.78667827\n",
      "Iteration 46, loss = 1.89193016\n",
      "Iteration 17, loss = 3.96403128\n",
      "Iteration 47, loss = 2.09578015\n",
      "Iteration 18, loss = 3.36473892\n",
      "Iteration 19, loss = 2.38592674\n",
      "Iteration 48, loss = 2.62554082\n",
      "Iteration 20, loss = 2.19415951\n",
      "Iteration 49, loss = 2.93834007\n",
      "Iteration 21, loss = 2.39795118\n",
      "Iteration 22, loss = 2.12662641\n",
      "Iteration 23, loss = 3.47487327\n",
      "Iteration 50, loss = 3.39635749\n",
      "Iteration 51, loss = 2.06434392\n",
      "Iteration 52, loss = 2.38853086\n",
      "Iteration 53, loss = 1.96973534\n",
      "Iteration 24, loss = 2.51482169\n",
      "Iteration 25, loss = 2.67236974\n",
      "Iteration 26, loss = 2.62595179\n",
      "Iteration 54, loss = 1.85359322\n",
      "Iteration 27, loss = 3.01559968\n",
      "Iteration 55, loss = 2.52725465\n",
      "Iteration 56, loss = 1.97505867\n",
      "Iteration 28, loss = 2.41816775\n",
      "Iteration 57, loss = 2.31255512\n",
      "Iteration 29, loss = 2.47528984\n",
      "Iteration 58, loss = 2.14859966\n",
      "Iteration 30, loss = 2.74368974\n",
      "Iteration 31, loss = 2.54154995\n",
      "Iteration 59, loss = 2.62118849\n",
      "Iteration 60, loss = 2.59054492\n",
      "Iteration 61, loss = 3.07768448\n",
      "Iteration 32, loss = 2.45996719\n",
      "Iteration 62, loss = 1.93115115\n",
      "Iteration 33, loss = 1.77058998\n",
      "Iteration 34, loss = 3.52756089\n",
      "Iteration 35, loss = 2.72727282\n",
      "Iteration 63, loss = 1.47313296\n",
      "Iteration 64, loss = 1.39620500\n",
      "Iteration 36, loss = 4.37069340\n",
      "Iteration 65, loss = 1.74455587\n",
      "Iteration 66, loss = 1.87184300\n",
      "Iteration 37, loss = 2.55330380\n",
      "Iteration 38, loss = 2.35358093\n",
      "Iteration 67, loss = 1.84636660\n",
      "Iteration 39, loss = 2.08989986\n",
      "Iteration 68, loss = 2.04380627\n",
      "Iteration 40, loss = 2.19869515\n",
      "Iteration 69, loss = 3.54447525\n",
      "Iteration 70, loss = 2.09817534\n",
      "Iteration 71, loss = 3.02355686\n",
      "Iteration 41, loss = 2.36288335\n",
      "Iteration 72, loss = 1.69896578\n",
      "Iteration 73, loss = 2.85620794\n",
      "Iteration 74, loss = 3.78754304\n",
      "Iteration 42, loss = 2.54302285\n",
      "Iteration 75, loss = 4.31094974\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 2.01939299\n",
      "Iteration 44, loss = 1.93326588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.42450705\n",
      "Iteration 2, loss = 14.62578750\n",
      "Iteration 3, loss = 14.01154592\n",
      "Iteration 4, loss = 13.04625901\n",
      "Iteration 5, loss = 7.00888558\n",
      "Iteration 6, loss = 7.21693797\n",
      "Iteration 7, loss = 7.56197032\n",
      "Iteration 8, loss = 6.25591351\n",
      "Iteration 1, loss = 14.86889258\n",
      "Iteration 2, loss = 14.13625788\n",
      "Iteration 9, loss = 8.44878402\n",
      "Iteration 3, loss = 11.67831268\n",
      "Iteration 10, loss = 10.03624130\n",
      "Iteration 11, loss = 6.54358208\n",
      "Iteration 4, loss = 14.12222615\n",
      "Iteration 12, loss = 5.64054426\n",
      "Iteration 13, loss = 6.91012760\n",
      "Iteration 5, loss = 13.50742748\n",
      "Iteration 6, loss = 10.65797728\n",
      "Iteration 7, loss = 8.29937077\n",
      "Iteration 14, loss = 6.16183614\n",
      "Iteration 8, loss = 8.98578394\n",
      "Iteration 15, loss = 6.34469269\n",
      "Iteration 16, loss = 4.37597913\n",
      "Iteration 17, loss = 4.43587849\n",
      "Iteration 9, loss = 7.94714644\n",
      "Iteration 10, loss = 7.52377026\n",
      "Iteration 18, loss = 4.29024629\n",
      "Iteration 11, loss = 6.63121819\n",
      "Iteration 12, loss = 5.17448979\n",
      "Iteration 19, loss = 4.22670720\n",
      "Iteration 13, loss = 6.08350545\n",
      "Iteration 20, loss = 3.74452252\n",
      "Iteration 21, loss = 4.22437925\n",
      "Iteration 14, loss = 6.78284295\n",
      "Iteration 22, loss = 4.17252198\n",
      "Iteration 15, loss = 5.25263794\n",
      "Iteration 16, loss = 4.83196710\n",
      "Iteration 23, loss = 4.00158895\n",
      "Iteration 17, loss = 4.93106250\n",
      "Iteration 24, loss = 2.62851383\n",
      "Iteration 25, loss = 2.53370838\n",
      "Iteration 18, loss = 4.20741987\n",
      "Iteration 26, loss = 2.83519799\n",
      "Iteration 19, loss = 3.59281705\n",
      "Iteration 20, loss = 4.01508709\n",
      "Iteration 27, loss = 2.60979313\n",
      "Iteration 21, loss = 4.09567203\n",
      "Iteration 28, loss = 3.64933435\n",
      "Iteration 29, loss = 2.18490535\n",
      "Iteration 22, loss = 4.35531770\n",
      "Iteration 30, loss = 2.92910006\n",
      "Iteration 23, loss = 4.02522217\n",
      "Iteration 24, loss = 2.80379392\n",
      "Iteration 25, loss = 2.91130620\n",
      "Iteration 31, loss = 2.32149955\n",
      "Iteration 32, loss = 2.46488689\n",
      "Iteration 26, loss = 2.64356012\n",
      "Iteration 33, loss = 3.49933002\n",
      "Iteration 27, loss = 2.48136965\n",
      "Iteration 34, loss = 3.24917543\n",
      "Iteration 28, loss = 2.27229264\n",
      "Iteration 29, loss = 3.38317002\n",
      "Iteration 35, loss = 2.80556575\n",
      "Iteration 30, loss = 3.68888158\n",
      "Iteration 31, loss = 3.41587021\n",
      "Iteration 36, loss = 2.29625259\n",
      "Iteration 37, loss = 2.01905765\n",
      "Iteration 38, loss = 2.76518026\n",
      "Iteration 32, loss = 2.83272706\n",
      "Iteration 39, loss = 2.39310962\n",
      "Iteration 40, loss = 2.60754247\n",
      "Iteration 41, loss = 3.06840655\n",
      "Iteration 33, loss = 3.30228915\n",
      "Iteration 42, loss = 3.63944105\n",
      "Iteration 43, loss = 1.66082965\n",
      "Iteration 44, loss = 1.83708758\n",
      "Iteration 34, loss = 2.94679540\n",
      "Iteration 35, loss = 2.24032719\n",
      "Iteration 36, loss = 2.96527310\n",
      "Iteration 37, loss = 2.62957039\n",
      "Iteration 45, loss = 2.36239151\n",
      "Iteration 46, loss = 1.94019031\n",
      "Iteration 38, loss = 2.48792692\n",
      "Iteration 47, loss = 2.03322409\n",
      "Iteration 39, loss = 2.27725173\n",
      "Iteration 48, loss = 2.04907681\n",
      "Iteration 40, loss = 2.21921835\n",
      "Iteration 41, loss = 2.22966768\n",
      "Iteration 42, loss = 2.02777486\n",
      "Iteration 49, loss = 1.89390098\n",
      "Iteration 50, loss = 1.82215580\n",
      "Iteration 51, loss = 2.60123880\n",
      "Iteration 43, loss = 1.94050844\n",
      "Iteration 44, loss = 2.60752315\n",
      "Iteration 52, loss = 2.54289232\n",
      "Iteration 45, loss = 2.30293443\n",
      "Iteration 46, loss = 1.90832388\n",
      "Iteration 53, loss = 2.37990069\n",
      "Iteration 47, loss = 2.56384411\n",
      "Iteration 54, loss = 2.12207922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 2.68192108\n",
      "Iteration 49, loss = 2.81311073\n",
      "Iteration 50, loss = 2.09762261\n",
      "Iteration 51, loss = 2.38176191\n",
      "Iteration 52, loss = 2.15009523\n",
      "Iteration 1, loss = 18.50793244\n",
      "Iteration 53, loss = 2.23225688\n",
      "Iteration 54, loss = 1.74747556\n",
      "Iteration 55, loss = 2.09336809\n",
      "Iteration 2, loss = 12.41107574\n",
      "Iteration 3, loss = 11.99228277\n",
      "Iteration 4, loss = 10.57047334\n",
      "Iteration 56, loss = 1.35623378\n",
      "Iteration 5, loss = 11.41778439\n",
      "Iteration 6, loss = 8.33443439\n",
      "Iteration 7, loss = 5.94973196\n",
      "Iteration 57, loss = 1.70515052\n",
      "Iteration 8, loss = 6.15636949\n",
      "Iteration 58, loss = 1.74919007\n",
      "Iteration 59, loss = 1.61223466\n",
      "Iteration 9, loss = 6.95694300\n",
      "Iteration 60, loss = 1.66199181\n",
      "Iteration 61, loss = 1.64283699\n",
      "Iteration 62, loss = 1.85439770\n",
      "Iteration 10, loss = 5.40906443\n",
      "Iteration 63, loss = 2.10513274\n",
      "Iteration 64, loss = 1.74145367\n",
      "Iteration 65, loss = 1.27695024\n",
      "Iteration 11, loss = 4.78786432\n",
      "Iteration 12, loss = 5.14542432\n",
      "Iteration 13, loss = 6.05507348\n",
      "Iteration 66, loss = 1.50161425\n",
      "Iteration 14, loss = 5.33268136\n",
      "Iteration 15, loss = 3.53741505\n",
      "Iteration 16, loss = 3.64956610\n",
      "Iteration 67, loss = 1.43436220\n",
      "Iteration 17, loss = 2.97604625\n",
      "Iteration 68, loss = 1.59294286\n",
      "Iteration 18, loss = 2.90014228\n",
      "Iteration 69, loss = 1.77417178\n",
      "Iteration 19, loss = 3.72301018\n",
      "Iteration 70, loss = 1.55501021\n",
      "Iteration 20, loss = 2.44028666\n",
      "Iteration 71, loss = 1.82704610\n",
      "Iteration 21, loss = 3.57547334\n",
      "Iteration 72, loss = 1.86264332\n",
      "Iteration 22, loss = 5.03011329\n",
      "Iteration 73, loss = 1.42172430\n",
      "Iteration 74, loss = 1.39036571\n",
      "Iteration 23, loss = 4.04675463\n",
      "Iteration 75, loss = 1.47512061\n",
      "Iteration 24, loss = 3.04767617\n",
      "Iteration 76, loss = 1.16285471\n",
      "Iteration 25, loss = 2.29370776\n",
      "Iteration 77, loss = 1.31213372\n",
      "Iteration 26, loss = 2.44738205\n",
      "Iteration 78, loss = 0.85122575\n",
      "Iteration 27, loss = 2.23126765\n",
      "Iteration 28, loss = 1.85794692\n",
      "Iteration 79, loss = 1.24334682\n",
      "Iteration 29, loss = 2.58459248\n",
      "Iteration 80, loss = 1.45925333\n",
      "Iteration 81, loss = 1.43195752\n",
      "Iteration 30, loss = 2.24338773\n",
      "Iteration 82, loss = 1.02991153\n",
      "Iteration 31, loss = 2.17381896\n",
      "Iteration 32, loss = 1.90248623\n",
      "Iteration 33, loss = 1.88139728\n",
      "Iteration 83, loss = 0.92550728\n",
      "Iteration 34, loss = 1.74634189\n",
      "Iteration 84, loss = 1.30329749\n",
      "Iteration 35, loss = 2.18061708\n",
      "Iteration 85, loss = 1.50982666\n",
      "Iteration 36, loss = 2.99384804\n",
      "Iteration 86, loss = 1.49637800\n",
      "Iteration 37, loss = 2.38940489\n",
      "Iteration 38, loss = 2.33803638\n",
      "Iteration 87, loss = 1.79736790\n",
      "Iteration 39, loss = 2.39086892\n",
      "Iteration 40, loss = 2.16470335\n",
      "Iteration 88, loss = 1.97626457\n",
      "Iteration 89, loss = 2.06218092\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 2.06331915\n",
      "Iteration 42, loss = 1.61425303\n",
      "Iteration 43, loss = 1.83602635\n",
      "Iteration 44, loss = 1.82183713\n",
      "Iteration 45, loss = 1.30632575\n",
      "Iteration 46, loss = 2.18940848\n",
      "Iteration 47, loss = 1.75878853\n",
      "Iteration 48, loss = 1.96335601\n",
      "Iteration 1, loss = 17.89376817\n",
      "Iteration 49, loss = 2.01543472\n",
      "Iteration 2, loss = 15.66941596\n",
      "Iteration 3, loss = 11.68200214\n",
      "Iteration 50, loss = 2.40715661\n",
      "Iteration 4, loss = 12.98979502\n",
      "Iteration 5, loss = 8.60990547\n",
      "Iteration 6, loss = 6.97485828\n",
      "Iteration 51, loss = 2.51505534\n",
      "Iteration 7, loss = 7.84918729\n",
      "Iteration 8, loss = 5.19225961\n",
      "Iteration 52, loss = 2.00725766\n",
      "Iteration 9, loss = 5.95482385\n",
      "Iteration 10, loss = 9.48143248\n",
      "Iteration 11, loss = 5.02899458\n",
      "Iteration 53, loss = 1.99578369\n",
      "Iteration 12, loss = 5.09880028\n",
      "Iteration 13, loss = 4.73886697\n",
      "Iteration 14, loss = 4.20611772\n",
      "Iteration 54, loss = 1.11349839\n",
      "Iteration 15, loss = 3.90806984\n",
      "Iteration 55, loss = 1.56419667\n",
      "Iteration 56, loss = 1.06185874\n",
      "Iteration 16, loss = 4.05085199\n",
      "Iteration 57, loss = 0.91238654\n",
      "Iteration 17, loss = 4.53787849\n",
      "Iteration 18, loss = 4.53653034\n",
      "Iteration 19, loss = 4.59450440\n",
      "Iteration 58, loss = 0.96067903\n",
      "Iteration 59, loss = 1.33144257\n",
      "Iteration 20, loss = 6.90305486\n",
      "Iteration 60, loss = 1.03716047\n",
      "Iteration 61, loss = 1.68325167\n",
      "Iteration 21, loss = 4.54236722\n",
      "Iteration 22, loss = 4.19989943\n",
      "Iteration 23, loss = 4.20874287\n",
      "Iteration 62, loss = 1.14741356\n",
      "Iteration 63, loss = 1.26135192\n",
      "Iteration 24, loss = 4.14344462\n",
      "Iteration 64, loss = 1.13848016\n",
      "Iteration 25, loss = 3.68733575\n",
      "Iteration 65, loss = 1.32482139\n",
      "Iteration 26, loss = 3.94138254\n",
      "Iteration 27, loss = 3.98093933\n",
      "Iteration 66, loss = 1.01945539\n",
      "Iteration 67, loss = 1.20497764\n",
      "Iteration 28, loss = 3.35256267\n",
      "Iteration 68, loss = 1.77413770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 2.98655192\n",
      "Iteration 30, loss = 2.29471659\n",
      "Iteration 31, loss = 2.60391536\n",
      "Iteration 32, loss = 2.94576667\n",
      "Iteration 33, loss = 2.57063257\n",
      "Iteration 34, loss = 2.00641571\n",
      "Iteration 35, loss = 2.27300901\n",
      "Iteration 36, loss = 3.28429900\n",
      "Iteration 37, loss = 2.60878974\n",
      "Iteration 38, loss = 1.96575177\n",
      "Iteration 39, loss = 2.52922402\n",
      "Iteration 1, loss = 17.10717485\n",
      "Iteration 2, loss = 14.64685025\n",
      "Iteration 40, loss = 2.37159648\n",
      "Iteration 3, loss = 12.30382958\n",
      "Iteration 41, loss = 2.85675539\n",
      "Iteration 4, loss = 12.00471335\n",
      "Iteration 5, loss = 8.83767033\n",
      "Iteration 42, loss = 2.76021697\n",
      "Iteration 6, loss = 8.06529967\n",
      "Iteration 43, loss = 2.73031944\n",
      "Iteration 7, loss = 8.54713046\n",
      "Iteration 8, loss = 7.13234818\n",
      "Iteration 44, loss = 3.27849912\n",
      "Iteration 9, loss = 5.39606872\n",
      "Iteration 45, loss = 3.20842794\n",
      "Iteration 10, loss = 4.64757096\n",
      "Iteration 46, loss = 2.42558512\n",
      "Iteration 11, loss = 4.66355861\n",
      "Iteration 47, loss = 2.36107511\n",
      "Iteration 12, loss = 4.80419457\n",
      "Iteration 48, loss = 2.61333036\n",
      "Iteration 13, loss = 4.37737756\n",
      "Iteration 49, loss = 2.81388775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 4.18799124\n",
      "Iteration 15, loss = 4.68306883\n",
      "Iteration 16, loss = 3.82081810\n",
      "Iteration 17, loss = 3.13394838\n",
      "Iteration 18, loss = 3.53229172\n",
      "Iteration 19, loss = 3.25425879\n",
      "Iteration 20, loss = 3.05044320\n",
      "Iteration 21, loss = 2.95871644\n",
      "Iteration 22, loss = 2.52813060\n",
      "Iteration 23, loss = 2.96542592\n",
      "Iteration 24, loss = 3.02717084\n",
      "Iteration 25, loss = 3.18456163\n",
      "Iteration 26, loss = 3.04459238\n",
      "Iteration 27, loss = 2.62689498\n",
      "Iteration 28, loss = 2.54296125\n",
      "Iteration 29, loss = 1.84027913\n",
      "Iteration 30, loss = 2.11975075\n",
      "Iteration 1, loss = 16.98564366\n",
      "Iteration 2, loss = 17.15371463\n",
      "Iteration 31, loss = 1.86431180\n",
      "Iteration 32, loss = 1.77712297\n",
      "Iteration 33, loss = 2.79417814\n",
      "Iteration 3, loss = 11.75791103\n",
      "Iteration 34, loss = 2.52438154\n",
      "Iteration 4, loss = 14.40854285\n",
      "Iteration 35, loss = 2.52225305\n",
      "Iteration 5, loss = 9.06872149\n",
      "Iteration 36, loss = 2.12059387\n",
      "Iteration 6, loss = 6.45160127\n",
      "Iteration 7, loss = 7.35175097\n",
      "Iteration 37, loss = 1.69436999\n",
      "Iteration 8, loss = 6.19864947\n",
      "Iteration 9, loss = 5.62008745\n",
      "Iteration 38, loss = 1.41477052\n",
      "Iteration 10, loss = 8.84120971\n",
      "Iteration 39, loss = 1.76955373\n",
      "Iteration 40, loss = 2.36106098\n",
      "Iteration 11, loss = 6.09299487\n",
      "Iteration 41, loss = 1.53398403\n",
      "Iteration 12, loss = 4.44990570\n",
      "Iteration 42, loss = 2.44653146\n",
      "Iteration 13, loss = 3.56133989\n",
      "Iteration 43, loss = 2.57448069\n",
      "Iteration 14, loss = 4.39604876\n",
      "Iteration 44, loss = 2.44360648\n",
      "Iteration 15, loss = 3.84196865\n",
      "Iteration 45, loss = 1.83643173\n",
      "Iteration 16, loss = 4.11517320\n",
      "Iteration 46, loss = 1.93792540\n",
      "Iteration 17, loss = 3.42651053\n",
      "Iteration 47, loss = 2.22834225\n",
      "Iteration 18, loss = 3.03532496\n",
      "Iteration 48, loss = 2.10915447\n",
      "Iteration 19, loss = 2.78591759\n",
      "Iteration 49, loss = 1.95797108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 3.32809971\n",
      "Iteration 21, loss = 3.24430631\n",
      "Iteration 22, loss = 2.87864521\n",
      "Iteration 23, loss = 2.47743840\n",
      "Iteration 24, loss = 2.29352580\n",
      "Iteration 25, loss = 3.35401582\n",
      "Iteration 26, loss = 2.72716359\n",
      "Iteration 27, loss = 2.36329216\n",
      "Iteration 28, loss = 2.05465672\n",
      "Iteration 29, loss = 2.28284186\n",
      "Iteration 1, loss = 18.63831241\n",
      "Iteration 2, loss = 12.68070402\n",
      "Iteration 30, loss = 1.82558917\n",
      "Iteration 3, loss = 10.97071891\n",
      "Iteration 4, loss = 6.59815519\n",
      "Iteration 31, loss = 2.08484878\n",
      "Iteration 5, loss = 6.87901680\n",
      "Iteration 32, loss = 2.14133092\n",
      "Iteration 6, loss = 6.34185369\n",
      "Iteration 33, loss = 2.57195463\n",
      "Iteration 7, loss = 9.82668593\n",
      "Iteration 34, loss = 1.64878985\n",
      "Iteration 8, loss = 9.22335428\n",
      "Iteration 9, loss = 4.55312985\n",
      "Iteration 35, loss = 1.55670216\n",
      "Iteration 10, loss = 4.82106668\n",
      "Iteration 36, loss = 1.84769360\n",
      "Iteration 11, loss = 3.89679998\n",
      "Iteration 37, loss = 1.86518027\n",
      "Iteration 12, loss = 3.93055104\n",
      "Iteration 38, loss = 1.85471219\n",
      "Iteration 13, loss = 4.10703953\n",
      "Iteration 14, loss = 4.28290891\n",
      "Iteration 39, loss = 2.11924176\n",
      "Iteration 15, loss = 5.74891937\n",
      "Iteration 16, loss = 4.13457626\n",
      "Iteration 40, loss = 2.05025337\n",
      "Iteration 17, loss = 4.20165932\n",
      "Iteration 18, loss = 4.22033646\n",
      "Iteration 41, loss = 1.57434212\n",
      "Iteration 19, loss = 3.72356470\n",
      "Iteration 42, loss = 2.12360870\n",
      "Iteration 20, loss = 2.74670295\n",
      "Iteration 43, loss = 1.92677912\n",
      "Iteration 21, loss = 3.44844527\n",
      "Iteration 44, loss = 1.30637516\n",
      "Iteration 22, loss = 2.64851818\n",
      "Iteration 45, loss = 1.42591003\n",
      "Iteration 23, loss = 3.57449396\n",
      "Iteration 46, loss = 1.61634506\n",
      "Iteration 24, loss = 3.28411094\n",
      "Iteration 47, loss = 1.66280107\n",
      "Iteration 25, loss = 2.35565454\n",
      "Iteration 48, loss = 1.54976951\n",
      "Iteration 26, loss = 2.05371041\n",
      "Iteration 49, loss = 1.53843923\n",
      "Iteration 27, loss = 3.02043055\n",
      "Iteration 50, loss = 2.33169655\n",
      "Iteration 28, loss = 4.76484148\n",
      "Iteration 51, loss = 2.69609057\n",
      "Iteration 29, loss = 3.85374043\n",
      "Iteration 52, loss = 2.42853691\n",
      "Iteration 30, loss = 2.75274638\n",
      "Iteration 31, loss = 3.03996735\n",
      "Iteration 32, loss = 2.63239907\n",
      "Iteration 53, loss = 1.47968277\n",
      "Iteration 33, loss = 3.78117728\n",
      "Iteration 34, loss = 4.78741287\n",
      "Iteration 54, loss = 1.77314502\n",
      "Iteration 35, loss = 2.94231667\n",
      "Iteration 36, loss = 2.26608646\n",
      "Iteration 55, loss = 2.60581999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 3.96440026\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.96188347\n",
      "Iteration 2, loss = 15.94046709\n",
      "Iteration 1, loss = 17.19844446\n",
      "Iteration 2, loss = 15.19546137\n",
      "Iteration 3, loss = 13.86097976\n",
      "Iteration 3, loss = 9.59849238\n",
      "Iteration 4, loss = 9.37672000\n",
      "Iteration 5, loss = 8.36798194\n",
      "Iteration 6, loss = 7.47705989\n",
      "Iteration 4, loss = 8.39191250\n",
      "Iteration 5, loss = 12.05100617\n",
      "Iteration 6, loss = 8.53042559\n",
      "Iteration 7, loss = 6.11529460\n",
      "Iteration 8, loss = 6.08587984\n",
      "Iteration 7, loss = 6.67777848\n",
      "Iteration 9, loss = 6.57939081\n",
      "Iteration 10, loss = 6.62674035\n",
      "Iteration 8, loss = 5.16384019\n",
      "Iteration 9, loss = 5.23625950\n",
      "Iteration 10, loss = 7.84578792\n",
      "Iteration 11, loss = 5.68783013\n",
      "Iteration 11, loss = 5.06818457\n",
      "Iteration 12, loss = 4.33818762\n",
      "Iteration 13, loss = 4.42492021\n",
      "Iteration 14, loss = 4.39709000\n",
      "Iteration 15, loss = 4.81960363\n",
      "Iteration 12, loss = 4.16061645\n",
      "Iteration 16, loss = 5.02905651\n",
      "Iteration 17, loss = 3.69776528\n",
      "Iteration 13, loss = 4.36639357\n",
      "Iteration 14, loss = 4.29031501\n",
      "Iteration 15, loss = 3.87586739\n",
      "Iteration 18, loss = 3.54249521\n",
      "Iteration 16, loss = 4.01527198\n",
      "Iteration 17, loss = 3.52627550\n",
      "Iteration 18, loss = 3.24369513\n",
      "Iteration 19, loss = 3.64996216Iteration 19, loss = 3.09562496\n",
      "\n",
      "Iteration 20, loss = 2.64314113\n",
      "Iteration 21, loss = 3.81659982\n",
      "Iteration 22, loss = 2.57027708\n",
      "Iteration 20, loss = 3.54696641\n",
      "Iteration 23, loss = 2.46620772\n",
      "Iteration 24, loss = 2.16254173\n",
      "Iteration 25, loss = 2.56191051\n",
      "Iteration 21, loss = 2.34292083\n",
      "Iteration 26, loss = 2.03986070\n",
      "Iteration 27, loss = 2.11651800\n",
      "Iteration 22, loss = 2.43969522\n",
      "Iteration 28, loss = 1.99638989\n",
      "Iteration 23, loss = 2.82364824\n",
      "Iteration 24, loss = 3.70166599\n",
      "Iteration 29, loss = 1.93641495\n",
      "Iteration 30, loss = 1.74337705\n",
      "Iteration 25, loss = 4.87092163\n",
      "Iteration 31, loss = 2.48103313\n",
      "Iteration 32, loss = 3.19077863\n",
      "Iteration 26, loss = 3.66625428\n",
      "Iteration 27, loss = 2.76538494\n",
      "Iteration 33, loss = 2.78990125\n",
      "Iteration 28, loss = 3.39840495\n",
      "Iteration 34, loss = 2.44021683\n",
      "Iteration 29, loss = 2.50121000\n",
      "Iteration 35, loss = 2.83260142\n",
      "Iteration 30, loss = 2.38093570\n",
      "Iteration 36, loss = 1.78961575\n",
      "Iteration 31, loss = 1.76559584\n",
      "Iteration 37, loss = 2.03643757\n",
      "Iteration 32, loss = 2.27838625\n",
      "Iteration 38, loss = 1.64584525\n",
      "Iteration 39, loss = 1.67319346\n",
      "Iteration 33, loss = 2.40925462\n",
      "Iteration 34, loss = 2.56614835\n",
      "Iteration 40, loss = 1.69125408\n",
      "Iteration 35, loss = 1.98238177\n",
      "Iteration 36, loss = 1.98872621\n",
      "Iteration 37, loss = 2.07186478\n",
      "Iteration 41, loss = 1.17008729\n",
      "Iteration 38, loss = 2.26851667\n",
      "Iteration 42, loss = 1.41662485\n",
      "Iteration 43, loss = 1.45441333\n",
      "Iteration 39, loss = 2.40672770\n",
      "Iteration 44, loss = 1.56949565\n",
      "Iteration 40, loss = 1.80758973\n",
      "Iteration 45, loss = 1.95910721\n",
      "Iteration 41, loss = 1.46359626\n",
      "Iteration 46, loss = 1.79296053\n",
      "Iteration 42, loss = 1.99960152\n",
      "Iteration 47, loss = 1.78427080\n",
      "Iteration 43, loss = 2.14939622\n",
      "Iteration 44, loss = 2.52807194\n",
      "Iteration 48, loss = 1.87808658\n",
      "Iteration 49, loss = 1.60249924\n",
      "Iteration 45, loss = 2.33270415\n",
      "Iteration 50, loss = 1.65048909\n",
      "Iteration 51, loss = 2.06777536\n",
      "Iteration 46, loss = 1.62949933\n",
      "Iteration 47, loss = 1.58819840\n",
      "Iteration 52, loss = 1.58945487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 1.24799326\n",
      "Iteration 49, loss = 1.76490765\n",
      "Iteration 50, loss = 1.94228277\n",
      "Iteration 51, loss = 2.04003623\n",
      "Iteration 52, loss = 1.82162925\n",
      "Iteration 53, loss = 1.79156157\n",
      "Iteration 54, loss = 2.17502870\n",
      "Iteration 55, loss = 1.96974406\n",
      "Iteration 56, loss = 1.58438911\n",
      "Iteration 57, loss = 1.82206594\n",
      "Iteration 1, loss = 13.83094396\n",
      "Iteration 58, loss = 1.53940105\n",
      "Iteration 2, loss = 14.96064730\n",
      "Iteration 59, loss = 1.36398275\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 16.97212337\n",
      "Iteration 4, loss = 10.59004136\n",
      "Iteration 5, loss = 13.42605520\n",
      "Iteration 6, loss = 7.89765544\n",
      "Iteration 7, loss = 7.79038569\n",
      "Iteration 8, loss = 7.62106827\n",
      "Iteration 9, loss = 6.65476969\n",
      "Iteration 10, loss = 8.67747998\n",
      "Iteration 11, loss = 7.21686093\n",
      "Iteration 12, loss = 6.44491710\n",
      "Iteration 13, loss = 6.22888510\n",
      "Iteration 14, loss = 4.24039904\n",
      "Iteration 1, loss = 17.51496597\n",
      "Iteration 2, loss = 13.89202032\n",
      "Iteration 15, loss = 9.16037935\n",
      "Iteration 3, loss = 13.74889449\n",
      "Iteration 16, loss = 6.07194614\n",
      "Iteration 4, loss = 14.06002520\n",
      "Iteration 5, loss = 13.25528395\n",
      "Iteration 17, loss = 5.26022176\n",
      "Iteration 6, loss = 14.55392954\n",
      "Iteration 7, loss = 15.95420071\n",
      "Iteration 18, loss = 4.97720038\n",
      "Iteration 8, loss = 14.03941465\n",
      "Iteration 19, loss = 4.00315697\n",
      "Iteration 9, loss = 13.68417327\n",
      "Iteration 10, loss = 13.83888763\n",
      "Iteration 20, loss = 3.66076826\n",
      "Iteration 11, loss = 14.38551880\n",
      "Iteration 12, loss = 16.19636554\n",
      "Iteration 21, loss = 3.73193524\n",
      "Iteration 13, loss = 15.24085309\n",
      "Iteration 14, loss = 14.73692207\n",
      "Iteration 15, loss = 15.58118817\n",
      "Iteration 22, loss = 3.73053420\n",
      "Iteration 16, loss = 15.70675315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 3.00538685\n",
      "Iteration 24, loss = 3.56065376\n",
      "Iteration 25, loss = 3.03142055\n",
      "Iteration 26, loss = 2.16037283\n",
      "Iteration 27, loss = 2.57843477\n",
      "Iteration 28, loss = 2.74576842\n",
      "Iteration 29, loss = 2.56167999\n",
      "Iteration 30, loss = 3.24242536\n",
      "Iteration 31, loss = 2.55328315\n",
      "Iteration 1, loss = 20.25711919\n",
      "Iteration 32, loss = 2.36803311\n",
      "Iteration 33, loss = 3.14369593\n",
      "Iteration 34, loss = 3.08406279\n",
      "Iteration 2, loss = 15.13985103\n",
      "Iteration 35, loss = 2.78135525\n",
      "Iteration 3, loss = 12.72626152\n",
      "Iteration 36, loss = 2.95110636\n",
      "Iteration 4, loss = 14.89399454\n",
      "Iteration 37, loss = 3.23512904\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 15.37052958\n",
      "Iteration 6, loss = 17.28347593\n",
      "Iteration 7, loss = 16.16363344\n",
      "Iteration 8, loss = 14.12149704\n",
      "Iteration 9, loss = 13.79122171\n",
      "Iteration 10, loss = 14.18887188\n",
      "Iteration 11, loss = 15.05285511\n",
      "Iteration 12, loss = 19.48646074\n",
      "Iteration 13, loss = 17.44765090\n",
      "Iteration 14, loss = 16.15836804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.63292877\n",
      "Iteration 2, loss = 16.78179923\n",
      "Iteration 3, loss = 12.96525774\n",
      "Iteration 4, loss = 14.27428912\n",
      "Iteration 5, loss = 13.94534851\n",
      "Iteration 6, loss = 13.32184114\n",
      "Iteration 1, loss = 19.79113353\n",
      "Iteration 2, loss = 21.92402512\n",
      "Iteration 3, loss = 13.89923285\n",
      "Iteration 7, loss = 13.07483363\n",
      "Iteration 4, loss = 14.40844528\n",
      "Iteration 5, loss = 14.90565169\n",
      "Iteration 8, loss = 14.63753070\n",
      "Iteration 9, loss = 17.19352400\n",
      "Iteration 6, loss = 16.72869923\n",
      "Iteration 10, loss = 14.11628312\n",
      "Iteration 7, loss = 15.49480850\n",
      "Iteration 8, loss = 16.84764989\n",
      "Iteration 11, loss = 13.83122477\n",
      "Iteration 12, loss = 14.06544258\n",
      "Iteration 9, loss = 16.73915269\n",
      "Iteration 13, loss = 14.01629619\n",
      "Iteration 10, loss = 16.79694225\n",
      "Iteration 14, loss = 14.95246892\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 17.10315121\n",
      "Iteration 12, loss = 15.65437681\n",
      "Iteration 13, loss = 17.32165248\n",
      "Iteration 14, loss = 15.83654425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.69839570\n",
      "Iteration 2, loss = 16.42488259\n",
      "Iteration 3, loss = 14.65617486\n",
      "Iteration 4, loss = 13.55694858\n",
      "Iteration 1, loss = 20.49286495\n",
      "Iteration 5, loss = 13.22536523\n",
      "Iteration 2, loss = 20.67237380\n",
      "Iteration 3, loss = 17.68430642\n",
      "Iteration 6, loss = 12.27561591\n",
      "Iteration 4, loss = 16.01657440\n",
      "Iteration 7, loss = 12.64986822\n",
      "Iteration 8, loss = 13.35663411\n",
      "Iteration 5, loss = 12.36437166\n",
      "Iteration 9, loss = 12.87963649\n",
      "Iteration 6, loss = 16.52179119\n",
      "Iteration 7, loss = 14.62957861\n",
      "Iteration 10, loss = 13.33781447\n",
      "Iteration 11, loss = 13.56049431\n",
      "Iteration 12, loss = 15.83841199\n",
      "Iteration 8, loss = 12.06148888\n",
      "Iteration 13, loss = 15.39451293\n",
      "Iteration 9, loss = 17.06759006\n",
      "Iteration 10, loss = 18.92209481\n",
      "Iteration 14, loss = 14.29169590\n",
      "Iteration 11, loss = 16.68751739\n",
      "Iteration 15, loss = 13.61005146\n",
      "Iteration 16, loss = 14.09262756\n",
      "Iteration 12, loss = 16.39667318\n",
      "Iteration 17, loss = 13.74944838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 15.69269675\n",
      "Iteration 14, loss = 15.05862664\n",
      "Iteration 15, loss = 14.56228507\n",
      "Iteration 16, loss = 14.16586460\n",
      "Iteration 17, loss = 14.84731665\n",
      "Iteration 18, loss = 14.83479237\n",
      "Iteration 19, loss = 14.28043441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.01985250\n",
      "Iteration 2, loss = 16.26404608\n",
      "Iteration 3, loss = 15.65832174\n",
      "Iteration 4, loss = 14.08016515\n",
      "Iteration 5, loss = 20.14513088\n",
      "Iteration 6, loss = 17.26274350\n",
      "Iteration 7, loss = 15.41904018\n",
      "Iteration 1, loss = 18.93450970\n",
      "Iteration 8, loss = 20.12362893\n",
      "Iteration 2, loss = 14.54769644\n",
      "Iteration 9, loss = 16.45561656\n",
      "Iteration 3, loss = 15.51806577\n",
      "Iteration 10, loss = 15.64406189\n",
      "Iteration 4, loss = 13.56637582\n",
      "Iteration 11, loss = 16.35993204\n",
      "Iteration 5, loss = 15.77256773\n",
      "Iteration 6, loss = 14.13345834\n",
      "Iteration 7, loss = 12.44586285\n",
      "Iteration 12, loss = 16.61083740\n",
      "Iteration 13, loss = 18.33507621\n",
      "Iteration 8, loss = 15.04229163\n",
      "Iteration 14, loss = 16.59530739\n",
      "Iteration 15, loss = 16.33254271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 13.08769925\n",
      "Iteration 10, loss = 12.56467985\n",
      "Iteration 11, loss = 13.22680867\n",
      "Iteration 12, loss = 14.10230626\n",
      "Iteration 13, loss = 14.70784213\n",
      "Iteration 14, loss = 13.95217274\n",
      "Iteration 15, loss = 13.23010287\n",
      "Iteration 16, loss = 13.77613458\n",
      "Iteration 17, loss = 13.64921776\n",
      "Iteration 18, loss = 13.55628565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.86456120\n",
      "Iteration 2, loss = 20.12594173\n",
      "Iteration 3, loss = 13.77632049\n",
      "Iteration 4, loss = 12.20246735\n",
      "Iteration 5, loss = 14.12430977\n",
      "Iteration 6, loss = 14.79700945\n",
      "Iteration 7, loss = 15.79464160\n",
      "Iteration 1, loss = 18.77966375\n",
      "Iteration 8, loss = 16.29821608\n",
      "Iteration 2, loss = 15.14287135\n",
      "Iteration 9, loss = 14.44488949\n",
      "Iteration 3, loss = 19.20534053\n",
      "Iteration 10, loss = 15.99815471\n",
      "Iteration 11, loss = 15.60874638\n",
      "Iteration 4, loss = 14.09470838\n",
      "Iteration 5, loss = 12.68187359\n",
      "Iteration 12, loss = 15.88262524\n",
      "Iteration 6, loss = 14.84276779\n",
      "Iteration 13, loss = 15.71035976\n",
      "Iteration 7, loss = 14.37872192\n",
      "Iteration 14, loss = 15.41900430\n",
      "Iteration 15, loss = 15.63151961\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 14.04993416\n",
      "Iteration 9, loss = 14.00633516\n",
      "Iteration 10, loss = 14.67319958\n",
      "Iteration 11, loss = 15.46978295\n",
      "Iteration 12, loss = 16.11343247\n",
      "Iteration 13, loss = 20.09931607\n",
      "Iteration 14, loss = 18.13248856\n",
      "Iteration 15, loss = 20.85904203\n",
      "Iteration 16, loss = 18.70864916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.88025498\n",
      "Iteration 2, loss = 19.72648725\n",
      "Iteration 3, loss = 15.33703436\n",
      "Iteration 4, loss = 12.60159426\n",
      "Iteration 5, loss = 14.38317258\n",
      "Iteration 6, loss = 13.93931498\n",
      "Iteration 7, loss = 15.67750581\n",
      "Iteration 8, loss = 15.38180619\n",
      "Iteration 9, loss = 20.21390976\n",
      "Iteration 1, loss = 15.92551517\n",
      "Iteration 10, loss = 16.15397023\n",
      "Iteration 2, loss = 15.85909029\n",
      "Iteration 11, loss = 16.17919621\n",
      "Iteration 3, loss = 15.15189839\n",
      "Iteration 12, loss = 15.71413846\n",
      "Iteration 4, loss = 11.80297937\n",
      "Iteration 13, loss = 15.12601822\n",
      "Iteration 14, loss = 14.85607686\n",
      "Iteration 5, loss = 12.00882856\n",
      "Iteration 15, loss = 14.17111114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 11.80389292\n",
      "Iteration 7, loss = 13.44540348\n",
      "Iteration 8, loss = 12.80266180\n",
      "Iteration 9, loss = 12.81126074\n",
      "Iteration 10, loss = 15.21129144\n",
      "Iteration 11, loss = 16.29998260\n",
      "Iteration 12, loss = 13.42998877\n",
      "Iteration 13, loss = 13.19726496\n",
      "Iteration 1, loss = 23.72161462\n",
      "Iteration 14, loss = 15.02654267\n",
      "Iteration 2, loss = 24.00969605\n",
      "Iteration 15, loss = 15.06489344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 15.94313013\n",
      "Iteration 4, loss = 15.33495537\n",
      "Iteration 5, loss = 14.99458438\n",
      "Iteration 6, loss = 15.51690298\n",
      "Iteration 7, loss = 16.44695062\n",
      "Iteration 8, loss = 16.21213030\n",
      "Iteration 9, loss = 15.93743255\n",
      "Iteration 10, loss = 16.07991084\n",
      "Iteration 11, loss = 16.47553878\n",
      "Iteration 12, loss = 18.34325615\n",
      "Iteration 13, loss = 17.79781955\n",
      "Iteration 14, loss = 19.03994147\n",
      "Iteration 1, loss = 13.88292496\n",
      "Iteration 15, loss = 17.86382207\n",
      "Iteration 2, loss = 22.87186899\n",
      "Iteration 16, loss = 16.18478729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 15.88265085\n",
      "Iteration 4, loss = 13.83618253\n",
      "Iteration 5, loss = 13.82170592\n",
      "Iteration 6, loss = 12.56047324\n",
      "Iteration 7, loss = 13.20744504\n",
      "Iteration 8, loss = 18.25368564\n",
      "Iteration 9, loss = 15.66644959\n",
      "Iteration 10, loss = 14.06670493\n",
      "Iteration 11, loss = 14.66984275\n",
      "Iteration 1, loss = 18.26434904\n",
      "Iteration 12, loss = 14.38945627\n",
      "Iteration 2, loss = 14.56451488\n",
      "Iteration 13, loss = 14.98871572\n",
      "Iteration 3, loss = 13.80378127\n",
      "Iteration 14, loss = 15.57075275\n",
      "Iteration 4, loss = 14.41572977\n",
      "Iteration 5, loss = 15.32004824\n",
      "Iteration 15, loss = 15.08543219\n",
      "Iteration 6, loss = 11.58911579\n",
      "Iteration 16, loss = 15.99982515\n",
      "Iteration 7, loss = 12.15961256\n",
      "Iteration 17, loss = 16.24144176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 12.90203801\n",
      "Iteration 9, loss = 14.99674388\n",
      "Iteration 10, loss = 13.36537119\n",
      "Iteration 11, loss = 13.58341833\n",
      "Iteration 12, loss = 13.43266886\n",
      "Iteration 13, loss = 13.74264173\n",
      "Iteration 14, loss = 13.99693444\n",
      "Iteration 15, loss = 13.43102432\n",
      "Iteration 16, loss = 14.09982582\n",
      "Iteration 17, loss = 14.20146622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.07360510\n",
      "Iteration 2, loss = 16.40041038\n",
      "Iteration 3, loss = 12.54752371\n",
      "Iteration 4, loss = 11.96356553\n",
      "Iteration 1, loss = 18.88737166\n",
      "Iteration 5, loss = 12.62590727\n",
      "Iteration 2, loss = 17.61311034\n",
      "Iteration 6, loss = 12.18277061\n",
      "Iteration 3, loss = 15.01741329\n",
      "Iteration 7, loss = 13.04292063\n",
      "Iteration 8, loss = 16.76355540\n",
      "Iteration 4, loss = 16.67416315\n",
      "Iteration 5, loss = 14.50528697\n",
      "Iteration 6, loss = 15.65876271\n",
      "Iteration 9, loss = 16.57867809\n",
      "Iteration 7, loss = 15.81241545\n",
      "Iteration 10, loss = 15.66055062\n",
      "Iteration 11, loss = 14.32457350\n",
      "Iteration 8, loss = 16.20050729\n",
      "Iteration 12, loss = 13.87282931\n",
      "Iteration 9, loss = 18.42602753\n",
      "Iteration 13, loss = 14.15853126\n",
      "Iteration 14, loss = 16.98476890\n",
      "Iteration 15, loss = 15.36110738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 14.64238608\n",
      "Iteration 11, loss = 15.66972894\n",
      "Iteration 12, loss = 14.98081357\n",
      "Iteration 13, loss = 16.57270236\n",
      "Iteration 14, loss = 15.15029122\n",
      "Iteration 15, loss = 15.54716394\n",
      "Iteration 16, loss = 13.77994646\n",
      "Iteration 17, loss = 14.02270338\n",
      "Iteration 18, loss = 14.20146319\n",
      "Iteration 19, loss = 13.81998279\n",
      "Iteration 1, loss = 18.93730348\n",
      "Iteration 20, loss = 17.54453375\n",
      "Iteration 21, loss = 14.55313850\n",
      "Iteration 2, loss = 14.55824447\n",
      "Iteration 22, loss = 14.66230942\n",
      "Iteration 3, loss = 18.33869368\n",
      "Iteration 23, loss = 14.46911343\n",
      "Iteration 24, loss = 13.74622239\n",
      "Iteration 25, loss = 13.92379613\n",
      "Iteration 4, loss = 11.51120007\n",
      "Iteration 26, loss = 13.53778150\n",
      "Iteration 27, loss = 14.01221096\n",
      "Iteration 5, loss = 10.54669707\n",
      "Iteration 28, loss = 14.25202302\n",
      "Iteration 29, loss = 14.63926931\n",
      "Iteration 30, loss = 13.41020528\n",
      "Iteration 6, loss = 12.74873541\n",
      "Iteration 31, loss = 13.07702270\n",
      "Iteration 32, loss = 13.81691161\n",
      "Iteration 7, loss = 11.38856902\n",
      "Iteration 8, loss = 13.77425671\n",
      "Iteration 33, loss = 13.18925995\n",
      "Iteration 9, loss = 13.55971353\n",
      "Iteration 34, loss = 13.40159229\n",
      "Iteration 35, loss = 13.21777796\n",
      "Iteration 10, loss = 13.83222053\n",
      "Iteration 36, loss = 12.99045819\n",
      "Iteration 11, loss = 13.86981182\n",
      "Iteration 37, loss = 12.91717352\n",
      "Iteration 12, loss = 14.61163851\n",
      "Iteration 13, loss = 13.46918635\n",
      "Iteration 38, loss = 12.68704459\n",
      "Iteration 39, loss = 13.08917255\n",
      "Iteration 14, loss = 13.61277342\n",
      "Iteration 40, loss = 12.60523352\n",
      "Iteration 41, loss = 13.09533875\n",
      "Iteration 15, loss = 13.93898043\n",
      "Iteration 16, loss = 13.71684453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 13.10734406\n",
      "Iteration 43, loss = 13.03778443\n",
      "Iteration 44, loss = 13.00097570\n",
      "Iteration 45, loss = 13.47113510\n",
      "Iteration 46, loss = 13.58740436\n",
      "Iteration 47, loss = 13.74346767\n",
      "Iteration 48, loss = 14.33214304\n",
      "Iteration 1, loss = 20.31270829\n",
      "Iteration 49, loss = 14.45216806\n",
      "Iteration 2, loss = 22.80883290\n",
      "Iteration 3, loss = 15.26042876\n",
      "Iteration 50, loss = 13.39854460\n",
      "Iteration 51, loss = 13.05611555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 15.27755684\n",
      "Iteration 5, loss = 14.38128497\n",
      "Iteration 6, loss = 12.63738014\n",
      "Iteration 7, loss = 15.30580590\n",
      "Iteration 8, loss = 15.27801803\n",
      "Iteration 9, loss = 13.36486764\n",
      "Iteration 10, loss = 13.86284475\n",
      "Iteration 11, loss = 14.74971544\n",
      "Iteration 12, loss = 14.82227095\n",
      "Iteration 13, loss = 15.74356919\n",
      "Iteration 14, loss = 19.64442023\n",
      "Iteration 15, loss = 16.31132064\n",
      "Iteration 1, loss = 17.58985881\n",
      "Iteration 2, loss = 16.61841986\n",
      "Iteration 16, loss = 14.88302403\n",
      "Iteration 3, loss = 15.37448971\n",
      "Iteration 17, loss = 15.36797858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 14.88490740\n",
      "Iteration 5, loss = 14.09883348\n",
      "Iteration 6, loss = 13.11264003\n",
      "Iteration 7, loss = 13.34304969\n",
      "Iteration 8, loss = 15.56962814\n",
      "Iteration 9, loss = 15.97038357\n",
      "Iteration 10, loss = 15.74590731\n",
      "Iteration 11, loss = 14.44852881\n",
      "Iteration 12, loss = 14.13529552\n",
      "Iteration 1, loss = 18.14057335\n",
      "Iteration 13, loss = 13.63667044\n",
      "Iteration 2, loss = 16.45001420\n",
      "Iteration 14, loss = 13.28577476\n",
      "Iteration 3, loss = 14.32734017\n",
      "Iteration 15, loss = 13.96628860\n",
      "Iteration 4, loss = 13.14117740\n",
      "Iteration 16, loss = 13.33265395\n",
      "Iteration 5, loss = 13.98112539\n",
      "Iteration 17, loss = 13.43985120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 14.18701163\n",
      "Iteration 7, loss = 15.40427252\n",
      "Iteration 8, loss = 15.34481755\n",
      "Iteration 9, loss = 14.67361214\n",
      "Iteration 10, loss = 16.78662554\n",
      "Iteration 11, loss = 15.89109602\n",
      "Iteration 12, loss = 15.09251804\n",
      "Iteration 13, loss = 15.66063129\n",
      "Iteration 14, loss = 15.13643255\n",
      "Iteration 1, loss = 17.30777584\n",
      "Iteration 2, loss = 18.20851451\n",
      "Iteration 15, loss = 15.77737345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 17.07798004\n",
      "Iteration 4, loss = 11.67969092\n",
      "Iteration 5, loss = 11.47235261\n",
      "Iteration 6, loss = 14.37105553\n",
      "Iteration 7, loss = 16.31484497\n",
      "Iteration 8, loss = 15.55650236\n",
      "Iteration 9, loss = 17.87676996\n",
      "Iteration 10, loss = 15.88165914\n",
      "Iteration 11, loss = 15.13730770\n",
      "Iteration 12, loss = 15.88179004\n",
      "Iteration 1, loss = 18.53109628\n",
      "Iteration 2, loss = 16.62359938\n",
      "Iteration 13, loss = 15.34212568\n",
      "Iteration 14, loss = 15.67507992\n",
      "Iteration 15, loss = 15.39268830\n",
      "Iteration 3, loss = 15.76039808\n",
      "Iteration 4, loss = 14.28012771\n",
      "Iteration 16, loss = 15.22356119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 15.97037349\n",
      "Iteration 6, loss = 15.42441726\n",
      "Iteration 7, loss = 18.19390003\n",
      "Iteration 8, loss = 18.59646214\n",
      "Iteration 9, loss = 18.35210663\n",
      "Iteration 10, loss = 17.12090094\n",
      "Iteration 11, loss = 15.81518208\n",
      "Iteration 12, loss = 14.32333444\n",
      "Iteration 1, loss = 19.73783023\n",
      "Iteration 2, loss = 15.99897753\n",
      "Iteration 3, loss = 16.19692244\n",
      "Iteration 13, loss = 15.46075036\n",
      "Iteration 4, loss = 15.45359462\n",
      "Iteration 5, loss = 16.58659953\n",
      "Iteration 6, loss = 14.17508067\n",
      "Iteration 14, loss = 14.50122358\n",
      "Iteration 7, loss = 16.35659674\n",
      "Iteration 8, loss = 17.78620365\n",
      "Iteration 9, loss = 15.51729595\n",
      "Iteration 15, loss = 14.46255297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 15.06884746\n",
      "Iteration 11, loss = 15.77473532\n",
      "Iteration 12, loss = 16.13027378\n",
      "Iteration 13, loss = 16.29577963\n",
      "Iteration 14, loss = 17.18038519\n",
      "Iteration 1, loss = 17.46722189\n",
      "Iteration 15, loss = 15.26335012\n",
      "Iteration 16, loss = 16.16910212\n",
      "Iteration 2, loss = 17.16179205\n",
      "Iteration 17, loss = 16.17984189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 16.16817156\n",
      "Iteration 4, loss = 13.50276048\n",
      "Iteration 5, loss = 12.85432747\n",
      "Iteration 6, loss = 10.97598225\n",
      "Iteration 7, loss = 10.97192614\n",
      "Iteration 8, loss = 12.67605905\n",
      "Iteration 9, loss = 13.52394600\n",
      "Iteration 10, loss = 12.71291972\n",
      "Iteration 11, loss = 13.47889794\n",
      "Iteration 12, loss = 12.73583473\n",
      "Iteration 13, loss = 13.44037093\n",
      "Iteration 14, loss = 14.51000448\n",
      "Iteration 1, loss = 20.17451122\n",
      "Iteration 15, loss = 16.62315806\n",
      "Iteration 2, loss = 20.79856165\n",
      "Iteration 3, loss = 17.59724513\n",
      "Iteration 16, loss = 14.74076665\n",
      "Iteration 4, loss = 13.64018614\n",
      "Iteration 5, loss = 14.55744245\n",
      "Iteration 17, loss = 13.89366097\n",
      "Iteration 6, loss = 12.68273561\n",
      "Iteration 18, loss = 13.91961134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 14.10493247\n",
      "Iteration 8, loss = 18.19243528\n",
      "Iteration 9, loss = 15.63929859\n",
      "Iteration 10, loss = 19.06180959\n",
      "Iteration 11, loss = 14.21786407\n",
      "Iteration 12, loss = 14.59445626\n",
      "Iteration 13, loss = 15.62594808\n",
      "Iteration 14, loss = 14.39969307\n",
      "Iteration 15, loss = 14.99844841\n",
      "Iteration 16, loss = 16.98160329\n",
      "Iteration 17, loss = 16.89197211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.45428885\n",
      "Iteration 2, loss = 15.53076641\n",
      "Iteration 3, loss = 16.77857528\n",
      "Iteration 4, loss = 14.13920853\n",
      "Iteration 5, loss = 14.28912438\n",
      "Iteration 6, loss = 13.86214342\n",
      "Iteration 7, loss = 14.85116033\n",
      "Iteration 8, loss = 14.99669428\n",
      "Iteration 9, loss = 14.92123860\n",
      "Iteration 10, loss = 16.52428890\n",
      "Iteration 11, loss = 16.61926712\n",
      "Iteration 12, loss = 15.09576552\n",
      "Iteration 1, loss = 20.28279658Iteration 13, loss = 14.11823356\n",
      "\n",
      "Iteration 2, loss = 18.64457910\n",
      "Iteration 3, loss = 14.76475043\n",
      "Iteration 4, loss = 11.52918840\n",
      "Iteration 14, loss = 14.49027016\n",
      "Iteration 5, loss = 11.96551584\n",
      "Iteration 15, loss = 15.24813716\n",
      "Iteration 6, loss = 12.18691037\n",
      "Iteration 16, loss = 15.42244396\n",
      "Iteration 7, loss = 17.06000792\n",
      "Iteration 17, loss = 14.82789003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 14.97869113\n",
      "Iteration 9, loss = 14.52037605\n",
      "Iteration 10, loss = 14.90114256\n",
      "Iteration 11, loss = 16.35401694\n",
      "Iteration 12, loss = 16.37199234\n",
      "Iteration 13, loss = 19.72256498\n",
      "Iteration 14, loss = 19.44915728\n",
      "Iteration 15, loss = 17.84264811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.83221194\n",
      "Iteration 2, loss = 13.79404268\n",
      "Iteration 3, loss = 13.75107666\n",
      "Iteration 4, loss = 11.04590370\n",
      "Iteration 5, loss = 12.25980113\n",
      "Iteration 6, loss = 15.46578884\n",
      "Iteration 7, loss = 14.16705915\n",
      "Iteration 1, loss = 16.44070212\n",
      "Iteration 8, loss = 14.13776359\n",
      "Iteration 2, loss = 16.96270026\n",
      "Iteration 3, loss = 14.67023493\n",
      "Iteration 9, loss = 13.19241458\n",
      "Iteration 4, loss = 13.83451281\n",
      "Iteration 10, loss = 12.86565524\n",
      "Iteration 5, loss = 13.32708022\n",
      "Iteration 6, loss = 14.08634285\n",
      "Iteration 11, loss = 13.58311868\n",
      "Iteration 7, loss = 18.24505949\n",
      "Iteration 12, loss = 13.08474348\n",
      "Iteration 8, loss = 18.35219577\n",
      "Iteration 13, loss = 12.98739815\n",
      "Iteration 9, loss = 16.72753824\n",
      "Iteration 14, loss = 12.55744175\n",
      "Iteration 10, loss = 17.44107869\n",
      "Iteration 15, loss = 12.54052723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 16.83968748\n",
      "Iteration 12, loss = 16.01231822\n",
      "Iteration 13, loss = 15.79718874\n",
      "Iteration 14, loss = 16.42686305\n",
      "Iteration 15, loss = 15.23174931\n",
      "Iteration 16, loss = 15.12299563\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.06726931\n",
      "Iteration 2, loss = 20.39692769\n",
      "Iteration 3, loss = 18.43499216\n",
      "Iteration 4, loss = 13.87079955\n",
      "Iteration 5, loss = 11.30638454\n",
      "Iteration 6, loss = 13.80179679\n",
      "Iteration 7, loss = 12.52293541\n",
      "Iteration 8, loss = 13.29738263\n",
      "Iteration 9, loss = 15.35517819\n",
      "Iteration 10, loss = 14.75982187\n",
      "Iteration 1, loss = 20.28333891\n",
      "Iteration 11, loss = 16.84486469\n",
      "Iteration 2, loss = 15.64552100\n",
      "Iteration 12, loss = 15.91395466\n",
      "Iteration 3, loss = 11.15827211\n",
      "Iteration 13, loss = 14.36905350\n",
      "Iteration 4, loss = 15.04957991\n",
      "Iteration 14, loss = 14.30921138\n",
      "Iteration 5, loss = 14.71688750\n",
      "Iteration 6, loss = 15.02052889\n",
      "Iteration 15, loss = 14.38532003\n",
      "Iteration 16, loss = 14.65629208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 14.25559671\n",
      "Iteration 8, loss = 15.57746322\n",
      "Iteration 9, loss = 14.63618810\n",
      "Iteration 10, loss = 14.75122900\n",
      "Iteration 11, loss = 16.21116938\n",
      "Iteration 12, loss = 16.88292228\n",
      "Iteration 13, loss = 16.34052767\n",
      "Iteration 14, loss = 15.33988718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.74207017\n",
      "Iteration 2, loss = 18.44981196\n",
      "Iteration 3, loss = 16.91375005\n",
      "Iteration 4, loss = 13.67377714\n",
      "Iteration 5, loss = 12.61763344\n",
      "Iteration 6, loss = 14.62247845\n",
      "Iteration 7, loss = 14.76498284\n",
      "Iteration 8, loss = 14.24454417\n",
      "Iteration 9, loss = 14.23435073\n",
      "Iteration 10, loss = 13.86625533\n",
      "Iteration 11, loss = 13.57534179\n",
      "Iteration 12, loss = 15.53379949\n",
      "Iteration 13, loss = 15.37687843\n",
      "Iteration 1, loss = 14.14514712\n",
      "Iteration 2, loss = 15.77213935\n",
      "Iteration 3, loss = 13.49936778\n",
      "Iteration 14, loss = 16.95520206\n",
      "Iteration 4, loss = 16.34224785\n",
      "Iteration 15, loss = 14.98159503\n",
      "Iteration 16, loss = 13.87516523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 15.18632552\n",
      "Iteration 6, loss = 14.44928721\n",
      "Iteration 7, loss = 15.30162310\n",
      "Iteration 8, loss = 14.96799748\n",
      "Iteration 9, loss = 16.69151281\n",
      "Iteration 10, loss = 14.65052955\n",
      "Iteration 11, loss = 13.99849505\n",
      "Iteration 1, loss = 18.39973111\n",
      "Iteration 12, loss = 12.98802929\n",
      "Iteration 13, loss = 13.56140490\n",
      "Iteration 14, loss = 13.29126071\n",
      "Iteration 2, loss = 16.76042456\n",
      "Iteration 15, loss = 13.97518994\n",
      "Iteration 3, loss = 12.53241636\n",
      "Iteration 4, loss = 15.07297185\n",
      "Iteration 16, loss = 14.07429225\n",
      "Iteration 5, loss = 13.51002178\n",
      "Iteration 17, loss = 13.16692756\n",
      "Iteration 6, loss = 15.54436501\n",
      "Iteration 18, loss = 13.06553590\n",
      "Iteration 7, loss = 16.42596994\n",
      "Iteration 19, loss = 12.87193008\n",
      "Iteration 8, loss = 16.30656272\n",
      "Iteration 20, loss = 13.22314321\n",
      "Iteration 9, loss = 19.15737821\n",
      "Iteration 21, loss = 12.71436126\n",
      "Iteration 10, loss = 14.93225224\n",
      "Iteration 22, loss = 13.45316667\n",
      "Iteration 11, loss = 15.92846095\n",
      "Iteration 23, loss = 14.38227665\n",
      "Iteration 12, loss = 16.04542916\n",
      "Iteration 24, loss = 13.34607164\n",
      "Iteration 13, loss = 15.98496578\n",
      "Iteration 25, loss = 12.66379992\n",
      "Iteration 14, loss = 16.14203639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 12.96039608\n",
      "Iteration 27, loss = 12.50465567\n",
      "Iteration 28, loss = 13.33913137\n",
      "Iteration 29, loss = 12.87664181\n",
      "Iteration 30, loss = 12.37029886\n",
      "Iteration 31, loss = 12.53091952\n",
      "Iteration 32, loss = 12.97963405\n",
      "Iteration 33, loss = 12.92839773\n",
      "Iteration 34, loss = 12.43587188\n",
      "Iteration 1, loss = 18.91623553\n",
      "Iteration 35, loss = 13.03520273\n",
      "Iteration 2, loss = 15.37885546\n",
      "Iteration 36, loss = 12.76888670\n",
      "Iteration 3, loss = 12.13732526\n",
      "Iteration 37, loss = 13.51978410\n",
      "Iteration 4, loss = 11.59142729\n",
      "Iteration 5, loss = 15.39435209\n",
      "Iteration 6, loss = 16.72111649\n",
      "Iteration 7, loss = 14.41258795\n",
      "Iteration 38, loss = 13.43664797\n",
      "Iteration 39, loss = 13.07688513\n",
      "Iteration 8, loss = 16.56829330\n",
      "Iteration 40, loss = 13.07425236\n",
      "Iteration 9, loss = 14.50577021\n",
      "Iteration 41, loss = 12.76302797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 15.46151071\n",
      "Iteration 11, loss = 16.35790563\n",
      "Iteration 12, loss = 16.17083069\n",
      "Iteration 13, loss = 15.38428805\n",
      "Iteration 14, loss = 15.10982666\n",
      "Iteration 15, loss = 15.58045480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 21.60566954\n",
      "Iteration 2, loss = 14.66202361\n",
      "Iteration 3, loss = 14.08023641\n",
      "Iteration 4, loss = 14.96226374\n",
      "Iteration 5, loss = 13.64929695\n",
      "Iteration 6, loss = 13.02553362\n",
      "Iteration 7, loss = 13.72825154\n",
      "Iteration 8, loss = 16.29205593\n",
      "Iteration 1, loss = 22.07697563\n",
      "Iteration 2, loss = 18.40931092\n",
      "Iteration 9, loss = 16.64488493\n",
      "Iteration 3, loss = 15.16069422\n",
      "Iteration 4, loss = 15.55135783\n",
      "Iteration 5, loss = 12.14914080\n",
      "Iteration 6, loss = 11.69579044\n",
      "Iteration 10, loss = 15.75360415\n",
      "Iteration 7, loss = 13.39372345\n",
      "Iteration 8, loss = 21.70157277\n",
      "Iteration 11, loss = 17.15198580\n",
      "Iteration 9, loss = 16.52718989\n",
      "Iteration 10, loss = 15.12915308\n",
      "Iteration 11, loss = 15.20681687\n",
      "Iteration 12, loss = 16.82038042\n",
      "Iteration 12, loss = 15.97310721\n",
      "Iteration 13, loss = 15.90772889\n",
      "Iteration 14, loss = 16.59609281\n",
      "Iteration 15, loss = 16.96973613\n",
      "Iteration 13, loss = 17.11541473\n",
      "Iteration 16, loss = 17.54637754\n",
      "Iteration 14, loss = 16.13496708\n",
      "Iteration 17, loss = 18.08015153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 16.76870930\n",
      "Iteration 16, loss = 17.38430940\n",
      "Iteration 17, loss = 16.88069288\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.62827126\n",
      "Iteration 2, loss = 17.88497176\n",
      "Iteration 3, loss = 14.27128915\n",
      "Iteration 4, loss = 10.89665606\n",
      "Iteration 5, loss = 12.88643711\n",
      "Iteration 6, loss = 16.61484871\n",
      "Iteration 7, loss = 14.06675394\n",
      "Iteration 1, loss = 22.93657536\n",
      "Iteration 2, loss = 25.46351726\n",
      "Iteration 8, loss = 13.42718071\n",
      "Iteration 3, loss = 13.37574538\n",
      "Iteration 9, loss = 13.31092291\n",
      "Iteration 4, loss = 12.24349866\n",
      "Iteration 10, loss = 13.36276559\n",
      "Iteration 5, loss = 13.32788936\n",
      "Iteration 11, loss = 14.31548036\n",
      "Iteration 6, loss = 14.31164347\n",
      "Iteration 12, loss = 13.93385551\n",
      "Iteration 13, loss = 14.07093809\n",
      "Iteration 7, loss = 14.51071881\n",
      "Iteration 14, loss = 14.26580910\n",
      "Iteration 8, loss = 13.49192854\n",
      "Iteration 9, loss = 13.29554427\n",
      "Iteration 15, loss = 14.93282820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 13.16321995\n",
      "Iteration 11, loss = 13.82349654\n",
      "Iteration 12, loss = 14.25881241\n",
      "Iteration 13, loss = 14.90299062\n",
      "Iteration 14, loss = 14.43194148\n",
      "Iteration 15, loss = 13.58633221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.88064479\n",
      "Iteration 2, loss = 0.53257376\n",
      "Iteration 3, loss = 0.41926906\n",
      "Iteration 4, loss = 0.37429052\n",
      "Iteration 5, loss = 0.34168699\n",
      "Iteration 6, loss = 0.30829187\n",
      "Iteration 7, loss = 0.28614114\n",
      "Iteration 8, loss = 0.28200050\n",
      "Iteration 9, loss = 0.24814080\n",
      "Iteration 10, loss = 0.22891225\n",
      "Iteration 11, loss = 0.22035240\n",
      "Iteration 12, loss = 0.20349932\n",
      "Iteration 13, loss = 0.18753687\n",
      "Iteration 14, loss = 0.17812715\n",
      "Iteration 15, loss = 0.17093552\n",
      "Iteration 16, loss = 0.17331436\n",
      "Iteration 17, loss = 0.17327103\n",
      "Iteration 1, loss = 1.05079640\n",
      "Iteration 18, loss = 0.14867880\n",
      "Iteration 2, loss = 0.55972460\n",
      "Iteration 19, loss = 0.14243690\n",
      "Iteration 3, loss = 0.44463022\n",
      "Iteration 4, loss = 0.38233441\n",
      "Iteration 20, loss = 0.14558738\n",
      "Iteration 5, loss = 0.34978929\n",
      "Iteration 21, loss = 0.13797424\n",
      "Iteration 6, loss = 0.33554336\n",
      "Iteration 22, loss = 0.13065111\n",
      "Iteration 7, loss = 0.30320699\n",
      "Iteration 8, loss = 0.27910436\n",
      "Iteration 9, loss = 0.26613411\n",
      "Iteration 10, loss = 0.24669657\n",
      "Iteration 23, loss = 0.12774967\n",
      "Iteration 11, loss = 0.23549809\n",
      "Iteration 12, loss = 0.22084646\n",
      "Iteration 13, loss = 0.20435605\n",
      "Iteration 24, loss = 0.12856311\n",
      "Iteration 14, loss = 0.18960249\n",
      "Iteration 25, loss = 0.12850118\n",
      "Iteration 15, loss = 0.18276415\n",
      "Iteration 16, loss = 0.17296735\n",
      "Iteration 26, loss = 0.14601360\n",
      "Iteration 17, loss = 0.17823458\n",
      "Iteration 18, loss = 0.17267548\n",
      "Iteration 19, loss = 0.15149319\n",
      "Iteration 20, loss = 0.15076817\n",
      "Iteration 27, loss = 0.14037018\n",
      "Iteration 21, loss = 0.14803067\n",
      "Iteration 22, loss = 0.15103577\n",
      "Iteration 23, loss = 0.13696068\n",
      "Iteration 24, loss = 0.13962505\n",
      "Iteration 28, loss = 0.12157854Iteration 25, loss = 0.13285703\n",
      "\n",
      "Iteration 26, loss = 0.12454797\n",
      "Iteration 27, loss = 0.12395633\n",
      "Iteration 29, loss = 0.11957415\n",
      "Iteration 28, loss = 0.13942490\n",
      "Iteration 29, loss = 0.14620588\n",
      "Iteration 30, loss = 0.12615558\n",
      "Iteration 30, loss = 0.16291532\n",
      "Iteration 31, loss = 0.10314593\n",
      "Iteration 31, loss = 0.15328636\n",
      "Iteration 32, loss = 0.10801562\n",
      "Iteration 32, loss = 0.12799721\n",
      "Iteration 33, loss = 0.09645037\n",
      "Iteration 33, loss = 0.13660917\n",
      "Iteration 34, loss = 0.08436943\n",
      "Iteration 34, loss = 0.12021575\n",
      "Iteration 35, loss = 0.07896947\n",
      "Iteration 36, loss = 0.07728573\n",
      "Iteration 35, loss = 0.11346580\n",
      "Iteration 37, loss = 0.06947811\n",
      "Iteration 36, loss = 0.10286380\n",
      "Iteration 38, loss = 0.06608405\n",
      "Iteration 39, loss = 0.07702528\n",
      "Iteration 37, loss = 0.09654290\n",
      "Iteration 40, loss = 0.08031879\n",
      "Iteration 41, loss = 0.07892546\n",
      "Iteration 42, loss = 0.07252479\n",
      "Iteration 43, loss = 0.07409216\n",
      "Iteration 38, loss = 0.08447785\n",
      "Iteration 44, loss = 0.07383311\n",
      "Iteration 45, loss = 0.08905999\n",
      "Iteration 46, loss = 0.09291114\n",
      "Iteration 39, loss = 0.08074601\n",
      "Iteration 47, loss = 0.07305137\n",
      "Iteration 48, loss = 0.06855593\n",
      "Iteration 49, loss = 0.06820873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.08106570\n",
      "Iteration 41, loss = 0.08598319\n",
      "Iteration 42, loss = 0.08898653\n",
      "Iteration 43, loss = 0.07354715\n",
      "Iteration 44, loss = 0.07089907\n",
      "Iteration 45, loss = 0.07098907\n",
      "Iteration 46, loss = 0.07422381\n",
      "Iteration 47, loss = 0.09668894\n",
      "Iteration 48, loss = 0.06418402\n",
      "Iteration 49, loss = 0.07058360\n",
      "Iteration 1, loss = 0.86755510\n",
      "Iteration 50, loss = 0.07458237\n",
      "Iteration 2, loss = 0.48365008\n",
      "Iteration 3, loss = 0.39128959\n",
      "Iteration 4, loss = 0.34699905\n",
      "Iteration 5, loss = 0.31524176\n",
      "Iteration 51, loss = 0.05918940\n",
      "Iteration 6, loss = 0.29655539\n",
      "Iteration 52, loss = 0.06721287\n",
      "Iteration 7, loss = 0.27279398\n",
      "Iteration 53, loss = 0.07010523\n",
      "Iteration 8, loss = 0.25690870\n",
      "Iteration 54, loss = 0.06715772\n",
      "Iteration 9, loss = 0.24138515\n",
      "Iteration 55, loss = 0.08220211\n",
      "Iteration 10, loss = 0.22628702\n",
      "Iteration 56, loss = 0.06514348\n",
      "Iteration 11, loss = 0.21308066\n",
      "Iteration 57, loss = 0.05869037\n",
      "Iteration 12, loss = 0.20098664\n",
      "Iteration 58, loss = 0.06577446\n",
      "Iteration 13, loss = 0.19698717\n",
      "Iteration 59, loss = 0.06605622\n",
      "Iteration 14, loss = 0.18803274\n",
      "Iteration 60, loss = 0.04803151\n",
      "Iteration 15, loss = 0.18051415\n",
      "Iteration 61, loss = 0.05817737\n",
      "Iteration 16, loss = 0.15988842\n",
      "Iteration 62, loss = 0.06387919\n",
      "Iteration 17, loss = 0.15484572\n",
      "Iteration 63, loss = 0.04490001\n",
      "Iteration 18, loss = 0.16494796\n",
      "Iteration 64, loss = 0.05421594\n",
      "Iteration 19, loss = 0.16897386\n",
      "Iteration 65, loss = 0.05110138\n",
      "Iteration 20, loss = 0.14354008\n",
      "Iteration 66, loss = 0.04977146\n",
      "Iteration 21, loss = 0.12776363\n",
      "Iteration 67, loss = 0.04861877\n",
      "Iteration 22, loss = 0.13190127\n",
      "Iteration 68, loss = 0.04412406\n",
      "Iteration 69, loss = 0.04375977\n",
      "Iteration 70, loss = 0.05075038\n",
      "Iteration 71, loss = 0.07238277\n",
      "Iteration 72, loss = 0.06465458\n",
      "Iteration 23, loss = 0.11862557\n",
      "Iteration 73, loss = 0.07928684\n",
      "Iteration 74, loss = 0.06285575\n",
      "Iteration 75, loss = 0.07262852\n",
      "Iteration 24, loss = 0.11815045\n",
      "Iteration 76, loss = 0.05146775\n",
      "Iteration 77, loss = 0.04148052\n",
      "Iteration 78, loss = 0.04754315\n",
      "Iteration 25, loss = 0.15014623\n",
      "Iteration 79, loss = 0.03811040\n",
      "Iteration 80, loss = 0.03995770\n",
      "Iteration 81, loss = 0.03712858\n",
      "Iteration 26, loss = 0.14215527\n",
      "Iteration 82, loss = 0.03528684\n",
      "Iteration 27, loss = 0.13188977\n",
      "Iteration 83, loss = 0.03379825\n",
      "Iteration 28, loss = 0.10274380\n",
      "Iteration 84, loss = 0.03117662\n",
      "Iteration 29, loss = 0.09364744\n",
      "Iteration 85, loss = 0.03539282\n",
      "Iteration 30, loss = 0.09113200\n",
      "Iteration 86, loss = 0.05271991\n",
      "Iteration 31, loss = 0.08410980\n",
      "Iteration 87, loss = 0.04863479\n",
      "Iteration 32, loss = 0.09072265\n",
      "Iteration 88, loss = 0.05045855\n",
      "Iteration 33, loss = 0.08836268\n",
      "Iteration 89, loss = 0.05569131\n",
      "Iteration 34, loss = 0.08886845\n",
      "Iteration 90, loss = 0.03810542\n",
      "Iteration 35, loss = 0.07916517\n",
      "Iteration 91, loss = 0.03654441\n",
      "Iteration 36, loss = 0.08159638\n",
      "Iteration 92, loss = 0.03529098\n",
      "Iteration 37, loss = 0.07595028\n",
      "Iteration 93, loss = 0.04136361\n",
      "Iteration 38, loss = 0.07632010\n",
      "Iteration 94, loss = 0.04421598\n",
      "Iteration 39, loss = 0.06655845\n",
      "Iteration 95, loss = 0.04363195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.05648123\n",
      "Iteration 41, loss = 0.06849691\n",
      "Iteration 42, loss = 0.05527844\n",
      "Iteration 43, loss = 0.06724566\n",
      "Iteration 44, loss = 0.05561499\n",
      "Iteration 45, loss = 0.04818160\n",
      "Iteration 46, loss = 0.04572511\n",
      "Iteration 47, loss = 0.05470891\n",
      "Iteration 48, loss = 0.04619616\n",
      "Iteration 49, loss = 0.05387031\n",
      "Iteration 50, loss = 0.04740716\n",
      "Iteration 51, loss = 0.05949267\n",
      "Iteration 52, loss = 0.03964254\n",
      "Iteration 53, loss = 0.04388951\n",
      "Iteration 54, loss = 0.05932709\n",
      "Iteration 55, loss = 0.09009477\n",
      "Iteration 56, loss = 0.08085531\n",
      "Iteration 57, loss = 0.05962908\n",
      "Iteration 1, loss = 0.84060289\n",
      "Iteration 58, loss = 0.07158225\n",
      "Iteration 2, loss = 0.49613529\n",
      "Iteration 59, loss = 0.07554986\n",
      "Iteration 3, loss = 0.43270797\n",
      "Iteration 60, loss = 0.07349337\n",
      "Iteration 4, loss = 0.39676538\n",
      "Iteration 61, loss = 0.07741655\n",
      "Iteration 5, loss = 0.35034084\n",
      "Iteration 62, loss = 0.05944954\n",
      "Iteration 6, loss = 0.31919055\n",
      "Iteration 63, loss = 0.05828249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.29650016\n",
      "Iteration 8, loss = 0.28568595\n",
      "Iteration 9, loss = 0.26619703\n",
      "Iteration 10, loss = 0.25297077\n",
      "Iteration 11, loss = 0.22900442\n",
      "Iteration 12, loss = 0.22811946\n",
      "Iteration 13, loss = 0.20820601\n",
      "Iteration 14, loss = 0.19153610\n",
      "Iteration 15, loss = 0.18456960\n",
      "Iteration 16, loss = 0.17013774\n",
      "Iteration 17, loss = 0.16238365\n",
      "Iteration 18, loss = 0.15363815\n",
      "Iteration 19, loss = 0.14500968\n",
      "Iteration 20, loss = 0.14642259\n",
      "Iteration 21, loss = 0.13201568\n",
      "Iteration 22, loss = 0.13011504\n",
      "Iteration 23, loss = 0.13983458\n",
      "Iteration 24, loss = 0.13520641\n",
      "Iteration 25, loss = 0.13737090\n",
      "Iteration 1, loss = 1.02826566\n",
      "Iteration 26, loss = 0.12310238\n",
      "Iteration 2, loss = 0.52654635\n",
      "Iteration 27, loss = 0.12492369\n",
      "Iteration 3, loss = 0.41946916\n",
      "Iteration 4, loss = 0.37157337\n",
      "Iteration 28, loss = 0.12905941\n",
      "Iteration 5, loss = 0.32956486\n",
      "Iteration 29, loss = 0.12636699\n",
      "Iteration 6, loss = 0.30234244\n",
      "Iteration 30, loss = 0.10788647\n",
      "Iteration 7, loss = 0.27630782\n",
      "Iteration 31, loss = 0.09918141\n",
      "Iteration 8, loss = 0.25128251\n",
      "Iteration 32, loss = 0.09291681\n",
      "Iteration 9, loss = 0.23364032\n",
      "Iteration 33, loss = 0.09715422\n",
      "Iteration 10, loss = 0.21927171\n",
      "Iteration 34, loss = 0.10431703\n",
      "Iteration 11, loss = 0.20025390\n",
      "Iteration 35, loss = 0.12388956\n",
      "Iteration 12, loss = 0.20129703\n",
      "Iteration 36, loss = 0.10424175\n",
      "Iteration 13, loss = 0.17450368\n",
      "Iteration 37, loss = 0.08138517\n",
      "Iteration 14, loss = 0.16672047\n",
      "Iteration 38, loss = 0.07619996\n",
      "Iteration 15, loss = 0.16455748\n",
      "Iteration 39, loss = 0.07466801\n",
      "Iteration 16, loss = 0.15534734\n",
      "Iteration 40, loss = 0.06802476\n",
      "Iteration 17, loss = 0.14550503\n",
      "Iteration 41, loss = 0.06238929\n",
      "Iteration 18, loss = 0.13806344\n",
      "Iteration 42, loss = 0.06666225\n",
      "Iteration 19, loss = 0.13068313\n",
      "Iteration 43, loss = 0.06933592\n",
      "Iteration 20, loss = 0.12846017\n",
      "Iteration 44, loss = 0.06836903\n",
      "Iteration 21, loss = 0.14403959\n",
      "Iteration 45, loss = 0.09044945\n",
      "Iteration 22, loss = 0.12904258\n",
      "Iteration 46, loss = 0.07071165\n",
      "Iteration 23, loss = 0.11343785\n",
      "Iteration 47, loss = 0.06893900\n",
      "Iteration 24, loss = 0.10707245\n",
      "Iteration 48, loss = 0.09079709\n",
      "Iteration 25, loss = 0.10053133\n",
      "Iteration 49, loss = 0.07852863\n",
      "Iteration 26, loss = 0.09987189\n",
      "Iteration 50, loss = 0.08456158\n",
      "Iteration 27, loss = 0.12078226\n",
      "Iteration 51, loss = 0.06677510\n",
      "Iteration 28, loss = 0.13029447\n",
      "Iteration 52, loss = 0.08384427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.11583952\n",
      "Iteration 30, loss = 0.12922356\n",
      "Iteration 31, loss = 0.10272075\n",
      "Iteration 32, loss = 0.08902061\n",
      "Iteration 33, loss = 0.09067381\n",
      "Iteration 34, loss = 0.09046718\n",
      "Iteration 35, loss = 0.07933359\n",
      "Iteration 36, loss = 0.08432284\n",
      "Iteration 37, loss = 0.07055591\n",
      "Iteration 38, loss = 0.08383257\n",
      "Iteration 39, loss = 0.08025899\n",
      "Iteration 40, loss = 0.07153288\n",
      "Iteration 41, loss = 0.08137194\n",
      "Iteration 42, loss = 0.06529508\n",
      "Iteration 43, loss = 0.05250750\n",
      "Iteration 44, loss = 0.05578140\n",
      "Iteration 45, loss = 0.05645572\n",
      "Iteration 46, loss = 0.04751931\n",
      "Iteration 47, loss = 0.04587934\n",
      "Iteration 1, loss = 0.81434203\n",
      "Iteration 48, loss = 0.04579956\n",
      "Iteration 2, loss = 0.43988741\n",
      "Iteration 49, loss = 0.04246075\n",
      "Iteration 3, loss = 0.37756931\n",
      "Iteration 50, loss = 0.04030193\n",
      "Iteration 4, loss = 0.34738890\n",
      "Iteration 51, loss = 0.04142371\n",
      "Iteration 5, loss = 0.31779534\n",
      "Iteration 52, loss = 0.04453203\n",
      "Iteration 6, loss = 0.27981486\n",
      "Iteration 53, loss = 0.04168579\n",
      "Iteration 7, loss = 0.26309187\n",
      "Iteration 54, loss = 0.03744083\n",
      "Iteration 8, loss = 0.25103534\n",
      "Iteration 55, loss = 0.04012863\n",
      "Iteration 9, loss = 0.24417257\n",
      "Iteration 56, loss = 0.04167805\n",
      "Iteration 10, loss = 0.22320352\n",
      "Iteration 57, loss = 0.03674185\n",
      "Iteration 11, loss = 0.21167935\n",
      "Iteration 58, loss = 0.03751022\n",
      "Iteration 12, loss = 0.21033315\n",
      "Iteration 59, loss = 0.03761676\n",
      "Iteration 13, loss = 0.18198119\n",
      "Iteration 60, loss = 0.03818777\n",
      "Iteration 14, loss = 0.17309936\n",
      "Iteration 61, loss = 0.03308981\n",
      "Iteration 15, loss = 0.16758084\n",
      "Iteration 62, loss = 0.03644502\n",
      "Iteration 16, loss = 0.15795782\n",
      "Iteration 63, loss = 0.03341057\n",
      "Iteration 17, loss = 0.16097851\n",
      "Iteration 64, loss = 0.03689174\n",
      "Iteration 18, loss = 0.14319568\n",
      "Iteration 65, loss = 0.02862480\n",
      "Iteration 19, loss = 0.13161114\n",
      "Iteration 66, loss = 0.02871331\n",
      "Iteration 20, loss = 0.12825618\n",
      "Iteration 67, loss = 0.03130940\n",
      "Iteration 21, loss = 0.11938233\n",
      "Iteration 22, loss = 0.11795077\n",
      "Iteration 68, loss = 0.03171214\n",
      "Iteration 23, loss = 0.13044643\n",
      "Iteration 69, loss = 0.03524649\n",
      "Iteration 24, loss = 0.12597980\n",
      "Iteration 70, loss = 0.02760977\n",
      "Iteration 25, loss = 0.11978065\n",
      "Iteration 71, loss = 0.03232573\n",
      "Iteration 26, loss = 0.13476384\n",
      "Iteration 72, loss = 0.03112924\n",
      "Iteration 27, loss = 0.15141524\n",
      "Iteration 73, loss = 0.04031724\n",
      "Iteration 28, loss = 0.12609045\n",
      "Iteration 74, loss = 0.02945050\n",
      "Iteration 29, loss = 0.12383330\n",
      "Iteration 75, loss = 0.03528146\n",
      "Iteration 30, loss = 0.09970092\n",
      "Iteration 76, loss = 0.03566220\n",
      "Iteration 31, loss = 0.08819676\n",
      "Iteration 77, loss = 0.02562048\n",
      "Iteration 32, loss = 0.08868131\n",
      "Iteration 78, loss = 0.02031260\n",
      "Iteration 33, loss = 0.08508100\n",
      "Iteration 79, loss = 0.02191703\n",
      "Iteration 34, loss = 0.08570934\n",
      "Iteration 80, loss = 0.02073117\n",
      "Iteration 35, loss = 0.07672437\n",
      "Iteration 81, loss = 0.02828821\n",
      "Iteration 36, loss = 0.07448044\n",
      "Iteration 82, loss = 0.06184374\n",
      "Iteration 37, loss = 0.07620074\n",
      "Iteration 83, loss = 0.05200337\n",
      "Iteration 38, loss = 0.06998724\n",
      "Iteration 84, loss = 0.04244326\n",
      "Iteration 39, loss = 0.07169813\n",
      "Iteration 85, loss = 0.05916419\n",
      "Iteration 40, loss = 0.08441182\n",
      "Iteration 86, loss = 0.04736161\n",
      "Iteration 41, loss = 0.06578725\n",
      "Iteration 87, loss = 0.03519239\n",
      "Iteration 42, loss = 0.07355409\n",
      "Iteration 88, loss = 0.03507593\n",
      "Iteration 43, loss = 0.06326460\n",
      "Iteration 89, loss = 0.04329783\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.06162889\n",
      "Iteration 45, loss = 0.06210666\n",
      "Iteration 46, loss = 0.06742107\n",
      "Iteration 47, loss = 0.05636135\n",
      "Iteration 48, loss = 0.05582540\n",
      "Iteration 49, loss = 0.05030029\n",
      "Iteration 50, loss = 0.05121471\n",
      "Iteration 51, loss = 0.04492877\n",
      "Iteration 52, loss = 0.04163104\n",
      "Iteration 53, loss = 0.04616590\n",
      "Iteration 54, loss = 0.04016101\n",
      "Iteration 55, loss = 0.04549302\n",
      "Iteration 56, loss = 0.03741698\n",
      "Iteration 57, loss = 0.03753475\n",
      "Iteration 58, loss = 0.05124043\n",
      "Iteration 59, loss = 0.04669380\n",
      "Iteration 60, loss = 0.03986984\n",
      "Iteration 61, loss = 0.05396492\n",
      "Iteration 1, loss = 0.93939563\n",
      "Iteration 62, loss = 0.05812039\n",
      "Iteration 2, loss = 0.52395065\n",
      "Iteration 63, loss = 0.09344211\n",
      "Iteration 3, loss = 0.43316854\n",
      "Iteration 64, loss = 0.05531009\n",
      "Iteration 4, loss = 0.40284875\n",
      "Iteration 65, loss = 0.06800205\n",
      "Iteration 5, loss = 0.36710227\n",
      "Iteration 66, loss = 0.04697276\n",
      "Iteration 6, loss = 0.34754260\n",
      "Iteration 67, loss = 0.03980444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.31103720\n",
      "Iteration 8, loss = 0.29222217\n",
      "Iteration 9, loss = 0.27326173\n",
      "Iteration 10, loss = 0.25850394\n",
      "Iteration 11, loss = 0.23453351\n",
      "Iteration 12, loss = 0.22232473\n",
      "Iteration 13, loss = 0.20523586\n",
      "Iteration 14, loss = 0.18916465\n",
      "Iteration 15, loss = 0.18368839\n",
      "Iteration 16, loss = 0.17458007\n",
      "Iteration 17, loss = 0.16256844\n",
      "Iteration 18, loss = 0.15281990\n",
      "Iteration 19, loss = 0.14843842\n",
      "Iteration 20, loss = 0.15902073\n",
      "Iteration 21, loss = 0.13092081\n",
      "Iteration 22, loss = 0.12213442\n",
      "Iteration 23, loss = 0.12008421\n",
      "Iteration 24, loss = 0.12277644\n",
      "Iteration 1, loss = 1.11289807\n",
      "Iteration 25, loss = 0.11714794\n",
      "Iteration 2, loss = 0.50064912\n",
      "Iteration 26, loss = 0.11944874\n",
      "Iteration 3, loss = 0.43404184\n",
      "Iteration 27, loss = 0.11377292\n",
      "Iteration 4, loss = 0.39191488\n",
      "Iteration 28, loss = 0.11104516\n",
      "Iteration 5, loss = 0.35323810\n",
      "Iteration 29, loss = 0.11214073\n",
      "Iteration 6, loss = 0.31657215\n",
      "Iteration 30, loss = 0.10331399\n",
      "Iteration 7, loss = 0.29836316\n",
      "Iteration 31, loss = 0.10938916\n",
      "Iteration 8, loss = 0.28765331\n",
      "Iteration 32, loss = 0.09028753\n",
      "Iteration 9, loss = 0.27037573\n",
      "Iteration 33, loss = 0.09732337\n",
      "Iteration 10, loss = 0.26145083\n",
      "Iteration 34, loss = 0.08144177\n",
      "Iteration 11, loss = 0.23059145\n",
      "Iteration 35, loss = 0.09274925\n",
      "Iteration 12, loss = 0.21251983\n",
      "Iteration 36, loss = 0.08133464\n",
      "Iteration 13, loss = 0.20088096\n",
      "Iteration 37, loss = 0.09187986\n",
      "Iteration 14, loss = 0.19071334\n",
      "Iteration 38, loss = 0.08098091\n",
      "Iteration 15, loss = 0.18687867\n",
      "Iteration 39, loss = 0.07383610\n",
      "Iteration 16, loss = 0.19659581\n",
      "Iteration 40, loss = 0.08853676\n",
      "Iteration 17, loss = 0.17315282\n",
      "Iteration 41, loss = 0.08977396\n",
      "Iteration 18, loss = 0.16377773\n",
      "Iteration 42, loss = 0.07014887\n",
      "Iteration 19, loss = 0.14977868\n",
      "Iteration 43, loss = 0.06512705\n",
      "Iteration 20, loss = 0.14182327\n",
      "Iteration 44, loss = 0.07179148\n",
      "Iteration 21, loss = 0.12977842\n",
      "Iteration 45, loss = 0.07135580\n",
      "Iteration 22, loss = 0.13413303\n",
      "Iteration 46, loss = 0.08793090\n",
      "Iteration 23, loss = 0.12964039\n",
      "Iteration 47, loss = 0.08750158\n",
      "Iteration 24, loss = 0.11383479\n",
      "Iteration 48, loss = 0.07095628\n",
      "Iteration 25, loss = 0.10869976\n",
      "Iteration 49, loss = 0.06674058\n",
      "Iteration 26, loss = 0.11057830\n",
      "Iteration 50, loss = 0.06318330\n",
      "Iteration 27, loss = 0.10327434\n",
      "Iteration 51, loss = 0.07474352\n",
      "Iteration 28, loss = 0.09492102\n",
      "Iteration 52, loss = 0.05820793Iteration 29, loss = 0.09872270\n",
      "\n",
      "Iteration 30, loss = 0.08451297\n",
      "Iteration 53, loss = 0.06302543\n",
      "Iteration 54, loss = 0.06814891\n",
      "Iteration 31, loss = 0.08317799\n",
      "Iteration 55, loss = 0.05658226\n",
      "Iteration 32, loss = 0.08454157\n",
      "Iteration 33, loss = 0.07986569\n",
      "Iteration 56, loss = 0.06115559\n",
      "Iteration 34, loss = 0.08537405\n",
      "Iteration 57, loss = 0.05688100\n",
      "Iteration 35, loss = 0.10886993\n",
      "Iteration 58, loss = 0.06711373\n",
      "Iteration 36, loss = 0.12030758\n",
      "Iteration 59, loss = 0.07247428\n",
      "Iteration 37, loss = 0.10193162\n",
      "Iteration 60, loss = 0.07957842\n",
      "Iteration 38, loss = 0.08416900\n",
      "Iteration 61, loss = 0.06804338\n",
      "Iteration 39, loss = 0.06996295\n",
      "Iteration 62, loss = 0.07338343\n",
      "Iteration 40, loss = 0.06767452\n",
      "Iteration 63, loss = 0.06206682\n",
      "Iteration 41, loss = 0.06608008\n",
      "Iteration 64, loss = 0.05288755\n",
      "Iteration 42, loss = 0.06777597\n",
      "Iteration 65, loss = 0.04456841\n",
      "Iteration 43, loss = 0.07043805\n",
      "Iteration 66, loss = 0.04031573\n",
      "Iteration 44, loss = 0.05127943\n",
      "Iteration 67, loss = 0.04336035\n",
      "Iteration 45, loss = 0.06021044\n",
      "Iteration 68, loss = 0.03468543\n",
      "Iteration 46, loss = 0.05550914\n",
      "Iteration 69, loss = 0.03872684\n",
      "Iteration 47, loss = 0.04759991\n",
      "Iteration 70, loss = 0.03371102\n",
      "Iteration 48, loss = 0.05641822\n",
      "Iteration 71, loss = 0.03476744\n",
      "Iteration 49, loss = 0.06282242\n",
      "Iteration 72, loss = 0.03513756\n",
      "Iteration 50, loss = 0.05954348\n",
      "Iteration 73, loss = 0.05126914\n",
      "Iteration 51, loss = 0.05044027\n",
      "Iteration 74, loss = 0.04722241\n",
      "Iteration 52, loss = 0.05793295\n",
      "Iteration 75, loss = 0.04128716\n",
      "Iteration 53, loss = 0.04301211\n",
      "Iteration 76, loss = 0.03415404\n",
      "Iteration 54, loss = 0.04150801\n",
      "Iteration 77, loss = 0.03577000\n",
      "Iteration 55, loss = 0.04378455\n",
      "Iteration 78, loss = 0.02930325\n",
      "Iteration 56, loss = 0.04942912\n",
      "Iteration 79, loss = 0.03010594\n",
      "Iteration 57, loss = 0.06904966\n",
      "Iteration 80, loss = 0.03396859\n",
      "Iteration 58, loss = 0.10533526\n",
      "Iteration 81, loss = 0.04710876\n",
      "Iteration 59, loss = 0.05420123\n",
      "Iteration 82, loss = 0.06087577\n",
      "Iteration 60, loss = 0.05968370\n",
      "Iteration 83, loss = 0.03850823\n",
      "Iteration 61, loss = 0.04607615\n",
      "Iteration 84, loss = 0.04420161\n",
      "Iteration 62, loss = 0.04032918\n",
      "Iteration 85, loss = 0.04305184\n",
      "Iteration 63, loss = 0.03369011\n",
      "Iteration 86, loss = 0.05250301\n",
      "Iteration 64, loss = 0.05729229\n",
      "Iteration 87, loss = 0.05790609\n",
      "Iteration 65, loss = 0.05300304\n",
      "Iteration 88, loss = 0.04944578\n",
      "Iteration 66, loss = 0.04477328\n",
      "Iteration 89, loss = 0.03177358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 67, loss = 0.03669182\n",
      "Iteration 68, loss = 0.02748182\n",
      "Iteration 69, loss = 0.02583666\n",
      "Iteration 70, loss = 0.02595670\n",
      "Iteration 71, loss = 0.03724232\n",
      "Iteration 72, loss = 0.03129960\n",
      "Iteration 73, loss = 0.03145836\n",
      "Iteration 74, loss = 0.02512336\n",
      "Iteration 75, loss = 0.02161739\n",
      "Iteration 76, loss = 0.01998664\n",
      "Iteration 77, loss = 0.02022294\n",
      "Iteration 78, loss = 0.01676214\n",
      "Iteration 79, loss = 0.01458480\n",
      "Iteration 80, loss = 0.01495267\n",
      "Iteration 81, loss = 0.01523028\n",
      "Iteration 82, loss = 0.01515565\n",
      "Iteration 83, loss = 0.01557153\n",
      "Iteration 84, loss = 0.01521039\n",
      "Iteration 85, loss = 0.01821071\n",
      "Iteration 1, loss = 0.90655699\n",
      "Iteration 86, loss = 0.01325546\n",
      "Iteration 2, loss = 0.50877850\n",
      "Iteration 87, loss = 0.01632400\n",
      "Iteration 3, loss = 0.42502376\n",
      "Iteration 88, loss = 0.02265263\n",
      "Iteration 4, loss = 0.40192435\n",
      "Iteration 89, loss = 0.01495315\n",
      "Iteration 5, loss = 0.35902426\n",
      "Iteration 90, loss = 0.01237586\n",
      "Iteration 6, loss = 0.32406984\n",
      "Iteration 91, loss = 0.01550316\n",
      "Iteration 7, loss = 0.30595833\n",
      "Iteration 92, loss = 0.01096261\n",
      "Iteration 8, loss = 0.28478973\n",
      "Iteration 93, loss = 0.01240476\n",
      "Iteration 9, loss = 0.25819232\n",
      "Iteration 94, loss = 0.01158693\n",
      "Iteration 10, loss = 0.24540431\n",
      "Iteration 95, loss = 0.02291511\n",
      "Iteration 11, loss = 0.22637254\n",
      "Iteration 96, loss = 0.02507195\n",
      "Iteration 12, loss = 0.21249956\n",
      "Iteration 97, loss = 0.19258995\n",
      "Iteration 13, loss = 0.20455617\n",
      "Iteration 98, loss = 0.20993618\n",
      "Iteration 14, loss = 0.19143559\n",
      "Iteration 99, loss = 0.22669278\n",
      "Iteration 15, loss = 0.18148107\n",
      "Iteration 100, loss = 0.11672361\n",
      "Iteration 16, loss = 0.16120962\n",
      "Iteration 17, loss = 0.15818959\n",
      "Iteration 18, loss = 0.15007776\n",
      "Iteration 19, loss = 0.14055493\n",
      "Iteration 20, loss = 0.13697198\n",
      "Iteration 21, loss = 0.14146321\n",
      "Iteration 22, loss = 0.13933593\n",
      "Iteration 23, loss = 0.13684812\n",
      "Iteration 24, loss = 0.12041547\n",
      "Iteration 25, loss = 0.12282712\n",
      "Iteration 26, loss = 0.12391845\n",
      "Iteration 27, loss = 0.12522169\n",
      "Iteration 28, loss = 0.12618533\n",
      "Iteration 29, loss = 0.11710842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.11909052\n",
      "Iteration 31, loss = 0.12546428\n",
      "Iteration 32, loss = 0.11571303\n",
      "Iteration 33, loss = 0.10771876\n",
      "Iteration 34, loss = 0.12658477\n",
      "Iteration 1, loss = 0.92815273\n",
      "Iteration 35, loss = 0.13160482\n",
      "Iteration 2, loss = 0.49470122\n",
      "Iteration 36, loss = 0.10566373\n",
      "Iteration 3, loss = 0.40357639\n",
      "Iteration 37, loss = 0.09825284\n",
      "Iteration 4, loss = 0.36487335\n",
      "Iteration 38, loss = 0.11454116\n",
      "Iteration 5, loss = 0.32881792\n",
      "Iteration 39, loss = 0.09490969\n",
      "Iteration 6, loss = 0.29998567\n",
      "Iteration 40, loss = 0.08154326\n",
      "Iteration 7, loss = 0.27046854\n",
      "Iteration 41, loss = 0.08267355\n",
      "Iteration 8, loss = 0.25974819\n",
      "Iteration 42, loss = 0.08065322\n",
      "Iteration 9, loss = 0.24021428\n",
      "Iteration 43, loss = 0.08304087\n",
      "Iteration 10, loss = 0.21827685\n",
      "Iteration 44, loss = 0.10686007\n",
      "Iteration 11, loss = 0.20607095\n",
      "Iteration 45, loss = 0.11892581\n",
      "Iteration 12, loss = 0.19917156\n",
      "Iteration 46, loss = 0.07479691\n",
      "Iteration 13, loss = 0.19363528\n",
      "Iteration 47, loss = 0.08410725\n",
      "Iteration 14, loss = 0.19397776\n",
      "Iteration 48, loss = 0.07320604\n",
      "Iteration 15, loss = 0.18519045\n",
      "Iteration 49, loss = 0.06961004\n",
      "Iteration 16, loss = 0.17979181\n",
      "Iteration 50, loss = 0.06720958\n",
      "Iteration 17, loss = 0.17617972\n",
      "Iteration 51, loss = 0.06485991\n",
      "Iteration 18, loss = 0.16309392\n",
      "Iteration 52, loss = 0.06250046\n",
      "Iteration 19, loss = 0.16502010\n",
      "Iteration 20, loss = 0.18050789\n",
      "Iteration 53, loss = 0.06014628\n",
      "Iteration 54, loss = 0.06471626\n",
      "Iteration 21, loss = 0.15223380\n",
      "Iteration 22, loss = 0.16771392\n",
      "Iteration 55, loss = 0.08033636\n",
      "Iteration 23, loss = 0.14217482\n",
      "Iteration 56, loss = 0.06404754\n",
      "Iteration 24, loss = 0.12667568\n",
      "Iteration 57, loss = 0.06302735\n",
      "Iteration 25, loss = 0.12240688\n",
      "Iteration 58, loss = 0.07720838\n",
      "Iteration 26, loss = 0.10947074\n",
      "Iteration 59, loss = 0.05627216\n",
      "Iteration 27, loss = 0.11207698\n",
      "Iteration 60, loss = 0.05445566\n",
      "Iteration 28, loss = 0.10212499\n",
      "Iteration 61, loss = 0.05569767\n",
      "Iteration 29, loss = 0.10766733\n",
      "Iteration 62, loss = 0.04707845\n",
      "Iteration 30, loss = 0.10361503\n",
      "Iteration 63, loss = 0.05302176\n",
      "Iteration 31, loss = 0.10381465\n",
      "Iteration 64, loss = 0.04828639\n",
      "Iteration 32, loss = 0.09887851\n",
      "Iteration 65, loss = 0.05100353\n",
      "Iteration 33, loss = 0.09746773\n",
      "Iteration 66, loss = 0.04545578\n",
      "Iteration 67, loss = 0.04680324\n",
      "Iteration 34, loss = 0.09161730\n",
      "Iteration 68, loss = 0.04144447\n",
      "Iteration 35, loss = 0.09828834\n",
      "Iteration 69, loss = 0.04072604Iteration 36, loss = 0.08158589\n",
      "\n",
      "Iteration 70, loss = 0.03883186\n",
      "Iteration 37, loss = 0.08081458\n",
      "Iteration 71, loss = 0.03812806\n",
      "Iteration 38, loss = 0.08363202\n",
      "Iteration 72, loss = 0.04129969\n",
      "Iteration 39, loss = 0.11124919\n",
      "Iteration 73, loss = 0.05252319\n",
      "Iteration 40, loss = 0.10208375\n",
      "Iteration 74, loss = 0.05400967\n",
      "Iteration 41, loss = 0.13723145\n",
      "Iteration 75, loss = 0.04250572\n",
      "Iteration 42, loss = 0.13678054\n",
      "Iteration 76, loss = 0.04323396\n",
      "Iteration 43, loss = 0.12058092\n",
      "Iteration 77, loss = 0.04722000\n",
      "Iteration 44, loss = 0.11656049\n",
      "Iteration 78, loss = 0.05116844\n",
      "Iteration 45, loss = 0.09867198\n",
      "Iteration 79, loss = 0.04363702\n",
      "Iteration 46, loss = 0.10232511\n",
      "Iteration 80, loss = 0.04187976\n",
      "Iteration 47, loss = 0.09898290\n",
      "Iteration 48, loss = 0.07663243Iteration 81, loss = 0.04938377\n",
      "\n",
      "Iteration 49, loss = 0.06074157Iteration 82, loss = 0.03488961\n",
      "\n",
      "Iteration 50, loss = 0.06358993\n",
      "Iteration 83, loss = 0.04318674\n",
      "Iteration 84, loss = 0.03040869\n",
      "Iteration 51, loss = 0.05519226\n",
      "Iteration 52, loss = 0.05552427\n",
      "Iteration 85, loss = 0.03239141\n",
      "Iteration 53, loss = 0.06524934\n",
      "Iteration 86, loss = 0.03368994\n",
      "Iteration 54, loss = 0.05700319\n",
      "Iteration 87, loss = 0.03465629\n",
      "Iteration 55, loss = 0.06218646\n",
      "Iteration 88, loss = 0.03846211\n",
      "Iteration 56, loss = 0.05948928\n",
      "Iteration 89, loss = 0.06875809\n",
      "Iteration 57, loss = 0.06903091\n",
      "Iteration 90, loss = 0.06017079\n",
      "Iteration 58, loss = 0.05319517\n",
      "Iteration 91, loss = 0.12790681\n",
      "Iteration 59, loss = 0.06667419\n",
      "Iteration 92, loss = 0.15264947\n",
      "Iteration 93, loss = 0.12955522\n",
      "Iteration 60, loss = 0.06426837\n",
      "Iteration 94, loss = 0.14244927\n",
      "Iteration 61, loss = 0.07792706\n",
      "Iteration 95, loss = 0.16599638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 62, loss = 0.06650017\n",
      "Iteration 63, loss = 0.05685075\n",
      "Iteration 64, loss = 0.06502698\n",
      "Iteration 65, loss = 0.05171741\n",
      "Iteration 66, loss = 0.05319626\n",
      "Iteration 67, loss = 0.04026902\n",
      "Iteration 68, loss = 0.03810264\n",
      "Iteration 69, loss = 0.04117741\n",
      "Iteration 70, loss = 0.03531411\n",
      "Iteration 71, loss = 0.03702386\n",
      "Iteration 72, loss = 0.03786234\n",
      "Iteration 73, loss = 0.03850127\n",
      "Iteration 74, loss = 0.03749517\n",
      "Iteration 75, loss = 0.03245066\n",
      "Iteration 76, loss = 0.03231092\n",
      "Iteration 77, loss = 0.02825936\n",
      "Iteration 78, loss = 0.03078684\n",
      "Iteration 79, loss = 0.03254758\n",
      "Iteration 80, loss = 0.03908317\n",
      "Iteration 1, loss = 1.03627877\n",
      "Iteration 81, loss = 0.03721467\n",
      "Iteration 2, loss = 0.53880553\n",
      "Iteration 82, loss = 0.03944965\n",
      "Iteration 3, loss = 0.42813747\n",
      "Iteration 83, loss = 0.06558902\n",
      "Iteration 4, loss = 0.37677022\n",
      "Iteration 84, loss = 0.04912280\n",
      "Iteration 5, loss = 0.34202471\n",
      "Iteration 85, loss = 0.06002406\n",
      "Iteration 6, loss = 0.32233356\n",
      "Iteration 86, loss = 0.05749998\n",
      "Iteration 7, loss = 0.29113425\n",
      "Iteration 87, loss = 0.04239519\n",
      "Iteration 8, loss = 0.27690813\n",
      "Iteration 88, loss = 0.05339676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.27019977\n",
      "Iteration 10, loss = 0.24641303\n",
      "Iteration 11, loss = 0.23401431\n",
      "Iteration 12, loss = 0.21748843\n",
      "Iteration 13, loss = 0.20945344\n",
      "Iteration 14, loss = 0.18881629\n",
      "Iteration 15, loss = 0.17833513\n",
      "Iteration 16, loss = 0.16770401\n",
      "Iteration 17, loss = 0.16038097\n",
      "Iteration 18, loss = 0.15434481\n",
      "Iteration 19, loss = 0.14468923\n",
      "Iteration 20, loss = 0.14990735\n",
      "Iteration 21, loss = 0.14152960\n",
      "Iteration 22, loss = 0.13523281\n",
      "Iteration 23, loss = 0.11965095\n",
      "Iteration 24, loss = 0.11144525\n",
      "Iteration 25, loss = 0.12141495\n",
      "Iteration 26, loss = 0.13670105\n",
      "Iteration 27, loss = 0.11619131\n",
      "Iteration 1, loss = 0.90571127\n",
      "Iteration 28, loss = 0.12387315\n",
      "Iteration 2, loss = 0.50150888\n",
      "Iteration 29, loss = 0.12714064\n",
      "Iteration 3, loss = 0.41314769\n",
      "Iteration 30, loss = 0.10601717\n",
      "Iteration 4, loss = 0.37410124\n",
      "Iteration 31, loss = 0.11465065\n",
      "Iteration 5, loss = 0.33955109\n",
      "Iteration 32, loss = 0.11775215\n",
      "Iteration 6, loss = 0.31003448\n",
      "Iteration 33, loss = 0.10405368\n",
      "Iteration 7, loss = 0.29488927\n",
      "Iteration 34, loss = 0.10079830\n",
      "Iteration 8, loss = 0.26581570\n",
      "Iteration 35, loss = 0.11794132\n",
      "Iteration 9, loss = 0.24809603\n",
      "Iteration 36, loss = 0.11624183\n",
      "Iteration 10, loss = 0.23358708\n",
      "Iteration 37, loss = 0.12693128\n",
      "Iteration 11, loss = 0.21613151\n",
      "Iteration 38, loss = 0.09872846\n",
      "Iteration 12, loss = 0.20716836\n",
      "Iteration 39, loss = 0.10112058\n",
      "Iteration 13, loss = 0.19878666\n",
      "Iteration 40, loss = 0.09318278\n",
      "Iteration 14, loss = 0.18698089\n",
      "Iteration 41, loss = 0.08465439\n",
      "Iteration 15, loss = 0.18345094\n",
      "Iteration 42, loss = 0.07016728\n",
      "Iteration 16, loss = 0.16454975\n",
      "Iteration 43, loss = 0.07199949\n",
      "Iteration 17, loss = 0.15737474\n",
      "Iteration 44, loss = 0.07473498\n",
      "Iteration 18, loss = 0.14854585\n",
      "Iteration 45, loss = 0.09701927\n",
      "Iteration 19, loss = 0.15077598\n",
      "Iteration 46, loss = 0.08457304\n",
      "Iteration 20, loss = 0.13675100\n",
      "Iteration 21, loss = 0.13195479\n",
      "Iteration 47, loss = 0.08406464\n",
      "Iteration 22, loss = 0.12594688\n",
      "Iteration 48, loss = 0.07483575\n",
      "Iteration 23, loss = 0.12852564\n",
      "Iteration 49, loss = 0.07511031\n",
      "Iteration 24, loss = 0.13655543\n",
      "Iteration 50, loss = 0.07928641\n",
      "Iteration 25, loss = 0.14312450\n",
      "Iteration 51, loss = 0.09534408\n",
      "Iteration 52, loss = 0.06468302\n",
      "Iteration 26, loss = 0.14720490\n",
      "Iteration 53, loss = 0.06241923\n",
      "Iteration 27, loss = 0.14167580\n",
      "Iteration 54, loss = 0.05902336\n",
      "Iteration 28, loss = 0.12466271\n",
      "Iteration 55, loss = 0.06056393\n",
      "Iteration 29, loss = 0.13619366\n",
      "Iteration 56, loss = 0.05495682\n",
      "Iteration 30, loss = 0.11716141\n",
      "Iteration 57, loss = 0.06194259\n",
      "Iteration 31, loss = 0.11042477\n",
      "Iteration 58, loss = 0.06004383\n",
      "Iteration 32, loss = 0.09648026\n",
      "Iteration 59, loss = 0.05057799\n",
      "Iteration 33, loss = 0.08942926\n",
      "Iteration 60, loss = 0.05527400\n",
      "Iteration 34, loss = 0.09553562\n",
      "Iteration 61, loss = 0.04912080\n",
      "Iteration 35, loss = 0.10322509\n",
      "Iteration 62, loss = 0.04668995\n",
      "Iteration 36, loss = 0.09105543\n",
      "Iteration 63, loss = 0.05708775\n",
      "Iteration 37, loss = 0.09536656\n",
      "Iteration 64, loss = 0.05972825\n",
      "Iteration 38, loss = 0.08380656\n",
      "Iteration 65, loss = 0.06760398\n",
      "Iteration 39, loss = 0.07186242\n",
      "Iteration 66, loss = 0.09049016\n",
      "Iteration 40, loss = 0.07323250\n",
      "Iteration 67, loss = 0.07144789\n",
      "Iteration 41, loss = 0.07220889\n",
      "Iteration 68, loss = 0.11318771\n",
      "Iteration 42, loss = 0.07199578\n",
      "Iteration 69, loss = 0.08937609\n",
      "Iteration 43, loss = 0.07489613\n",
      "Iteration 70, loss = 0.05658934\n",
      "Iteration 44, loss = 0.07167326\n",
      "Iteration 71, loss = 0.05018741\n",
      "Iteration 45, loss = 0.10399268\n",
      "Iteration 72, loss = 0.06329696\n",
      "Iteration 46, loss = 0.07253593\n",
      "Iteration 73, loss = 0.04885558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.08043484\n",
      "Iteration 48, loss = 0.13234197\n",
      "Iteration 49, loss = 0.09745810\n",
      "Iteration 50, loss = 0.09554119\n",
      "Iteration 51, loss = 0.09448939\n",
      "Iteration 52, loss = 0.09289229\n",
      "Iteration 53, loss = 0.08704600\n",
      "Iteration 54, loss = 0.08647561\n",
      "Iteration 55, loss = 0.06993748\n",
      "Iteration 56, loss = 0.08099384\n",
      "Iteration 57, loss = 0.05452939\n",
      "Iteration 58, loss = 0.05372905\n",
      "Iteration 59, loss = 0.05697521\n",
      "Iteration 60, loss = 0.05245689\n",
      "Iteration 61, loss = 0.05426186\n",
      "Iteration 62, loss = 0.05129735\n",
      "Iteration 63, loss = 0.05612387\n",
      "Iteration 64, loss = 0.05955218\n",
      "Iteration 65, loss = 0.08923748\n",
      "Iteration 1, loss = 0.77934211\n",
      "Iteration 2, loss = 0.43540411\n",
      "Iteration 66, loss = 0.05725262\n",
      "Iteration 3, loss = 0.36202363\n",
      "Iteration 67, loss = 0.06849920\n",
      "Iteration 4, loss = 0.33496789\n",
      "Iteration 68, loss = 0.11224435\n",
      "Iteration 5, loss = 0.30133479\n",
      "Iteration 69, loss = 0.10006360\n",
      "Iteration 6, loss = 0.28078142\n",
      "Iteration 70, loss = 0.09841909\n",
      "Iteration 7, loss = 0.26256595\n",
      "Iteration 71, loss = 0.07072050\n",
      "Iteration 8, loss = 0.24003104\n",
      "Iteration 72, loss = 0.08227728\n",
      "Iteration 9, loss = 0.22529885\n",
      "Iteration 73, loss = 0.08269293\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.21176297\n",
      "Iteration 11, loss = 0.20429787\n",
      "Iteration 12, loss = 0.19546642\n",
      "Iteration 13, loss = 0.18391915\n",
      "Iteration 14, loss = 0.18331849\n",
      "Iteration 15, loss = 0.16395900\n",
      "Iteration 16, loss = 0.16196572\n",
      "Iteration 17, loss = 0.16912534\n",
      "Iteration 18, loss = 0.14095326\n",
      "Iteration 19, loss = 0.14080668\n",
      "Iteration 20, loss = 0.13016229\n",
      "Iteration 21, loss = 0.12650659\n",
      "Iteration 22, loss = 0.11866845\n",
      "Iteration 23, loss = 0.13090341\n",
      "Iteration 24, loss = 0.12861996\n",
      "Iteration 25, loss = 0.11087837\n",
      "Iteration 26, loss = 0.10838715\n",
      "Iteration 27, loss = 0.10541526\n",
      "Iteration 28, loss = 0.09868227\n",
      "Iteration 29, loss = 0.08898563\n",
      "Iteration 1, loss = 0.88611217\n",
      "Iteration 30, loss = 0.08799014\n",
      "Iteration 2, loss = 0.47723829\n",
      "Iteration 31, loss = 0.08113961\n",
      "Iteration 3, loss = 0.42297946\n",
      "Iteration 32, loss = 0.08277117\n",
      "Iteration 4, loss = 0.37733170\n",
      "Iteration 33, loss = 0.08109851\n",
      "Iteration 5, loss = 0.34698340\n",
      "Iteration 34, loss = 0.08019581\n",
      "Iteration 6, loss = 0.30831175\n",
      "Iteration 35, loss = 0.08843407\n",
      "Iteration 7, loss = 0.28223423\n",
      "Iteration 36, loss = 0.10200465\n",
      "Iteration 8, loss = 0.25972893\n",
      "Iteration 37, loss = 0.13568478\n",
      "Iteration 9, loss = 0.24399972\n",
      "Iteration 38, loss = 0.13696837\n",
      "Iteration 10, loss = 0.23132265\n",
      "Iteration 39, loss = 0.10905983\n",
      "Iteration 11, loss = 0.21572209\n",
      "Iteration 40, loss = 0.10481319\n",
      "Iteration 12, loss = 0.20659704\n",
      "Iteration 41, loss = 0.09813527\n",
      "Iteration 13, loss = 0.19398996\n",
      "Iteration 42, loss = 0.09458219\n",
      "Iteration 14, loss = 0.17991018\n",
      "Iteration 43, loss = 0.08364857\n",
      "Iteration 15, loss = 0.16421393\n",
      "Iteration 44, loss = 0.06467076\n",
      "Iteration 16, loss = 0.17295470\n",
      "Iteration 45, loss = 0.07090349\n",
      "Iteration 17, loss = 0.19441200\n",
      "Iteration 46, loss = 0.06959622\n",
      "Iteration 18, loss = 0.16453994\n",
      "Iteration 47, loss = 0.06263315\n",
      "Iteration 19, loss = 0.17451601\n",
      "Iteration 48, loss = 0.06574224\n",
      "Iteration 20, loss = 0.16065335\n",
      "Iteration 49, loss = 0.06253258\n",
      "Iteration 21, loss = 0.14569715\n",
      "Iteration 50, loss = 0.06031183\n",
      "Iteration 22, loss = 0.14469289\n",
      "Iteration 51, loss = 0.05255538\n",
      "Iteration 23, loss = 0.16062425\n",
      "Iteration 52, loss = 0.05214461\n",
      "Iteration 24, loss = 0.14544541\n",
      "Iteration 53, loss = 0.04963437\n",
      "Iteration 25, loss = 0.12720850\n",
      "Iteration 54, loss = 0.05298100\n",
      "Iteration 26, loss = 0.12153345\n",
      "Iteration 55, loss = 0.04761819\n",
      "Iteration 27, loss = 0.11006060\n",
      "Iteration 56, loss = 0.07190687\n",
      "Iteration 28, loss = 0.12684622\n",
      "Iteration 57, loss = 0.08058440\n",
      "Iteration 29, loss = 0.11146050\n",
      "Iteration 58, loss = 0.07176987\n",
      "Iteration 30, loss = 0.10182799\n",
      "Iteration 59, loss = 0.06192903\n",
      "Iteration 31, loss = 0.10262625\n",
      "Iteration 60, loss = 0.05023609\n",
      "Iteration 32, loss = 0.09935362\n",
      "Iteration 61, loss = 0.05734808\n",
      "Iteration 33, loss = 0.10920153\n",
      "Iteration 62, loss = 0.05189596\n",
      "Iteration 34, loss = 0.10855816\n",
      "Iteration 63, loss = 0.04265308\n",
      "Iteration 35, loss = 0.10606091\n",
      "Iteration 64, loss = 0.04131090\n",
      "Iteration 36, loss = 0.11574624\n",
      "Iteration 65, loss = 0.04495668\n",
      "Iteration 37, loss = 0.14445194\n",
      "Iteration 66, loss = 0.05280533\n",
      "Iteration 38, loss = 0.10359806\n",
      "Iteration 67, loss = 0.04746438\n",
      "Iteration 39, loss = 0.09172192\n",
      "Iteration 68, loss = 0.03844520\n",
      "Iteration 40, loss = 0.07850970\n",
      "Iteration 69, loss = 0.05937117\n",
      "Iteration 41, loss = 0.06777974\n",
      "Iteration 70, loss = 0.04509776\n",
      "Iteration 42, loss = 0.06852858\n",
      "Iteration 71, loss = 0.03339954\n",
      "Iteration 43, loss = 0.06585126\n",
      "Iteration 72, loss = 0.04213855\n",
      "Iteration 44, loss = 0.06464607\n",
      "Iteration 73, loss = 0.04960805\n",
      "Iteration 45, loss = 0.06492920\n",
      "Iteration 74, loss = 0.04096366\n",
      "Iteration 46, loss = 0.06154017\n",
      "Iteration 75, loss = 0.03305791\n",
      "Iteration 47, loss = 0.05439981\n",
      "Iteration 76, loss = 0.04330601\n",
      "Iteration 48, loss = 0.05500103\n",
      "Iteration 77, loss = 0.04955197\n",
      "Iteration 49, loss = 0.06076373\n",
      "Iteration 78, loss = 0.05458953\n",
      "Iteration 50, loss = 0.07066467\n",
      "Iteration 79, loss = 0.05819651\n",
      "Iteration 51, loss = 0.07162170\n",
      "Iteration 80, loss = 0.06692694\n",
      "Iteration 52, loss = 0.06033811\n",
      "Iteration 81, loss = 0.06356879\n",
      "Iteration 53, loss = 0.05642950\n",
      "Iteration 82, loss = 0.05622967\n",
      "Iteration 54, loss = 0.05346594\n",
      "Iteration 83, loss = 0.06701530\n",
      "Iteration 55, loss = 0.04919271\n",
      "Iteration 84, loss = 0.05950031\n",
      "Iteration 56, loss = 0.05572559\n",
      "Iteration 85, loss = 0.04217800\n",
      "Iteration 57, loss = 0.04889185\n",
      "Iteration 86, loss = 0.05058934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.04716049\n",
      "Iteration 59, loss = 0.03939911\n",
      "Iteration 60, loss = 0.04635025\n",
      "Iteration 61, loss = 0.06041463\n",
      "Iteration 62, loss = 0.06050021\n",
      "Iteration 63, loss = 0.07293049\n",
      "Iteration 64, loss = 0.08716991\n",
      "Iteration 65, loss = 0.08129725\n",
      "Iteration 66, loss = 0.07994480\n",
      "Iteration 67, loss = 0.06759238\n",
      "Iteration 68, loss = 0.11444539\n",
      "Iteration 69, loss = 0.09515960\n",
      "Iteration 70, loss = 0.07992065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96854700\n",
      "Iteration 2, loss = 0.49609393\n",
      "Iteration 3, loss = 0.40824863\n",
      "Iteration 4, loss = 0.36256724\n",
      "Iteration 5, loss = 0.32532749\n",
      "Iteration 6, loss = 0.29769729\n",
      "Iteration 7, loss = 0.27710245\n",
      "Iteration 8, loss = 0.25157982\n",
      "Iteration 9, loss = 0.23372391\n",
      "Iteration 10, loss = 0.21519856\n",
      "Iteration 11, loss = 0.20104936\n",
      "Iteration 12, loss = 0.19039508\n",
      "Iteration 13, loss = 0.17464279\n",
      "Iteration 14, loss = 0.16476079\n",
      "Iteration 15, loss = 0.16384524\n",
      "Iteration 16, loss = 0.15238768\n",
      "Iteration 17, loss = 0.12494442\n",
      "Iteration 1, loss = 0.87427248\n",
      "Iteration 18, loss = 0.12293684\n",
      "Iteration 2, loss = 0.47959010\n",
      "Iteration 19, loss = 0.11299489\n",
      "Iteration 3, loss = 0.39300012\n",
      "Iteration 20, loss = 0.11262669\n",
      "Iteration 4, loss = 0.34830327\n",
      "Iteration 21, loss = 0.10791888\n",
      "Iteration 5, loss = 0.32048496\n",
      "Iteration 22, loss = 0.10529530\n",
      "Iteration 6, loss = 0.29115257\n",
      "Iteration 23, loss = 0.09740651\n",
      "Iteration 7, loss = 0.26697223\n",
      "Iteration 24, loss = 0.08582388\n",
      "Iteration 8, loss = 0.24570472\n",
      "Iteration 25, loss = 0.08708376\n",
      "Iteration 9, loss = 0.24068898\n",
      "Iteration 26, loss = 0.08260905\n",
      "Iteration 10, loss = 0.21299596\n",
      "Iteration 27, loss = 0.07687234\n",
      "Iteration 11, loss = 0.19799315\n",
      "Iteration 28, loss = 0.07798441\n",
      "Iteration 12, loss = 0.18674577\n",
      "Iteration 29, loss = 0.08461017\n",
      "Iteration 13, loss = 0.17949946\n",
      "Iteration 30, loss = 0.07986193\n",
      "Iteration 14, loss = 0.16424031\n",
      "Iteration 31, loss = 0.07793932\n",
      "Iteration 15, loss = 0.16490291\n",
      "Iteration 32, loss = 0.11048626\n",
      "Iteration 16, loss = 0.15471147\n",
      "Iteration 33, loss = 0.08524127\n",
      "Iteration 17, loss = 0.14888941\n",
      "Iteration 34, loss = 0.06816488\n",
      "Iteration 18, loss = 0.14315185\n",
      "Iteration 35, loss = 0.07157503\n",
      "Iteration 19, loss = 0.15782567\n",
      "Iteration 36, loss = 0.08661816\n",
      "Iteration 20, loss = 0.15854979\n",
      "Iteration 37, loss = 0.07331127\n",
      "Iteration 21, loss = 0.14234083\n",
      "Iteration 38, loss = 0.06417682\n",
      "Iteration 22, loss = 0.13186274\n",
      "Iteration 39, loss = 0.05435214\n",
      "Iteration 23, loss = 0.13429366\n",
      "Iteration 24, loss = 0.11569597\n",
      "Iteration 40, loss = 0.05143602\n",
      "Iteration 25, loss = 0.11167783\n",
      "Iteration 41, loss = 0.04746542\n",
      "Iteration 26, loss = 0.11533346\n",
      "Iteration 42, loss = 0.04128662\n",
      "Iteration 27, loss = 0.11718256\n",
      "Iteration 43, loss = 0.04131218\n",
      "Iteration 28, loss = 0.10259948\n",
      "Iteration 44, loss = 0.03936167\n",
      "Iteration 29, loss = 0.11133516\n",
      "Iteration 45, loss = 0.05776983\n",
      "Iteration 30, loss = 0.08887997\n",
      "Iteration 46, loss = 0.06811869\n",
      "Iteration 31, loss = 0.08710260\n",
      "Iteration 47, loss = 0.04783959\n",
      "Iteration 32, loss = 0.08333535\n",
      "Iteration 48, loss = 0.04795512\n",
      "Iteration 33, loss = 0.07868631\n",
      "Iteration 49, loss = 0.05041243\n",
      "Iteration 34, loss = 0.08648422\n",
      "Iteration 50, loss = 0.04843881\n",
      "Iteration 35, loss = 0.08463627\n",
      "Iteration 51, loss = 0.04986026\n",
      "Iteration 36, loss = 0.08353922\n",
      "Iteration 52, loss = 0.04322144\n",
      "Iteration 37, loss = 0.07449662\n",
      "Iteration 53, loss = 0.03716445\n",
      "Iteration 38, loss = 0.07652821\n",
      "Iteration 54, loss = 0.03672415\n",
      "Iteration 39, loss = 0.07155636\n",
      "Iteration 55, loss = 0.04053477\n",
      "Iteration 40, loss = 0.05673300\n",
      "Iteration 56, loss = 0.03403036\n",
      "Iteration 41, loss = 0.05787001\n",
      "Iteration 57, loss = 0.04010675\n",
      "Iteration 42, loss = 0.05484187\n",
      "Iteration 58, loss = 0.08218076\n",
      "Iteration 43, loss = 0.05398276\n",
      "Iteration 59, loss = 0.05621994\n",
      "Iteration 44, loss = 0.05668117\n",
      "Iteration 60, loss = 0.06813916\n",
      "Iteration 45, loss = 0.05990885\n",
      "Iteration 61, loss = 0.04597154\n",
      "Iteration 46, loss = 0.06380749\n",
      "Iteration 62, loss = 0.04690976\n",
      "Iteration 47, loss = 0.08879593\n",
      "Iteration 63, loss = 0.05300640\n",
      "Iteration 48, loss = 0.15982840\n",
      "Iteration 64, loss = 0.03960023\n",
      "Iteration 49, loss = 0.21412669\n",
      "Iteration 65, loss = 0.04076262\n",
      "Iteration 50, loss = 0.15960070\n",
      "Iteration 66, loss = 0.03718980\n",
      "Iteration 51, loss = 0.16803156\n",
      "Iteration 67, loss = 0.03148663\n",
      "Iteration 52, loss = 0.11967266\n",
      "Iteration 68, loss = 0.04481921\n",
      "Iteration 53, loss = 0.12138432\n",
      "Iteration 69, loss = 0.03798746\n",
      "Iteration 54, loss = 0.07550024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 70, loss = 0.06889110\n",
      "Iteration 71, loss = 0.06120677\n",
      "Iteration 72, loss = 0.04474012\n",
      "Iteration 73, loss = 0.04006077\n",
      "Iteration 74, loss = 0.04695957\n",
      "Iteration 75, loss = 0.06405202\n",
      "Iteration 76, loss = 0.05923070\n",
      "Iteration 77, loss = 0.05623033\n",
      "Iteration 78, loss = 0.06574371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72367682\n",
      "Iteration 2, loss = 0.45502489\n",
      "Iteration 3, loss = 0.39610267\n",
      "Iteration 4, loss = 0.35810658\n",
      "Iteration 5, loss = 0.32185666\n",
      "Iteration 6, loss = 0.29551345\n",
      "Iteration 7, loss = 0.28242682\n",
      "Iteration 8, loss = 0.27720856\n",
      "Iteration 9, loss = 0.26333508\n",
      "Iteration 1, loss = 1.06184329\n",
      "Iteration 10, loss = 0.24803843\n",
      "Iteration 2, loss = 0.55545723\n",
      "Iteration 11, loss = 0.21790102\n",
      "Iteration 3, loss = 0.43374201\n",
      "Iteration 12, loss = 0.20112514\n",
      "Iteration 4, loss = 0.38561189\n",
      "Iteration 13, loss = 0.20343980\n",
      "Iteration 5, loss = 0.35726843\n",
      "Iteration 14, loss = 0.18018404\n",
      "Iteration 6, loss = 0.32553539\n",
      "Iteration 15, loss = 0.17391211\n",
      "Iteration 7, loss = 0.30131685\n",
      "Iteration 16, loss = 0.16781922\n",
      "Iteration 8, loss = 0.27419715\n",
      "Iteration 17, loss = 0.16373280\n",
      "Iteration 9, loss = 0.24813465\n",
      "Iteration 18, loss = 0.15085961\n",
      "Iteration 10, loss = 0.23684362\n",
      "Iteration 19, loss = 0.14599496\n",
      "Iteration 11, loss = 0.22151081\n",
      "Iteration 20, loss = 0.14538512\n",
      "Iteration 12, loss = 0.20608271\n",
      "Iteration 21, loss = 0.13233127\n",
      "Iteration 13, loss = 0.19820927\n",
      "Iteration 22, loss = 0.13547163\n",
      "Iteration 14, loss = 0.18753079\n",
      "Iteration 23, loss = 0.13669088\n",
      "Iteration 15, loss = 0.18641962\n",
      "Iteration 24, loss = 0.13036733\n",
      "Iteration 16, loss = 0.16414283\n",
      "Iteration 25, loss = 0.13031511\n",
      "Iteration 17, loss = 0.15525625\n",
      "Iteration 26, loss = 0.14421150\n",
      "Iteration 18, loss = 0.14721784\n",
      "Iteration 27, loss = 0.11159782\n",
      "Iteration 19, loss = 0.14587692\n",
      "Iteration 28, loss = 0.10788011\n",
      "Iteration 20, loss = 0.14627475\n",
      "Iteration 29, loss = 0.10053721\n",
      "Iteration 21, loss = 0.12943700\n",
      "Iteration 30, loss = 0.08746126\n",
      "Iteration 22, loss = 0.12483221\n",
      "Iteration 31, loss = 0.08439487\n",
      "Iteration 23, loss = 0.11684093\n",
      "Iteration 32, loss = 0.08235387\n",
      "Iteration 24, loss = 0.10239541\n",
      "Iteration 33, loss = 0.08348303\n",
      "Iteration 25, loss = 0.09882654\n",
      "Iteration 34, loss = 0.07258313\n",
      "Iteration 26, loss = 0.09117436\n",
      "Iteration 35, loss = 0.07289522\n",
      "Iteration 27, loss = 0.09434275\n",
      "Iteration 36, loss = 0.07781173\n",
      "Iteration 28, loss = 0.08223770\n",
      "Iteration 37, loss = 0.06982800\n",
      "Iteration 29, loss = 0.10777753\n",
      "Iteration 38, loss = 0.07731579\n",
      "Iteration 30, loss = 0.07749789\n",
      "Iteration 39, loss = 0.07672340\n",
      "Iteration 31, loss = 0.07231159\n",
      "Iteration 40, loss = 0.08738618\n",
      "Iteration 32, loss = 0.07354256\n",
      "Iteration 41, loss = 0.09288339\n",
      "Iteration 33, loss = 0.06577475\n",
      "Iteration 42, loss = 0.07004821\n",
      "Iteration 34, loss = 0.07486098\n",
      "Iteration 43, loss = 0.06401718\n",
      "Iteration 35, loss = 0.05898015\n",
      "Iteration 44, loss = 0.06673209\n",
      "Iteration 36, loss = 0.08115328\n",
      "Iteration 45, loss = 0.05477715\n",
      "Iteration 37, loss = 0.06796646\n",
      "Iteration 46, loss = 0.07938238\n",
      "Iteration 38, loss = 0.07259901\n",
      "Iteration 47, loss = 0.07008731\n",
      "Iteration 39, loss = 0.07725539\n",
      "Iteration 48, loss = 0.09128739\n",
      "Iteration 40, loss = 0.06226021\n",
      "Iteration 49, loss = 0.08959425\n",
      "Iteration 41, loss = 0.06235708\n",
      "Iteration 50, loss = 0.10236557\n",
      "Iteration 42, loss = 0.05909247\n",
      "Iteration 51, loss = 0.10438903\n",
      "Iteration 43, loss = 0.06043800\n",
      "Iteration 52, loss = 0.07597347\n",
      "Iteration 44, loss = 0.04765220\n",
      "Iteration 53, loss = 0.09391423\n",
      "Iteration 45, loss = 0.03977964\n",
      "Iteration 54, loss = 0.15259101\n",
      "Iteration 46, loss = 0.03936113\n",
      "Iteration 55, loss = 0.09133017\n",
      "Iteration 47, loss = 0.03927994\n",
      "Iteration 56, loss = 0.09787502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.04780213\n",
      "Iteration 49, loss = 0.05489803\n",
      "Iteration 50, loss = 0.07982966\n",
      "Iteration 51, loss = 0.07519392\n",
      "Iteration 52, loss = 0.06296114\n",
      "Iteration 53, loss = 0.05665300\n",
      "Iteration 54, loss = 0.06370253\n",
      "Iteration 55, loss = 0.04868209\n",
      "Iteration 56, loss = 0.05604567\n",
      "Iteration 57, loss = 0.04148365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95335696\n",
      "Iteration 2, loss = 0.50461353\n",
      "Iteration 3, loss = 0.41524736\n",
      "Iteration 4, loss = 0.37672432\n",
      "Iteration 5, loss = 0.34122872\n",
      "Iteration 6, loss = 0.31250603\n",
      "Iteration 7, loss = 0.29136477\n",
      "Iteration 8, loss = 0.28563192\n",
      "Iteration 9, loss = 0.27449618\n",
      "Iteration 10, loss = 0.25035740\n",
      "Iteration 11, loss = 0.23182755\n",
      "Iteration 1, loss = 1.16506357\n",
      "Iteration 12, loss = 0.21277553\n",
      "Iteration 2, loss = 0.55135768\n",
      "Iteration 13, loss = 0.19951563\n",
      "Iteration 3, loss = 0.43778252\n",
      "Iteration 14, loss = 0.18265390\n",
      "Iteration 4, loss = 0.41508443\n",
      "Iteration 15, loss = 0.17597034\n",
      "Iteration 5, loss = 0.36390321\n",
      "Iteration 16, loss = 0.17091238\n",
      "Iteration 6, loss = 0.34390023\n",
      "Iteration 17, loss = 0.17227821\n",
      "Iteration 7, loss = 0.31619133\n",
      "Iteration 18, loss = 0.15556064\n",
      "Iteration 8, loss = 0.29915507\n",
      "Iteration 19, loss = 0.14416645\n",
      "Iteration 9, loss = 0.28014778\n",
      "Iteration 20, loss = 0.14738196\n",
      "Iteration 10, loss = 0.25899881\n",
      "Iteration 21, loss = 0.13641018\n",
      "Iteration 11, loss = 0.24470458\n",
      "Iteration 22, loss = 0.13949399\n",
      "Iteration 12, loss = 0.23767355\n",
      "Iteration 23, loss = 0.16523692\n",
      "Iteration 13, loss = 0.21671850\n",
      "Iteration 24, loss = 0.13181532\n",
      "Iteration 14, loss = 0.20775712\n",
      "Iteration 25, loss = 0.14352513\n",
      "Iteration 15, loss = 0.18887907\n",
      "Iteration 26, loss = 0.12478886\n",
      "Iteration 16, loss = 0.18123182\n",
      "Iteration 17, loss = 0.17089491\n",
      "Iteration 27, loss = 0.11966590\n",
      "Iteration 18, loss = 0.16122772\n",
      "Iteration 28, loss = 0.11997725\n",
      "Iteration 19, loss = 0.15517729\n",
      "Iteration 29, loss = 0.12724281\n",
      "Iteration 20, loss = 0.14754059\n",
      "Iteration 30, loss = 0.11956225\n",
      "Iteration 21, loss = 0.13906301\n",
      "Iteration 31, loss = 0.10544638\n",
      "Iteration 22, loss = 0.13429002\n",
      "Iteration 32, loss = 0.12470065\n",
      "Iteration 23, loss = 0.13060057\n",
      "Iteration 33, loss = 0.10424293\n",
      "Iteration 24, loss = 0.14575128\n",
      "Iteration 34, loss = 0.10376720\n",
      "Iteration 25, loss = 0.12878689\n",
      "Iteration 35, loss = 0.09549148\n",
      "Iteration 26, loss = 0.12137633\n",
      "Iteration 36, loss = 0.08790839\n",
      "Iteration 27, loss = 0.12642797\n",
      "Iteration 37, loss = 0.08671045\n",
      "Iteration 28, loss = 0.13055136\n",
      "Iteration 38, loss = 0.09282401\n",
      "Iteration 29, loss = 0.11826142\n",
      "Iteration 39, loss = 0.08582191\n",
      "Iteration 30, loss = 0.12027487\n",
      "Iteration 40, loss = 0.09236977\n",
      "Iteration 31, loss = 0.11814000\n",
      "Iteration 41, loss = 0.08104914\n",
      "Iteration 32, loss = 0.11445420\n",
      "Iteration 42, loss = 0.08372386\n",
      "Iteration 33, loss = 0.10361205\n",
      "Iteration 43, loss = 0.08073843\n",
      "Iteration 34, loss = 0.10490914\n",
      "Iteration 44, loss = 0.06795404\n",
      "Iteration 35, loss = 0.11289951\n",
      "Iteration 45, loss = 0.07671250\n",
      "Iteration 36, loss = 0.12120433\n",
      "Iteration 46, loss = 0.07783564\n",
      "Iteration 37, loss = 0.11845179\n",
      "Iteration 47, loss = 0.06329173\n",
      "Iteration 38, loss = 0.10306003\n",
      "Iteration 48, loss = 0.06516181\n",
      "Iteration 39, loss = 0.11401631\n",
      "Iteration 49, loss = 0.05867954\n",
      "Iteration 40, loss = 0.10823080\n",
      "Iteration 50, loss = 0.07900035\n",
      "Iteration 41, loss = 0.10022262\n",
      "Iteration 51, loss = 0.07513599\n",
      "Iteration 42, loss = 0.07888604\n",
      "Iteration 52, loss = 0.06476633\n",
      "Iteration 43, loss = 0.07752210\n",
      "Iteration 53, loss = 0.06745518\n",
      "Iteration 44, loss = 0.07549563\n",
      "Iteration 54, loss = 0.06255064\n",
      "Iteration 45, loss = 0.06847372\n",
      "Iteration 55, loss = 0.06167608\n",
      "Iteration 46, loss = 0.06905931\n",
      "Iteration 56, loss = 0.05553529\n",
      "Iteration 47, loss = 0.06366349\n",
      "Iteration 57, loss = 0.04783658\n",
      "Iteration 48, loss = 0.07031307\n",
      "Iteration 58, loss = 0.04812449\n",
      "Iteration 49, loss = 0.07034160\n",
      "Iteration 59, loss = 0.04826759\n",
      "Iteration 50, loss = 0.06275786\n",
      "Iteration 60, loss = 0.05348029\n",
      "Iteration 51, loss = 0.06667700\n",
      "Iteration 61, loss = 0.07596738\n",
      "Iteration 52, loss = 0.08396915\n",
      "Iteration 62, loss = 0.06008354\n",
      "Iteration 53, loss = 0.06890307\n",
      "Iteration 63, loss = 0.07037918\n",
      "Iteration 54, loss = 0.06393713\n",
      "Iteration 64, loss = 0.05482196\n",
      "Iteration 55, loss = 0.06371125\n",
      "Iteration 65, loss = 0.08397312\n",
      "Iteration 56, loss = 0.08000155\n",
      "Iteration 66, loss = 0.08985715\n",
      "Iteration 57, loss = 0.05656416\n",
      "Iteration 67, loss = 0.09024237\n",
      "Iteration 58, loss = 0.06050095\n",
      "Iteration 68, loss = 0.07742750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.05287798\n",
      "Iteration 60, loss = 0.04633463\n",
      "Iteration 61, loss = 0.04748316\n",
      "Iteration 62, loss = 0.04856415\n",
      "Iteration 63, loss = 0.04894645\n",
      "Iteration 64, loss = 0.03765487\n",
      "Iteration 65, loss = 0.04502675\n",
      "Iteration 66, loss = 0.06620179\n",
      "Iteration 67, loss = 0.06228147\n",
      "Iteration 68, loss = 0.06927590\n",
      "Iteration 69, loss = 0.04808264\n",
      "Iteration 70, loss = 0.06478493\n",
      "Iteration 71, loss = 0.05640154\n",
      "Iteration 72, loss = 0.07298375\n",
      "Iteration 73, loss = 0.05904310\n",
      "Iteration 74, loss = 0.05084891\n",
      "Iteration 75, loss = 0.03884678\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.91566013\n",
      "Iteration 2, loss = 0.51757833\n",
      "Iteration 3, loss = 0.44397267\n",
      "Iteration 4, loss = 0.40065282\n",
      "Iteration 5, loss = 0.35454056\n",
      "Iteration 6, loss = 0.32547650\n",
      "Iteration 7, loss = 0.29546575\n",
      "Iteration 8, loss = 0.27551978\n",
      "Iteration 9, loss = 0.25949005\n",
      "Iteration 10, loss = 0.24220764\n",
      "Iteration 11, loss = 0.22929919\n",
      "Iteration 12, loss = 0.21695456\n",
      "Iteration 13, loss = 0.20041543\n",
      "Iteration 14, loss = 0.19178872\n",
      "Iteration 15, loss = 0.17462123\n",
      "Iteration 16, loss = 0.16839551\n",
      "Iteration 17, loss = 0.15149621\n",
      "Iteration 1, loss = 0.83308290\n",
      "Iteration 18, loss = 0.15402127\n",
      "Iteration 2, loss = 0.49515944\n",
      "Iteration 19, loss = 0.13649099\n",
      "Iteration 3, loss = 0.40758965\n",
      "Iteration 20, loss = 0.13680137\n",
      "Iteration 4, loss = 0.37251563\n",
      "Iteration 21, loss = 0.16281518\n",
      "Iteration 5, loss = 0.32661636\n",
      "Iteration 22, loss = 0.19386064\n",
      "Iteration 6, loss = 0.29556278\n",
      "Iteration 23, loss = 0.17088282\n",
      "Iteration 7, loss = 0.27985968\n",
      "Iteration 24, loss = 0.16603777\n",
      "Iteration 8, loss = 0.25997634\n",
      "Iteration 25, loss = 0.14495268\n",
      "Iteration 9, loss = 0.23409051\n",
      "Iteration 26, loss = 0.13940817\n",
      "Iteration 10, loss = 0.22834051\n",
      "Iteration 27, loss = 0.13249969\n",
      "Iteration 11, loss = 0.20932251\n",
      "Iteration 28, loss = 0.10935367\n",
      "Iteration 12, loss = 0.19572898\n",
      "Iteration 29, loss = 0.10105865\n",
      "Iteration 13, loss = 0.18867668\n",
      "Iteration 30, loss = 0.11017321\n",
      "Iteration 14, loss = 0.18175898\n",
      "Iteration 31, loss = 0.11855899\n",
      "Iteration 15, loss = 0.17329272\n",
      "Iteration 32, loss = 0.11700655\n",
      "Iteration 16, loss = 0.16781259\n",
      "Iteration 33, loss = 0.09435051\n",
      "Iteration 17, loss = 0.16828685\n",
      "Iteration 34, loss = 0.09631869\n",
      "Iteration 18, loss = 0.16509876\n",
      "Iteration 35, loss = 0.07919206\n",
      "Iteration 19, loss = 0.15624715\n",
      "Iteration 36, loss = 0.08554672\n",
      "Iteration 20, loss = 0.15743054\n",
      "Iteration 37, loss = 0.09939841\n",
      "Iteration 21, loss = 0.14081286\n",
      "Iteration 38, loss = 0.07641100\n",
      "Iteration 22, loss = 0.13625159\n",
      "Iteration 39, loss = 0.07616903\n",
      "Iteration 23, loss = 0.12735545\n",
      "Iteration 40, loss = 0.07293365\n",
      "Iteration 24, loss = 0.12666875\n",
      "Iteration 41, loss = 0.07165377\n",
      "Iteration 25, loss = 0.11791785\n",
      "Iteration 42, loss = 0.07668055\n",
      "Iteration 26, loss = 0.11907445\n",
      "Iteration 43, loss = 0.08660776\n",
      "Iteration 27, loss = 0.12814829\n",
      "Iteration 44, loss = 0.07291105\n",
      "Iteration 28, loss = 0.11557689\n",
      "Iteration 45, loss = 0.09581129\n",
      "Iteration 29, loss = 0.11258257\n",
      "Iteration 46, loss = 0.07021807\n",
      "Iteration 30, loss = 0.10368113\n",
      "Iteration 47, loss = 0.08703845\n",
      "Iteration 31, loss = 0.10370684\n",
      "Iteration 48, loss = 0.07551226\n",
      "Iteration 32, loss = 0.10094930\n",
      "Iteration 49, loss = 0.06866727\n",
      "Iteration 33, loss = 0.10614646\n",
      "Iteration 50, loss = 0.08968229\n",
      "Iteration 34, loss = 0.10924877\n",
      "Iteration 51, loss = 0.06538014\n",
      "Iteration 35, loss = 0.10399826\n",
      "Iteration 52, loss = 0.07255028\n",
      "Iteration 36, loss = 0.10444210\n",
      "Iteration 53, loss = 0.05525319\n",
      "Iteration 37, loss = 0.09297359\n",
      "Iteration 54, loss = 0.05271838\n",
      "Iteration 38, loss = 0.08650357\n",
      "Iteration 55, loss = 0.04782870\n",
      "Iteration 39, loss = 0.07730895\n",
      "Iteration 56, loss = 0.07334686\n",
      "Iteration 40, loss = 0.07646987\n",
      "Iteration 57, loss = 0.09130895\n",
      "Iteration 41, loss = 0.06902666\n",
      "Iteration 58, loss = 0.08125706\n",
      "Iteration 42, loss = 0.06797688\n",
      "Iteration 59, loss = 0.06409128\n",
      "Iteration 43, loss = 0.06173909\n",
      "Iteration 60, loss = 0.05636967\n",
      "Iteration 44, loss = 0.06496157\n",
      "Iteration 61, loss = 0.04886478\n",
      "Iteration 45, loss = 0.06445309\n",
      "Iteration 62, loss = 0.05080945\n",
      "Iteration 46, loss = 0.05401349\n",
      "Iteration 63, loss = 0.04581888\n",
      "Iteration 47, loss = 0.05786801\n",
      "Iteration 64, loss = 0.04514418\n",
      "Iteration 48, loss = 0.06337100\n",
      "Iteration 49, loss = 0.04919143\n",
      "Iteration 65, loss = 0.05085276\n",
      "Iteration 50, loss = 0.04779488\n",
      "Iteration 66, loss = 0.04449543\n",
      "Iteration 67, loss = 0.08018974\n",
      "Iteration 51, loss = 0.04628252\n",
      "Iteration 68, loss = 0.08360600\n",
      "Iteration 52, loss = 0.04975249\n",
      "Iteration 69, loss = 0.06721451\n",
      "Iteration 53, loss = 0.04808230\n",
      "Iteration 70, loss = 0.05525470\n",
      "Iteration 54, loss = 0.04417383\n",
      "Iteration 71, loss = 0.04750682\n",
      "Iteration 55, loss = 0.04927764\n",
      "Iteration 72, loss = 0.05255320\n",
      "Iteration 56, loss = 0.05171804\n",
      "Iteration 73, loss = 0.03906715\n",
      "Iteration 57, loss = 0.05265889\n",
      "Iteration 74, loss = 0.04574623\n",
      "Iteration 58, loss = 0.05237571\n",
      "Iteration 75, loss = 0.05261058\n",
      "Iteration 59, loss = 0.04910865\n",
      "Iteration 76, loss = 0.04497215\n",
      "Iteration 60, loss = 0.05126722\n",
      "Iteration 77, loss = 0.05186711\n",
      "Iteration 61, loss = 0.07093998\n",
      "Iteration 78, loss = 0.03887611\n",
      "Iteration 62, loss = 0.07209543\n",
      "Iteration 79, loss = 0.03859680\n",
      "Iteration 63, loss = 0.05003698\n",
      "Iteration 80, loss = 0.03792986\n",
      "Iteration 64, loss = 0.04370219\n",
      "Iteration 81, loss = 0.03344350\n",
      "Iteration 65, loss = 0.04321911\n",
      "Iteration 82, loss = 0.03646325\n",
      "Iteration 66, loss = 0.04948721\n",
      "Iteration 83, loss = 0.03090086\n",
      "Iteration 67, loss = 0.03663915\n",
      "Iteration 84, loss = 0.02724523\n",
      "Iteration 68, loss = 0.04588833\n",
      "Iteration 85, loss = 0.02838936\n",
      "Iteration 69, loss = 0.05180303\n",
      "Iteration 86, loss = 0.03322454\n",
      "Iteration 70, loss = 0.04708878\n",
      "Iteration 87, loss = 0.03089121\n",
      "Iteration 71, loss = 0.04392753\n",
      "Iteration 88, loss = 0.02853264\n",
      "Iteration 72, loss = 0.04406083\n",
      "Iteration 89, loss = 0.02766286\n",
      "Iteration 73, loss = 0.03612228\n",
      "Iteration 90, loss = 0.03987994\n",
      "Iteration 74, loss = 0.08321104\n",
      "Iteration 91, loss = 0.03253747\n",
      "Iteration 75, loss = 0.10373794\n",
      "Iteration 92, loss = 0.06548251\n",
      "Iteration 76, loss = 0.12404050\n",
      "Iteration 93, loss = 0.04653193\n",
      "Iteration 77, loss = 0.10581824\n",
      "Iteration 94, loss = 0.04365665\n",
      "Iteration 78, loss = 0.07235292\n",
      "Iteration 95, loss = 0.04026490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 79, loss = 0.07761469\n",
      "Iteration 80, loss = 0.06349666\n",
      "Iteration 81, loss = 0.05933193\n",
      "Iteration 82, loss = 0.05085317\n",
      "Iteration 83, loss = 0.04832458\n",
      "Iteration 84, loss = 0.03522404\n",
      "Iteration 85, loss = 0.04640162\n",
      "Iteration 86, loss = 0.03594803\n",
      "Iteration 87, loss = 0.05141568\n",
      "Iteration 88, loss = 0.04074186\n",
      "Iteration 89, loss = 0.06874860\n",
      "Iteration 90, loss = 0.05034704\n",
      "Iteration 91, loss = 0.04948894\n",
      "Iteration 92, loss = 0.05957871\n",
      "Iteration 93, loss = 0.08026497\n",
      "Iteration 94, loss = 0.12075671\n",
      "Iteration 95, loss = 0.19196861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.85299466\n",
      "Iteration 2, loss = 0.48223962\n",
      "Iteration 3, loss = 0.38143013\n",
      "Iteration 4, loss = 0.35619563\n",
      "Iteration 5, loss = 0.33428387\n",
      "Iteration 6, loss = 0.29671203\n",
      "Iteration 7, loss = 0.26464878\n",
      "Iteration 8, loss = 0.24830967\n",
      "Iteration 9, loss = 0.23833260\n",
      "Iteration 10, loss = 0.22234194\n",
      "Iteration 11, loss = 0.20596200\n",
      "Iteration 12, loss = 0.19558935\n",
      "Iteration 13, loss = 0.18763002\n",
      "Iteration 14, loss = 0.17896801\n",
      "Iteration 15, loss = 0.16367406\n",
      "Iteration 16, loss = 0.15694382\n",
      "Iteration 17, loss = 0.14609201\n",
      "Iteration 1, loss = 0.90669536\n",
      "Iteration 18, loss = 0.14257067\n",
      "Iteration 2, loss = 0.50118273\n",
      "Iteration 19, loss = 0.14625538\n",
      "Iteration 3, loss = 0.42694939\n",
      "Iteration 20, loss = 0.15422835\n",
      "Iteration 4, loss = 0.39019389\n",
      "Iteration 21, loss = 0.14465595\n",
      "Iteration 5, loss = 0.35447076\n",
      "Iteration 22, loss = 0.13835271\n",
      "Iteration 6, loss = 0.32414730\n",
      "Iteration 23, loss = 0.13027145\n",
      "Iteration 7, loss = 0.30066272\n",
      "Iteration 24, loss = 0.10809320\n",
      "Iteration 8, loss = 0.28036218\n",
      "Iteration 25, loss = 0.10852871\n",
      "Iteration 9, loss = 0.25737063\n",
      "Iteration 26, loss = 0.10912775\n",
      "Iteration 10, loss = 0.23645850\n",
      "Iteration 11, loss = 0.23694183Iteration 27, loss = 0.10836614\n",
      "\n",
      "Iteration 12, loss = 0.21476562\n",
      "Iteration 28, loss = 0.08194119\n",
      "Iteration 13, loss = 0.20312160\n",
      "Iteration 29, loss = 0.09246653\n",
      "Iteration 14, loss = 0.19099109\n",
      "Iteration 30, loss = 0.09608283\n",
      "Iteration 15, loss = 0.18283594\n",
      "Iteration 31, loss = 0.11674662\n",
      "Iteration 16, loss = 0.17215528\n",
      "Iteration 32, loss = 0.09308170\n",
      "Iteration 17, loss = 0.15943031\n",
      "Iteration 33, loss = 0.09840083\n",
      "Iteration 18, loss = 0.14980322\n",
      "Iteration 34, loss = 0.08741131\n",
      "Iteration 19, loss = 0.14150773\n",
      "Iteration 35, loss = 0.07592470\n",
      "Iteration 20, loss = 0.13674403\n",
      "Iteration 36, loss = 0.07771925\n",
      "Iteration 21, loss = 0.13930258\n",
      "Iteration 37, loss = 0.06494694\n",
      "Iteration 22, loss = 0.13708221\n",
      "Iteration 38, loss = 0.06354262\n",
      "Iteration 23, loss = 0.14519054\n",
      "Iteration 39, loss = 0.07059583\n",
      "Iteration 24, loss = 0.12393999\n",
      "Iteration 40, loss = 0.07995141\n",
      "Iteration 25, loss = 0.11691622\n",
      "Iteration 41, loss = 0.06853986\n",
      "Iteration 42, loss = 0.05815262\n",
      "Iteration 26, loss = 0.12687277\n",
      "Iteration 43, loss = 0.05654185Iteration 27, loss = 0.11006783\n",
      "\n",
      "Iteration 28, loss = 0.09469098Iteration 44, loss = 0.06243759\n",
      "\n",
      "Iteration 29, loss = 0.09697118\n",
      "Iteration 45, loss = 0.05023171\n",
      "Iteration 30, loss = 0.09868201\n",
      "Iteration 46, loss = 0.05861992\n",
      "Iteration 31, loss = 0.10760802\n",
      "Iteration 47, loss = 0.04837505\n",
      "Iteration 32, loss = 0.11530639\n",
      "Iteration 48, loss = 0.04854191\n",
      "Iteration 33, loss = 0.10844283\n",
      "Iteration 49, loss = 0.06278041\n",
      "Iteration 34, loss = 0.09901576\n",
      "Iteration 50, loss = 0.04970763\n",
      "Iteration 35, loss = 0.08627627\n",
      "Iteration 51, loss = 0.04484319\n",
      "Iteration 36, loss = 0.08455772\n",
      "Iteration 52, loss = 0.03932257\n",
      "Iteration 37, loss = 0.08754939\n",
      "Iteration 53, loss = 0.04019270\n",
      "Iteration 38, loss = 0.08729064\n",
      "Iteration 54, loss = 0.03496062\n",
      "Iteration 39, loss = 0.07503663\n",
      "Iteration 55, loss = 0.03691742\n",
      "Iteration 40, loss = 0.07883381\n",
      "Iteration 56, loss = 0.04981383\n",
      "Iteration 41, loss = 0.07062306\n",
      "Iteration 57, loss = 0.04376313\n",
      "Iteration 42, loss = 0.08148906\n",
      "Iteration 58, loss = 0.03821291\n",
      "Iteration 43, loss = 0.08487649\n",
      "Iteration 59, loss = 0.04058755\n",
      "Iteration 44, loss = 0.09712779\n",
      "Iteration 60, loss = 0.03490845\n",
      "Iteration 45, loss = 0.08191417\n",
      "Iteration 61, loss = 0.03417949\n",
      "Iteration 46, loss = 0.06502467\n",
      "Iteration 62, loss = 0.03383058\n",
      "Iteration 47, loss = 0.06461151\n",
      "Iteration 63, loss = 0.05548366\n",
      "Iteration 48, loss = 0.06571352\n",
      "Iteration 64, loss = 0.04618652\n",
      "Iteration 49, loss = 0.06257833\n",
      "Iteration 65, loss = 0.04694590\n",
      "Iteration 50, loss = 0.05676397\n",
      "Iteration 66, loss = 0.04545558\n",
      "Iteration 51, loss = 0.05263026\n",
      "Iteration 67, loss = 0.03737911\n",
      "Iteration 52, loss = 0.04902583\n",
      "Iteration 68, loss = 0.02869640\n",
      "Iteration 53, loss = 0.04637087\n",
      "Iteration 69, loss = 0.03233819\n",
      "Iteration 54, loss = 0.05206874\n",
      "Iteration 70, loss = 0.03082528\n",
      "Iteration 55, loss = 0.06900340\n",
      "Iteration 71, loss = 0.04360523\n",
      "Iteration 56, loss = 0.07225097\n",
      "Iteration 72, loss = 0.06124249\n",
      "Iteration 57, loss = 0.10428175\n",
      "Iteration 73, loss = 0.08629591\n",
      "Iteration 58, loss = 0.09131094\n",
      "Iteration 74, loss = 0.15455082\n",
      "Iteration 59, loss = 0.09728683\n",
      "Iteration 75, loss = 0.12296752\n",
      "Iteration 60, loss = 0.09459892\n",
      "Iteration 76, loss = 0.08049992\n",
      "Iteration 61, loss = 0.07982482\n",
      "Iteration 77, loss = 0.06671223\n",
      "Iteration 62, loss = 0.08177445\n",
      "Iteration 78, loss = 0.04103773\n",
      "Iteration 63, loss = 0.06342739\n",
      "Iteration 79, loss = 0.04663655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.06441640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07626280\n",
      "Iteration 2, loss = 0.57559596\n",
      "Iteration 1, loss = 0.97701799\n",
      "Iteration 3, loss = 0.40898192\n",
      "Iteration 2, loss = 0.51755744\n",
      "Iteration 4, loss = 0.36442640\n",
      "Iteration 3, loss = 0.41410664\n",
      "Iteration 5, loss = 0.33121054\n",
      "Iteration 4, loss = 0.36652903\n",
      "Iteration 6, loss = 0.29710578\n",
      "Iteration 5, loss = 0.33025887\n",
      "Iteration 7, loss = 0.27246148\n",
      "Iteration 6, loss = 0.30577655\n",
      "Iteration 8, loss = 0.25631572\n",
      "Iteration 7, loss = 0.27317823\n",
      "Iteration 9, loss = 0.23935874\n",
      "Iteration 8, loss = 0.25272117\n",
      "Iteration 10, loss = 0.21676570\n",
      "Iteration 9, loss = 0.23292134\n",
      "Iteration 11, loss = 0.20623636\n",
      "Iteration 10, loss = 0.22292884\n",
      "Iteration 12, loss = 0.18686205\n",
      "Iteration 11, loss = 0.22369794\n",
      "Iteration 13, loss = 0.17129504\n",
      "Iteration 12, loss = 0.20879846\n",
      "Iteration 14, loss = 0.16352855\n",
      "Iteration 13, loss = 0.18768385\n",
      "Iteration 15, loss = 0.14879191\n",
      "Iteration 14, loss = 0.18866223\n",
      "Iteration 16, loss = 0.14289686\n",
      "Iteration 15, loss = 0.18294879\n",
      "Iteration 17, loss = 0.13149612\n",
      "Iteration 16, loss = 0.16861366\n",
      "Iteration 18, loss = 0.12379254\n",
      "Iteration 17, loss = 0.15448661\n",
      "Iteration 19, loss = 0.12797563\n",
      "Iteration 18, loss = 0.15281324\n",
      "Iteration 20, loss = 0.12045058\n",
      "Iteration 19, loss = 0.14294813\n",
      "Iteration 21, loss = 0.11882862\n",
      "Iteration 20, loss = 0.13282244\n",
      "Iteration 22, loss = 0.11616463\n",
      "Iteration 21, loss = 0.11946254\n",
      "Iteration 23, loss = 0.12331074\n",
      "Iteration 22, loss = 0.12248123\n",
      "Iteration 24, loss = 0.13881947\n",
      "Iteration 23, loss = 0.11558865\n",
      "Iteration 25, loss = 0.11185075\n",
      "Iteration 24, loss = 0.13619301\n",
      "Iteration 26, loss = 0.11461693\n",
      "Iteration 25, loss = 0.11850689\n",
      "Iteration 27, loss = 0.10458177\n",
      "Iteration 26, loss = 0.13197063\n",
      "Iteration 28, loss = 0.09966521\n",
      "Iteration 27, loss = 0.11417384\n",
      "Iteration 29, loss = 0.08256162\n",
      "Iteration 28, loss = 0.09634494\n",
      "Iteration 30, loss = 0.07875834\n",
      "Iteration 29, loss = 0.08796584\n",
      "Iteration 31, loss = 0.06908975\n",
      "Iteration 30, loss = 0.09432436\n",
      "Iteration 32, loss = 0.06828390\n",
      "Iteration 31, loss = 0.09560295\n",
      "Iteration 33, loss = 0.07173383\n",
      "Iteration 32, loss = 0.09688768\n",
      "Iteration 34, loss = 0.09133334\n",
      "Iteration 33, loss = 0.09797773\n",
      "Iteration 35, loss = 0.06952694\n",
      "Iteration 34, loss = 0.07568176\n",
      "Iteration 36, loss = 0.06847691\n",
      "Iteration 35, loss = 0.08443126\n",
      "Iteration 37, loss = 0.07421063\n",
      "Iteration 36, loss = 0.08753482\n",
      "Iteration 38, loss = 0.07343047\n",
      "Iteration 37, loss = 0.08358789\n",
      "Iteration 39, loss = 0.05591836\n",
      "Iteration 38, loss = 0.08620877\n",
      "Iteration 40, loss = 0.05106144\n",
      "Iteration 39, loss = 0.09196139\n",
      "Iteration 41, loss = 0.04788348\n",
      "Iteration 40, loss = 0.07934088\n",
      "Iteration 42, loss = 0.04672735\n",
      "Iteration 41, loss = 0.06720871\n",
      "Iteration 43, loss = 0.06014421\n",
      "Iteration 42, loss = 0.06661055\n",
      "Iteration 44, loss = 0.07658624\n",
      "Iteration 43, loss = 0.06413534\n",
      "Iteration 45, loss = 0.05207714\n",
      "Iteration 44, loss = 0.07080845\n",
      "Iteration 46, loss = 0.04664935\n",
      "Iteration 45, loss = 0.06454507\n",
      "Iteration 47, loss = 0.05183509\n",
      "Iteration 46, loss = 0.05397422\n",
      "Iteration 48, loss = 0.04965228\n",
      "Iteration 47, loss = 0.05232636\n",
      "Iteration 49, loss = 0.05247259\n",
      "Iteration 48, loss = 0.05369204\n",
      "Iteration 50, loss = 0.04549015\n",
      "Iteration 49, loss = 0.04870278\n",
      "Iteration 51, loss = 0.04673277\n",
      "Iteration 50, loss = 0.04461324\n",
      "Iteration 52, loss = 0.04376764\n",
      "Iteration 51, loss = 0.05722130\n",
      "Iteration 53, loss = 0.04683323\n",
      "Iteration 52, loss = 0.06472750\n",
      "Iteration 54, loss = 0.04549520\n",
      "Iteration 53, loss = 0.05030564\n",
      "Iteration 55, loss = 0.05174861\n",
      "Iteration 54, loss = 0.04450265\n",
      "Iteration 56, loss = 0.05049309\n",
      "Iteration 55, loss = 0.04600852\n",
      "Iteration 57, loss = 0.05821466\n",
      "Iteration 56, loss = 0.04868268\n",
      "Iteration 58, loss = 0.06221146\n",
      "Iteration 57, loss = 0.04650291\n",
      "Iteration 59, loss = 0.05640791\n",
      "Iteration 58, loss = 0.04141239\n",
      "Iteration 60, loss = 0.04419400\n",
      "Iteration 59, loss = 0.07775530\n",
      "Iteration 61, loss = 0.04337021\n",
      "Iteration 60, loss = 0.06732661\n",
      "Iteration 62, loss = 0.04112064\n",
      "Iteration 61, loss = 0.05899533\n",
      "Iteration 63, loss = 0.04484011\n",
      "Iteration 62, loss = 0.07088245\n",
      "Iteration 64, loss = 0.04906353\n",
      "Iteration 63, loss = 0.05825751\n",
      "Iteration 65, loss = 0.05669157\n",
      "Iteration 64, loss = 0.05878083\n",
      "Iteration 66, loss = 0.06011413\n",
      "Iteration 65, loss = 0.05784479\n",
      "Iteration 67, loss = 0.07721547\n",
      "Iteration 66, loss = 0.05098517\n",
      "Iteration 68, loss = 0.07694464\n",
      "Iteration 67, loss = 0.06052328\n",
      "Iteration 69, loss = 0.09728672\n",
      "Iteration 68, loss = 0.05095998\n",
      "Iteration 70, loss = 0.10097382\n",
      "Iteration 69, loss = 0.04686584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 71, loss = 0.07129062\n",
      "Iteration 72, loss = 0.05441057\n",
      "Iteration 73, loss = 0.04052200\n",
      "Iteration 74, loss = 0.04161833\n",
      "Iteration 75, loss = 0.04235963\n",
      "Iteration 76, loss = 0.05147966\n",
      "Iteration 77, loss = 0.04776014\n",
      "Iteration 78, loss = 0.03901709\n",
      "Iteration 79, loss = 0.03374808\n",
      "Iteration 80, loss = 0.03323274\n",
      "Iteration 81, loss = 0.02501965\n",
      "Iteration 82, loss = 0.02683197\n",
      "Iteration 83, loss = 0.02518292\n",
      "Iteration 84, loss = 0.02366553\n",
      "Iteration 85, loss = 0.02570917\n",
      "Iteration 86, loss = 0.02483695\n",
      "Iteration 87, loss = 0.02437617\n",
      "Iteration 88, loss = 0.02833221\n",
      "Iteration 1, loss = 0.86864743\n",
      "Iteration 89, loss = 0.03418586\n",
      "Iteration 2, loss = 0.51570725\n",
      "Iteration 90, loss = 0.06316741\n",
      "Iteration 3, loss = 0.40515055\n",
      "Iteration 91, loss = 0.06889266\n",
      "Iteration 4, loss = 0.38174045\n",
      "Iteration 92, loss = 0.05657674\n",
      "Iteration 5, loss = 0.34924240\n",
      "Iteration 93, loss = 0.05751135\n",
      "Iteration 6, loss = 0.31808973\n",
      "Iteration 94, loss = 0.02563153\n",
      "Iteration 7, loss = 0.28276489\n",
      "Iteration 95, loss = 0.02533550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.26586444\n",
      "Iteration 9, loss = 0.25581284\n",
      "Iteration 10, loss = 0.23775699\n",
      "Iteration 11, loss = 0.21807797\n",
      "Iteration 12, loss = 0.20510274\n",
      "Iteration 13, loss = 0.18963612\n",
      "Iteration 14, loss = 0.17313676\n",
      "Iteration 15, loss = 0.17586703\n",
      "Iteration 16, loss = 0.15856252\n",
      "Iteration 17, loss = 0.15997249\n",
      "Iteration 18, loss = 0.15042994\n",
      "Iteration 19, loss = 0.14589510\n",
      "Iteration 20, loss = 0.14438693\n",
      "Iteration 21, loss = 0.13546324\n",
      "Iteration 22, loss = 0.13245648\n",
      "Iteration 23, loss = 0.11438616\n",
      "Iteration 24, loss = 0.11135329\n",
      "Iteration 25, loss = 0.10579767\n",
      "Iteration 1, loss = 0.78082756\n",
      "Iteration 26, loss = 0.12718731\n",
      "Iteration 2, loss = 0.45304532\n",
      "Iteration 27, loss = 0.12231370\n",
      "Iteration 3, loss = 0.40693957\n",
      "Iteration 28, loss = 0.11009369\n",
      "Iteration 4, loss = 0.36759050\n",
      "Iteration 29, loss = 0.10421860\n",
      "Iteration 5, loss = 0.33906626\n",
      "Iteration 30, loss = 0.10896036\n",
      "Iteration 6, loss = 0.30581857\n",
      "Iteration 31, loss = 0.10307214\n",
      "Iteration 7, loss = 0.28655344\n",
      "Iteration 32, loss = 0.11544845\n",
      "Iteration 8, loss = 0.26937283\n",
      "Iteration 33, loss = 0.12875615\n",
      "Iteration 9, loss = 0.26869718\n",
      "Iteration 34, loss = 0.10144778\n",
      "Iteration 10, loss = 0.25476646\n",
      "Iteration 35, loss = 0.09534179\n",
      "Iteration 11, loss = 0.22147573\n",
      "Iteration 36, loss = 0.09417822\n",
      "Iteration 12, loss = 0.19896271\n",
      "Iteration 37, loss = 0.09948123\n",
      "Iteration 13, loss = 0.18839302\n",
      "Iteration 38, loss = 0.10353896\n",
      "Iteration 14, loss = 0.17499431\n",
      "Iteration 39, loss = 0.09435996\n",
      "Iteration 15, loss = 0.16159199\n",
      "Iteration 40, loss = 0.08249612\n",
      "Iteration 16, loss = 0.15819721\n",
      "Iteration 41, loss = 0.08940359\n",
      "Iteration 17, loss = 0.15240579\n",
      "Iteration 42, loss = 0.08640819\n",
      "Iteration 18, loss = 0.15371041\n",
      "Iteration 43, loss = 0.07654318\n",
      "Iteration 19, loss = 0.12955202\n",
      "Iteration 44, loss = 0.07809763\n",
      "Iteration 20, loss = 0.13259210\n",
      "Iteration 45, loss = 0.06436000\n",
      "Iteration 21, loss = 0.13200531\n",
      "Iteration 46, loss = 0.05905776\n",
      "Iteration 22, loss = 0.11706218\n",
      "Iteration 47, loss = 0.06558879\n",
      "Iteration 23, loss = 0.10987936\n",
      "Iteration 48, loss = 0.06289770\n",
      "Iteration 24, loss = 0.09319764\n",
      "Iteration 49, loss = 0.07249027\n",
      "Iteration 25, loss = 0.09483341\n",
      "Iteration 50, loss = 0.07602640\n",
      "Iteration 26, loss = 0.09334801\n",
      "Iteration 51, loss = 0.05807558\n",
      "Iteration 27, loss = 0.08296490\n",
      "Iteration 52, loss = 0.05903288\n",
      "Iteration 28, loss = 0.07665246\n",
      "Iteration 53, loss = 0.05976970\n",
      "Iteration 29, loss = 0.07088979\n",
      "Iteration 54, loss = 0.05726084\n",
      "Iteration 30, loss = 0.07466105\n",
      "Iteration 55, loss = 0.06589087\n",
      "Iteration 31, loss = 0.07121561\n",
      "Iteration 56, loss = 0.05125967\n",
      "Iteration 32, loss = 0.07152166\n",
      "Iteration 57, loss = 0.05039529\n",
      "Iteration 33, loss = 0.05737207\n",
      "Iteration 58, loss = 0.04647274\n",
      "Iteration 34, loss = 0.07420082\n",
      "Iteration 59, loss = 0.04171870\n",
      "Iteration 35, loss = 0.06255346\n",
      "Iteration 60, loss = 0.04079841\n",
      "Iteration 36, loss = 0.07586450\n",
      "Iteration 61, loss = 0.06142674\n",
      "Iteration 37, loss = 0.06771119\n",
      "Iteration 62, loss = 0.05573525\n",
      "Iteration 38, loss = 0.04882634\n",
      "Iteration 63, loss = 0.04050263\n",
      "Iteration 39, loss = 0.05242495\n",
      "Iteration 64, loss = 0.03924982\n",
      "Iteration 40, loss = 0.04109577\n",
      "Iteration 65, loss = 0.03816527\n",
      "Iteration 41, loss = 0.04926367\n",
      "Iteration 66, loss = 0.03448993\n",
      "Iteration 42, loss = 0.05140473\n",
      "Iteration 67, loss = 0.03546701\n",
      "Iteration 43, loss = 0.05861286\n",
      "Iteration 68, loss = 0.03841305\n",
      "Iteration 44, loss = 0.07129445\n",
      "Iteration 69, loss = 0.03424921\n",
      "Iteration 45, loss = 0.06975070\n",
      "Iteration 70, loss = 0.03354123\n",
      "Iteration 46, loss = 0.11200221\n",
      "Iteration 71, loss = 0.02951907\n",
      "Iteration 47, loss = 0.11870351\n",
      "Iteration 72, loss = 0.03154627\n",
      "Iteration 48, loss = 0.16409280\n",
      "Iteration 73, loss = 0.03542005\n",
      "Iteration 49, loss = 0.13404654\n",
      "Iteration 74, loss = 0.03024679\n",
      "Iteration 50, loss = 0.18176880\n",
      "Iteration 75, loss = 0.03093981\n",
      "Iteration 51, loss = 0.11494791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 76, loss = 0.02767035\n",
      "Iteration 77, loss = 0.03176152\n",
      "Iteration 78, loss = 0.02675083\n",
      "Iteration 79, loss = 0.02752017\n",
      "Iteration 80, loss = 0.03955312\n",
      "Iteration 81, loss = 0.04037037\n",
      "Iteration 82, loss = 0.04199751\n",
      "Iteration 83, loss = 0.03175068\n",
      "Iteration 84, loss = 0.03916205\n",
      "Iteration 85, loss = 0.04513281\n",
      "Iteration 86, loss = 0.03468932\n",
      "Iteration 87, loss = 0.03294173\n",
      "Iteration 88, loss = 0.04131703\n",
      "Iteration 89, loss = 0.03221475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93049238\n",
      "Iteration 2, loss = 0.50005812\n",
      "Iteration 3, loss = 0.41355295\n",
      "Iteration 4, loss = 0.37175069\n",
      "Iteration 5, loss = 0.33822561\n",
      "Iteration 6, loss = 0.31602594\n",
      "Iteration 7, loss = 0.28755304\n",
      "Iteration 8, loss = 0.26873005\n",
      "Iteration 9, loss = 0.25523077\n",
      "Iteration 10, loss = 0.23942986\n",
      "Iteration 11, loss = 0.22401503\n",
      "Iteration 12, loss = 0.20832804\n",
      "Iteration 13, loss = 0.19755256\n",
      "Iteration 14, loss = 0.20575568\n",
      "Iteration 15, loss = 0.19957320\n",
      "Iteration 1, loss = 0.96041590\n",
      "Iteration 16, loss = 0.18480384\n",
      "Iteration 2, loss = 0.46741180\n",
      "Iteration 17, loss = 0.18646936\n",
      "Iteration 3, loss = 0.39287724\n",
      "Iteration 18, loss = 0.16027958\n",
      "Iteration 4, loss = 0.37161566\n",
      "Iteration 19, loss = 0.14985583\n",
      "Iteration 5, loss = 0.34773750\n",
      "Iteration 6, loss = 0.31437319\n",
      "Iteration 20, loss = 0.16760466\n",
      "Iteration 7, loss = 0.29997277\n",
      "Iteration 21, loss = 0.14784494\n",
      "Iteration 8, loss = 0.27250878\n",
      "Iteration 22, loss = 0.14712639\n",
      "Iteration 9, loss = 0.25712243\n",
      "Iteration 23, loss = 0.13514599\n",
      "Iteration 10, loss = 0.23873217\n",
      "Iteration 24, loss = 0.13702122\n",
      "Iteration 11, loss = 0.23113813\n",
      "Iteration 25, loss = 0.13438422\n",
      "Iteration 12, loss = 0.21212924\n",
      "Iteration 26, loss = 0.13606619\n",
      "Iteration 13, loss = 0.20407294\n",
      "Iteration 27, loss = 0.12140875\n",
      "Iteration 14, loss = 0.18650712\n",
      "Iteration 28, loss = 0.11387055\n",
      "Iteration 15, loss = 0.17586882\n",
      "Iteration 29, loss = 0.10993682\n",
      "Iteration 16, loss = 0.16519152\n",
      "Iteration 30, loss = 0.10806292\n",
      "Iteration 17, loss = 0.15774446\n",
      "Iteration 31, loss = 0.10452938\n",
      "Iteration 18, loss = 0.15268639\n",
      "Iteration 32, loss = 0.09990899\n",
      "Iteration 19, loss = 0.16200906\n",
      "Iteration 33, loss = 0.09603871\n",
      "Iteration 20, loss = 0.14032648\n",
      "Iteration 34, loss = 0.08740114\n",
      "Iteration 21, loss = 0.12957662\n",
      "Iteration 35, loss = 0.08796710\n",
      "Iteration 22, loss = 0.13542401\n",
      "Iteration 36, loss = 0.08366892\n",
      "Iteration 23, loss = 0.12193579\n",
      "Iteration 37, loss = 0.09294066\n",
      "Iteration 24, loss = 0.12745250\n",
      "Iteration 38, loss = 0.08880473\n",
      "Iteration 25, loss = 0.11977244\n",
      "Iteration 39, loss = 0.09491016\n",
      "Iteration 26, loss = 0.12879784\n",
      "Iteration 40, loss = 0.08477591\n",
      "Iteration 27, loss = 0.11713041\n",
      "Iteration 41, loss = 0.10549620\n",
      "Iteration 28, loss = 0.11459082\n",
      "Iteration 42, loss = 0.09404651\n",
      "Iteration 29, loss = 0.10685201\n",
      "Iteration 43, loss = 0.09445818\n",
      "Iteration 30, loss = 0.10259373\n",
      "Iteration 44, loss = 0.09552376\n",
      "Iteration 31, loss = 0.11197334\n",
      "Iteration 45, loss = 0.09694771\n",
      "Iteration 32, loss = 0.10919109\n",
      "Iteration 46, loss = 0.10036178\n",
      "Iteration 33, loss = 0.10611921\n",
      "Iteration 47, loss = 0.08524581\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.08600210\n",
      "Iteration 35, loss = 0.08715887\n",
      "Iteration 36, loss = 0.08485023\n",
      "Iteration 37, loss = 0.09687351\n",
      "Iteration 38, loss = 0.09315709\n",
      "Iteration 39, loss = 0.08098766\n",
      "Iteration 40, loss = 0.08457403\n",
      "Iteration 41, loss = 0.07695767\n",
      "Iteration 42, loss = 0.11388176\n",
      "Iteration 43, loss = 0.09420804\n",
      "Iteration 44, loss = 0.11912885\n",
      "Iteration 45, loss = 0.07819617\n",
      "Iteration 46, loss = 0.07751980\n",
      "Iteration 47, loss = 0.09406233\n",
      "Iteration 48, loss = 0.12027841\n",
      "Iteration 49, loss = 0.10602054\n",
      "Iteration 50, loss = 0.09893067\n",
      "Iteration 51, loss = 0.06685803\n",
      "Iteration 52, loss = 0.07118338\n",
      "Iteration 1, loss = 1.04578641\n",
      "Iteration 53, loss = 0.06670685\n",
      "Iteration 2, loss = 0.54181972\n",
      "Iteration 54, loss = 0.05760867\n",
      "Iteration 3, loss = 0.42908451\n",
      "Iteration 55, loss = 0.05530661\n",
      "Iteration 4, loss = 0.38843951\n",
      "Iteration 56, loss = 0.05351630\n",
      "Iteration 5, loss = 0.34924927\n",
      "Iteration 57, loss = 0.05571445\n",
      "Iteration 6, loss = 0.32873284\n",
      "Iteration 58, loss = 0.04758807\n",
      "Iteration 7, loss = 0.29812623\n",
      "Iteration 59, loss = 0.05230443\n",
      "Iteration 8, loss = 0.29221069\n",
      "Iteration 60, loss = 0.06157069\n",
      "Iteration 9, loss = 0.26210805\n",
      "Iteration 61, loss = 0.05886495\n",
      "Iteration 10, loss = 0.24952417\n",
      "Iteration 62, loss = 0.04401457\n",
      "Iteration 11, loss = 0.24359561\n",
      "Iteration 63, loss = 0.04563354\n",
      "Iteration 12, loss = 0.25067638\n",
      "Iteration 64, loss = 0.04732292\n",
      "Iteration 13, loss = 0.20454227\n",
      "Iteration 14, loss = 0.19563666\n",
      "Iteration 65, loss = 0.05378176\n",
      "Iteration 15, loss = 0.19445868\n",
      "Iteration 66, loss = 0.05889468\n",
      "Iteration 16, loss = 0.18682988\n",
      "Iteration 67, loss = 0.06680504\n",
      "Iteration 17, loss = 0.17229053\n",
      "Iteration 68, loss = 0.06576154\n",
      "Iteration 18, loss = 0.17360082\n",
      "Iteration 69, loss = 0.05156054\n",
      "Iteration 19, loss = 0.15746560\n",
      "Iteration 70, loss = 0.04291251\n",
      "Iteration 20, loss = 0.14077053\n",
      "Iteration 71, loss = 0.06779759\n",
      "Iteration 21, loss = 0.15295410\n",
      "Iteration 72, loss = 0.05850651\n",
      "Iteration 22, loss = 0.14960697\n",
      "Iteration 73, loss = 0.06771677\n",
      "Iteration 23, loss = 0.14411261\n",
      "Iteration 74, loss = 0.09795659\n",
      "Iteration 24, loss = 0.11979193\n",
      "Iteration 75, loss = 0.06602239\n",
      "Iteration 25, loss = 0.12341300\n",
      "Iteration 76, loss = 0.06046127\n",
      "Iteration 26, loss = 0.11502562\n",
      "Iteration 77, loss = 0.05072824\n",
      "Iteration 27, loss = 0.12314392\n",
      "Iteration 78, loss = 0.07340320\n",
      "Iteration 28, loss = 0.10648516\n",
      "Iteration 79, loss = 0.06728184\n",
      "Iteration 29, loss = 0.09691735\n",
      "Iteration 80, loss = 0.06185934\n",
      "Iteration 30, loss = 0.09666662\n",
      "Iteration 81, loss = 0.06248302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.09101959\n",
      "Iteration 32, loss = 0.09313961\n",
      "Iteration 33, loss = 0.09405204\n",
      "Iteration 34, loss = 0.08084827\n",
      "Iteration 35, loss = 0.08660059\n",
      "Iteration 36, loss = 0.08802237\n",
      "Iteration 37, loss = 0.09104294\n",
      "Iteration 38, loss = 0.08456490\n",
      "Iteration 39, loss = 0.10706442\n",
      "Iteration 40, loss = 0.08981207\n",
      "Iteration 41, loss = 0.07842111\n",
      "Iteration 42, loss = 0.06730085\n",
      "Iteration 43, loss = 0.06432539\n",
      "Iteration 44, loss = 0.06426826\n",
      "Iteration 45, loss = 0.06588131\n",
      "Iteration 46, loss = 0.07359898\n",
      "Iteration 47, loss = 0.07420211\n",
      "Iteration 48, loss = 0.07184030\n",
      "Iteration 1, loss = 0.84804517\n",
      "Iteration 49, loss = 0.06129348\n",
      "Iteration 2, loss = 0.54184341\n",
      "Iteration 50, loss = 0.06162852\n",
      "Iteration 3, loss = 0.41828387\n",
      "Iteration 51, loss = 0.06475798\n",
      "Iteration 4, loss = 0.37495163\n",
      "Iteration 52, loss = 0.07975221\n",
      "Iteration 5, loss = 0.34743798\n",
      "Iteration 53, loss = 0.05899923\n",
      "Iteration 6, loss = 0.31844795\n",
      "Iteration 54, loss = 0.08392568\n",
      "Iteration 7, loss = 0.29225702\n",
      "Iteration 55, loss = 0.07629022\n",
      "Iteration 8, loss = 0.27667559\n",
      "Iteration 56, loss = 0.08106824\n",
      "Iteration 9, loss = 0.27781433\n",
      "Iteration 57, loss = 0.11187001\n",
      "Iteration 10, loss = 0.24682002\n",
      "Iteration 58, loss = 0.08062602\n",
      "Iteration 11, loss = 0.23799124\n",
      "Iteration 59, loss = 0.07139827\n",
      "Iteration 12, loss = 0.22082442\n",
      "Iteration 60, loss = 0.06156512\n",
      "Iteration 13, loss = 0.20154398\n",
      "Iteration 61, loss = 0.06764672\n",
      "Iteration 14, loss = 0.19039569\n",
      "Iteration 62, loss = 0.07524177\n",
      "Iteration 15, loss = 0.17923530\n",
      "Iteration 63, loss = 0.06199241\n",
      "Iteration 16, loss = 0.17284471\n",
      "Iteration 64, loss = 0.05079383\n",
      "Iteration 17, loss = 0.16092910\n",
      "Iteration 65, loss = 0.05397109\n",
      "Iteration 18, loss = 0.15948756\n",
      "Iteration 66, loss = 0.04688536\n",
      "Iteration 19, loss = 0.15270280\n",
      "Iteration 67, loss = 0.04537837\n",
      "Iteration 20, loss = 0.14328924\n",
      "Iteration 68, loss = 0.04353304\n",
      "Iteration 21, loss = 0.13316303\n",
      "Iteration 69, loss = 0.04159312\n",
      "Iteration 22, loss = 0.13762173\n",
      "Iteration 70, loss = 0.03574137\n",
      "Iteration 23, loss = 0.13260400\n",
      "Iteration 71, loss = 0.03790233\n",
      "Iteration 24, loss = 0.13188959\n",
      "Iteration 72, loss = 0.03444092\n",
      "Iteration 25, loss = 0.13211474\n",
      "Iteration 73, loss = 0.03275583\n",
      "Iteration 26, loss = 0.14179865\n",
      "Iteration 74, loss = 0.03385643\n",
      "Iteration 27, loss = 0.14316052\n",
      "Iteration 75, loss = 0.03403220\n",
      "Iteration 28, loss = 0.17322690\n",
      "Iteration 76, loss = 0.02962207\n",
      "Iteration 29, loss = 0.15329591\n",
      "Iteration 77, loss = 0.02931439\n",
      "Iteration 30, loss = 0.17767263\n",
      "Iteration 78, loss = 0.02711045\n",
      "Iteration 31, loss = 0.18156617\n",
      "Iteration 79, loss = 0.02837677\n",
      "Iteration 32, loss = 0.15884878\n",
      "Iteration 80, loss = 0.03069951\n",
      "Iteration 33, loss = 0.13123215\n",
      "Iteration 81, loss = 0.05243124\n",
      "Iteration 34, loss = 0.11498683\n",
      "Iteration 82, loss = 0.04920615\n",
      "Iteration 35, loss = 0.11190104\n",
      "Iteration 83, loss = 0.03658125\n",
      "Iteration 36, loss = 0.09698889\n",
      "Iteration 84, loss = 0.04721434\n",
      "Iteration 37, loss = 0.09796688\n",
      "Iteration 85, loss = 0.04244193\n",
      "Iteration 38, loss = 0.09464889\n",
      "Iteration 86, loss = 0.07258870\n",
      "Iteration 39, loss = 0.08825613\n",
      "Iteration 87, loss = 0.04250903\n",
      "Iteration 40, loss = 0.08473514\n",
      "Iteration 88, loss = 0.05728970\n",
      "Iteration 41, loss = 0.09036747\n",
      "Iteration 89, loss = 0.05040412\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.08426727\n",
      "Iteration 43, loss = 0.07755360\n",
      "Iteration 44, loss = 0.07321235\n",
      "Iteration 45, loss = 0.07065248\n",
      "Iteration 46, loss = 0.07371155\n",
      "Iteration 47, loss = 0.07003632\n",
      "Iteration 48, loss = 0.06869801\n",
      "Iteration 49, loss = 0.06420633\n",
      "Iteration 50, loss = 0.08568075\n",
      "Iteration 51, loss = 0.12139327\n",
      "Iteration 52, loss = 0.07243868\n",
      "Iteration 53, loss = 0.07447683\n",
      "Iteration 54, loss = 0.08052192\n",
      "Iteration 55, loss = 0.07674088\n",
      "Iteration 56, loss = 0.07729803\n",
      "Iteration 57, loss = 0.09761063\n",
      "Iteration 58, loss = 0.08665584\n",
      "Iteration 59, loss = 0.08054536\n",
      "Iteration 60, loss = 0.08467998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97080788\n",
      "Iteration 2, loss = 0.53494052\n",
      "Iteration 3, loss = 0.41358890\n",
      "Iteration 4, loss = 0.37548321\n",
      "Iteration 5, loss = 0.32933273\n",
      "Iteration 6, loss = 0.30497898\n",
      "Iteration 7, loss = 0.28653243\n",
      "Iteration 8, loss = 0.27030088\n",
      "Iteration 9, loss = 0.25868387\n",
      "Iteration 10, loss = 0.25132239\n",
      "Iteration 11, loss = 0.23267615\n",
      "Iteration 12, loss = 0.21851620\n",
      "Iteration 13, loss = 0.20637095\n",
      "Iteration 14, loss = 0.19428204\n",
      "Iteration 15, loss = 0.18622294\n",
      "Iteration 16, loss = 0.17316117\n",
      "Iteration 17, loss = 0.16714758\n",
      "Iteration 18, loss = 0.15940942\n",
      "Iteration 19, loss = 0.17683063\n",
      "Iteration 1, loss = 0.81897277\n",
      "Iteration 20, loss = 0.17681294\n",
      "Iteration 2, loss = 0.52421785\n",
      "Iteration 21, loss = 0.16118017\n",
      "Iteration 3, loss = 0.44625062\n",
      "Iteration 22, loss = 0.15381411\n",
      "Iteration 4, loss = 0.39049723\n",
      "Iteration 23, loss = 0.13625567\n",
      "Iteration 5, loss = 0.36252257\n",
      "Iteration 24, loss = 0.12668776\n",
      "Iteration 6, loss = 0.35495821\n",
      "Iteration 25, loss = 0.12283781\n",
      "Iteration 7, loss = 0.30873128\n",
      "Iteration 26, loss = 0.11912376\n",
      "Iteration 8, loss = 0.28784224\n",
      "Iteration 27, loss = 0.10570358\n",
      "Iteration 9, loss = 0.26171106\n",
      "Iteration 28, loss = 0.10036106\n",
      "Iteration 10, loss = 0.24156380\n",
      "Iteration 29, loss = 0.09608318\n",
      "Iteration 11, loss = 0.23839512\n",
      "Iteration 30, loss = 0.10777749\n",
      "Iteration 12, loss = 0.21972351\n",
      "Iteration 31, loss = 0.10370894\n",
      "Iteration 13, loss = 0.20182980\n",
      "Iteration 32, loss = 0.09056436\n",
      "Iteration 14, loss = 0.19271205\n",
      "Iteration 33, loss = 0.10701885\n",
      "Iteration 15, loss = 0.17477722\n",
      "Iteration 34, loss = 0.09343550\n",
      "Iteration 16, loss = 0.17229513\n",
      "Iteration 35, loss = 0.08562846\n",
      "Iteration 17, loss = 0.16402654\n",
      "Iteration 36, loss = 0.08687799\n",
      "Iteration 18, loss = 0.15549205\n",
      "Iteration 37, loss = 0.09653839\n",
      "Iteration 19, loss = 0.14268665\n",
      "Iteration 38, loss = 0.08655208\n",
      "Iteration 20, loss = 0.14337105\n",
      "Iteration 39, loss = 0.09070674\n",
      "Iteration 21, loss = 0.12799965\n",
      "Iteration 40, loss = 0.10200051\n",
      "Iteration 22, loss = 0.12674533\n",
      "Iteration 41, loss = 0.07456828\n",
      "Iteration 23, loss = 0.11581377\n",
      "Iteration 42, loss = 0.06001296\n",
      "Iteration 24, loss = 0.11074472\n",
      "Iteration 43, loss = 0.06174797\n",
      "Iteration 25, loss = 0.11212253\n",
      "Iteration 44, loss = 0.05649718\n",
      "Iteration 26, loss = 0.10930301\n",
      "Iteration 45, loss = 0.06668948\n",
      "Iteration 27, loss = 0.09605274\n",
      "Iteration 46, loss = 0.05498149\n",
      "Iteration 28, loss = 0.09611008\n",
      "Iteration 47, loss = 0.05478598\n",
      "Iteration 29, loss = 0.08895053\n",
      "Iteration 48, loss = 0.05427181\n",
      "Iteration 30, loss = 0.08888994\n",
      "Iteration 49, loss = 0.06818085\n",
      "Iteration 31, loss = 0.08244577\n",
      "Iteration 50, loss = 0.06316217\n",
      "Iteration 32, loss = 0.08371899\n",
      "Iteration 51, loss = 0.06776051\n",
      "Iteration 33, loss = 0.08328727\n",
      "Iteration 52, loss = 0.08096207\n",
      "Iteration 34, loss = 0.08354694\n",
      "Iteration 53, loss = 0.08419528\n",
      "Iteration 35, loss = 0.08113617\n",
      "Iteration 54, loss = 0.13458401\n",
      "Iteration 36, loss = 0.07255114\n",
      "Iteration 55, loss = 0.12390494\n",
      "Iteration 37, loss = 0.07565950\n",
      "Iteration 56, loss = 0.10488679\n",
      "Iteration 38, loss = 0.09081503\n",
      "Iteration 57, loss = 0.10103006\n",
      "Iteration 39, loss = 0.07184637\n",
      "Iteration 58, loss = 0.09713429\n",
      "Iteration 40, loss = 0.07139921\n",
      "Iteration 59, loss = 0.09052869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.08813560\n",
      "Iteration 42, loss = 0.08225510\n",
      "Iteration 43, loss = 0.09296299\n",
      "Iteration 44, loss = 0.10638681\n",
      "Iteration 45, loss = 0.09708487\n",
      "Iteration 46, loss = 0.07947120\n",
      "Iteration 47, loss = 0.06705094\n",
      "Iteration 48, loss = 0.05741682\n",
      "Iteration 49, loss = 0.06259778\n",
      "Iteration 50, loss = 0.05129904\n",
      "Iteration 51, loss = 0.04688117\n",
      "Iteration 52, loss = 0.04373514\n",
      "Iteration 53, loss = 0.04439693\n",
      "Iteration 54, loss = 0.04475351\n",
      "Iteration 55, loss = 0.03740929\n",
      "Iteration 56, loss = 0.03616020\n",
      "Iteration 57, loss = 0.03637042\n",
      "Iteration 58, loss = 0.04036978\n",
      "Iteration 1, loss = 0.87282790\n",
      "Iteration 59, loss = 0.04410434\n",
      "Iteration 2, loss = 0.46403443\n",
      "Iteration 60, loss = 0.04063594\n",
      "Iteration 3, loss = 0.37352139\n",
      "Iteration 61, loss = 0.04840870\n",
      "Iteration 4, loss = 0.34254401\n",
      "Iteration 62, loss = 0.05295941\n",
      "Iteration 5, loss = 0.31621347\n",
      "Iteration 63, loss = 0.03765938\n",
      "Iteration 64, loss = 0.04140910\n",
      "Iteration 6, loss = 0.27687957\n",
      "Iteration 65, loss = 0.04702789\n",
      "Iteration 7, loss = 0.25198047\n",
      "Iteration 66, loss = 0.04928395\n",
      "Iteration 8, loss = 0.23595119\n",
      "Iteration 67, loss = 0.06225588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 0.21948256\n",
      "Iteration 10, loss = 0.20708250\n",
      "Iteration 11, loss = 0.19255595\n",
      "Iteration 12, loss = 0.17102426\n",
      "Iteration 13, loss = 0.16881551\n",
      "Iteration 14, loss = 0.15634461\n",
      "Iteration 15, loss = 0.15482643\n",
      "Iteration 16, loss = 0.13407722\n",
      "Iteration 17, loss = 0.12801450\n",
      "Iteration 18, loss = 0.13055329\n",
      "Iteration 19, loss = 0.11622629\n",
      "Iteration 20, loss = 0.10129663\n",
      "Iteration 21, loss = 0.09217227\n",
      "Iteration 22, loss = 0.09409782\n",
      "Iteration 23, loss = 0.09201422\n",
      "Iteration 24, loss = 0.08909597\n",
      "Iteration 25, loss = 0.09773008\n",
      "Iteration 26, loss = 0.08936868\n",
      "Iteration 27, loss = 0.08478373\n",
      "Iteration 1, loss = 0.85391162\n",
      "Iteration 28, loss = 0.08943272\n",
      "Iteration 2, loss = 0.46991640\n",
      "Iteration 29, loss = 0.10800193\n",
      "Iteration 3, loss = 0.39680206\n",
      "Iteration 30, loss = 0.10018573\n",
      "Iteration 4, loss = 0.34737717\n",
      "Iteration 31, loss = 0.09288866\n",
      "Iteration 5, loss = 0.30735842\n",
      "Iteration 32, loss = 0.10158129\n",
      "Iteration 6, loss = 0.28451836\n",
      "Iteration 33, loss = 0.08250444\n",
      "Iteration 7, loss = 0.25805542\n",
      "Iteration 34, loss = 0.06780303\n",
      "Iteration 8, loss = 0.23920877\n",
      "Iteration 35, loss = 0.06516563\n",
      "Iteration 9, loss = 0.22149758\n",
      "Iteration 36, loss = 0.06339443\n",
      "Iteration 10, loss = 0.20686356\n",
      "Iteration 37, loss = 0.07210915\n",
      "Iteration 11, loss = 0.20134734\n",
      "Iteration 38, loss = 0.07548875\n",
      "Iteration 12, loss = 0.18412796\n",
      "Iteration 39, loss = 0.06213194\n",
      "Iteration 13, loss = 0.17378770\n",
      "Iteration 40, loss = 0.05816534\n",
      "Iteration 14, loss = 0.17136197\n",
      "Iteration 41, loss = 0.05570877\n",
      "Iteration 15, loss = 0.15907409\n",
      "Iteration 42, loss = 0.04721699\n",
      "Iteration 16, loss = 0.15228804\n",
      "Iteration 43, loss = 0.04098302\n",
      "Iteration 17, loss = 0.17215806\n",
      "Iteration 44, loss = 0.04057895\n",
      "Iteration 18, loss = 0.14797421\n",
      "Iteration 45, loss = 0.03942701\n",
      "Iteration 19, loss = 0.14149786\n",
      "Iteration 46, loss = 0.03982866\n",
      "Iteration 20, loss = 0.13439211\n",
      "Iteration 47, loss = 0.05161574\n",
      "Iteration 21, loss = 0.14909595\n",
      "Iteration 48, loss = 0.04987786\n",
      "Iteration 22, loss = 0.13545810\n",
      "Iteration 49, loss = 0.04902257\n",
      "Iteration 23, loss = 0.14959217\n",
      "Iteration 50, loss = 0.04543280\n",
      "Iteration 24, loss = 0.15240392\n",
      "Iteration 51, loss = 0.04176871\n",
      "Iteration 25, loss = 0.15453013\n",
      "Iteration 52, loss = 0.03911858\n",
      "Iteration 26, loss = 0.18465493\n",
      "Iteration 53, loss = 0.03481275\n",
      "Iteration 27, loss = 0.12584919\n",
      "Iteration 54, loss = 0.04465322\n",
      "Iteration 28, loss = 0.12812036\n",
      "Iteration 55, loss = 0.04843831\n",
      "Iteration 29, loss = 0.12494846\n",
      "Iteration 56, loss = 0.04735711\n",
      "Iteration 30, loss = 0.09836282\n",
      "Iteration 57, loss = 0.06323759\n",
      "Iteration 31, loss = 0.08830441\n",
      "Iteration 58, loss = 0.06939301\n",
      "Iteration 32, loss = 0.08290304\n",
      "Iteration 33, loss = 0.08702312\n",
      "Iteration 59, loss = 0.07366098\n",
      "Iteration 34, loss = 0.07703092\n",
      "Iteration 60, loss = 0.06793013\n",
      "Iteration 35, loss = 0.07784194\n",
      "Iteration 61, loss = 0.04390773\n",
      "Iteration 36, loss = 0.07385116\n",
      "Iteration 62, loss = 0.03511128\n",
      "Iteration 37, loss = 0.08055481\n",
      "Iteration 63, loss = 0.05180380\n",
      "Iteration 38, loss = 0.07813368\n",
      "Iteration 64, loss = 0.05567673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.09610541\n",
      "Iteration 40, loss = 0.07660561\n",
      "Iteration 41, loss = 0.06280922\n",
      "Iteration 42, loss = 0.05946509\n",
      "Iteration 43, loss = 0.06952938\n",
      "Iteration 44, loss = 0.05659523\n",
      "Iteration 45, loss = 0.06058083\n",
      "Iteration 46, loss = 0.07297111\n",
      "Iteration 47, loss = 0.06641899\n",
      "Iteration 48, loss = 0.06258854\n",
      "Iteration 49, loss = 0.04913351\n",
      "Iteration 50, loss = 0.05035293\n",
      "Iteration 51, loss = 0.04567518\n",
      "Iteration 52, loss = 0.05585735\n",
      "Iteration 53, loss = 0.05503346\n",
      "Iteration 54, loss = 0.05695735\n",
      "Iteration 55, loss = 0.04449177\n",
      "Iteration 56, loss = 0.04719009\n",
      "Iteration 1, loss = 0.93458792\n",
      "Iteration 57, loss = 0.04167747\n",
      "Iteration 2, loss = 0.49637688\n",
      "Iteration 58, loss = 0.04376018\n",
      "Iteration 3, loss = 0.42707872\n",
      "Iteration 59, loss = 0.05886444\n",
      "Iteration 4, loss = 0.39723288\n",
      "Iteration 60, loss = 0.06125908\n",
      "Iteration 5, loss = 0.35049318\n",
      "Iteration 61, loss = 0.07673451\n",
      "Iteration 6, loss = 0.33440626\n",
      "Iteration 62, loss = 0.04968839\n",
      "Iteration 7, loss = 0.28921237\n",
      "Iteration 63, loss = 0.05661386\n",
      "Iteration 8, loss = 0.27592967\n",
      "Iteration 64, loss = 0.05170210\n",
      "Iteration 9, loss = 0.26140536\n",
      "Iteration 65, loss = 0.07749496\n",
      "Iteration 10, loss = 0.23989342\n",
      "Iteration 66, loss = 0.06998122\n",
      "Iteration 11, loss = 0.21695618\n",
      "Iteration 67, loss = 0.06271428\n",
      "Iteration 12, loss = 0.20713322\n",
      "Iteration 68, loss = 0.05892973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.21451894\n",
      "Iteration 14, loss = 0.18916808\n",
      "Iteration 15, loss = 0.17193665\n",
      "Iteration 16, loss = 0.16429704\n",
      "Iteration 17, loss = 0.15412114\n",
      "Iteration 18, loss = 0.15348972\n",
      "Iteration 19, loss = 0.15783379\n",
      "Iteration 20, loss = 0.16148091\n",
      "Iteration 21, loss = 0.14714053\n",
      "Iteration 22, loss = 0.15444277\n",
      "Iteration 23, loss = 0.13898626\n",
      "Iteration 24, loss = 0.13283610\n",
      "Iteration 25, loss = 0.12447927\n",
      "Iteration 26, loss = 0.10839178\n",
      "Iteration 27, loss = 0.11713889\n",
      "Iteration 28, loss = 0.09917956\n",
      "Iteration 29, loss = 0.09699874\n",
      "Iteration 30, loss = 0.09970253\n",
      "Iteration 31, loss = 0.11311740\n",
      "Iteration 1, loss = 0.83834809\n",
      "Iteration 32, loss = 0.09260092\n",
      "Iteration 2, loss = 0.46457600\n",
      "Iteration 33, loss = 0.09215695\n",
      "Iteration 3, loss = 0.39291691\n",
      "Iteration 34, loss = 0.09639433\n",
      "Iteration 4, loss = 0.37243994\n",
      "Iteration 35, loss = 0.10051382\n",
      "Iteration 5, loss = 0.34011210\n",
      "Iteration 36, loss = 0.11063631\n",
      "Iteration 6, loss = 0.30338201\n",
      "Iteration 37, loss = 0.09669155\n",
      "Iteration 7, loss = 0.28287916\n",
      "Iteration 38, loss = 0.11872898\n",
      "Iteration 8, loss = 0.26728904\n",
      "Iteration 39, loss = 0.08957600\n",
      "Iteration 9, loss = 0.24482371\n",
      "Iteration 40, loss = 0.06838204\n",
      "Iteration 10, loss = 0.22589458\n",
      "Iteration 41, loss = 0.07201644\n",
      "Iteration 11, loss = 0.22080911\n",
      "Iteration 42, loss = 0.07393652\n",
      "Iteration 12, loss = 0.20997840\n",
      "Iteration 43, loss = 0.07306236\n",
      "Iteration 13, loss = 0.18784011\n",
      "Iteration 44, loss = 0.05963082\n",
      "Iteration 14, loss = 0.17931507\n",
      "Iteration 45, loss = 0.05932256\n",
      "Iteration 15, loss = 0.16520373\n",
      "Iteration 46, loss = 0.05602515\n",
      "Iteration 16, loss = 0.15937275\n",
      "Iteration 47, loss = 0.05373101\n",
      "Iteration 17, loss = 0.14531014\n",
      "Iteration 48, loss = 0.06473372\n",
      "Iteration 18, loss = 0.13931086\n",
      "Iteration 49, loss = 0.06972606\n",
      "Iteration 19, loss = 0.14066176\n",
      "Iteration 50, loss = 0.06417842\n",
      "Iteration 20, loss = 0.12795225\n",
      "Iteration 51, loss = 0.05015447\n",
      "Iteration 21, loss = 0.12784484\n",
      "Iteration 52, loss = 0.04743851\n",
      "Iteration 22, loss = 0.12554429\n",
      "Iteration 53, loss = 0.05479232\n",
      "Iteration 23, loss = 0.12594701\n",
      "Iteration 54, loss = 0.06310532\n",
      "Iteration 24, loss = 0.11982615\n",
      "Iteration 55, loss = 0.06436426\n",
      "Iteration 25, loss = 0.10178651\n",
      "Iteration 56, loss = 0.06263959\n",
      "Iteration 26, loss = 0.09749353\n",
      "Iteration 57, loss = 0.07239700\n",
      "Iteration 27, loss = 0.11363537\n",
      "Iteration 58, loss = 0.06010470\n",
      "Iteration 28, loss = 0.10794504\n",
      "Iteration 59, loss = 0.05356452\n",
      "Iteration 29, loss = 0.10125153\n",
      "Iteration 60, loss = 0.04689029\n",
      "Iteration 30, loss = 0.09489592\n",
      "Iteration 61, loss = 0.06180140\n",
      "Iteration 31, loss = 0.09567709\n",
      "Iteration 62, loss = 0.07726319\n",
      "Iteration 32, loss = 0.08368680\n",
      "Iteration 63, loss = 0.04976074\n",
      "Iteration 33, loss = 0.09519165\n",
      "Iteration 64, loss = 0.05153832\n",
      "Iteration 34, loss = 0.09566843\n",
      "Iteration 65, loss = 0.04901643\n",
      "Iteration 35, loss = 0.09683417\n",
      "Iteration 66, loss = 0.04399479\n",
      "Iteration 36, loss = 0.09184281\n",
      "Iteration 67, loss = 0.04210602\n",
      "Iteration 37, loss = 0.09485996\n",
      "Iteration 68, loss = 0.04423345\n",
      "Iteration 38, loss = 0.10076428\n",
      "Iteration 69, loss = 0.03316117\n",
      "Iteration 39, loss = 0.10918208\n",
      "Iteration 70, loss = 0.04133979\n",
      "Iteration 40, loss = 0.08008281\n",
      "Iteration 71, loss = 0.04799405\n",
      "Iteration 41, loss = 0.07558760\n",
      "Iteration 72, loss = 0.03334604\n",
      "Iteration 42, loss = 0.06568696\n",
      "Iteration 73, loss = 0.04129610\n",
      "Iteration 43, loss = 0.06009620\n",
      "Iteration 74, loss = 0.03579280\n",
      "Iteration 44, loss = 0.05572441\n",
      "Iteration 75, loss = 0.03503216\n",
      "Iteration 45, loss = 0.05168743\n",
      "Iteration 76, loss = 0.03232778\n",
      "Iteration 46, loss = 0.04791351\n",
      "Iteration 77, loss = 0.03215952\n",
      "Iteration 47, loss = 0.04068077\n",
      "Iteration 78, loss = 0.03834671\n",
      "Iteration 48, loss = 0.04440553\n",
      "Iteration 79, loss = 0.03640838\n",
      "Iteration 49, loss = 0.03484878\n",
      "Iteration 80, loss = 0.05248086\n",
      "Iteration 50, loss = 0.03685824\n",
      "Iteration 81, loss = 0.03249061\n",
      "Iteration 51, loss = 0.04061883\n",
      "Iteration 82, loss = 0.03387884\n",
      "Iteration 52, loss = 0.03584808\n",
      "Iteration 83, loss = 0.04130782\n",
      "Iteration 53, loss = 0.03512292\n",
      "Iteration 84, loss = 0.03833539\n",
      "Iteration 54, loss = 0.03771668\n",
      "Iteration 85, loss = 0.03514749\n",
      "Iteration 55, loss = 0.04198558\n",
      "Iteration 86, loss = 0.04312901\n",
      "Iteration 56, loss = 0.03793398\n",
      "Iteration 87, loss = 0.03405250\n",
      "Iteration 57, loss = 0.03120067\n",
      "Iteration 88, loss = 0.04401540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.04006302\n",
      "Iteration 59, loss = 0.03031218\n",
      "Iteration 60, loss = 0.03031306\n",
      "Iteration 61, loss = 0.02918030\n",
      "Iteration 62, loss = 0.02955025\n",
      "Iteration 63, loss = 0.02434665\n",
      "Iteration 64, loss = 0.02256763\n",
      "Iteration 65, loss = 0.02437756\n",
      "Iteration 66, loss = 0.02322871\n",
      "Iteration 67, loss = 0.04424129\n",
      "Iteration 68, loss = 0.02253478\n",
      "Iteration 69, loss = 0.02823558\n",
      "Iteration 70, loss = 0.02650098\n",
      "Iteration 71, loss = 0.01709209\n",
      "Iteration 72, loss = 0.02120784\n",
      "Iteration 73, loss = 0.02335512\n",
      "Iteration 74, loss = 0.02314583\n",
      "Iteration 75, loss = 0.04103677\n",
      "Iteration 76, loss = 0.05742676\n",
      "Iteration 1, loss = 0.79916220\n",
      "Iteration 77, loss = 0.02188581\n",
      "Iteration 2, loss = 0.49742895\n",
      "Iteration 78, loss = 0.01811592\n",
      "Iteration 3, loss = 0.41011788\n",
      "Iteration 79, loss = 0.01577037\n",
      "Iteration 4, loss = 0.38144770\n",
      "Iteration 80, loss = 0.01975194\n",
      "Iteration 5, loss = 0.34157668\n",
      "Iteration 81, loss = 0.02017900\n",
      "Iteration 6, loss = 0.31017587\n",
      "Iteration 82, loss = 0.02452020\n",
      "Iteration 7, loss = 0.27973855\n",
      "Iteration 83, loss = 0.03380093\n",
      "Iteration 8, loss = 0.25989557\n",
      "Iteration 84, loss = 0.04186613\n",
      "Iteration 9, loss = 0.24224229\n",
      "Iteration 85, loss = 0.03857785\n",
      "Iteration 10, loss = 0.22974115\n",
      "Iteration 86, loss = 0.02784619\n",
      "Iteration 11, loss = 0.21887198\n",
      "Iteration 87, loss = 0.04652439\n",
      "Iteration 12, loss = 0.20058511\n",
      "Iteration 88, loss = 0.02567400\n",
      "Iteration 13, loss = 0.19115716\n",
      "Iteration 89, loss = 0.03035812\n",
      "Iteration 14, loss = 0.18104719\n",
      "Iteration 90, loss = 0.04587975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.17329466\n",
      "Iteration 16, loss = 0.16180082\n",
      "Iteration 17, loss = 0.14530146\n",
      "Iteration 18, loss = 0.14151568\n",
      "Iteration 19, loss = 0.13871813\n",
      "Iteration 20, loss = 0.13661193\n",
      "Iteration 21, loss = 0.13304852\n",
      "Iteration 22, loss = 0.15047591\n",
      "Iteration 23, loss = 0.15313878\n",
      "Iteration 24, loss = 0.15638521\n",
      "Iteration 25, loss = 0.13406682\n",
      "Iteration 26, loss = 0.12857650\n",
      "Iteration 27, loss = 0.11459864\n",
      "Iteration 28, loss = 0.11010857\n",
      "Iteration 29, loss = 0.10462703\n",
      "Iteration 30, loss = 0.09782590\n",
      "Iteration 31, loss = 0.09722880\n",
      "Iteration 32, loss = 0.09441578\n",
      "Iteration 33, loss = 0.09138605\n",
      "Iteration 1, loss = 1.07858800\n",
      "Iteration 34, loss = 0.09322315\n",
      "Iteration 2, loss = 0.55622465\n",
      "Iteration 35, loss = 0.10744587\n",
      "Iteration 3, loss = 0.45568899\n",
      "Iteration 36, loss = 0.10286202\n",
      "Iteration 4, loss = 0.40999407\n",
      "Iteration 37, loss = 0.11660627\n",
      "Iteration 5, loss = 0.37584240\n",
      "Iteration 38, loss = 0.10664590\n",
      "Iteration 6, loss = 0.32966067\n",
      "Iteration 39, loss = 0.16843094\n",
      "Iteration 7, loss = 0.30742760\n",
      "Iteration 40, loss = 0.12131093\n",
      "Iteration 8, loss = 0.28704910\n",
      "Iteration 9, loss = 0.26296062\n",
      "Iteration 41, loss = 0.13327791\n",
      "Iteration 10, loss = 0.24516207\n",
      "Iteration 42, loss = 0.11717227\n",
      "Iteration 11, loss = 0.23435081\n",
      "Iteration 43, loss = 0.09383787\n",
      "Iteration 12, loss = 0.21293388\n",
      "Iteration 44, loss = 0.09621580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.20671652\n",
      "Iteration 14, loss = 0.20373663\n",
      "Iteration 15, loss = 0.19222436\n",
      "Iteration 16, loss = 0.19753464\n",
      "Iteration 17, loss = 0.17656217\n",
      "Iteration 18, loss = 0.17648857\n",
      "Iteration 19, loss = 0.19651355\n",
      "Iteration 20, loss = 0.16552738\n",
      "Iteration 21, loss = 0.16004449\n",
      "Iteration 22, loss = 0.15988103\n",
      "Iteration 23, loss = 0.14052466\n",
      "Iteration 24, loss = 0.14037078\n",
      "Iteration 25, loss = 0.12712154\n",
      "Iteration 26, loss = 0.12156103\n",
      "Iteration 27, loss = 0.13321976\n",
      "Iteration 28, loss = 0.11765922\n",
      "Iteration 29, loss = 0.11802698\n",
      "Iteration 30, loss = 0.10830556\n",
      "Iteration 1, loss = 1.23970876\n",
      "Iteration 31, loss = 0.10057702\n",
      "Iteration 2, loss = 1.02869466\n",
      "Iteration 32, loss = 0.10326710\n",
      "Iteration 3, loss = 0.89977124\n",
      "Iteration 33, loss = 0.09721203\n",
      "Iteration 4, loss = 0.80241309\n",
      "Iteration 34, loss = 0.08902581\n",
      "Iteration 5, loss = 0.73193570\n",
      "Iteration 35, loss = 0.10998977\n",
      "Iteration 6, loss = 0.67453038\n",
      "Iteration 36, loss = 0.08656807\n",
      "Iteration 7, loss = 0.62544033\n",
      "Iteration 37, loss = 0.08347678\n",
      "Iteration 8, loss = 0.58580607\n",
      "Iteration 38, loss = 0.08613691\n",
      "Iteration 9, loss = 0.55576587\n",
      "Iteration 39, loss = 0.07452338\n",
      "Iteration 10, loss = 0.52867661\n",
      "Iteration 40, loss = 0.07840667\n",
      "Iteration 11, loss = 0.50599025\n",
      "Iteration 41, loss = 0.06989718\n",
      "Iteration 12, loss = 0.48736567\n",
      "Iteration 42, loss = 0.06832526\n",
      "Iteration 13, loss = 0.47037295\n",
      "Iteration 43, loss = 0.06566755\n",
      "Iteration 14, loss = 0.45488591\n",
      "Iteration 44, loss = 0.06586949\n",
      "Iteration 15, loss = 0.44057230\n",
      "Iteration 45, loss = 0.07151733\n",
      "Iteration 16, loss = 0.42839285\n",
      "Iteration 46, loss = 0.06122024\n",
      "Iteration 17, loss = 0.41687577\n",
      "Iteration 47, loss = 0.06076934\n",
      "Iteration 18, loss = 0.40635914\n",
      "Iteration 48, loss = 0.06106686\n",
      "Iteration 19, loss = 0.39718303\n",
      "Iteration 49, loss = 0.06585347\n",
      "Iteration 20, loss = 0.38818980\n",
      "Iteration 50, loss = 0.06096027\n",
      "Iteration 21, loss = 0.38000044\n",
      "Iteration 51, loss = 0.06751015\n",
      "Iteration 22, loss = 0.37155920\n",
      "Iteration 52, loss = 0.06154794\n",
      "Iteration 23, loss = 0.36447418\n",
      "Iteration 53, loss = 0.06604280\n",
      "Iteration 24, loss = 0.35806077\n",
      "Iteration 54, loss = 0.04963508\n",
      "Iteration 25, loss = 0.35118052\n",
      "Iteration 55, loss = 0.06782003\n",
      "Iteration 26, loss = 0.34510924\n",
      "Iteration 56, loss = 0.07852661\n",
      "Iteration 27, loss = 0.33937963\n",
      "Iteration 57, loss = 0.11889397\n",
      "Iteration 28, loss = 0.33316366\n",
      "Iteration 58, loss = 0.13115695\n",
      "Iteration 29, loss = 0.32814016\n",
      "Iteration 59, loss = 0.11322371\n",
      "Iteration 30, loss = 0.32280935\n",
      "Iteration 60, loss = 0.10513554\n",
      "Iteration 31, loss = 0.31775337\n",
      "Iteration 61, loss = 0.07790043\n",
      "Iteration 32, loss = 0.31313893\n",
      "Iteration 62, loss = 0.09973616\n",
      "Iteration 33, loss = 0.30789909\n",
      "Iteration 63, loss = 0.08307673\n",
      "Iteration 34, loss = 0.30324045\n",
      "Iteration 64, loss = 0.07930647\n",
      "Iteration 35, loss = 0.29887912\n",
      "Iteration 65, loss = 0.06877480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.29416950\n",
      "Iteration 37, loss = 0.29172342\n",
      "Iteration 38, loss = 0.28601254\n",
      "Iteration 39, loss = 0.28224138\n",
      "Iteration 40, loss = 0.27760526\n",
      "Iteration 41, loss = 0.27417971\n",
      "Iteration 42, loss = 0.27001209\n",
      "Iteration 43, loss = 0.26733565\n",
      "Iteration 44, loss = 0.26241720\n",
      "Iteration 45, loss = 0.25819658\n",
      "Iteration 46, loss = 0.25534702\n",
      "Iteration 47, loss = 0.25140952\n",
      "Iteration 48, loss = 0.24806673\n",
      "Iteration 49, loss = 0.24421903\n",
      "Iteration 50, loss = 0.24076875\n",
      "Iteration 51, loss = 0.23781759\n",
      "Iteration 52, loss = 0.23411269\n",
      "Iteration 53, loss = 0.23188633\n",
      "Iteration 54, loss = 0.22875786\n",
      "Iteration 1, loss = 1.34911355\n",
      "Iteration 55, loss = 0.22480334\n",
      "Iteration 2, loss = 1.10738262\n",
      "Iteration 56, loss = 0.22171696\n",
      "Iteration 3, loss = 0.95128878\n",
      "Iteration 57, loss = 0.21850075\n",
      "Iteration 4, loss = 0.85455697\n",
      "Iteration 58, loss = 0.21572373\n",
      "Iteration 5, loss = 0.77509053\n",
      "Iteration 59, loss = 0.21323134\n",
      "Iteration 6, loss = 0.70453663\n",
      "Iteration 60, loss = 0.21013732\n",
      "Iteration 7, loss = 0.65292921\n",
      "Iteration 61, loss = 0.20818305\n",
      "Iteration 8, loss = 0.61227801\n",
      "Iteration 62, loss = 0.20512242\n",
      "Iteration 9, loss = 0.57600617\n",
      "Iteration 63, loss = 0.20341846\n",
      "Iteration 10, loss = 0.54614050\n",
      "Iteration 64, loss = 0.20040403\n",
      "Iteration 11, loss = 0.52107662\n",
      "Iteration 65, loss = 0.19817714\n",
      "Iteration 12, loss = 0.49832738\n",
      "Iteration 66, loss = 0.19616706\n",
      "Iteration 13, loss = 0.47900239\n",
      "Iteration 14, loss = 0.46219658\n",
      "Iteration 67, loss = 0.19458849\n",
      "Iteration 15, loss = 0.44634801\n",
      "Iteration 68, loss = 0.19233211\n",
      "Iteration 16, loss = 0.43271549\n",
      "Iteration 69, loss = 0.18879386\n",
      "Iteration 17, loss = 0.42071776\n",
      "Iteration 70, loss = 0.18677037\n",
      "Iteration 18, loss = 0.40974691\n",
      "Iteration 71, loss = 0.18393305\n",
      "Iteration 19, loss = 0.39896949\n",
      "Iteration 72, loss = 0.18420190\n",
      "Iteration 20, loss = 0.38922157\n",
      "Iteration 73, loss = 0.18050123\n",
      "Iteration 21, loss = 0.38054372\n",
      "Iteration 74, loss = 0.17896436\n",
      "Iteration 22, loss = 0.37277787\n",
      "Iteration 75, loss = 0.17667486\n",
      "Iteration 23, loss = 0.36461090\n",
      "Iteration 76, loss = 0.17552165\n",
      "Iteration 24, loss = 0.35702961\n",
      "Iteration 77, loss = 0.17208796\n",
      "Iteration 25, loss = 0.35091925\n",
      "Iteration 78, loss = 0.17019210\n",
      "Iteration 26, loss = 0.34428199\n",
      "Iteration 79, loss = 0.16888597\n",
      "Iteration 80, loss = 0.16691149\n",
      "Iteration 27, loss = 0.33781985\n",
      "Iteration 81, loss = 0.16505151\n",
      "Iteration 28, loss = 0.33173330\n",
      "Iteration 82, loss = 0.16263373\n",
      "Iteration 29, loss = 0.32561216\n",
      "Iteration 83, loss = 0.16213951\n",
      "Iteration 30, loss = 0.32062059\n",
      "Iteration 84, loss = 0.16016121\n",
      "Iteration 31, loss = 0.31480762\n",
      "Iteration 85, loss = 0.15774483\n",
      "Iteration 32, loss = 0.31108502\n",
      "Iteration 86, loss = 0.15645136\n",
      "Iteration 33, loss = 0.30555900\n",
      "Iteration 87, loss = 0.15470692\n",
      "Iteration 34, loss = 0.30009677\n",
      "Iteration 88, loss = 0.15365192\n",
      "Iteration 35, loss = 0.29635297\n",
      "Iteration 89, loss = 0.15218765\n",
      "Iteration 36, loss = 0.29125587\n",
      "Iteration 90, loss = 0.15042194\n",
      "Iteration 37, loss = 0.28596010\n",
      "Iteration 91, loss = 0.15075332\n",
      "Iteration 38, loss = 0.28150504\n",
      "Iteration 92, loss = 0.14740718\n",
      "Iteration 39, loss = 0.27789058\n",
      "Iteration 93, loss = 0.14788283\n",
      "Iteration 40, loss = 0.27358748\n",
      "Iteration 94, loss = 0.14423716\n",
      "Iteration 41, loss = 0.26944657\n",
      "Iteration 95, loss = 0.14370659\n",
      "Iteration 42, loss = 0.26664278\n",
      "Iteration 96, loss = 0.14191860\n",
      "Iteration 43, loss = 0.26230982\n",
      "Iteration 97, loss = 0.14069593\n",
      "Iteration 44, loss = 0.25794378\n",
      "Iteration 98, loss = 0.13949131\n",
      "Iteration 45, loss = 0.25407488\n",
      "Iteration 99, loss = 0.13788170\n",
      "Iteration 46, loss = 0.25083457\n",
      "Iteration 100, loss = 0.13728476\n",
      "Iteration 47, loss = 0.24730583\n",
      "Iteration 48, loss = 0.24334547\n",
      "Iteration 49, loss = 0.24085590\n",
      "Iteration 50, loss = 0.23814882\n",
      "Iteration 51, loss = 0.23459379\n",
      "Iteration 52, loss = 0.23153576\n",
      "Iteration 53, loss = 0.22868207\n",
      "Iteration 54, loss = 0.22562098\n",
      "Iteration 55, loss = 0.22326316\n",
      "Iteration 56, loss = 0.22085281\n",
      "Iteration 57, loss = 0.21753384\n",
      "Iteration 58, loss = 0.21548105\n",
      "Iteration 59, loss = 0.21297950\n",
      "Iteration 60, loss = 0.21004719\n",
      "Iteration 61, loss = 0.20769652\n",
      "Iteration 62, loss = 0.20658846\n",
      "Iteration 63, loss = 0.20430788\n",
      "Iteration 64, loss = 0.20151097\n",
      "Iteration 65, loss = 0.19918916\n",
      "Iteration 1, loss = 1.20951751\n",
      "Iteration 66, loss = 0.19629001\n",
      "Iteration 2, loss = 1.06384766\n",
      "Iteration 67, loss = 0.19363015\n",
      "Iteration 3, loss = 0.96927873\n",
      "Iteration 68, loss = 0.19189519\n",
      "Iteration 4, loss = 0.89251031\n",
      "Iteration 69, loss = 0.18966598\n",
      "Iteration 5, loss = 0.81522164\n",
      "Iteration 70, loss = 0.18820126\n",
      "Iteration 6, loss = 0.74213404\n",
      "Iteration 71, loss = 0.18656798\n",
      "Iteration 7, loss = 0.68018842\n",
      "Iteration 72, loss = 0.18402558\n",
      "Iteration 8, loss = 0.62796214\n",
      "Iteration 73, loss = 0.18197769\n",
      "Iteration 9, loss = 0.58255778\n",
      "Iteration 74, loss = 0.17956576\n",
      "Iteration 10, loss = 0.54561071\n",
      "Iteration 75, loss = 0.17761792\n",
      "Iteration 11, loss = 0.51560715\n",
      "Iteration 76, loss = 0.17625809\n",
      "Iteration 12, loss = 0.48920361\n",
      "Iteration 77, loss = 0.17441699\n",
      "Iteration 13, loss = 0.46929391\n",
      "Iteration 78, loss = 0.17184773\n",
      "Iteration 14, loss = 0.45096935\n",
      "Iteration 79, loss = 0.17066765\n",
      "Iteration 15, loss = 0.43591404\n",
      "Iteration 80, loss = 0.16974967\n",
      "Iteration 16, loss = 0.42173415\n",
      "Iteration 81, loss = 0.16698004\n",
      "Iteration 17, loss = 0.40917427\n",
      "Iteration 82, loss = 0.16618115\n",
      "Iteration 18, loss = 0.39865133\n",
      "Iteration 83, loss = 0.16457079\n",
      "Iteration 19, loss = 0.38865064\n",
      "Iteration 84, loss = 0.16284911\n",
      "Iteration 20, loss = 0.37984312\n",
      "Iteration 85, loss = 0.16082303\n",
      "Iteration 21, loss = 0.37161546\n",
      "Iteration 86, loss = 0.15923763\n",
      "Iteration 22, loss = 0.36392671\n",
      "Iteration 87, loss = 0.15825105\n",
      "Iteration 23, loss = 0.35696780\n",
      "Iteration 88, loss = 0.15578344\n",
      "Iteration 24, loss = 0.34972223\n",
      "Iteration 89, loss = 0.15486214\n",
      "Iteration 25, loss = 0.34281903\n",
      "Iteration 90, loss = 0.15445488\n",
      "Iteration 26, loss = 0.33710617\n",
      "Iteration 91, loss = 0.15271567\n",
      "Iteration 27, loss = 0.33199268\n",
      "Iteration 92, loss = 0.15112124\n",
      "Iteration 28, loss = 0.32643203\n",
      "Iteration 93, loss = 0.14932233\n",
      "Iteration 29, loss = 0.32010773\n",
      "Iteration 94, loss = 0.14772397\n",
      "Iteration 30, loss = 0.31598014\n",
      "Iteration 95, loss = 0.14727640\n",
      "Iteration 31, loss = 0.31066820\n",
      "Iteration 96, loss = 0.14513904\n",
      "Iteration 32, loss = 0.30651566\n",
      "Iteration 97, loss = 0.14557778\n",
      "Iteration 33, loss = 0.30196528\n",
      "Iteration 98, loss = 0.14325519\n",
      "Iteration 34, loss = 0.29713357\n",
      "Iteration 99, loss = 0.14241468\n",
      "Iteration 35, loss = 0.29378300\n",
      "Iteration 100, loss = 0.14012922\n",
      "Iteration 36, loss = 0.28860936\n",
      "Iteration 37, loss = 0.28557851\n",
      "Iteration 38, loss = 0.28154789\n",
      "Iteration 39, loss = 0.27793952\n",
      "Iteration 40, loss = 0.27424223\n",
      "Iteration 41, loss = 0.27071153\n",
      "Iteration 42, loss = 0.26782579\n",
      "Iteration 43, loss = 0.26385575\n",
      "Iteration 44, loss = 0.26126226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45, loss = 0.25774901\n",
      "Iteration 46, loss = 0.25519046\n",
      "Iteration 47, loss = 0.25126472\n",
      "Iteration 48, loss = 0.24855773\n",
      "Iteration 49, loss = 0.24682221\n",
      "Iteration 50, loss = 0.24298702\n",
      "Iteration 51, loss = 0.24148995\n",
      "Iteration 52, loss = 0.23745966\n",
      "Iteration 53, loss = 0.23565064\n",
      "Iteration 54, loss = 0.23285291\n",
      "Iteration 1, loss = 1.50756600\n",
      "Iteration 55, loss = 0.22995857\n",
      "Iteration 2, loss = 1.20845991\n",
      "Iteration 56, loss = 0.22702136\n",
      "Iteration 3, loss = 1.02022866\n",
      "Iteration 57, loss = 0.22441584\n",
      "Iteration 4, loss = 0.87761896\n",
      "Iteration 58, loss = 0.22256713\n",
      "Iteration 5, loss = 0.77096095\n",
      "Iteration 59, loss = 0.22000621\n",
      "Iteration 6, loss = 0.69032621\n",
      "Iteration 60, loss = 0.21787628\n",
      "Iteration 7, loss = 0.63454699\n",
      "Iteration 61, loss = 0.21595744\n",
      "Iteration 8, loss = 0.59048771\n",
      "Iteration 62, loss = 0.21364267\n",
      "Iteration 9, loss = 0.55384982\n",
      "Iteration 63, loss = 0.21190443\n",
      "Iteration 10, loss = 0.52603129\n",
      "Iteration 64, loss = 0.20885569\n",
      "Iteration 11, loss = 0.50222774\n",
      "Iteration 65, loss = 0.20689955\n",
      "Iteration 12, loss = 0.48281235\n",
      "Iteration 66, loss = 0.20471203\n",
      "Iteration 13, loss = 0.46664488\n",
      "Iteration 67, loss = 0.20257221\n",
      "Iteration 14, loss = 0.45211060\n",
      "Iteration 68, loss = 0.20027884\n",
      "Iteration 15, loss = 0.43919039\n",
      "Iteration 69, loss = 0.19832434\n",
      "Iteration 16, loss = 0.42723362\n",
      "Iteration 70, loss = 0.19655655\n",
      "Iteration 17, loss = 0.41775406\n",
      "Iteration 71, loss = 0.19385835\n",
      "Iteration 18, loss = 0.40947117\n",
      "Iteration 72, loss = 0.19392601\n",
      "Iteration 19, loss = 0.40221159\n",
      "Iteration 73, loss = 0.19006912\n",
      "Iteration 20, loss = 0.39187910\n",
      "Iteration 74, loss = 0.18876995\n",
      "Iteration 21, loss = 0.38475834\n",
      "Iteration 75, loss = 0.18641015\n",
      "Iteration 22, loss = 0.37723082\n",
      "Iteration 76, loss = 0.18517796\n",
      "Iteration 23, loss = 0.37092027\n",
      "Iteration 77, loss = 0.18377336\n",
      "Iteration 24, loss = 0.36456792\n",
      "Iteration 78, loss = 0.18095668\n",
      "Iteration 25, loss = 0.35856497\n",
      "Iteration 79, loss = 0.17900223\n",
      "Iteration 26, loss = 0.35180897\n",
      "Iteration 80, loss = 0.17794140\n",
      "Iteration 27, loss = 0.34615865\n",
      "Iteration 81, loss = 0.17581505\n",
      "Iteration 28, loss = 0.34052848\n",
      "Iteration 82, loss = 0.17500100\n",
      "Iteration 29, loss = 0.33561076\n",
      "Iteration 83, loss = 0.17274491\n",
      "Iteration 30, loss = 0.33036479\n",
      "Iteration 84, loss = 0.17043797\n",
      "Iteration 31, loss = 0.32518577\n",
      "Iteration 85, loss = 0.17108493\n",
      "Iteration 32, loss = 0.32062709\n",
      "Iteration 86, loss = 0.16683938\n",
      "Iteration 33, loss = 0.31548977\n",
      "Iteration 87, loss = 0.16629531\n",
      "Iteration 34, loss = 0.31179701\n",
      "Iteration 88, loss = 0.16547496\n",
      "Iteration 35, loss = 0.30654635\n",
      "Iteration 89, loss = 0.16283769\n",
      "Iteration 36, loss = 0.30152730\n",
      "Iteration 90, loss = 0.16117865\n",
      "Iteration 37, loss = 0.29840090\n",
      "Iteration 91, loss = 0.16020075\n",
      "Iteration 38, loss = 0.29342005\n",
      "Iteration 92, loss = 0.15812510\n",
      "Iteration 39, loss = 0.28832041\n",
      "Iteration 93, loss = 0.15640321\n",
      "Iteration 40, loss = 0.28394735\n",
      "Iteration 94, loss = 0.15519921\n",
      "Iteration 41, loss = 0.28065277\n",
      "Iteration 95, loss = 0.15354063\n",
      "Iteration 42, loss = 0.27606213\n",
      "Iteration 96, loss = 0.15194761\n",
      "Iteration 43, loss = 0.27274461\n",
      "Iteration 97, loss = 0.15160893\n",
      "Iteration 44, loss = 0.26804361\n",
      "Iteration 98, loss = 0.15053302\n",
      "Iteration 45, loss = 0.26548125\n",
      "Iteration 99, loss = 0.14810354\n",
      "Iteration 46, loss = 0.26027164\n",
      "Iteration 100, loss = 0.14707303\n",
      "Iteration 47, loss = 0.25800271\n",
      "Iteration 48, loss = 0.25374594\n",
      "Iteration 49, loss = 0.25017320\n",
      "Iteration 50, loss = 0.24638524\n",
      "Iteration 51, loss = 0.24233083\n",
      "Iteration 52, loss = 0.24034730\n",
      "Iteration 53, loss = 0.23591083\n",
      "Iteration 54, loss = 0.23405429\n",
      "Iteration 55, loss = 0.23026503\n",
      "Iteration 56, loss = 0.22834885\n",
      "Iteration 57, loss = 0.22644347\n",
      "Iteration 58, loss = 0.22192380\n",
      "Iteration 59, loss = 0.21975757\n",
      "Iteration 60, loss = 0.21545440\n",
      "Iteration 61, loss = 0.21718949\n",
      "Iteration 62, loss = 0.21072500\n",
      "Iteration 63, loss = 0.21152699\n",
      "Iteration 64, loss = 0.20727004\n",
      "Iteration 65, loss = 0.20458644\n",
      "Iteration 1, loss = 1.27377056\n",
      "Iteration 66, loss = 0.20136181\n",
      "Iteration 2, loss = 1.08195595\n",
      "Iteration 67, loss = 0.19952987\n",
      "Iteration 3, loss = 0.94326436\n",
      "Iteration 68, loss = 0.19706071\n",
      "Iteration 4, loss = 0.84198926\n",
      "Iteration 69, loss = 0.19454868\n",
      "Iteration 5, loss = 0.76225872\n",
      "Iteration 70, loss = 0.19254407\n",
      "Iteration 6, loss = 0.69914497\n",
      "Iteration 71, loss = 0.19005977\n",
      "Iteration 7, loss = 0.64855427\n",
      "Iteration 72, loss = 0.18830387\n",
      "Iteration 8, loss = 0.60512944\n",
      "Iteration 73, loss = 0.18557196\n",
      "Iteration 9, loss = 0.57052701\n",
      "Iteration 74, loss = 0.18403910\n",
      "Iteration 10, loss = 0.54055688\n",
      "Iteration 75, loss = 0.18176999\n",
      "Iteration 11, loss = 0.51449122\n",
      "Iteration 76, loss = 0.18040160\n",
      "Iteration 12, loss = 0.49127278\n",
      "Iteration 77, loss = 0.17821807\n",
      "Iteration 13, loss = 0.47156900\n",
      "Iteration 78, loss = 0.17722511\n",
      "Iteration 14, loss = 0.45334976\n",
      "Iteration 79, loss = 0.17488433\n",
      "Iteration 15, loss = 0.43737415\n",
      "Iteration 80, loss = 0.17289958\n",
      "Iteration 16, loss = 0.42380055\n",
      "Iteration 81, loss = 0.17012620\n",
      "Iteration 17, loss = 0.40961481\n",
      "Iteration 82, loss = 0.16935097\n",
      "Iteration 18, loss = 0.39912474\n",
      "Iteration 83, loss = 0.16790188\n",
      "Iteration 19, loss = 0.38750882\n",
      "Iteration 84, loss = 0.16491867\n",
      "Iteration 20, loss = 0.37830765\n",
      "Iteration 85, loss = 0.16428832\n",
      "Iteration 21, loss = 0.36780313\n",
      "Iteration 86, loss = 0.16190333\n",
      "Iteration 22, loss = 0.36024701\n",
      "Iteration 87, loss = 0.16188435\n",
      "Iteration 23, loss = 0.35174043\n",
      "Iteration 88, loss = 0.15996173\n",
      "Iteration 24, loss = 0.34375969\n",
      "Iteration 89, loss = 0.15806178\n",
      "Iteration 25, loss = 0.33641382\n",
      "Iteration 90, loss = 0.15635551\n",
      "Iteration 26, loss = 0.33025936\n",
      "Iteration 91, loss = 0.15376945\n",
      "Iteration 27, loss = 0.32367446\n",
      "Iteration 92, loss = 0.15309778\n",
      "Iteration 28, loss = 0.31735108\n",
      "Iteration 93, loss = 0.15220488\n",
      "Iteration 29, loss = 0.31165186\n",
      "Iteration 94, loss = 0.15061266\n",
      "Iteration 30, loss = 0.30580346\n",
      "Iteration 95, loss = 0.14976749\n",
      "Iteration 31, loss = 0.29998389\n",
      "Iteration 96, loss = 0.14794379\n",
      "Iteration 32, loss = 0.29460541\n",
      "Iteration 97, loss = 0.14594501\n",
      "Iteration 33, loss = 0.28963626\n",
      "Iteration 98, loss = 0.14494282\n",
      "Iteration 34, loss = 0.28512526\n",
      "Iteration 99, loss = 0.14370725\n",
      "Iteration 35, loss = 0.27978649\n",
      "Iteration 100, loss = 0.14178875\n",
      "Iteration 36, loss = 0.27538837\n",
      "Iteration 37, loss = 0.27078509\n",
      "Iteration 38, loss = 0.26681429\n",
      "Iteration 39, loss = 0.26383262\n",
      "Iteration 40, loss = 0.25757384\n",
      "Iteration 41, loss = 0.25376562\n",
      "Iteration 42, loss = 0.24934092\n",
      "Iteration 43, loss = 0.24572128\n",
      "Iteration 44, loss = 0.24181708\n",
      "Iteration 45, loss = 0.23783247\n",
      "Iteration 46, loss = 0.23427306\n",
      "Iteration 47, loss = 0.22996386\n",
      "Iteration 48, loss = 0.22814662\n",
      "Iteration 49, loss = 0.22204858\n",
      "Iteration 50, loss = 0.22104654\n",
      "Iteration 51, loss = 0.21794108\n",
      "Iteration 52, loss = 0.21398258\n",
      "Iteration 53, loss = 0.21031970\n",
      "Iteration 54, loss = 0.20660349\n",
      "Iteration 1, loss = 1.22821941\n",
      "Iteration 55, loss = 0.20297344\n",
      "Iteration 2, loss = 1.04867504\n",
      "Iteration 56, loss = 0.19953538\n",
      "Iteration 3, loss = 0.92990650\n",
      "Iteration 57, loss = 0.19697611\n",
      "Iteration 4, loss = 0.82821737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58, loss = 0.19649701\n",
      "Iteration 5, loss = 0.75202167\n",
      "Iteration 59, loss = 0.19255805\n",
      "Iteration 6, loss = 0.68807608\n",
      "Iteration 60, loss = 0.19094162\n",
      "Iteration 7, loss = 0.63554730\n",
      "Iteration 61, loss = 0.18708267\n",
      "Iteration 8, loss = 0.59176862\n",
      "Iteration 62, loss = 0.18351167\n",
      "Iteration 9, loss = 0.55640409\n",
      "Iteration 63, loss = 0.18082771\n",
      "Iteration 10, loss = 0.52543228\n",
      "Iteration 64, loss = 0.17807841\n",
      "Iteration 11, loss = 0.50101491\n",
      "Iteration 65, loss = 0.17585579\n",
      "Iteration 12, loss = 0.47910804\n",
      "Iteration 66, loss = 0.17299662\n",
      "Iteration 13, loss = 0.46002042\n",
      "Iteration 67, loss = 0.17046129\n",
      "Iteration 14, loss = 0.44350027\n",
      "Iteration 68, loss = 0.16806661\n",
      "Iteration 15, loss = 0.42859103\n",
      "Iteration 69, loss = 0.16516258\n",
      "Iteration 16, loss = 0.41580670\n",
      "Iteration 70, loss = 0.16365588\n",
      "Iteration 17, loss = 0.40391391\n",
      "Iteration 71, loss = 0.16160944\n",
      "Iteration 18, loss = 0.39254411\n",
      "Iteration 72, loss = 0.15833157\n",
      "Iteration 19, loss = 0.38259963\n",
      "Iteration 73, loss = 0.15855021\n",
      "Iteration 20, loss = 0.37333600\n",
      "Iteration 74, loss = 0.15504687\n",
      "Iteration 21, loss = 0.36353397\n",
      "Iteration 75, loss = 0.15481364\n",
      "Iteration 22, loss = 0.35529767\n",
      "Iteration 76, loss = 0.15043465\n",
      "Iteration 23, loss = 0.34711049\n",
      "Iteration 77, loss = 0.15010792\n",
      "Iteration 24, loss = 0.33965215\n",
      "Iteration 78, loss = 0.14647719\n",
      "Iteration 25, loss = 0.33305761\n",
      "Iteration 79, loss = 0.14486554\n",
      "Iteration 26, loss = 0.32622216\n",
      "Iteration 80, loss = 0.14469731\n",
      "Iteration 27, loss = 0.31959192\n",
      "Iteration 81, loss = 0.14219794\n",
      "Iteration 28, loss = 0.31422089\n",
      "Iteration 82, loss = 0.13909536\n",
      "Iteration 29, loss = 0.30750402\n",
      "Iteration 83, loss = 0.13852869\n",
      "Iteration 30, loss = 0.30203514\n",
      "Iteration 84, loss = 0.13606313\n",
      "Iteration 31, loss = 0.29583903\n",
      "Iteration 85, loss = 0.13452155\n",
      "Iteration 32, loss = 0.29073324\n",
      "Iteration 86, loss = 0.13403329\n",
      "Iteration 33, loss = 0.28556811\n",
      "Iteration 87, loss = 0.13064802\n",
      "Iteration 34, loss = 0.28142807\n",
      "Iteration 88, loss = 0.13074026\n",
      "Iteration 35, loss = 0.27568573\n",
      "Iteration 89, loss = 0.12924495\n",
      "Iteration 36, loss = 0.27150426\n",
      "Iteration 90, loss = 0.12997401\n",
      "Iteration 37, loss = 0.26782228\n",
      "Iteration 91, loss = 0.12559128\n",
      "Iteration 38, loss = 0.26398454\n",
      "Iteration 92, loss = 0.12367077\n",
      "Iteration 39, loss = 0.25854175\n",
      "Iteration 93, loss = 0.12111639\n",
      "Iteration 40, loss = 0.25569991\n",
      "Iteration 94, loss = 0.12001575\n",
      "Iteration 41, loss = 0.25019005\n",
      "Iteration 95, loss = 0.11825657\n",
      "Iteration 42, loss = 0.24857686\n",
      "Iteration 96, loss = 0.11735710\n",
      "Iteration 43, loss = 0.24427574\n",
      "Iteration 97, loss = 0.11568228\n",
      "Iteration 44, loss = 0.24099971\n",
      "Iteration 98, loss = 0.11420368\n",
      "Iteration 45, loss = 0.23733170\n",
      "Iteration 99, loss = 0.11257448\n",
      "Iteration 46, loss = 0.23387244\n",
      "Iteration 100, loss = 0.11168117\n",
      "Iteration 47, loss = 0.23117038\n",
      "Iteration 48, loss = 0.22765140\n",
      "Iteration 49, loss = 0.22520859\n",
      "Iteration 50, loss = 0.22215966\n",
      "Iteration 51, loss = 0.21995999\n",
      "Iteration 52, loss = 0.21643141\n",
      "Iteration 53, loss = 0.21596757\n",
      "Iteration 54, loss = 0.21199738\n",
      "Iteration 55, loss = 0.20890057\n",
      "Iteration 56, loss = 0.20740199\n",
      "Iteration 57, loss = 0.20477229\n",
      "Iteration 58, loss = 0.20177424\n",
      "Iteration 59, loss = 0.19928958\n",
      "Iteration 60, loss = 0.19735968\n",
      "Iteration 61, loss = 0.19538333\n",
      "Iteration 62, loss = 0.19286336\n",
      "Iteration 63, loss = 0.19054966\n",
      "Iteration 64, loss = 0.18960402\n",
      "Iteration 1, loss = 1.25037354\n",
      "Iteration 65, loss = 0.18670610\n",
      "Iteration 2, loss = 1.02783616\n",
      "Iteration 66, loss = 0.18419128\n",
      "Iteration 3, loss = 0.90907753\n",
      "Iteration 67, loss = 0.18203276\n",
      "Iteration 4, loss = 0.80967188\n",
      "Iteration 68, loss = 0.18276531\n",
      "Iteration 5, loss = 0.73385866\n",
      "Iteration 69, loss = 0.17830983\n",
      "Iteration 6, loss = 0.67570412\n",
      "Iteration 70, loss = 0.17716447\n",
      "Iteration 7, loss = 0.62917019\n",
      "Iteration 71, loss = 0.17638753\n",
      "Iteration 8, loss = 0.59108493\n",
      "Iteration 72, loss = 0.17298692\n",
      "Iteration 9, loss = 0.56074141\n",
      "Iteration 73, loss = 0.17199662\n",
      "Iteration 10, loss = 0.53511182\n",
      "Iteration 74, loss = 0.16902508\n",
      "Iteration 11, loss = 0.51516225\n",
      "Iteration 75, loss = 0.16781681\n",
      "Iteration 12, loss = 0.49436283\n",
      "Iteration 76, loss = 0.16532179\n",
      "Iteration 13, loss = 0.47743387\n",
      "Iteration 77, loss = 0.16383564\n",
      "Iteration 14, loss = 0.46303209\n",
      "Iteration 78, loss = 0.16220863\n",
      "Iteration 15, loss = 0.44991803\n",
      "Iteration 79, loss = 0.16031988\n",
      "Iteration 16, loss = 0.43716731\n",
      "Iteration 80, loss = 0.15984994\n",
      "Iteration 17, loss = 0.42660193\n",
      "Iteration 81, loss = 0.15736953\n",
      "Iteration 18, loss = 0.41613411\n",
      "Iteration 82, loss = 0.15658661\n",
      "Iteration 19, loss = 0.40724300\n",
      "Iteration 83, loss = 0.15453219\n",
      "Iteration 20, loss = 0.39848235\n",
      "Iteration 84, loss = 0.15213956\n",
      "Iteration 21, loss = 0.39100958\n",
      "Iteration 85, loss = 0.15139774\n",
      "Iteration 22, loss = 0.38281553\n",
      "Iteration 86, loss = 0.14973054\n",
      "Iteration 23, loss = 0.37541388\n",
      "Iteration 87, loss = 0.14832916\n",
      "Iteration 24, loss = 0.36887580\n",
      "Iteration 88, loss = 0.14650520\n",
      "Iteration 25, loss = 0.36236799\n",
      "Iteration 89, loss = 0.14501537\n",
      "Iteration 26, loss = 0.35647897\n",
      "Iteration 90, loss = 0.14471615\n",
      "Iteration 27, loss = 0.35031355\n",
      "Iteration 91, loss = 0.14244049\n",
      "Iteration 28, loss = 0.34529590\n",
      "Iteration 92, loss = 0.14147878\n",
      "Iteration 29, loss = 0.33913132\n",
      "Iteration 93, loss = 0.14142746\n",
      "Iteration 30, loss = 0.33464475\n",
      "Iteration 94, loss = 0.13881797\n",
      "Iteration 31, loss = 0.32863759\n",
      "Iteration 95, loss = 0.13760234\n",
      "Iteration 32, loss = 0.32343641\n",
      "Iteration 96, loss = 0.13635825\n",
      "Iteration 33, loss = 0.31835254\n",
      "Iteration 97, loss = 0.13432487\n",
      "Iteration 34, loss = 0.31388059\n",
      "Iteration 35, loss = 0.30974788\n",
      "Iteration 98, loss = 0.13527161\n",
      "Iteration 99, loss = 0.13095407\n",
      "Iteration 36, loss = 0.30431591\n",
      "Iteration 100, loss = 0.13171970\n",
      "Iteration 37, loss = 0.30105296\n",
      "Iteration 38, loss = 0.29622189\n",
      "Iteration 39, loss = 0.29285575\n",
      "Iteration 40, loss = 0.28945236\n",
      "Iteration 41, loss = 0.28418969\n",
      "Iteration 42, loss = 0.28204470\n",
      "Iteration 43, loss = 0.27704506\n",
      "Iteration 44, loss = 0.27604006\n",
      "Iteration 45, loss = 0.27057650\n",
      "Iteration 46, loss = 0.26700780\n",
      "Iteration 47, loss = 0.26329626\n",
      "Iteration 48, loss = 0.25913601\n",
      "Iteration 49, loss = 0.25577078\n",
      "Iteration 50, loss = 0.25242005\n",
      "Iteration 51, loss = 0.24894076\n",
      "Iteration 52, loss = 0.24522804\n",
      "Iteration 53, loss = 0.24181279\n",
      "Iteration 54, loss = 0.23902133\n",
      "Iteration 55, loss = 0.23587252\n",
      "Iteration 1, loss = 1.42063714\n",
      "Iteration 56, loss = 0.23266548\n",
      "Iteration 2, loss = 1.17231250\n",
      "Iteration 57, loss = 0.22992941\n",
      "Iteration 3, loss = 1.00459653\n",
      "Iteration 58, loss = 0.22628101\n",
      "Iteration 4, loss = 0.87123947\n",
      "Iteration 59, loss = 0.22445806\n",
      "Iteration 5, loss = 0.77329193\n",
      "Iteration 60, loss = 0.22173382\n",
      "Iteration 6, loss = 0.69691308\n",
      "Iteration 61, loss = 0.21870685\n",
      "Iteration 7, loss = 0.63681457\n",
      "Iteration 62, loss = 0.21607762\n",
      "Iteration 8, loss = 0.58930213\n",
      "Iteration 63, loss = 0.21277815\n",
      "Iteration 9, loss = 0.55435999\n",
      "Iteration 64, loss = 0.21122782\n",
      "Iteration 10, loss = 0.52485156\n",
      "Iteration 65, loss = 0.20908757\n",
      "Iteration 11, loss = 0.49870364\n",
      "Iteration 66, loss = 0.20553297\n",
      "Iteration 12, loss = 0.47898778\n",
      "Iteration 67, loss = 0.20292653\n",
      "Iteration 13, loss = 0.46075942\n",
      "Iteration 68, loss = 0.20059240\n",
      "Iteration 14, loss = 0.44683165\n",
      "Iteration 69, loss = 0.19782799\n",
      "Iteration 15, loss = 0.43401177\n",
      "Iteration 70, loss = 0.19585293\n",
      "Iteration 16, loss = 0.42085629\n",
      "Iteration 71, loss = 0.19310243\n",
      "Iteration 17, loss = 0.41066924\n",
      "Iteration 72, loss = 0.19247814\n",
      "Iteration 18, loss = 0.40008179\n",
      "Iteration 73, loss = 0.18865130\n",
      "Iteration 19, loss = 0.39135638\n",
      "Iteration 74, loss = 0.18757771\n",
      "Iteration 20, loss = 0.38283278\n",
      "Iteration 75, loss = 0.18468846\n",
      "Iteration 21, loss = 0.37516454\n",
      "Iteration 76, loss = 0.18204455\n",
      "Iteration 22, loss = 0.36804430\n",
      "Iteration 77, loss = 0.18019874\n",
      "Iteration 23, loss = 0.36108746\n",
      "Iteration 78, loss = 0.17787936\n",
      "Iteration 24, loss = 0.35532374\n",
      "Iteration 79, loss = 0.17571585\n",
      "Iteration 25, loss = 0.34936081\n",
      "Iteration 80, loss = 0.17425410\n",
      "Iteration 26, loss = 0.34313985\n",
      "Iteration 81, loss = 0.17274252\n",
      "Iteration 27, loss = 0.33743642\n",
      "Iteration 82, loss = 0.17101051\n",
      "Iteration 28, loss = 0.33184613\n",
      "Iteration 83, loss = 0.17026724\n",
      "Iteration 29, loss = 0.32658468\n",
      "Iteration 84, loss = 0.16739165\n",
      "Iteration 30, loss = 0.32151055\n",
      "Iteration 85, loss = 0.16619175\n",
      "Iteration 31, loss = 0.31611321\n",
      "Iteration 86, loss = 0.16360671\n",
      "Iteration 32, loss = 0.31148610\n",
      "Iteration 87, loss = 0.16130067\n",
      "Iteration 33, loss = 0.30681666\n",
      "Iteration 88, loss = 0.16030585\n",
      "Iteration 34, loss = 0.30206905\n",
      "Iteration 89, loss = 0.15807358\n",
      "Iteration 35, loss = 0.29848982\n",
      "Iteration 90, loss = 0.15807492\n",
      "Iteration 36, loss = 0.29380982\n",
      "Iteration 91, loss = 0.15492980\n",
      "Iteration 37, loss = 0.28951731\n",
      "Iteration 92, loss = 0.15465568\n",
      "Iteration 38, loss = 0.28541580\n",
      "Iteration 93, loss = 0.15329702\n",
      "Iteration 39, loss = 0.28178585\n",
      "Iteration 94, loss = 0.15148591\n",
      "Iteration 40, loss = 0.27769658\n",
      "Iteration 95, loss = 0.14900293\n",
      "Iteration 41, loss = 0.27412864\n",
      "Iteration 96, loss = 0.14743374\n",
      "Iteration 42, loss = 0.27048081\n",
      "Iteration 97, loss = 0.14613454\n",
      "Iteration 43, loss = 0.26709028\n",
      "Iteration 98, loss = 0.14616840\n",
      "Iteration 44, loss = 0.26360973\n",
      "Iteration 99, loss = 0.14271444\n",
      "Iteration 45, loss = 0.25927836\n",
      "Iteration 100, loss = 0.14231081\n",
      "Iteration 46, loss = 0.25546460\n",
      "Iteration 47, loss = 0.25227082\n",
      "Iteration 48, loss = 0.24914023\n",
      "Iteration 49, loss = 0.24600122\n",
      "Iteration 50, loss = 0.24295660\n",
      "Iteration 51, loss = 0.23888617\n",
      "Iteration 52, loss = 0.23553263\n",
      "Iteration 53, loss = 0.23351395\n",
      "Iteration 54, loss = 0.23009534\n",
      "Iteration 55, loss = 0.22684479\n",
      "Iteration 56, loss = 0.22462609\n",
      "Iteration 57, loss = 0.22127620\n",
      "Iteration 58, loss = 0.21876322\n",
      "Iteration 59, loss = 0.21775565\n",
      "Iteration 60, loss = 0.21350921\n",
      "Iteration 61, loss = 0.21213947\n",
      "Iteration 62, loss = 0.20874338\n",
      "Iteration 63, loss = 0.20613950\n",
      "Iteration 1, loss = 1.32265446\n",
      "Iteration 64, loss = 0.20358920\n",
      "Iteration 2, loss = 1.07775686\n",
      "Iteration 65, loss = 0.20194652\n",
      "Iteration 3, loss = 0.91251229\n",
      "Iteration 66, loss = 0.19925434\n",
      "Iteration 4, loss = 0.81236093\n",
      "Iteration 67, loss = 0.19788370\n",
      "Iteration 5, loss = 0.74668691\n",
      "Iteration 68, loss = 0.19424184\n",
      "Iteration 6, loss = 0.69226001\n",
      "Iteration 69, loss = 0.19243152\n",
      "Iteration 7, loss = 0.64256842\n",
      "Iteration 70, loss = 0.18931530\n",
      "Iteration 8, loss = 0.60000149\n",
      "Iteration 71, loss = 0.18807502\n",
      "Iteration 9, loss = 0.56696176\n",
      "Iteration 72, loss = 0.18567187\n",
      "Iteration 10, loss = 0.53842370\n",
      "Iteration 73, loss = 0.18417874\n",
      "Iteration 11, loss = 0.51547051\n",
      "Iteration 74, loss = 0.18150489\n",
      "Iteration 12, loss = 0.49515680\n",
      "Iteration 75, loss = 0.18038707\n",
      "Iteration 13, loss = 0.47783941\n",
      "Iteration 76, loss = 0.17751455\n",
      "Iteration 14, loss = 0.46329348\n",
      "Iteration 77, loss = 0.17628686\n",
      "Iteration 15, loss = 0.44936348\n",
      "Iteration 78, loss = 0.17429139\n",
      "Iteration 16, loss = 0.43743853\n",
      "Iteration 79, loss = 0.17290665\n",
      "Iteration 17, loss = 0.42684214\n",
      "Iteration 80, loss = 0.16988987\n",
      "Iteration 18, loss = 0.41607095\n",
      "Iteration 81, loss = 0.16857595\n",
      "Iteration 19, loss = 0.40659765\n",
      "Iteration 82, loss = 0.16639866\n",
      "Iteration 20, loss = 0.39788818\n",
      "Iteration 83, loss = 0.16728423\n",
      "Iteration 21, loss = 0.39028533\n",
      "Iteration 84, loss = 0.16409369\n",
      "Iteration 22, loss = 0.38239943\n",
      "Iteration 85, loss = 0.16300292\n",
      "Iteration 23, loss = 0.37556003\n",
      "Iteration 86, loss = 0.16140897\n",
      "Iteration 24, loss = 0.36821109\n",
      "Iteration 87, loss = 0.15766135\n",
      "Iteration 25, loss = 0.36190855\n",
      "Iteration 88, loss = 0.15599771\n",
      "Iteration 26, loss = 0.35524007\n",
      "Iteration 27, loss = 0.34926056\n",
      "Iteration 89, loss = 0.15389862\n",
      "Iteration 90, loss = 0.15275081\n",
      "Iteration 28, loss = 0.34314159\n",
      "Iteration 91, loss = 0.15213365\n",
      "Iteration 29, loss = 0.33773795\n",
      "Iteration 92, loss = 0.14982052\n",
      "Iteration 30, loss = 0.33181262\n",
      "Iteration 93, loss = 0.14947913\n",
      "Iteration 31, loss = 0.32679666\n",
      "Iteration 32, loss = 0.32219935\n",
      "Iteration 94, loss = 0.14716214\n",
      "Iteration 33, loss = 0.31673032\n",
      "Iteration 95, loss = 0.14514407\n",
      "Iteration 34, loss = 0.31194353\n",
      "Iteration 96, loss = 0.14363897\n",
      "Iteration 35, loss = 0.30715329\n",
      "Iteration 97, loss = 0.14441666\n",
      "Iteration 36, loss = 0.30246935\n",
      "Iteration 98, loss = 0.14196412\n",
      "Iteration 37, loss = 0.29808190\n",
      "Iteration 99, loss = 0.14146669\n",
      "Iteration 38, loss = 0.29332961\n",
      "Iteration 100, loss = 0.13730490\n",
      "Iteration 39, loss = 0.28835345\n",
      "Iteration 40, loss = 0.28470440\n",
      "Iteration 41, loss = 0.28082266\n",
      "Iteration 42, loss = 0.27605525\n",
      "Iteration 43, loss = 0.27262243\n",
      "Iteration 44, loss = 0.26778233\n",
      "Iteration 45, loss = 0.26442543\n",
      "Iteration 46, loss = 0.26021970\n",
      "Iteration 47, loss = 0.25582490\n",
      "Iteration 48, loss = 0.25381177\n",
      "Iteration 49, loss = 0.24922219\n",
      "Iteration 50, loss = 0.24582019\n",
      "Iteration 51, loss = 0.24225472\n",
      "Iteration 52, loss = 0.23961451\n",
      "Iteration 53, loss = 0.23579170\n",
      "Iteration 54, loss = 0.23208961\n",
      "Iteration 55, loss = 0.23039526\n",
      "Iteration 56, loss = 0.22559097\n",
      "Iteration 1, loss = 1.39220669\n",
      "Iteration 57, loss = 0.22324170\n",
      "Iteration 2, loss = 1.07408467\n",
      "Iteration 58, loss = 0.22002586\n",
      "Iteration 3, loss = 0.94652100\n",
      "Iteration 59, loss = 0.21625280\n",
      "Iteration 4, loss = 0.87887698\n",
      "Iteration 60, loss = 0.21350588\n",
      "Iteration 5, loss = 0.80196040\n",
      "Iteration 61, loss = 0.21093244\n",
      "Iteration 6, loss = 0.73436668\n",
      "Iteration 62, loss = 0.20827296\n",
      "Iteration 7, loss = 0.68051037\n",
      "Iteration 63, loss = 0.20584781\n",
      "Iteration 8, loss = 0.63700180\n",
      "Iteration 64, loss = 0.20372306\n",
      "Iteration 9, loss = 0.59636609\n",
      "Iteration 65, loss = 0.20085211\n",
      "Iteration 10, loss = 0.56303655\n",
      "Iteration 66, loss = 0.19916018\n",
      "Iteration 11, loss = 0.53541460\n",
      "Iteration 67, loss = 0.19625903\n",
      "Iteration 12, loss = 0.51184799\n",
      "Iteration 68, loss = 0.19318550\n",
      "Iteration 13, loss = 0.49071433\n",
      "Iteration 69, loss = 0.19052395\n",
      "Iteration 14, loss = 0.47225580\n",
      "Iteration 70, loss = 0.18872272\n",
      "Iteration 15, loss = 0.45631123\n",
      "Iteration 71, loss = 0.18680774\n",
      "Iteration 16, loss = 0.44213480\n",
      "Iteration 72, loss = 0.18512050\n",
      "Iteration 17, loss = 0.42838861\n",
      "Iteration 73, loss = 0.18209484\n",
      "Iteration 18, loss = 0.41659592\n",
      "Iteration 74, loss = 0.18071551\n",
      "Iteration 19, loss = 0.40510387\n",
      "Iteration 75, loss = 0.18047186\n",
      "Iteration 20, loss = 0.39541892\n",
      "Iteration 76, loss = 0.17749211\n",
      "Iteration 21, loss = 0.38636059\n",
      "Iteration 77, loss = 0.17514475\n",
      "Iteration 22, loss = 0.37737789\n",
      "Iteration 78, loss = 0.17320433\n",
      "Iteration 23, loss = 0.36977733\n",
      "Iteration 79, loss = 0.17045744\n",
      "Iteration 24, loss = 0.36289122\n",
      "Iteration 80, loss = 0.16846049\n",
      "Iteration 25, loss = 0.35468082\n",
      "Iteration 81, loss = 0.16721656\n",
      "Iteration 26, loss = 0.34927598\n",
      "Iteration 82, loss = 0.16480515\n",
      "Iteration 27, loss = 0.34224596\n",
      "Iteration 83, loss = 0.16423382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.33631868\n",
      "Iteration 84, loss = 0.16195393\n",
      "Iteration 29, loss = 0.33001135\n",
      "Iteration 85, loss = 0.16105766\n",
      "Iteration 30, loss = 0.32430842\n",
      "Iteration 86, loss = 0.16054253\n",
      "Iteration 31, loss = 0.31913762\n",
      "Iteration 87, loss = 0.15814046\n",
      "Iteration 32, loss = 0.31421446\n",
      "Iteration 88, loss = 0.15596129\n",
      "Iteration 33, loss = 0.30891219\n",
      "Iteration 89, loss = 0.15351582\n",
      "Iteration 34, loss = 0.30409971\n",
      "Iteration 90, loss = 0.15283376\n",
      "Iteration 35, loss = 0.29941703\n",
      "Iteration 91, loss = 0.15151790\n",
      "Iteration 36, loss = 0.29504169\n",
      "Iteration 92, loss = 0.15088386\n",
      "Iteration 37, loss = 0.28986438\n",
      "Iteration 93, loss = 0.14820094\n",
      "Iteration 38, loss = 0.28621739\n",
      "Iteration 94, loss = 0.14837542\n",
      "Iteration 39, loss = 0.28178542\n",
      "Iteration 95, loss = 0.14592561\n",
      "Iteration 40, loss = 0.27673246\n",
      "Iteration 96, loss = 0.14447627\n",
      "Iteration 41, loss = 0.27377093\n",
      "Iteration 97, loss = 0.14310346\n",
      "Iteration 42, loss = 0.26854149\n",
      "Iteration 98, loss = 0.14250761\n",
      "Iteration 43, loss = 0.26619894\n",
      "Iteration 99, loss = 0.14009378\n",
      "Iteration 44, loss = 0.26204656\n",
      "Iteration 100, loss = 0.13979531\n",
      "Iteration 45, loss = 0.25815361\n",
      "Iteration 46, loss = 0.25343699\n",
      "Iteration 47, loss = 0.24968172\n",
      "Iteration 48, loss = 0.24641709\n",
      "Iteration 49, loss = 0.24274528\n",
      "Iteration 50, loss = 0.23984403\n",
      "Iteration 51, loss = 0.23644008\n",
      "Iteration 52, loss = 0.23268047\n",
      "Iteration 53, loss = 0.23075280\n",
      "Iteration 54, loss = 0.22797120\n",
      "Iteration 55, loss = 0.22368642\n",
      "Iteration 56, loss = 0.22036505\n",
      "Iteration 57, loss = 0.21746387\n",
      "Iteration 58, loss = 0.21487538\n",
      "Iteration 59, loss = 0.21151908\n",
      "Iteration 60, loss = 0.20881554\n",
      "Iteration 61, loss = 0.20600620\n",
      "Iteration 62, loss = 0.20352978\n",
      "Iteration 1, loss = 1.13645272\n",
      "Iteration 63, loss = 0.20138277\n",
      "Iteration 2, loss = 0.93878796\n",
      "Iteration 64, loss = 0.19962461\n",
      "Iteration 3, loss = 0.80483261\n",
      "Iteration 65, loss = 0.19621727\n",
      "Iteration 4, loss = 0.71093309\n",
      "Iteration 66, loss = 0.19356276\n",
      "Iteration 5, loss = 0.64692805\n",
      "Iteration 67, loss = 0.19102363\n",
      "Iteration 6, loss = 0.60235001\n",
      "Iteration 68, loss = 0.18900932\n",
      "Iteration 7, loss = 0.56340138\n",
      "Iteration 69, loss = 0.18628860\n",
      "Iteration 8, loss = 0.52884273\n",
      "Iteration 70, loss = 0.18410095\n",
      "Iteration 9, loss = 0.50416119\n",
      "Iteration 71, loss = 0.18177105\n",
      "Iteration 10, loss = 0.48238805\n",
      "Iteration 72, loss = 0.17991406\n",
      "Iteration 11, loss = 0.46232710\n",
      "Iteration 73, loss = 0.17779712\n",
      "Iteration 12, loss = 0.44525998\n",
      "Iteration 74, loss = 0.17573764\n",
      "Iteration 13, loss = 0.43137054\n",
      "Iteration 75, loss = 0.17326048\n",
      "Iteration 14, loss = 0.41792986\n",
      "Iteration 76, loss = 0.17199920\n",
      "Iteration 15, loss = 0.40680080\n",
      "Iteration 77, loss = 0.16921763\n",
      "Iteration 16, loss = 0.39594334\n",
      "Iteration 78, loss = 0.16763016\n",
      "Iteration 17, loss = 0.38656124\n",
      "Iteration 79, loss = 0.16573999\n",
      "Iteration 18, loss = 0.37848128\n",
      "Iteration 80, loss = 0.16275117\n",
      "Iteration 19, loss = 0.37158329\n",
      "Iteration 81, loss = 0.16310583\n",
      "Iteration 20, loss = 0.36250799\n",
      "Iteration 82, loss = 0.16046386\n",
      "Iteration 21, loss = 0.35585514\n",
      "Iteration 83, loss = 0.15924566\n",
      "Iteration 22, loss = 0.34842711\n",
      "Iteration 84, loss = 0.15714635\n",
      "Iteration 23, loss = 0.34232965\n",
      "Iteration 85, loss = 0.15427510\n",
      "Iteration 24, loss = 0.33604712\n",
      "Iteration 86, loss = 0.15494843\n",
      "Iteration 25, loss = 0.33090929\n",
      "Iteration 87, loss = 0.15240021\n",
      "Iteration 26, loss = 0.32575877\n",
      "Iteration 88, loss = 0.14990581\n",
      "Iteration 27, loss = 0.32015614\n",
      "Iteration 89, loss = 0.14811742\n",
      "Iteration 28, loss = 0.31528602\n",
      "Iteration 90, loss = 0.14706981\n",
      "Iteration 29, loss = 0.31050464\n",
      "Iteration 91, loss = 0.14463736\n",
      "Iteration 30, loss = 0.30583886\n",
      "Iteration 92, loss = 0.14403734\n",
      "Iteration 31, loss = 0.30040562\n",
      "Iteration 93, loss = 0.14238599\n",
      "Iteration 32, loss = 0.29683581\n",
      "Iteration 94, loss = 0.14075135\n",
      "Iteration 33, loss = 0.29191896\n",
      "Iteration 95, loss = 0.13911251\n",
      "Iteration 34, loss = 0.28859378\n",
      "Iteration 96, loss = 0.13982997\n",
      "Iteration 35, loss = 0.28356035\n",
      "Iteration 97, loss = 0.13599132\n",
      "Iteration 36, loss = 0.28003697\n",
      "Iteration 98, loss = 0.13638195\n",
      "Iteration 37, loss = 0.27625684\n",
      "Iteration 99, loss = 0.13531238\n",
      "Iteration 38, loss = 0.27156985\n",
      "Iteration 100, loss = 0.13248514\n",
      "Iteration 39, loss = 0.26865295\n",
      "Iteration 40, loss = 0.26435858\n",
      "Iteration 41, loss = 0.26117869\n",
      "Iteration 42, loss = 0.25728936\n",
      "Iteration 43, loss = 0.25476778\n",
      "Iteration 44, loss = 0.25181968\n",
      "Iteration 45, loss = 0.24778278\n",
      "Iteration 46, loss = 0.24540388\n",
      "Iteration 47, loss = 0.24160517\n",
      "Iteration 48, loss = 0.23906199\n",
      "Iteration 49, loss = 0.23553300\n",
      "Iteration 50, loss = 0.23182485\n",
      "Iteration 51, loss = 0.22918306\n",
      "Iteration 52, loss = 0.22680587\n",
      "Iteration 53, loss = 0.22355157\n",
      "Iteration 54, loss = 0.22142682\n",
      "Iteration 55, loss = 0.21953479\n",
      "Iteration 56, loss = 0.21660229\n",
      "Iteration 57, loss = 0.21510299\n",
      "Iteration 1, loss = 1.46939391\n",
      "Iteration 58, loss = 0.21144699\n",
      "Iteration 2, loss = 1.22400157\n",
      "Iteration 59, loss = 0.20899223\n",
      "Iteration 3, loss = 1.04155653\n",
      "Iteration 60, loss = 0.20609580\n",
      "Iteration 4, loss = 0.90925432\n",
      "Iteration 61, loss = 0.20368542\n",
      "Iteration 5, loss = 0.81090090\n",
      "Iteration 62, loss = 0.20194495\n",
      "Iteration 6, loss = 0.73834593\n",
      "Iteration 63, loss = 0.20001387\n",
      "Iteration 7, loss = 0.68280843\n",
      "Iteration 64, loss = 0.19793016\n",
      "Iteration 8, loss = 0.63763716\n",
      "Iteration 65, loss = 0.19604315\n",
      "Iteration 9, loss = 0.59975179\n",
      "Iteration 66, loss = 0.19284295\n",
      "Iteration 10, loss = 0.56877412\n",
      "Iteration 67, loss = 0.19109876\n",
      "Iteration 11, loss = 0.54195051\n",
      "Iteration 68, loss = 0.18801369\n",
      "Iteration 12, loss = 0.51773036\n",
      "Iteration 69, loss = 0.18697201\n",
      "Iteration 13, loss = 0.49904008\n",
      "Iteration 70, loss = 0.18408892\n",
      "Iteration 14, loss = 0.47881248\n",
      "Iteration 71, loss = 0.18252250\n",
      "Iteration 15, loss = 0.46314623\n",
      "Iteration 72, loss = 0.18057185\n",
      "Iteration 16, loss = 0.44852288\n",
      "Iteration 73, loss = 0.17854777\n",
      "Iteration 17, loss = 0.43471142\n",
      "Iteration 74, loss = 0.17671660\n",
      "Iteration 18, loss = 0.42383744\n",
      "Iteration 75, loss = 0.17443560\n",
      "Iteration 19, loss = 0.41254807\n",
      "Iteration 76, loss = 0.17219307\n",
      "Iteration 20, loss = 0.40219368\n",
      "Iteration 77, loss = 0.17013628\n",
      "Iteration 21, loss = 0.39368655\n",
      "Iteration 78, loss = 0.16864572\n",
      "Iteration 22, loss = 0.38500137\n",
      "Iteration 79, loss = 0.16701695\n",
      "Iteration 23, loss = 0.37758833\n",
      "Iteration 80, loss = 0.16469700\n",
      "Iteration 24, loss = 0.36990672\n",
      "Iteration 81, loss = 0.16323006\n",
      "Iteration 25, loss = 0.36288039\n",
      "Iteration 82, loss = 0.16200726\n",
      "Iteration 26, loss = 0.35688839\n",
      "Iteration 83, loss = 0.16064431\n",
      "Iteration 27, loss = 0.35075992\n",
      "Iteration 84, loss = 0.15725860\n",
      "Iteration 28, loss = 0.34475289\n",
      "Iteration 85, loss = 0.15802941\n",
      "Iteration 29, loss = 0.34003983\n",
      "Iteration 86, loss = 0.15432235\n",
      "Iteration 30, loss = 0.33407445\n",
      "Iteration 87, loss = 0.15482960\n",
      "Iteration 31, loss = 0.32919796\n",
      "Iteration 88, loss = 0.15137325\n",
      "Iteration 32, loss = 0.32346567\n",
      "Iteration 89, loss = 0.14964842\n",
      "Iteration 33, loss = 0.31885627\n",
      "Iteration 90, loss = 0.14863963\n",
      "Iteration 34, loss = 0.31468679\n",
      "Iteration 91, loss = 0.14711886\n",
      "Iteration 35, loss = 0.31010505\n",
      "Iteration 36, loss = 0.30507559\n",
      "Iteration 92, loss = 0.14604475\n",
      "Iteration 37, loss = 0.30034194\n",
      "Iteration 93, loss = 0.14429223\n",
      "Iteration 38, loss = 0.29628958\n",
      "Iteration 94, loss = 0.14263829\n",
      "Iteration 39, loss = 0.29210303\n",
      "Iteration 95, loss = 0.14194678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.28849646\n",
      "Iteration 96, loss = 0.14010742\n",
      "Iteration 41, loss = 0.28341527\n",
      "Iteration 97, loss = 0.14030649\n",
      "Iteration 42, loss = 0.28078357\n",
      "Iteration 98, loss = 0.13752118\n",
      "Iteration 99, loss = 0.13606654\n",
      "Iteration 43, loss = 0.27638564\n",
      "Iteration 100, loss = 0.13528740\n",
      "Iteration 44, loss = 0.27235972\n",
      "Iteration 101, loss = 0.13389366\n",
      "Iteration 45, loss = 0.26843792\n",
      "Iteration 46, loss = 0.26465675\n",
      "Iteration 102, loss = 0.13379951\n",
      "Iteration 47, loss = 0.26043647\n",
      "Iteration 103, loss = 0.13264680\n",
      "Iteration 48, loss = 0.25649686\n",
      "Iteration 104, loss = 0.13040212\n",
      "Iteration 49, loss = 0.25322144\n",
      "Iteration 105, loss = 0.13101854\n",
      "Iteration 106, loss = 0.13071022\n",
      "Iteration 50, loss = 0.24955459\n",
      "Iteration 51, loss = 0.24648056\n",
      "Iteration 107, loss = 0.12586179\n",
      "Iteration 52, loss = 0.24423737\n",
      "Iteration 108, loss = 0.12805188\n",
      "Iteration 109, loss = 0.12677173\n",
      "Iteration 53, loss = 0.23955905\n",
      "Iteration 110, loss = 0.12368841\n",
      "Iteration 54, loss = 0.23756734\n",
      "Iteration 55, loss = 0.23444656\n",
      "Iteration 111, loss = 0.12432370\n",
      "Iteration 56, loss = 0.22997979\n",
      "Iteration 112, loss = 0.12107580\n",
      "Iteration 57, loss = 0.22763408\n",
      "Iteration 113, loss = 0.12103326\n",
      "Iteration 58, loss = 0.22389119\n",
      "Iteration 114, loss = 0.12076759\n",
      "Iteration 59, loss = 0.22168492\n",
      "Iteration 115, loss = 0.11869401\n",
      "Iteration 60, loss = 0.21776018\n",
      "Iteration 116, loss = 0.11775772\n",
      "Iteration 61, loss = 0.21767619\n",
      "Iteration 117, loss = 0.11615397\n",
      "Iteration 62, loss = 0.21398911\n",
      "Iteration 118, loss = 0.11623152\n",
      "Iteration 63, loss = 0.21041302\n",
      "Iteration 119, loss = 0.11438568\n",
      "Iteration 64, loss = 0.20858680\n",
      "Iteration 120, loss = 0.11298184\n",
      "Iteration 65, loss = 0.20733815\n",
      "Iteration 121, loss = 0.11232436\n",
      "Iteration 66, loss = 0.20324181\n",
      "Iteration 122, loss = 0.11084594\n",
      "Iteration 67, loss = 0.20136724\n",
      "Iteration 123, loss = 0.11011816\n",
      "Iteration 68, loss = 0.19868443\n",
      "Iteration 124, loss = 0.10954392\n",
      "Iteration 125, loss = 0.11258714\n",
      "Iteration 69, loss = 0.19729229\n",
      "Iteration 126, loss = 0.10909886\n",
      "Iteration 70, loss = 0.19517779\n",
      "Iteration 127, loss = 0.10796582\n",
      "Iteration 71, loss = 0.19189481\n",
      "Iteration 72, loss = 0.19006745\n",
      "Iteration 128, loss = 0.10977878\n",
      "Iteration 129, loss = 0.10468279\n",
      "Iteration 73, loss = 0.18775628\n",
      "Iteration 130, loss = 0.10690561\n",
      "Iteration 74, loss = 0.18632267\n",
      "Iteration 131, loss = 0.10385069\n",
      "Iteration 75, loss = 0.18403286\n",
      "Iteration 132, loss = 0.10291771\n",
      "Iteration 76, loss = 0.18256527\n",
      "Iteration 133, loss = 0.10091915\n",
      "Iteration 77, loss = 0.18022534\n",
      "Iteration 134, loss = 0.10293347\n",
      "Iteration 78, loss = 0.17983125\n",
      "Iteration 135, loss = 0.10452218\n",
      "Iteration 79, loss = 0.17652456\n",
      "Iteration 136, loss = 0.09861601\n",
      "Iteration 80, loss = 0.17497711\n",
      "Iteration 137, loss = 0.10389965\n",
      "Iteration 81, loss = 0.17336661\n",
      "Iteration 138, loss = 0.10036110\n",
      "Iteration 82, loss = 0.17241342\n",
      "Iteration 139, loss = 0.09907073\n",
      "Iteration 83, loss = 0.16997169\n",
      "Iteration 140, loss = 0.09672721\n",
      "Iteration 84, loss = 0.17083273\n",
      "Iteration 141, loss = 0.09579622\n",
      "Iteration 85, loss = 0.16979464\n",
      "Iteration 86, loss = 0.16481537\n",
      "Iteration 142, loss = 0.09379364\n",
      "Iteration 87, loss = 0.16496311\n",
      "Iteration 143, loss = 0.09382588\n",
      "Iteration 88, loss = 0.16157518Iteration 144, loss = 0.09329755\n",
      "\n",
      "Iteration 145, loss = 0.09301430\n",
      "Iteration 89, loss = 0.16027500\n",
      "Iteration 146, loss = 0.09076347\n",
      "Iteration 90, loss = 0.15935961\n",
      "Iteration 147, loss = 0.09110400\n",
      "Iteration 91, loss = 0.15714795\n",
      "Iteration 148, loss = 0.08974103\n",
      "Iteration 92, loss = 0.15536741\n",
      "Iteration 149, loss = 0.09008630\n",
      "Iteration 93, loss = 0.15565416\n",
      "Iteration 150, loss = 0.08872400\n",
      "Iteration 94, loss = 0.15495802\n",
      "Iteration 151, loss = 0.08939184\n",
      "Iteration 95, loss = 0.15323476\n",
      "Iteration 152, loss = 0.08900756\n",
      "Iteration 96, loss = 0.15012074\n",
      "Iteration 153, loss = 0.08557158\n",
      "Iteration 97, loss = 0.14887152\n",
      "Iteration 154, loss = 0.08618344\n",
      "Iteration 98, loss = 0.14811310\n",
      "Iteration 155, loss = 0.08480473\n",
      "Iteration 99, loss = 0.14630348\n",
      "Iteration 156, loss = 0.08417813\n",
      "Iteration 100, loss = 0.14516373\n",
      "Iteration 157, loss = 0.08334133\n",
      "Iteration 101, loss = 0.14379315\n",
      "Iteration 158, loss = 0.08208161\n",
      "Iteration 102, loss = 0.14253897\n",
      "Iteration 159, loss = 0.08128827\n",
      "Iteration 103, loss = 0.14135115\n",
      "Iteration 160, loss = 0.08183113\n",
      "Iteration 104, loss = 0.14018441\n",
      "Iteration 161, loss = 0.08099989\n",
      "Iteration 105, loss = 0.13868448\n",
      "Iteration 162, loss = 0.08022455\n",
      "Iteration 106, loss = 0.13836107\n",
      "Iteration 163, loss = 0.07952909\n",
      "Iteration 107, loss = 0.13647145\n",
      "Iteration 164, loss = 0.07907981\n",
      "Iteration 108, loss = 0.13547715\n",
      "Iteration 165, loss = 0.07775553\n",
      "Iteration 109, loss = 0.13509565\n",
      "Iteration 166, loss = 0.07909494\n",
      "Iteration 110, loss = 0.13441686\n",
      "Iteration 167, loss = 0.07644033\n",
      "Iteration 111, loss = 0.13258937\n",
      "Iteration 168, loss = 0.08011402\n",
      "Iteration 112, loss = 0.13141852\n",
      "Iteration 169, loss = 0.07620512\n",
      "Iteration 113, loss = 0.13014944\n",
      "Iteration 170, loss = 0.07594450\n",
      "Iteration 114, loss = 0.12961070\n",
      "Iteration 171, loss = 0.07560914\n",
      "Iteration 115, loss = 0.12905730\n",
      "Iteration 172, loss = 0.07389935\n",
      "Iteration 116, loss = 0.12716831\n",
      "Iteration 173, loss = 0.07357068\n",
      "Iteration 117, loss = 0.12674788\n",
      "Iteration 174, loss = 0.07379775\n",
      "Iteration 118, loss = 0.12492758\n",
      "Iteration 175, loss = 0.07209116\n",
      "Iteration 119, loss = 0.12411659\n",
      "Iteration 176, loss = 0.07181206\n",
      "Iteration 120, loss = 0.12563314\n",
      "Iteration 177, loss = 0.07019699\n",
      "Iteration 121, loss = 0.12264366\n",
      "Iteration 178, loss = 0.06978500\n",
      "Iteration 122, loss = 0.12242511\n",
      "Iteration 179, loss = 0.06984745\n",
      "Iteration 123, loss = 0.12300154\n",
      "Iteration 180, loss = 0.06949825\n",
      "Iteration 124, loss = 0.11974029\n",
      "Iteration 181, loss = 0.06859708\n",
      "Iteration 125, loss = 0.12215734\n",
      "Iteration 182, loss = 0.06766396\n",
      "Iteration 126, loss = 0.12105331\n",
      "Iteration 183, loss = 0.06764707\n",
      "Iteration 127, loss = 0.12036244\n",
      "Iteration 184, loss = 0.06756909\n",
      "Iteration 128, loss = 0.11863957\n",
      "Iteration 185, loss = 0.06662293\n",
      "Iteration 129, loss = 0.11666744\n",
      "Iteration 186, loss = 0.06588155\n",
      "Iteration 130, loss = 0.11475014\n",
      "Iteration 187, loss = 0.06560517\n",
      "Iteration 131, loss = 0.11504336\n",
      "Iteration 188, loss = 0.06529484\n",
      "Iteration 132, loss = 0.11199617\n",
      "Iteration 189, loss = 0.06481690\n",
      "Iteration 133, loss = 0.11192669\n",
      "Iteration 190, loss = 0.06642508\n",
      "Iteration 134, loss = 0.11101102\n",
      "Iteration 191, loss = 0.06323465\n",
      "Iteration 135, loss = 0.11099337\n",
      "Iteration 192, loss = 0.06499580\n",
      "Iteration 136, loss = 0.10946449\n",
      "Iteration 193, loss = 0.06440246\n",
      "Iteration 137, loss = 0.10906262\n",
      "Iteration 194, loss = 0.06171168\n",
      "Iteration 138, loss = 0.10802409\n",
      "Iteration 195, loss = 0.06389817\n",
      "Iteration 139, loss = 0.10785783\n",
      "Iteration 196, loss = 0.06183943\n",
      "Iteration 140, loss = 0.10770296\n",
      "Iteration 197, loss = 0.06279684\n",
      "Iteration 141, loss = 0.10476956\n",
      "Iteration 198, loss = 0.06037785\n",
      "Iteration 142, loss = 0.10633741\n",
      "Iteration 199, loss = 0.06151204\n",
      "Iteration 143, loss = 0.10419723\n",
      "Iteration 200, loss = 0.05904263\n",
      "Iteration 144, loss = 0.10418336\n",
      "Iteration 145, loss = 0.10550869\n",
      "Iteration 146, loss = 0.10356300\n",
      "Iteration 147, loss = 0.10042086\n",
      "Iteration 148, loss = 0.10098183\n",
      "Iteration 149, loss = 0.10277170\n",
      "Iteration 150, loss = 0.09981098\n",
      "Iteration 151, loss = 0.09894272\n",
      "Iteration 152, loss = 0.09811267\n",
      "Iteration 153, loss = 0.09610283\n",
      "Iteration 154, loss = 0.09568521\n",
      "Iteration 155, loss = 0.09495899\n",
      "Iteration 156, loss = 0.09364927\n",
      "Iteration 157, loss = 0.09376854\n",
      "Iteration 158, loss = 0.09392020\n",
      "Iteration 159, loss = 0.09204332\n",
      "Iteration 160, loss = 0.09106657\n",
      "Iteration 161, loss = 0.09089272\n",
      "Iteration 1, loss = 1.40875269\n",
      "Iteration 162, loss = 0.08940473\n",
      "Iteration 2, loss = 1.17040854\n",
      "Iteration 163, loss = 0.08938633\n",
      "Iteration 3, loss = 1.02943751\n",
      "Iteration 164, loss = 0.08797358\n",
      "Iteration 4, loss = 0.93030446\n",
      "Iteration 165, loss = 0.08723175\n",
      "Iteration 5, loss = 0.84621520\n",
      "Iteration 166, loss = 0.08691082\n",
      "Iteration 6, loss = 0.77550838\n",
      "Iteration 167, loss = 0.08752396\n",
      "Iteration 7, loss = 0.71758312\n",
      "Iteration 168, loss = 0.08566317\n",
      "Iteration 8, loss = 0.66880279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 169, loss = 0.08495958\n",
      "Iteration 9, loss = 0.62315802\n",
      "Iteration 170, loss = 0.08588776\n",
      "Iteration 10, loss = 0.58257621\n",
      "Iteration 171, loss = 0.08398641\n",
      "Iteration 11, loss = 0.54684714\n",
      "Iteration 172, loss = 0.08387283\n",
      "Iteration 12, loss = 0.51608540\n",
      "Iteration 173, loss = 0.08296106\n",
      "Iteration 13, loss = 0.48957628\n",
      "Iteration 174, loss = 0.08401543\n",
      "Iteration 14, loss = 0.46658254\n",
      "Iteration 175, loss = 0.08203569\n",
      "Iteration 15, loss = 0.44750218\n",
      "Iteration 176, loss = 0.08123963\n",
      "Iteration 16, loss = 0.43032610\n",
      "Iteration 177, loss = 0.08011533\n",
      "Iteration 17, loss = 0.41575236\n",
      "Iteration 178, loss = 0.08031417\n",
      "Iteration 18, loss = 0.40266963\n",
      "Iteration 179, loss = 0.07888808\n",
      "Iteration 19, loss = 0.39059895\n",
      "Iteration 180, loss = 0.07971319\n",
      "Iteration 20, loss = 0.38046925\n",
      "Iteration 181, loss = 0.07807589\n",
      "Iteration 21, loss = 0.37076357\n",
      "Iteration 182, loss = 0.07728318\n",
      "Iteration 22, loss = 0.36187204\n",
      "Iteration 183, loss = 0.07854201\n",
      "Iteration 23, loss = 0.35432230\n",
      "Iteration 184, loss = 0.07701689\n",
      "Iteration 24, loss = 0.34738406\n",
      "Iteration 185, loss = 0.07786594\n",
      "Iteration 25, loss = 0.34039805\n",
      "Iteration 186, loss = 0.07526108\n",
      "Iteration 26, loss = 0.33449826\n",
      "Iteration 187, loss = 0.07631261\n",
      "Iteration 27, loss = 0.32796543\n",
      "Iteration 188, loss = 0.07471131\n",
      "Iteration 28, loss = 0.32229693\n",
      "Iteration 189, loss = 0.07454080\n",
      "Iteration 29, loss = 0.31729693\n",
      "Iteration 190, loss = 0.07343345\n",
      "Iteration 30, loss = 0.31251654\n",
      "Iteration 191, loss = 0.07311043\n",
      "Iteration 31, loss = 0.30716473\n",
      "Iteration 192, loss = 0.07221124\n",
      "Iteration 32, loss = 0.30272040\n",
      "Iteration 193, loss = 0.07166708\n",
      "Iteration 33, loss = 0.29851574\n",
      "Iteration 194, loss = 0.07231962\n",
      "Iteration 34, loss = 0.29396124\n",
      "Iteration 195, loss = 0.06993312\n",
      "Iteration 35, loss = 0.29030727\n",
      "Iteration 196, loss = 0.07004671\n",
      "Iteration 36, loss = 0.28691366\n",
      "Iteration 197, loss = 0.07022342\n",
      "Iteration 37, loss = 0.28242647\n",
      "Iteration 198, loss = 0.07057105\n",
      "Iteration 38, loss = 0.27952326\n",
      "Iteration 199, loss = 0.06884854\n",
      "Iteration 39, loss = 0.27543372\n",
      "Iteration 200, loss = 0.06790668\n",
      "Iteration 40, loss = 0.27199254\n",
      "Iteration 41, loss = 0.26837638\n",
      "Iteration 42, loss = 0.26601789\n",
      "Iteration 43, loss = 0.26200755\n",
      "Iteration 44, loss = 0.25891191\n",
      "Iteration 45, loss = 0.25650214\n",
      "Iteration 46, loss = 0.25285750\n",
      "Iteration 47, loss = 0.25068159\n",
      "Iteration 48, loss = 0.24766575\n",
      "Iteration 49, loss = 0.24405048\n",
      "Iteration 50, loss = 0.24096584\n",
      "Iteration 51, loss = 0.23852648\n",
      "Iteration 52, loss = 0.23653765\n",
      "Iteration 53, loss = 0.23372881\n",
      "Iteration 54, loss = 0.23085918\n",
      "Iteration 55, loss = 0.22837151\n",
      "Iteration 56, loss = 0.22573519\n",
      "Iteration 57, loss = 0.22384072\n",
      "Iteration 1, loss = 1.09488863\n",
      "Iteration 58, loss = 0.22107374\n",
      "Iteration 2, loss = 0.98409676\n",
      "Iteration 59, loss = 0.21896210\n",
      "Iteration 3, loss = 0.89590088\n",
      "Iteration 60, loss = 0.21642080\n",
      "Iteration 61, loss = 0.21433749\n",
      "Iteration 4, loss = 0.81830509\n",
      "Iteration 62, loss = 0.21247774\n",
      "Iteration 5, loss = 0.74746963\n",
      "Iteration 63, loss = 0.21076512\n",
      "Iteration 6, loss = 0.68625982\n",
      "Iteration 64, loss = 0.20941283\n",
      "Iteration 7, loss = 0.63509488\n",
      "Iteration 65, loss = 0.20578303\n",
      "Iteration 8, loss = 0.59210427\n",
      "Iteration 66, loss = 0.20310877\n",
      "Iteration 9, loss = 0.55577567\n",
      "Iteration 10, loss = 0.52621666\n",
      "Iteration 67, loss = 0.20248134\n",
      "Iteration 11, loss = 0.50097543\n",
      "Iteration 68, loss = 0.19982292\n",
      "Iteration 12, loss = 0.47985839\n",
      "Iteration 69, loss = 0.19706299\n",
      "Iteration 13, loss = 0.46190993\n",
      "Iteration 70, loss = 0.19549715\n",
      "Iteration 14, loss = 0.44647841\n",
      "Iteration 71, loss = 0.19399067\n",
      "Iteration 15, loss = 0.43247183\n",
      "Iteration 72, loss = 0.19275270\n",
      "Iteration 16, loss = 0.42096835\n",
      "Iteration 73, loss = 0.18965476\n",
      "Iteration 17, loss = 0.40992527\n",
      "Iteration 74, loss = 0.18937502\n",
      "Iteration 18, loss = 0.40030098\n",
      "Iteration 75, loss = 0.18569095\n",
      "Iteration 19, loss = 0.39172089\n",
      "Iteration 76, loss = 0.18502808\n",
      "Iteration 20, loss = 0.38490542\n",
      "Iteration 77, loss = 0.18287070\n",
      "Iteration 21, loss = 0.37642343\n",
      "Iteration 78, loss = 0.18167013\n",
      "Iteration 22, loss = 0.36980733\n",
      "Iteration 79, loss = 0.17890777\n",
      "Iteration 23, loss = 0.36270293\n",
      "Iteration 80, loss = 0.17735421\n",
      "Iteration 24, loss = 0.35761335\n",
      "Iteration 81, loss = 0.17575399\n",
      "Iteration 25, loss = 0.35157740\n",
      "Iteration 82, loss = 0.17489880\n",
      "Iteration 26, loss = 0.34533424\n",
      "Iteration 83, loss = 0.17271835\n",
      "Iteration 27, loss = 0.34103668\n",
      "Iteration 84, loss = 0.17106669\n",
      "Iteration 28, loss = 0.33503054\n",
      "Iteration 85, loss = 0.16946778\n",
      "Iteration 29, loss = 0.33033717\n",
      "Iteration 86, loss = 0.16805834\n",
      "Iteration 30, loss = 0.32442423\n",
      "Iteration 87, loss = 0.16825346\n",
      "Iteration 31, loss = 0.32023382\n",
      "Iteration 88, loss = 0.16516367\n",
      "Iteration 32, loss = 0.31483867\n",
      "Iteration 89, loss = 0.16425369\n",
      "Iteration 33, loss = 0.31050199\n",
      "Iteration 90, loss = 0.16160539\n",
      "Iteration 34, loss = 0.30657763\n",
      "Iteration 91, loss = 0.16058731\n",
      "Iteration 35, loss = 0.30161509\n",
      "Iteration 92, loss = 0.15889912\n",
      "Iteration 36, loss = 0.29911416\n",
      "Iteration 93, loss = 0.15870100\n",
      "Iteration 37, loss = 0.29322487\n",
      "Iteration 94, loss = 0.15569036\n",
      "Iteration 38, loss = 0.28905239\n",
      "Iteration 95, loss = 0.15468653\n",
      "Iteration 39, loss = 0.28527788\n",
      "Iteration 96, loss = 0.15291267\n",
      "Iteration 40, loss = 0.28058764\n",
      "Iteration 97, loss = 0.15148796\n",
      "Iteration 41, loss = 0.27672850\n",
      "Iteration 98, loss = 0.15101992\n",
      "Iteration 42, loss = 0.27325220\n",
      "Iteration 99, loss = 0.14908570\n",
      "Iteration 43, loss = 0.26919438\n",
      "Iteration 100, loss = 0.14880832\n",
      "Iteration 44, loss = 0.26610797\n",
      "Iteration 101, loss = 0.14628670\n",
      "Iteration 45, loss = 0.26210271\n",
      "Iteration 102, loss = 0.14510874\n",
      "Iteration 46, loss = 0.25813706\n",
      "Iteration 103, loss = 0.14368006\n",
      "Iteration 47, loss = 0.25505518\n",
      "Iteration 104, loss = 0.14272419\n",
      "Iteration 48, loss = 0.25269863\n",
      "Iteration 105, loss = 0.14174825\n",
      "Iteration 49, loss = 0.24849826\n",
      "Iteration 106, loss = 0.14060960\n",
      "Iteration 50, loss = 0.24477756\n",
      "Iteration 107, loss = 0.13901269\n",
      "Iteration 51, loss = 0.24119316\n",
      "Iteration 108, loss = 0.13904814\n",
      "Iteration 52, loss = 0.23824019\n",
      "Iteration 109, loss = 0.13757096\n",
      "Iteration 53, loss = 0.23456082\n",
      "Iteration 110, loss = 0.13598878\n",
      "Iteration 54, loss = 0.23147009\n",
      "Iteration 111, loss = 0.13502914\n",
      "Iteration 55, loss = 0.22819704\n",
      "Iteration 112, loss = 0.13351116\n",
      "Iteration 56, loss = 0.22605577\n",
      "Iteration 113, loss = 0.13317188\n",
      "Iteration 57, loss = 0.22214926\n",
      "Iteration 114, loss = 0.13096797\n",
      "Iteration 58, loss = 0.22048708\n",
      "Iteration 115, loss = 0.13183889\n",
      "Iteration 59, loss = 0.21668538\n",
      "Iteration 116, loss = 0.12956275\n",
      "Iteration 60, loss = 0.21357053\n",
      "Iteration 117, loss = 0.12927723\n",
      "Iteration 61, loss = 0.21112406\n",
      "Iteration 118, loss = 0.12826836\n",
      "Iteration 62, loss = 0.20812535\n",
      "Iteration 119, loss = 0.12626043\n",
      "Iteration 63, loss = 0.20611909\n",
      "Iteration 120, loss = 0.12500905\n",
      "Iteration 64, loss = 0.20437655\n",
      "Iteration 121, loss = 0.12387137\n",
      "Iteration 65, loss = 0.20077665\n",
      "Iteration 122, loss = 0.12248018\n",
      "Iteration 66, loss = 0.19886704\n",
      "Iteration 123, loss = 0.12256492\n",
      "Iteration 67, loss = 0.19762869\n",
      "Iteration 124, loss = 0.12075287\n",
      "Iteration 68, loss = 0.19376293\n",
      "Iteration 125, loss = 0.11999070\n",
      "Iteration 69, loss = 0.19155671\n",
      "Iteration 126, loss = 0.11912573\n",
      "Iteration 70, loss = 0.18892057\n",
      "Iteration 127, loss = 0.11803163\n",
      "Iteration 71, loss = 0.18704057\n",
      "Iteration 128, loss = 0.11703166\n",
      "Iteration 72, loss = 0.18487354\n",
      "Iteration 129, loss = 0.11605396\n",
      "Iteration 73, loss = 0.18330221\n",
      "Iteration 130, loss = 0.11495885\n",
      "Iteration 74, loss = 0.18094359\n",
      "Iteration 131, loss = 0.11447746\n",
      "Iteration 75, loss = 0.17955901\n",
      "Iteration 132, loss = 0.11343155\n",
      "Iteration 76, loss = 0.17778094\n",
      "Iteration 133, loss = 0.11228932\n",
      "Iteration 77, loss = 0.17581432\n",
      "Iteration 134, loss = 0.11133611\n",
      "Iteration 78, loss = 0.17424338\n",
      "Iteration 135, loss = 0.11058299\n",
      "Iteration 79, loss = 0.17199306\n",
      "Iteration 136, loss = 0.11039899\n",
      "Iteration 80, loss = 0.17076097\n",
      "Iteration 137, loss = 0.10999748\n",
      "Iteration 81, loss = 0.16708214\n",
      "Iteration 138, loss = 0.10935595\n",
      "Iteration 82, loss = 0.16672322\n",
      "Iteration 139, loss = 0.10762804\n",
      "Iteration 83, loss = 0.16389484\n",
      "Iteration 140, loss = 0.10713813\n",
      "Iteration 84, loss = 0.16233768\n",
      "Iteration 141, loss = 0.10505858\n",
      "Iteration 85, loss = 0.16015892\n",
      "Iteration 142, loss = 0.10474462\n",
      "Iteration 86, loss = 0.15901668\n",
      "Iteration 143, loss = 0.10413276\n",
      "Iteration 87, loss = 0.15790090\n",
      "Iteration 144, loss = 0.10224637\n",
      "Iteration 88, loss = 0.15563830\n",
      "Iteration 145, loss = 0.10366233\n",
      "Iteration 89, loss = 0.15532487\n",
      "Iteration 146, loss = 0.10106257\n",
      "Iteration 90, loss = 0.15244765\n",
      "Iteration 147, loss = 0.10353446\n",
      "Iteration 91, loss = 0.15169061\n",
      "Iteration 148, loss = 0.09998483\n",
      "Iteration 92, loss = 0.14965412\n",
      "Iteration 149, loss = 0.09825202\n",
      "Iteration 93, loss = 0.14856345\n",
      "Iteration 150, loss = 0.09812740\n",
      "Iteration 94, loss = 0.14676121\n",
      "Iteration 151, loss = 0.09962820\n",
      "Iteration 95, loss = 0.14725247\n",
      "Iteration 152, loss = 0.09611045\n",
      "Iteration 96, loss = 0.14432449\n",
      "Iteration 153, loss = 0.09651150\n",
      "Iteration 97, loss = 0.14347389\n",
      "Iteration 154, loss = 0.09452454\n",
      "Iteration 98, loss = 0.14203221\n",
      "Iteration 155, loss = 0.09359728\n",
      "Iteration 99, loss = 0.13999086\n",
      "Iteration 156, loss = 0.09236128\n",
      "Iteration 100, loss = 0.13960083\n",
      "Iteration 157, loss = 0.09186225\n",
      "Iteration 101, loss = 0.13833363\n",
      "Iteration 158, loss = 0.09195897\n",
      "Iteration 159, loss = 0.09078017Iteration 102, loss = 0.13661643\n",
      "\n",
      "Iteration 160, loss = 0.08987197\n",
      "Iteration 103, loss = 0.13601276\n",
      "Iteration 161, loss = 0.08847826\n",
      "Iteration 104, loss = 0.13465442\n",
      "Iteration 162, loss = 0.08740550\n",
      "Iteration 105, loss = 0.13397667\n",
      "Iteration 163, loss = 0.08709441\n",
      "Iteration 106, loss = 0.13151565\n",
      "Iteration 164, loss = 0.08551938\n",
      "Iteration 107, loss = 0.13179768\n",
      "Iteration 165, loss = 0.08765632\n",
      "Iteration 108, loss = 0.13278065\n",
      "Iteration 166, loss = 0.08803022\n",
      "Iteration 109, loss = 0.12992772\n",
      "Iteration 167, loss = 0.08565508\n",
      "Iteration 110, loss = 0.12918897\n",
      "Iteration 168, loss = 0.08515911\n",
      "Iteration 111, loss = 0.12574047\n",
      "Iteration 169, loss = 0.08485101\n",
      "Iteration 112, loss = 0.12838644\n",
      "Iteration 170, loss = 0.08225255\n",
      "Iteration 113, loss = 0.12493189\n",
      "Iteration 171, loss = 0.08173046\n",
      "Iteration 114, loss = 0.12422479\n",
      "Iteration 172, loss = 0.08126519\n",
      "Iteration 115, loss = 0.12200572\n",
      "Iteration 173, loss = 0.07974508\n",
      "Iteration 116, loss = 0.12238074\n",
      "Iteration 174, loss = 0.07978520\n",
      "Iteration 117, loss = 0.12059947\n",
      "Iteration 175, loss = 0.08099990\n",
      "Iteration 118, loss = 0.12064420\n",
      "Iteration 176, loss = 0.07916679\n",
      "Iteration 119, loss = 0.11975953\n",
      "Iteration 177, loss = 0.07751446\n",
      "Iteration 120, loss = 0.11781521\n",
      "Iteration 178, loss = 0.07827602\n",
      "Iteration 121, loss = 0.11566385\n",
      "Iteration 179, loss = 0.07692408\n",
      "Iteration 122, loss = 0.11504818\n",
      "Iteration 180, loss = 0.07679401\n",
      "Iteration 123, loss = 0.11441147\n",
      "Iteration 181, loss = 0.07490967\n",
      "Iteration 124, loss = 0.11372496\n",
      "Iteration 182, loss = 0.07417512\n",
      "Iteration 125, loss = 0.11269482\n",
      "Iteration 183, loss = 0.07520948\n",
      "Iteration 126, loss = 0.11148760\n",
      "Iteration 184, loss = 0.07403650\n",
      "Iteration 127, loss = 0.11031075\n",
      "Iteration 185, loss = 0.07244068\n",
      "Iteration 128, loss = 0.10929208\n",
      "Iteration 186, loss = 0.07153002\n",
      "Iteration 129, loss = 0.10843679\n",
      "Iteration 187, loss = 0.07295025\n",
      "Iteration 130, loss = 0.10764027\n",
      "Iteration 188, loss = 0.07064766\n",
      "Iteration 131, loss = 0.10705149\n",
      "Iteration 189, loss = 0.07045164\n",
      "Iteration 132, loss = 0.10597035\n",
      "Iteration 190, loss = 0.06940959\n",
      "Iteration 133, loss = 0.10655232\n",
      "Iteration 191, loss = 0.06765984\n",
      "Iteration 134, loss = 0.10492386\n",
      "Iteration 192, loss = 0.06868366\n",
      "Iteration 135, loss = 0.10345819\n",
      "Iteration 193, loss = 0.06838676\n",
      "Iteration 136, loss = 0.10351030\n",
      "Iteration 194, loss = 0.06641120\n",
      "Iteration 137, loss = 0.10150239\n",
      "Iteration 195, loss = 0.06646800\n",
      "Iteration 138, loss = 0.10256319\n",
      "Iteration 196, loss = 0.06608466\n",
      "Iteration 139, loss = 0.10102798\n",
      "Iteration 197, loss = 0.06675642\n",
      "Iteration 140, loss = 0.09950850\n",
      "Iteration 198, loss = 0.06389624\n",
      "Iteration 141, loss = 0.09826619\n",
      "Iteration 199, loss = 0.06423638\n",
      "Iteration 142, loss = 0.09808716\n",
      "Iteration 200, loss = 0.06362391\n",
      "Iteration 143, loss = 0.09726057\n",
      "Iteration 144, loss = 0.09598942\n",
      "Iteration 145, loss = 0.09647865\n",
      "Iteration 146, loss = 0.09489937\n",
      "Iteration 147, loss = 0.09476251\n",
      "Iteration 148, loss = 0.09379287\n",
      "Iteration 149, loss = 0.09297191\n",
      "Iteration 150, loss = 0.09318669\n",
      "Iteration 151, loss = 0.09211551\n",
      "Iteration 152, loss = 0.09182623\n",
      "Iteration 153, loss = 0.09020698\n",
      "Iteration 154, loss = 0.08873272\n",
      "Iteration 155, loss = 0.08837237\n",
      "Iteration 156, loss = 0.08927828\n",
      "Iteration 157, loss = 0.08742446\n",
      "Iteration 158, loss = 0.09017021\n",
      "Iteration 159, loss = 0.08700493\n",
      "Iteration 160, loss = 0.08560674\n",
      "Iteration 1, loss = 1.48810344\n",
      "Iteration 161, loss = 0.08765379\n",
      "Iteration 2, loss = 1.21754999\n",
      "Iteration 162, loss = 0.08438191\n",
      "Iteration 3, loss = 1.04725050\n",
      "Iteration 163, loss = 0.08442998\n",
      "Iteration 4, loss = 0.93201213\n",
      "Iteration 164, loss = 0.08265288\n",
      "Iteration 5, loss = 0.85018378\n",
      "Iteration 165, loss = 0.08122274\n",
      "Iteration 6, loss = 0.78193358\n",
      "Iteration 166, loss = 0.08107435\n",
      "Iteration 7, loss = 0.72246733\n",
      "Iteration 167, loss = 0.08156854\n",
      "Iteration 8, loss = 0.67069762\n",
      "Iteration 168, loss = 0.08077237\n",
      "Iteration 9, loss = 0.62928732\n",
      "Iteration 169, loss = 0.07877450\n",
      "Iteration 10, loss = 0.59224840\n",
      "Iteration 170, loss = 0.07871877\n",
      "Iteration 11, loss = 0.55815108\n",
      "Iteration 171, loss = 0.07720728\n",
      "Iteration 12, loss = 0.52974187\n",
      "Iteration 172, loss = 0.07874282\n",
      "Iteration 13, loss = 0.50529042\n",
      "Iteration 173, loss = 0.07848662\n",
      "Iteration 14, loss = 0.48372974\n",
      "Iteration 174, loss = 0.07790011\n",
      "Iteration 15, loss = 0.46564626\n",
      "Iteration 175, loss = 0.07720564\n",
      "Iteration 16, loss = 0.44766912\n",
      "Iteration 17, loss = 0.43324067\n",
      "Iteration 176, loss = 0.07617305\n",
      "Iteration 18, loss = 0.41981365\n",
      "Iteration 177, loss = 0.07466808\n",
      "Iteration 19, loss = 0.40755595\n",
      "Iteration 178, loss = 0.07432690\n",
      "Iteration 20, loss = 0.39665489\n",
      "Iteration 179, loss = 0.07338591\n",
      "Iteration 21, loss = 0.38575627\n",
      "Iteration 180, loss = 0.07184700\n",
      "Iteration 22, loss = 0.37675547\n",
      "Iteration 181, loss = 0.07279789\n",
      "Iteration 23, loss = 0.36778099\n",
      "Iteration 182, loss = 0.07148130\n",
      "Iteration 24, loss = 0.35940689\n",
      "Iteration 183, loss = 0.07111341\n",
      "Iteration 25, loss = 0.35094278\n",
      "Iteration 184, loss = 0.06932193\n",
      "Iteration 26, loss = 0.34435252\n",
      "Iteration 185, loss = 0.07166471\n",
      "Iteration 27, loss = 0.33727444\n",
      "Iteration 186, loss = 0.06968442\n",
      "Iteration 28, loss = 0.33014138\n",
      "Iteration 187, loss = 0.07005381\n",
      "Iteration 29, loss = 0.32351769\n",
      "Iteration 188, loss = 0.06799375\n",
      "Iteration 30, loss = 0.31828098\n",
      "Iteration 189, loss = 0.06834339\n",
      "Iteration 31, loss = 0.31183765\n",
      "Iteration 190, loss = 0.06804318\n",
      "Iteration 32, loss = 0.30694518\n",
      "Iteration 191, loss = 0.07148467\n",
      "Iteration 33, loss = 0.30074088\n",
      "Iteration 192, loss = 0.06879618\n",
      "Iteration 34, loss = 0.29610802\n",
      "Iteration 193, loss = 0.06596212\n",
      "Iteration 35, loss = 0.29049497\n",
      "Iteration 194, loss = 0.06877984\n",
      "Iteration 36, loss = 0.28582149\n",
      "Iteration 195, loss = 0.06484261\n",
      "Iteration 37, loss = 0.28112197\n",
      "Iteration 196, loss = 0.06526743\n",
      "Iteration 38, loss = 0.27681233\n",
      "Iteration 197, loss = 0.06363392\n",
      "Iteration 39, loss = 0.27248352\n",
      "Iteration 198, loss = 0.06422643\n",
      "Iteration 40, loss = 0.26876586\n",
      "Iteration 199, loss = 0.06292649\n",
      "Iteration 41, loss = 0.26736241\n",
      "Iteration 200, loss = 0.06224645\n",
      "Iteration 42, loss = 0.26118532\n",
      "Iteration 43, loss = 0.25808283\n",
      "Iteration 44, loss = 0.25090324\n",
      "Iteration 45, loss = 0.24821479\n",
      "Iteration 46, loss = 0.24413792\n",
      "Iteration 47, loss = 0.24056319\n",
      "Iteration 48, loss = 0.23675131\n",
      "Iteration 49, loss = 0.23286024\n",
      "Iteration 50, loss = 0.23014987\n",
      "Iteration 51, loss = 0.22532925\n",
      "Iteration 52, loss = 0.22336083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.21899572\n",
      "Iteration 54, loss = 0.21683722\n",
      "Iteration 55, loss = 0.21363216\n",
      "Iteration 56, loss = 0.21013383\n",
      "Iteration 57, loss = 0.20652768\n",
      "Iteration 58, loss = 0.20578877\n",
      "Iteration 59, loss = 0.20101232\n",
      "Iteration 60, loss = 0.19948884\n",
      "Iteration 61, loss = 0.19555905\n",
      "Iteration 1, loss = 1.19912521\n",
      "Iteration 62, loss = 0.19298481\n",
      "Iteration 2, loss = 1.02948470\n",
      "Iteration 63, loss = 0.18988009\n",
      "Iteration 3, loss = 0.91484510\n",
      "Iteration 64, loss = 0.18806278\n",
      "Iteration 4, loss = 0.81766846\n",
      "Iteration 65, loss = 0.18518682\n",
      "Iteration 5, loss = 0.74151592\n",
      "Iteration 66, loss = 0.18374554\n",
      "Iteration 6, loss = 0.68006228\n",
      "Iteration 67, loss = 0.18090747\n",
      "Iteration 7, loss = 0.62949985\n",
      "Iteration 8, loss = 0.58810661\n",
      "Iteration 68, loss = 0.17877356\n",
      "Iteration 69, loss = 0.17513794\n",
      "Iteration 9, loss = 0.55597874\n",
      "Iteration 70, loss = 0.17415188\n",
      "Iteration 10, loss = 0.52762618\n",
      "Iteration 71, loss = 0.17142007\n",
      "Iteration 11, loss = 0.50426919\n",
      "Iteration 72, loss = 0.16863808\n",
      "Iteration 12, loss = 0.48363580\n",
      "Iteration 73, loss = 0.16704919\n",
      "Iteration 13, loss = 0.46601932\n",
      "Iteration 74, loss = 0.16499206\n",
      "Iteration 14, loss = 0.45131735\n",
      "Iteration 75, loss = 0.16240004\n",
      "Iteration 15, loss = 0.43721094\n",
      "Iteration 76, loss = 0.16120916\n",
      "Iteration 16, loss = 0.42414401\n",
      "Iteration 77, loss = 0.15834801\n",
      "Iteration 17, loss = 0.41265291\n",
      "Iteration 78, loss = 0.15674711\n",
      "Iteration 18, loss = 0.40164976\n",
      "Iteration 79, loss = 0.15596322\n",
      "Iteration 19, loss = 0.39246670\n",
      "Iteration 80, loss = 0.15356512\n",
      "Iteration 20, loss = 0.38233096\n",
      "Iteration 81, loss = 0.15071888\n",
      "Iteration 21, loss = 0.37449382\n",
      "Iteration 82, loss = 0.15039923\n",
      "Iteration 22, loss = 0.36595351\n",
      "Iteration 83, loss = 0.14713625\n",
      "Iteration 23, loss = 0.35898119\n",
      "Iteration 84, loss = 0.14650197\n",
      "Iteration 24, loss = 0.35047273\n",
      "Iteration 85, loss = 0.14403039\n",
      "Iteration 25, loss = 0.34350642\n",
      "Iteration 86, loss = 0.14277104\n",
      "Iteration 26, loss = 0.33683648\n",
      "Iteration 87, loss = 0.14025997\n",
      "Iteration 27, loss = 0.33119314\n",
      "Iteration 88, loss = 0.13829325\n",
      "Iteration 28, loss = 0.32422829\n",
      "Iteration 89, loss = 0.13691923\n",
      "Iteration 29, loss = 0.31877748\n",
      "Iteration 90, loss = 0.13560363\n",
      "Iteration 30, loss = 0.31343071\n",
      "Iteration 91, loss = 0.13340189\n",
      "Iteration 31, loss = 0.30730435\n",
      "Iteration 92, loss = 0.13215352\n",
      "Iteration 32, loss = 0.30037588\n",
      "Iteration 93, loss = 0.13092385\n",
      "Iteration 33, loss = 0.29636388\n",
      "Iteration 94, loss = 0.13026883\n",
      "Iteration 95, loss = 0.12806246\n",
      "Iteration 34, loss = 0.29183679\n",
      "Iteration 96, loss = 0.12624467\n",
      "Iteration 35, loss = 0.28711117\n",
      "Iteration 97, loss = 0.12544103\n",
      "Iteration 36, loss = 0.28145696\n",
      "Iteration 98, loss = 0.12452842\n",
      "Iteration 37, loss = 0.27692472\n",
      "Iteration 99, loss = 0.12153812\n",
      "Iteration 38, loss = 0.27274770\n",
      "Iteration 100, loss = 0.12127103\n",
      "Iteration 39, loss = 0.26823939\n",
      "Iteration 101, loss = 0.11951489\n",
      "Iteration 40, loss = 0.26457415\n",
      "Iteration 102, loss = 0.11943572\n",
      "Iteration 41, loss = 0.26032433\n",
      "Iteration 103, loss = 0.11625586\n",
      "Iteration 42, loss = 0.25720818\n",
      "Iteration 104, loss = 0.11720569\n",
      "Iteration 43, loss = 0.25352870\n",
      "Iteration 105, loss = 0.11390747\n",
      "Iteration 44, loss = 0.25008203\n",
      "Iteration 106, loss = 0.11392226\n",
      "Iteration 45, loss = 0.24609150\n",
      "Iteration 107, loss = 0.11179887\n",
      "Iteration 46, loss = 0.24380761\n",
      "Iteration 108, loss = 0.11118430\n",
      "Iteration 47, loss = 0.24017446\n",
      "Iteration 109, loss = 0.10928239\n",
      "Iteration 48, loss = 0.23705071\n",
      "Iteration 110, loss = 0.10964303\n",
      "Iteration 49, loss = 0.23382711\n",
      "Iteration 111, loss = 0.10751588\n",
      "Iteration 50, loss = 0.22940357\n",
      "Iteration 112, loss = 0.10639061\n",
      "Iteration 51, loss = 0.22762356\n",
      "Iteration 113, loss = 0.10544168\n",
      "Iteration 52, loss = 0.22428916\n",
      "Iteration 114, loss = 0.10360939\n",
      "Iteration 53, loss = 0.22175449\n",
      "Iteration 115, loss = 0.10346558\n",
      "Iteration 54, loss = 0.21914622\n",
      "Iteration 116, loss = 0.10202518\n",
      "Iteration 55, loss = 0.21620522\n",
      "Iteration 117, loss = 0.10095991\n",
      "Iteration 56, loss = 0.21379369\n",
      "Iteration 118, loss = 0.10008568\n",
      "Iteration 57, loss = 0.21156192\n",
      "Iteration 119, loss = 0.10028170\n",
      "Iteration 58, loss = 0.20831632\n",
      "Iteration 120, loss = 0.09825732\n",
      "Iteration 59, loss = 0.20654629\n",
      "Iteration 121, loss = 0.09737755\n",
      "Iteration 122, loss = 0.09614061\n",
      "Iteration 60, loss = 0.20281823\n",
      "Iteration 123, loss = 0.09577127\n",
      "Iteration 61, loss = 0.20141588\n",
      "Iteration 62, loss = 0.19861322\n",
      "Iteration 124, loss = 0.09408471\n",
      "Iteration 63, loss = 0.19557253\n",
      "Iteration 125, loss = 0.09361746\n",
      "Iteration 64, loss = 0.19268215\n",
      "Iteration 126, loss = 0.09208907\n",
      "Iteration 127, loss = 0.09174860\n",
      "Iteration 65, loss = 0.19181827\n",
      "Iteration 128, loss = 0.09188248\n",
      "Iteration 66, loss = 0.19069957\n",
      "Iteration 129, loss = 0.09118869\n",
      "Iteration 67, loss = 0.18657420\n",
      "Iteration 130, loss = 0.08951326\n",
      "Iteration 68, loss = 0.18505368\n",
      "Iteration 131, loss = 0.08950307\n",
      "Iteration 69, loss = 0.18188393\n",
      "Iteration 132, loss = 0.08672286\n",
      "Iteration 70, loss = 0.17989874\n",
      "Iteration 133, loss = 0.08668178\n",
      "Iteration 71, loss = 0.17796439\n",
      "Iteration 134, loss = 0.08596206\n",
      "Iteration 72, loss = 0.17584506\n",
      "Iteration 135, loss = 0.08488362\n",
      "Iteration 73, loss = 0.17403278\n",
      "Iteration 136, loss = 0.08461769\n",
      "Iteration 74, loss = 0.17364225\n",
      "Iteration 137, loss = 0.08395350\n",
      "Iteration 75, loss = 0.16914875\n",
      "Iteration 138, loss = 0.08294127\n",
      "Iteration 76, loss = 0.16980813\n",
      "Iteration 139, loss = 0.08282064\n",
      "Iteration 77, loss = 0.16627432\n",
      "Iteration 140, loss = 0.08104701\n",
      "Iteration 78, loss = 0.16505546\n",
      "Iteration 141, loss = 0.08104328\n",
      "Iteration 79, loss = 0.16273622\n",
      "Iteration 142, loss = 0.08171192\n",
      "Iteration 80, loss = 0.16089870\n",
      "Iteration 143, loss = 0.07892356\n",
      "Iteration 81, loss = 0.15904033\n",
      "Iteration 144, loss = 0.07892211\n",
      "Iteration 82, loss = 0.15618049\n",
      "Iteration 145, loss = 0.08133896\n",
      "Iteration 83, loss = 0.15491956\n",
      "Iteration 146, loss = 0.07569529\n",
      "Iteration 84, loss = 0.15305127\n",
      "Iteration 147, loss = 0.07786982\n",
      "Iteration 85, loss = 0.15115485\n",
      "Iteration 148, loss = 0.07478317\n",
      "Iteration 86, loss = 0.14976822\n",
      "Iteration 149, loss = 0.07436716\n",
      "Iteration 87, loss = 0.14946543\n",
      "Iteration 150, loss = 0.07366864\n",
      "Iteration 88, loss = 0.14653030\n",
      "Iteration 151, loss = 0.07292505\n",
      "Iteration 89, loss = 0.14558679\n",
      "Iteration 152, loss = 0.07268636\n",
      "Iteration 90, loss = 0.14365474\n",
      "Iteration 153, loss = 0.07206074\n",
      "Iteration 91, loss = 0.14171401\n",
      "Iteration 154, loss = 0.07130877\n",
      "Iteration 92, loss = 0.14142280\n",
      "Iteration 155, loss = 0.07108024\n",
      "Iteration 93, loss = 0.13766417\n",
      "Iteration 156, loss = 0.07062097\n",
      "Iteration 94, loss = 0.13786527\n",
      "Iteration 157, loss = 0.06922733\n",
      "Iteration 95, loss = 0.13618350\n",
      "Iteration 158, loss = 0.06958533\n",
      "Iteration 96, loss = 0.13444816\n",
      "Iteration 159, loss = 0.06943587\n",
      "Iteration 97, loss = 0.13276982\n",
      "Iteration 160, loss = 0.06920600\n",
      "Iteration 98, loss = 0.13097671\n",
      "Iteration 161, loss = 0.06813039\n",
      "Iteration 99, loss = 0.12979375\n",
      "Iteration 162, loss = 0.06654992\n",
      "Iteration 100, loss = 0.12825985\n",
      "Iteration 163, loss = 0.06619692\n",
      "Iteration 101, loss = 0.12763243\n",
      "Iteration 164, loss = 0.06592168\n",
      "Iteration 102, loss = 0.12484545\n",
      "Iteration 165, loss = 0.06576792\n",
      "Iteration 103, loss = 0.12467716\n",
      "Iteration 166, loss = 0.06430203\n",
      "Iteration 104, loss = 0.12361125\n",
      "Iteration 167, loss = 0.06476379\n",
      "Iteration 105, loss = 0.12279752\n",
      "Iteration 168, loss = 0.06426983\n",
      "Iteration 106, loss = 0.12151802\n",
      "Iteration 169, loss = 0.06249836\n",
      "Iteration 107, loss = 0.11908012\n",
      "Iteration 170, loss = 0.06380945\n",
      "Iteration 108, loss = 0.11862333\n",
      "Iteration 171, loss = 0.06342442\n",
      "Iteration 109, loss = 0.11704580\n",
      "Iteration 172, loss = 0.06076864\n",
      "Iteration 110, loss = 0.11549624\n",
      "Iteration 173, loss = 0.06155773\n",
      "Iteration 111, loss = 0.11531075\n",
      "Iteration 174, loss = 0.06046034\n",
      "Iteration 112, loss = 0.11382299\n",
      "Iteration 175, loss = 0.06161620\n",
      "Iteration 113, loss = 0.11403663\n",
      "Iteration 176, loss = 0.06133262\n",
      "Iteration 114, loss = 0.11172932\n",
      "Iteration 177, loss = 0.05883755\n",
      "Iteration 115, loss = 0.11038639\n",
      "Iteration 178, loss = 0.05840677\n",
      "Iteration 116, loss = 0.11236859\n",
      "Iteration 179, loss = 0.05787724\n",
      "Iteration 117, loss = 0.10903256\n",
      "Iteration 180, loss = 0.05693072\n",
      "Iteration 118, loss = 0.10809927\n",
      "Iteration 181, loss = 0.05658747\n",
      "Iteration 119, loss = 0.10612431\n",
      "Iteration 182, loss = 0.05648322\n",
      "Iteration 120, loss = 0.10576267\n",
      "Iteration 183, loss = 0.05783991\n",
      "Iteration 121, loss = 0.10504817\n",
      "Iteration 184, loss = 0.05572491\n",
      "Iteration 122, loss = 0.10348895\n",
      "Iteration 185, loss = 0.05456066\n",
      "Iteration 123, loss = 0.10459026\n",
      "Iteration 186, loss = 0.05426568\n",
      "Iteration 124, loss = 0.10074942\n",
      "Iteration 187, loss = 0.05377637\n",
      "Iteration 125, loss = 0.10322375\n",
      "Iteration 188, loss = 0.05305530\n",
      "Iteration 126, loss = 0.10008034\n",
      "Iteration 189, loss = 0.05360165\n",
      "Iteration 127, loss = 0.09839472\n",
      "Iteration 190, loss = 0.05289336\n",
      "Iteration 128, loss = 0.09840449\n",
      "Iteration 191, loss = 0.05203001\n",
      "Iteration 129, loss = 0.09733728\n",
      "Iteration 192, loss = 0.05164799\n",
      "Iteration 130, loss = 0.09625036\n",
      "Iteration 193, loss = 0.05202147\n",
      "Iteration 131, loss = 0.09712564\n",
      "Iteration 194, loss = 0.05111849\n",
      "Iteration 132, loss = 0.09447619\n",
      "Iteration 195, loss = 0.05102578\n",
      "Iteration 133, loss = 0.09453049\n",
      "Iteration 196, loss = 0.05146144\n",
      "Iteration 134, loss = 0.09320231\n",
      "Iteration 197, loss = 0.04898628\n",
      "Iteration 135, loss = 0.09106210\n",
      "Iteration 198, loss = 0.05063206\n",
      "Iteration 136, loss = 0.09086198\n",
      "Iteration 199, loss = 0.05107321\n",
      "Iteration 137, loss = 0.08932982\n",
      "Iteration 200, loss = 0.04833253\n",
      "Iteration 138, loss = 0.08869537\n",
      "Iteration 139, loss = 0.08802390\n",
      "Iteration 140, loss = 0.08748472\n",
      "Iteration 141, loss = 0.08641208\n",
      "Iteration 142, loss = 0.08609592\n",
      "Iteration 143, loss = 0.08499228\n",
      "Iteration 144, loss = 0.08544819\n",
      "Iteration 145, loss = 0.08316255\n",
      "Iteration 146, loss = 0.08261667\n",
      "Iteration 147, loss = 0.08189915\n",
      "Iteration 148, loss = 0.08119253\n",
      "Iteration 149, loss = 0.08041303\n",
      "Iteration 150, loss = 0.08014176\n",
      "Iteration 151, loss = 0.07821876\n",
      "Iteration 152, loss = 0.07986440\n",
      "Iteration 153, loss = 0.07818835\n",
      "Iteration 154, loss = 0.07726214\n",
      "Iteration 155, loss = 0.07649392\n",
      "Iteration 1, loss = 1.46323755\n",
      "Iteration 156, loss = 0.08030164\n",
      "Iteration 2, loss = 1.10974645\n",
      "Iteration 157, loss = 0.07881684\n",
      "Iteration 3, loss = 0.93105524\n",
      "Iteration 158, loss = 0.07827292\n",
      "Iteration 4, loss = 0.81399617\n",
      "Iteration 159, loss = 0.07552146\n",
      "Iteration 5, loss = 0.73202141\n",
      "Iteration 160, loss = 0.07496333\n",
      "Iteration 6, loss = 0.66297098\n",
      "Iteration 161, loss = 0.07419886\n",
      "Iteration 7, loss = 0.61067349\n",
      "Iteration 162, loss = 0.07090226\n",
      "Iteration 8, loss = 0.57458520\n",
      "Iteration 163, loss = 0.07175723\n",
      "Iteration 9, loss = 0.54545253\n",
      "Iteration 164, loss = 0.06916864\n",
      "Iteration 10, loss = 0.51960274\n",
      "Iteration 165, loss = 0.06912446\n",
      "Iteration 11, loss = 0.49730407\n",
      "Iteration 166, loss = 0.06908702\n",
      "Iteration 12, loss = 0.47947006\n",
      "Iteration 167, loss = 0.06864121\n",
      "Iteration 13, loss = 0.46146017\n",
      "Iteration 168, loss = 0.06751401\n",
      "Iteration 14, loss = 0.44685286\n",
      "Iteration 169, loss = 0.06779830\n",
      "Iteration 15, loss = 0.43306460\n",
      "Iteration 170, loss = 0.06624702\n",
      "Iteration 16, loss = 0.42066935\n",
      "Iteration 171, loss = 0.06697433\n",
      "Iteration 17, loss = 0.40911009\n",
      "Iteration 172, loss = 0.06559851\n",
      "Iteration 18, loss = 0.39860713\n",
      "Iteration 173, loss = 0.06367814\n",
      "Iteration 19, loss = 0.38883743\n",
      "Iteration 174, loss = 0.06425391\n",
      "Iteration 20, loss = 0.38019664\n",
      "Iteration 175, loss = 0.06452807\n",
      "Iteration 21, loss = 0.37127083\n",
      "Iteration 176, loss = 0.06417491\n",
      "Iteration 22, loss = 0.36330738\n",
      "Iteration 177, loss = 0.06411864\n",
      "Iteration 23, loss = 0.35631843\n",
      "Iteration 178, loss = 0.06164865\n",
      "Iteration 24, loss = 0.34897152\n",
      "Iteration 179, loss = 0.06373061\n",
      "Iteration 25, loss = 0.34240721\n",
      "Iteration 180, loss = 0.06092091\n",
      "Iteration 26, loss = 0.33594838\n",
      "Iteration 181, loss = 0.05974277\n",
      "Iteration 27, loss = 0.32993830\n",
      "Iteration 182, loss = 0.06018978\n",
      "Iteration 28, loss = 0.32460104\n",
      "Iteration 183, loss = 0.05960672\n",
      "Iteration 29, loss = 0.31810347\n",
      "Iteration 184, loss = 0.05894139\n",
      "Iteration 30, loss = 0.31265309\n",
      "Iteration 185, loss = 0.05905093\n",
      "Iteration 31, loss = 0.30722585\n",
      "Iteration 186, loss = 0.05709526\n",
      "Iteration 32, loss = 0.30253687\n",
      "Iteration 187, loss = 0.05808958\n",
      "Iteration 33, loss = 0.29796642\n",
      "Iteration 188, loss = 0.05730982\n",
      "Iteration 34, loss = 0.29307846\n",
      "Iteration 189, loss = 0.05692915\n",
      "Iteration 35, loss = 0.28814228\n",
      "Iteration 190, loss = 0.05623499\n",
      "Iteration 36, loss = 0.28431701\n",
      "Iteration 191, loss = 0.05784741\n",
      "Iteration 37, loss = 0.28079003\n",
      "Iteration 192, loss = 0.06153182\n",
      "Iteration 38, loss = 0.27508105\n",
      "Iteration 193, loss = 0.05337790\n",
      "Iteration 39, loss = 0.27189421\n",
      "Iteration 194, loss = 0.05573687\n",
      "Iteration 40, loss = 0.26788527\n",
      "Iteration 195, loss = 0.05524276\n",
      "Iteration 41, loss = 0.26562840\n",
      "Iteration 196, loss = 0.05349841\n",
      "Iteration 42, loss = 0.26027644\n",
      "Iteration 197, loss = 0.05299191\n",
      "Iteration 43, loss = 0.25780549\n",
      "Iteration 198, loss = 0.05228603\n",
      "Iteration 44, loss = 0.25319941\n",
      "Iteration 199, loss = 0.05204725\n",
      "Iteration 45, loss = 0.24977259\n",
      "Iteration 200, loss = 0.05115549\n",
      "Iteration 46, loss = 0.24670431\n",
      "Iteration 47, loss = 0.24342782\n",
      "Iteration 48, loss = 0.24036791\n",
      "Iteration 49, loss = 0.23685512\n",
      "Iteration 50, loss = 0.23420331\n",
      "Iteration 51, loss = 0.23081410\n",
      "Iteration 52, loss = 0.22890846\n",
      "Iteration 53, loss = 0.22538340\n",
      "Iteration 54, loss = 0.22288710\n",
      "Iteration 55, loss = 0.21914468\n",
      "Iteration 56, loss = 0.21806951\n",
      "Iteration 57, loss = 0.21607364\n",
      "Iteration 58, loss = 0.21224942\n",
      "Iteration 59, loss = 0.21038764\n",
      "Iteration 60, loss = 0.20651866\n",
      "Iteration 61, loss = 0.20470539\n",
      "Iteration 62, loss = 0.20148555\n",
      "Iteration 63, loss = 0.20027310\n",
      "Iteration 64, loss = 0.19727379\n",
      "Iteration 1, loss = 1.16315384\n",
      "Iteration 65, loss = 0.19463381\n",
      "Iteration 2, loss = 0.99016268\n",
      "Iteration 66, loss = 0.19265981\n",
      "Iteration 3, loss = 0.85826538\n",
      "Iteration 67, loss = 0.19019515\n",
      "Iteration 4, loss = 0.76489112\n",
      "Iteration 68, loss = 0.18853125\n",
      "Iteration 5, loss = 0.68976263\n",
      "Iteration 69, loss = 0.18573718\n",
      "Iteration 6, loss = 0.63084181\n",
      "Iteration 70, loss = 0.18476491\n",
      "Iteration 7, loss = 0.58607110\n",
      "Iteration 71, loss = 0.18155412\n",
      "Iteration 8, loss = 0.55004124\n",
      "Iteration 72, loss = 0.18024163\n",
      "Iteration 9, loss = 0.52109490\n",
      "Iteration 73, loss = 0.17764059\n",
      "Iteration 10, loss = 0.49760001\n",
      "Iteration 74, loss = 0.17596899\n",
      "Iteration 11, loss = 0.47632229\n",
      "Iteration 75, loss = 0.17363542\n",
      "Iteration 12, loss = 0.45908159\n",
      "Iteration 76, loss = 0.17162420\n",
      "Iteration 13, loss = 0.44295675\n",
      "Iteration 77, loss = 0.17231008\n",
      "Iteration 14, loss = 0.43038322\n",
      "Iteration 78, loss = 0.17123359\n",
      "Iteration 15, loss = 0.41663592\n",
      "Iteration 79, loss = 0.16691200\n",
      "Iteration 16, loss = 0.40495899\n",
      "Iteration 80, loss = 0.16511913\n",
      "Iteration 17, loss = 0.39461613\n",
      "Iteration 81, loss = 0.16397167\n",
      "Iteration 18, loss = 0.38607774\n",
      "Iteration 82, loss = 0.16113667\n",
      "Iteration 19, loss = 0.37667351\n",
      "Iteration 83, loss = 0.15971393\n",
      "Iteration 20, loss = 0.36831891\n",
      "Iteration 84, loss = 0.15869316\n",
      "Iteration 21, loss = 0.36057384\n",
      "Iteration 85, loss = 0.15733341\n",
      "Iteration 22, loss = 0.35363550\n",
      "Iteration 86, loss = 0.15506280\n",
      "Iteration 23, loss = 0.34829570\n",
      "Iteration 87, loss = 0.15355014\n",
      "Iteration 24, loss = 0.34181438\n",
      "Iteration 88, loss = 0.15242049\n",
      "Iteration 25, loss = 0.33450264\n",
      "Iteration 89, loss = 0.14906281\n",
      "Iteration 26, loss = 0.32802292\n",
      "Iteration 90, loss = 0.15035006\n",
      "Iteration 27, loss = 0.32320735\n",
      "Iteration 91, loss = 0.14710325\n",
      "Iteration 28, loss = 0.31713786\n",
      "Iteration 92, loss = 0.14729420\n",
      "Iteration 29, loss = 0.31181181\n",
      "Iteration 93, loss = 0.14582869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.30650021\n",
      "Iteration 94, loss = 0.14276988\n",
      "Iteration 31, loss = 0.30223554\n",
      "Iteration 95, loss = 0.14366446\n",
      "Iteration 32, loss = 0.29642631\n",
      "Iteration 96, loss = 0.14028023\n",
      "Iteration 33, loss = 0.29210288\n",
      "Iteration 97, loss = 0.13959599\n",
      "Iteration 34, loss = 0.28732474\n",
      "Iteration 98, loss = 0.13808600\n",
      "Iteration 35, loss = 0.28269061\n",
      "Iteration 99, loss = 0.13619264\n",
      "Iteration 36, loss = 0.27841613\n",
      "Iteration 100, loss = 0.13518900\n",
      "Iteration 37, loss = 0.27472410\n",
      "Iteration 101, loss = 0.13374526\n",
      "Iteration 38, loss = 0.27037339\n",
      "Iteration 102, loss = 0.13339682\n",
      "Iteration 39, loss = 0.26640611\n",
      "Iteration 103, loss = 0.13137328\n",
      "Iteration 40, loss = 0.26250639\n",
      "Iteration 104, loss = 0.13055753\n",
      "Iteration 41, loss = 0.25845228\n",
      "Iteration 105, loss = 0.12864442\n",
      "Iteration 42, loss = 0.25430682\n",
      "Iteration 106, loss = 0.12725204\n",
      "Iteration 43, loss = 0.25101513\n",
      "Iteration 107, loss = 0.12707022\n",
      "Iteration 44, loss = 0.24726736\n",
      "Iteration 108, loss = 0.12550902\n",
      "Iteration 45, loss = 0.24324669\n",
      "Iteration 109, loss = 0.12518736\n",
      "Iteration 46, loss = 0.24023378\n",
      "Iteration 110, loss = 0.12376409\n",
      "Iteration 47, loss = 0.23742733\n",
      "Iteration 111, loss = 0.12186713\n",
      "Iteration 48, loss = 0.23362050\n",
      "Iteration 112, loss = 0.12052573\n",
      "Iteration 49, loss = 0.23054183\n",
      "Iteration 113, loss = 0.12059225\n",
      "Iteration 50, loss = 0.22669488\n",
      "Iteration 114, loss = 0.11954600\n",
      "Iteration 51, loss = 0.22398558\n",
      "Iteration 115, loss = 0.11826645\n",
      "Iteration 52, loss = 0.22043449\n",
      "Iteration 116, loss = 0.11860338\n",
      "Iteration 53, loss = 0.21727933\n",
      "Iteration 117, loss = 0.11563445\n",
      "Iteration 54, loss = 0.21500646\n",
      "Iteration 118, loss = 0.11476104\n",
      "Iteration 55, loss = 0.21276476\n",
      "Iteration 119, loss = 0.11342215\n",
      "Iteration 56, loss = 0.20965990\n",
      "Iteration 120, loss = 0.11272653\n",
      "Iteration 57, loss = 0.20688196\n",
      "Iteration 121, loss = 0.11275869\n",
      "Iteration 58, loss = 0.20381569\n",
      "Iteration 122, loss = 0.11214515\n",
      "Iteration 59, loss = 0.20054612\n",
      "Iteration 123, loss = 0.10970239\n",
      "Iteration 60, loss = 0.19824947\n",
      "Iteration 124, loss = 0.11013839\n",
      "Iteration 61, loss = 0.19602304\n",
      "Iteration 125, loss = 0.10769344\n",
      "Iteration 62, loss = 0.19347847\n",
      "Iteration 126, loss = 0.10742706\n",
      "Iteration 63, loss = 0.19124116\n",
      "Iteration 127, loss = 0.10551606\n",
      "Iteration 64, loss = 0.18995158\n",
      "Iteration 128, loss = 0.10582131\n",
      "Iteration 65, loss = 0.18695377\n",
      "Iteration 129, loss = 0.10457331\n",
      "Iteration 66, loss = 0.18428390\n",
      "Iteration 130, loss = 0.10318150\n",
      "Iteration 67, loss = 0.18119314\n",
      "Iteration 131, loss = 0.10352713\n",
      "Iteration 68, loss = 0.17947039\n",
      "Iteration 132, loss = 0.10213050\n",
      "Iteration 69, loss = 0.17690271\n",
      "Iteration 133, loss = 0.10092307\n",
      "Iteration 70, loss = 0.17534322\n",
      "Iteration 134, loss = 0.10013330\n",
      "Iteration 71, loss = 0.17428887\n",
      "Iteration 135, loss = 0.09923297\n",
      "Iteration 72, loss = 0.17235760\n",
      "Iteration 136, loss = 0.09782817\n",
      "Iteration 73, loss = 0.16942428\n",
      "Iteration 137, loss = 0.09772338\n",
      "Iteration 74, loss = 0.16769244\n",
      "Iteration 138, loss = 0.09651849\n",
      "Iteration 75, loss = 0.16584673\n",
      "Iteration 139, loss = 0.09494140\n",
      "Iteration 76, loss = 0.16459409\n",
      "Iteration 140, loss = 0.09514258\n",
      "Iteration 77, loss = 0.16134349\n",
      "Iteration 141, loss = 0.09446213\n",
      "Iteration 78, loss = 0.15904498\n",
      "Iteration 142, loss = 0.09374613\n",
      "Iteration 79, loss = 0.15794488\n",
      "Iteration 143, loss = 0.09240486\n",
      "Iteration 80, loss = 0.15510563\n",
      "Iteration 144, loss = 0.09110041\n",
      "Iteration 81, loss = 0.15412049\n",
      "Iteration 145, loss = 0.09372246\n",
      "Iteration 82, loss = 0.15505414\n",
      "Iteration 146, loss = 0.09114782\n",
      "Iteration 83, loss = 0.14977225\n",
      "Iteration 147, loss = 0.09196182\n",
      "Iteration 84, loss = 0.15039306\n",
      "Iteration 148, loss = 0.08974348\n",
      "Iteration 85, loss = 0.14689881\n",
      "Iteration 149, loss = 0.08809686\n",
      "Iteration 86, loss = 0.14626127\n",
      "Iteration 150, loss = 0.08727735\n",
      "Iteration 87, loss = 0.14500649\n",
      "Iteration 151, loss = 0.08978290\n",
      "Iteration 88, loss = 0.14328950\n",
      "Iteration 152, loss = 0.08562894\n",
      "Iteration 89, loss = 0.14063123\n",
      "Iteration 153, loss = 0.08604963\n",
      "Iteration 90, loss = 0.13933198\n",
      "Iteration 154, loss = 0.08419131\n",
      "Iteration 91, loss = 0.13777364\n",
      "Iteration 155, loss = 0.08392124\n",
      "Iteration 92, loss = 0.13501876\n",
      "Iteration 156, loss = 0.08216846\n",
      "Iteration 93, loss = 0.13603270\n",
      "Iteration 157, loss = 0.08194786\n",
      "Iteration 94, loss = 0.13583111\n",
      "Iteration 158, loss = 0.08305307\n",
      "Iteration 95, loss = 0.13275146\n",
      "Iteration 159, loss = 0.08154965\n",
      "Iteration 96, loss = 0.13181020\n",
      "Iteration 160, loss = 0.08061593\n",
      "Iteration 97, loss = 0.12844839\n",
      "Iteration 161, loss = 0.07981143\n",
      "Iteration 98, loss = 0.12689972\n",
      "Iteration 162, loss = 0.07884113\n",
      "Iteration 99, loss = 0.12539126\n",
      "Iteration 163, loss = 0.07753543Iteration 100, loss = 0.12452530\n",
      "\n",
      "Iteration 101, loss = 0.12344410\n",
      "Iteration 164, loss = 0.07873109\n",
      "Iteration 102, loss = 0.12352841\n",
      "Iteration 165, loss = 0.07759443\n",
      "Iteration 103, loss = 0.12036850\n",
      "Iteration 166, loss = 0.07691712\n",
      "Iteration 104, loss = 0.11904665\n",
      "Iteration 167, loss = 0.07614814\n",
      "Iteration 105, loss = 0.11886713\n",
      "Iteration 168, loss = 0.07908329\n",
      "Iteration 106, loss = 0.11621305\n",
      "Iteration 169, loss = 0.07629968\n",
      "Iteration 107, loss = 0.11740390\n",
      "Iteration 170, loss = 0.07681524\n",
      "Iteration 108, loss = 0.11309859\n",
      "Iteration 171, loss = 0.07503775\n",
      "Iteration 109, loss = 0.11311238\n",
      "Iteration 172, loss = 0.07348003\n",
      "Iteration 110, loss = 0.11161999\n",
      "Iteration 173, loss = 0.07364124\n",
      "Iteration 111, loss = 0.11011272\n",
      "Iteration 174, loss = 0.07542139\n",
      "Iteration 112, loss = 0.10906974\n",
      "Iteration 175, loss = 0.07030419\n",
      "Iteration 113, loss = 0.10801966\n",
      "Iteration 176, loss = 0.07235450\n",
      "Iteration 114, loss = 0.10614202\n",
      "Iteration 177, loss = 0.07198771\n",
      "Iteration 115, loss = 0.10506937\n",
      "Iteration 178, loss = 0.07025085\n",
      "Iteration 116, loss = 0.10515364\n",
      "Iteration 179, loss = 0.07023768\n",
      "Iteration 117, loss = 0.10303989\n",
      "Iteration 180, loss = 0.07016103\n",
      "Iteration 118, loss = 0.10257753\n",
      "Iteration 181, loss = 0.06822454\n",
      "Iteration 119, loss = 0.10103975\n",
      "Iteration 182, loss = 0.06862676\n",
      "Iteration 120, loss = 0.10018645\n",
      "Iteration 183, loss = 0.06878713\n",
      "Iteration 121, loss = 0.09915682\n",
      "Iteration 184, loss = 0.06621060\n",
      "Iteration 122, loss = 0.09835425\n",
      "Iteration 185, loss = 0.06609168\n",
      "Iteration 123, loss = 0.09659689\n",
      "Iteration 186, loss = 0.06591888\n",
      "Iteration 124, loss = 0.09583503\n",
      "Iteration 187, loss = 0.06439867\n",
      "Iteration 125, loss = 0.09426496\n",
      "Iteration 188, loss = 0.06398728\n",
      "Iteration 126, loss = 0.09363154\n",
      "Iteration 189, loss = 0.06566167\n",
      "Iteration 127, loss = 0.09194603\n",
      "Iteration 190, loss = 0.06615946\n",
      "Iteration 128, loss = 0.09159888\n",
      "Iteration 191, loss = 0.06344029\n",
      "Iteration 129, loss = 0.09084215\n",
      "Iteration 192, loss = 0.06292120\n",
      "Iteration 130, loss = 0.08915923\n",
      "Iteration 193, loss = 0.06343395\n",
      "Iteration 131, loss = 0.08827953\n",
      "Iteration 194, loss = 0.06336776\n",
      "Iteration 132, loss = 0.08810367\n",
      "Iteration 195, loss = 0.06240344\n",
      "Iteration 133, loss = 0.08933268\n",
      "Iteration 196, loss = 0.06171283\n",
      "Iteration 134, loss = 0.08786546\n",
      "Iteration 197, loss = 0.06076725\n",
      "Iteration 135, loss = 0.08479306\n",
      "Iteration 198, loss = 0.06076819\n",
      "Iteration 136, loss = 0.08567704\n",
      "Iteration 199, loss = 0.05947877\n",
      "Iteration 137, loss = 0.08647851\n",
      "Iteration 200, loss = 0.05856133\n",
      "Iteration 138, loss = 0.08123606\n",
      "Iteration 139, loss = 0.08511576\n",
      "Iteration 140, loss = 0.08509402\n",
      "Iteration 141, loss = 0.08279264\n",
      "Iteration 142, loss = 0.08208124\n",
      "Iteration 143, loss = 0.08266845\n",
      "Iteration 144, loss = 0.07951941\n",
      "Iteration 145, loss = 0.07946290\n",
      "Iteration 146, loss = 0.07696742\n",
      "Iteration 147, loss = 0.07722919\n",
      "Iteration 148, loss = 0.07499724\n",
      "Iteration 149, loss = 0.07424495\n",
      "Iteration 150, loss = 0.07373539\n",
      "Iteration 151, loss = 0.07157696\n",
      "Iteration 152, loss = 0.07099720\n",
      "Iteration 153, loss = 0.07002936\n",
      "Iteration 154, loss = 0.06944547\n",
      "Iteration 155, loss = 0.06989085\n",
      "Iteration 1, loss = 1.11702434\n",
      "Iteration 156, loss = 0.06769691\n",
      "Iteration 2, loss = 0.95434538\n",
      "Iteration 157, loss = 0.06809257\n",
      "Iteration 3, loss = 0.83505841\n",
      "Iteration 158, loss = 0.07002345\n",
      "Iteration 4, loss = 0.74172999\n",
      "Iteration 159, loss = 0.06523467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.66802594\n",
      "Iteration 160, loss = 0.06542209\n",
      "Iteration 6, loss = 0.61324661\n",
      "Iteration 161, loss = 0.06468593\n",
      "Iteration 7, loss = 0.56998035\n",
      "Iteration 162, loss = 0.06348095\n",
      "Iteration 8, loss = 0.53730610\n",
      "Iteration 163, loss = 0.06394668\n",
      "Iteration 9, loss = 0.51076024\n",
      "Iteration 164, loss = 0.06499324\n",
      "Iteration 10, loss = 0.48751295\n",
      "Iteration 165, loss = 0.06558188\n",
      "Iteration 11, loss = 0.46829681\n",
      "Iteration 166, loss = 0.06328699\n",
      "Iteration 12, loss = 0.45217108\n",
      "Iteration 167, loss = 0.06197381\n",
      "Iteration 13, loss = 0.43803519\n",
      "Iteration 168, loss = 0.06031782\n",
      "Iteration 14, loss = 0.42487247\n",
      "Iteration 169, loss = 0.06012963\n",
      "Iteration 15, loss = 0.41313494\n",
      "Iteration 170, loss = 0.05842420\n",
      "Iteration 16, loss = 0.40417140\n",
      "Iteration 171, loss = 0.05895738\n",
      "Iteration 17, loss = 0.39387042\n",
      "Iteration 172, loss = 0.05863607\n",
      "Iteration 18, loss = 0.38536892\n",
      "Iteration 173, loss = 0.05693662\n",
      "Iteration 19, loss = 0.37678202\n",
      "Iteration 174, loss = 0.05606866\n",
      "Iteration 20, loss = 0.36858779\n",
      "Iteration 175, loss = 0.05575856\n",
      "Iteration 21, loss = 0.36175361\n",
      "Iteration 176, loss = 0.05458422\n",
      "Iteration 177, loss = 0.05503824\n",
      "Iteration 22, loss = 0.35422604\n",
      "Iteration 178, loss = 0.05420649\n",
      "Iteration 23, loss = 0.34757975\n",
      "Iteration 179, loss = 0.05347584\n",
      "Iteration 24, loss = 0.34149366\n",
      "Iteration 180, loss = 0.05320418\n",
      "Iteration 25, loss = 0.33493463\n",
      "Iteration 181, loss = 0.05251926\n",
      "Iteration 26, loss = 0.32951471\n",
      "Iteration 182, loss = 0.05200170\n",
      "Iteration 27, loss = 0.32318224\n",
      "Iteration 183, loss = 0.05134561\n",
      "Iteration 28, loss = 0.31827660\n",
      "Iteration 184, loss = 0.05109740\n",
      "Iteration 29, loss = 0.31273084\n",
      "Iteration 185, loss = 0.04978864\n",
      "Iteration 30, loss = 0.30776046\n",
      "Iteration 186, loss = 0.05033940\n",
      "Iteration 31, loss = 0.30177106\n",
      "Iteration 187, loss = 0.04953247\n",
      "Iteration 32, loss = 0.29751386\n",
      "Iteration 188, loss = 0.04940833\n",
      "Iteration 33, loss = 0.29209608\n",
      "Iteration 189, loss = 0.04799504\n",
      "Iteration 34, loss = 0.28835052\n",
      "Iteration 190, loss = 0.04849373\n",
      "Iteration 35, loss = 0.28298339\n",
      "Iteration 191, loss = 0.04758143\n",
      "Iteration 36, loss = 0.27948034\n",
      "Iteration 192, loss = 0.04653270\n",
      "Iteration 37, loss = 0.27499865\n",
      "Iteration 193, loss = 0.04611538\n",
      "Iteration 38, loss = 0.26998975\n",
      "Iteration 194, loss = 0.04607634\n",
      "Iteration 39, loss = 0.26532398\n",
      "Iteration 195, loss = 0.04689053\n",
      "Iteration 40, loss = 0.26109594\n",
      "Iteration 196, loss = 0.04551917\n",
      "Iteration 41, loss = 0.25806901\n",
      "Iteration 197, loss = 0.04414261\n",
      "Iteration 42, loss = 0.25373676\n",
      "Iteration 198, loss = 0.04448629\n",
      "Iteration 43, loss = 0.24919123\n",
      "Iteration 199, loss = 0.04502143\n",
      "Iteration 44, loss = 0.24526422\n",
      "Iteration 200, loss = 0.04379559\n",
      "Iteration 45, loss = 0.24192582\n",
      "Iteration 46, loss = 0.23782311\n",
      "Iteration 47, loss = 0.23465738\n",
      "Iteration 48, loss = 0.23077493\n",
      "Iteration 49, loss = 0.22707304\n",
      "Iteration 50, loss = 0.22379711\n",
      "Iteration 51, loss = 0.22127412\n",
      "Iteration 52, loss = 0.21827577\n",
      "Iteration 53, loss = 0.21439420\n",
      "Iteration 54, loss = 0.21226664\n",
      "Iteration 55, loss = 0.20914060\n",
      "Iteration 56, loss = 0.20697353\n",
      "Iteration 57, loss = 0.20316009\n",
      "Iteration 58, loss = 0.20087871\n",
      "Iteration 59, loss = 0.19822137\n",
      "Iteration 60, loss = 0.19739888\n",
      "Iteration 61, loss = 0.19295100\n",
      "Iteration 62, loss = 0.19154496\n",
      "Iteration 1, loss = 1.05953840\n",
      "Iteration 63, loss = 0.18820332\n",
      "Iteration 2, loss = 0.93559587\n",
      "Iteration 64, loss = 0.18575408\n",
      "Iteration 3, loss = 0.83581172\n",
      "Iteration 65, loss = 0.18350623\n",
      "Iteration 4, loss = 0.75514002\n",
      "Iteration 66, loss = 0.18161250\n",
      "Iteration 5, loss = 0.69004915\n",
      "Iteration 67, loss = 0.17921172\n",
      "Iteration 6, loss = 0.63438225\n",
      "Iteration 68, loss = 0.17679803\n",
      "Iteration 7, loss = 0.59135281\n",
      "Iteration 69, loss = 0.17511463\n",
      "Iteration 8, loss = 0.55287039\n",
      "Iteration 70, loss = 0.17341232\n",
      "Iteration 9, loss = 0.52136680\n",
      "Iteration 71, loss = 0.17168201\n",
      "Iteration 10, loss = 0.49589372\n",
      "Iteration 72, loss = 0.16939714\n",
      "Iteration 11, loss = 0.47479509\n",
      "Iteration 73, loss = 0.16779358\n",
      "Iteration 12, loss = 0.45496203\n",
      "Iteration 74, loss = 0.16631586\n",
      "Iteration 13, loss = 0.43963297\n",
      "Iteration 75, loss = 0.16403797\n",
      "Iteration 14, loss = 0.42490932\n",
      "Iteration 76, loss = 0.16550599\n",
      "Iteration 15, loss = 0.41189870\n",
      "Iteration 77, loss = 0.16220462\n",
      "Iteration 16, loss = 0.40174380\n",
      "Iteration 78, loss = 0.15971823\n",
      "Iteration 17, loss = 0.39063784\n",
      "Iteration 79, loss = 0.15829573\n",
      "Iteration 18, loss = 0.38215838\n",
      "Iteration 80, loss = 0.15844511\n",
      "Iteration 19, loss = 0.37439656\n",
      "Iteration 81, loss = 0.15662826\n",
      "Iteration 20, loss = 0.36637595\n",
      "Iteration 82, loss = 0.15331653\n",
      "Iteration 21, loss = 0.35952876\n",
      "Iteration 83, loss = 0.15164127\n",
      "Iteration 22, loss = 0.35116993\n",
      "Iteration 84, loss = 0.15036079\n",
      "Iteration 23, loss = 0.34583643\n",
      "Iteration 85, loss = 0.14848053\n",
      "Iteration 24, loss = 0.33859921\n",
      "Iteration 86, loss = 0.14747878\n",
      "Iteration 25, loss = 0.33377305\n",
      "Iteration 87, loss = 0.14558368\n",
      "Iteration 26, loss = 0.32682028\n",
      "Iteration 88, loss = 0.14512728\n",
      "Iteration 27, loss = 0.32249331\n",
      "Iteration 89, loss = 0.14265344\n",
      "Iteration 28, loss = 0.31690957\n",
      "Iteration 90, loss = 0.14187464\n",
      "Iteration 29, loss = 0.31146121\n",
      "Iteration 91, loss = 0.13994835\n",
      "Iteration 30, loss = 0.30767950\n",
      "Iteration 92, loss = 0.13880887\n",
      "Iteration 31, loss = 0.30315135\n",
      "Iteration 93, loss = 0.13820887\n",
      "Iteration 32, loss = 0.29679521\n",
      "Iteration 94, loss = 0.13886341\n",
      "Iteration 33, loss = 0.29224020\n",
      "Iteration 95, loss = 0.13589201\n",
      "Iteration 34, loss = 0.28719859\n",
      "Iteration 96, loss = 0.13492689\n",
      "Iteration 35, loss = 0.28312616\n",
      "Iteration 97, loss = 0.13340727\n",
      "Iteration 36, loss = 0.27893531\n",
      "Iteration 98, loss = 0.13160787\n",
      "Iteration 37, loss = 0.27403583\n",
      "Iteration 99, loss = 0.13175732\n",
      "Iteration 38, loss = 0.27039524\n",
      "Iteration 100, loss = 0.13095339\n",
      "Iteration 39, loss = 0.26593360\n",
      "Iteration 40, loss = 0.26260909\n",
      "Iteration 101, loss = 0.12958869\n",
      "Iteration 41, loss = 0.25816670\n",
      "Iteration 102, loss = 0.12811405\n",
      "Iteration 42, loss = 0.25438050\n",
      "Iteration 103, loss = 0.12867415\n",
      "Iteration 43, loss = 0.25235679\n",
      "Iteration 104, loss = 0.12722694\n",
      "Iteration 105, loss = 0.12482214\n",
      "Iteration 44, loss = 0.24800188\n",
      "Iteration 45, loss = 0.24414502\n",
      "Iteration 106, loss = 0.12466438\n",
      "Iteration 46, loss = 0.24010702\n",
      "Iteration 107, loss = 0.12352340\n",
      "Iteration 47, loss = 0.23687414\n",
      "Iteration 108, loss = 0.12214012\n",
      "Iteration 48, loss = 0.23396632\n",
      "Iteration 109, loss = 0.12195420\n",
      "Iteration 49, loss = 0.23113366\n",
      "Iteration 110, loss = 0.11937842\n",
      "Iteration 50, loss = 0.22706343\n",
      "Iteration 111, loss = 0.12162151\n",
      "Iteration 51, loss = 0.22437832\n",
      "Iteration 112, loss = 0.11945901\n",
      "Iteration 52, loss = 0.22083535\n",
      "Iteration 113, loss = 0.11875653\n",
      "Iteration 53, loss = 0.21796898\n",
      "Iteration 114, loss = 0.11801923\n",
      "Iteration 115, loss = 0.11638281\n",
      "Iteration 54, loss = 0.21503138\n",
      "Iteration 116, loss = 0.11503982\n",
      "Iteration 55, loss = 0.21356289\n",
      "Iteration 117, loss = 0.11441738\n",
      "Iteration 56, loss = 0.20967968\n",
      "Iteration 118, loss = 0.11326310\n",
      "Iteration 57, loss = 0.20731254\n",
      "Iteration 119, loss = 0.11285331\n",
      "Iteration 58, loss = 0.20359652\n",
      "Iteration 120, loss = 0.11227697\n",
      "Iteration 59, loss = 0.20203957\n",
      "Iteration 121, loss = 0.11105399\n",
      "Iteration 60, loss = 0.19836282\n",
      "Iteration 122, loss = 0.10984328\n",
      "Iteration 61, loss = 0.19643897\n",
      "Iteration 123, loss = 0.10917546\n",
      "Iteration 62, loss = 0.19391875\n",
      "Iteration 124, loss = 0.10825974\n",
      "Iteration 63, loss = 0.19083868\n",
      "Iteration 125, loss = 0.10832711\n",
      "Iteration 64, loss = 0.18942539\n",
      "Iteration 126, loss = 0.10788735\n",
      "Iteration 65, loss = 0.18858132\n",
      "Iteration 127, loss = 0.10625421\n",
      "Iteration 66, loss = 0.18483297\n",
      "Iteration 128, loss = 0.10835074\n",
      "Iteration 67, loss = 0.18186902\n",
      "Iteration 129, loss = 0.10608459\n",
      "Iteration 68, loss = 0.18030672\n",
      "Iteration 130, loss = 0.10352872\n",
      "Iteration 69, loss = 0.17740368\n",
      "Iteration 131, loss = 0.10515512\n",
      "Iteration 70, loss = 0.17525574\n",
      "Iteration 132, loss = 0.10576897\n",
      "Iteration 71, loss = 0.17322510\n",
      "Iteration 133, loss = 0.10158244\n",
      "Iteration 72, loss = 0.17094415\n",
      "Iteration 134, loss = 0.10433750\n",
      "Iteration 73, loss = 0.16900247\n",
      "Iteration 135, loss = 0.10262000\n",
      "Iteration 74, loss = 0.16786768\n",
      "Iteration 136, loss = 0.10094399\n",
      "Iteration 75, loss = 0.16495755\n",
      "Iteration 137, loss = 0.10144062\n",
      "Iteration 76, loss = 0.16345611\n",
      "Iteration 138, loss = 0.09873319\n",
      "Iteration 77, loss = 0.16083526\n",
      "Iteration 139, loss = 0.09819316\n",
      "Iteration 78, loss = 0.15910624\n",
      "Iteration 140, loss = 0.09781565\n",
      "Iteration 79, loss = 0.15744683\n",
      "Iteration 80, loss = 0.15522762\n",
      "Iteration 141, loss = 0.09658856\n",
      "Iteration 142, loss = 0.09580736Iteration 81, loss = 0.15415818\n",
      "\n",
      "Iteration 82, loss = 0.15187710\n",
      "Iteration 143, loss = 0.09577572\n",
      "Iteration 144, loss = 0.09424705\n",
      "Iteration 83, loss = 0.15046799\n",
      "Iteration 84, loss = 0.14955621\n",
      "Iteration 145, loss = 0.09440528\n",
      "Iteration 146, loss = 0.09316428\n",
      "Iteration 85, loss = 0.14739269\n",
      "Iteration 147, loss = 0.09241662\n",
      "Iteration 86, loss = 0.14512362\n",
      "Iteration 148, loss = 0.09224969\n",
      "Iteration 87, loss = 0.14443020\n",
      "Iteration 149, loss = 0.09120674Iteration 88, loss = 0.14232030\n",
      "\n",
      "Iteration 150, loss = 0.09083754\n",
      "Iteration 89, loss = 0.14045492\n",
      "Iteration 151, loss = 0.09058992\n",
      "Iteration 90, loss = 0.13936482\n",
      "Iteration 152, loss = 0.08956689\n",
      "Iteration 91, loss = 0.13735008\n",
      "Iteration 153, loss = 0.08899574\n",
      "Iteration 92, loss = 0.13590990\n",
      "Iteration 154, loss = 0.08884889\n",
      "Iteration 93, loss = 0.13569130\n",
      "Iteration 155, loss = 0.09012741\n",
      "Iteration 94, loss = 0.13329714\n",
      "Iteration 156, loss = 0.08816693\n",
      "Iteration 95, loss = 0.13373046\n",
      "Iteration 157, loss = 0.08866207\n",
      "Iteration 96, loss = 0.12976682\n",
      "Iteration 158, loss = 0.08925114\n",
      "Iteration 97, loss = 0.13001403\n",
      "Iteration 159, loss = 0.08467075\n",
      "Iteration 98, loss = 0.12855532\n",
      "Iteration 160, loss = 0.08510702\n",
      "Iteration 99, loss = 0.12620110\n",
      "Iteration 161, loss = 0.08635236\n",
      "Iteration 100, loss = 0.12712093\n",
      "Iteration 162, loss = 0.08526187\n",
      "Iteration 101, loss = 0.12464835\n",
      "Iteration 163, loss = 0.08361674\n",
      "Iteration 102, loss = 0.12293717\n",
      "Iteration 164, loss = 0.08242498\n",
      "Iteration 103, loss = 0.12269526\n",
      "Iteration 165, loss = 0.08079009\n",
      "Iteration 104, loss = 0.12158847\n",
      "Iteration 166, loss = 0.08178047\n",
      "Iteration 105, loss = 0.11989158\n",
      "Iteration 167, loss = 0.08017157\n",
      "Iteration 106, loss = 0.11933170\n",
      "Iteration 168, loss = 0.07981646\n",
      "Iteration 107, loss = 0.11829377\n",
      "Iteration 169, loss = 0.07865247\n",
      "Iteration 108, loss = 0.12227725\n",
      "Iteration 170, loss = 0.07800374\n",
      "Iteration 109, loss = 0.11882521\n",
      "Iteration 171, loss = 0.08058850\n",
      "Iteration 110, loss = 0.11563606\n",
      "Iteration 172, loss = 0.07790233\n",
      "Iteration 111, loss = 0.11855200\n",
      "Iteration 173, loss = 0.07730360\n",
      "Iteration 112, loss = 0.11748485\n",
      "Iteration 174, loss = 0.07753441\n",
      "Iteration 113, loss = 0.11112530\n",
      "Iteration 175, loss = 0.07599973\n",
      "Iteration 114, loss = 0.11293159\n",
      "Iteration 176, loss = 0.07506355\n",
      "Iteration 115, loss = 0.11299157\n",
      "Iteration 177, loss = 0.07592250\n",
      "Iteration 116, loss = 0.11279920\n",
      "Iteration 178, loss = 0.07418796\n",
      "Iteration 117, loss = 0.10932337\n",
      "Iteration 179, loss = 0.07520424\n",
      "Iteration 118, loss = 0.10800850\n",
      "Iteration 180, loss = 0.07289791\n",
      "Iteration 119, loss = 0.10598751\n",
      "Iteration 181, loss = 0.07340515\n",
      "Iteration 120, loss = 0.10493675\n",
      "Iteration 182, loss = 0.07315652\n",
      "Iteration 121, loss = 0.10458284\n",
      "Iteration 183, loss = 0.07189567\n",
      "Iteration 122, loss = 0.10285735\n",
      "Iteration 184, loss = 0.07081030\n",
      "Iteration 123, loss = 0.10202533\n",
      "Iteration 185, loss = 0.07069003\n",
      "Iteration 124, loss = 0.10057002\n",
      "Iteration 186, loss = 0.07075643\n",
      "Iteration 125, loss = 0.10041713\n",
      "Iteration 187, loss = 0.06911513\n",
      "Iteration 126, loss = 0.09938705\n",
      "Iteration 188, loss = 0.06878978\n",
      "Iteration 127, loss = 0.09843406\n",
      "Iteration 189, loss = 0.06794074\n",
      "Iteration 128, loss = 0.09780475\n",
      "Iteration 190, loss = 0.06854527\n",
      "Iteration 129, loss = 0.09699744\n",
      "Iteration 191, loss = 0.07175388\n",
      "Iteration 130, loss = 0.09568353\n",
      "Iteration 192, loss = 0.06974378\n",
      "Iteration 131, loss = 0.09451053\n",
      "Iteration 193, loss = 0.06940810\n",
      "Iteration 132, loss = 0.09360754\n",
      "Iteration 194, loss = 0.06909944\n",
      "Iteration 133, loss = 0.09307392\n",
      "Iteration 195, loss = 0.06710469\n",
      "Iteration 134, loss = 0.09220841\n",
      "Iteration 196, loss = 0.06670286\n",
      "Iteration 135, loss = 0.09156080\n",
      "Iteration 197, loss = 0.06633466\n",
      "Iteration 136, loss = 0.09162538\n",
      "Iteration 198, loss = 0.06325864\n",
      "Iteration 137, loss = 0.09100038\n",
      "Iteration 199, loss = 0.06619498\n",
      "Iteration 138, loss = 0.08956062\n",
      "Iteration 200, loss = 0.06317185\n",
      "Iteration 139, loss = 0.08979699\n",
      "Iteration 140, loss = 0.08778104\n",
      "Iteration 141, loss = 0.08949477\n",
      "Iteration 142, loss = 0.08639474\n",
      "Iteration 143, loss = 0.08700656\n",
      "Iteration 144, loss = 0.08479818\n",
      "Iteration 145, loss = 0.08538673\n",
      "Iteration 146, loss = 0.08391458\n",
      "Iteration 147, loss = 0.08389134\n",
      "Iteration 148, loss = 0.08469845\n",
      "Iteration 149, loss = 0.08199159\n",
      "Iteration 150, loss = 0.08247497\n",
      "Iteration 151, loss = 0.07996298\n",
      "Iteration 152, loss = 0.08000313\n",
      "Iteration 153, loss = 0.08063846\n",
      "Iteration 154, loss = 0.07813841\n",
      "Iteration 155, loss = 0.07978412\n",
      "Iteration 156, loss = 0.07942766\n",
      "Iteration 1, loss = 1.48346825\n",
      "Iteration 157, loss = 0.07631971\n",
      "Iteration 2, loss = 1.22504137\n",
      "Iteration 158, loss = 0.07698482\n",
      "Iteration 3, loss = 1.11449847\n",
      "Iteration 159, loss = 0.07736048\n",
      "Iteration 4, loss = 1.01520008\n",
      "Iteration 160, loss = 0.07423106\n",
      "Iteration 5, loss = 0.93907997\n",
      "Iteration 161, loss = 0.07504251\n",
      "Iteration 6, loss = 0.87122315\n",
      "Iteration 162, loss = 0.07929078\n",
      "Iteration 7, loss = 0.80429598\n",
      "Iteration 163, loss = 0.07948077\n",
      "Iteration 8, loss = 0.74581193\n",
      "Iteration 164, loss = 0.07263752\n",
      "Iteration 9, loss = 0.69513325\n",
      "Iteration 165, loss = 0.07308071\n",
      "Iteration 10, loss = 0.64953502\n",
      "Iteration 11, loss = 0.60770369\n",
      "Iteration 166, loss = 0.07823421\n",
      "Iteration 167, loss = 0.07323004\n",
      "Iteration 12, loss = 0.57276813\n",
      "Iteration 168, loss = 0.07015001\n",
      "Iteration 13, loss = 0.54291440\n",
      "Iteration 14, loss = 0.51565671\n",
      "Iteration 169, loss = 0.07195554\n",
      "Iteration 170, loss = 0.07078507\n",
      "Iteration 15, loss = 0.49264009\n",
      "Iteration 16, loss = 0.47387684\n",
      "Iteration 171, loss = 0.06842442\n",
      "Iteration 17, loss = 0.45604227\n",
      "Iteration 172, loss = 0.06753499\n",
      "Iteration 18, loss = 0.44107580\n",
      "Iteration 173, loss = 0.06700380\n",
      "Iteration 19, loss = 0.42834057\n",
      "Iteration 174, loss = 0.06703359\n",
      "Iteration 20, loss = 0.41612476\n",
      "Iteration 175, loss = 0.06949769\n",
      "Iteration 21, loss = 0.40525644\n",
      "Iteration 176, loss = 0.06884357\n",
      "Iteration 22, loss = 0.39579579\n",
      "Iteration 177, loss = 0.06608284\n",
      "Iteration 23, loss = 0.38636152\n",
      "Iteration 178, loss = 0.06944055\n",
      "Iteration 24, loss = 0.37806983\n",
      "Iteration 179, loss = 0.06531611\n",
      "Iteration 25, loss = 0.37051524\n",
      "Iteration 180, loss = 0.06347601\n",
      "Iteration 26, loss = 0.36300479\n",
      "Iteration 181, loss = 0.06351720\n",
      "Iteration 27, loss = 0.35651895\n",
      "Iteration 182, loss = 0.06220691\n",
      "Iteration 28, loss = 0.35036348\n",
      "Iteration 183, loss = 0.06123009\n",
      "Iteration 29, loss = 0.34371863\n",
      "Iteration 184, loss = 0.06156587\n",
      "Iteration 30, loss = 0.33796139\n",
      "Iteration 185, loss = 0.06040138\n",
      "Iteration 31, loss = 0.33246384\n",
      "Iteration 186, loss = 0.06014663\n",
      "Iteration 32, loss = 0.32713812\n",
      "Iteration 187, loss = 0.06093079\n",
      "Iteration 33, loss = 0.32226647\n",
      "Iteration 188, loss = 0.05938340\n",
      "Iteration 34, loss = 0.31725119\n",
      "Iteration 189, loss = 0.05816015\n",
      "Iteration 35, loss = 0.31244692\n",
      "Iteration 190, loss = 0.06012084\n",
      "Iteration 36, loss = 0.30827179\n",
      "Iteration 191, loss = 0.05956620\n",
      "Iteration 37, loss = 0.30316263\n",
      "Iteration 192, loss = 0.05909163\n",
      "Iteration 38, loss = 0.29883722\n",
      "Iteration 193, loss = 0.05724051\n",
      "Iteration 39, loss = 0.29440963\n",
      "Iteration 194, loss = 0.05772411\n",
      "Iteration 40, loss = 0.29035709\n",
      "Iteration 195, loss = 0.05835553\n",
      "Iteration 41, loss = 0.28735292\n",
      "Iteration 196, loss = 0.05802353\n",
      "Iteration 42, loss = 0.28286026\n",
      "Iteration 197, loss = 0.05563615\n",
      "Iteration 43, loss = 0.27896139\n",
      "Iteration 198, loss = 0.05614767\n",
      "Iteration 44, loss = 0.27580760\n",
      "Iteration 199, loss = 0.05430485\n",
      "Iteration 45, loss = 0.27260499\n",
      "Iteration 200, loss = 0.05421332\n",
      "Iteration 46, loss = 0.26841389\n",
      "Iteration 47, loss = 0.26549251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.26161837\n",
      "Iteration 49, loss = 0.25958673\n",
      "Iteration 50, loss = 0.25659373\n",
      "Iteration 51, loss = 0.25255776\n",
      "Iteration 52, loss = 0.25045955\n",
      "Iteration 53, loss = 0.24799088\n",
      "Iteration 54, loss = 0.24477720\n",
      "Iteration 55, loss = 0.24210650\n",
      "Iteration 56, loss = 0.23833231\n",
      "Iteration 57, loss = 0.23716603\n",
      "Iteration 58, loss = 0.23349312\n",
      "Iteration 59, loss = 0.23085936\n",
      "Iteration 60, loss = 0.22794097\n",
      "Iteration 61, loss = 0.22508439\n",
      "Iteration 62, loss = 0.22288459\n",
      "Iteration 63, loss = 0.22068677\n",
      "Iteration 64, loss = 0.21779494\n",
      "Iteration 1, loss = 1.12034401\n",
      "Iteration 65, loss = 0.21662157\n",
      "Iteration 2, loss = 0.93650887\n",
      "Iteration 66, loss = 0.21292795\n",
      "Iteration 3, loss = 0.82811318\n",
      "Iteration 67, loss = 0.21104494\n",
      "Iteration 4, loss = 0.74276367\n",
      "Iteration 68, loss = 0.20985812\n",
      "Iteration 5, loss = 0.67551809\n",
      "Iteration 69, loss = 0.20608320\n",
      "Iteration 6, loss = 0.61997179\n",
      "Iteration 70, loss = 0.20413829\n",
      "Iteration 7, loss = 0.57769048\n",
      "Iteration 71, loss = 0.20298848\n",
      "Iteration 8, loss = 0.54446804\n",
      "Iteration 72, loss = 0.19955450\n",
      "Iteration 9, loss = 0.51402063\n",
      "Iteration 73, loss = 0.19837692\n",
      "Iteration 10, loss = 0.48994616\n",
      "Iteration 74, loss = 0.19691255\n",
      "Iteration 11, loss = 0.47035344\n",
      "Iteration 12, loss = 0.45258344\n",
      "Iteration 75, loss = 0.19421809\n",
      "Iteration 13, loss = 0.43693350\n",
      "Iteration 76, loss = 0.19156899\n",
      "Iteration 14, loss = 0.42395015\n",
      "Iteration 77, loss = 0.18919121\n",
      "Iteration 15, loss = 0.41192847\n",
      "Iteration 78, loss = 0.18803874\n",
      "Iteration 79, loss = 0.18578097Iteration 16, loss = 0.39962814\n",
      "\n",
      "Iteration 80, loss = 0.18324165\n",
      "Iteration 17, loss = 0.39090598\n",
      "Iteration 81, loss = 0.18126430\n",
      "Iteration 18, loss = 0.38069948\n",
      "Iteration 82, loss = 0.17946876\n",
      "Iteration 19, loss = 0.37134216\n",
      "Iteration 83, loss = 0.17795064\n",
      "Iteration 20, loss = 0.36247756\n",
      "Iteration 21, loss = 0.35474143\n",
      "Iteration 84, loss = 0.17611150\n",
      "Iteration 22, loss = 0.34705249\n",
      "Iteration 85, loss = 0.17456080\n",
      "Iteration 23, loss = 0.33995020\n",
      "Iteration 86, loss = 0.17268101\n",
      "Iteration 24, loss = 0.33351782\n",
      "Iteration 87, loss = 0.17090758\n",
      "Iteration 25, loss = 0.32678274\n",
      "Iteration 88, loss = 0.16927027\n",
      "Iteration 26, loss = 0.32068119\n",
      "Iteration 89, loss = 0.16727065\n",
      "Iteration 27, loss = 0.31483877\n",
      "Iteration 90, loss = 0.16655112\n",
      "Iteration 28, loss = 0.30874919\n",
      "Iteration 91, loss = 0.16418902\n",
      "Iteration 29, loss = 0.30369023\n",
      "Iteration 92, loss = 0.16267093\n",
      "Iteration 30, loss = 0.29797381\n",
      "Iteration 93, loss = 0.16034664\n",
      "Iteration 31, loss = 0.29350129\n",
      "Iteration 94, loss = 0.15904887\n",
      "Iteration 32, loss = 0.28886384\n",
      "Iteration 95, loss = 0.15744675\n",
      "Iteration 33, loss = 0.28477674\n",
      "Iteration 96, loss = 0.15544226\n",
      "Iteration 34, loss = 0.27833644\n",
      "Iteration 97, loss = 0.15508423\n",
      "Iteration 35, loss = 0.27438767\n",
      "Iteration 98, loss = 0.15289698\n",
      "Iteration 36, loss = 0.26894193\n",
      "Iteration 99, loss = 0.15192200\n",
      "Iteration 37, loss = 0.26575525\n",
      "Iteration 100, loss = 0.15044215\n",
      "Iteration 38, loss = 0.26213257\n",
      "Iteration 101, loss = 0.14926132\n",
      "Iteration 39, loss = 0.25731408\n",
      "Iteration 102, loss = 0.14724797\n",
      "Iteration 40, loss = 0.25216759\n",
      "Iteration 103, loss = 0.14661750\n",
      "Iteration 41, loss = 0.25053411\n",
      "Iteration 104, loss = 0.14459474\n",
      "Iteration 42, loss = 0.24654270\n",
      "Iteration 105, loss = 0.14359699\n",
      "Iteration 43, loss = 0.24302386\n",
      "Iteration 106, loss = 0.14234558\n",
      "Iteration 44, loss = 0.23876461\n",
      "Iteration 107, loss = 0.14033024\n",
      "Iteration 108, loss = 0.14019455\n",
      "Iteration 45, loss = 0.23555370\n",
      "Iteration 46, loss = 0.23438994\n",
      "Iteration 109, loss = 0.13751441\n",
      "Iteration 47, loss = 0.22841230\n",
      "Iteration 110, loss = 0.13731950\n",
      "Iteration 48, loss = 0.22645706\n",
      "Iteration 111, loss = 0.13570455\n",
      "Iteration 49, loss = 0.22598972\n",
      "Iteration 112, loss = 0.13499772\n",
      "Iteration 50, loss = 0.21870905\n",
      "Iteration 113, loss = 0.13505682\n",
      "Iteration 51, loss = 0.21963065\n",
      "Iteration 114, loss = 0.13232251\n",
      "Iteration 52, loss = 0.21685176\n",
      "Iteration 115, loss = 0.13246015\n",
      "Iteration 53, loss = 0.21361877\n",
      "Iteration 116, loss = 0.13094022\n",
      "Iteration 54, loss = 0.21046250\n",
      "Iteration 117, loss = 0.12960404\n",
      "Iteration 55, loss = 0.20652993\n",
      "Iteration 118, loss = 0.12861812\n",
      "Iteration 56, loss = 0.20475976\n",
      "Iteration 119, loss = 0.12658614\n",
      "Iteration 57, loss = 0.20377235\n",
      "Iteration 120, loss = 0.12664347\n",
      "Iteration 58, loss = 0.19979943\n",
      "Iteration 121, loss = 0.12480731\n",
      "Iteration 59, loss = 0.19823013\n",
      "Iteration 122, loss = 0.12371468\n",
      "Iteration 60, loss = 0.19560267\n",
      "Iteration 123, loss = 0.12387741\n",
      "Iteration 61, loss = 0.19146422\n",
      "Iteration 124, loss = 0.12115937\n",
      "Iteration 62, loss = 0.18977896\n",
      "Iteration 125, loss = 0.12100448\n",
      "Iteration 63, loss = 0.18694988\n",
      "Iteration 126, loss = 0.12056845\n",
      "Iteration 64, loss = 0.18661763\n",
      "Iteration 127, loss = 0.11908633\n",
      "Iteration 65, loss = 0.18294617\n",
      "Iteration 128, loss = 0.12071168\n",
      "Iteration 66, loss = 0.18117323\n",
      "Iteration 129, loss = 0.11760611\n",
      "Iteration 67, loss = 0.17899171\n",
      "Iteration 130, loss = 0.11610035\n",
      "Iteration 68, loss = 0.17692982\n",
      "Iteration 131, loss = 0.11795455\n",
      "Iteration 69, loss = 0.17465195\n",
      "Iteration 132, loss = 0.12380119\n",
      "Iteration 70, loss = 0.17229362\n",
      "Iteration 133, loss = 0.11286111\n",
      "Iteration 71, loss = 0.17091141\n",
      "Iteration 134, loss = 0.11786132\n",
      "Iteration 72, loss = 0.16890238\n",
      "Iteration 135, loss = 0.11084847\n",
      "Iteration 73, loss = 0.16813214\n",
      "Iteration 136, loss = 0.11066540\n",
      "Iteration 74, loss = 0.16531359\n",
      "Iteration 137, loss = 0.11402092\n",
      "Iteration 75, loss = 0.16452626\n",
      "Iteration 138, loss = 0.10728043\n",
      "Iteration 76, loss = 0.16315141\n",
      "Iteration 139, loss = 0.10939965\n",
      "Iteration 77, loss = 0.16266624\n",
      "Iteration 140, loss = 0.10633858\n",
      "Iteration 78, loss = 0.16111076\n",
      "Iteration 141, loss = 0.10682918\n",
      "Iteration 142, loss = 0.10582652\n",
      "Iteration 79, loss = 0.15816885\n",
      "Iteration 143, loss = 0.10427483\n",
      "Iteration 80, loss = 0.15708554\n",
      "Iteration 144, loss = 0.10454264\n",
      "Iteration 81, loss = 0.15402900\n",
      "Iteration 145, loss = 0.10262044\n",
      "Iteration 82, loss = 0.15272657\n",
      "Iteration 146, loss = 0.10230885\n",
      "Iteration 83, loss = 0.15120995\n",
      "Iteration 147, loss = 0.10061522\n",
      "Iteration 84, loss = 0.14988730\n",
      "Iteration 148, loss = 0.10090429\n",
      "Iteration 85, loss = 0.14850211\n",
      "Iteration 149, loss = 0.10048479\n",
      "Iteration 86, loss = 0.14699434\n",
      "Iteration 150, loss = 0.09828403\n",
      "Iteration 87, loss = 0.14652110\n",
      "Iteration 151, loss = 0.09775657\n",
      "Iteration 88, loss = 0.14707411\n",
      "Iteration 152, loss = 0.09685348\n",
      "Iteration 89, loss = 0.14383897\n",
      "Iteration 153, loss = 0.09586327\n",
      "Iteration 90, loss = 0.14189011\n",
      "Iteration 154, loss = 0.09542647\n",
      "Iteration 91, loss = 0.14137058\n",
      "Iteration 155, loss = 0.09587545\n",
      "Iteration 92, loss = 0.13972946\n",
      "Iteration 156, loss = 0.09388761\n",
      "Iteration 93, loss = 0.13809894\n",
      "Iteration 157, loss = 0.09708734\n",
      "Iteration 94, loss = 0.13931399\n",
      "Iteration 158, loss = 0.09296269\n",
      "Iteration 95, loss = 0.13665869\n",
      "Iteration 159, loss = 0.09207392\n",
      "Iteration 96, loss = 0.13419080\n",
      "Iteration 160, loss = 0.09121744\n",
      "Iteration 97, loss = 0.13423003\n",
      "Iteration 161, loss = 0.09224070\n",
      "Iteration 98, loss = 0.13284547\n",
      "Iteration 162, loss = 0.09075618\n",
      "Iteration 99, loss = 0.13260411\n",
      "Iteration 163, loss = 0.08859746\n",
      "Iteration 100, loss = 0.13219446\n",
      "Iteration 164, loss = 0.08852637\n",
      "Iteration 101, loss = 0.13120000\n",
      "Iteration 165, loss = 0.08855341\n",
      "Iteration 102, loss = 0.13123111\n",
      "Iteration 166, loss = 0.08644789\n",
      "Iteration 103, loss = 0.12633421\n",
      "Iteration 167, loss = 0.08642264\n",
      "Iteration 104, loss = 0.12725458\n",
      "Iteration 168, loss = 0.08602860\n",
      "Iteration 105, loss = 0.12564588\n",
      "Iteration 169, loss = 0.08490020\n",
      "Iteration 106, loss = 0.12784401\n",
      "Iteration 170, loss = 0.08414381\n",
      "Iteration 107, loss = 0.12617159\n",
      "Iteration 171, loss = 0.08487625\n",
      "Iteration 108, loss = 0.12569479\n",
      "Iteration 172, loss = 0.08656860\n",
      "Iteration 109, loss = 0.12156468\n",
      "Iteration 173, loss = 0.08312685\n",
      "Iteration 110, loss = 0.12247907\n",
      "Iteration 174, loss = 0.08211841\n",
      "Iteration 111, loss = 0.11826138\n",
      "Iteration 175, loss = 0.08204481\n",
      "Iteration 112, loss = 0.12108378\n",
      "Iteration 176, loss = 0.08011072\n",
      "Iteration 113, loss = 0.11708071\n",
      "Iteration 177, loss = 0.08062944\n",
      "Iteration 114, loss = 0.11790819\n",
      "Iteration 178, loss = 0.08023066\n",
      "Iteration 115, loss = 0.11930182\n",
      "Iteration 179, loss = 0.08033880\n",
      "Iteration 116, loss = 0.11464374\n",
      "Iteration 180, loss = 0.07766499\n",
      "Iteration 117, loss = 0.11535724\n",
      "Iteration 181, loss = 0.07714534\n",
      "Iteration 118, loss = 0.11686968\n",
      "Iteration 182, loss = 0.07717134\n",
      "Iteration 119, loss = 0.11397557\n",
      "Iteration 183, loss = 0.07609002\n",
      "Iteration 120, loss = 0.11396121\n",
      "Iteration 184, loss = 0.07682672\n",
      "Iteration 121, loss = 0.11213025\n",
      "Iteration 185, loss = 0.07514098\n",
      "Iteration 122, loss = 0.10955125\n",
      "Iteration 186, loss = 0.07476970\n",
      "Iteration 123, loss = 0.10981658\n",
      "Iteration 187, loss = 0.07392429\n",
      "Iteration 124, loss = 0.10796265\n",
      "Iteration 188, loss = 0.07366114\n",
      "Iteration 125, loss = 0.10679271\n",
      "Iteration 189, loss = 0.07323942\n",
      "Iteration 126, loss = 0.10637769\n",
      "Iteration 190, loss = 0.07143831\n",
      "Iteration 127, loss = 0.10716308\n",
      "Iteration 191, loss = 0.07211696\n",
      "Iteration 128, loss = 0.10618048\n",
      "Iteration 192, loss = 0.07105030\n",
      "Iteration 129, loss = 0.10378822\n",
      "Iteration 193, loss = 0.07069445\n",
      "Iteration 130, loss = 0.10315660\n",
      "Iteration 194, loss = 0.06935919\n",
      "Iteration 131, loss = 0.10192044\n",
      "Iteration 195, loss = 0.06916881\n",
      "Iteration 132, loss = 0.10275117\n",
      "Iteration 196, loss = 0.06902568\n",
      "Iteration 133, loss = 0.10116701\n",
      "Iteration 197, loss = 0.06900534\n",
      "Iteration 134, loss = 0.10000854\n",
      "Iteration 198, loss = 0.06799780\n",
      "Iteration 135, loss = 0.09828511\n",
      "Iteration 199, loss = 0.06753527\n",
      "Iteration 136, loss = 0.09787796\n",
      "Iteration 200, loss = 0.06926197\n",
      "Iteration 137, loss = 0.09680322\n",
      "Iteration 201, loss = 0.06989917\n",
      "Iteration 138, loss = 0.09701307\n",
      "Iteration 202, loss = 0.06908849\n",
      "Iteration 139, loss = 0.09589464\n",
      "Iteration 203, loss = 0.06623854\n",
      "Iteration 140, loss = 0.09568711\n",
      "Iteration 204, loss = 0.06639778\n",
      "Iteration 141, loss = 0.09496462\n",
      "Iteration 205, loss = 0.06862920\n",
      "Iteration 142, loss = 0.09441714\n",
      "Iteration 206, loss = 0.06341328\n",
      "Iteration 143, loss = 0.09366849\n",
      "Iteration 207, loss = 0.06531623\n",
      "Iteration 144, loss = 0.09283913\n",
      "Iteration 208, loss = 0.06290619\n",
      "Iteration 145, loss = 0.09208334\n",
      "Iteration 209, loss = 0.06337100\n",
      "Iteration 146, loss = 0.09154146\n",
      "Iteration 210, loss = 0.06191812\n",
      "Iteration 147, loss = 0.09045566\n",
      "Iteration 211, loss = 0.06164942\n",
      "Iteration 148, loss = 0.08957736\n",
      "Iteration 212, loss = 0.06145569\n",
      "Iteration 149, loss = 0.09071409\n",
      "Iteration 213, loss = 0.06127555\n",
      "Iteration 150, loss = 0.08807707\n",
      "Iteration 214, loss = 0.06047230\n",
      "Iteration 151, loss = 0.08781826\n",
      "Iteration 215, loss = 0.06114645\n",
      "Iteration 152, loss = 0.08713789\n",
      "Iteration 216, loss = 0.05892091\n",
      "Iteration 153, loss = 0.08776663\n",
      "Iteration 217, loss = 0.05964151\n",
      "Iteration 154, loss = 0.08539434\n",
      "Iteration 218, loss = 0.05903317\n",
      "Iteration 155, loss = 0.08595297\n",
      "Iteration 219, loss = 0.05849358\n",
      "Iteration 156, loss = 0.08468024\n",
      "Iteration 220, loss = 0.05863978\n",
      "Iteration 157, loss = 0.08469972\n",
      "Iteration 221, loss = 0.05748096\n",
      "Iteration 158, loss = 0.08411994\n",
      "Iteration 222, loss = 0.05766000\n",
      "Iteration 159, loss = 0.08338912\n",
      "Iteration 223, loss = 0.05798319\n",
      "Iteration 160, loss = 0.08177569\n",
      "Iteration 224, loss = 0.05690336\n",
      "Iteration 161, loss = 0.08123465\n",
      "Iteration 225, loss = 0.05583665\n",
      "Iteration 162, loss = 0.07981139\n",
      "Iteration 226, loss = 0.05483799\n",
      "Iteration 163, loss = 0.08101232\n",
      "Iteration 227, loss = 0.05881948\n",
      "Iteration 164, loss = 0.07895456\n",
      "Iteration 228, loss = 0.05568157\n",
      "Iteration 165, loss = 0.07960875\n",
      "Iteration 229, loss = 0.05920279\n",
      "Iteration 166, loss = 0.07955088\n",
      "Iteration 230, loss = 0.05471566\n",
      "Iteration 167, loss = 0.08443229\n",
      "Iteration 231, loss = 0.05445502\n",
      "Iteration 168, loss = 0.08086770\n",
      "Iteration 232, loss = 0.05409483\n",
      "Iteration 169, loss = 0.07608900\n",
      "Iteration 233, loss = 0.05406114\n",
      "Iteration 170, loss = 0.07815221\n",
      "Iteration 234, loss = 0.05282294\n",
      "Iteration 171, loss = 0.07700332\n",
      "Iteration 235, loss = 0.05275078\n",
      "Iteration 172, loss = 0.07733199\n",
      "Iteration 236, loss = 0.05161020\n",
      "Iteration 173, loss = 0.07567109\n",
      "Iteration 237, loss = 0.05190556\n",
      "Iteration 174, loss = 0.07366206\n",
      "Iteration 238, loss = 0.05119445\n",
      "Iteration 175, loss = 0.07289795\n",
      "Iteration 239, loss = 0.05102722\n",
      "Iteration 176, loss = 0.07532928\n",
      "Iteration 240, loss = 0.05085725\n",
      "Iteration 177, loss = 0.07167417\n",
      "Iteration 241, loss = 0.05073532\n",
      "Iteration 178, loss = 0.07124477\n",
      "Iteration 242, loss = 0.05017672\n",
      "Iteration 179, loss = 0.07237756\n",
      "Iteration 243, loss = 0.05041611\n",
      "Iteration 180, loss = 0.07037669\n",
      "Iteration 244, loss = 0.04929739\n",
      "Iteration 181, loss = 0.07068536\n",
      "Iteration 245, loss = 0.04919105\n",
      "Iteration 182, loss = 0.07155348\n",
      "Iteration 246, loss = 0.04821190\n",
      "Iteration 183, loss = 0.06972831\n",
      "Iteration 247, loss = 0.04966184\n",
      "Iteration 184, loss = 0.06878515\n",
      "Iteration 248, loss = 0.05059036\n",
      "Iteration 185, loss = 0.07009534\n",
      "Iteration 249, loss = 0.05205989\n",
      "Iteration 186, loss = 0.06919992\n",
      "Iteration 250, loss = 0.04842788\n",
      "Iteration 187, loss = 0.06955020\n",
      "Iteration 251, loss = 0.04911069\n",
      "Iteration 188, loss = 0.06641480\n",
      "Iteration 252, loss = 0.04760194\n",
      "Iteration 189, loss = 0.06643391\n",
      "Iteration 253, loss = 0.04739773\n",
      "Iteration 190, loss = 0.06673525\n",
      "Iteration 254, loss = 0.04670413\n",
      "Iteration 191, loss = 0.06672665\n",
      "Iteration 255, loss = 0.04607849\n",
      "Iteration 192, loss = 0.06731841\n",
      "Iteration 256, loss = 0.04962102\n",
      "Iteration 193, loss = 0.06402175\n",
      "Iteration 257, loss = 0.04568375\n",
      "Iteration 194, loss = 0.06385742\n",
      "Iteration 258, loss = 0.04581938\n",
      "Iteration 195, loss = 0.06504361\n",
      "Iteration 259, loss = 0.04659029\n",
      "Iteration 196, loss = 0.06486129\n",
      "Iteration 260, loss = 0.04507949\n",
      "Iteration 197, loss = 0.06624223\n",
      "Iteration 261, loss = 0.04762363\n",
      "Iteration 198, loss = 0.06356440\n",
      "Iteration 262, loss = 0.04365292\n",
      "Iteration 199, loss = 0.06295242\n",
      "Iteration 263, loss = 0.04401796\n",
      "Iteration 200, loss = 0.06321777\n",
      "Iteration 264, loss = 0.04491244\n",
      "Iteration 201, loss = 0.06240928\n",
      "Iteration 265, loss = 0.04343043\n",
      "Iteration 202, loss = 0.06127776\n",
      "Iteration 266, loss = 0.04260511\n",
      "Iteration 203, loss = 0.06080749\n",
      "Iteration 267, loss = 0.04305623\n",
      "Iteration 204, loss = 0.05945615\n",
      "Iteration 268, loss = 0.04375713\n",
      "Iteration 205, loss = 0.05915501\n",
      "Iteration 269, loss = 0.04330550\n",
      "Iteration 206, loss = 0.05910990\n",
      "Iteration 270, loss = 0.04214866\n",
      "Iteration 207, loss = 0.05998860\n",
      "Iteration 271, loss = 0.04123036\n",
      "Iteration 208, loss = 0.05832202\n",
      "Iteration 272, loss = 0.04166719\n",
      "Iteration 209, loss = 0.05784464\n",
      "Iteration 273, loss = 0.04128971\n",
      "Iteration 210, loss = 0.05733117\n",
      "Iteration 274, loss = 0.04075942\n",
      "Iteration 211, loss = 0.05760749\n",
      "Iteration 275, loss = 0.04055408\n",
      "Iteration 212, loss = 0.05649232\n",
      "Iteration 276, loss = 0.04018721\n",
      "Iteration 213, loss = 0.05818002\n",
      "Iteration 277, loss = 0.04008109\n",
      "Iteration 214, loss = 0.05816982\n",
      "Iteration 278, loss = 0.03968406\n",
      "Iteration 215, loss = 0.05727797\n",
      "Iteration 279, loss = 0.03962585\n",
      "Iteration 216, loss = 0.05669319\n",
      "Iteration 280, loss = 0.03951951\n",
      "Iteration 217, loss = 0.05418736\n",
      "Iteration 281, loss = 0.03957141\n",
      "Iteration 218, loss = 0.05520735\n",
      "Iteration 282, loss = 0.03854026\n",
      "Iteration 219, loss = 0.05729091\n",
      "Iteration 283, loss = 0.03881140\n",
      "Iteration 220, loss = 0.05416194\n",
      "Iteration 284, loss = 0.03894263\n",
      "Iteration 221, loss = 0.05266524\n",
      "Iteration 285, loss = 0.03885478\n",
      "Iteration 222, loss = 0.05262588\n",
      "Iteration 286, loss = 0.03811987\n",
      "Iteration 223, loss = 0.05272250\n",
      "Iteration 287, loss = 0.03773231\n",
      "Iteration 224, loss = 0.05228178\n",
      "Iteration 288, loss = 0.03737511\n",
      "Iteration 225, loss = 0.05184357\n",
      "Iteration 289, loss = 0.03776148\n",
      "Iteration 226, loss = 0.05120674\n",
      "Iteration 290, loss = 0.03782145\n",
      "Iteration 227, loss = 0.05091309\n",
      "Iteration 291, loss = 0.03688970\n",
      "Iteration 228, loss = 0.05149088\n",
      "Iteration 292, loss = 0.03682870\n",
      "Iteration 229, loss = 0.05101307\n",
      "Iteration 293, loss = 0.03667869\n",
      "Iteration 230, loss = 0.05004906\n",
      "Iteration 294, loss = 0.03806955\n",
      "Iteration 295, loss = 0.04166445\n",
      "Iteration 231, loss = 0.05095227\n",
      "Iteration 296, loss = 0.03842005\n",
      "Iteration 232, loss = 0.04886410\n",
      "Iteration 233, loss = 0.04920868\n",
      "Iteration 297, loss = 0.04041846\n",
      "Iteration 234, loss = 0.05048229\n",
      "Iteration 298, loss = 0.03923397\n",
      "Iteration 299, loss = 0.03749353\n",
      "Iteration 235, loss = 0.04877159\n",
      "Iteration 300, loss = 0.03559764\n",
      "Iteration 236, loss = 0.05055051\n",
      "Iteration 301, loss = 0.03856155\n",
      "Iteration 237, loss = 0.04998679\n",
      "Iteration 302, loss = 0.03494864\n",
      "Iteration 238, loss = 0.04789647\n",
      "Iteration 303, loss = 0.03457777\n",
      "Iteration 239, loss = 0.04934626\n",
      "Iteration 304, loss = 0.03438810\n",
      "Iteration 240, loss = 0.04968297\n",
      "Iteration 241, loss = 0.04998778\n",
      "Iteration 305, loss = 0.03451957\n",
      "Iteration 242, loss = 0.05076991\n",
      "Iteration 306, loss = 0.03432632\n",
      "Iteration 243, loss = 0.05318257\n",
      "Iteration 307, loss = 0.03386538\n",
      "Iteration 244, loss = 0.04939480\n",
      "Iteration 308, loss = 0.03442046\n",
      "Iteration 245, loss = 0.04779183\n",
      "Iteration 309, loss = 0.03419830\n",
      "Iteration 246, loss = 0.04685216\n",
      "Iteration 310, loss = 0.03405019\n",
      "Iteration 247, loss = 0.04661241\n",
      "Iteration 311, loss = 0.03476312\n",
      "Iteration 248, loss = 0.04525722\n",
      "Iteration 312, loss = 0.03536433\n",
      "Iteration 249, loss = 0.04552792\n",
      "Iteration 313, loss = 0.03286254\n",
      "Iteration 250, loss = 0.04625047\n",
      "Iteration 314, loss = 0.03486556\n",
      "Iteration 251, loss = 0.04457189\n",
      "Iteration 315, loss = 0.03868876\n",
      "Iteration 252, loss = 0.04371505\n",
      "Iteration 316, loss = 0.03518067\n",
      "Iteration 253, loss = 0.04527131\n",
      "Iteration 317, loss = 0.03326630\n",
      "Iteration 254, loss = 0.04469586\n",
      "Iteration 318, loss = 0.03332652\n",
      "Iteration 255, loss = 0.04295062\n",
      "Iteration 319, loss = 0.03478907\n",
      "Iteration 256, loss = 0.04323575\n",
      "Iteration 320, loss = 0.03532554\n",
      "Iteration 257, loss = 0.04372755\n",
      "Iteration 321, loss = 0.03383498\n",
      "Iteration 258, loss = 0.04305931\n",
      "Iteration 322, loss = 0.03261877\n",
      "Iteration 323, loss = 0.03240498\n",
      "Iteration 259, loss = 0.04235965\n",
      "Iteration 324, loss = 0.03125110\n",
      "Iteration 260, loss = 0.04211588\n",
      "Iteration 325, loss = 0.03247978\n",
      "Iteration 261, loss = 0.04227671\n",
      "Iteration 262, loss = 0.04110967\n",
      "Iteration 326, loss = 0.03330507\n",
      "Iteration 263, loss = 0.04060989\n",
      "Iteration 327, loss = 0.03937673\n",
      "Iteration 264, loss = 0.04070454\n",
      "Iteration 328, loss = 0.03739847\n",
      "Iteration 329, loss = 0.03038993\n",
      "Iteration 265, loss = 0.04101943\n",
      "Iteration 330, loss = 0.03272310\n",
      "Iteration 266, loss = 0.04200832\n",
      "Iteration 331, loss = 0.03087010\n",
      "Iteration 267, loss = 0.04082480\n",
      "Iteration 332, loss = 0.03065672\n",
      "Iteration 268, loss = 0.04287304\n",
      "Iteration 333, loss = 0.03067305\n",
      "Iteration 269, loss = 0.04162354\n",
      "Iteration 334, loss = 0.02971692\n",
      "Iteration 270, loss = 0.03956995\n",
      "Iteration 335, loss = 0.03096212\n",
      "Iteration 271, loss = 0.04042132\n",
      "Iteration 336, loss = 0.03100137\n",
      "Iteration 272, loss = 0.04517155\n",
      "Iteration 337, loss = 0.02946227\n",
      "Iteration 273, loss = 0.04153124\n",
      "Iteration 338, loss = 0.02894937\n",
      "Iteration 274, loss = 0.04038982\n",
      "Iteration 339, loss = 0.03202877\n",
      "Iteration 275, loss = 0.03757358\n",
      "Iteration 340, loss = 0.03022903\n",
      "Iteration 276, loss = 0.03909352\n",
      "Iteration 341, loss = 0.02870406\n",
      "Iteration 277, loss = 0.03827558\n",
      "Iteration 342, loss = 0.02892816\n",
      "Iteration 278, loss = 0.03805724\n",
      "Iteration 343, loss = 0.02958058\n",
      "Iteration 279, loss = 0.03742502\n",
      "Iteration 344, loss = 0.03064252\n",
      "Iteration 280, loss = 0.04187178\n",
      "Iteration 345, loss = 0.03009279\n",
      "Iteration 281, loss = 0.03978739\n",
      "Iteration 346, loss = 0.02844797\n",
      "Iteration 282, loss = 0.04105773\n",
      "Iteration 347, loss = 0.02760011\n",
      "Iteration 283, loss = 0.04177453\n",
      "Iteration 348, loss = 0.02843966\n",
      "Iteration 284, loss = 0.04430311\n",
      "Iteration 349, loss = 0.02961411\n",
      "Iteration 285, loss = 0.04013005\n",
      "Iteration 350, loss = 0.03123041\n",
      "Iteration 286, loss = 0.03990631\n",
      "Iteration 351, loss = 0.02730237\n",
      "Iteration 287, loss = 0.03630722\n",
      "Iteration 352, loss = 0.02725078\n",
      "Iteration 288, loss = 0.03874257\n",
      "Iteration 353, loss = 0.02690727\n",
      "Iteration 289, loss = 0.03772254\n",
      "Iteration 354, loss = 0.02668224\n",
      "Iteration 290, loss = 0.03652819\n",
      "Iteration 355, loss = 0.02964574\n",
      "Iteration 291, loss = 0.03633051\n",
      "Iteration 356, loss = 0.02817302\n",
      "Iteration 292, loss = 0.03594169\n",
      "Iteration 357, loss = 0.02721994\n",
      "Iteration 293, loss = 0.03534188\n",
      "Iteration 358, loss = 0.02648617\n",
      "Iteration 294, loss = 0.03468016\n",
      "Iteration 359, loss = 0.02754055\n",
      "Iteration 295, loss = 0.03547540\n",
      "Iteration 360, loss = 0.02656477\n",
      "Iteration 296, loss = 0.03458802\n",
      "Iteration 361, loss = 0.02602711\n",
      "Iteration 297, loss = 0.03411673\n",
      "Iteration 362, loss = 0.02661168\n",
      "Iteration 298, loss = 0.03535748\n",
      "Iteration 363, loss = 0.02598818\n",
      "Iteration 299, loss = 0.03421308\n",
      "Iteration 364, loss = 0.02567177\n",
      "Iteration 300, loss = 0.03363737\n",
      "Iteration 365, loss = 0.02699188\n",
      "Iteration 301, loss = 0.03441868\n",
      "Iteration 366, loss = 0.02633621\n",
      "Iteration 302, loss = 0.03399920\n",
      "Iteration 367, loss = 0.02506131\n",
      "Iteration 303, loss = 0.03393658\n",
      "Iteration 368, loss = 0.02572510\n",
      "Iteration 304, loss = 0.03527800\n",
      "Iteration 369, loss = 0.02581135\n",
      "Iteration 305, loss = 0.03216548\n",
      "Iteration 370, loss = 0.02626938\n",
      "Iteration 306, loss = 0.03356101\n",
      "Iteration 371, loss = 0.02564959\n",
      "Iteration 307, loss = 0.03256127\n",
      "Iteration 372, loss = 0.02461626\n",
      "Iteration 308, loss = 0.03213756\n",
      "Iteration 373, loss = 0.02669854\n",
      "Iteration 309, loss = 0.03160296\n",
      "Iteration 374, loss = 0.02663810\n",
      "Iteration 310, loss = 0.03275973\n",
      "Iteration 375, loss = 0.02531371\n",
      "Iteration 311, loss = 0.03121209\n",
      "Iteration 376, loss = 0.02612342\n",
      "Iteration 312, loss = 0.03221534\n",
      "Iteration 377, loss = 0.02579785\n",
      "Iteration 313, loss = 0.03473811\n",
      "Iteration 378, loss = 0.02590808\n",
      "Iteration 314, loss = 0.03215789\n",
      "Iteration 379, loss = 0.02535802\n",
      "Iteration 315, loss = 0.03211957\n",
      "Iteration 380, loss = 0.02386325\n",
      "Iteration 316, loss = 0.03108683\n",
      "Iteration 381, loss = 0.02636259\n",
      "Iteration 317, loss = 0.03171460\n",
      "Iteration 382, loss = 0.02641532\n",
      "Iteration 318, loss = 0.03097182\n",
      "Iteration 383, loss = 0.02485201\n",
      "Iteration 319, loss = 0.03069030\n",
      "Iteration 384, loss = 0.02555102\n",
      "Iteration 320, loss = 0.03058286\n",
      "Iteration 385, loss = 0.02420356\n",
      "Iteration 321, loss = 0.03090771\n",
      "Iteration 386, loss = 0.02443826\n",
      "Iteration 322, loss = 0.03325939\n",
      "Iteration 387, loss = 0.02349513\n",
      "Iteration 323, loss = 0.03006809\n",
      "Iteration 388, loss = 0.02397538\n",
      "Iteration 324, loss = 0.03053761\n",
      "Iteration 389, loss = 0.02271989\n",
      "Iteration 325, loss = 0.02999912\n",
      "Iteration 390, loss = 0.02393370\n",
      "Iteration 326, loss = 0.03024309\n",
      "Iteration 391, loss = 0.02322563\n",
      "Iteration 327, loss = 0.03059102\n",
      "Iteration 392, loss = 0.02314057\n",
      "Iteration 328, loss = 0.02972665\n",
      "Iteration 393, loss = 0.02342555\n",
      "Iteration 329, loss = 0.02883075\n",
      "Iteration 394, loss = 0.02283139\n",
      "Iteration 330, loss = 0.03070938\n",
      "Iteration 395, loss = 0.02209709\n",
      "Iteration 331, loss = 0.02878106\n",
      "Iteration 396, loss = 0.02242376\n",
      "Iteration 332, loss = 0.02859853\n",
      "Iteration 397, loss = 0.02303183\n",
      "Iteration 333, loss = 0.03125479\n",
      "Iteration 398, loss = 0.02200722\n",
      "Iteration 334, loss = 0.03002779\n",
      "Iteration 399, loss = 0.02388426\n",
      "Iteration 335, loss = 0.02957205\n",
      "Iteration 400, loss = 0.02166431\n",
      "Iteration 336, loss = 0.02742968\n",
      "Iteration 401, loss = 0.02254983\n",
      "Iteration 337, loss = 0.02820809\n",
      "Iteration 402, loss = 0.02358078\n",
      "Iteration 338, loss = 0.02798421\n",
      "Iteration 403, loss = 0.02699990\n",
      "Iteration 339, loss = 0.02828701\n",
      "Iteration 404, loss = 0.02662942\n",
      "Iteration 340, loss = 0.02894809\n",
      "Iteration 405, loss = 0.02135263\n",
      "Iteration 341, loss = 0.02827990\n",
      "Iteration 406, loss = 0.02280696\n",
      "Iteration 342, loss = 0.03037376\n",
      "Iteration 407, loss = 0.02345644\n",
      "Iteration 343, loss = 0.03377550\n",
      "Iteration 408, loss = 0.02232804\n",
      "Iteration 344, loss = 0.03229493\n",
      "Iteration 409, loss = 0.02174254\n",
      "Iteration 345, loss = 0.02822872\n",
      "Iteration 410, loss = 0.02160086\n",
      "Iteration 346, loss = 0.02648643\n",
      "Iteration 411, loss = 0.02132426\n",
      "Iteration 347, loss = 0.02818544\n",
      "Iteration 412, loss = 0.02071435\n",
      "Iteration 348, loss = 0.02778974\n",
      "Iteration 413, loss = 0.02253269\n",
      "Iteration 349, loss = 0.02573653\n",
      "Iteration 414, loss = 0.02169766\n",
      "Iteration 350, loss = 0.02722444\n",
      "Iteration 415, loss = 0.02308648\n",
      "Iteration 351, loss = 0.02635298\n",
      "Iteration 416, loss = 0.02311819\n",
      "Iteration 352, loss = 0.02692604\n",
      "Iteration 417, loss = 0.02474712\n",
      "Iteration 353, loss = 0.02562493\n",
      "Iteration 418, loss = 0.02154252\n",
      "Iteration 354, loss = 0.02593955\n",
      "Iteration 419, loss = 0.02279648\n",
      "Iteration 355, loss = 0.02582838\n",
      "Iteration 420, loss = 0.02043374\n",
      "Iteration 356, loss = 0.02632341\n",
      "Iteration 421, loss = 0.02092806\n",
      "Iteration 357, loss = 0.02786941\n",
      "Iteration 422, loss = 0.02020267\n",
      "Iteration 358, loss = 0.02916041\n",
      "Iteration 423, loss = 0.02106401\n",
      "Iteration 359, loss = 0.02718224\n",
      "Iteration 424, loss = 0.02138085\n",
      "Iteration 360, loss = 0.02755981\n",
      "Iteration 425, loss = 0.02312720\n",
      "Iteration 361, loss = 0.02731868\n",
      "Iteration 426, loss = 0.02157688\n",
      "Iteration 362, loss = 0.02501031\n",
      "Iteration 427, loss = 0.02217794\n",
      "Iteration 363, loss = 0.02530854\n",
      "Iteration 428, loss = 0.01908736\n",
      "Iteration 364, loss = 0.02439729\n",
      "Iteration 429, loss = 0.01994327\n",
      "Iteration 365, loss = 0.02510125\n",
      "Iteration 430, loss = 0.01956224\n",
      "Iteration 366, loss = 0.02429752\n",
      "Iteration 431, loss = 0.01909379\n",
      "Iteration 367, loss = 0.02381223\n",
      "Iteration 432, loss = 0.01905623\n",
      "Iteration 368, loss = 0.02400860\n",
      "Iteration 433, loss = 0.01913745\n",
      "Iteration 369, loss = 0.02461195\n",
      "Iteration 434, loss = 0.01978859\n",
      "Iteration 370, loss = 0.02437058\n",
      "Iteration 435, loss = 0.01916584\n",
      "Iteration 371, loss = 0.02456346\n",
      "Iteration 436, loss = 0.01831587\n",
      "Iteration 372, loss = 0.02403178\n",
      "Iteration 437, loss = 0.01903516\n",
      "Iteration 373, loss = 0.02380167\n",
      "Iteration 438, loss = 0.01876471\n",
      "Iteration 374, loss = 0.02366071\n",
      "Iteration 439, loss = 0.01839001\n",
      "Iteration 375, loss = 0.02341787\n",
      "Iteration 440, loss = 0.01807973\n",
      "Iteration 376, loss = 0.02323540\n",
      "Iteration 441, loss = 0.01778629\n",
      "Iteration 377, loss = 0.02314744\n",
      "Iteration 442, loss = 0.01776834\n",
      "Iteration 378, loss = 0.02293456\n",
      "Iteration 443, loss = 0.01884547\n",
      "Iteration 379, loss = 0.02377763\n",
      "Iteration 444, loss = 0.01885051\n",
      "Iteration 380, loss = 0.02310338\n",
      "Iteration 445, loss = 0.01882297\n",
      "Iteration 381, loss = 0.02376828\n",
      "Iteration 446, loss = 0.01828182\n",
      "Iteration 382, loss = 0.02247401\n",
      "Iteration 447, loss = 0.01809908\n",
      "Iteration 383, loss = 0.02246818\n",
      "Iteration 448, loss = 0.01953145\n",
      "Iteration 384, loss = 0.02275376\n",
      "Iteration 449, loss = 0.01925135\n",
      "Iteration 450, loss = 0.01899907Iteration 385, loss = 0.02261422\n",
      "\n",
      "Iteration 451, loss = 0.01725573\n",
      "Iteration 386, loss = 0.02295719\n",
      "Iteration 452, loss = 0.01725896\n",
      "Iteration 387, loss = 0.02231228\n",
      "Iteration 453, loss = 0.01804939\n",
      "Iteration 388, loss = 0.02191252\n",
      "Iteration 454, loss = 0.01671214\n",
      "Iteration 389, loss = 0.02210542\n",
      "Iteration 455, loss = 0.01744814\n",
      "Iteration 390, loss = 0.02360266\n",
      "Iteration 456, loss = 0.01822623\n",
      "Iteration 391, loss = 0.02198302\n",
      "Iteration 457, loss = 0.01792332\n",
      "Iteration 392, loss = 0.02356627\n",
      "Iteration 458, loss = 0.01729869\n",
      "Iteration 393, loss = 0.02125081\n",
      "Iteration 459, loss = 0.01735797\n",
      "Iteration 394, loss = 0.02170378\n",
      "Iteration 460, loss = 0.01618399\n",
      "Iteration 395, loss = 0.02269126\n",
      "Iteration 461, loss = 0.01666068\n",
      "Iteration 396, loss = 0.02137162\n",
      "Iteration 462, loss = 0.01659511\n",
      "Iteration 397, loss = 0.02135308\n",
      "Iteration 463, loss = 0.01612695\n",
      "Iteration 398, loss = 0.02100312\n",
      "Iteration 464, loss = 0.01688081\n",
      "Iteration 399, loss = 0.02264461\n",
      "Iteration 465, loss = 0.01640372\n",
      "Iteration 400, loss = 0.02136996\n",
      "Iteration 466, loss = 0.01612095\n",
      "Iteration 401, loss = 0.02100017\n",
      "Iteration 467, loss = 0.01668438\n",
      "Iteration 402, loss = 0.02214842\n",
      "Iteration 468, loss = 0.01572693\n",
      "Iteration 403, loss = 0.02023354\n",
      "Iteration 469, loss = 0.01717029\n",
      "Iteration 404, loss = 0.02050723\n",
      "Iteration 470, loss = 0.02560042\n",
      "Iteration 405, loss = 0.02011400\n",
      "Iteration 471, loss = 0.02715664\n",
      "Iteration 406, loss = 0.02071123\n",
      "Iteration 472, loss = 0.02410599\n",
      "Iteration 407, loss = 0.02090801\n",
      "Iteration 473, loss = 0.02375114\n",
      "Iteration 408, loss = 0.02079785\n",
      "Iteration 474, loss = 0.01867472\n",
      "Iteration 409, loss = 0.02000309\n",
      "Iteration 475, loss = 0.01752914\n",
      "Iteration 410, loss = 0.02001553\n",
      "Iteration 476, loss = 0.01794835\n",
      "Iteration 411, loss = 0.01960293\n",
      "Iteration 477, loss = 0.01806258\n",
      "Iteration 412, loss = 0.01921480\n",
      "Iteration 478, loss = 0.01703389\n",
      "Iteration 413, loss = 0.02078817\n",
      "Iteration 479, loss = 0.01606804\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 414, loss = 0.01927608\n",
      "Iteration 415, loss = 0.01911809\n",
      "Iteration 416, loss = 0.01988678\n",
      "Iteration 417, loss = 0.01901373\n",
      "Iteration 418, loss = 0.01923942\n",
      "Iteration 419, loss = 0.01893481\n",
      "Iteration 420, loss = 0.01865473\n",
      "Iteration 421, loss = 0.01868304\n",
      "Iteration 422, loss = 0.01841854\n",
      "Iteration 423, loss = 0.01895165\n",
      "Iteration 424, loss = 0.01875106\n",
      "Iteration 425, loss = 0.01789011\n",
      "Iteration 426, loss = 0.01804194\n",
      "Iteration 427, loss = 0.01974761\n",
      "Iteration 428, loss = 0.01900288\n",
      "Iteration 429, loss = 0.01934875\n",
      "Iteration 430, loss = 0.02196643\n",
      "Iteration 431, loss = 0.01821264\n",
      "Iteration 1, loss = 1.08619286\n",
      "Iteration 432, loss = 0.01848902\n",
      "Iteration 2, loss = 0.92102714\n",
      "Iteration 433, loss = 0.01889301\n",
      "Iteration 3, loss = 0.81551084\n",
      "Iteration 434, loss = 0.01859298\n",
      "Iteration 4, loss = 0.73053820\n",
      "Iteration 435, loss = 0.01740227\n",
      "Iteration 5, loss = 0.66287803\n",
      "Iteration 436, loss = 0.01855934\n",
      "Iteration 6, loss = 0.60797735\n",
      "Iteration 437, loss = 0.01973738\n",
      "Iteration 7, loss = 0.56374113\n",
      "Iteration 438, loss = 0.01788717\n",
      "Iteration 8, loss = 0.52614167\n",
      "Iteration 439, loss = 0.01799045\n",
      "Iteration 9, loss = 0.49631189\n",
      "Iteration 440, loss = 0.01770241\n",
      "Iteration 10, loss = 0.46959522\n",
      "Iteration 441, loss = 0.01982238\n",
      "Iteration 11, loss = 0.45009326\n",
      "Iteration 442, loss = 0.01743155\n",
      "Iteration 12, loss = 0.42893282\n",
      "Iteration 443, loss = 0.01824035\n",
      "Iteration 13, loss = 0.41326860\n",
      "Iteration 444, loss = 0.01683711\n",
      "Iteration 14, loss = 0.39913789\n",
      "Iteration 445, loss = 0.01688635\n",
      "Iteration 15, loss = 0.38748035\n",
      "Iteration 446, loss = 0.01666763\n",
      "Iteration 16, loss = 0.37540761\n",
      "Iteration 447, loss = 0.01626027\n",
      "Iteration 17, loss = 0.36586818\n",
      "Iteration 448, loss = 0.01696834\n",
      "Iteration 18, loss = 0.35662431\n",
      "Iteration 449, loss = 0.01692550\n",
      "Iteration 19, loss = 0.34883415\n",
      "Iteration 450, loss = 0.01715413\n",
      "Iteration 20, loss = 0.34053962\n",
      "Iteration 451, loss = 0.02088563\n",
      "Iteration 21, loss = 0.33477696\n",
      "Iteration 452, loss = 0.02093386\n",
      "Iteration 22, loss = 0.32720914\n",
      "Iteration 453, loss = 0.01786978\n",
      "Iteration 23, loss = 0.32122910\n",
      "Iteration 24, loss = 0.31538402\n",
      "Iteration 454, loss = 0.01631464\n",
      "Iteration 25, loss = 0.31043180\n",
      "Iteration 455, loss = 0.01661840\n",
      "Iteration 26, loss = 0.30502942\n",
      "Iteration 456, loss = 0.01810624\n",
      "Iteration 27, loss = 0.29981538\n",
      "Iteration 457, loss = 0.01659859\n",
      "Iteration 28, loss = 0.29460689\n",
      "Iteration 458, loss = 0.01643375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.28948976\n",
      "Iteration 30, loss = 0.28665378\n",
      "Iteration 31, loss = 0.28107085\n",
      "Iteration 32, loss = 0.27684169\n",
      "Iteration 33, loss = 0.27275485\n",
      "Iteration 34, loss = 0.26960362\n",
      "Iteration 35, loss = 0.26407614\n",
      "Iteration 36, loss = 0.26207256\n",
      "Iteration 37, loss = 0.25858591\n",
      "Iteration 38, loss = 0.25384834\n",
      "Iteration 39, loss = 0.25005694\n",
      "Iteration 40, loss = 0.24683393\n",
      "Iteration 41, loss = 0.24332573\n",
      "Iteration 42, loss = 0.23990651\n",
      "Iteration 43, loss = 0.23763907\n",
      "Iteration 44, loss = 0.23412195\n",
      "Iteration 45, loss = 0.23105400\n",
      "Iteration 46, loss = 0.22770892\n",
      "Iteration 1, loss = 1.21644666\n",
      "Iteration 47, loss = 0.22639594\n",
      "Iteration 2, loss = 1.02434693\n",
      "Iteration 48, loss = 0.22184993\n",
      "Iteration 3, loss = 0.89435530\n",
      "Iteration 49, loss = 0.21951005\n",
      "Iteration 4, loss = 0.79819506\n",
      "Iteration 50, loss = 0.21675633\n",
      "Iteration 5, loss = 0.71699548\n",
      "Iteration 51, loss = 0.21421531\n",
      "Iteration 6, loss = 0.65275006\n",
      "Iteration 52, loss = 0.21151524\n",
      "Iteration 7, loss = 0.60509070\n",
      "Iteration 53, loss = 0.20844953\n",
      "Iteration 8, loss = 0.56597899\n",
      "Iteration 54, loss = 0.20585506\n",
      "Iteration 9, loss = 0.53494815\n",
      "Iteration 55, loss = 0.20308643\n",
      "Iteration 10, loss = 0.50860512\n",
      "Iteration 56, loss = 0.20130295\n",
      "Iteration 11, loss = 0.48672466\n",
      "Iteration 57, loss = 0.19962771\n",
      "Iteration 12, loss = 0.46773359\n",
      "Iteration 58, loss = 0.19757628\n",
      "Iteration 13, loss = 0.45118124\n",
      "Iteration 59, loss = 0.19360982\n",
      "Iteration 14, loss = 0.43701564\n",
      "Iteration 60, loss = 0.19176360\n",
      "Iteration 15, loss = 0.42483855\n",
      "Iteration 61, loss = 0.18910542\n",
      "Iteration 16, loss = 0.41286212\n",
      "Iteration 62, loss = 0.18714417\n",
      "Iteration 17, loss = 0.40245502\n",
      "Iteration 63, loss = 0.18581956\n",
      "Iteration 18, loss = 0.39340566\n",
      "Iteration 64, loss = 0.18545158\n",
      "Iteration 19, loss = 0.38564380\n",
      "Iteration 65, loss = 0.18122262\n",
      "Iteration 20, loss = 0.37817939\n",
      "Iteration 66, loss = 0.17921875\n",
      "Iteration 21, loss = 0.37002097\n",
      "Iteration 67, loss = 0.17678948\n",
      "Iteration 22, loss = 0.36321441\n",
      "Iteration 68, loss = 0.17383211\n",
      "Iteration 23, loss = 0.35717844\n",
      "Iteration 69, loss = 0.17265572\n",
      "Iteration 24, loss = 0.35212539\n",
      "Iteration 70, loss = 0.17078488\n",
      "Iteration 25, loss = 0.34564212\n",
      "Iteration 71, loss = 0.16858249\n",
      "Iteration 26, loss = 0.34018720\n",
      "Iteration 72, loss = 0.16737354\n",
      "Iteration 27, loss = 0.33487426\n",
      "Iteration 73, loss = 0.16555423\n",
      "Iteration 28, loss = 0.33027311\n",
      "Iteration 74, loss = 0.16264628\n",
      "Iteration 29, loss = 0.32548696\n",
      "Iteration 75, loss = 0.16152351\n",
      "Iteration 30, loss = 0.32035723\n",
      "Iteration 76, loss = 0.15919082\n",
      "Iteration 31, loss = 0.31767770\n",
      "Iteration 77, loss = 0.15753674\n",
      "Iteration 32, loss = 0.31143729\n",
      "Iteration 78, loss = 0.15611955\n",
      "Iteration 33, loss = 0.30766182\n",
      "Iteration 79, loss = 0.15442756\n",
      "Iteration 34, loss = 0.30374848\n",
      "Iteration 80, loss = 0.15314769\n",
      "Iteration 35, loss = 0.29995180\n",
      "Iteration 81, loss = 0.15434694\n",
      "Iteration 36, loss = 0.29618013\n",
      "Iteration 82, loss = 0.14989390\n",
      "Iteration 37, loss = 0.29179919\n",
      "Iteration 83, loss = 0.14944351\n",
      "Iteration 38, loss = 0.28805297\n",
      "Iteration 84, loss = 0.14614402\n",
      "Iteration 39, loss = 0.28662804\n",
      "Iteration 85, loss = 0.14519860\n",
      "Iteration 40, loss = 0.28142717\n",
      "Iteration 86, loss = 0.14316943\n",
      "Iteration 41, loss = 0.27778780\n",
      "Iteration 87, loss = 0.14240182\n",
      "Iteration 42, loss = 0.27397014\n",
      "Iteration 88, loss = 0.14039070\n",
      "Iteration 43, loss = 0.27034340\n",
      "Iteration 89, loss = 0.14023521\n",
      "Iteration 44, loss = 0.26648469\n",
      "Iteration 90, loss = 0.13826601\n",
      "Iteration 45, loss = 0.26315306\n",
      "Iteration 91, loss = 0.13620372\n",
      "Iteration 46, loss = 0.26172688\n",
      "Iteration 92, loss = 0.13463169\n",
      "Iteration 47, loss = 0.25627772\n",
      "Iteration 93, loss = 0.13345281\n",
      "Iteration 48, loss = 0.25356235\n",
      "Iteration 94, loss = 0.13168540\n",
      "Iteration 49, loss = 0.25026158\n",
      "Iteration 95, loss = 0.13118461\n",
      "Iteration 50, loss = 0.24740429\n",
      "Iteration 96, loss = 0.12985607\n",
      "Iteration 51, loss = 0.24404950\n",
      "Iteration 97, loss = 0.12897278\n",
      "Iteration 52, loss = 0.24202209\n",
      "Iteration 98, loss = 0.12701351\n",
      "Iteration 53, loss = 0.23832711\n",
      "Iteration 99, loss = 0.12568015\n",
      "Iteration 54, loss = 0.23469236\n",
      "Iteration 100, loss = 0.12556999\n",
      "Iteration 55, loss = 0.23288214\n",
      "Iteration 101, loss = 0.12331193\n",
      "Iteration 56, loss = 0.23001024\n",
      "Iteration 102, loss = 0.12269811\n",
      "Iteration 57, loss = 0.22677903\n",
      "Iteration 103, loss = 0.12170444\n",
      "Iteration 104, loss = 0.11973748Iteration 58, loss = 0.22244541\n",
      "\n",
      "Iteration 105, loss = 0.12018481\n",
      "Iteration 59, loss = 0.22192978\n",
      "Iteration 106, loss = 0.11823192\n",
      "Iteration 60, loss = 0.21718140\n",
      "Iteration 107, loss = 0.11646157\n",
      "Iteration 61, loss = 0.21588234\n",
      "Iteration 108, loss = 0.11499562Iteration 62, loss = 0.21277661\n",
      "\n",
      "Iteration 109, loss = 0.11335542\n",
      "Iteration 63, loss = 0.21124675\n",
      "Iteration 110, loss = 0.11401702\n",
      "Iteration 64, loss = 0.20810610\n",
      "Iteration 111, loss = 0.11228137\n",
      "Iteration 65, loss = 0.20656911\n",
      "Iteration 112, loss = 0.11082080\n",
      "Iteration 66, loss = 0.20391325\n",
      "Iteration 113, loss = 0.11006845\n",
      "Iteration 67, loss = 0.20076501\n",
      "Iteration 68, loss = 0.19873003\n",
      "Iteration 114, loss = 0.10934657\n",
      "Iteration 69, loss = 0.19778840\n",
      "Iteration 115, loss = 0.10805091\n",
      "Iteration 70, loss = 0.19411445\n",
      "Iteration 116, loss = 0.10790901\n",
      "Iteration 71, loss = 0.19374434\n",
      "Iteration 117, loss = 0.10660921\n",
      "Iteration 72, loss = 0.19107139Iteration 118, loss = 0.10489375\n",
      "\n",
      "Iteration 119, loss = 0.10496189Iteration 73, loss = 0.18906311\n",
      "\n",
      "Iteration 120, loss = 0.10260223\n",
      "Iteration 74, loss = 0.18735781\n",
      "Iteration 121, loss = 0.10233314\n",
      "Iteration 75, loss = 0.18417208\n",
      "Iteration 122, loss = 0.09959764\n",
      "Iteration 76, loss = 0.18370716\n",
      "Iteration 123, loss = 0.10100212\n",
      "Iteration 77, loss = 0.18013672\n",
      "Iteration 124, loss = 0.10149915\n",
      "Iteration 78, loss = 0.18070565\n",
      "Iteration 125, loss = 0.09914683\n",
      "Iteration 79, loss = 0.17637211\n",
      "Iteration 126, loss = 0.09769944\n",
      "Iteration 80, loss = 0.17509636\n",
      "Iteration 127, loss = 0.09908908\n",
      "Iteration 81, loss = 0.17339864\n",
      "Iteration 128, loss = 0.09451063\n",
      "Iteration 82, loss = 0.17157902\n",
      "Iteration 129, loss = 0.09414977\n",
      "Iteration 83, loss = 0.16955361\n",
      "Iteration 130, loss = 0.09296774Iteration 84, loss = 0.16747930\n",
      "\n",
      "Iteration 85, loss = 0.16608150\n",
      "Iteration 131, loss = 0.09206210\n",
      "Iteration 86, loss = 0.16428929\n",
      "Iteration 132, loss = 0.09154195\n",
      "Iteration 87, loss = 0.16275270\n",
      "Iteration 133, loss = 0.09045823\n",
      "Iteration 134, loss = 0.08917094\n",
      "Iteration 88, loss = 0.16165479\n",
      "Iteration 135, loss = 0.08935997\n",
      "Iteration 89, loss = 0.15988126\n",
      "Iteration 136, loss = 0.08859541\n",
      "Iteration 90, loss = 0.15877170\n",
      "Iteration 137, loss = 0.08774619\n",
      "Iteration 91, loss = 0.15660985\n",
      "Iteration 138, loss = 0.08888986\n",
      "Iteration 92, loss = 0.15655239\n",
      "Iteration 139, loss = 0.08683332\n",
      "Iteration 93, loss = 0.15526397\n",
      "Iteration 140, loss = 0.08594493\n",
      "Iteration 94, loss = 0.15396272\n",
      "Iteration 141, loss = 0.08568387\n",
      "Iteration 95, loss = 0.15084815\n",
      "Iteration 142, loss = 0.08303705\n",
      "Iteration 96, loss = 0.14983181\n",
      "Iteration 143, loss = 0.08385618\n",
      "Iteration 97, loss = 0.14846071\n",
      "Iteration 144, loss = 0.08238090\n",
      "Iteration 98, loss = 0.14699064\n",
      "Iteration 145, loss = 0.08156036\n",
      "Iteration 99, loss = 0.14531041\n",
      "Iteration 146, loss = 0.08030990\n",
      "Iteration 100, loss = 0.14440980\n",
      "Iteration 147, loss = 0.07825318\n",
      "Iteration 101, loss = 0.14329721\n",
      "Iteration 148, loss = 0.07806775\n",
      "Iteration 102, loss = 0.14122177\n",
      "Iteration 149, loss = 0.07687398\n",
      "Iteration 103, loss = 0.14080840\n",
      "Iteration 150, loss = 0.07643565\n",
      "Iteration 104, loss = 0.13888083\n",
      "Iteration 151, loss = 0.07683498\n",
      "Iteration 105, loss = 0.13949283\n",
      "Iteration 152, loss = 0.07594368\n",
      "Iteration 106, loss = 0.13690429\n",
      "Iteration 153, loss = 0.07532414\n",
      "Iteration 107, loss = 0.13648749\n",
      "Iteration 154, loss = 0.07613606\n",
      "Iteration 108, loss = 0.13456924\n",
      "Iteration 155, loss = 0.07459259\n",
      "Iteration 109, loss = 0.13306417\n",
      "Iteration 156, loss = 0.07388591\n",
      "Iteration 110, loss = 0.13151109\n",
      "Iteration 157, loss = 0.07172763\n",
      "Iteration 111, loss = 0.13153419\n",
      "Iteration 158, loss = 0.07223553\n",
      "Iteration 112, loss = 0.12928197\n",
      "Iteration 159, loss = 0.07093929\n",
      "Iteration 113, loss = 0.12889170\n",
      "Iteration 160, loss = 0.07162961\n",
      "Iteration 114, loss = 0.12931535\n",
      "Iteration 161, loss = 0.07246024\n",
      "Iteration 115, loss = 0.12810662\n",
      "Iteration 162, loss = 0.07169258\n",
      "Iteration 116, loss = 0.12746452\n",
      "Iteration 163, loss = 0.07117294\n",
      "Iteration 117, loss = 0.12488781\n",
      "Iteration 164, loss = 0.06776774\n",
      "Iteration 118, loss = 0.12347792\n",
      "Iteration 165, loss = 0.06893413\n",
      "Iteration 119, loss = 0.12230108\n",
      "Iteration 166, loss = 0.06560931\n",
      "Iteration 120, loss = 0.12105438\n",
      "Iteration 167, loss = 0.06518920\n",
      "Iteration 121, loss = 0.11992973\n",
      "Iteration 168, loss = 0.06534492\n",
      "Iteration 122, loss = 0.11899811\n",
      "Iteration 169, loss = 0.06428225\n",
      "Iteration 123, loss = 0.11896563\n",
      "Iteration 124, loss = 0.11774167\n",
      "Iteration 170, loss = 0.06307406\n",
      "Iteration 171, loss = 0.06444020\n",
      "Iteration 125, loss = 0.11664304\n",
      "Iteration 172, loss = 0.06246029\n",
      "Iteration 126, loss = 0.11538396\n",
      "Iteration 173, loss = 0.06180678\n",
      "Iteration 127, loss = 0.11514567\n",
      "Iteration 174, loss = 0.06220692\n",
      "Iteration 128, loss = 0.11300760\n",
      "Iteration 175, loss = 0.06180911\n",
      "Iteration 129, loss = 0.11245055\n",
      "Iteration 176, loss = 0.06213179\n",
      "Iteration 130, loss = 0.11123816\n",
      "Iteration 177, loss = 0.06084464\n",
      "Iteration 131, loss = 0.11106964\n",
      "Iteration 178, loss = 0.06136788\n",
      "Iteration 132, loss = 0.11111623\n",
      "Iteration 179, loss = 0.05915494\n",
      "Iteration 133, loss = 0.10891855\n",
      "Iteration 180, loss = 0.05893317\n",
      "Iteration 134, loss = 0.10877463\n",
      "Iteration 181, loss = 0.05739015\n",
      "Iteration 135, loss = 0.10673892\n",
      "Iteration 182, loss = 0.05715766\n",
      "Iteration 136, loss = 0.10932137\n",
      "Iteration 183, loss = 0.05664080\n",
      "Iteration 137, loss = 0.10519316\n",
      "Iteration 184, loss = 0.05598836\n",
      "Iteration 138, loss = 0.10584169\n",
      "Iteration 185, loss = 0.05566366\n",
      "Iteration 139, loss = 0.10379471\n",
      "Iteration 186, loss = 0.05500279\n",
      "Iteration 140, loss = 0.10258864\n",
      "Iteration 187, loss = 0.05483778\n",
      "Iteration 141, loss = 0.10279177\n",
      "Iteration 188, loss = 0.05446123\n",
      "Iteration 142, loss = 0.10036095\n",
      "Iteration 189, loss = 0.05415438\n",
      "Iteration 143, loss = 0.10084373\n",
      "Iteration 190, loss = 0.05592021\n",
      "Iteration 144, loss = 0.09976618\n",
      "Iteration 191, loss = 0.05322273\n",
      "Iteration 145, loss = 0.09905885\n",
      "Iteration 192, loss = 0.05368946\n",
      "Iteration 146, loss = 0.09822672\n",
      "Iteration 193, loss = 0.05210541\n",
      "Iteration 147, loss = 0.09792222\n",
      "Iteration 194, loss = 0.05208295\n",
      "Iteration 148, loss = 0.09606890\n",
      "Iteration 195, loss = 0.05231429\n",
      "Iteration 149, loss = 0.09611478\n",
      "Iteration 196, loss = 0.05170552\n",
      "Iteration 150, loss = 0.09506827\n",
      "Iteration 197, loss = 0.05097651\n",
      "Iteration 151, loss = 0.09383909\n",
      "Iteration 198, loss = 0.05022345\n",
      "Iteration 152, loss = 0.09287670\n",
      "Iteration 199, loss = 0.05104947\n",
      "Iteration 153, loss = 0.09192646\n",
      "Iteration 200, loss = 0.05020086\n",
      "Iteration 154, loss = 0.09280555\n",
      "Iteration 201, loss = 0.04909635\n",
      "Iteration 155, loss = 0.09292479\n",
      "Iteration 202, loss = 0.04891781\n",
      "Iteration 156, loss = 0.09147111\n",
      "Iteration 203, loss = 0.05003926\n",
      "Iteration 157, loss = 0.09170432\n",
      "Iteration 204, loss = 0.04926324\n",
      "Iteration 158, loss = 0.09054265\n",
      "Iteration 205, loss = 0.04874193\n",
      "Iteration 159, loss = 0.08877778\n",
      "Iteration 206, loss = 0.04814946\n",
      "Iteration 160, loss = 0.08776981\n",
      "Iteration 207, loss = 0.04806022\n",
      "Iteration 161, loss = 0.08859202\n",
      "Iteration 208, loss = 0.04758211\n",
      "Iteration 162, loss = 0.08625298\n",
      "Iteration 209, loss = 0.04754122\n",
      "Iteration 163, loss = 0.08600838\n",
      "Iteration 210, loss = 0.04664406\n",
      "Iteration 164, loss = 0.08416121\n",
      "Iteration 211, loss = 0.04627953\n",
      "Iteration 165, loss = 0.08321368\n",
      "Iteration 212, loss = 0.04641718\n",
      "Iteration 166, loss = 0.08276838\n",
      "Iteration 213, loss = 0.04600309\n",
      "Iteration 167, loss = 0.08238725\n",
      "Iteration 214, loss = 0.04624932\n",
      "Iteration 168, loss = 0.08304705\n",
      "Iteration 215, loss = 0.04574160\n",
      "Iteration 169, loss = 0.08114070\n",
      "Iteration 216, loss = 0.04641828\n",
      "Iteration 170, loss = 0.08096735\n",
      "Iteration 217, loss = 0.04615118\n",
      "Iteration 171, loss = 0.07959356\n",
      "Iteration 218, loss = 0.04552934\n",
      "Iteration 172, loss = 0.08136519\n",
      "Iteration 219, loss = 0.04434326\n",
      "Iteration 173, loss = 0.07999543\n",
      "Iteration 220, loss = 0.04282888\n",
      "Iteration 174, loss = 0.08250902\n",
      "Iteration 221, loss = 0.04324277\n",
      "Iteration 175, loss = 0.07802388\n",
      "Iteration 222, loss = 0.04369467\n",
      "Iteration 176, loss = 0.07815542\n",
      "Iteration 223, loss = 0.04264477\n",
      "Iteration 177, loss = 0.07888127\n",
      "Iteration 224, loss = 0.04205823\n",
      "Iteration 178, loss = 0.07604780\n",
      "Iteration 225, loss = 0.04210220\n",
      "Iteration 179, loss = 0.07708341\n",
      "Iteration 226, loss = 0.04245926\n",
      "Iteration 180, loss = 0.07485308\n",
      "Iteration 227, loss = 0.04097609\n",
      "Iteration 181, loss = 0.07594946\n",
      "Iteration 228, loss = 0.04139036\n",
      "Iteration 182, loss = 0.07294628\n",
      "Iteration 229, loss = 0.04184777\n",
      "Iteration 183, loss = 0.07388309\n",
      "Iteration 230, loss = 0.04061509\n",
      "Iteration 184, loss = 0.07246681\n",
      "Iteration 231, loss = 0.04122265\n",
      "Iteration 185, loss = 0.07121591\n",
      "Iteration 232, loss = 0.03970596\n",
      "Iteration 186, loss = 0.07146296\n",
      "Iteration 233, loss = 0.04213039\n",
      "Iteration 187, loss = 0.06981822\n",
      "Iteration 234, loss = 0.03979404\n",
      "Iteration 188, loss = 0.06935349\n",
      "Iteration 235, loss = 0.03940104\n",
      "Iteration 189, loss = 0.06985676\n",
      "Iteration 236, loss = 0.04029075\n",
      "Iteration 190, loss = 0.07007768\n",
      "Iteration 237, loss = 0.03992786\n",
      "Iteration 191, loss = 0.06793124\n",
      "Iteration 238, loss = 0.03890021\n",
      "Iteration 192, loss = 0.06779781\n",
      "Iteration 239, loss = 0.03861917\n",
      "Iteration 193, loss = 0.06805249\n",
      "Iteration 240, loss = 0.03878421\n",
      "Iteration 194, loss = 0.06940568\n",
      "Iteration 241, loss = 0.03886751\n",
      "Iteration 195, loss = 0.06686854\n",
      "Iteration 242, loss = 0.03823449\n",
      "Iteration 196, loss = 0.06673957\n",
      "Iteration 243, loss = 0.03739425\n",
      "Iteration 197, loss = 0.06637836\n",
      "Iteration 244, loss = 0.03775976\n",
      "Iteration 198, loss = 0.06470130\n",
      "Iteration 245, loss = 0.03936100\n",
      "Iteration 199, loss = 0.06574501\n",
      "Iteration 246, loss = 0.03711935\n",
      "Iteration 200, loss = 0.06286842\n",
      "Iteration 247, loss = 0.03725969\n",
      "Iteration 201, loss = 0.06578508\n",
      "Iteration 248, loss = 0.03734724\n",
      "Iteration 202, loss = 0.06387265\n",
      "Iteration 249, loss = 0.03566848\n",
      "Iteration 203, loss = 0.06290419\n",
      "Iteration 250, loss = 0.03721373\n",
      "Iteration 204, loss = 0.06388575\n",
      "Iteration 251, loss = 0.03689831\n",
      "Iteration 205, loss = 0.06183787\n",
      "Iteration 252, loss = 0.03741225\n",
      "Iteration 206, loss = 0.06203272\n",
      "Iteration 253, loss = 0.03739543\n",
      "Iteration 207, loss = 0.06139540\n",
      "Iteration 254, loss = 0.03614203\n",
      "Iteration 208, loss = 0.06160171\n",
      "Iteration 255, loss = 0.03494412\n",
      "Iteration 209, loss = 0.06039389\n",
      "Iteration 256, loss = 0.03512046\n",
      "Iteration 210, loss = 0.06164219\n",
      "Iteration 257, loss = 0.04074354\n",
      "Iteration 211, loss = 0.05830945\n",
      "Iteration 258, loss = 0.03941996\n",
      "Iteration 212, loss = 0.06034036\n",
      "Iteration 259, loss = 0.03581782\n",
      "Iteration 213, loss = 0.05812129\n",
      "Iteration 260, loss = 0.03673874\n",
      "Iteration 214, loss = 0.05812917\n",
      "Iteration 215, loss = 0.05825438\n",
      "Iteration 261, loss = 0.03574720\n",
      "Iteration 216, loss = 0.05667698\n",
      "Iteration 262, loss = 0.03508589\n",
      "Iteration 217, loss = 0.05620293\n",
      "Iteration 263, loss = 0.03504299\n",
      "Iteration 218, loss = 0.05574965\n",
      "Iteration 264, loss = 0.03359531\n",
      "Iteration 219, loss = 0.05639941\n",
      "Iteration 265, loss = 0.03319126\n",
      "Iteration 220, loss = 0.05507302\n",
      "Iteration 266, loss = 0.03364695\n",
      "Iteration 221, loss = 0.05649023\n",
      "Iteration 267, loss = 0.03370820\n",
      "Iteration 222, loss = 0.05661479\n",
      "Iteration 268, loss = 0.03435755\n",
      "Iteration 223, loss = 0.05478870\n",
      "Iteration 269, loss = 0.03287100\n",
      "Iteration 224, loss = 0.05371460\n",
      "Iteration 270, loss = 0.03298661\n",
      "Iteration 225, loss = 0.05418817\n",
      "Iteration 271, loss = 0.03283347\n",
      "Iteration 226, loss = 0.05419752\n",
      "Iteration 272, loss = 0.03380542\n",
      "Iteration 227, loss = 0.05237287\n",
      "Iteration 273, loss = 0.03359135\n",
      "Iteration 228, loss = 0.05334503\n",
      "Iteration 274, loss = 0.03605815\n",
      "Iteration 229, loss = 0.05459678\n",
      "Iteration 275, loss = 0.03199105\n",
      "Iteration 230, loss = 0.05279796\n",
      "Iteration 276, loss = 0.03229520\n",
      "Iteration 231, loss = 0.05328890\n",
      "Iteration 277, loss = 0.03544330\n",
      "Iteration 232, loss = 0.05136879\n",
      "Iteration 278, loss = 0.03185975\n",
      "Iteration 233, loss = 0.05150725\n",
      "Iteration 279, loss = 0.03156164\n",
      "Iteration 234, loss = 0.05193118\n",
      "Iteration 280, loss = 0.03114368\n",
      "Iteration 235, loss = 0.05020579\n",
      "Iteration 281, loss = 0.03207259\n",
      "Iteration 236, loss = 0.04964335\n",
      "Iteration 282, loss = 0.03148167\n",
      "Iteration 237, loss = 0.04963593\n",
      "Iteration 283, loss = 0.03039477\n",
      "Iteration 238, loss = 0.05002230\n",
      "Iteration 284, loss = 0.03112468\n",
      "Iteration 239, loss = 0.04966943\n",
      "Iteration 285, loss = 0.03191176\n",
      "Iteration 240, loss = 0.04932579\n",
      "Iteration 286, loss = 0.03514118\n",
      "Iteration 241, loss = 0.05029135\n",
      "Iteration 287, loss = 0.03225275\n",
      "Iteration 242, loss = 0.04853083\n",
      "Iteration 288, loss = 0.03119675\n",
      "Iteration 243, loss = 0.04870278\n",
      "Iteration 289, loss = 0.03258522\n",
      "Iteration 244, loss = 0.04705661\n",
      "Iteration 290, loss = 0.03406432\n",
      "Iteration 245, loss = 0.04828704\n",
      "Iteration 291, loss = 0.03102657\n",
      "Iteration 246, loss = 0.04689104\n",
      "Iteration 292, loss = 0.03213697\n",
      "Iteration 247, loss = 0.04705689\n",
      "Iteration 293, loss = 0.03031595\n",
      "Iteration 248, loss = 0.04739464\n",
      "Iteration 294, loss = 0.02916242\n",
      "Iteration 249, loss = 0.04654950\n",
      "Iteration 295, loss = 0.03093758\n",
      "Iteration 250, loss = 0.04593109\n",
      "Iteration 296, loss = 0.02946666\n",
      "Iteration 251, loss = 0.04511090\n",
      "Iteration 297, loss = 0.03052370\n",
      "Iteration 252, loss = 0.04600289\n",
      "Iteration 298, loss = 0.02935400\n",
      "Iteration 253, loss = 0.04462474\n",
      "Iteration 299, loss = 0.02809230\n",
      "Iteration 254, loss = 0.04500699\n",
      "Iteration 300, loss = 0.02955591\n",
      "Iteration 255, loss = 0.04598448\n",
      "Iteration 301, loss = 0.03022978\n",
      "Iteration 256, loss = 0.04473261\n",
      "Iteration 302, loss = 0.02825178\n",
      "Iteration 257, loss = 0.04387951\n",
      "Iteration 303, loss = 0.02861249\n",
      "Iteration 258, loss = 0.04413175\n",
      "Iteration 304, loss = 0.02821265\n",
      "Iteration 259, loss = 0.04243779\n",
      "Iteration 305, loss = 0.02854228\n",
      "Iteration 260, loss = 0.04267098\n",
      "Iteration 306, loss = 0.02799824\n",
      "Iteration 261, loss = 0.04209383\n",
      "Iteration 307, loss = 0.02754648\n",
      "Iteration 262, loss = 0.04315022\n",
      "Iteration 308, loss = 0.02829822\n",
      "Iteration 263, loss = 0.04142259\n",
      "Iteration 309, loss = 0.02744830\n",
      "Iteration 264, loss = 0.04206508\n",
      "Iteration 310, loss = 0.02733592\n",
      "Iteration 265, loss = 0.04118315\n",
      "Iteration 311, loss = 0.02702407\n",
      "Iteration 312, loss = 0.02699271\n",
      "Iteration 266, loss = 0.04196091\n",
      "Iteration 313, loss = 0.02645498\n",
      "Iteration 267, loss = 0.04074765\n",
      "Iteration 314, loss = 0.02726681\n",
      "Iteration 268, loss = 0.04170562\n",
      "Iteration 315, loss = 0.02801179\n",
      "Iteration 269, loss = 0.04373307\n",
      "Iteration 316, loss = 0.02946415\n",
      "Iteration 270, loss = 0.03966863\n",
      "Iteration 317, loss = 0.02861842\n",
      "Iteration 271, loss = 0.04214787\n",
      "Iteration 318, loss = 0.02479049\n",
      "Iteration 272, loss = 0.04025392\n",
      "Iteration 273, loss = 0.03928574\n",
      "Iteration 319, loss = 0.02937077\n",
      "Iteration 274, loss = 0.04078829\n",
      "Iteration 320, loss = 0.02644939\n",
      "Iteration 275, loss = 0.04244013\n",
      "Iteration 321, loss = 0.02594121\n",
      "Iteration 276, loss = 0.03807654\n",
      "Iteration 322, loss = 0.02756973\n",
      "Iteration 277, loss = 0.03971607\n",
      "Iteration 323, loss = 0.02609424\n",
      "Iteration 278, loss = 0.03872010\n",
      "Iteration 324, loss = 0.02544866\n",
      "Iteration 279, loss = 0.03852178\n",
      "Iteration 325, loss = 0.02710695\n",
      "Iteration 280, loss = 0.03846887\n",
      "Iteration 326, loss = 0.02518355\n",
      "Iteration 281, loss = 0.03796107\n",
      "Iteration 327, loss = 0.02599802\n",
      "Iteration 282, loss = 0.03798627\n",
      "Iteration 328, loss = 0.02471714\n",
      "Iteration 283, loss = 0.03774533\n",
      "Iteration 329, loss = 0.02531601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 284, loss = 0.03785274\n",
      "Iteration 285, loss = 0.03797003\n",
      "Iteration 286, loss = 0.03656946\n",
      "Iteration 287, loss = 0.03746672\n",
      "Iteration 288, loss = 0.03736826\n",
      "Iteration 289, loss = 0.03630018\n",
      "Iteration 290, loss = 0.03629028\n",
      "Iteration 291, loss = 0.03651428\n",
      "Iteration 292, loss = 0.03625566\n",
      "Iteration 293, loss = 0.03502644\n",
      "Iteration 294, loss = 0.03570714\n",
      "Iteration 295, loss = 0.03482866\n",
      "Iteration 296, loss = 0.03513026\n",
      "Iteration 297, loss = 0.03433538\n",
      "Iteration 298, loss = 0.03420719\n",
      "Iteration 299, loss = 0.03449524\n",
      "Iteration 300, loss = 0.03393645\n",
      "Iteration 301, loss = 0.03332077\n",
      "Iteration 1, loss = 1.13982494\n",
      "Iteration 302, loss = 0.03314723\n",
      "Iteration 2, loss = 0.97529734\n",
      "Iteration 303, loss = 0.03295307\n",
      "Iteration 3, loss = 0.85311617\n",
      "Iteration 304, loss = 0.03328071\n",
      "Iteration 4, loss = 0.75991167\n",
      "Iteration 305, loss = 0.03274962\n",
      "Iteration 5, loss = 0.68664773\n",
      "Iteration 306, loss = 0.03400947\n",
      "Iteration 6, loss = 0.62977256\n",
      "Iteration 307, loss = 0.03213928\n",
      "Iteration 7, loss = 0.58434888\n",
      "Iteration 308, loss = 0.03287055\n",
      "Iteration 8, loss = 0.54766404\n",
      "Iteration 309, loss = 0.03323054\n",
      "Iteration 9, loss = 0.51687473\n",
      "Iteration 310, loss = 0.03382886\n",
      "Iteration 10, loss = 0.49203371\n",
      "Iteration 311, loss = 0.03192001\n",
      "Iteration 11, loss = 0.47050316\n",
      "Iteration 312, loss = 0.03268799\n",
      "Iteration 12, loss = 0.45217568\n",
      "Iteration 313, loss = 0.03237847\n",
      "Iteration 13, loss = 0.43546370\n",
      "Iteration 314, loss = 0.03120362\n",
      "Iteration 14, loss = 0.42114064\n",
      "Iteration 315, loss = 0.03288789\n",
      "Iteration 15, loss = 0.40740624\n",
      "Iteration 316, loss = 0.03079418\n",
      "Iteration 16, loss = 0.39581966\n",
      "Iteration 317, loss = 0.03112474\n",
      "Iteration 17, loss = 0.38463580\n",
      "Iteration 318, loss = 0.03072440\n",
      "Iteration 18, loss = 0.37541623\n",
      "Iteration 319, loss = 0.03056562\n",
      "Iteration 19, loss = 0.36633923\n",
      "Iteration 320, loss = 0.03053609\n",
      "Iteration 20, loss = 0.35755830\n",
      "Iteration 321, loss = 0.03034608\n",
      "Iteration 21, loss = 0.34943399\n",
      "Iteration 322, loss = 0.03036303\n",
      "Iteration 22, loss = 0.34208951\n",
      "Iteration 323, loss = 0.02959798\n",
      "Iteration 23, loss = 0.33547038\n",
      "Iteration 324, loss = 0.02970092\n",
      "Iteration 24, loss = 0.32842457\n",
      "Iteration 325, loss = 0.02992763\n",
      "Iteration 25, loss = 0.32226423\n",
      "Iteration 326, loss = 0.02970359\n",
      "Iteration 26, loss = 0.31633544\n",
      "Iteration 327, loss = 0.02997779\n",
      "Iteration 27, loss = 0.30986852\n",
      "Iteration 328, loss = 0.03125317\n",
      "Iteration 28, loss = 0.30535916\n",
      "Iteration 329, loss = 0.03083583\n",
      "Iteration 29, loss = 0.29897084\n",
      "Iteration 330, loss = 0.03005358\n",
      "Iteration 30, loss = 0.29522959\n",
      "Iteration 331, loss = 0.02937865\n",
      "Iteration 31, loss = 0.28891180\n",
      "Iteration 332, loss = 0.02868925\n",
      "Iteration 32, loss = 0.28474438\n",
      "Iteration 333, loss = 0.02900177\n",
      "Iteration 33, loss = 0.27827489\n",
      "Iteration 334, loss = 0.02894202\n",
      "Iteration 34, loss = 0.27490599\n",
      "Iteration 335, loss = 0.02820263\n",
      "Iteration 35, loss = 0.26941213\n",
      "Iteration 336, loss = 0.02784405\n",
      "Iteration 36, loss = 0.26495486\n",
      "Iteration 337, loss = 0.02770364\n",
      "Iteration 37, loss = 0.26038173\n",
      "Iteration 38, loss = 0.25645502\n",
      "Iteration 338, loss = 0.02767617\n",
      "Iteration 39, loss = 0.25340676\n",
      "Iteration 339, loss = 0.02700924\n",
      "Iteration 40, loss = 0.24800403\n",
      "Iteration 340, loss = 0.02779295\n",
      "Iteration 41, loss = 0.24424512\n",
      "Iteration 341, loss = 0.03042977\n",
      "Iteration 42, loss = 0.24167476\n",
      "Iteration 342, loss = 0.02774246\n",
      "Iteration 43, loss = 0.23701415\n",
      "Iteration 343, loss = 0.02654096\n",
      "Iteration 44, loss = 0.23332991\n",
      "Iteration 344, loss = 0.02722945\n",
      "Iteration 45, loss = 0.22883062\n",
      "Iteration 345, loss = 0.02636512\n",
      "Iteration 46, loss = 0.22746609\n",
      "Iteration 346, loss = 0.02709815\n",
      "Iteration 47, loss = 0.22216005\n",
      "Iteration 347, loss = 0.02663705\n",
      "Iteration 48, loss = 0.21962502\n",
      "Iteration 348, loss = 0.02655871\n",
      "Iteration 49, loss = 0.21588567\n",
      "Iteration 349, loss = 0.02668096\n",
      "Iteration 50, loss = 0.21285131\n",
      "Iteration 350, loss = 0.02592729\n",
      "Iteration 51, loss = 0.20906487\n",
      "Iteration 351, loss = 0.02597395\n",
      "Iteration 52, loss = 0.20606518\n",
      "Iteration 352, loss = 0.02544045\n",
      "Iteration 53, loss = 0.20426029\n",
      "Iteration 353, loss = 0.02538838\n",
      "Iteration 54, loss = 0.19875361\n",
      "Iteration 354, loss = 0.02603836\n",
      "Iteration 55, loss = 0.19678683\n",
      "Iteration 355, loss = 0.02679707\n",
      "Iteration 56, loss = 0.19483877\n",
      "Iteration 356, loss = 0.02561519\n",
      "Iteration 57, loss = 0.19055679\n",
      "Iteration 357, loss = 0.02507087\n",
      "Iteration 58, loss = 0.18893690\n",
      "Iteration 358, loss = 0.02566676\n",
      "Iteration 59, loss = 0.18498406\n",
      "Iteration 359, loss = 0.02582885\n",
      "Iteration 60, loss = 0.18374762\n",
      "Iteration 360, loss = 0.02513152\n",
      "Iteration 61, loss = 0.17901537\n",
      "Iteration 361, loss = 0.02440245\n",
      "Iteration 62, loss = 0.17954222\n",
      "Iteration 362, loss = 0.02424419\n",
      "Iteration 63, loss = 0.17425017\n",
      "Iteration 363, loss = 0.02451741\n",
      "Iteration 64, loss = 0.17477258\n",
      "Iteration 364, loss = 0.02560919\n",
      "Iteration 65, loss = 0.17049280\n",
      "Iteration 365, loss = 0.02626668\n",
      "Iteration 66, loss = 0.16877478\n",
      "Iteration 366, loss = 0.02360804\n",
      "Iteration 67, loss = 0.16573269\n",
      "Iteration 367, loss = 0.02441602\n",
      "Iteration 68, loss = 0.16468141\n",
      "Iteration 368, loss = 0.02439042\n",
      "Iteration 69, loss = 0.16310664\n",
      "Iteration 369, loss = 0.02504530\n",
      "Iteration 70, loss = 0.15950465\n",
      "Iteration 370, loss = 0.02497431\n",
      "Iteration 71, loss = 0.15648684\n",
      "Iteration 371, loss = 0.02438267\n",
      "Iteration 72, loss = 0.15438886\n",
      "Iteration 372, loss = 0.02417116\n",
      "Iteration 73, loss = 0.15233639\n",
      "Iteration 373, loss = 0.02389606\n",
      "Iteration 74, loss = 0.15080530\n",
      "Iteration 374, loss = 0.02338068\n",
      "Iteration 75, loss = 0.14938795\n",
      "Iteration 375, loss = 0.02276537\n",
      "Iteration 76, loss = 0.14772043\n",
      "Iteration 376, loss = 0.02369333\n",
      "Iteration 77, loss = 0.14463574\n",
      "Iteration 377, loss = 0.02376980\n",
      "Iteration 78, loss = 0.14377940\n",
      "Iteration 378, loss = 0.02339990\n",
      "Iteration 79, loss = 0.14114924\n",
      "Iteration 379, loss = 0.02250182\n",
      "Iteration 80, loss = 0.13907041\n",
      "Iteration 380, loss = 0.02283340\n",
      "Iteration 81, loss = 0.13777609\n",
      "Iteration 381, loss = 0.02222892\n",
      "Iteration 82, loss = 0.13646742\n",
      "Iteration 382, loss = 0.02202355\n",
      "Iteration 83, loss = 0.13415451\n",
      "Iteration 383, loss = 0.02382406\n",
      "Iteration 84, loss = 0.13341687\n",
      "Iteration 384, loss = 0.02323660\n",
      "Iteration 85, loss = 0.13140357\n",
      "Iteration 385, loss = 0.02180221\n",
      "Iteration 86, loss = 0.13025362\n",
      "Iteration 386, loss = 0.02473163\n",
      "Iteration 87, loss = 0.12843422\n",
      "Iteration 387, loss = 0.02187428\n",
      "Iteration 88, loss = 0.12687735\n",
      "Iteration 388, loss = 0.02143456\n",
      "Iteration 89, loss = 0.12488987\n",
      "Iteration 389, loss = 0.02137576\n",
      "Iteration 90, loss = 0.12455885\n",
      "Iteration 390, loss = 0.02205353\n",
      "Iteration 91, loss = 0.12163217\n",
      "Iteration 391, loss = 0.02215926\n",
      "Iteration 92, loss = 0.12130578\n",
      "Iteration 392, loss = 0.02150874\n",
      "Iteration 93, loss = 0.11959090\n",
      "Iteration 393, loss = 0.02061775\n",
      "Iteration 94, loss = 0.11987665\n",
      "Iteration 394, loss = 0.02072173\n",
      "Iteration 95, loss = 0.11764611\n",
      "Iteration 395, loss = 0.02206826\n",
      "Iteration 96, loss = 0.11516930\n",
      "Iteration 396, loss = 0.02048462\n",
      "Iteration 97, loss = 0.11385561\n",
      "Iteration 397, loss = 0.02185583\n",
      "Iteration 98, loss = 0.11225502\n",
      "Iteration 398, loss = 0.01968807\n",
      "Iteration 99, loss = 0.11145202\n",
      "Iteration 399, loss = 0.02095199\n",
      "Iteration 100, loss = 0.11131768\n",
      "Iteration 400, loss = 0.02187109\n",
      "Iteration 101, loss = 0.11003147\n",
      "Iteration 401, loss = 0.02352526\n",
      "Iteration 102, loss = 0.10839454\n",
      "Iteration 402, loss = 0.02535195\n",
      "Iteration 103, loss = 0.10653363\n",
      "Iteration 403, loss = 0.02441339\n",
      "Iteration 104, loss = 0.10660282\n",
      "Iteration 404, loss = 0.02189944\n",
      "Iteration 105, loss = 0.10422536\n",
      "Iteration 405, loss = 0.02252652\n",
      "Iteration 106, loss = 0.10496619\n",
      "Iteration 406, loss = 0.02199088\n",
      "Iteration 107, loss = 0.10278104\n",
      "Iteration 407, loss = 0.01975809\n",
      "Iteration 108, loss = 0.10278352\n",
      "Iteration 408, loss = 0.01970922\n",
      "Iteration 109, loss = 0.10085454\n",
      "Iteration 409, loss = 0.01962252\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 110, loss = 0.09916826\n",
      "Iteration 111, loss = 0.09868679\n",
      "Iteration 112, loss = 0.09727059\n",
      "Iteration 113, loss = 0.09725642\n",
      "Iteration 114, loss = 0.09614745\n",
      "Iteration 115, loss = 0.09396648\n",
      "Iteration 116, loss = 0.09431825\n",
      "Iteration 117, loss = 0.09263722\n",
      "Iteration 118, loss = 0.09124402\n",
      "Iteration 119, loss = 0.09043244\n",
      "Iteration 120, loss = 0.08931365\n",
      "Iteration 121, loss = 0.08881177\n",
      "Iteration 122, loss = 0.08971227\n",
      "Iteration 123, loss = 0.08704248\n",
      "Iteration 124, loss = 0.08685445\n",
      "Iteration 125, loss = 0.08593130\n",
      "Iteration 126, loss = 0.08670067\n",
      "Iteration 127, loss = 0.08405940\n",
      "Iteration 128, loss = 0.08496655\n",
      "Iteration 1, loss = 1.60095198\n",
      "Iteration 129, loss = 0.08444957\n",
      "Iteration 2, loss = 1.21480351\n",
      "Iteration 130, loss = 0.08185804\n",
      "Iteration 3, loss = 0.97789807\n",
      "Iteration 131, loss = 0.08091243\n",
      "Iteration 4, loss = 0.82324363\n",
      "Iteration 132, loss = 0.08093393\n",
      "Iteration 5, loss = 0.72398141\n",
      "Iteration 133, loss = 0.08172456\n",
      "Iteration 6, loss = 0.65657906\n",
      "Iteration 134, loss = 0.07956278\n",
      "Iteration 7, loss = 0.60821519\n",
      "Iteration 135, loss = 0.07964332\n",
      "Iteration 8, loss = 0.57225253\n",
      "Iteration 136, loss = 0.07816843\n",
      "Iteration 9, loss = 0.54102489\n",
      "Iteration 137, loss = 0.07666473\n",
      "Iteration 10, loss = 0.51763987\n",
      "Iteration 138, loss = 0.07541204\n",
      "Iteration 11, loss = 0.49607265\n",
      "Iteration 139, loss = 0.07550993\n",
      "Iteration 12, loss = 0.47805726\n",
      "Iteration 140, loss = 0.07407121\n",
      "Iteration 13, loss = 0.46272358\n",
      "Iteration 141, loss = 0.07356269\n",
      "Iteration 14, loss = 0.44927343\n",
      "Iteration 142, loss = 0.07300959\n",
      "Iteration 15, loss = 0.43710294\n",
      "Iteration 143, loss = 0.07267281\n",
      "Iteration 16, loss = 0.42637526\n",
      "Iteration 144, loss = 0.07378463\n",
      "Iteration 17, loss = 0.41547284\n",
      "Iteration 18, loss = 0.40583822\n",
      "Iteration 145, loss = 0.07175865\n",
      "Iteration 19, loss = 0.39807756\n",
      "Iteration 146, loss = 0.07284842\n",
      "Iteration 20, loss = 0.38975410\n",
      "Iteration 147, loss = 0.07048933\n",
      "Iteration 21, loss = 0.38186838\n",
      "Iteration 148, loss = 0.07176443\n",
      "Iteration 22, loss = 0.37460768\n",
      "Iteration 149, loss = 0.06842693\n",
      "Iteration 23, loss = 0.36780412\n",
      "Iteration 150, loss = 0.06910290\n",
      "Iteration 151, loss = 0.06821920\n",
      "Iteration 24, loss = 0.36088701\n",
      "Iteration 152, loss = 0.06724755\n",
      "Iteration 25, loss = 0.35497975\n",
      "Iteration 153, loss = 0.06657604\n",
      "Iteration 26, loss = 0.34889579\n",
      "Iteration 154, loss = 0.06624060\n",
      "Iteration 27, loss = 0.34426687\n",
      "Iteration 155, loss = 0.06533246\n",
      "Iteration 28, loss = 0.33770900\n",
      "Iteration 29, loss = 0.33298457\n",
      "Iteration 156, loss = 0.06489389\n",
      "Iteration 30, loss = 0.32739775\n",
      "Iteration 157, loss = 0.06602796\n",
      "Iteration 158, loss = 0.06440085\n",
      "Iteration 31, loss = 0.32218041\n",
      "Iteration 159, loss = 0.06395269\n",
      "Iteration 32, loss = 0.31696682\n",
      "Iteration 160, loss = 0.06335045\n",
      "Iteration 33, loss = 0.31220856\n",
      "Iteration 161, loss = 0.06491913\n",
      "Iteration 34, loss = 0.30756601\n",
      "Iteration 162, loss = 0.06120667\n",
      "Iteration 35, loss = 0.30259180\n",
      "Iteration 163, loss = 0.06326949\n",
      "Iteration 36, loss = 0.29872854\n",
      "Iteration 164, loss = 0.06172376\n",
      "Iteration 37, loss = 0.29403102\n",
      "Iteration 165, loss = 0.06049942\n",
      "Iteration 38, loss = 0.28953515\n",
      "Iteration 166, loss = 0.06029705\n",
      "Iteration 39, loss = 0.28475558\n",
      "Iteration 167, loss = 0.05874406\n",
      "Iteration 40, loss = 0.28145447\n",
      "Iteration 168, loss = 0.05997937\n",
      "Iteration 41, loss = 0.27642411\n",
      "Iteration 169, loss = 0.05982109\n",
      "Iteration 42, loss = 0.27316528\n",
      "Iteration 43, loss = 0.26893540\n",
      "Iteration 170, loss = 0.05795302\n",
      "Iteration 44, loss = 0.26585454\n",
      "Iteration 171, loss = 0.05826290\n",
      "Iteration 45, loss = 0.26185940\n",
      "Iteration 172, loss = 0.05634361\n",
      "Iteration 46, loss = 0.25888908\n",
      "Iteration 173, loss = 0.05668873\n",
      "Iteration 47, loss = 0.25540747\n",
      "Iteration 174, loss = 0.05618058\n",
      "Iteration 48, loss = 0.25181147\n",
      "Iteration 175, loss = 0.05635785\n",
      "Iteration 49, loss = 0.24799892\n",
      "Iteration 176, loss = 0.05746375\n",
      "Iteration 50, loss = 0.24480855\n",
      "Iteration 177, loss = 0.05497313\n",
      "Iteration 178, loss = 0.05562649Iteration 51, loss = 0.24167610\n",
      "\n",
      "Iteration 52, loss = 0.23884909\n",
      "Iteration 179, loss = 0.05554755\n",
      "Iteration 53, loss = 0.23593389\n",
      "Iteration 180, loss = 0.05319797\n",
      "Iteration 54, loss = 0.23361253\n",
      "Iteration 181, loss = 0.05354913\n",
      "Iteration 55, loss = 0.22987796\n",
      "Iteration 182, loss = 0.05254083\n",
      "Iteration 56, loss = 0.22773507\n",
      "Iteration 183, loss = 0.05205528\n",
      "Iteration 57, loss = 0.22548041\n",
      "Iteration 184, loss = 0.05177625\n",
      "Iteration 58, loss = 0.22296510\n",
      "Iteration 185, loss = 0.05222668\n",
      "Iteration 59, loss = 0.21957432\n",
      "Iteration 186, loss = 0.05064870\n",
      "Iteration 60, loss = 0.21703860\n",
      "Iteration 187, loss = 0.05061252\n",
      "Iteration 61, loss = 0.21487846\n",
      "Iteration 188, loss = 0.05111655\n",
      "Iteration 62, loss = 0.21271515\n",
      "Iteration 189, loss = 0.05014820\n",
      "Iteration 63, loss = 0.20963993\n",
      "Iteration 190, loss = 0.05019155\n",
      "Iteration 64, loss = 0.20728097\n",
      "Iteration 191, loss = 0.04946741\n",
      "Iteration 65, loss = 0.20577922\n",
      "Iteration 192, loss = 0.04860146\n",
      "Iteration 66, loss = 0.20262776\n",
      "Iteration 193, loss = 0.04869720\n",
      "Iteration 67, loss = 0.20081142\n",
      "Iteration 194, loss = 0.04807632\n",
      "Iteration 68, loss = 0.19850356\n",
      "Iteration 195, loss = 0.04778896\n",
      "Iteration 69, loss = 0.19618693\n",
      "Iteration 196, loss = 0.04761005\n",
      "Iteration 70, loss = 0.19467827\n",
      "Iteration 197, loss = 0.04720991\n",
      "Iteration 71, loss = 0.19251736\n",
      "Iteration 198, loss = 0.04687644\n",
      "Iteration 72, loss = 0.19013592\n",
      "Iteration 199, loss = 0.04803247\n",
      "Iteration 73, loss = 0.18797592\n",
      "Iteration 200, loss = 0.04537694\n",
      "Iteration 74, loss = 0.18577280\n",
      "Iteration 201, loss = 0.04682813\n",
      "Iteration 75, loss = 0.18406507\n",
      "Iteration 202, loss = 0.04511152\n",
      "Iteration 76, loss = 0.18323845\n",
      "Iteration 203, loss = 0.04582568\n",
      "Iteration 77, loss = 0.18074252\n",
      "Iteration 204, loss = 0.04463108\n",
      "Iteration 78, loss = 0.17883332\n",
      "Iteration 205, loss = 0.04584742\n",
      "Iteration 79, loss = 0.17746835\n",
      "Iteration 206, loss = 0.04529069\n",
      "Iteration 80, loss = 0.17527814\n",
      "Iteration 207, loss = 0.04396831\n",
      "Iteration 81, loss = 0.17346480\n",
      "Iteration 208, loss = 0.04361562\n",
      "Iteration 82, loss = 0.17131406\n",
      "Iteration 209, loss = 0.04457742\n",
      "Iteration 83, loss = 0.16998974\n",
      "Iteration 210, loss = 0.04375182\n",
      "Iteration 84, loss = 0.16863643\n",
      "Iteration 211, loss = 0.04344345\n",
      "Iteration 85, loss = 0.16789945\n",
      "Iteration 212, loss = 0.04371449\n",
      "Iteration 86, loss = 0.16431015\n",
      "Iteration 213, loss = 0.04159926\n",
      "Iteration 87, loss = 0.16295406\n",
      "Iteration 214, loss = 0.04428717\n",
      "Iteration 88, loss = 0.16111357\n",
      "Iteration 215, loss = 0.04243997\n",
      "Iteration 89, loss = 0.16004702\n",
      "Iteration 216, loss = 0.04107533\n",
      "Iteration 90, loss = 0.15836438\n",
      "Iteration 217, loss = 0.04188639\n",
      "Iteration 91, loss = 0.15665863\n",
      "Iteration 218, loss = 0.04104544\n",
      "Iteration 92, loss = 0.15565365\n",
      "Iteration 219, loss = 0.04047828\n",
      "Iteration 93, loss = 0.15438731\n",
      "Iteration 220, loss = 0.04047113\n",
      "Iteration 94, loss = 0.15165210\n",
      "Iteration 221, loss = 0.04020692\n",
      "Iteration 95, loss = 0.15058989\n",
      "Iteration 222, loss = 0.04134258\n",
      "Iteration 96, loss = 0.14901176\n",
      "Iteration 223, loss = 0.04039827\n",
      "Iteration 97, loss = 0.14792985\n",
      "Iteration 224, loss = 0.03839048\n",
      "Iteration 98, loss = 0.14637683\n",
      "Iteration 225, loss = 0.04042646\n",
      "Iteration 99, loss = 0.14477952\n",
      "Iteration 226, loss = 0.04088816\n",
      "Iteration 100, loss = 0.14350565\n",
      "Iteration 227, loss = 0.03850080\n",
      "Iteration 101, loss = 0.14181993\n",
      "Iteration 228, loss = 0.03837733\n",
      "Iteration 102, loss = 0.14094776\n",
      "Iteration 229, loss = 0.03826920\n",
      "Iteration 103, loss = 0.13963695\n",
      "Iteration 230, loss = 0.03676286\n",
      "Iteration 104, loss = 0.13880059\n",
      "Iteration 231, loss = 0.03792990\n",
      "Iteration 105, loss = 0.13680103\n",
      "Iteration 232, loss = 0.03791776\n",
      "Iteration 106, loss = 0.13600155\n",
      "Iteration 233, loss = 0.03740476\n",
      "Iteration 107, loss = 0.13396732\n",
      "Iteration 234, loss = 0.03818831\n",
      "Iteration 108, loss = 0.13341269\n",
      "Iteration 235, loss = 0.03639304\n",
      "Iteration 109, loss = 0.13434968\n",
      "Iteration 236, loss = 0.03670404\n",
      "Iteration 110, loss = 0.13229899\n",
      "Iteration 237, loss = 0.03649073\n",
      "Iteration 111, loss = 0.12943659\n",
      "Iteration 238, loss = 0.03699179\n",
      "Iteration 112, loss = 0.12905607\n",
      "Iteration 239, loss = 0.03496776\n",
      "Iteration 113, loss = 0.12678951\n",
      "Iteration 240, loss = 0.03556029\n",
      "Iteration 114, loss = 0.12595006\n",
      "Iteration 241, loss = 0.03471016\n",
      "Iteration 115, loss = 0.12511927\n",
      "Iteration 242, loss = 0.03485460\n",
      "Iteration 116, loss = 0.12387666\n",
      "Iteration 243, loss = 0.03447065\n",
      "Iteration 117, loss = 0.12241077\n",
      "Iteration 244, loss = 0.03456445\n",
      "Iteration 118, loss = 0.12314817\n",
      "Iteration 245, loss = 0.03425279\n",
      "Iteration 119, loss = 0.11935034\n",
      "Iteration 246, loss = 0.03386412\n",
      "Iteration 120, loss = 0.12089023\n",
      "Iteration 247, loss = 0.03366510\n",
      "Iteration 121, loss = 0.11844849\n",
      "Iteration 248, loss = 0.03365438\n",
      "Iteration 122, loss = 0.11840352\n",
      "Iteration 123, loss = 0.11721634\n",
      "Iteration 249, loss = 0.03390351\n",
      "Iteration 250, loss = 0.03332369\n",
      "Iteration 124, loss = 0.11628509\n",
      "Iteration 251, loss = 0.03287101\n",
      "Iteration 125, loss = 0.11453998\n",
      "Iteration 252, loss = 0.03304016\n",
      "Iteration 126, loss = 0.11479938\n",
      "Iteration 253, loss = 0.03351288\n",
      "Iteration 127, loss = 0.11205311\n",
      "Iteration 254, loss = 0.03182103\n",
      "Iteration 128, loss = 0.11180272\n",
      "Iteration 255, loss = 0.03297656\n",
      "Iteration 129, loss = 0.11033472\n",
      "Iteration 256, loss = 0.03274250\n",
      "Iteration 130, loss = 0.10901937\n",
      "Iteration 257, loss = 0.03182020\n",
      "Iteration 131, loss = 0.10929763\n",
      "Iteration 258, loss = 0.03197128\n",
      "Iteration 132, loss = 0.10740485\n",
      "Iteration 133, loss = 0.10618252\n",
      "Iteration 259, loss = 0.03144136\n",
      "Iteration 260, loss = 0.03093427\n",
      "Iteration 134, loss = 0.10585761\n",
      "Iteration 261, loss = 0.03074012Iteration 135, loss = 0.10610088\n",
      "\n",
      "Iteration 136, loss = 0.10325641\n",
      "Iteration 262, loss = 0.03046249\n",
      "Iteration 137, loss = 0.10342089\n",
      "Iteration 263, loss = 0.03000807\n",
      "Iteration 138, loss = 0.10341787\n",
      "Iteration 264, loss = 0.02974454\n",
      "Iteration 139, loss = 0.10145467\n",
      "Iteration 265, loss = 0.02994962\n",
      "Iteration 140, loss = 0.10164287\n",
      "Iteration 266, loss = 0.02957303\n",
      "Iteration 141, loss = 0.10072754\n",
      "Iteration 267, loss = 0.02958238\n",
      "Iteration 142, loss = 0.09941465\n",
      "Iteration 268, loss = 0.02977187\n",
      "Iteration 143, loss = 0.09803450\n",
      "Iteration 269, loss = 0.03030063\n",
      "Iteration 144, loss = 0.09810340\n",
      "Iteration 270, loss = 0.02841386\n",
      "Iteration 145, loss = 0.09738180\n",
      "Iteration 271, loss = 0.02951283\n",
      "Iteration 146, loss = 0.09575073\n",
      "Iteration 272, loss = 0.02957844\n",
      "Iteration 147, loss = 0.09625406\n",
      "Iteration 273, loss = 0.02935501\n",
      "Iteration 148, loss = 0.09415041\n",
      "Iteration 274, loss = 0.02903790\n",
      "Iteration 149, loss = 0.09392564\n",
      "Iteration 275, loss = 0.02839726\n",
      "Iteration 150, loss = 0.09254712\n",
      "Iteration 276, loss = 0.02845222\n",
      "Iteration 151, loss = 0.09304175\n",
      "Iteration 277, loss = 0.02817570\n",
      "Iteration 152, loss = 0.09108384\n",
      "Iteration 278, loss = 0.02755999\n",
      "Iteration 153, loss = 0.09186566\n",
      "Iteration 279, loss = 0.02810438\n",
      "Iteration 154, loss = 0.09264312\n",
      "Iteration 280, loss = 0.02772728\n",
      "Iteration 155, loss = 0.09116371\n",
      "Iteration 281, loss = 0.02791898\n",
      "Iteration 156, loss = 0.08904020\n",
      "Iteration 282, loss = 0.02746040\n",
      "Iteration 157, loss = 0.08837245\n",
      "Iteration 283, loss = 0.02699502\n",
      "Iteration 158, loss = 0.08656547\n",
      "Iteration 284, loss = 0.02671043\n",
      "Iteration 159, loss = 0.08736877\n",
      "Iteration 285, loss = 0.02653119\n",
      "Iteration 160, loss = 0.08553989\n",
      "Iteration 286, loss = 0.02617873\n",
      "Iteration 161, loss = 0.08496361\n",
      "Iteration 287, loss = 0.02671053\n",
      "Iteration 162, loss = 0.08417258\n",
      "Iteration 288, loss = 0.02720865\n",
      "Iteration 163, loss = 0.08363595\n",
      "Iteration 289, loss = 0.02598026\n",
      "Iteration 164, loss = 0.08305592\n",
      "Iteration 290, loss = 0.02626393\n",
      "Iteration 165, loss = 0.08235406\n",
      "Iteration 291, loss = 0.02780760\n",
      "Iteration 166, loss = 0.08140796\n",
      "Iteration 292, loss = 0.02701587\n",
      "Iteration 167, loss = 0.08147462\n",
      "Iteration 293, loss = 0.02652806\n",
      "Iteration 168, loss = 0.08119855\n",
      "Iteration 294, loss = 0.02646532\n",
      "Iteration 169, loss = 0.08176414\n",
      "Iteration 295, loss = 0.02597928\n",
      "Iteration 170, loss = 0.07937971\n",
      "Iteration 296, loss = 0.02556668\n",
      "Iteration 171, loss = 0.07946817\n",
      "Iteration 297, loss = 0.02489111\n",
      "Iteration 172, loss = 0.07907363\n",
      "Iteration 298, loss = 0.02573914\n",
      "Iteration 173, loss = 0.07724845\n",
      "Iteration 299, loss = 0.02556319\n",
      "Iteration 174, loss = 0.07920129\n",
      "Iteration 300, loss = 0.02607898\n",
      "Iteration 175, loss = 0.07704285\n",
      "Iteration 301, loss = 0.02418151\n",
      "Iteration 176, loss = 0.07618993\n",
      "Iteration 302, loss = 0.02455226\n",
      "Iteration 177, loss = 0.07546141\n",
      "Iteration 303, loss = 0.02444747\n",
      "Iteration 178, loss = 0.07459549\n",
      "Iteration 304, loss = 0.02382255\n",
      "Iteration 179, loss = 0.07551874\n",
      "Iteration 305, loss = 0.02373289\n",
      "Iteration 180, loss = 0.07331396\n",
      "Iteration 306, loss = 0.02412820\n",
      "Iteration 181, loss = 0.07367215\n",
      "Iteration 307, loss = 0.02350669\n",
      "Iteration 182, loss = 0.07256011\n",
      "Iteration 308, loss = 0.02421968\n",
      "Iteration 183, loss = 0.07205380\n",
      "Iteration 309, loss = 0.02422852\n",
      "Iteration 184, loss = 0.07181505\n",
      "Iteration 310, loss = 0.02292767\n",
      "Iteration 185, loss = 0.07095457\n",
      "Iteration 311, loss = 0.02347634\n",
      "Iteration 186, loss = 0.07021125\n",
      "Iteration 312, loss = 0.02473978\n",
      "Iteration 187, loss = 0.06983983\n",
      "Iteration 313, loss = 0.02378391\n",
      "Iteration 188, loss = 0.06952274\n",
      "Iteration 314, loss = 0.02248608\n",
      "Iteration 189, loss = 0.06857900\n",
      "Iteration 315, loss = 0.02409287\n",
      "Iteration 190, loss = 0.06824156\n",
      "Iteration 316, loss = 0.02474552\n",
      "Iteration 191, loss = 0.06786224\n",
      "Iteration 317, loss = 0.02193274\n",
      "Iteration 192, loss = 0.06745823\n",
      "Iteration 318, loss = 0.02240175\n",
      "Iteration 193, loss = 0.06634301\n",
      "Iteration 319, loss = 0.02269577\n",
      "Iteration 194, loss = 0.06618560\n",
      "Iteration 320, loss = 0.02275687\n",
      "Iteration 195, loss = 0.06594237\n",
      "Iteration 196, loss = 0.06554004\n",
      "Iteration 321, loss = 0.02234444\n",
      "Iteration 197, loss = 0.06648674\n",
      "Iteration 322, loss = 0.02189605\n",
      "Iteration 198, loss = 0.06442591\n",
      "Iteration 323, loss = 0.02279080\n",
      "Iteration 199, loss = 0.06631880\n",
      "Iteration 324, loss = 0.02161423\n",
      "Iteration 200, loss = 0.06408407\n",
      "Iteration 325, loss = 0.02190609\n",
      "Iteration 201, loss = 0.06398116\n",
      "Iteration 326, loss = 0.02195263\n",
      "Iteration 202, loss = 0.06513561\n",
      "Iteration 327, loss = 0.02247754\n",
      "Iteration 203, loss = 0.06167270\n",
      "Iteration 328, loss = 0.02147682\n",
      "Iteration 204, loss = 0.06324150\n",
      "Iteration 329, loss = 0.02345389\n",
      "Iteration 205, loss = 0.06095997\n",
      "Iteration 330, loss = 0.02206262\n",
      "Iteration 206, loss = 0.06119941\n",
      "Iteration 331, loss = 0.02153452\n",
      "Iteration 207, loss = 0.06074166\n",
      "Iteration 332, loss = 0.02169141\n",
      "Iteration 208, loss = 0.05955716\n",
      "Iteration 333, loss = 0.02017498\n",
      "Iteration 209, loss = 0.05971544\n",
      "Iteration 334, loss = 0.02110896\n",
      "Iteration 210, loss = 0.05941327\n",
      "Iteration 335, loss = 0.02035770\n",
      "Iteration 211, loss = 0.05743612\n",
      "Iteration 336, loss = 0.02050125\n",
      "Iteration 212, loss = 0.05894903\n",
      "Iteration 337, loss = 0.02081165\n",
      "Iteration 213, loss = 0.05702756\n",
      "Iteration 338, loss = 0.02210529\n",
      "Iteration 214, loss = 0.05809987\n",
      "Iteration 339, loss = 0.02029217\n",
      "Iteration 215, loss = 0.05740087\n",
      "Iteration 340, loss = 0.02070803\n",
      "Iteration 216, loss = 0.05602205\n",
      "Iteration 341, loss = 0.01995656\n",
      "Iteration 217, loss = 0.05624323\n",
      "Iteration 342, loss = 0.01932084\n",
      "Iteration 218, loss = 0.05514194\n",
      "Iteration 343, loss = 0.01938558\n",
      "Iteration 219, loss = 0.05450901\n",
      "Iteration 344, loss = 0.01925391\n",
      "Iteration 220, loss = 0.05380914\n",
      "Iteration 345, loss = 0.01917723\n",
      "Iteration 221, loss = 0.05379034\n",
      "Iteration 346, loss = 0.01899555\n",
      "Iteration 222, loss = 0.05366559\n",
      "Iteration 347, loss = 0.01988588\n",
      "Iteration 223, loss = 0.05323698\n",
      "Iteration 348, loss = 0.01928472\n",
      "Iteration 224, loss = 0.05270635\n",
      "Iteration 349, loss = 0.01929695\n",
      "Iteration 225, loss = 0.05255878\n",
      "Iteration 350, loss = 0.01850884\n",
      "Iteration 226, loss = 0.05365706\n",
      "Iteration 351, loss = 0.02054612\n",
      "Iteration 227, loss = 0.05300282\n",
      "Iteration 352, loss = 0.02177260\n",
      "Iteration 228, loss = 0.05486308\n",
      "Iteration 353, loss = 0.02250082\n",
      "Iteration 229, loss = 0.05268403\n",
      "Iteration 354, loss = 0.02162916\n",
      "Iteration 230, loss = 0.05473813\n",
      "Iteration 355, loss = 0.01974302\n",
      "Iteration 231, loss = 0.05138352\n",
      "Iteration 356, loss = 0.01932657\n",
      "Iteration 232, loss = 0.05084288\n",
      "Iteration 357, loss = 0.01817887\n",
      "Iteration 233, loss = 0.05013419\n",
      "Iteration 358, loss = 0.01813831\n",
      "Iteration 234, loss = 0.04962393\n",
      "Iteration 359, loss = 0.01774203\n",
      "Iteration 235, loss = 0.05038369\n",
      "Iteration 360, loss = 0.01811973\n",
      "Iteration 236, loss = 0.05019394\n",
      "Iteration 361, loss = 0.02039454\n",
      "Iteration 237, loss = 0.04801758\n",
      "Iteration 362, loss = 0.02121600\n",
      "Iteration 238, loss = 0.04874471\n",
      "Iteration 363, loss = 0.01806934\n",
      "Iteration 239, loss = 0.04895569\n",
      "Iteration 364, loss = 0.02003696\n",
      "Iteration 240, loss = 0.04738790\n",
      "Iteration 365, loss = 0.01920915\n",
      "Iteration 241, loss = 0.04784879\n",
      "Iteration 366, loss = 0.01895936\n",
      "Iteration 242, loss = 0.04681299\n",
      "Iteration 367, loss = 0.01883854\n",
      "Iteration 243, loss = 0.04621104\n",
      "Iteration 368, loss = 0.01748730\n",
      "Iteration 244, loss = 0.04699348\n",
      "Iteration 369, loss = 0.01710891\n",
      "Iteration 245, loss = 0.04582263\n",
      "Iteration 370, loss = 0.01704875\n",
      "Iteration 246, loss = 0.04642227\n",
      "Iteration 371, loss = 0.01749824\n",
      "Iteration 247, loss = 0.04792824\n",
      "Iteration 372, loss = 0.01849423\n",
      "Iteration 248, loss = 0.05075637\n",
      "Iteration 373, loss = 0.01804130\n",
      "Iteration 249, loss = 0.04723150\n",
      "Iteration 374, loss = 0.01694554\n",
      "Iteration 250, loss = 0.04543013\n",
      "Iteration 375, loss = 0.01731036\n",
      "Iteration 251, loss = 0.04645084\n",
      "Iteration 376, loss = 0.01629960\n",
      "Iteration 252, loss = 0.04481123\n",
      "Iteration 377, loss = 0.01664057\n",
      "Iteration 253, loss = 0.04418344\n",
      "Iteration 378, loss = 0.01588497\n",
      "Iteration 254, loss = 0.04348585\n",
      "Iteration 379, loss = 0.01613339\n",
      "Iteration 255, loss = 0.04362676\n",
      "Iteration 380, loss = 0.01623743\n",
      "Iteration 256, loss = 0.04248969\n",
      "Iteration 381, loss = 0.01676510\n",
      "Iteration 257, loss = 0.04271084\n",
      "Iteration 382, loss = 0.01600965\n",
      "Iteration 258, loss = 0.04216411\n",
      "Iteration 383, loss = 0.01598315\n",
      "Iteration 259, loss = 0.04201973\n",
      "Iteration 384, loss = 0.01816510\n",
      "Iteration 260, loss = 0.04196025\n",
      "Iteration 385, loss = 0.01841667\n",
      "Iteration 261, loss = 0.04144019\n",
      "Iteration 386, loss = 0.01551230\n",
      "Iteration 262, loss = 0.04095081\n",
      "Iteration 387, loss = 0.01721490\n",
      "Iteration 263, loss = 0.04172020\n",
      "Iteration 388, loss = 0.01520352\n",
      "Iteration 264, loss = 0.04124539\n",
      "Iteration 389, loss = 0.01535394\n",
      "Iteration 265, loss = 0.04109400\n",
      "Iteration 390, loss = 0.01528850\n",
      "Iteration 266, loss = 0.04148808\n",
      "Iteration 391, loss = 0.01573852\n",
      "Iteration 267, loss = 0.04032080\n",
      "Iteration 392, loss = 0.01514961\n",
      "Iteration 268, loss = 0.04065501\n",
      "Iteration 393, loss = 0.01478198\n",
      "Iteration 269, loss = 0.04118465\n",
      "Iteration 394, loss = 0.01486244\n",
      "Iteration 270, loss = 0.03902814\n",
      "Iteration 395, loss = 0.01469995\n",
      "Iteration 271, loss = 0.04047049\n",
      "Iteration 396, loss = 0.01472494\n",
      "Iteration 272, loss = 0.03932207\n",
      "Iteration 397, loss = 0.01457381\n",
      "Iteration 273, loss = 0.03879703\n",
      "Iteration 398, loss = 0.01471265\n",
      "Iteration 274, loss = 0.03848899\n",
      "Iteration 399, loss = 0.01654801\n",
      "Iteration 275, loss = 0.03768219\n",
      "Iteration 400, loss = 0.01808503\n",
      "Iteration 276, loss = 0.03862759\n",
      "Iteration 401, loss = 0.01760106\n",
      "Iteration 277, loss = 0.03848626\n",
      "Iteration 402, loss = 0.01699884\n",
      "Iteration 278, loss = 0.03714063\n",
      "Iteration 403, loss = 0.01775129\n",
      "Iteration 279, loss = 0.03891685\n",
      "Iteration 404, loss = 0.01411851\n",
      "Iteration 280, loss = 0.03697747\n",
      "Iteration 405, loss = 0.01492712\n",
      "Iteration 281, loss = 0.03735818\n",
      "Iteration 406, loss = 0.01540948\n",
      "Iteration 282, loss = 0.03674656\n",
      "Iteration 407, loss = 0.01440463\n",
      "Iteration 283, loss = 0.03721053\n",
      "Iteration 408, loss = 0.01435804\n",
      "Iteration 284, loss = 0.03757146\n",
      "Iteration 409, loss = 0.01480002\n",
      "Iteration 285, loss = 0.03701934\n",
      "Iteration 286, loss = 0.03659600Iteration 410, loss = 0.01348342\n",
      "\n",
      "Iteration 287, loss = 0.03629348\n",
      "Iteration 411, loss = 0.01370823\n",
      "Iteration 288, loss = 0.03600736\n",
      "Iteration 412, loss = 0.01427860\n",
      "Iteration 289, loss = 0.03595592\n",
      "Iteration 413, loss = 0.01325097\n",
      "Iteration 290, loss = 0.03537745\n",
      "Iteration 414, loss = 0.01448605\n",
      "Iteration 291, loss = 0.03473015\n",
      "Iteration 415, loss = 0.01388812\n",
      "Iteration 416, loss = 0.01463889\n",
      "Iteration 292, loss = 0.03577735\n",
      "Iteration 417, loss = 0.01502640\n",
      "Iteration 293, loss = 0.03515223\n",
      "Iteration 418, loss = 0.01368172\n",
      "Iteration 294, loss = 0.03456788\n",
      "Iteration 419, loss = 0.01505249\n",
      "Iteration 295, loss = 0.03448885\n",
      "Iteration 420, loss = 0.02183935\n",
      "Iteration 296, loss = 0.03532691\n",
      "Iteration 421, loss = 0.01810488\n",
      "Iteration 297, loss = 0.03521467\n",
      "Iteration 422, loss = 0.01439277\n",
      "Iteration 298, loss = 0.03399249\n",
      "Iteration 423, loss = 0.01339583\n",
      "Iteration 299, loss = 0.03422277\n",
      "Iteration 300, loss = 0.03376835\n",
      "Iteration 424, loss = 0.01411712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 301, loss = 0.03365558\n",
      "Iteration 302, loss = 0.03288192\n",
      "Iteration 303, loss = 0.03403542\n",
      "Iteration 304, loss = 0.03275926\n",
      "Iteration 305, loss = 0.03218368\n",
      "Iteration 306, loss = 0.03255428\n",
      "Iteration 307, loss = 0.03251197\n",
      "Iteration 308, loss = 0.03246858\n",
      "Iteration 309, loss = 0.03187230\n",
      "Iteration 310, loss = 0.03202920\n",
      "Iteration 311, loss = 0.03107351\n",
      "Iteration 312, loss = 0.03178952\n",
      "Iteration 313, loss = 0.03109762\n",
      "Iteration 314, loss = 0.03079769\n",
      "Iteration 315, loss = 0.03147879\n",
      "Iteration 316, loss = 0.03064233\n",
      "Iteration 317, loss = 0.03164280\n",
      "Iteration 318, loss = 0.03014407\n",
      "Iteration 1, loss = 1.04072334\n",
      "Iteration 319, loss = 0.03151482\n",
      "Iteration 2, loss = 0.89985623\n",
      "Iteration 320, loss = 0.03292069\n",
      "Iteration 3, loss = 0.80349077\n",
      "Iteration 321, loss = 0.03627460\n",
      "Iteration 4, loss = 0.73086175\n",
      "Iteration 322, loss = 0.03191492\n",
      "Iteration 5, loss = 0.67456846\n",
      "Iteration 323, loss = 0.03075847\n",
      "Iteration 6, loss = 0.62849602\n",
      "Iteration 324, loss = 0.02964298\n",
      "Iteration 7, loss = 0.59065981\n",
      "Iteration 325, loss = 0.03062835\n",
      "Iteration 8, loss = 0.56030762\n",
      "Iteration 326, loss = 0.02881735\n",
      "Iteration 9, loss = 0.53434090\n",
      "Iteration 327, loss = 0.03034601\n",
      "Iteration 10, loss = 0.50996647\n",
      "Iteration 328, loss = 0.03050854\n",
      "Iteration 11, loss = 0.49110999\n",
      "Iteration 329, loss = 0.03025629\n",
      "Iteration 12, loss = 0.47360353\n",
      "Iteration 330, loss = 0.02953104\n",
      "Iteration 13, loss = 0.45830740\n",
      "Iteration 331, loss = 0.03088453\n",
      "Iteration 14, loss = 0.44393183\n",
      "Iteration 332, loss = 0.02784827\n",
      "Iteration 15, loss = 0.43191304\n",
      "Iteration 333, loss = 0.02964975\n",
      "Iteration 16, loss = 0.42075608\n",
      "Iteration 334, loss = 0.02964723\n",
      "Iteration 17, loss = 0.41012713\n",
      "Iteration 335, loss = 0.02900918\n",
      "Iteration 18, loss = 0.40060995\n",
      "Iteration 336, loss = 0.02921076\n",
      "Iteration 19, loss = 0.39135879\n",
      "Iteration 337, loss = 0.02922947\n",
      "Iteration 20, loss = 0.38326114\n",
      "Iteration 338, loss = 0.02892074\n",
      "Iteration 21, loss = 0.37545697\n",
      "Iteration 339, loss = 0.02794067\n",
      "Iteration 22, loss = 0.36770087\n",
      "Iteration 340, loss = 0.03108488\n",
      "Iteration 23, loss = 0.36094169\n",
      "Iteration 341, loss = 0.02943776\n",
      "Iteration 24, loss = 0.35490002\n",
      "Iteration 342, loss = 0.02789063\n",
      "Iteration 25, loss = 0.34790575\n",
      "Iteration 343, loss = 0.03181475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 0.34197604\n",
      "Iteration 27, loss = 0.33598743\n",
      "Iteration 28, loss = 0.33100995\n",
      "Iteration 29, loss = 0.32539338\n",
      "Iteration 30, loss = 0.31982933\n",
      "Iteration 31, loss = 0.31434987\n",
      "Iteration 32, loss = 0.30917990\n",
      "Iteration 33, loss = 0.30462042\n",
      "Iteration 34, loss = 0.29934797\n",
      "Iteration 35, loss = 0.29580885\n",
      "Iteration 36, loss = 0.29163087\n",
      "Iteration 37, loss = 0.28614367\n",
      "Iteration 38, loss = 0.28154717\n",
      "Iteration 39, loss = 0.27798349\n",
      "Iteration 40, loss = 0.27323464\n",
      "Iteration 41, loss = 0.26904305\n",
      "Iteration 42, loss = 0.26487064\n",
      "Iteration 43, loss = 0.26083843\n",
      "Iteration 44, loss = 0.25657173\n",
      "Iteration 1, loss = 1.19750465\n",
      "Iteration 45, loss = 0.25246174\n",
      "Iteration 2, loss = 1.08175068\n",
      "Iteration 46, loss = 0.24872843\n",
      "Iteration 3, loss = 0.98459713\n",
      "Iteration 47, loss = 0.24567889\n",
      "Iteration 4, loss = 0.89258821\n",
      "Iteration 48, loss = 0.24300383\n",
      "Iteration 5, loss = 0.81437513\n",
      "Iteration 49, loss = 0.23749955\n",
      "Iteration 6, loss = 0.74302422\n",
      "Iteration 50, loss = 0.23471630\n",
      "Iteration 7, loss = 0.68292494\n",
      "Iteration 51, loss = 0.23126856\n",
      "Iteration 8, loss = 0.63282243\n",
      "Iteration 52, loss = 0.22627429\n",
      "Iteration 9, loss = 0.59116027\n",
      "Iteration 53, loss = 0.22354886\n",
      "Iteration 10, loss = 0.55563469\n",
      "Iteration 54, loss = 0.21987189\n",
      "Iteration 11, loss = 0.52753758\n",
      "Iteration 55, loss = 0.21769048\n",
      "Iteration 12, loss = 0.50348755\n",
      "Iteration 56, loss = 0.21707784\n",
      "Iteration 13, loss = 0.48315153\n",
      "Iteration 57, loss = 0.20946985\n",
      "Iteration 14, loss = 0.46721187\n",
      "Iteration 58, loss = 0.20850440\n",
      "Iteration 15, loss = 0.45127539\n",
      "Iteration 59, loss = 0.20394802\n",
      "Iteration 16, loss = 0.43799573\n",
      "Iteration 60, loss = 0.20178770\n",
      "Iteration 17, loss = 0.42549853\n",
      "Iteration 61, loss = 0.19824035\n",
      "Iteration 18, loss = 0.41509627\n",
      "Iteration 62, loss = 0.19770535\n",
      "Iteration 19, loss = 0.40530429\n",
      "Iteration 63, loss = 0.19406192\n",
      "Iteration 20, loss = 0.39598963\n",
      "Iteration 64, loss = 0.19194297\n",
      "Iteration 21, loss = 0.38805231\n",
      "Iteration 65, loss = 0.18859376\n",
      "Iteration 22, loss = 0.38021478\n",
      "Iteration 66, loss = 0.18623865\n",
      "Iteration 23, loss = 0.37321223\n",
      "Iteration 67, loss = 0.18487337\n",
      "Iteration 24, loss = 0.36606573\n",
      "Iteration 68, loss = 0.18117080\n",
      "Iteration 25, loss = 0.35953855\n",
      "Iteration 69, loss = 0.18038285\n",
      "Iteration 26, loss = 0.35360355\n",
      "Iteration 70, loss = 0.17765472\n",
      "Iteration 27, loss = 0.34762849\n",
      "Iteration 71, loss = 0.17484081\n",
      "Iteration 28, loss = 0.34145626\n",
      "Iteration 72, loss = 0.17359180\n",
      "Iteration 29, loss = 0.33614132\n",
      "Iteration 73, loss = 0.17146126\n",
      "Iteration 30, loss = 0.33107362\n",
      "Iteration 74, loss = 0.16923333\n",
      "Iteration 31, loss = 0.32637055\n",
      "Iteration 75, loss = 0.16751312\n",
      "Iteration 32, loss = 0.32075223\n",
      "Iteration 76, loss = 0.16542136\n",
      "Iteration 33, loss = 0.31557210\n",
      "Iteration 77, loss = 0.16447181\n",
      "Iteration 34, loss = 0.31130836\n",
      "Iteration 35, loss = 0.30676625\n",
      "Iteration 78, loss = 0.16114877\n",
      "Iteration 36, loss = 0.30189883\n",
      "Iteration 79, loss = 0.16108988\n",
      "Iteration 37, loss = 0.29724231\n",
      "Iteration 80, loss = 0.15832716\n",
      "Iteration 38, loss = 0.29280903\n",
      "Iteration 81, loss = 0.15773768\n",
      "Iteration 39, loss = 0.28847824\n",
      "Iteration 82, loss = 0.15544785\n",
      "Iteration 40, loss = 0.28472807\n",
      "Iteration 83, loss = 0.15364140\n",
      "Iteration 41, loss = 0.27965458\n",
      "Iteration 84, loss = 0.15139877\n",
      "Iteration 42, loss = 0.27660702\n",
      "Iteration 85, loss = 0.14989980\n",
      "Iteration 43, loss = 0.27249017\n",
      "Iteration 86, loss = 0.14800446\n",
      "Iteration 44, loss = 0.26805388\n",
      "Iteration 87, loss = 0.14666511\n",
      "Iteration 45, loss = 0.26466037\n",
      "Iteration 88, loss = 0.14633931\n",
      "Iteration 46, loss = 0.26127122\n",
      "Iteration 89, loss = 0.14466249\n",
      "Iteration 47, loss = 0.25728081\n",
      "Iteration 90, loss = 0.14258702\n",
      "Iteration 48, loss = 0.25320375\n",
      "Iteration 91, loss = 0.14180081\n",
      "Iteration 49, loss = 0.24942769\n",
      "Iteration 92, loss = 0.13923471\n",
      "Iteration 50, loss = 0.24644555\n",
      "Iteration 93, loss = 0.13807269\n",
      "Iteration 51, loss = 0.24199969\n",
      "Iteration 94, loss = 0.13653623\n",
      "Iteration 52, loss = 0.23876797\n",
      "Iteration 95, loss = 0.13555439\n",
      "Iteration 53, loss = 0.23516247\n",
      "Iteration 96, loss = 0.13448031\n",
      "Iteration 54, loss = 0.23172204\n",
      "Iteration 97, loss = 0.13323866\n",
      "Iteration 55, loss = 0.22804558\n",
      "Iteration 98, loss = 0.13133485\n",
      "Iteration 56, loss = 0.22510702\n",
      "Iteration 99, loss = 0.13075777\n",
      "Iteration 57, loss = 0.22203347\n",
      "Iteration 100, loss = 0.12978037\n",
      "Iteration 58, loss = 0.21909413\n",
      "Iteration 101, loss = 0.12982977\n",
      "Iteration 59, loss = 0.21565107\n",
      "Iteration 102, loss = 0.12656179\n",
      "Iteration 60, loss = 0.21252556\n",
      "Iteration 103, loss = 0.12619538\n",
      "Iteration 61, loss = 0.21060295\n",
      "Iteration 104, loss = 0.12403948\n",
      "Iteration 62, loss = 0.20666372\n",
      "Iteration 105, loss = 0.12326372\n",
      "Iteration 63, loss = 0.20406014\n",
      "Iteration 106, loss = 0.12245661\n",
      "Iteration 64, loss = 0.20141229\n",
      "Iteration 107, loss = 0.12082451\n",
      "Iteration 65, loss = 0.19935786\n",
      "Iteration 108, loss = 0.11977110\n",
      "Iteration 66, loss = 0.19681882\n",
      "Iteration 109, loss = 0.12096297\n",
      "Iteration 67, loss = 0.19442705\n",
      "Iteration 110, loss = 0.11852031\n",
      "Iteration 68, loss = 0.19216677\n",
      "Iteration 111, loss = 0.11553779\n",
      "Iteration 69, loss = 0.18907400\n",
      "Iteration 112, loss = 0.11658205\n",
      "Iteration 70, loss = 0.18724733\n",
      "Iteration 113, loss = 0.11572824\n",
      "Iteration 71, loss = 0.18503799\n",
      "Iteration 114, loss = 0.11354758\n",
      "Iteration 72, loss = 0.18204996\n",
      "Iteration 115, loss = 0.11310261\n",
      "Iteration 73, loss = 0.18145181\n",
      "Iteration 116, loss = 0.11155221\n",
      "Iteration 74, loss = 0.17892838\n",
      "Iteration 117, loss = 0.11232975\n",
      "Iteration 75, loss = 0.17756764\n",
      "Iteration 118, loss = 0.11305765\n",
      "Iteration 76, loss = 0.17524614\n",
      "Iteration 119, loss = 0.11359397\n",
      "Iteration 77, loss = 0.17202763\n",
      "Iteration 120, loss = 0.10890350\n",
      "Iteration 78, loss = 0.17105928\n",
      "Iteration 121, loss = 0.10732062\n",
      "Iteration 79, loss = 0.16833497\n",
      "Iteration 122, loss = 0.10697978\n",
      "Iteration 80, loss = 0.16682975\n",
      "Iteration 123, loss = 0.10501740\n",
      "Iteration 81, loss = 0.16482353\n",
      "Iteration 124, loss = 0.10287290\n",
      "Iteration 82, loss = 0.16383722\n",
      "Iteration 125, loss = 0.10324902\n",
      "Iteration 83, loss = 0.16264825\n",
      "Iteration 126, loss = 0.10199882\n",
      "Iteration 84, loss = 0.15981709\n",
      "Iteration 127, loss = 0.10056014\n",
      "Iteration 85, loss = 0.15737883\n",
      "Iteration 128, loss = 0.10175496\n",
      "Iteration 86, loss = 0.15453012\n",
      "Iteration 129, loss = 0.09938137\n",
      "Iteration 87, loss = 0.15494337\n",
      "Iteration 130, loss = 0.10138584\n",
      "Iteration 88, loss = 0.15133743\n",
      "Iteration 131, loss = 0.09926221\n",
      "Iteration 89, loss = 0.15066679\n",
      "Iteration 132, loss = 0.09723133\n",
      "Iteration 90, loss = 0.14846099\n",
      "Iteration 133, loss = 0.09700701\n",
      "Iteration 91, loss = 0.14674940\n",
      "Iteration 134, loss = 0.09525311\n",
      "Iteration 92, loss = 0.14527267\n",
      "Iteration 135, loss = 0.09435756\n",
      "Iteration 93, loss = 0.14369321\n",
      "Iteration 136, loss = 0.09369138\n",
      "Iteration 94, loss = 0.14230803\n",
      "Iteration 137, loss = 0.09279708\n",
      "Iteration 95, loss = 0.14107196\n",
      "Iteration 138, loss = 0.09271512\n",
      "Iteration 96, loss = 0.14049916\n",
      "Iteration 139, loss = 0.09206692\n",
      "Iteration 97, loss = 0.13910050\n",
      "Iteration 140, loss = 0.08967696\n",
      "Iteration 98, loss = 0.13577702\n",
      "Iteration 141, loss = 0.09091042\n",
      "Iteration 99, loss = 0.13544874\n",
      "Iteration 142, loss = 0.09107634\n",
      "Iteration 100, loss = 0.13299098\n",
      "Iteration 143, loss = 0.08801677\n",
      "Iteration 101, loss = 0.13167836\n",
      "Iteration 144, loss = 0.08808096\n",
      "Iteration 102, loss = 0.13062547\n",
      "Iteration 145, loss = 0.08705102\n",
      "Iteration 103, loss = 0.12934293\n",
      "Iteration 146, loss = 0.08616108\n",
      "Iteration 104, loss = 0.12826304\n",
      "Iteration 147, loss = 0.08554450\n",
      "Iteration 105, loss = 0.12619983\n",
      "Iteration 148, loss = 0.08536130\n",
      "Iteration 106, loss = 0.12603499\n",
      "Iteration 149, loss = 0.08315587\n",
      "Iteration 107, loss = 0.12381723\n",
      "Iteration 150, loss = 0.08307589\n",
      "Iteration 108, loss = 0.12352378\n",
      "Iteration 151, loss = 0.08265038\n",
      "Iteration 109, loss = 0.12094347\n",
      "Iteration 152, loss = 0.08071201\n",
      "Iteration 110, loss = 0.11990403\n",
      "Iteration 153, loss = 0.08088285\n",
      "Iteration 111, loss = 0.11886566\n",
      "Iteration 154, loss = 0.08074718\n",
      "Iteration 112, loss = 0.11726305\n",
      "Iteration 155, loss = 0.07929446\n",
      "Iteration 113, loss = 0.11648934\n",
      "Iteration 156, loss = 0.07913738\n",
      "Iteration 114, loss = 0.11616946\n",
      "Iteration 157, loss = 0.07918398\n",
      "Iteration 115, loss = 0.11386292\n",
      "Iteration 158, loss = 0.07762001\n",
      "Iteration 116, loss = 0.11230199\n",
      "Iteration 159, loss = 0.07795105\n",
      "Iteration 117, loss = 0.11331458\n",
      "Iteration 160, loss = 0.07801088\n",
      "Iteration 118, loss = 0.11017298\n",
      "Iteration 161, loss = 0.07736219\n",
      "Iteration 119, loss = 0.11064793\n",
      "Iteration 162, loss = 0.07673096\n",
      "Iteration 120, loss = 0.10870546\n",
      "Iteration 163, loss = 0.07677312\n",
      "Iteration 121, loss = 0.10847736\n",
      "Iteration 164, loss = 0.07616221\n",
      "Iteration 122, loss = 0.10838827\n",
      "Iteration 165, loss = 0.07522232\n",
      "Iteration 123, loss = 0.10539173\n",
      "Iteration 166, loss = 0.07366620\n",
      "Iteration 124, loss = 0.10496769\n",
      "Iteration 167, loss = 0.07280989\n",
      "Iteration 125, loss = 0.10470684\n",
      "Iteration 168, loss = 0.07169539\n",
      "Iteration 126, loss = 0.10039842\n",
      "Iteration 169, loss = 0.07060406\n",
      "Iteration 127, loss = 0.10157358\n",
      "Iteration 170, loss = 0.07031079\n",
      "Iteration 128, loss = 0.10059764\n",
      "Iteration 171, loss = 0.06898605\n",
      "Iteration 129, loss = 0.09810083\n",
      "Iteration 172, loss = 0.06955065\n",
      "Iteration 130, loss = 0.09743503\n",
      "Iteration 173, loss = 0.06806197\n",
      "Iteration 131, loss = 0.09601366\n",
      "Iteration 174, loss = 0.06771843\n",
      "Iteration 132, loss = 0.09596822\n",
      "Iteration 175, loss = 0.06786086\n",
      "Iteration 133, loss = 0.09400815\n",
      "Iteration 176, loss = 0.06625816\n",
      "Iteration 134, loss = 0.09359734\n",
      "Iteration 177, loss = 0.06633639\n",
      "Iteration 135, loss = 0.09260606\n",
      "Iteration 178, loss = 0.06594180\n",
      "Iteration 136, loss = 0.09227565\n",
      "Iteration 179, loss = 0.06740535\n",
      "Iteration 137, loss = 0.08986618\n",
      "Iteration 180, loss = 0.06543923\n",
      "Iteration 138, loss = 0.09002520\n",
      "Iteration 181, loss = 0.06558044\n",
      "Iteration 139, loss = 0.08904691\n",
      "Iteration 182, loss = 0.06388054\n",
      "Iteration 140, loss = 0.08871089\n",
      "Iteration 183, loss = 0.06381395\n",
      "Iteration 141, loss = 0.08707236\n",
      "Iteration 184, loss = 0.06271496\n",
      "Iteration 142, loss = 0.08649088\n",
      "Iteration 185, loss = 0.06206013\n",
      "Iteration 143, loss = 0.08635269\n",
      "Iteration 186, loss = 0.06189517\n",
      "Iteration 144, loss = 0.08433542\n",
      "Iteration 187, loss = 0.06274912\n",
      "Iteration 145, loss = 0.08302634\n",
      "Iteration 188, loss = 0.06110005\n",
      "Iteration 146, loss = 0.08445465\n",
      "Iteration 189, loss = 0.06243685\n",
      "Iteration 147, loss = 0.08550947\n",
      "Iteration 190, loss = 0.06101386\n",
      "Iteration 148, loss = 0.08404573\n",
      "Iteration 191, loss = 0.05905684\n",
      "Iteration 149, loss = 0.08309813\n",
      "Iteration 192, loss = 0.06082943\n",
      "Iteration 150, loss = 0.08110575\n",
      "Iteration 193, loss = 0.05834988\n",
      "Iteration 151, loss = 0.07923288\n",
      "Iteration 194, loss = 0.05877534\n",
      "Iteration 152, loss = 0.07730568\n",
      "Iteration 195, loss = 0.05772671\n",
      "Iteration 153, loss = 0.07707150\n",
      "Iteration 196, loss = 0.05689774\n",
      "Iteration 154, loss = 0.07528734\n",
      "Iteration 197, loss = 0.05727136\n",
      "Iteration 155, loss = 0.07579150\n",
      "Iteration 198, loss = 0.05638650\n",
      "Iteration 156, loss = 0.07418416\n",
      "Iteration 199, loss = 0.05647747\n",
      "Iteration 157, loss = 0.07397939\n",
      "Iteration 200, loss = 0.05506163\n",
      "Iteration 158, loss = 0.07307706\n",
      "Iteration 201, loss = 0.05448051\n",
      "Iteration 159, loss = 0.07148895\n",
      "Iteration 202, loss = 0.05464834\n",
      "Iteration 160, loss = 0.07065112\n",
      "Iteration 203, loss = 0.05415396\n",
      "Iteration 161, loss = 0.07157420\n",
      "Iteration 204, loss = 0.05351183\n",
      "Iteration 162, loss = 0.06960140\n",
      "Iteration 205, loss = 0.05402571\n",
      "Iteration 163, loss = 0.07058249\n",
      "Iteration 206, loss = 0.05301408\n",
      "Iteration 164, loss = 0.06847617\n",
      "Iteration 207, loss = 0.05289563\n",
      "Iteration 165, loss = 0.06730359\n",
      "Iteration 208, loss = 0.05313421\n",
      "Iteration 166, loss = 0.06771406\n",
      "Iteration 209, loss = 0.05198626\n",
      "Iteration 167, loss = 0.06667063\n",
      "Iteration 210, loss = 0.05228258\n",
      "Iteration 168, loss = 0.06583629\n",
      "Iteration 211, loss = 0.05176742\n",
      "Iteration 169, loss = 0.06547417\n",
      "Iteration 212, loss = 0.05172096\n",
      "Iteration 170, loss = 0.06458933\n",
      "Iteration 213, loss = 0.05015105\n",
      "Iteration 171, loss = 0.06317704\n",
      "Iteration 214, loss = 0.05055586\n",
      "Iteration 172, loss = 0.06278113\n",
      "Iteration 215, loss = 0.04985404\n",
      "Iteration 173, loss = 0.06266793\n",
      "Iteration 216, loss = 0.04954154\n",
      "Iteration 174, loss = 0.06300812\n",
      "Iteration 217, loss = 0.05116270\n",
      "Iteration 175, loss = 0.06119030\n",
      "Iteration 218, loss = 0.05055386\n",
      "Iteration 176, loss = 0.06130594\n",
      "Iteration 219, loss = 0.04928859\n",
      "Iteration 177, loss = 0.06030036\n",
      "Iteration 220, loss = 0.04898683\n",
      "Iteration 178, loss = 0.05942458\n",
      "Iteration 221, loss = 0.04875957\n",
      "Iteration 179, loss = 0.05882592\n",
      "Iteration 222, loss = 0.04728218\n",
      "Iteration 180, loss = 0.05764401\n",
      "Iteration 223, loss = 0.04715154\n",
      "Iteration 181, loss = 0.05697073\n",
      "Iteration 224, loss = 0.04686582\n",
      "Iteration 182, loss = 0.05755172\n",
      "Iteration 225, loss = 0.04626432\n",
      "Iteration 183, loss = 0.05629589\n",
      "Iteration 226, loss = 0.04652872\n",
      "Iteration 184, loss = 0.05565502\n",
      "Iteration 227, loss = 0.04609290\n",
      "Iteration 185, loss = 0.05595900\n",
      "Iteration 228, loss = 0.04564591\n",
      "Iteration 186, loss = 0.05501869\n",
      "Iteration 229, loss = 0.04578612\n",
      "Iteration 187, loss = 0.05398870\n",
      "Iteration 230, loss = 0.04581758\n",
      "Iteration 188, loss = 0.05334866\n",
      "Iteration 231, loss = 0.04553635\n",
      "Iteration 189, loss = 0.05307532\n",
      "Iteration 232, loss = 0.04552533\n",
      "Iteration 190, loss = 0.05263504\n",
      "Iteration 233, loss = 0.04443767\n",
      "Iteration 191, loss = 0.05236829\n",
      "Iteration 234, loss = 0.04406671\n",
      "Iteration 192, loss = 0.05138612\n",
      "Iteration 235, loss = 0.04379595\n",
      "Iteration 193, loss = 0.05090378\n",
      "Iteration 236, loss = 0.04374968\n",
      "Iteration 194, loss = 0.05059913\n",
      "Iteration 237, loss = 0.04391962\n",
      "Iteration 195, loss = 0.05024583\n",
      "Iteration 238, loss = 0.04357079\n",
      "Iteration 196, loss = 0.04931130\n",
      "Iteration 239, loss = 0.04306618\n",
      "Iteration 197, loss = 0.04857188\n",
      "Iteration 240, loss = 0.04224241\n",
      "Iteration 198, loss = 0.04851526\n",
      "Iteration 241, loss = 0.04362288\n",
      "Iteration 199, loss = 0.04796157\n",
      "Iteration 242, loss = 0.04429783\n",
      "Iteration 200, loss = 0.04717216\n",
      "Iteration 243, loss = 0.04332526\n",
      "Iteration 201, loss = 0.04989968\n",
      "Iteration 244, loss = 0.04161439\n",
      "Iteration 202, loss = 0.04702960\n",
      "Iteration 245, loss = 0.04234775\n",
      "Iteration 203, loss = 0.04600029\n",
      "Iteration 246, loss = 0.04124042\n",
      "Iteration 204, loss = 0.04540185\n",
      "Iteration 247, loss = 0.04642689\n",
      "Iteration 205, loss = 0.04534428\n",
      "Iteration 248, loss = 0.04347151\n",
      "Iteration 206, loss = 0.04485271\n",
      "Iteration 249, loss = 0.04305608\n",
      "Iteration 207, loss = 0.04419101\n",
      "Iteration 250, loss = 0.04032804\n",
      "Iteration 208, loss = 0.04456803\n",
      "Iteration 251, loss = 0.04222027\n",
      "Iteration 209, loss = 0.04386370\n",
      "Iteration 252, loss = 0.04088688\n",
      "Iteration 210, loss = 0.04368084\n",
      "Iteration 253, loss = 0.04055514\n",
      "Iteration 211, loss = 0.04432470\n",
      "Iteration 254, loss = 0.04001688\n",
      "Iteration 212, loss = 0.04335202\n",
      "Iteration 255, loss = 0.03899806\n",
      "Iteration 213, loss = 0.04401025\n",
      "Iteration 256, loss = 0.03916953\n",
      "Iteration 214, loss = 0.04193183\n",
      "Iteration 257, loss = 0.03959720\n",
      "Iteration 215, loss = 0.04115192\n",
      "Iteration 258, loss = 0.03833824\n",
      "Iteration 216, loss = 0.04171232\n",
      "Iteration 259, loss = 0.03843567\n",
      "Iteration 217, loss = 0.03999986\n",
      "Iteration 260, loss = 0.03821631\n",
      "Iteration 218, loss = 0.03959289\n",
      "Iteration 261, loss = 0.03771948\n",
      "Iteration 219, loss = 0.04120684\n",
      "Iteration 262, loss = 0.03790671\n",
      "Iteration 220, loss = 0.04135047\n",
      "Iteration 263, loss = 0.03796053\n",
      "Iteration 221, loss = 0.03896143\n",
      "Iteration 264, loss = 0.03709285\n",
      "Iteration 222, loss = 0.03882085\n",
      "Iteration 265, loss = 0.03858361\n",
      "Iteration 223, loss = 0.03866462\n",
      "Iteration 266, loss = 0.03691412\n",
      "Iteration 224, loss = 0.04016970\n",
      "Iteration 267, loss = 0.03774345\n",
      "Iteration 225, loss = 0.03916908\n",
      "Iteration 268, loss = 0.03835648\n",
      "Iteration 226, loss = 0.04419642\n",
      "Iteration 269, loss = 0.03738754\n",
      "Iteration 227, loss = 0.03899753\n",
      "Iteration 270, loss = 0.03665327\n",
      "Iteration 228, loss = 0.03909629\n",
      "Iteration 271, loss = 0.03626244\n",
      "Iteration 229, loss = 0.03767878\n",
      "Iteration 272, loss = 0.03647974\n",
      "Iteration 230, loss = 0.03609184\n",
      "Iteration 273, loss = 0.03559228\n",
      "Iteration 231, loss = 0.03692719\n",
      "Iteration 274, loss = 0.03547116\n",
      "Iteration 232, loss = 0.03553190\n",
      "Iteration 275, loss = 0.03586686\n",
      "Iteration 233, loss = 0.03556558\n",
      "Iteration 276, loss = 0.03531726\n",
      "Iteration 234, loss = 0.03488274\n",
      "Iteration 277, loss = 0.03543369\n",
      "Iteration 235, loss = 0.03409735\n",
      "Iteration 278, loss = 0.03692719\n",
      "Iteration 236, loss = 0.03499743\n",
      "Iteration 279, loss = 0.03446670\n",
      "Iteration 237, loss = 0.03450524\n",
      "Iteration 280, loss = 0.03499895\n",
      "Iteration 238, loss = 0.03337396\n",
      "Iteration 281, loss = 0.03555587\n",
      "Iteration 239, loss = 0.03327383\n",
      "Iteration 282, loss = 0.03564106\n",
      "Iteration 240, loss = 0.03524164\n",
      "Iteration 283, loss = 0.03459730\n",
      "Iteration 241, loss = 0.03499489\n",
      "Iteration 284, loss = 0.03597278\n",
      "Iteration 242, loss = 0.03186069\n",
      "Iteration 285, loss = 0.03408026\n",
      "Iteration 243, loss = 0.03251695\n",
      "Iteration 286, loss = 0.03521504\n",
      "Iteration 244, loss = 0.03375568\n",
      "Iteration 245, loss = 0.03127487\n",
      "Iteration 287, loss = 0.03468070\n",
      "Iteration 246, loss = 0.03178407\n",
      "Iteration 288, loss = 0.03382188\n",
      "Iteration 247, loss = 0.03103919\n",
      "Iteration 289, loss = 0.03491293\n",
      "Iteration 248, loss = 0.03031547\n",
      "Iteration 290, loss = 0.03300643\n",
      "Iteration 249, loss = 0.03031111\n",
      "Iteration 291, loss = 0.03350642\n",
      "Iteration 250, loss = 0.02970762\n",
      "Iteration 292, loss = 0.03346410\n",
      "Iteration 251, loss = 0.02941674\n",
      "Iteration 293, loss = 0.03345194\n",
      "Iteration 252, loss = 0.02985284\n",
      "Iteration 294, loss = 0.03408687\n",
      "Iteration 253, loss = 0.03030376\n",
      "Iteration 295, loss = 0.03382011\n",
      "Iteration 254, loss = 0.02948552\n",
      "Iteration 296, loss = 0.03387409\n",
      "Iteration 255, loss = 0.02856440\n",
      "Iteration 297, loss = 0.03339948\n",
      "Iteration 256, loss = 0.02950778\n",
      "Iteration 298, loss = 0.03244328\n",
      "Iteration 257, loss = 0.02796538\n",
      "Iteration 299, loss = 0.03213383\n",
      "Iteration 258, loss = 0.02823602\n",
      "Iteration 300, loss = 0.03192567\n",
      "Iteration 259, loss = 0.02843173\n",
      "Iteration 301, loss = 0.03146865\n",
      "Iteration 260, loss = 0.02838683\n",
      "Iteration 302, loss = 0.03135633\n",
      "Iteration 261, loss = 0.02681914\n",
      "Iteration 303, loss = 0.03107026\n",
      "Iteration 262, loss = 0.02770818\n",
      "Iteration 304, loss = 0.03132016\n",
      "Iteration 263, loss = 0.02686838\n",
      "Iteration 305, loss = 0.03117079\n",
      "Iteration 264, loss = 0.02640438\n",
      "Iteration 306, loss = 0.03080636\n",
      "Iteration 265, loss = 0.02721193\n",
      "Iteration 307, loss = 0.03049309\n",
      "Iteration 266, loss = 0.02593376\n",
      "Iteration 308, loss = 0.03090033\n",
      "Iteration 267, loss = 0.02883456\n",
      "Iteration 309, loss = 0.03149880\n",
      "Iteration 268, loss = 0.02793107\n",
      "Iteration 310, loss = 0.03135737\n",
      "Iteration 269, loss = 0.02900093\n",
      "Iteration 311, loss = 0.02996659\n",
      "Iteration 270, loss = 0.02601288\n",
      "Iteration 312, loss = 0.03094379\n",
      "Iteration 271, loss = 0.02567137\n",
      "Iteration 313, loss = 0.03060822\n",
      "Iteration 272, loss = 0.02767610\n",
      "Iteration 314, loss = 0.03024895\n",
      "Iteration 273, loss = 0.03060751\n",
      "Iteration 315, loss = 0.03006534\n",
      "Iteration 274, loss = 0.02487070\n",
      "Iteration 316, loss = 0.02994138\n",
      "Iteration 275, loss = 0.02508342\n",
      "Iteration 317, loss = 0.03037412\n",
      "Iteration 276, loss = 0.02407099\n",
      "Iteration 318, loss = 0.02921740\n",
      "Iteration 277, loss = 0.02417729\n",
      "Iteration 319, loss = 0.02960543\n",
      "Iteration 278, loss = 0.02366024\n",
      "Iteration 320, loss = 0.03025626\n",
      "Iteration 279, loss = 0.02342328\n",
      "Iteration 321, loss = 0.02940490\n",
      "Iteration 280, loss = 0.02334646\n",
      "Iteration 322, loss = 0.03096569\n",
      "Iteration 281, loss = 0.02268333\n",
      "Iteration 323, loss = 0.03125438\n",
      "Iteration 282, loss = 0.02309922\n",
      "Iteration 324, loss = 0.03142659\n",
      "Iteration 283, loss = 0.02259702\n",
      "Iteration 325, loss = 0.03169454\n",
      "Iteration 284, loss = 0.02314852\n",
      "Iteration 326, loss = 0.03108179\n",
      "Iteration 285, loss = 0.02211141\n",
      "Iteration 327, loss = 0.03163933\n",
      "Iteration 286, loss = 0.02277518\n",
      "Iteration 328, loss = 0.02916004\n",
      "Iteration 287, loss = 0.02346813\n",
      "Iteration 329, loss = 0.02939785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 288, loss = 0.02197070\n",
      "Iteration 289, loss = 0.02231952\n",
      "Iteration 290, loss = 0.02222545\n",
      "Iteration 291, loss = 0.02131117\n",
      "Iteration 292, loss = 0.02153323\n",
      "Iteration 293, loss = 0.02127704\n",
      "Iteration 294, loss = 0.02042434\n",
      "Iteration 295, loss = 0.02116812\n",
      "Iteration 296, loss = 0.02125912\n",
      "Iteration 297, loss = 0.02099377\n",
      "Iteration 298, loss = 0.02103313\n",
      "Iteration 299, loss = 0.02034039\n",
      "Iteration 300, loss = 0.01978785\n",
      "Iteration 301, loss = 0.02019698\n",
      "Iteration 302, loss = 0.02040940\n",
      "Iteration 303, loss = 0.01933236\n",
      "Iteration 304, loss = 0.02039602\n",
      "Iteration 305, loss = 0.01913931\n",
      "Iteration 1, loss = 1.25864988\n",
      "Iteration 306, loss = 0.01910692\n",
      "Iteration 2, loss = 1.02933264\n",
      "Iteration 307, loss = 0.01881841\n",
      "Iteration 3, loss = 0.90905817\n",
      "Iteration 308, loss = 0.01873551\n",
      "Iteration 4, loss = 0.81545330\n",
      "Iteration 309, loss = 0.02188624\n",
      "Iteration 5, loss = 0.73503054\n",
      "Iteration 310, loss = 0.02088494\n",
      "Iteration 6, loss = 0.66981812\n",
      "Iteration 311, loss = 0.01993202\n",
      "Iteration 7, loss = 0.62148918\n",
      "Iteration 312, loss = 0.01862942\n",
      "Iteration 8, loss = 0.58399400\n",
      "Iteration 313, loss = 0.01905593\n",
      "Iteration 9, loss = 0.55213731\n",
      "Iteration 314, loss = 0.01837831\n",
      "Iteration 10, loss = 0.52586081\n",
      "Iteration 315, loss = 0.01763648\n",
      "Iteration 11, loss = 0.50406413\n",
      "Iteration 316, loss = 0.01784081\n",
      "Iteration 12, loss = 0.48440419\n",
      "Iteration 317, loss = 0.01753342\n",
      "Iteration 13, loss = 0.46782210\n",
      "Iteration 318, loss = 0.01746846\n",
      "Iteration 14, loss = 0.45229973\n",
      "Iteration 319, loss = 0.01720080\n",
      "Iteration 15, loss = 0.43852075\n",
      "Iteration 320, loss = 0.01710287\n",
      "Iteration 16, loss = 0.42683649\n",
      "Iteration 321, loss = 0.01699763\n",
      "Iteration 17, loss = 0.41517186\n",
      "Iteration 322, loss = 0.01723939\n",
      "Iteration 18, loss = 0.40520186\n",
      "Iteration 323, loss = 0.01668057\n",
      "Iteration 19, loss = 0.39588240\n",
      "Iteration 324, loss = 0.01665788\n",
      "Iteration 20, loss = 0.38707743\n",
      "Iteration 325, loss = 0.01652278\n",
      "Iteration 21, loss = 0.37936761\n",
      "Iteration 326, loss = 0.01657470\n",
      "Iteration 22, loss = 0.37168638\n",
      "Iteration 327, loss = 0.01604019\n",
      "Iteration 23, loss = 0.36515914\n",
      "Iteration 328, loss = 0.01625092\n",
      "Iteration 24, loss = 0.35851754\n",
      "Iteration 329, loss = 0.01673405\n",
      "Iteration 25, loss = 0.35188591\n",
      "Iteration 330, loss = 0.01555999\n",
      "Iteration 26, loss = 0.34569772\n",
      "Iteration 331, loss = 0.01639200\n",
      "Iteration 27, loss = 0.33981650\n",
      "Iteration 332, loss = 0.01584174\n",
      "Iteration 28, loss = 0.33417253\n",
      "Iteration 333, loss = 0.01552734\n",
      "Iteration 29, loss = 0.32943570\n",
      "Iteration 334, loss = 0.01587296\n",
      "Iteration 30, loss = 0.32351720\n",
      "Iteration 335, loss = 0.01544385\n",
      "Iteration 31, loss = 0.31883226\n",
      "Iteration 336, loss = 0.01519790\n",
      "Iteration 32, loss = 0.31390767\n",
      "Iteration 337, loss = 0.01549659\n",
      "Iteration 33, loss = 0.30870349\n",
      "Iteration 338, loss = 0.01535751\n",
      "Iteration 34, loss = 0.30407106\n",
      "Iteration 339, loss = 0.01517162\n",
      "Iteration 35, loss = 0.30012274\n",
      "Iteration 340, loss = 0.01476765\n",
      "Iteration 36, loss = 0.29518407\n",
      "Iteration 341, loss = 0.01545610\n",
      "Iteration 37, loss = 0.29337169\n",
      "Iteration 342, loss = 0.01516765\n",
      "Iteration 38, loss = 0.28685975\n",
      "Iteration 343, loss = 0.01504925\n",
      "Iteration 39, loss = 0.28391490\n",
      "Iteration 344, loss = 0.01554361\n",
      "Iteration 40, loss = 0.28020723\n",
      "Iteration 345, loss = 0.01436260\n",
      "Iteration 41, loss = 0.27581038\n",
      "Iteration 346, loss = 0.01475339\n",
      "Iteration 42, loss = 0.27250372\n",
      "Iteration 347, loss = 0.01472451\n",
      "Iteration 43, loss = 0.26726829\n",
      "Iteration 348, loss = 0.01436315\n",
      "Iteration 44, loss = 0.26441720\n",
      "Iteration 349, loss = 0.01397078\n",
      "Iteration 45, loss = 0.26009932\n",
      "Iteration 350, loss = 0.01430160\n",
      "Iteration 46, loss = 0.25645675\n",
      "Iteration 351, loss = 0.01383629\n",
      "Iteration 47, loss = 0.25338685\n",
      "Iteration 352, loss = 0.01409017\n",
      "Iteration 48, loss = 0.25125494\n",
      "Iteration 353, loss = 0.01404554\n",
      "Iteration 49, loss = 0.24637117\n",
      "Iteration 354, loss = 0.01404477\n",
      "Iteration 50, loss = 0.24310063\n",
      "Iteration 355, loss = 0.01361844\n",
      "Iteration 51, loss = 0.24012086\n",
      "Iteration 356, loss = 0.01338931\n",
      "Iteration 52, loss = 0.23652727\n",
      "Iteration 357, loss = 0.01390002\n",
      "Iteration 53, loss = 0.23335530\n",
      "Iteration 358, loss = 0.01421972\n",
      "Iteration 54, loss = 0.22983174\n",
      "Iteration 359, loss = 0.01314164\n",
      "Iteration 55, loss = 0.22793304\n",
      "Iteration 360, loss = 0.01485173\n",
      "Iteration 56, loss = 0.22401392\n",
      "Iteration 361, loss = 0.01302647\n",
      "Iteration 57, loss = 0.22232323\n",
      "Iteration 362, loss = 0.01294082\n",
      "Iteration 58, loss = 0.21980815\n",
      "Iteration 59, loss = 0.21634654\n",
      "Iteration 363, loss = 0.01272184\n",
      "Iteration 60, loss = 0.21357485\n",
      "Iteration 364, loss = 0.01265167\n",
      "Iteration 61, loss = 0.21113731\n",
      "Iteration 365, loss = 0.01390099\n",
      "Iteration 62, loss = 0.20861799\n",
      "Iteration 366, loss = 0.01379331\n",
      "Iteration 63, loss = 0.20580814\n",
      "Iteration 367, loss = 0.01222974\n",
      "Iteration 64, loss = 0.20366754\n",
      "Iteration 368, loss = 0.01300242\n",
      "Iteration 65, loss = 0.20130236\n",
      "Iteration 369, loss = 0.01334730\n",
      "Iteration 66, loss = 0.19971556\n",
      "Iteration 370, loss = 0.01225994\n",
      "Iteration 67, loss = 0.19742851\n",
      "Iteration 371, loss = 0.01226703\n",
      "Iteration 68, loss = 0.19452193\n",
      "Iteration 372, loss = 0.01232844\n",
      "Iteration 69, loss = 0.19171872\n",
      "Iteration 373, loss = 0.01195898\n",
      "Iteration 70, loss = 0.18929408\n",
      "Iteration 374, loss = 0.01187102\n",
      "Iteration 71, loss = 0.18748663\n",
      "Iteration 375, loss = 0.01166721\n",
      "Iteration 72, loss = 0.18542708\n",
      "Iteration 376, loss = 0.01161160\n",
      "Iteration 73, loss = 0.18439791\n",
      "Iteration 377, loss = 0.01182140\n",
      "Iteration 74, loss = 0.18229262\n",
      "Iteration 378, loss = 0.01167337\n",
      "Iteration 75, loss = 0.17923874\n",
      "Iteration 379, loss = 0.01163911\n",
      "Iteration 76, loss = 0.17869135\n",
      "Iteration 380, loss = 0.01204718\n",
      "Iteration 77, loss = 0.17470785\n",
      "Iteration 381, loss = 0.01134765\n",
      "Iteration 78, loss = 0.17391911\n",
      "Iteration 382, loss = 0.01249322\n",
      "Iteration 79, loss = 0.17227194\n",
      "Iteration 383, loss = 0.01196932\n",
      "Iteration 80, loss = 0.17071135\n",
      "Iteration 384, loss = 0.01300251\n",
      "Iteration 81, loss = 0.16782244\n",
      "Iteration 385, loss = 0.01100559\n",
      "Iteration 82, loss = 0.16678560\n",
      "Iteration 386, loss = 0.01189922\n",
      "Iteration 83, loss = 0.16498735\n",
      "Iteration 387, loss = 0.01162339\n",
      "Iteration 84, loss = 0.16361968\n",
      "Iteration 388, loss = 0.01147625\n",
      "Iteration 85, loss = 0.16170262\n",
      "Iteration 389, loss = 0.01082017\n",
      "Iteration 86, loss = 0.16094786\n",
      "Iteration 390, loss = 0.01103549\n",
      "Iteration 87, loss = 0.15816671\n",
      "Iteration 391, loss = 0.01117748\n",
      "Iteration 88, loss = 0.15703098\n",
      "Iteration 392, loss = 0.01048147\n",
      "Iteration 89, loss = 0.15522201\n",
      "Iteration 393, loss = 0.01074065\n",
      "Iteration 90, loss = 0.15415220\n",
      "Iteration 394, loss = 0.01048782\n",
      "Iteration 91, loss = 0.15325610\n",
      "Iteration 395, loss = 0.01125045\n",
      "Iteration 92, loss = 0.15185997\n",
      "Iteration 396, loss = 0.01132446\n",
      "Iteration 93, loss = 0.15001670\n",
      "Iteration 397, loss = 0.01003573\n",
      "Iteration 94, loss = 0.14888268\n",
      "Iteration 398, loss = 0.01057604\n",
      "Iteration 95, loss = 0.14684216\n",
      "Iteration 399, loss = 0.01054461\n",
      "Iteration 96, loss = 0.14607214\n",
      "Iteration 400, loss = 0.01017045\n",
      "Iteration 97, loss = 0.14477154\n",
      "Iteration 401, loss = 0.01002129\n",
      "Iteration 98, loss = 0.14371043\n",
      "Iteration 402, loss = 0.00998912\n",
      "Iteration 99, loss = 0.14250711\n",
      "Iteration 403, loss = 0.00987945\n",
      "Iteration 100, loss = 0.14144964\n",
      "Iteration 404, loss = 0.01019128\n",
      "Iteration 101, loss = 0.14039595\n",
      "Iteration 405, loss = 0.01254698\n",
      "Iteration 102, loss = 0.13903197\n",
      "Iteration 406, loss = 0.01136675\n",
      "Iteration 103, loss = 0.13732750\n",
      "Iteration 407, loss = 0.01216979\n",
      "Iteration 104, loss = 0.13779606\n",
      "Iteration 408, loss = 0.01165091\n",
      "Iteration 105, loss = 0.13491886\n",
      "Iteration 409, loss = 0.01110084\n",
      "Iteration 106, loss = 0.13400862\n",
      "Iteration 410, loss = 0.01250014\n",
      "Iteration 107, loss = 0.13292042\n",
      "Iteration 411, loss = 0.01188583\n",
      "Iteration 108, loss = 0.13257442\n",
      "Iteration 412, loss = 0.01047125\n",
      "Iteration 109, loss = 0.13086471\n",
      "Iteration 413, loss = 0.00921992\n",
      "Iteration 110, loss = 0.13068558\n",
      "Iteration 414, loss = 0.01000527\n",
      "Iteration 111, loss = 0.12864616\n",
      "Iteration 415, loss = 0.01012926\n",
      "Iteration 112, loss = 0.12815597\n",
      "Iteration 416, loss = 0.00958653\n",
      "Iteration 113, loss = 0.12656182\n",
      "Iteration 417, loss = 0.01037109\n",
      "Iteration 114, loss = 0.12601502\n",
      "Iteration 418, loss = 0.00902459\n",
      "Iteration 115, loss = 0.12491833\n",
      "Iteration 419, loss = 0.01002082\n",
      "Iteration 116, loss = 0.12424952\n",
      "Iteration 420, loss = 0.00919004\n",
      "Iteration 117, loss = 0.12474883\n",
      "Iteration 421, loss = 0.00963853\n",
      "Iteration 118, loss = 0.12489933\n",
      "Iteration 422, loss = 0.00924106\n",
      "Iteration 119, loss = 0.12247463\n",
      "Iteration 423, loss = 0.00907561\n",
      "Iteration 120, loss = 0.12147299\n",
      "Iteration 424, loss = 0.00874986\n",
      "Iteration 121, loss = 0.11974576\n",
      "Iteration 425, loss = 0.00876303\n",
      "Iteration 122, loss = 0.12078890\n",
      "Iteration 426, loss = 0.00893021\n",
      "Iteration 123, loss = 0.11910700\n",
      "Iteration 427, loss = 0.00914573\n",
      "Iteration 124, loss = 0.11769459\n",
      "Iteration 428, loss = 0.00899108\n",
      "Iteration 125, loss = 0.11798358\n",
      "Iteration 429, loss = 0.00853472\n",
      "Iteration 126, loss = 0.11538191\n",
      "Iteration 430, loss = 0.00885127\n",
      "Iteration 127, loss = 0.11491535\n",
      "Iteration 431, loss = 0.00863670\n",
      "Iteration 128, loss = 0.11449470\n",
      "Iteration 432, loss = 0.00966319\n",
      "Iteration 129, loss = 0.11368261\n",
      "Iteration 433, loss = 0.00852809\n",
      "Iteration 130, loss = 0.11165808\n",
      "Iteration 434, loss = 0.00796117\n",
      "Iteration 131, loss = 0.11114077\n",
      "Iteration 435, loss = 0.00963646\n",
      "Iteration 132, loss = 0.11082770\n",
      "Iteration 436, loss = 0.00886625\n",
      "Iteration 133, loss = 0.11033289\n",
      "Iteration 437, loss = 0.00890524\n",
      "Iteration 134, loss = 0.10920772\n",
      "Iteration 438, loss = 0.00787066\n",
      "Iteration 135, loss = 0.10912556\n",
      "Iteration 439, loss = 0.00819623\n",
      "Iteration 136, loss = 0.10957560\n",
      "Iteration 440, loss = 0.00810688\n",
      "Iteration 137, loss = 0.10721469\n",
      "Iteration 441, loss = 0.00786119\n",
      "Iteration 138, loss = 0.10683238\n",
      "Iteration 442, loss = 0.00776970\n",
      "Iteration 139, loss = 0.10640480\n",
      "Iteration 443, loss = 0.00865389\n",
      "Iteration 140, loss = 0.10599776\n",
      "Iteration 444, loss = 0.00806628\n",
      "Iteration 141, loss = 0.10441632\n",
      "Iteration 445, loss = 0.00809278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 142, loss = 0.10325915\n",
      "Iteration 143, loss = 0.10495180\n",
      "Iteration 144, loss = 0.10395819\n",
      "Iteration 145, loss = 0.10039233\n",
      "Iteration 146, loss = 0.10149648\n",
      "Iteration 147, loss = 0.10044585\n",
      "Iteration 148, loss = 0.10023621\n",
      "Iteration 149, loss = 0.09952941\n",
      "Iteration 150, loss = 0.09863021\n",
      "Iteration 151, loss = 0.09762348\n",
      "Iteration 152, loss = 0.09639164\n",
      "Iteration 153, loss = 0.09624171\n",
      "Iteration 154, loss = 0.09707610\n",
      "Iteration 155, loss = 0.09603815\n",
      "Iteration 156, loss = 0.09436757\n",
      "Iteration 157, loss = 0.09400843\n",
      "Iteration 158, loss = 0.09316114\n",
      "Iteration 159, loss = 0.09181404\n",
      "Iteration 160, loss = 0.09153553\n",
      "Iteration 1, loss = 1.12394197\n",
      "Iteration 161, loss = 0.09114265\n",
      "Iteration 2, loss = 0.94337923\n",
      "Iteration 162, loss = 0.09142786\n",
      "Iteration 3, loss = 0.81889186\n",
      "Iteration 163, loss = 0.08979373\n",
      "Iteration 4, loss = 0.72156263\n",
      "Iteration 164, loss = 0.08881380\n",
      "Iteration 5, loss = 0.64919588\n",
      "Iteration 165, loss = 0.08923265\n",
      "Iteration 6, loss = 0.59841942\n",
      "Iteration 166, loss = 0.08924631\n",
      "Iteration 7, loss = 0.55655790\n",
      "Iteration 167, loss = 0.08879746\n",
      "Iteration 8, loss = 0.52475831\n",
      "Iteration 168, loss = 0.08721515\n",
      "Iteration 9, loss = 0.49969682\n",
      "Iteration 169, loss = 0.08713878\n",
      "Iteration 10, loss = 0.47827474\n",
      "Iteration 170, loss = 0.08644903\n",
      "Iteration 11, loss = 0.45971650\n",
      "Iteration 171, loss = 0.08594490\n",
      "Iteration 12, loss = 0.44339533\n",
      "Iteration 172, loss = 0.08588604\n",
      "Iteration 13, loss = 0.43018574\n",
      "Iteration 173, loss = 0.08389049\n",
      "Iteration 14, loss = 0.41932432\n",
      "Iteration 174, loss = 0.08360889\n",
      "Iteration 15, loss = 0.40789697\n",
      "Iteration 175, loss = 0.08429749\n",
      "Iteration 16, loss = 0.39845879\n",
      "Iteration 176, loss = 0.08328542\n",
      "Iteration 17, loss = 0.38917632\n",
      "Iteration 177, loss = 0.08297486\n",
      "Iteration 18, loss = 0.38128495\n",
      "Iteration 178, loss = 0.08194425\n",
      "Iteration 19, loss = 0.37399556\n",
      "Iteration 179, loss = 0.08310249\n",
      "Iteration 20, loss = 0.36720300\n",
      "Iteration 180, loss = 0.08076774\n",
      "Iteration 21, loss = 0.36011665\n",
      "Iteration 181, loss = 0.08126337\n",
      "Iteration 22, loss = 0.35327634\n",
      "Iteration 182, loss = 0.08541908\n",
      "Iteration 23, loss = 0.34737644\n",
      "Iteration 183, loss = 0.08188503\n",
      "Iteration 24, loss = 0.34101541\n",
      "Iteration 184, loss = 0.08026254\n",
      "Iteration 25, loss = 0.33576197\n",
      "Iteration 185, loss = 0.07825524\n",
      "Iteration 26, loss = 0.32969915\n",
      "Iteration 186, loss = 0.07769036\n",
      "Iteration 27, loss = 0.32533089\n",
      "Iteration 187, loss = 0.07725009\n",
      "Iteration 28, loss = 0.31965425\n",
      "Iteration 188, loss = 0.07613001\n",
      "Iteration 29, loss = 0.31486517\n",
      "Iteration 189, loss = 0.07725287\n",
      "Iteration 30, loss = 0.30996149\n",
      "Iteration 190, loss = 0.07578724\n",
      "Iteration 31, loss = 0.30591495\n",
      "Iteration 191, loss = 0.07450254\n",
      "Iteration 32, loss = 0.30089253\n",
      "Iteration 192, loss = 0.07494471\n",
      "Iteration 33, loss = 0.29655271\n",
      "Iteration 193, loss = 0.07482252\n",
      "Iteration 34, loss = 0.29268652\n",
      "Iteration 194, loss = 0.07779767\n",
      "Iteration 35, loss = 0.28761368\n",
      "Iteration 195, loss = 0.07718753\n",
      "Iteration 36, loss = 0.28478130\n",
      "Iteration 196, loss = 0.07553698\n",
      "Iteration 37, loss = 0.28017568\n",
      "Iteration 197, loss = 0.07305083\n",
      "Iteration 38, loss = 0.27648225\n",
      "Iteration 198, loss = 0.07395607\n",
      "Iteration 39, loss = 0.27158645\n",
      "Iteration 199, loss = 0.07146625\n",
      "Iteration 40, loss = 0.26716359\n",
      "Iteration 200, loss = 0.07165461\n",
      "Iteration 41, loss = 0.26320188\n",
      "Iteration 201, loss = 0.07139494\n",
      "Iteration 42, loss = 0.25943525\n",
      "Iteration 202, loss = 0.07109249\n",
      "Iteration 43, loss = 0.25588505\n",
      "Iteration 203, loss = 0.06898755\n",
      "Iteration 44, loss = 0.25201686\n",
      "Iteration 204, loss = 0.07082112\n",
      "Iteration 45, loss = 0.24826925\n",
      "Iteration 205, loss = 0.06967179\n",
      "Iteration 46, loss = 0.24519719\n",
      "Iteration 206, loss = 0.06777933\n",
      "Iteration 47, loss = 0.24025812\n",
      "Iteration 207, loss = 0.07107964\n",
      "Iteration 48, loss = 0.23693993\n",
      "Iteration 208, loss = 0.07344413\n",
      "Iteration 49, loss = 0.23309937\n",
      "Iteration 209, loss = 0.07254756\n",
      "Iteration 50, loss = 0.23116003\n",
      "Iteration 210, loss = 0.06811200\n",
      "Iteration 51, loss = 0.22962943\n",
      "Iteration 52, loss = 0.22405224\n",
      "Iteration 211, loss = 0.07191019\n",
      "Iteration 53, loss = 0.22055331\n",
      "Iteration 212, loss = 0.06563391\n",
      "Iteration 54, loss = 0.21655794\n",
      "Iteration 213, loss = 0.06648426\n",
      "Iteration 55, loss = 0.21419686\n",
      "Iteration 214, loss = 0.06662375\n",
      "Iteration 56, loss = 0.21130351\n",
      "Iteration 215, loss = 0.06416759\n",
      "Iteration 57, loss = 0.20721231\n",
      "Iteration 216, loss = 0.06613780\n",
      "Iteration 58, loss = 0.20470506\n",
      "Iteration 217, loss = 0.06371529\n",
      "Iteration 59, loss = 0.20176839\n",
      "Iteration 218, loss = 0.06548098\n",
      "Iteration 60, loss = 0.19995778\n",
      "Iteration 219, loss = 0.06493345\n",
      "Iteration 61, loss = 0.19729195\n",
      "Iteration 220, loss = 0.06237449\n",
      "Iteration 62, loss = 0.19343714\n",
      "Iteration 221, loss = 0.06255687\n",
      "Iteration 63, loss = 0.19188886\n",
      "Iteration 222, loss = 0.06223521\n",
      "Iteration 64, loss = 0.18876950\n",
      "Iteration 223, loss = 0.06244429\n",
      "Iteration 65, loss = 0.18662465\n",
      "Iteration 224, loss = 0.06147030\n",
      "Iteration 66, loss = 0.18431820\n",
      "Iteration 225, loss = 0.05971516\n",
      "Iteration 67, loss = 0.18159576\n",
      "Iteration 226, loss = 0.05988634\n",
      "Iteration 68, loss = 0.17917867\n",
      "Iteration 227, loss = 0.05974794\n",
      "Iteration 69, loss = 0.17697253\n",
      "Iteration 228, loss = 0.05920140\n",
      "Iteration 70, loss = 0.17454055\n",
      "Iteration 229, loss = 0.06182324\n",
      "Iteration 71, loss = 0.17241751\n",
      "Iteration 230, loss = 0.06120442\n",
      "Iteration 72, loss = 0.17029997\n",
      "Iteration 231, loss = 0.05879597\n",
      "Iteration 73, loss = 0.16937767\n",
      "Iteration 232, loss = 0.06062845\n",
      "Iteration 74, loss = 0.16591701\n",
      "Iteration 233, loss = 0.05961458\n",
      "Iteration 75, loss = 0.16458459\n",
      "Iteration 234, loss = 0.05653507\n",
      "Iteration 76, loss = 0.16263174\n",
      "Iteration 235, loss = 0.05880686\n",
      "Iteration 77, loss = 0.16107716\n",
      "Iteration 236, loss = 0.05973941\n",
      "Iteration 78, loss = 0.15935455\n",
      "Iteration 237, loss = 0.05871763\n",
      "Iteration 79, loss = 0.15868534\n",
      "Iteration 238, loss = 0.05510785\n",
      "Iteration 80, loss = 0.15483843\n",
      "Iteration 239, loss = 0.05693370\n",
      "Iteration 81, loss = 0.15338418\n",
      "Iteration 240, loss = 0.05793326\n",
      "Iteration 82, loss = 0.15110404\n",
      "Iteration 241, loss = 0.05398730\n",
      "Iteration 83, loss = 0.14989167\n",
      "Iteration 242, loss = 0.05747407\n",
      "Iteration 84, loss = 0.14982338\n",
      "Iteration 243, loss = 0.05936790\n",
      "Iteration 85, loss = 0.14555231\n",
      "Iteration 244, loss = 0.05480144\n",
      "Iteration 86, loss = 0.14741696\n",
      "Iteration 245, loss = 0.05470581\n",
      "Iteration 87, loss = 0.14277589\n",
      "Iteration 246, loss = 0.05316346\n",
      "Iteration 88, loss = 0.14186459\n",
      "Iteration 247, loss = 0.05255778\n",
      "Iteration 89, loss = 0.14012128\n",
      "Iteration 248, loss = 0.05362678\n",
      "Iteration 90, loss = 0.13867498\n",
      "Iteration 249, loss = 0.05154332\n",
      "Iteration 91, loss = 0.13692724\n",
      "Iteration 250, loss = 0.05266876\n",
      "Iteration 92, loss = 0.13659055\n",
      "Iteration 251, loss = 0.05164574\n",
      "Iteration 93, loss = 0.13411312\n",
      "Iteration 252, loss = 0.05271161\n",
      "Iteration 94, loss = 0.13347220\n",
      "Iteration 253, loss = 0.05013616\n",
      "Iteration 95, loss = 0.13235870\n",
      "Iteration 254, loss = 0.05147123\n",
      "Iteration 96, loss = 0.13144635\n",
      "Iteration 255, loss = 0.05065672\n",
      "Iteration 97, loss = 0.12969902\n",
      "Iteration 256, loss = 0.05107255\n",
      "Iteration 98, loss = 0.12918603\n",
      "Iteration 257, loss = 0.05003990\n",
      "Iteration 99, loss = 0.12667111\n",
      "Iteration 258, loss = 0.04933861\n",
      "Iteration 100, loss = 0.12577449\n",
      "Iteration 259, loss = 0.04875101\n",
      "Iteration 101, loss = 0.12360845\n",
      "Iteration 260, loss = 0.04863707\n",
      "Iteration 102, loss = 0.12261554\n",
      "Iteration 261, loss = 0.04879301\n",
      "Iteration 103, loss = 0.12164176\n",
      "Iteration 262, loss = 0.04835667\n",
      "Iteration 104, loss = 0.12117102\n",
      "Iteration 263, loss = 0.04786788\n",
      "Iteration 105, loss = 0.12007475\n",
      "Iteration 264, loss = 0.04862381\n",
      "Iteration 106, loss = 0.12032968\n",
      "Iteration 265, loss = 0.04755649\n",
      "Iteration 107, loss = 0.11736885\n",
      "Iteration 266, loss = 0.05189933\n",
      "Iteration 108, loss = 0.11645015\n",
      "Iteration 267, loss = 0.04759369\n",
      "Iteration 109, loss = 0.11584257\n",
      "Iteration 268, loss = 0.04898875\n",
      "Iteration 110, loss = 0.11563139\n",
      "Iteration 269, loss = 0.04682078\n",
      "Iteration 111, loss = 0.11587769\n",
      "Iteration 270, loss = 0.04798362\n",
      "Iteration 112, loss = 0.11184606\n",
      "Iteration 271, loss = 0.04513511\n",
      "Iteration 113, loss = 0.11084519\n",
      "Iteration 272, loss = 0.04660655\n",
      "Iteration 114, loss = 0.10936675\n",
      "Iteration 273, loss = 0.04647390\n",
      "Iteration 115, loss = 0.10985704\n",
      "Iteration 274, loss = 0.04730706\n",
      "Iteration 116, loss = 0.10880994\n",
      "Iteration 275, loss = 0.04622715\n",
      "Iteration 117, loss = 0.10609132\n",
      "Iteration 276, loss = 0.04560080\n",
      "Iteration 118, loss = 0.10610551\n",
      "Iteration 277, loss = 0.04484061\n",
      "Iteration 119, loss = 0.10475626\n",
      "Iteration 278, loss = 0.04580895\n",
      "Iteration 120, loss = 0.10505721\n",
      "Iteration 279, loss = 0.04429272\n",
      "Iteration 121, loss = 0.10248581\n",
      "Iteration 280, loss = 0.04360030\n",
      "Iteration 122, loss = 0.10285787\n",
      "Iteration 281, loss = 0.04357041\n",
      "Iteration 123, loss = 0.10286850\n",
      "Iteration 282, loss = 0.04445958\n",
      "Iteration 124, loss = 0.10082231\n",
      "Iteration 283, loss = 0.04224714\n",
      "Iteration 125, loss = 0.09972050\n",
      "Iteration 284, loss = 0.04436905\n",
      "Iteration 126, loss = 0.10122596\n",
      "Iteration 285, loss = 0.04514554\n",
      "Iteration 127, loss = 0.09687725\n",
      "Iteration 286, loss = 0.04230543\n",
      "Iteration 128, loss = 0.09790719\n",
      "Iteration 287, loss = 0.04204018\n",
      "Iteration 129, loss = 0.09780015\n",
      "Iteration 288, loss = 0.04193080\n",
      "Iteration 130, loss = 0.09923572\n",
      "Iteration 289, loss = 0.04158075\n",
      "Iteration 131, loss = 0.09394326\n",
      "Iteration 290, loss = 0.04272629\n",
      "Iteration 132, loss = 0.09482790\n",
      "Iteration 291, loss = 0.04447211\n",
      "Iteration 133, loss = 0.09295715\n",
      "Iteration 292, loss = 0.04138819\n",
      "Iteration 134, loss = 0.09271577\n",
      "Iteration 293, loss = 0.04144433\n",
      "Iteration 135, loss = 0.09220747\n",
      "Iteration 294, loss = 0.04089345\n",
      "Iteration 136, loss = 0.09258386\n",
      "Iteration 295, loss = 0.04170207\n",
      "Iteration 137, loss = 0.08979313\n",
      "Iteration 296, loss = 0.03998470\n",
      "Iteration 138, loss = 0.08893748\n",
      "Iteration 297, loss = 0.04023001\n",
      "Iteration 139, loss = 0.08710597\n",
      "Iteration 298, loss = 0.04105203\n",
      "Iteration 140, loss = 0.08736062\n",
      "Iteration 299, loss = 0.04054750\n",
      "Iteration 141, loss = 0.08706107\n",
      "Iteration 300, loss = 0.04028527\n",
      "Iteration 142, loss = 0.08626371\n",
      "Iteration 301, loss = 0.04167442\n",
      "Iteration 143, loss = 0.08468150\n",
      "Iteration 302, loss = 0.03858164\n",
      "Iteration 144, loss = 0.08489614\n",
      "Iteration 303, loss = 0.04213896\n",
      "Iteration 145, loss = 0.08714634\n",
      "Iteration 304, loss = 0.04073863\n",
      "Iteration 146, loss = 0.08143789\n",
      "Iteration 305, loss = 0.03754142\n",
      "Iteration 147, loss = 0.08528299\n",
      "Iteration 306, loss = 0.04030184\n",
      "Iteration 148, loss = 0.08349522\n",
      "Iteration 307, loss = 0.03865600\n",
      "Iteration 149, loss = 0.08042895\n",
      "Iteration 308, loss = 0.03868961\n",
      "Iteration 150, loss = 0.08530820\n",
      "Iteration 309, loss = 0.03824272\n",
      "Iteration 151, loss = 0.07968701\n",
      "Iteration 310, loss = 0.03910287\n",
      "Iteration 152, loss = 0.08212270\n",
      "Iteration 311, loss = 0.03972634\n",
      "Iteration 153, loss = 0.08025197\n",
      "Iteration 312, loss = 0.03946740\n",
      "Iteration 154, loss = 0.08069731\n",
      "Iteration 313, loss = 0.03947875\n",
      "Iteration 155, loss = 0.07658628\n",
      "Iteration 314, loss = 0.03686903\n",
      "Iteration 156, loss = 0.07795299\n",
      "Iteration 315, loss = 0.03738162\n",
      "Iteration 157, loss = 0.08017925\n",
      "Iteration 316, loss = 0.03631614\n",
      "Iteration 158, loss = 0.07660083\n",
      "Iteration 317, loss = 0.03648279\n",
      "Iteration 159, loss = 0.07542691\n",
      "Iteration 318, loss = 0.03598732\n",
      "Iteration 160, loss = 0.07456174\n",
      "Iteration 319, loss = 0.03585898\n",
      "Iteration 161, loss = 0.07364557\n",
      "Iteration 320, loss = 0.03584911\n",
      "Iteration 162, loss = 0.07088467\n",
      "Iteration 321, loss = 0.03575698\n",
      "Iteration 163, loss = 0.07286452\n",
      "Iteration 322, loss = 0.03732074\n",
      "Iteration 164, loss = 0.07093632\n",
      "Iteration 323, loss = 0.03606338\n",
      "Iteration 165, loss = 0.06977337\n",
      "Iteration 324, loss = 0.03482351\n",
      "Iteration 166, loss = 0.07047670\n",
      "Iteration 325, loss = 0.03758924\n",
      "Iteration 167, loss = 0.07058939\n",
      "Iteration 326, loss = 0.03496137\n",
      "Iteration 168, loss = 0.06838230\n",
      "Iteration 327, loss = 0.03617335\n",
      "Iteration 169, loss = 0.06842613\n",
      "Iteration 328, loss = 0.03488993\n",
      "Iteration 170, loss = 0.06783998\n",
      "Iteration 329, loss = 0.03502538\n",
      "Iteration 171, loss = 0.06618580\n",
      "Iteration 330, loss = 0.03461051\n",
      "Iteration 172, loss = 0.06762684\n",
      "Iteration 331, loss = 0.03414018\n",
      "Iteration 173, loss = 0.06579914\n",
      "Iteration 332, loss = 0.03354735\n",
      "Iteration 174, loss = 0.06545825\n",
      "Iteration 333, loss = 0.03405016\n",
      "Iteration 175, loss = 0.06536507\n",
      "Iteration 334, loss = 0.03327003\n",
      "Iteration 176, loss = 0.06575500\n",
      "Iteration 335, loss = 0.03346766\n",
      "Iteration 177, loss = 0.06312959\n",
      "Iteration 336, loss = 0.03417892\n",
      "Iteration 178, loss = 0.06400830\n",
      "Iteration 337, loss = 0.03379679\n",
      "Iteration 179, loss = 0.06222385\n",
      "Iteration 338, loss = 0.03291551\n",
      "Iteration 180, loss = 0.06458352\n",
      "Iteration 339, loss = 0.03358280\n",
      "Iteration 181, loss = 0.06184883\n",
      "Iteration 340, loss = 0.03346992\n",
      "Iteration 182, loss = 0.06109929\n",
      "Iteration 341, loss = 0.03289369\n",
      "Iteration 183, loss = 0.06103927\n",
      "Iteration 342, loss = 0.03280912\n",
      "Iteration 184, loss = 0.06029719\n",
      "Iteration 343, loss = 0.03235438\n",
      "Iteration 185, loss = 0.05916577\n",
      "Iteration 344, loss = 0.03172596\n",
      "Iteration 186, loss = 0.05982854\n",
      "Iteration 345, loss = 0.03282649\n",
      "Iteration 187, loss = 0.05930020\n",
      "Iteration 346, loss = 0.03221300\n",
      "Iteration 188, loss = 0.05813257\n",
      "Iteration 347, loss = 0.03128282\n",
      "Iteration 189, loss = 0.05834742\n",
      "Iteration 348, loss = 0.03102640\n",
      "Iteration 190, loss = 0.05891543\n",
      "Iteration 349, loss = 0.03168602\n",
      "Iteration 191, loss = 0.05852980\n",
      "Iteration 350, loss = 0.03084438\n",
      "Iteration 192, loss = 0.05870751\n",
      "Iteration 351, loss = 0.03152664\n",
      "Iteration 193, loss = 0.05521433\n",
      "Iteration 352, loss = 0.03162839\n",
      "Iteration 194, loss = 0.05654134\n",
      "Iteration 353, loss = 0.03090688\n",
      "Iteration 195, loss = 0.05623007\n",
      "Iteration 354, loss = 0.03252064\n",
      "Iteration 196, loss = 0.05451284\n",
      "Iteration 355, loss = 0.03164606\n",
      "Iteration 197, loss = 0.05500109\n",
      "Iteration 356, loss = 0.03046829\n",
      "Iteration 198, loss = 0.05671990\n",
      "Iteration 357, loss = 0.03187842\n",
      "Iteration 199, loss = 0.05674067\n",
      "Iteration 358, loss = 0.03639669\n",
      "Iteration 200, loss = 0.05245380\n",
      "Iteration 359, loss = 0.03149356\n",
      "Iteration 201, loss = 0.05711025\n",
      "Iteration 360, loss = 0.03746572\n",
      "Iteration 202, loss = 0.05699546\n",
      "Iteration 361, loss = 0.03695437\n",
      "Iteration 203, loss = 0.05373346\n",
      "Iteration 362, loss = 0.03636649\n",
      "Iteration 204, loss = 0.05394065\n",
      "Iteration 363, loss = 0.03128688\n",
      "Iteration 205, loss = 0.05369873\n",
      "Iteration 364, loss = 0.03374367\n",
      "Iteration 206, loss = 0.05442311\n",
      "Iteration 365, loss = 0.03066131\n",
      "Iteration 207, loss = 0.05101145\n",
      "Iteration 366, loss = 0.03214665\n",
      "Iteration 208, loss = 0.05230057\n",
      "Iteration 367, loss = 0.03114106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 209, loss = 0.04863389\n",
      "Iteration 210, loss = 0.05027130\n",
      "Iteration 211, loss = 0.05196774\n",
      "Iteration 212, loss = 0.05057464\n",
      "Iteration 213, loss = 0.04895192\n",
      "Iteration 214, loss = 0.04832634\n",
      "Iteration 215, loss = 0.04825161\n",
      "Iteration 216, loss = 0.04584365\n",
      "Iteration 217, loss = 0.04872918\n",
      "Iteration 218, loss = 0.04907723\n",
      "Iteration 219, loss = 0.05380385\n",
      "Iteration 220, loss = 0.04822220\n",
      "Iteration 221, loss = 0.04590384\n",
      "Iteration 222, loss = 0.04783198\n",
      "Iteration 223, loss = 0.04728395\n",
      "Iteration 224, loss = 0.04509583\n",
      "Iteration 225, loss = 0.04587530\n",
      "Iteration 226, loss = 0.04492988\n",
      "Iteration 227, loss = 0.04328239\n",
      "Iteration 1, loss = 1.39299581\n",
      "Iteration 228, loss = 0.04479320\n",
      "Iteration 2, loss = 1.16044443\n",
      "Iteration 229, loss = 0.04308258\n",
      "Iteration 3, loss = 1.02578660\n",
      "Iteration 230, loss = 0.04183303\n",
      "Iteration 4, loss = 0.93480589\n",
      "Iteration 231, loss = 0.04228630\n",
      "Iteration 5, loss = 0.85136749\n",
      "Iteration 232, loss = 0.04206353\n",
      "Iteration 6, loss = 0.77712940\n",
      "Iteration 233, loss = 0.04263536\n",
      "Iteration 7, loss = 0.71951356\n",
      "Iteration 234, loss = 0.04247427\n",
      "Iteration 8, loss = 0.66663961\n",
      "Iteration 235, loss = 0.04149492\n",
      "Iteration 9, loss = 0.62171002\n",
      "Iteration 236, loss = 0.04076983\n",
      "Iteration 10, loss = 0.58193964\n",
      "Iteration 237, loss = 0.04043013\n",
      "Iteration 11, loss = 0.54810730\n",
      "Iteration 238, loss = 0.04009003\n",
      "Iteration 12, loss = 0.52072972\n",
      "Iteration 239, loss = 0.04012428\n",
      "Iteration 13, loss = 0.49651051\n",
      "Iteration 240, loss = 0.03928702\n",
      "Iteration 14, loss = 0.47553836\n",
      "Iteration 241, loss = 0.03912258\n",
      "Iteration 15, loss = 0.45785485\n",
      "Iteration 242, loss = 0.03916423\n",
      "Iteration 16, loss = 0.44301319\n",
      "Iteration 243, loss = 0.03895834\n",
      "Iteration 17, loss = 0.42929174\n",
      "Iteration 244, loss = 0.03894917\n",
      "Iteration 18, loss = 0.41663217\n",
      "Iteration 245, loss = 0.03872763\n",
      "Iteration 19, loss = 0.40594498\n",
      "Iteration 20, loss = 0.39673948\n",
      "Iteration 246, loss = 0.03929100\n",
      "Iteration 247, loss = 0.03745632\n",
      "Iteration 21, loss = 0.38752989\n",
      "Iteration 248, loss = 0.03940999\n",
      "Iteration 22, loss = 0.37908220\n",
      "Iteration 249, loss = 0.03736852\n",
      "Iteration 23, loss = 0.37148662\n",
      "Iteration 250, loss = 0.03689344\n",
      "Iteration 24, loss = 0.36468641\n",
      "Iteration 251, loss = 0.03839141\n",
      "Iteration 25, loss = 0.35895891\n",
      "Iteration 26, loss = 0.35179068\n",
      "Iteration 252, loss = 0.03581074\n",
      "Iteration 27, loss = 0.34697004\n",
      "Iteration 253, loss = 0.03821392\n",
      "Iteration 28, loss = 0.34139209\n",
      "Iteration 254, loss = 0.03695042\n",
      "Iteration 29, loss = 0.33634657\n",
      "Iteration 255, loss = 0.03792070\n",
      "Iteration 30, loss = 0.33162065\n",
      "Iteration 256, loss = 0.03542745\n",
      "Iteration 31, loss = 0.32531078\n",
      "Iteration 257, loss = 0.03533930\n",
      "Iteration 32, loss = 0.32129957\n",
      "Iteration 258, loss = 0.03788188\n",
      "Iteration 33, loss = 0.31668252\n",
      "Iteration 259, loss = 0.03537238\n",
      "Iteration 34, loss = 0.31251611\n",
      "Iteration 260, loss = 0.03458793\n",
      "Iteration 35, loss = 0.30795791\n",
      "Iteration 261, loss = 0.03473926\n",
      "Iteration 36, loss = 0.30346405\n",
      "Iteration 262, loss = 0.03420613\n",
      "Iteration 37, loss = 0.29959199\n",
      "Iteration 263, loss = 0.03432417\n",
      "Iteration 38, loss = 0.29561781\n",
      "Iteration 264, loss = 0.03359304\n",
      "Iteration 39, loss = 0.29164301\n",
      "Iteration 265, loss = 0.03371793\n",
      "Iteration 40, loss = 0.28807521\n",
      "Iteration 266, loss = 0.03309331\n",
      "Iteration 41, loss = 0.28426006\n",
      "Iteration 267, loss = 0.03284755\n",
      "Iteration 42, loss = 0.28097283\n",
      "Iteration 268, loss = 0.03359067\n",
      "Iteration 43, loss = 0.27869427\n",
      "Iteration 269, loss = 0.03318208\n",
      "Iteration 44, loss = 0.27408483\n",
      "Iteration 270, loss = 0.03259682\n",
      "Iteration 45, loss = 0.26990738\n",
      "Iteration 271, loss = 0.03240040\n",
      "Iteration 46, loss = 0.26665590\n",
      "Iteration 272, loss = 0.03244149\n",
      "Iteration 47, loss = 0.26333909\n",
      "Iteration 273, loss = 0.03170176\n",
      "Iteration 48, loss = 0.26013714\n",
      "Iteration 274, loss = 0.03116526\n",
      "Iteration 49, loss = 0.25713078\n",
      "Iteration 275, loss = 0.03209735\n",
      "Iteration 50, loss = 0.25341557\n",
      "Iteration 276, loss = 0.03101129\n",
      "Iteration 51, loss = 0.25185120\n",
      "Iteration 277, loss = 0.03183215\n",
      "Iteration 52, loss = 0.24684034\n",
      "Iteration 278, loss = 0.03141795\n",
      "Iteration 53, loss = 0.24625985\n",
      "Iteration 279, loss = 0.03416794\n",
      "Iteration 54, loss = 0.24287039\n",
      "Iteration 280, loss = 0.03149382\n",
      "Iteration 55, loss = 0.23901869\n",
      "Iteration 281, loss = 0.03056954\n",
      "Iteration 56, loss = 0.23799838\n",
      "Iteration 282, loss = 0.03115741\n",
      "Iteration 57, loss = 0.23373921\n",
      "Iteration 283, loss = 0.03099280\n",
      "Iteration 58, loss = 0.23072798\n",
      "Iteration 284, loss = 0.02987859\n",
      "Iteration 59, loss = 0.22616047\n",
      "Iteration 285, loss = 0.02972935\n",
      "Iteration 60, loss = 0.22493227\n",
      "Iteration 286, loss = 0.02886194\n",
      "Iteration 61, loss = 0.22339367\n",
      "Iteration 287, loss = 0.02961617\n",
      "Iteration 62, loss = 0.21861719\n",
      "Iteration 288, loss = 0.03159427\n",
      "Iteration 63, loss = 0.21691659\n",
      "Iteration 289, loss = 0.02817574\n",
      "Iteration 64, loss = 0.21383019\n",
      "Iteration 290, loss = 0.03127512\n",
      "Iteration 65, loss = 0.21178113\n",
      "Iteration 291, loss = 0.02882822\n",
      "Iteration 66, loss = 0.20893975\n",
      "Iteration 292, loss = 0.02952890\n",
      "Iteration 67, loss = 0.20663437\n",
      "Iteration 293, loss = 0.02930799\n",
      "Iteration 68, loss = 0.20422127\n",
      "Iteration 294, loss = 0.02753021\n",
      "Iteration 69, loss = 0.20198023\n",
      "Iteration 295, loss = 0.02933698\n",
      "Iteration 70, loss = 0.19979280\n",
      "Iteration 296, loss = 0.02797821\n",
      "Iteration 71, loss = 0.19755345\n",
      "Iteration 297, loss = 0.02805184\n",
      "Iteration 72, loss = 0.19479313\n",
      "Iteration 298, loss = 0.02660684\n",
      "Iteration 73, loss = 0.19341624\n",
      "Iteration 299, loss = 0.02788809\n",
      "Iteration 74, loss = 0.19115070\n",
      "Iteration 300, loss = 0.02683202\n",
      "Iteration 75, loss = 0.18909886\n",
      "Iteration 301, loss = 0.02670606\n",
      "Iteration 76, loss = 0.18752780\n",
      "Iteration 302, loss = 0.02784003\n",
      "Iteration 77, loss = 0.18472118\n",
      "Iteration 303, loss = 0.02657206\n",
      "Iteration 78, loss = 0.18312538\n",
      "Iteration 304, loss = 0.02927338\n",
      "Iteration 79, loss = 0.18037922\n",
      "Iteration 80, loss = 0.17897121\n",
      "Iteration 305, loss = 0.02577276\n",
      "Iteration 81, loss = 0.17695873\n",
      "Iteration 306, loss = 0.02672556\n",
      "Iteration 82, loss = 0.17561618\n",
      "Iteration 307, loss = 0.02606428\n",
      "Iteration 308, loss = 0.02517827\n",
      "Iteration 83, loss = 0.17370579\n",
      "Iteration 309, loss = 0.02527961\n",
      "Iteration 84, loss = 0.17166403\n",
      "Iteration 310, loss = 0.02683963\n",
      "Iteration 85, loss = 0.16955767\n",
      "Iteration 86, loss = 0.16808189\n",
      "Iteration 311, loss = 0.02514400\n",
      "Iteration 87, loss = 0.16689522\n",
      "Iteration 312, loss = 0.02470319\n",
      "Iteration 88, loss = 0.16561095\n",
      "Iteration 313, loss = 0.02442463\n",
      "Iteration 89, loss = 0.16300071\n",
      "Iteration 314, loss = 0.02530063\n",
      "Iteration 90, loss = 0.16154623\n",
      "Iteration 315, loss = 0.02543900\n",
      "Iteration 91, loss = 0.15968408\n",
      "Iteration 316, loss = 0.02367553\n",
      "Iteration 92, loss = 0.15784934\n",
      "Iteration 317, loss = 0.02415653\n",
      "Iteration 93, loss = 0.15649664\n",
      "Iteration 318, loss = 0.02344158\n",
      "Iteration 94, loss = 0.15504959\n",
      "Iteration 319, loss = 0.02401330\n",
      "Iteration 95, loss = 0.15327655\n",
      "Iteration 320, loss = 0.02408545\n",
      "Iteration 96, loss = 0.15200527\n",
      "Iteration 321, loss = 0.02354356\n",
      "Iteration 97, loss = 0.15084672\n",
      "Iteration 322, loss = 0.02437282\n",
      "Iteration 98, loss = 0.15273582\n",
      "Iteration 323, loss = 0.02260016\n",
      "Iteration 99, loss = 0.14810135\n",
      "Iteration 324, loss = 0.02345351\n",
      "Iteration 100, loss = 0.14727654\n",
      "Iteration 325, loss = 0.02511833\n",
      "Iteration 101, loss = 0.14508972\n",
      "Iteration 326, loss = 0.02259469\n",
      "Iteration 102, loss = 0.14458951\n",
      "Iteration 327, loss = 0.02367714\n",
      "Iteration 103, loss = 0.14238239\n",
      "Iteration 328, loss = 0.02745577\n",
      "Iteration 104, loss = 0.14339831\n",
      "Iteration 329, loss = 0.02251514\n",
      "Iteration 105, loss = 0.14126934\n",
      "Iteration 330, loss = 0.02480110\n",
      "Iteration 106, loss = 0.13989176\n",
      "Iteration 331, loss = 0.02262425\n",
      "Iteration 107, loss = 0.13718947\n",
      "Iteration 332, loss = 0.02144219\n",
      "Iteration 108, loss = 0.13720450\n",
      "Iteration 333, loss = 0.02165033\n",
      "Iteration 109, loss = 0.13449540\n",
      "Iteration 334, loss = 0.02144329\n",
      "Iteration 110, loss = 0.13453160\n",
      "Iteration 335, loss = 0.02087007\n",
      "Iteration 111, loss = 0.13314975\n",
      "Iteration 336, loss = 0.02163875\n",
      "Iteration 112, loss = 0.13066338\n",
      "Iteration 337, loss = 0.02097950\n",
      "Iteration 113, loss = 0.12995526\n",
      "Iteration 338, loss = 0.02071778\n",
      "Iteration 114, loss = 0.13134883\n",
      "Iteration 339, loss = 0.02173768\n",
      "Iteration 115, loss = 0.12775974\n",
      "Iteration 340, loss = 0.02013097\n",
      "Iteration 116, loss = 0.12825922\n",
      "Iteration 341, loss = 0.02052578\n",
      "Iteration 117, loss = 0.12516789\n",
      "Iteration 342, loss = 0.02040728\n",
      "Iteration 118, loss = 0.12515120\n",
      "Iteration 343, loss = 0.02030464\n",
      "Iteration 119, loss = 0.12527232\n",
      "Iteration 344, loss = 0.02036055\n",
      "Iteration 120, loss = 0.12395288\n",
      "Iteration 345, loss = 0.01978340\n",
      "Iteration 121, loss = 0.12170978\n",
      "Iteration 346, loss = 0.01998974\n",
      "Iteration 122, loss = 0.12261366\n",
      "Iteration 347, loss = 0.01911760\n",
      "Iteration 123, loss = 0.12178717\n",
      "Iteration 348, loss = 0.01947217\n",
      "Iteration 124, loss = 0.12028347\n",
      "Iteration 349, loss = 0.02135904\n",
      "Iteration 125, loss = 0.11770449\n",
      "Iteration 350, loss = 0.01935352\n",
      "Iteration 126, loss = 0.11700447\n",
      "Iteration 351, loss = 0.02004897\n",
      "Iteration 127, loss = 0.11645971\n",
      "Iteration 352, loss = 0.01919752\n",
      "Iteration 128, loss = 0.11471653\n",
      "Iteration 353, loss = 0.01871232\n",
      "Iteration 129, loss = 0.11462213\n",
      "Iteration 354, loss = 0.01921268\n",
      "Iteration 130, loss = 0.11366108\n",
      "Iteration 355, loss = 0.01917531\n",
      "Iteration 131, loss = 0.11285166\n",
      "Iteration 356, loss = 0.01962982\n",
      "Iteration 132, loss = 0.11308380\n",
      "Iteration 357, loss = 0.02002609\n",
      "Iteration 133, loss = 0.11186934\n",
      "Iteration 358, loss = 0.02049344\n",
      "Iteration 134, loss = 0.11194997\n",
      "Iteration 359, loss = 0.01967527\n",
      "Iteration 135, loss = 0.10886784\n",
      "Iteration 360, loss = 0.01843446\n",
      "Iteration 136, loss = 0.10790023\n",
      "Iteration 361, loss = 0.01879687\n",
      "Iteration 137, loss = 0.10749794\n",
      "Iteration 362, loss = 0.01815674\n",
      "Iteration 138, loss = 0.10569534\n",
      "Iteration 363, loss = 0.01751695\n",
      "Iteration 139, loss = 0.10688979\n",
      "Iteration 364, loss = 0.01782298\n",
      "Iteration 140, loss = 0.10686789\n",
      "Iteration 365, loss = 0.01726531\n",
      "Iteration 141, loss = 0.10283683\n",
      "Iteration 366, loss = 0.01732036\n",
      "Iteration 142, loss = 0.10328315\n",
      "Iteration 367, loss = 0.01906718\n",
      "Iteration 143, loss = 0.10211297\n",
      "Iteration 368, loss = 0.01926038\n",
      "Iteration 144, loss = 0.10151641\n",
      "Iteration 369, loss = 0.01724250\n",
      "Iteration 145, loss = 0.10253480\n",
      "Iteration 370, loss = 0.01755190\n",
      "Iteration 146, loss = 0.10061009\n",
      "Iteration 371, loss = 0.01720729\n",
      "Iteration 147, loss = 0.10030191\n",
      "Iteration 372, loss = 0.01680232\n",
      "Iteration 148, loss = 0.09925327\n",
      "Iteration 373, loss = 0.01877024\n",
      "Iteration 149, loss = 0.09831660\n",
      "Iteration 374, loss = 0.01719022\n",
      "Iteration 150, loss = 0.09642521\n",
      "Iteration 375, loss = 0.01620665\n",
      "Iteration 151, loss = 0.09723266\n",
      "Iteration 376, loss = 0.01736985\n",
      "Iteration 152, loss = 0.09586207\n",
      "Iteration 153, loss = 0.09493228\n",
      "Iteration 377, loss = 0.01647739\n",
      "Iteration 154, loss = 0.09537509\n",
      "Iteration 378, loss = 0.01757461\n",
      "Iteration 155, loss = 0.09290392\n",
      "Iteration 379, loss = 0.01672476\n",
      "Iteration 156, loss = 0.09289365\n",
      "Iteration 380, loss = 0.01674056\n",
      "Iteration 381, loss = 0.01664155\n",
      "Iteration 157, loss = 0.09228569\n",
      "Iteration 158, loss = 0.09206998\n",
      "Iteration 382, loss = 0.01625545\n",
      "Iteration 159, loss = 0.09137823\n",
      "Iteration 383, loss = 0.01722205\n",
      "Iteration 160, loss = 0.09055300\n",
      "Iteration 384, loss = 0.01748192\n",
      "Iteration 161, loss = 0.08941363\n",
      "Iteration 385, loss = 0.01614843\n",
      "Iteration 162, loss = 0.08857621\n",
      "Iteration 386, loss = 0.01818399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 163, loss = 0.08809162\n",
      "Iteration 164, loss = 0.08796056\n",
      "Iteration 165, loss = 0.08682991\n",
      "Iteration 166, loss = 0.08581845\n",
      "Iteration 167, loss = 0.08588514\n",
      "Iteration 168, loss = 0.08405461\n",
      "Iteration 169, loss = 0.08520127\n",
      "Iteration 170, loss = 0.08307091\n",
      "Iteration 171, loss = 0.08435501\n",
      "Iteration 172, loss = 0.08126096\n",
      "Iteration 173, loss = 0.08144857\n",
      "Iteration 174, loss = 0.08072678\n",
      "Iteration 175, loss = 0.08032737\n",
      "Iteration 176, loss = 0.07958282\n",
      "Iteration 177, loss = 0.08163892\n",
      "Iteration 178, loss = 0.07858524\n",
      "Iteration 179, loss = 0.07927716\n",
      "Iteration 180, loss = 0.08193289\n",
      "Iteration 1, loss = 1.26616848\n",
      "Iteration 181, loss = 0.07961776\n",
      "Iteration 2, loss = 1.12328433\n",
      "Iteration 182, loss = 0.07928479\n",
      "Iteration 3, loss = 1.00921862\n",
      "Iteration 183, loss = 0.07592558\n",
      "Iteration 4, loss = 0.91282193\n",
      "Iteration 184, loss = 0.07629029\n",
      "Iteration 5, loss = 0.83159509\n",
      "Iteration 185, loss = 0.07560786\n",
      "Iteration 6, loss = 0.75724413\n",
      "Iteration 186, loss = 0.07337403\n",
      "Iteration 7, loss = 0.69533900\n",
      "Iteration 187, loss = 0.07346035\n",
      "Iteration 8, loss = 0.64221553\n",
      "Iteration 188, loss = 0.07251290\n",
      "Iteration 9, loss = 0.59865440\n",
      "Iteration 189, loss = 0.07258504\n",
      "Iteration 10, loss = 0.56231723\n",
      "Iteration 190, loss = 0.07189840\n",
      "Iteration 11, loss = 0.53278444\n",
      "Iteration 191, loss = 0.07137748\n",
      "Iteration 12, loss = 0.50744424\n",
      "Iteration 192, loss = 0.07046736\n",
      "Iteration 13, loss = 0.48441441\n",
      "Iteration 193, loss = 0.07085297\n",
      "Iteration 14, loss = 0.46640318\n",
      "Iteration 194, loss = 0.06956558\n",
      "Iteration 15, loss = 0.45009576\n",
      "Iteration 195, loss = 0.06890706\n",
      "Iteration 16, loss = 0.43567496\n",
      "Iteration 196, loss = 0.06986539\n",
      "Iteration 17, loss = 0.42294874\n",
      "Iteration 197, loss = 0.07138091\n",
      "Iteration 18, loss = 0.41126214\n",
      "Iteration 198, loss = 0.06765199\n",
      "Iteration 19, loss = 0.40087249\n",
      "Iteration 199, loss = 0.06833621\n",
      "Iteration 20, loss = 0.39128410\n",
      "Iteration 200, loss = 0.06726298\n",
      "Iteration 21, loss = 0.38182640\n",
      "Iteration 201, loss = 0.06581506\n",
      "Iteration 22, loss = 0.37375143\n",
      "Iteration 202, loss = 0.06597969\n",
      "Iteration 23, loss = 0.36657302\n",
      "Iteration 203, loss = 0.06562061\n",
      "Iteration 24, loss = 0.35835788\n",
      "Iteration 204, loss = 0.06482349\n",
      "Iteration 25, loss = 0.35118822\n",
      "Iteration 205, loss = 0.06482067\n",
      "Iteration 26, loss = 0.34473941\n",
      "Iteration 206, loss = 0.06383800\n",
      "Iteration 27, loss = 0.33797607\n",
      "Iteration 207, loss = 0.06531216\n",
      "Iteration 28, loss = 0.33364818\n",
      "Iteration 208, loss = 0.06489667\n",
      "Iteration 29, loss = 0.32629996\n",
      "Iteration 209, loss = 0.06422435\n",
      "Iteration 30, loss = 0.32038910\n",
      "Iteration 210, loss = 0.06246896\n",
      "Iteration 31, loss = 0.31502854\n",
      "Iteration 211, loss = 0.06413492\n",
      "Iteration 32, loss = 0.30967544\n",
      "Iteration 212, loss = 0.06259155\n",
      "Iteration 33, loss = 0.30561461\n",
      "Iteration 213, loss = 0.06128093\n",
      "Iteration 34, loss = 0.30006926\n",
      "Iteration 214, loss = 0.06192903\n",
      "Iteration 35, loss = 0.29448259\n",
      "Iteration 215, loss = 0.06125327\n",
      "Iteration 36, loss = 0.29051326\n",
      "Iteration 216, loss = 0.06054030\n",
      "Iteration 37, loss = 0.28546209\n",
      "Iteration 217, loss = 0.06008331\n",
      "Iteration 38, loss = 0.28184541\n",
      "Iteration 218, loss = 0.05897081\n",
      "Iteration 39, loss = 0.27644090\n",
      "Iteration 219, loss = 0.06050801\n",
      "Iteration 40, loss = 0.27286600\n",
      "Iteration 220, loss = 0.05855024\n",
      "Iteration 41, loss = 0.26832808\n",
      "Iteration 221, loss = 0.05809357\n",
      "Iteration 42, loss = 0.26407213\n",
      "Iteration 222, loss = 0.05769586\n",
      "Iteration 43, loss = 0.26219242\n",
      "Iteration 223, loss = 0.05766314\n",
      "Iteration 44, loss = 0.25711596\n",
      "Iteration 224, loss = 0.05674334\n",
      "Iteration 45, loss = 0.25297244\n",
      "Iteration 225, loss = 0.05615839\n",
      "Iteration 46, loss = 0.24956140\n",
      "Iteration 226, loss = 0.05577279\n",
      "Iteration 47, loss = 0.24594779\n",
      "Iteration 227, loss = 0.05607500\n",
      "Iteration 48, loss = 0.24222892\n",
      "Iteration 228, loss = 0.05513372\n",
      "Iteration 49, loss = 0.23944716\n",
      "Iteration 229, loss = 0.05506636\n",
      "Iteration 50, loss = 0.23564190\n",
      "Iteration 230, loss = 0.05513929\n",
      "Iteration 51, loss = 0.23172257\n",
      "Iteration 231, loss = 0.05430929\n",
      "Iteration 52, loss = 0.22948598\n",
      "Iteration 232, loss = 0.05387152\n",
      "Iteration 53, loss = 0.22553605\n",
      "Iteration 233, loss = 0.05367884\n",
      "Iteration 54, loss = 0.22357026\n",
      "Iteration 234, loss = 0.05332909\n",
      "Iteration 55, loss = 0.22005559\n",
      "Iteration 235, loss = 0.05328641\n",
      "Iteration 56, loss = 0.21677183\n",
      "Iteration 236, loss = 0.05257143\n",
      "Iteration 57, loss = 0.21445121\n",
      "Iteration 237, loss = 0.05249239\n",
      "Iteration 58, loss = 0.21231118\n",
      "Iteration 238, loss = 0.05168089\n",
      "Iteration 59, loss = 0.20890945\n",
      "Iteration 239, loss = 0.05173869\n",
      "Iteration 60, loss = 0.20601612\n",
      "Iteration 240, loss = 0.05182205\n",
      "Iteration 61, loss = 0.20386588\n",
      "Iteration 241, loss = 0.05192728\n",
      "Iteration 62, loss = 0.20208505\n",
      "Iteration 242, loss = 0.05100690\n",
      "Iteration 63, loss = 0.19952701\n",
      "Iteration 243, loss = 0.05085174\n",
      "Iteration 64, loss = 0.19695906\n",
      "Iteration 244, loss = 0.05103905\n",
      "Iteration 65, loss = 0.19520878\n",
      "Iteration 245, loss = 0.05217519\n",
      "Iteration 66, loss = 0.19221590\n",
      "Iteration 246, loss = 0.05119739\n",
      "Iteration 67, loss = 0.18979133\n",
      "Iteration 247, loss = 0.04966084\n",
      "Iteration 68, loss = 0.18831087\n",
      "Iteration 248, loss = 0.05039466\n",
      "Iteration 69, loss = 0.18640719\n",
      "Iteration 249, loss = 0.04825813\n",
      "Iteration 70, loss = 0.18444143\n",
      "Iteration 250, loss = 0.04952511\n",
      "Iteration 71, loss = 0.18399872\n",
      "Iteration 251, loss = 0.04812558\n",
      "Iteration 72, loss = 0.17971878\n",
      "Iteration 252, loss = 0.04734519\n",
      "Iteration 73, loss = 0.17908372\n",
      "Iteration 253, loss = 0.04893939\n",
      "Iteration 74, loss = 0.17532096\n",
      "Iteration 254, loss = 0.05027815\n",
      "Iteration 75, loss = 0.17565504\n",
      "Iteration 255, loss = 0.04622600\n",
      "Iteration 76, loss = 0.17304519\n",
      "Iteration 256, loss = 0.04747865\n",
      "Iteration 77, loss = 0.17343426\n",
      "Iteration 257, loss = 0.04762215\n",
      "Iteration 78, loss = 0.16875843\n",
      "Iteration 258, loss = 0.04793052\n",
      "Iteration 79, loss = 0.16862648\n",
      "Iteration 259, loss = 0.04593123\n",
      "Iteration 80, loss = 0.16545603\n",
      "Iteration 260, loss = 0.04893928\n",
      "Iteration 81, loss = 0.16474028\n",
      "Iteration 261, loss = 0.04451033\n",
      "Iteration 82, loss = 0.16222625\n",
      "Iteration 262, loss = 0.04590159\n",
      "Iteration 83, loss = 0.16117853\n",
      "Iteration 263, loss = 0.04494102\n",
      "Iteration 84, loss = 0.15990249\n",
      "Iteration 264, loss = 0.04471809\n",
      "Iteration 85, loss = 0.15766015\n",
      "Iteration 265, loss = 0.04379136\n",
      "Iteration 86, loss = 0.15701762\n",
      "Iteration 266, loss = 0.04603420\n",
      "Iteration 87, loss = 0.15473954\n",
      "Iteration 267, loss = 0.04418506\n",
      "Iteration 88, loss = 0.15372137\n",
      "Iteration 268, loss = 0.04418505\n",
      "Iteration 89, loss = 0.15194975\n",
      "Iteration 269, loss = 0.04430526\n",
      "Iteration 90, loss = 0.14982748\n",
      "Iteration 270, loss = 0.04403583\n",
      "Iteration 91, loss = 0.14953140\n",
      "Iteration 271, loss = 0.04344094\n",
      "Iteration 92, loss = 0.14869830\n",
      "Iteration 272, loss = 0.04310488\n",
      "Iteration 93, loss = 0.14704983\n",
      "Iteration 273, loss = 0.04374645\n",
      "Iteration 94, loss = 0.14575083\n",
      "Iteration 274, loss = 0.04221319\n",
      "Iteration 95, loss = 0.14384987\n",
      "Iteration 275, loss = 0.04205561\n",
      "Iteration 96, loss = 0.14271644\n",
      "Iteration 276, loss = 0.04241254\n",
      "Iteration 97, loss = 0.14091872\n",
      "Iteration 277, loss = 0.04185477\n",
      "Iteration 98, loss = 0.14066631\n",
      "Iteration 278, loss = 0.04162437\n",
      "Iteration 99, loss = 0.13984336\n",
      "Iteration 279, loss = 0.04168292\n",
      "Iteration 100, loss = 0.13832273\n",
      "Iteration 280, loss = 0.04224167\n",
      "Iteration 101, loss = 0.13642951\n",
      "Iteration 281, loss = 0.04114948\n",
      "Iteration 102, loss = 0.13477180\n",
      "Iteration 282, loss = 0.04040291\n",
      "Iteration 103, loss = 0.13583299\n",
      "Iteration 283, loss = 0.04138619\n",
      "Iteration 104, loss = 0.13335361\n",
      "Iteration 284, loss = 0.04216783\n",
      "Iteration 105, loss = 0.13290249\n",
      "Iteration 285, loss = 0.03963618\n",
      "Iteration 106, loss = 0.13103010\n",
      "Iteration 286, loss = 0.03958764\n",
      "Iteration 107, loss = 0.12930033\n",
      "Iteration 287, loss = 0.03933155\n",
      "Iteration 108, loss = 0.12972357\n",
      "Iteration 288, loss = 0.03952765\n",
      "Iteration 109, loss = 0.12655464\n",
      "Iteration 289, loss = 0.03858470\n",
      "Iteration 110, loss = 0.12701453\n",
      "Iteration 290, loss = 0.03865728\n",
      "Iteration 111, loss = 0.12547484\n",
      "Iteration 291, loss = 0.04015918\n",
      "Iteration 112, loss = 0.12486841\n",
      "Iteration 292, loss = 0.04013135\n",
      "Iteration 113, loss = 0.12435252\n",
      "Iteration 293, loss = 0.03656693\n",
      "Iteration 114, loss = 0.12428506\n",
      "Iteration 294, loss = 0.03970235\n",
      "Iteration 115, loss = 0.12255118\n",
      "Iteration 295, loss = 0.03996538\n",
      "Iteration 116, loss = 0.12131733\n",
      "Iteration 296, loss = 0.03936126\n",
      "Iteration 117, loss = 0.11948707\n",
      "Iteration 297, loss = 0.03655025\n",
      "Iteration 118, loss = 0.11821438\n",
      "Iteration 298, loss = 0.03832211\n",
      "Iteration 119, loss = 0.11737982\n",
      "Iteration 299, loss = 0.03975203\n",
      "Iteration 120, loss = 0.11751444\n",
      "Iteration 300, loss = 0.04000058\n",
      "Iteration 121, loss = 0.11539392\n",
      "Iteration 301, loss = 0.04047527\n",
      "Iteration 122, loss = 0.11426352\n",
      "Iteration 302, loss = 0.03764096\n",
      "Iteration 123, loss = 0.11558981\n",
      "Iteration 303, loss = 0.03731176\n",
      "Iteration 124, loss = 0.11631609\n",
      "Iteration 304, loss = 0.03857781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.11205953\n",
      "Iteration 126, loss = 0.11167111\n",
      "Iteration 127, loss = 0.11233056\n",
      "Iteration 128, loss = 0.11212841\n",
      "Iteration 129, loss = 0.11292403\n",
      "Iteration 130, loss = 0.11313496\n",
      "Iteration 131, loss = 0.11210095\n",
      "Iteration 132, loss = 0.10715643\n",
      "Iteration 133, loss = 0.10782901\n",
      "Iteration 134, loss = 0.10468488\n",
      "Iteration 135, loss = 0.10418508\n",
      "Iteration 136, loss = 0.10266612\n",
      "Iteration 137, loss = 0.10243838\n",
      "Iteration 138, loss = 0.10257566\n",
      "Iteration 139, loss = 0.10111574\n",
      "Iteration 140, loss = 0.10110860\n",
      "Iteration 141, loss = 0.10273390\n",
      "Iteration 142, loss = 0.10039424\n",
      "Iteration 143, loss = 0.09851021\n",
      "Iteration 1, loss = 1.64543361\n",
      "Iteration 144, loss = 0.09825872\n",
      "Iteration 2, loss = 1.30323500\n",
      "Iteration 145, loss = 0.09812993\n",
      "Iteration 3, loss = 1.08528835\n",
      "Iteration 146, loss = 0.09652681\n",
      "Iteration 4, loss = 0.94373502\n",
      "Iteration 147, loss = 0.09689151\n",
      "Iteration 5, loss = 0.84600362\n",
      "Iteration 148, loss = 0.09411240\n",
      "Iteration 6, loss = 0.77205358\n",
      "Iteration 149, loss = 0.09535193\n",
      "Iteration 7, loss = 0.70702960\n",
      "Iteration 150, loss = 0.09255932\n",
      "Iteration 8, loss = 0.65270160\n",
      "Iteration 151, loss = 0.09220878\n",
      "Iteration 9, loss = 0.60921019\n",
      "Iteration 152, loss = 0.09105207\n",
      "Iteration 10, loss = 0.57617956\n",
      "Iteration 153, loss = 0.09095614\n",
      "Iteration 11, loss = 0.54729364\n",
      "Iteration 154, loss = 0.09026926\n",
      "Iteration 12, loss = 0.52146182\n",
      "Iteration 155, loss = 0.09257221\n",
      "Iteration 13, loss = 0.49975818\n",
      "Iteration 156, loss = 0.08898463\n",
      "Iteration 14, loss = 0.47998036\n",
      "Iteration 157, loss = 0.09063421\n",
      "Iteration 15, loss = 0.46254409\n",
      "Iteration 158, loss = 0.09163613\n",
      "Iteration 16, loss = 0.44672440\n",
      "Iteration 159, loss = 0.09226956\n",
      "Iteration 17, loss = 0.43279186\n",
      "Iteration 160, loss = 0.08882995\n",
      "Iteration 18, loss = 0.41980012\n",
      "Iteration 161, loss = 0.08570028\n",
      "Iteration 19, loss = 0.40771310\n",
      "Iteration 162, loss = 0.08607962\n",
      "Iteration 20, loss = 0.39698352\n",
      "Iteration 163, loss = 0.08445991\n",
      "Iteration 21, loss = 0.38674928\n",
      "Iteration 164, loss = 0.08368811\n",
      "Iteration 22, loss = 0.37693848\n",
      "Iteration 165, loss = 0.08489412\n",
      "Iteration 23, loss = 0.36859646\n",
      "Iteration 166, loss = 0.08446000\n",
      "Iteration 24, loss = 0.36050901\n",
      "Iteration 25, loss = 0.35203717\n",
      "Iteration 167, loss = 0.08246201\n",
      "Iteration 26, loss = 0.34496568\n",
      "Iteration 168, loss = 0.08074399\n",
      "Iteration 27, loss = 0.33823384\n",
      "Iteration 169, loss = 0.07998827\n",
      "Iteration 28, loss = 0.33187958\n",
      "Iteration 170, loss = 0.08103211\n",
      "Iteration 171, loss = 0.08116152\n",
      "Iteration 29, loss = 0.32611259\n",
      "Iteration 172, loss = 0.07865197\n",
      "Iteration 30, loss = 0.32033054\n",
      "Iteration 31, loss = 0.31476891\n",
      "Iteration 173, loss = 0.07894566\n",
      "Iteration 32, loss = 0.30896143\n",
      "Iteration 174, loss = 0.08046052\n",
      "Iteration 33, loss = 0.30416087\n",
      "Iteration 175, loss = 0.07750260\n",
      "Iteration 34, loss = 0.29881890\n",
      "Iteration 176, loss = 0.07677152\n",
      "Iteration 35, loss = 0.29436660\n",
      "Iteration 177, loss = 0.07580510\n",
      "Iteration 36, loss = 0.29027084\n",
      "Iteration 178, loss = 0.07519808\n",
      "Iteration 37, loss = 0.28565785\n",
      "Iteration 179, loss = 0.07559198\n",
      "Iteration 38, loss = 0.28163596Iteration 180, loss = 0.07626673\n",
      "\n",
      "Iteration 39, loss = 0.27804210\n",
      "Iteration 181, loss = 0.07436994\n",
      "Iteration 182, loss = 0.07382175\n",
      "Iteration 40, loss = 0.27365404\n",
      "Iteration 41, loss = 0.27172038\n",
      "Iteration 183, loss = 0.07302677\n",
      "Iteration 184, loss = 0.07355679\n",
      "Iteration 42, loss = 0.26664396\n",
      "Iteration 43, loss = 0.26432636\n",
      "Iteration 185, loss = 0.07203289\n",
      "Iteration 44, loss = 0.25973327\n",
      "Iteration 186, loss = 0.07280412\n",
      "Iteration 45, loss = 0.25716335\n",
      "Iteration 187, loss = 0.07173275\n",
      "Iteration 46, loss = 0.25371243\n",
      "Iteration 188, loss = 0.07161829\n",
      "Iteration 47, loss = 0.25056622\n",
      "Iteration 189, loss = 0.07088417\n",
      "Iteration 48, loss = 0.24766169\n",
      "Iteration 190, loss = 0.06885195\n",
      "Iteration 49, loss = 0.24479906\n",
      "Iteration 191, loss = 0.06879455\n",
      "Iteration 50, loss = 0.24195011\n",
      "Iteration 192, loss = 0.06731267\n",
      "Iteration 51, loss = 0.23991947\n",
      "Iteration 193, loss = 0.06846556\n",
      "Iteration 52, loss = 0.23671754\n",
      "Iteration 194, loss = 0.06747192\n",
      "Iteration 53, loss = 0.23389031\n",
      "Iteration 195, loss = 0.06696187\n",
      "Iteration 54, loss = 0.23259706\n",
      "Iteration 196, loss = 0.06665317\n",
      "Iteration 55, loss = 0.22865952\n",
      "Iteration 197, loss = 0.06557860\n",
      "Iteration 56, loss = 0.22695995\n",
      "Iteration 198, loss = 0.06694151\n",
      "Iteration 57, loss = 0.22374137\n",
      "Iteration 199, loss = 0.06503318\n",
      "Iteration 58, loss = 0.22092600\n",
      "Iteration 200, loss = 0.06497868\n",
      "Iteration 59, loss = 0.21922822\n",
      "Iteration 201, loss = 0.06422598\n",
      "Iteration 60, loss = 0.21660059\n",
      "Iteration 202, loss = 0.06362519\n",
      "Iteration 61, loss = 0.21457569\n",
      "Iteration 203, loss = 0.06330030\n",
      "Iteration 62, loss = 0.21173504\n",
      "Iteration 204, loss = 0.06348907\n",
      "Iteration 63, loss = 0.21041068\n",
      "Iteration 205, loss = 0.06205438\n",
      "Iteration 64, loss = 0.20804800\n",
      "Iteration 206, loss = 0.06181186\n",
      "Iteration 65, loss = 0.20630862\n",
      "Iteration 207, loss = 0.06113731\n",
      "Iteration 66, loss = 0.20464230\n",
      "Iteration 208, loss = 0.06331423\n",
      "Iteration 67, loss = 0.20116498\n",
      "Iteration 209, loss = 0.06235275\n",
      "Iteration 68, loss = 0.20040469\n",
      "Iteration 210, loss = 0.05954803\n",
      "Iteration 69, loss = 0.19891258\n",
      "Iteration 211, loss = 0.06380301\n",
      "Iteration 70, loss = 0.19547270\n",
      "Iteration 212, loss = 0.06242492\n",
      "Iteration 71, loss = 0.19403409\n",
      "Iteration 213, loss = 0.05947444\n",
      "Iteration 72, loss = 0.19179913\n",
      "Iteration 214, loss = 0.06032208\n",
      "Iteration 73, loss = 0.18951442\n",
      "Iteration 215, loss = 0.05975138\n",
      "Iteration 74, loss = 0.18830823\n",
      "Iteration 216, loss = 0.05864966\n",
      "Iteration 75, loss = 0.18616822\n",
      "Iteration 217, loss = 0.05891879\n",
      "Iteration 76, loss = 0.18401285\n",
      "Iteration 218, loss = 0.05720472\n",
      "Iteration 77, loss = 0.18245555\n",
      "Iteration 219, loss = 0.05781211\n",
      "Iteration 78, loss = 0.18160390\n",
      "Iteration 220, loss = 0.05720633\n",
      "Iteration 79, loss = 0.17930953\n",
      "Iteration 221, loss = 0.05543136\n",
      "Iteration 80, loss = 0.17754999\n",
      "Iteration 222, loss = 0.05653761\n",
      "Iteration 81, loss = 0.17561716\n",
      "Iteration 223, loss = 0.05545033\n",
      "Iteration 82, loss = 0.17517597\n",
      "Iteration 224, loss = 0.05734300\n",
      "Iteration 83, loss = 0.17281366\n",
      "Iteration 225, loss = 0.05717129\n",
      "Iteration 84, loss = 0.17115928\n",
      "Iteration 226, loss = 0.05487375\n",
      "Iteration 85, loss = 0.16940784\n",
      "Iteration 227, loss = 0.05390502\n",
      "Iteration 86, loss = 0.16912037\n",
      "Iteration 228, loss = 0.05304008\n",
      "Iteration 87, loss = 0.16590050\n",
      "Iteration 229, loss = 0.05292332\n",
      "Iteration 88, loss = 0.16544845\n",
      "Iteration 230, loss = 0.05275120\n",
      "Iteration 89, loss = 0.16392106\n",
      "Iteration 231, loss = 0.05365471\n",
      "Iteration 90, loss = 0.16161020\n",
      "Iteration 232, loss = 0.05458614\n",
      "Iteration 91, loss = 0.16022643\n",
      "Iteration 233, loss = 0.05392713\n",
      "Iteration 92, loss = 0.15881198\n",
      "Iteration 234, loss = 0.05324475\n",
      "Iteration 93, loss = 0.15762181\n",
      "Iteration 235, loss = 0.05352837\n",
      "Iteration 94, loss = 0.15554585\n",
      "Iteration 236, loss = 0.05264140\n",
      "Iteration 95, loss = 0.15672929\n",
      "Iteration 237, loss = 0.05117229\n",
      "Iteration 96, loss = 0.15259309\n",
      "Iteration 238, loss = 0.05167129\n",
      "Iteration 97, loss = 0.15064101\n",
      "Iteration 239, loss = 0.05317167\n",
      "Iteration 98, loss = 0.14978857\n",
      "Iteration 240, loss = 0.04944838\n",
      "Iteration 99, loss = 0.14825815\n",
      "Iteration 241, loss = 0.04920882\n",
      "Iteration 100, loss = 0.14663394\n",
      "Iteration 242, loss = 0.04924074\n",
      "Iteration 101, loss = 0.14577626\n",
      "Iteration 243, loss = 0.04862433\n",
      "Iteration 102, loss = 0.14455301\n",
      "Iteration 244, loss = 0.04880964\n",
      "Iteration 103, loss = 0.14378091\n",
      "Iteration 245, loss = 0.04820468\n",
      "Iteration 104, loss = 0.14164052\n",
      "Iteration 246, loss = 0.04758896\n",
      "Iteration 105, loss = 0.14089668\n",
      "Iteration 247, loss = 0.04801590\n",
      "Iteration 106, loss = 0.13935144\n",
      "Iteration 248, loss = 0.04697204\n",
      "Iteration 107, loss = 0.13792587\n",
      "Iteration 249, loss = 0.04664374\n",
      "Iteration 108, loss = 0.13692406\n",
      "Iteration 250, loss = 0.04663053\n",
      "Iteration 109, loss = 0.13546914\n",
      "Iteration 251, loss = 0.04830760\n",
      "Iteration 110, loss = 0.13517034\n",
      "Iteration 252, loss = 0.04630470\n",
      "Iteration 111, loss = 0.13335438\n",
      "Iteration 253, loss = 0.04575733\n",
      "Iteration 112, loss = 0.13148074\n",
      "Iteration 254, loss = 0.04498461\n",
      "Iteration 113, loss = 0.13016111\n",
      "Iteration 255, loss = 0.04468884\n",
      "Iteration 114, loss = 0.12906789\n",
      "Iteration 256, loss = 0.04536361\n",
      "Iteration 115, loss = 0.12844371\n",
      "Iteration 257, loss = 0.04783693\n",
      "Iteration 116, loss = 0.12653594\n",
      "Iteration 258, loss = 0.04572770\n",
      "Iteration 117, loss = 0.12890836\n",
      "Iteration 259, loss = 0.04443584\n",
      "Iteration 118, loss = 0.12588001\n",
      "Iteration 260, loss = 0.04466497\n",
      "Iteration 119, loss = 0.12437679\n",
      "Iteration 261, loss = 0.04600686\n",
      "Iteration 120, loss = 0.12272422\n",
      "Iteration 262, loss = 0.04405630\n",
      "Iteration 121, loss = 0.12197083\n",
      "Iteration 263, loss = 0.04314763\n",
      "Iteration 122, loss = 0.12082631\n",
      "Iteration 264, loss = 0.04223121\n",
      "Iteration 123, loss = 0.11965479\n",
      "Iteration 265, loss = 0.04231619\n",
      "Iteration 124, loss = 0.11934663\n",
      "Iteration 266, loss = 0.04190857\n",
      "Iteration 125, loss = 0.11690378\n",
      "Iteration 267, loss = 0.04197417\n",
      "Iteration 126, loss = 0.11683872\n",
      "Iteration 268, loss = 0.04290752\n",
      "Iteration 127, loss = 0.11500640\n",
      "Iteration 269, loss = 0.04088775\n",
      "Iteration 128, loss = 0.11534361\n",
      "Iteration 270, loss = 0.04159042\n",
      "Iteration 129, loss = 0.11417247\n",
      "Iteration 271, loss = 0.04114454\n",
      "Iteration 130, loss = 0.11247166\n",
      "Iteration 272, loss = 0.04074308\n",
      "Iteration 131, loss = 0.11261308\n",
      "Iteration 273, loss = 0.04254670\n",
      "Iteration 132, loss = 0.11131748\n",
      "Iteration 274, loss = 0.04176569\n",
      "Iteration 133, loss = 0.11062960\n",
      "Iteration 275, loss = 0.04106939\n",
      "Iteration 134, loss = 0.10952553\n",
      "Iteration 276, loss = 0.04175848\n",
      "Iteration 135, loss = 0.10733178\n",
      "Iteration 277, loss = 0.04067460\n",
      "Iteration 136, loss = 0.10759401\n",
      "Iteration 278, loss = 0.04011904\n",
      "Iteration 137, loss = 0.10694655\n",
      "Iteration 279, loss = 0.03983559\n",
      "Iteration 138, loss = 0.10564480\n",
      "Iteration 280, loss = 0.04106877\n",
      "Iteration 139, loss = 0.10529018\n",
      "Iteration 281, loss = 0.04022026\n",
      "Iteration 140, loss = 0.10655798\n",
      "Iteration 282, loss = 0.04004467\n",
      "Iteration 141, loss = 0.10162742\n",
      "Iteration 283, loss = 0.03882407\n",
      "Iteration 142, loss = 0.10349570\n",
      "Iteration 284, loss = 0.03801150\n",
      "Iteration 143, loss = 0.10158287\n",
      "Iteration 285, loss = 0.03876060\n",
      "Iteration 144, loss = 0.10116626\n",
      "Iteration 286, loss = 0.03779693\n",
      "Iteration 145, loss = 0.09921257\n",
      "Iteration 287, loss = 0.03791846\n",
      "Iteration 146, loss = 0.09743707\n",
      "Iteration 288, loss = 0.03704943\n",
      "Iteration 147, loss = 0.09730628\n",
      "Iteration 289, loss = 0.03887321\n",
      "Iteration 148, loss = 0.09599525\n",
      "Iteration 290, loss = 0.03820681\n",
      "Iteration 149, loss = 0.09513836\n",
      "Iteration 291, loss = 0.03776125\n",
      "Iteration 150, loss = 0.09470767\n",
      "Iteration 292, loss = 0.03653825\n",
      "Iteration 151, loss = 0.09414819\n",
      "Iteration 293, loss = 0.03646159\n",
      "Iteration 152, loss = 0.09401510\n",
      "Iteration 294, loss = 0.03638324\n",
      "Iteration 153, loss = 0.09308538\n",
      "Iteration 295, loss = 0.03608794\n",
      "Iteration 154, loss = 0.09196711\n",
      "Iteration 296, loss = 0.03719089\n",
      "Iteration 155, loss = 0.09195947\n",
      "Iteration 297, loss = 0.03575941\n",
      "Iteration 156, loss = 0.08996474\n",
      "Iteration 298, loss = 0.03657821\n",
      "Iteration 157, loss = 0.09131880\n",
      "Iteration 299, loss = 0.03630457\n",
      "Iteration 158, loss = 0.08886957\n",
      "Iteration 300, loss = 0.03577411\n",
      "Iteration 159, loss = 0.08991217\n",
      "Iteration 301, loss = 0.03651453\n",
      "Iteration 160, loss = 0.08669783\n",
      "Iteration 302, loss = 0.03481337\n",
      "Iteration 161, loss = 0.08547577\n",
      "Iteration 303, loss = 0.03625483\n",
      "Iteration 162, loss = 0.08472495\n",
      "Iteration 304, loss = 0.03573729\n",
      "Iteration 163, loss = 0.08637589\n",
      "Iteration 305, loss = 0.03354214\n",
      "Iteration 164, loss = 0.08419840\n",
      "Iteration 306, loss = 0.03548966\n",
      "Iteration 165, loss = 0.08244017\n",
      "Iteration 307, loss = 0.03407614\n",
      "Iteration 166, loss = 0.08339535\n",
      "Iteration 308, loss = 0.03368381\n",
      "Iteration 167, loss = 0.08194614\n",
      "Iteration 309, loss = 0.03323993\n",
      "Iteration 168, loss = 0.08099548\n",
      "Iteration 310, loss = 0.03305244\n",
      "Iteration 169, loss = 0.07937130\n",
      "Iteration 311, loss = 0.03298832\n",
      "Iteration 170, loss = 0.08039454\n",
      "Iteration 312, loss = 0.03262735\n",
      "Iteration 171, loss = 0.07852787\n",
      "Iteration 313, loss = 0.03250564\n",
      "Iteration 172, loss = 0.07813509\n",
      "Iteration 314, loss = 0.03299145\n",
      "Iteration 173, loss = 0.07791569\n",
      "Iteration 315, loss = 0.03200464\n",
      "Iteration 174, loss = 0.07618144\n",
      "Iteration 316, loss = 0.03312348\n",
      "Iteration 175, loss = 0.07586773\n",
      "Iteration 317, loss = 0.03205222\n",
      "Iteration 176, loss = 0.07542396\n",
      "Iteration 318, loss = 0.03155056\n",
      "Iteration 177, loss = 0.07500472\n",
      "Iteration 319, loss = 0.03124385\n",
      "Iteration 178, loss = 0.07439083\n",
      "Iteration 179, loss = 0.07564882\n",
      "Iteration 320, loss = 0.03173533\n",
      "Iteration 180, loss = 0.07304780\n",
      "Iteration 321, loss = 0.03193311\n",
      "Iteration 181, loss = 0.07224033\n",
      "Iteration 322, loss = 0.03204298\n",
      "Iteration 182, loss = 0.07471637\n",
      "Iteration 323, loss = 0.03063539\n",
      "Iteration 183, loss = 0.07230333\n",
      "Iteration 324, loss = 0.03125260\n",
      "Iteration 184, loss = 0.07205754\n",
      "Iteration 325, loss = 0.03075411\n",
      "Iteration 185, loss = 0.07053521\n",
      "Iteration 326, loss = 0.03243469\n",
      "Iteration 186, loss = 0.06995476\n",
      "Iteration 327, loss = 0.03163866\n",
      "Iteration 187, loss = 0.07054266\n",
      "Iteration 328, loss = 0.03081181\n",
      "Iteration 188, loss = 0.06881579\n",
      "Iteration 329, loss = 0.03269448\n",
      "Iteration 189, loss = 0.06765558\n",
      "Iteration 330, loss = 0.03007385\n",
      "Iteration 190, loss = 0.06936120\n",
      "Iteration 331, loss = 0.03092477\n",
      "Iteration 191, loss = 0.06747265\n",
      "Iteration 332, loss = 0.03048507\n",
      "Iteration 192, loss = 0.06651496\n",
      "Iteration 333, loss = 0.02967650\n",
      "Iteration 193, loss = 0.06730756\n",
      "Iteration 334, loss = 0.02885213\n",
      "Iteration 194, loss = 0.06615061\n",
      "Iteration 335, loss = 0.02921433\n",
      "Iteration 195, loss = 0.06419836\n",
      "Iteration 336, loss = 0.02978031\n",
      "Iteration 196, loss = 0.06413885\n",
      "Iteration 337, loss = 0.03009494\n",
      "Iteration 197, loss = 0.06493433\n",
      "Iteration 338, loss = 0.02939798\n",
      "Iteration 198, loss = 0.06560735\n",
      "Iteration 339, loss = 0.02965436\n",
      "Iteration 199, loss = 0.06493955\n",
      "Iteration 340, loss = 0.03198513\n",
      "Iteration 200, loss = 0.06454609\n",
      "Iteration 341, loss = 0.02811418\n",
      "Iteration 201, loss = 0.06269559\n",
      "Iteration 342, loss = 0.02976088\n",
      "Iteration 202, loss = 0.06253518\n",
      "Iteration 343, loss = 0.02917546\n",
      "Iteration 203, loss = 0.06113368\n",
      "Iteration 344, loss = 0.02905487\n",
      "Iteration 204, loss = 0.06017113\n",
      "Iteration 345, loss = 0.02880552\n",
      "Iteration 205, loss = 0.06043881\n",
      "Iteration 346, loss = 0.02797920\n",
      "Iteration 206, loss = 0.05932428\n",
      "Iteration 347, loss = 0.02725799\n",
      "Iteration 207, loss = 0.05868146\n",
      "Iteration 348, loss = 0.02746127\n",
      "Iteration 208, loss = 0.05919453\n",
      "Iteration 349, loss = 0.02761349\n",
      "Iteration 209, loss = 0.05788781\n",
      "Iteration 350, loss = 0.02741524\n",
      "Iteration 210, loss = 0.05753470\n",
      "Iteration 351, loss = 0.02726937\n",
      "Iteration 211, loss = 0.05816483\n",
      "Iteration 352, loss = 0.02693617\n",
      "Iteration 212, loss = 0.05717210\n",
      "Iteration 353, loss = 0.02756944\n",
      "Iteration 213, loss = 0.05674362\n",
      "Iteration 354, loss = 0.02737000\n",
      "Iteration 214, loss = 0.05740815\n",
      "Iteration 355, loss = 0.02703446\n",
      "Iteration 215, loss = 0.05607177\n",
      "Iteration 356, loss = 0.02924768\n",
      "Iteration 216, loss = 0.05679544\n",
      "Iteration 357, loss = 0.03015696\n",
      "Iteration 217, loss = 0.05526188\n",
      "Iteration 358, loss = 0.02946462\n",
      "Iteration 218, loss = 0.05542804\n",
      "Iteration 359, loss = 0.02663339\n",
      "Iteration 219, loss = 0.05443788\n",
      "Iteration 360, loss = 0.02868782\n",
      "Iteration 220, loss = 0.05375606\n",
      "Iteration 361, loss = 0.02823984\n",
      "Iteration 221, loss = 0.05342991\n",
      "Iteration 362, loss = 0.02618066\n",
      "Iteration 222, loss = 0.05271333\n",
      "Iteration 363, loss = 0.02613512\n",
      "Iteration 223, loss = 0.05584139\n",
      "Iteration 364, loss = 0.02631051\n",
      "Iteration 224, loss = 0.05528289\n",
      "Iteration 365, loss = 0.02786928\n",
      "Iteration 225, loss = 0.05205140\n",
      "Iteration 366, loss = 0.02810082\n",
      "Iteration 226, loss = 0.05302438\n",
      "Iteration 367, loss = 0.02759758\n",
      "Iteration 227, loss = 0.05196449\n",
      "Iteration 368, loss = 0.02545104\n",
      "Iteration 228, loss = 0.05106468\n",
      "Iteration 369, loss = 0.02708731\n",
      "Iteration 229, loss = 0.05077540\n",
      "Iteration 370, loss = 0.02538785\n",
      "Iteration 230, loss = 0.05085144\n",
      "Iteration 371, loss = 0.02472524\n",
      "Iteration 231, loss = 0.05038443\n",
      "Iteration 372, loss = 0.02492222\n",
      "Iteration 232, loss = 0.05146200\n",
      "Iteration 373, loss = 0.02462766\n",
      "Iteration 233, loss = 0.04945722\n",
      "Iteration 374, loss = 0.02499565\n",
      "Iteration 234, loss = 0.04948748\n",
      "Iteration 375, loss = 0.02643761\n",
      "Iteration 235, loss = 0.04904936\n",
      "Iteration 376, loss = 0.02407503\n",
      "Iteration 236, loss = 0.04915931\n",
      "Iteration 377, loss = 0.02444810\n",
      "Iteration 237, loss = 0.05016764\n",
      "Iteration 378, loss = 0.02392142\n",
      "Iteration 238, loss = 0.05227223\n",
      "Iteration 379, loss = 0.02425389\n",
      "Iteration 239, loss = 0.04672889\n",
      "Iteration 380, loss = 0.02411899\n",
      "Iteration 240, loss = 0.05048833\n",
      "Iteration 381, loss = 0.02534347\n",
      "Iteration 241, loss = 0.04771304\n",
      "Iteration 382, loss = 0.02316001\n",
      "Iteration 242, loss = 0.04693465\n",
      "Iteration 383, loss = 0.02360122\n",
      "Iteration 243, loss = 0.04662412\n",
      "Iteration 384, loss = 0.02367253\n",
      "Iteration 244, loss = 0.04844795\n",
      "Iteration 385, loss = 0.02331908\n",
      "Iteration 245, loss = 0.04582325\n",
      "Iteration 386, loss = 0.02286330\n",
      "Iteration 246, loss = 0.04823744\n",
      "Iteration 387, loss = 0.02390658\n",
      "Iteration 247, loss = 0.04599076\n",
      "Iteration 388, loss = 0.02248783\n",
      "Iteration 248, loss = 0.04650748\n",
      "Iteration 389, loss = 0.02410636\n",
      "Iteration 249, loss = 0.04523484\n",
      "Iteration 390, loss = 0.02267743\n",
      "Iteration 250, loss = 0.04508285\n",
      "Iteration 251, loss = 0.04498278Iteration 391, loss = 0.02286894\n",
      "\n",
      "Iteration 252, loss = 0.04505148\n",
      "Iteration 392, loss = 0.02257902\n",
      "Iteration 393, loss = 0.02249549\n",
      "Iteration 253, loss = 0.04356400\n",
      "Iteration 394, loss = 0.02286649\n",
      "Iteration 254, loss = 0.04409074\n",
      "Iteration 395, loss = 0.02336512\n",
      "Iteration 255, loss = 0.04369851\n",
      "Iteration 396, loss = 0.02527612\n",
      "Iteration 256, loss = 0.04359959\n",
      "Iteration 397, loss = 0.02189048\n",
      "Iteration 257, loss = 0.04363335\n",
      "Iteration 398, loss = 0.02328514\n",
      "Iteration 258, loss = 0.04501637\n",
      "Iteration 399, loss = 0.02826115\n",
      "Iteration 259, loss = 0.04355667\n",
      "Iteration 400, loss = 0.02546167\n",
      "Iteration 260, loss = 0.04297803\n",
      "Iteration 401, loss = 0.02185748\n",
      "Iteration 261, loss = 0.04387079\n",
      "Iteration 402, loss = 0.02206173\n",
      "Iteration 262, loss = 0.04271411\n",
      "Iteration 403, loss = 0.02186814\n",
      "Iteration 263, loss = 0.04168800\n",
      "Iteration 404, loss = 0.02188452\n",
      "Iteration 264, loss = 0.04299619\n",
      "Iteration 405, loss = 0.02220247\n",
      "Iteration 265, loss = 0.04352273\n",
      "Iteration 266, loss = 0.04382316\n",
      "Iteration 406, loss = 0.02183006\n",
      "Iteration 267, loss = 0.04333493\n",
      "Iteration 407, loss = 0.02102214\n",
      "Iteration 268, loss = 0.04308837\n",
      "Iteration 408, loss = 0.02295896\n",
      "Iteration 269, loss = 0.04224709\n",
      "Iteration 409, loss = 0.02399533\n",
      "Iteration 270, loss = 0.04050715\n",
      "Iteration 410, loss = 0.02397586\n",
      "Iteration 271, loss = 0.04003797\n",
      "Iteration 411, loss = 0.02023421\n",
      "Iteration 272, loss = 0.04218687\n",
      "Iteration 412, loss = 0.02275535\n",
      "Iteration 273, loss = 0.04145342\n",
      "Iteration 413, loss = 0.02136295\n",
      "Iteration 274, loss = 0.04098187\n",
      "Iteration 414, loss = 0.02435187\n",
      "Iteration 275, loss = 0.04150311\n",
      "Iteration 415, loss = 0.02164995\n",
      "Iteration 276, loss = 0.03957636\n",
      "Iteration 416, loss = 0.02207867\n",
      "Iteration 277, loss = 0.03941287\n",
      "Iteration 417, loss = 0.02115321\n",
      "Iteration 278, loss = 0.03898345\n",
      "Iteration 418, loss = 0.02053729\n",
      "Iteration 419, loss = 0.01978064\n",
      "Iteration 279, loss = 0.03815674\n",
      "Iteration 420, loss = 0.02073976\n",
      "Iteration 280, loss = 0.03864728\n",
      "Iteration 421, loss = 0.02125236\n",
      "Iteration 281, loss = 0.03847116\n",
      "Iteration 422, loss = 0.02209722\n",
      "Iteration 282, loss = 0.03832846\n",
      "Iteration 423, loss = 0.02010077\n",
      "Iteration 283, loss = 0.03714723\n",
      "Iteration 424, loss = 0.01971571\n",
      "Iteration 284, loss = 0.03763893\n",
      "Iteration 285, loss = 0.03705627\n",
      "Iteration 425, loss = 0.02105616\n",
      "Iteration 286, loss = 0.03749621\n",
      "Iteration 426, loss = 0.01973340\n",
      "Iteration 287, loss = 0.03673200\n",
      "Iteration 427, loss = 0.02250952\n",
      "Iteration 288, loss = 0.03655864\n",
      "Iteration 428, loss = 0.02042080\n",
      "Iteration 289, loss = 0.03645419\n",
      "Iteration 429, loss = 0.02101790\n",
      "Iteration 290, loss = 0.03616021\n",
      "Iteration 430, loss = 0.02240196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 291, loss = 0.03632315\n",
      "Iteration 292, loss = 0.03578942\n",
      "Iteration 293, loss = 0.03621703\n",
      "Iteration 294, loss = 0.03516600\n",
      "Iteration 295, loss = 0.03569041\n",
      "Iteration 296, loss = 0.03561891\n",
      "Iteration 297, loss = 0.03564124\n",
      "Iteration 298, loss = 0.03500721\n",
      "Iteration 299, loss = 0.03441138\n",
      "Iteration 300, loss = 0.03600064\n",
      "Iteration 301, loss = 0.03465658\n",
      "Iteration 302, loss = 0.03450608\n",
      "Iteration 303, loss = 0.03599510\n",
      "Iteration 304, loss = 0.03718413\n",
      "Iteration 305, loss = 0.03532364\n",
      "Iteration 306, loss = 0.03543731\n",
      "Iteration 307, loss = 0.03431777\n",
      "Iteration 308, loss = 0.03610758\n",
      "Iteration 1, loss = 1.28297522\n",
      "Iteration 309, loss = 0.03641409\n",
      "Iteration 2, loss = 1.16268825\n",
      "Iteration 310, loss = 0.03426945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 1.06429901\n",
      "Iteration 4, loss = 0.97932405\n",
      "Iteration 5, loss = 0.90022174\n",
      "Iteration 6, loss = 0.82360591\n",
      "Iteration 7, loss = 0.75275243\n",
      "Iteration 8, loss = 0.68900680\n",
      "Iteration 9, loss = 0.63653074\n",
      "Iteration 10, loss = 0.59316814\n",
      "Iteration 11, loss = 0.55845473\n",
      "Iteration 12, loss = 0.52971352\n",
      "Iteration 13, loss = 0.50551341\n",
      "Iteration 14, loss = 0.48769015\n",
      "Iteration 15, loss = 0.46912059\n",
      "Iteration 16, loss = 0.45312291\n",
      "Iteration 17, loss = 0.43989237\n",
      "Iteration 18, loss = 0.42767663\n",
      "Iteration 19, loss = 0.41619042\n",
      "Iteration 20, loss = 0.40635832\n",
      "Iteration 21, loss = 0.39750066\n",
      "Iteration 1, loss = 1.23565131\n",
      "Iteration 22, loss = 0.38956420\n",
      "Iteration 2, loss = 1.09442723\n",
      "Iteration 23, loss = 0.38101935\n",
      "Iteration 3, loss = 0.99260932\n",
      "Iteration 24, loss = 0.37430663\n",
      "Iteration 4, loss = 0.90758166\n",
      "Iteration 25, loss = 0.36683568\n",
      "Iteration 5, loss = 0.83087043\n",
      "Iteration 26, loss = 0.36047013\n",
      "Iteration 6, loss = 0.75888835\n",
      "Iteration 27, loss = 0.35420489\n",
      "Iteration 7, loss = 0.69344763\n",
      "Iteration 28, loss = 0.34840234\n",
      "Iteration 8, loss = 0.63469270\n",
      "Iteration 29, loss = 0.34411045\n",
      "Iteration 9, loss = 0.58768364\n",
      "Iteration 30, loss = 0.33707138\n",
      "Iteration 10, loss = 0.54728478\n",
      "Iteration 31, loss = 0.33180868\n",
      "Iteration 11, loss = 0.51367746\n",
      "Iteration 32, loss = 0.32675298\n",
      "Iteration 12, loss = 0.48590689\n",
      "Iteration 33, loss = 0.32158676\n",
      "Iteration 13, loss = 0.46353765\n",
      "Iteration 34, loss = 0.31703343\n",
      "Iteration 14, loss = 0.44447098\n",
      "Iteration 35, loss = 0.31227917\n",
      "Iteration 15, loss = 0.42753594\n",
      "Iteration 36, loss = 0.30733241\n",
      "Iteration 16, loss = 0.41215788\n",
      "Iteration 37, loss = 0.30327733\n",
      "Iteration 17, loss = 0.39931755\n",
      "Iteration 38, loss = 0.29905640\n",
      "Iteration 18, loss = 0.38815136\n",
      "Iteration 39, loss = 0.29430558\n",
      "Iteration 19, loss = 0.37694450\n",
      "Iteration 40, loss = 0.28945196\n",
      "Iteration 20, loss = 0.36764722\n",
      "Iteration 41, loss = 0.28727758\n",
      "Iteration 21, loss = 0.35797029\n",
      "Iteration 42, loss = 0.28216338\n",
      "Iteration 22, loss = 0.35020316\n",
      "Iteration 43, loss = 0.27862798\n",
      "Iteration 23, loss = 0.34204713\n",
      "Iteration 44, loss = 0.27447912\n",
      "Iteration 24, loss = 0.33434481\n",
      "Iteration 45, loss = 0.26951239\n",
      "Iteration 25, loss = 0.32769219\n",
      "Iteration 46, loss = 0.26669432\n",
      "Iteration 26, loss = 0.32116042\n",
      "Iteration 47, loss = 0.26237833\n",
      "Iteration 27, loss = 0.31484950\n",
      "Iteration 48, loss = 0.25840427\n",
      "Iteration 28, loss = 0.30893552\n",
      "Iteration 49, loss = 0.25527004\n",
      "Iteration 29, loss = 0.30304323\n",
      "Iteration 50, loss = 0.25145875\n",
      "Iteration 30, loss = 0.29797693\n",
      "Iteration 51, loss = 0.24843441\n",
      "Iteration 31, loss = 0.29261908\n",
      "Iteration 52, loss = 0.24494661\n",
      "Iteration 32, loss = 0.28706030\n",
      "Iteration 53, loss = 0.24243858\n",
      "Iteration 33, loss = 0.28152661\n",
      "Iteration 54, loss = 0.23918915\n",
      "Iteration 34, loss = 0.27746691\n",
      "Iteration 55, loss = 0.23563331\n",
      "Iteration 35, loss = 0.27255094\n",
      "Iteration 56, loss = 0.23294126Iteration 36, loss = 0.26830595\n",
      "\n",
      "Iteration 37, loss = 0.26330842\n",
      "Iteration 57, loss = 0.22954554\n",
      "Iteration 38, loss = 0.25894362\n",
      "Iteration 39, loss = 0.25460585\n",
      "Iteration 40, loss = 0.25007367\n",
      "Iteration 58, loss = 0.22704576\n",
      "Iteration 41, loss = 0.24832522\n",
      "Iteration 59, loss = 0.22407461\n",
      "Iteration 42, loss = 0.24316379\n",
      "Iteration 60, loss = 0.22045268\n",
      "Iteration 43, loss = 0.23989046\n",
      "Iteration 61, loss = 0.21909485\n",
      "Iteration 44, loss = 0.23435223\n",
      "Iteration 62, loss = 0.21760016\n",
      "Iteration 45, loss = 0.23235635\n",
      "Iteration 63, loss = 0.21497631\n",
      "Iteration 46, loss = 0.22726842\n",
      "Iteration 64, loss = 0.21101265\n",
      "Iteration 47, loss = 0.22550319\n",
      "Iteration 65, loss = 0.21018727\n",
      "Iteration 48, loss = 0.22148562\n",
      "Iteration 66, loss = 0.20621611\n",
      "Iteration 49, loss = 0.21760735\n",
      "Iteration 67, loss = 0.20365294\n",
      "Iteration 50, loss = 0.21462407\n",
      "Iteration 68, loss = 0.20121301\n",
      "Iteration 51, loss = 0.20962597\n",
      "Iteration 69, loss = 0.20028957\n",
      "Iteration 52, loss = 0.20846775\n",
      "Iteration 70, loss = 0.19737995\n",
      "Iteration 53, loss = 0.20589798\n",
      "Iteration 71, loss = 0.19622565\n",
      "Iteration 54, loss = 0.20301887\n",
      "Iteration 72, loss = 0.19232845\n",
      "Iteration 55, loss = 0.19813260\n",
      "Iteration 73, loss = 0.18982512\n",
      "Iteration 56, loss = 0.19666914\n",
      "Iteration 74, loss = 0.18849663\n",
      "Iteration 57, loss = 0.19190398\n",
      "Iteration 75, loss = 0.18700413\n",
      "Iteration 58, loss = 0.19065253\n",
      "Iteration 76, loss = 0.18415818\n",
      "Iteration 59, loss = 0.18686869\n",
      "Iteration 77, loss = 0.18246313\n",
      "Iteration 60, loss = 0.18415879\n",
      "Iteration 78, loss = 0.18062392\n",
      "Iteration 61, loss = 0.18015268\n",
      "Iteration 79, loss = 0.17845415\n",
      "Iteration 62, loss = 0.17748370\n",
      "Iteration 80, loss = 0.17566677\n",
      "Iteration 63, loss = 0.17418231\n",
      "Iteration 81, loss = 0.17391318\n",
      "Iteration 64, loss = 0.17379626\n",
      "Iteration 82, loss = 0.17129614\n",
      "Iteration 65, loss = 0.17080861\n",
      "Iteration 83, loss = 0.17044128\n",
      "Iteration 66, loss = 0.16695743\n",
      "Iteration 84, loss = 0.16989213\n",
      "Iteration 67, loss = 0.16544120\n",
      "Iteration 85, loss = 0.16714146\n",
      "Iteration 68, loss = 0.16404900\n",
      "Iteration 86, loss = 0.16558687\n",
      "Iteration 69, loss = 0.16004068\n",
      "Iteration 87, loss = 0.16338140\n",
      "Iteration 70, loss = 0.15791379\n",
      "Iteration 88, loss = 0.16168711\n",
      "Iteration 71, loss = 0.15585363\n",
      "Iteration 89, loss = 0.15998341\n",
      "Iteration 72, loss = 0.15340183\n",
      "Iteration 90, loss = 0.16041697\n",
      "Iteration 73, loss = 0.15146364\n",
      "Iteration 91, loss = 0.15745925\n",
      "Iteration 74, loss = 0.14908620\n",
      "Iteration 92, loss = 0.15603886\n",
      "Iteration 75, loss = 0.14758062\n",
      "Iteration 93, loss = 0.15404544\n",
      "Iteration 76, loss = 0.14593305\n",
      "Iteration 94, loss = 0.15247484\n",
      "Iteration 77, loss = 0.14335763\n",
      "Iteration 95, loss = 0.15115779\n",
      "Iteration 78, loss = 0.14231778\n",
      "Iteration 96, loss = 0.15032561\n",
      "Iteration 79, loss = 0.13935419\n",
      "Iteration 97, loss = 0.14743404\n",
      "Iteration 80, loss = 0.13767244\n",
      "Iteration 98, loss = 0.14589709\n",
      "Iteration 81, loss = 0.13657319\n",
      "Iteration 99, loss = 0.14486633\n",
      "Iteration 82, loss = 0.13472206\n",
      "Iteration 100, loss = 0.14340726\n",
      "Iteration 83, loss = 0.13280491\n",
      "Iteration 101, loss = 0.14222398\n",
      "Iteration 84, loss = 0.13110017\n",
      "Iteration 102, loss = 0.14225847\n",
      "Iteration 85, loss = 0.12973224\n",
      "Iteration 103, loss = 0.14083677\n",
      "Iteration 86, loss = 0.12755108\n",
      "Iteration 104, loss = 0.13867251\n",
      "Iteration 87, loss = 0.12588673\n",
      "Iteration 105, loss = 0.13749237\n",
      "Iteration 88, loss = 0.12449555\n",
      "Iteration 106, loss = 0.13790058\n",
      "Iteration 89, loss = 0.12285887\n",
      "Iteration 107, loss = 0.13814254\n",
      "Iteration 90, loss = 0.12213097\n",
      "Iteration 108, loss = 0.13524358\n",
      "Iteration 91, loss = 0.12009824\n",
      "Iteration 109, loss = 0.13287383\n",
      "Iteration 92, loss = 0.11879514\n",
      "Iteration 110, loss = 0.13251128\n",
      "Iteration 93, loss = 0.11686454\n",
      "Iteration 111, loss = 0.13035215\n",
      "Iteration 94, loss = 0.11555155\n",
      "Iteration 112, loss = 0.13034813\n",
      "Iteration 95, loss = 0.11579882\n",
      "Iteration 113, loss = 0.12782107\n",
      "Iteration 96, loss = 0.11405073\n",
      "Iteration 114, loss = 0.12702726\n",
      "Iteration 97, loss = 0.11243521\n",
      "Iteration 115, loss = 0.12501647\n",
      "Iteration 98, loss = 0.11015315\n",
      "Iteration 116, loss = 0.12432320\n",
      "Iteration 99, loss = 0.11029376\n",
      "Iteration 117, loss = 0.12494763\n",
      "Iteration 100, loss = 0.10816970\n",
      "Iteration 118, loss = 0.12313590\n",
      "Iteration 101, loss = 0.10685982\n",
      "Iteration 119, loss = 0.12254367\n",
      "Iteration 102, loss = 0.10562130\n",
      "Iteration 120, loss = 0.12259539\n",
      "Iteration 103, loss = 0.10422088\n",
      "Iteration 121, loss = 0.11986348\n",
      "Iteration 104, loss = 0.10358691\n",
      "Iteration 122, loss = 0.11824356\n",
      "Iteration 105, loss = 0.10169854\n",
      "Iteration 123, loss = 0.11687057\n",
      "Iteration 106, loss = 0.10399112\n",
      "Iteration 124, loss = 0.11625964\n",
      "Iteration 107, loss = 0.10073649\n",
      "Iteration 125, loss = 0.11462455\n",
      "Iteration 108, loss = 0.09926540\n",
      "Iteration 126, loss = 0.11332290\n",
      "Iteration 109, loss = 0.09806661\n",
      "Iteration 127, loss = 0.11253780\n",
      "Iteration 110, loss = 0.09683396\n",
      "Iteration 128, loss = 0.11212567\n",
      "Iteration 111, loss = 0.09517183\n",
      "Iteration 129, loss = 0.11064635\n",
      "Iteration 112, loss = 0.09440404\n",
      "Iteration 130, loss = 0.11057201\n",
      "Iteration 113, loss = 0.09514365\n",
      "Iteration 131, loss = 0.10994094\n",
      "Iteration 114, loss = 0.09289399\n",
      "Iteration 132, loss = 0.10801766\n",
      "Iteration 115, loss = 0.09207864\n",
      "Iteration 133, loss = 0.10754352\n",
      "Iteration 116, loss = 0.09039175\n",
      "Iteration 134, loss = 0.10725820\n",
      "Iteration 117, loss = 0.08958491\n",
      "Iteration 135, loss = 0.10718371\n",
      "Iteration 118, loss = 0.08935649\n",
      "Iteration 136, loss = 0.10539685\n",
      "Iteration 119, loss = 0.08807769\n",
      "Iteration 137, loss = 0.10469021\n",
      "Iteration 120, loss = 0.08784662\n",
      "Iteration 138, loss = 0.10218530\n",
      "Iteration 121, loss = 0.08646035\n",
      "Iteration 139, loss = 0.10175866\n",
      "Iteration 122, loss = 0.08765352\n",
      "Iteration 140, loss = 0.10093806\n",
      "Iteration 123, loss = 0.08400332\n",
      "Iteration 141, loss = 0.09981344\n",
      "Iteration 124, loss = 0.08370625\n",
      "Iteration 142, loss = 0.09895792\n",
      "Iteration 125, loss = 0.08348977\n",
      "Iteration 143, loss = 0.09803310\n",
      "Iteration 126, loss = 0.08137921\n",
      "Iteration 144, loss = 0.10018190\n",
      "Iteration 127, loss = 0.08142564\n",
      "Iteration 145, loss = 0.09916440\n",
      "Iteration 128, loss = 0.08015965\n",
      "Iteration 146, loss = 0.09694415\n",
      "Iteration 129, loss = 0.07976862\n",
      "Iteration 147, loss = 0.09735693\n",
      "Iteration 130, loss = 0.07926950\n",
      "Iteration 148, loss = 0.09404646\n",
      "Iteration 131, loss = 0.07835495\n",
      "Iteration 149, loss = 0.09356170\n",
      "Iteration 132, loss = 0.07860552\n",
      "Iteration 150, loss = 0.09339083\n",
      "Iteration 133, loss = 0.07679514\n",
      "Iteration 151, loss = 0.09376201\n",
      "Iteration 134, loss = 0.07606210\n",
      "Iteration 152, loss = 0.09204219\n",
      "Iteration 135, loss = 0.07568917\n",
      "Iteration 153, loss = 0.09241693\n",
      "Iteration 136, loss = 0.07457769\n",
      "Iteration 154, loss = 0.08924590\n",
      "Iteration 137, loss = 0.07334959\n",
      "Iteration 155, loss = 0.09086242\n",
      "Iteration 138, loss = 0.07358375\n",
      "Iteration 156, loss = 0.08798919\n",
      "Iteration 139, loss = 0.07248817\n",
      "Iteration 157, loss = 0.08832715\n",
      "Iteration 140, loss = 0.07221816\n",
      "Iteration 158, loss = 0.08754448\n",
      "Iteration 141, loss = 0.07119122\n",
      "Iteration 159, loss = 0.08575513\n",
      "Iteration 142, loss = 0.07055733\n",
      "Iteration 160, loss = 0.08615430\n",
      "Iteration 143, loss = 0.06988647\n",
      "Iteration 161, loss = 0.08541213\n",
      "Iteration 144, loss = 0.06908098\n",
      "Iteration 162, loss = 0.08425634\n",
      "Iteration 145, loss = 0.06848030\n",
      "Iteration 163, loss = 0.08294789\n",
      "Iteration 146, loss = 0.06792464\n",
      "Iteration 164, loss = 0.08352938\n",
      "Iteration 147, loss = 0.06732878\n",
      "Iteration 165, loss = 0.08394261\n",
      "Iteration 148, loss = 0.06831633\n",
      "Iteration 166, loss = 0.08018710\n",
      "Iteration 149, loss = 0.06613664\n",
      "Iteration 167, loss = 0.08140342\n",
      "Iteration 150, loss = 0.06603084\n",
      "Iteration 168, loss = 0.08080640\n",
      "Iteration 151, loss = 0.06743249\n",
      "Iteration 169, loss = 0.08037212\n",
      "Iteration 152, loss = 0.06495325\n",
      "Iteration 170, loss = 0.07874890\n",
      "Iteration 153, loss = 0.06449588\n",
      "Iteration 154, loss = 0.06441420\n",
      "Iteration 171, loss = 0.07771357\n",
      "Iteration 155, loss = 0.06304652\n",
      "Iteration 172, loss = 0.07825012\n",
      "Iteration 156, loss = 0.06241258\n",
      "Iteration 173, loss = 0.07752757\n",
      "Iteration 157, loss = 0.06189154\n",
      "Iteration 174, loss = 0.07549779\n",
      "Iteration 158, loss = 0.06132787\n",
      "Iteration 175, loss = 0.07662904\n",
      "Iteration 159, loss = 0.06190208\n",
      "Iteration 176, loss = 0.07818741\n",
      "Iteration 160, loss = 0.06115801\n",
      "Iteration 177, loss = 0.07530267\n",
      "Iteration 161, loss = 0.06081413\n",
      "Iteration 178, loss = 0.07622111\n",
      "Iteration 162, loss = 0.05926826\n",
      "Iteration 179, loss = 0.07719802\n",
      "Iteration 163, loss = 0.05933666\n",
      "Iteration 180, loss = 0.07088075\n",
      "Iteration 164, loss = 0.05798656\n",
      "Iteration 181, loss = 0.07636418\n",
      "Iteration 165, loss = 0.05843392\n",
      "Iteration 182, loss = 0.07623657\n",
      "Iteration 166, loss = 0.05720248\n",
      "Iteration 183, loss = 0.07772345\n",
      "Iteration 167, loss = 0.05940181\n",
      "Iteration 184, loss = 0.07187564\n",
      "Iteration 168, loss = 0.05708710\n",
      "Iteration 185, loss = 0.07139161\n",
      "Iteration 169, loss = 0.05691550\n",
      "Iteration 186, loss = 0.07035981\n",
      "Iteration 170, loss = 0.05674563\n",
      "Iteration 187, loss = 0.06880557\n",
      "Iteration 171, loss = 0.05547668\n",
      "Iteration 188, loss = 0.07001719\n",
      "Iteration 172, loss = 0.05617264\n",
      "Iteration 189, loss = 0.06829690\n",
      "Iteration 173, loss = 0.05358391\n",
      "Iteration 190, loss = 0.06806559\n",
      "Iteration 174, loss = 0.05429539\n",
      "Iteration 191, loss = 0.06800655\n",
      "Iteration 175, loss = 0.05471561\n",
      "Iteration 192, loss = 0.06884591\n",
      "Iteration 176, loss = 0.05344146\n",
      "Iteration 193, loss = 0.06754274\n",
      "Iteration 177, loss = 0.05297372\n",
      "Iteration 194, loss = 0.06493909\n",
      "Iteration 178, loss = 0.05207362\n",
      "Iteration 195, loss = 0.06522314\n",
      "Iteration 179, loss = 0.05251328\n",
      "Iteration 196, loss = 0.06494652\n",
      "Iteration 180, loss = 0.05149826\n",
      "Iteration 197, loss = 0.06499636\n",
      "Iteration 181, loss = 0.05106230\n",
      "Iteration 198, loss = 0.06264316\n",
      "Iteration 182, loss = 0.05148880\n",
      "Iteration 199, loss = 0.06370817\n",
      "Iteration 183, loss = 0.05134414\n",
      "Iteration 200, loss = 0.06364633\n",
      "Iteration 184, loss = 0.05000216\n",
      "Iteration 201, loss = 0.06388582\n",
      "Iteration 185, loss = 0.04948183\n",
      "Iteration 202, loss = 0.06212906\n",
      "Iteration 186, loss = 0.04886556\n",
      "Iteration 203, loss = 0.06259747\n",
      "Iteration 187, loss = 0.04864521\n",
      "Iteration 204, loss = 0.06055215\n",
      "Iteration 188, loss = 0.04830576\n",
      "Iteration 205, loss = 0.06101147\n",
      "Iteration 189, loss = 0.04752704\n",
      "Iteration 206, loss = 0.06164656\n",
      "Iteration 190, loss = 0.04725061\n",
      "Iteration 207, loss = 0.06129363\n",
      "Iteration 191, loss = 0.04681317\n",
      "Iteration 208, loss = 0.06765340\n",
      "Iteration 192, loss = 0.04672542\n",
      "Iteration 209, loss = 0.06340888\n",
      "Iteration 193, loss = 0.04614331\n",
      "Iteration 210, loss = 0.05907334\n",
      "Iteration 194, loss = 0.04682570\n",
      "Iteration 211, loss = 0.06077243\n",
      "Iteration 195, loss = 0.04591679\n",
      "Iteration 212, loss = 0.05924895\n",
      "Iteration 196, loss = 0.04529465\n",
      "Iteration 213, loss = 0.05830099\n",
      "Iteration 197, loss = 0.04485224\n",
      "Iteration 214, loss = 0.05809087\n",
      "Iteration 198, loss = 0.04433679\n",
      "Iteration 215, loss = 0.05657141\n",
      "Iteration 199, loss = 0.04417663\n",
      "Iteration 216, loss = 0.05587275\n",
      "Iteration 200, loss = 0.04446652\n",
      "Iteration 217, loss = 0.05554809\n",
      "Iteration 201, loss = 0.04351108\n",
      "Iteration 218, loss = 0.05588365\n",
      "Iteration 202, loss = 0.04344055\n",
      "Iteration 219, loss = 0.05440906\n",
      "Iteration 203, loss = 0.04291924\n",
      "Iteration 220, loss = 0.05472480\n",
      "Iteration 204, loss = 0.04358781\n",
      "Iteration 221, loss = 0.05474966\n",
      "Iteration 205, loss = 0.04227123\n",
      "Iteration 222, loss = 0.05443803\n",
      "Iteration 206, loss = 0.04348366\n",
      "Iteration 223, loss = 0.05375980\n",
      "Iteration 207, loss = 0.04192228\n",
      "Iteration 224, loss = 0.05380003\n",
      "Iteration 208, loss = 0.04173282\n",
      "Iteration 225, loss = 0.05303001\n",
      "Iteration 209, loss = 0.04339418\n",
      "Iteration 226, loss = 0.05244271\n",
      "Iteration 210, loss = 0.04205042\n",
      "Iteration 227, loss = 0.05258318\n",
      "Iteration 211, loss = 0.04116585\n",
      "Iteration 228, loss = 0.05243729\n",
      "Iteration 212, loss = 0.04199962\n",
      "Iteration 229, loss = 0.05265522\n",
      "Iteration 213, loss = 0.04038794\n",
      "Iteration 230, loss = 0.05207178\n",
      "Iteration 214, loss = 0.04017202\n",
      "Iteration 231, loss = 0.05041395\n",
      "Iteration 215, loss = 0.03964727\n",
      "Iteration 232, loss = 0.05062545\n",
      "Iteration 216, loss = 0.03929972\n",
      "Iteration 233, loss = 0.04977783\n",
      "Iteration 217, loss = 0.03993871\n",
      "Iteration 234, loss = 0.05023853\n",
      "Iteration 218, loss = 0.03924279\n",
      "Iteration 235, loss = 0.05015767\n",
      "Iteration 219, loss = 0.04039621\n",
      "Iteration 236, loss = 0.05072121\n",
      "Iteration 220, loss = 0.03845395\n",
      "Iteration 237, loss = 0.04947026\n",
      "Iteration 221, loss = 0.03743119\n",
      "Iteration 238, loss = 0.05038303\n",
      "Iteration 222, loss = 0.03774502\n",
      "Iteration 239, loss = 0.04754759\n",
      "Iteration 223, loss = 0.03704078\n",
      "Iteration 240, loss = 0.04899677\n",
      "Iteration 224, loss = 0.03706768\n",
      "Iteration 241, loss = 0.04773690\n",
      "Iteration 225, loss = 0.03755745\n",
      "Iteration 242, loss = 0.04745295\n",
      "Iteration 226, loss = 0.03671267\n",
      "Iteration 243, loss = 0.04966305\n",
      "Iteration 227, loss = 0.03614569\n",
      "Iteration 244, loss = 0.04839649\n",
      "Iteration 228, loss = 0.03691116\n",
      "Iteration 245, loss = 0.05210874\n",
      "Iteration 229, loss = 0.03779117\n",
      "Iteration 246, loss = 0.05254359\n",
      "Iteration 230, loss = 0.03612604\n",
      "Iteration 247, loss = 0.04639183\n",
      "Iteration 231, loss = 0.03713986\n",
      "Iteration 248, loss = 0.04738483\n",
      "Iteration 232, loss = 0.03593944\n",
      "Iteration 249, loss = 0.04580312\n",
      "Iteration 233, loss = 0.03465928\n",
      "Iteration 250, loss = 0.04613988\n",
      "Iteration 234, loss = 0.03609033\n",
      "Iteration 251, loss = 0.04431849\n",
      "Iteration 235, loss = 0.03507108\n",
      "Iteration 252, loss = 0.04454537\n",
      "Iteration 236, loss = 0.03484839\n",
      "Iteration 253, loss = 0.04468297\n",
      "Iteration 237, loss = 0.03511507\n",
      "Iteration 254, loss = 0.04449835\n",
      "Iteration 238, loss = 0.03395197\n",
      "Iteration 255, loss = 0.04406189\n",
      "Iteration 239, loss = 0.03545038\n",
      "Iteration 256, loss = 0.04988546\n",
      "Iteration 240, loss = 0.03386865\n",
      "Iteration 257, loss = 0.04682165\n",
      "Iteration 241, loss = 0.03300895\n",
      "Iteration 258, loss = 0.04450300\n",
      "Iteration 242, loss = 0.03339444\n",
      "Iteration 259, loss = 0.04593816\n",
      "Iteration 243, loss = 0.03230754\n",
      "Iteration 260, loss = 0.04586069\n",
      "Iteration 244, loss = 0.03402385\n",
      "Iteration 261, loss = 0.04441318\n",
      "Iteration 245, loss = 0.03162747\n",
      "Iteration 262, loss = 0.04508868\n",
      "Iteration 246, loss = 0.03253007\n",
      "Iteration 263, loss = 0.04307099\n",
      "Iteration 247, loss = 0.03245161\n",
      "Iteration 248, loss = 0.03167044\n",
      "Iteration 264, loss = 0.04340645\n",
      "Iteration 249, loss = 0.03149127\n",
      "Iteration 265, loss = 0.04307300\n",
      "Iteration 250, loss = 0.03134088\n",
      "Iteration 266, loss = 0.04152548\n",
      "Iteration 251, loss = 0.03171096\n",
      "Iteration 267, loss = 0.04087055\n",
      "Iteration 252, loss = 0.03295134\n",
      "Iteration 268, loss = 0.03967166\n",
      "Iteration 253, loss = 0.03113417\n",
      "Iteration 269, loss = 0.04152237\n",
      "Iteration 254, loss = 0.03053402\n",
      "Iteration 270, loss = 0.03991070\n",
      "Iteration 255, loss = 0.03326025\n",
      "Iteration 271, loss = 0.03906479\n",
      "Iteration 256, loss = 0.03220714\n",
      "Iteration 272, loss = 0.03923899\n",
      "Iteration 257, loss = 0.03002552\n",
      "Iteration 273, loss = 0.04129639\n",
      "Iteration 258, loss = 0.03052400\n",
      "Iteration 274, loss = 0.03982100\n",
      "Iteration 259, loss = 0.03107782\n",
      "Iteration 275, loss = 0.03916186\n",
      "Iteration 260, loss = 0.02942827\n",
      "Iteration 276, loss = 0.03814838\n",
      "Iteration 261, loss = 0.03039850\n",
      "Iteration 277, loss = 0.03990270\n",
      "Iteration 262, loss = 0.02918524\n",
      "Iteration 278, loss = 0.04136544\n",
      "Iteration 263, loss = 0.02858652\n",
      "Iteration 279, loss = 0.03879491\n",
      "Iteration 264, loss = 0.02875371\n",
      "Iteration 280, loss = 0.03920454\n",
      "Iteration 265, loss = 0.02831372\n",
      "Iteration 281, loss = 0.03915925\n",
      "Iteration 266, loss = 0.02828060\n",
      "Iteration 282, loss = 0.03758710\n",
      "Iteration 267, loss = 0.02772419\n",
      "Iteration 283, loss = 0.03666566\n",
      "Iteration 268, loss = 0.02971340\n",
      "Iteration 284, loss = 0.03903510\n",
      "Iteration 269, loss = 0.03012742\n",
      "Iteration 285, loss = 0.03744278\n",
      "Iteration 270, loss = 0.03105011\n",
      "Iteration 286, loss = 0.03776492\n",
      "Iteration 287, loss = 0.03736726Iteration 271, loss = 0.02811148\n",
      "\n",
      "Iteration 288, loss = 0.03587827\n",
      "Iteration 272, loss = 0.02850853\n",
      "Iteration 289, loss = 0.03720481\n",
      "Iteration 273, loss = 0.02856682\n",
      "Iteration 290, loss = 0.03597595\n",
      "Iteration 274, loss = 0.02722019\n",
      "Iteration 291, loss = 0.03692835\n",
      "Iteration 275, loss = 0.02894779\n",
      "Iteration 292, loss = 0.03641957\n",
      "Iteration 276, loss = 0.03063528\n",
      "Iteration 293, loss = 0.03783079\n",
      "Iteration 277, loss = 0.03042422\n",
      "Iteration 294, loss = 0.03416058\n",
      "Iteration 278, loss = 0.02778250\n",
      "Iteration 295, loss = 0.03640609\n",
      "Iteration 279, loss = 0.02753372\n",
      "Iteration 296, loss = 0.03505153\n",
      "Iteration 280, loss = 0.03054736\n",
      "Iteration 297, loss = 0.03434642\n",
      "Iteration 281, loss = 0.02592923\n",
      "Iteration 298, loss = 0.03361280\n",
      "Iteration 282, loss = 0.02761705\n",
      "Iteration 299, loss = 0.03399176\n",
      "Iteration 283, loss = 0.02590660\n",
      "Iteration 300, loss = 0.03326743\n",
      "Iteration 284, loss = 0.02507883\n",
      "Iteration 301, loss = 0.03302712\n",
      "Iteration 285, loss = 0.02628952\n",
      "Iteration 302, loss = 0.03349455\n",
      "Iteration 286, loss = 0.02802029\n",
      "Iteration 303, loss = 0.03296794\n",
      "Iteration 287, loss = 0.02580354\n",
      "Iteration 304, loss = 0.03257391\n",
      "Iteration 288, loss = 0.02850876\n",
      "Iteration 305, loss = 0.03299131\n",
      "Iteration 289, loss = 0.02496029\n",
      "Iteration 306, loss = 0.03245482\n",
      "Iteration 290, loss = 0.02666153\n",
      "Iteration 307, loss = 0.03340999\n",
      "Iteration 291, loss = 0.02895588\n",
      "Iteration 308, loss = 0.03526868\n",
      "Iteration 292, loss = 0.02660372\n",
      "Iteration 309, loss = 0.03360040\n",
      "Iteration 293, loss = 0.02788257\n",
      "Iteration 310, loss = 0.03348885\n",
      "Iteration 294, loss = 0.02659897\n",
      "Iteration 311, loss = 0.03190824\n",
      "Iteration 295, loss = 0.02411273\n",
      "Iteration 312, loss = 0.03172740\n",
      "Iteration 296, loss = 0.02569834\n",
      "Iteration 313, loss = 0.03205313\n",
      "Iteration 297, loss = 0.02503701\n",
      "Iteration 314, loss = 0.03278674\n",
      "Iteration 298, loss = 0.02510615\n",
      "Iteration 315, loss = 0.03037721\n",
      "Iteration 299, loss = 0.02502007\n",
      "Iteration 316, loss = 0.03131769\n",
      "Iteration 300, loss = 0.02322635\n",
      "Iteration 317, loss = 0.03083870\n",
      "Iteration 301, loss = 0.02450829\n",
      "Iteration 318, loss = 0.03040528\n",
      "Iteration 302, loss = 0.02453393\n",
      "Iteration 319, loss = 0.03085443\n",
      "Iteration 303, loss = 0.02407951\n",
      "Iteration 320, loss = 0.03075059\n",
      "Iteration 304, loss = 0.02374704\n",
      "Iteration 321, loss = 0.02968749\n",
      "Iteration 305, loss = 0.02348202\n",
      "Iteration 322, loss = 0.03058280\n",
      "Iteration 306, loss = 0.02256589\n",
      "Iteration 323, loss = 0.03094718\n",
      "Iteration 307, loss = 0.02219437\n",
      "Iteration 324, loss = 0.03114563\n",
      "Iteration 308, loss = 0.02283509\n",
      "Iteration 325, loss = 0.02954632\n",
      "Iteration 309, loss = 0.02311497\n",
      "Iteration 326, loss = 0.03085045\n",
      "Iteration 310, loss = 0.02432811\n",
      "Iteration 327, loss = 0.03074799\n",
      "Iteration 311, loss = 0.02440244\n",
      "Iteration 328, loss = 0.02928244\n",
      "Iteration 312, loss = 0.02152748\n",
      "Iteration 329, loss = 0.02877899\n",
      "Iteration 313, loss = 0.02352806\n",
      "Iteration 330, loss = 0.03118198\n",
      "Iteration 314, loss = 0.02238135\n",
      "Iteration 331, loss = 0.03013378\n",
      "Iteration 315, loss = 0.02120709\n",
      "Iteration 332, loss = 0.02854158\n",
      "Iteration 316, loss = 0.02144618\n",
      "Iteration 333, loss = 0.02915709\n",
      "Iteration 317, loss = 0.02122903\n",
      "Iteration 334, loss = 0.02944663\n",
      "Iteration 318, loss = 0.02100946\n",
      "Iteration 335, loss = 0.02740359\n",
      "Iteration 319, loss = 0.02099018\n",
      "Iteration 336, loss = 0.02889614\n",
      "Iteration 320, loss = 0.02301527\n",
      "Iteration 337, loss = 0.02922567\n",
      "Iteration 321, loss = 0.02156076\n",
      "Iteration 338, loss = 0.02790154\n",
      "Iteration 322, loss = 0.02107959\n",
      "Iteration 339, loss = 0.02781961\n",
      "Iteration 323, loss = 0.02246353\n",
      "Iteration 340, loss = 0.02885310\n",
      "Iteration 324, loss = 0.02040638\n",
      "Iteration 341, loss = 0.02805828\n",
      "Iteration 325, loss = 0.02103725\n",
      "Iteration 342, loss = 0.02637742\n",
      "Iteration 326, loss = 0.02027249\n",
      "Iteration 343, loss = 0.02825020\n",
      "Iteration 327, loss = 0.01989039\n",
      "Iteration 344, loss = 0.02965502\n",
      "Iteration 328, loss = 0.02040915\n",
      "Iteration 345, loss = 0.02838377\n",
      "Iteration 329, loss = 0.01960579\n",
      "Iteration 346, loss = 0.02569096\n",
      "Iteration 330, loss = 0.02015430\n",
      "Iteration 347, loss = 0.02909119\n",
      "Iteration 331, loss = 0.01956924\n",
      "Iteration 348, loss = 0.02731833\n",
      "Iteration 332, loss = 0.02038600\n",
      "Iteration 349, loss = 0.02658049\n",
      "Iteration 333, loss = 0.01922018\n",
      "Iteration 350, loss = 0.02596538\n",
      "Iteration 334, loss = 0.01963971\n",
      "Iteration 351, loss = 0.02567205\n",
      "Iteration 335, loss = 0.01968814\n",
      "Iteration 352, loss = 0.02583341\n",
      "Iteration 336, loss = 0.02047473\n",
      "Iteration 353, loss = 0.02602315\n",
      "Iteration 337, loss = 0.01977637\n",
      "Iteration 354, loss = 0.02509615\n",
      "Iteration 338, loss = 0.01910134\n",
      "Iteration 355, loss = 0.02536380\n",
      "Iteration 339, loss = 0.01905979\n",
      "Iteration 356, loss = 0.02503069\n",
      "Iteration 340, loss = 0.01860970\n",
      "Iteration 357, loss = 0.02520190\n",
      "Iteration 341, loss = 0.01865592\n",
      "Iteration 358, loss = 0.02485075\n",
      "Iteration 342, loss = 0.01887968\n",
      "Iteration 359, loss = 0.02481799\n",
      "Iteration 343, loss = 0.02099882\n",
      "Iteration 360, loss = 0.02471455\n",
      "Iteration 344, loss = 0.02085072\n",
      "Iteration 361, loss = 0.02460039\n",
      "Iteration 345, loss = 0.01796420\n",
      "Iteration 362, loss = 0.02437331\n",
      "Iteration 346, loss = 0.01981537\n",
      "Iteration 363, loss = 0.02437753\n",
      "Iteration 347, loss = 0.02012492\n",
      "Iteration 364, loss = 0.02445035\n",
      "Iteration 348, loss = 0.01780196\n",
      "Iteration 365, loss = 0.02564966\n",
      "Iteration 349, loss = 0.01896706\n",
      "Iteration 366, loss = 0.02471993\n",
      "Iteration 350, loss = 0.01983995\n",
      "Iteration 367, loss = 0.02372554\n",
      "Iteration 351, loss = 0.01791155\n",
      "Iteration 368, loss = 0.02690382\n",
      "Iteration 352, loss = 0.01802232\n",
      "Iteration 369, loss = 0.02709149\n",
      "Iteration 353, loss = 0.01970187\n",
      "Iteration 370, loss = 0.02536050\n",
      "Iteration 354, loss = 0.01833300\n",
      "Iteration 371, loss = 0.02499951\n",
      "Iteration 355, loss = 0.01826360\n",
      "Iteration 372, loss = 0.02415182\n",
      "Iteration 356, loss = 0.01837611\n",
      "Iteration 373, loss = 0.02379320\n",
      "Iteration 357, loss = 0.01686468\n",
      "Iteration 374, loss = 0.02424029\n",
      "Iteration 358, loss = 0.01763501\n",
      "Iteration 375, loss = 0.02388136\n",
      "Iteration 359, loss = 0.01721253\n",
      "Iteration 376, loss = 0.02436298\n",
      "Iteration 360, loss = 0.01660946\n",
      "Iteration 377, loss = 0.02299273\n",
      "Iteration 361, loss = 0.01712107\n",
      "Iteration 362, loss = 0.01677861\n",
      "Iteration 378, loss = 0.02235064\n",
      "Iteration 379, loss = 0.02389312\n",
      "Iteration 363, loss = 0.01655257\n",
      "Iteration 380, loss = 0.02376683\n",
      "Iteration 364, loss = 0.01715507\n",
      "Iteration 381, loss = 0.02314906\n",
      "Iteration 365, loss = 0.01615456\n",
      "Iteration 382, loss = 0.02246525\n",
      "Iteration 366, loss = 0.01769218\n",
      "Iteration 383, loss = 0.02294747\n",
      "Iteration 367, loss = 0.01672665\n",
      "Iteration 384, loss = 0.02479284\n",
      "Iteration 368, loss = 0.01609603\n",
      "Iteration 385, loss = 0.02186213\n",
      "Iteration 369, loss = 0.01814934\n",
      "Iteration 386, loss = 0.02205800\n",
      "Iteration 370, loss = 0.01558101\n",
      "Iteration 387, loss = 0.02281332\n",
      "Iteration 371, loss = 0.01615850\n",
      "Iteration 388, loss = 0.02307115\n",
      "Iteration 372, loss = 0.01598894\n",
      "Iteration 389, loss = 0.02170833\n",
      "Iteration 373, loss = 0.01651443\n",
      "Iteration 390, loss = 0.02291177\n",
      "Iteration 374, loss = 0.01667879\n",
      "Iteration 391, loss = 0.02178733\n",
      "Iteration 375, loss = 0.01600418\n",
      "Iteration 392, loss = 0.02184493\n",
      "Iteration 376, loss = 0.01570087\n",
      "Iteration 393, loss = 0.02194999\n",
      "Iteration 377, loss = 0.01542740\n",
      "Iteration 394, loss = 0.02321165\n",
      "Iteration 378, loss = 0.01521404\n",
      "Iteration 395, loss = 0.02096153\n",
      "Iteration 379, loss = 0.01554294\n",
      "Iteration 396, loss = 0.02054999\n",
      "Iteration 380, loss = 0.01524485\n",
      "Iteration 397, loss = 0.02207162\n",
      "Iteration 381, loss = 0.01524357\n",
      "Iteration 398, loss = 0.02141441\n",
      "Iteration 382, loss = 0.01509579\n",
      "Iteration 399, loss = 0.02117975\n",
      "Iteration 383, loss = 0.01507927\n",
      "Iteration 400, loss = 0.02068812\n",
      "Iteration 384, loss = 0.01498318\n",
      "Iteration 401, loss = 0.02280612\n",
      "Iteration 385, loss = 0.01475643\n",
      "Iteration 402, loss = 0.02088633\n",
      "Iteration 386, loss = 0.01453914\n",
      "Iteration 403, loss = 0.01926129\n",
      "Iteration 387, loss = 0.01438164\n",
      "Iteration 404, loss = 0.02012810\n",
      "Iteration 388, loss = 0.01465372\n",
      "Iteration 405, loss = 0.02094576\n",
      "Iteration 389, loss = 0.01464577\n",
      "Iteration 406, loss = 0.01971187\n",
      "Iteration 390, loss = 0.01579272\n",
      "Iteration 407, loss = 0.02047048\n",
      "Iteration 391, loss = 0.01452899\n",
      "Iteration 408, loss = 0.01973388\n",
      "Iteration 392, loss = 0.01534871\n",
      "Iteration 409, loss = 0.01905699\n",
      "Iteration 393, loss = 0.01561244\n",
      "Iteration 410, loss = 0.02007265\n",
      "Iteration 394, loss = 0.01516262\n",
      "Iteration 411, loss = 0.01911658\n",
      "Iteration 395, loss = 0.01428016\n",
      "Iteration 412, loss = 0.01871318\n",
      "Iteration 396, loss = 0.01444348\n",
      "Iteration 413, loss = 0.01907858\n",
      "Iteration 397, loss = 0.01366064\n",
      "Iteration 414, loss = 0.01947864\n",
      "Iteration 398, loss = 0.01387498\n",
      "Iteration 415, loss = 0.01892942\n",
      "Iteration 399, loss = 0.01401380\n",
      "Iteration 416, loss = 0.01897257\n",
      "Iteration 400, loss = 0.01361977\n",
      "Iteration 417, loss = 0.01825843\n",
      "Iteration 401, loss = 0.01363902\n",
      "Iteration 418, loss = 0.01863694\n",
      "Iteration 402, loss = 0.01317800\n",
      "Iteration 419, loss = 0.01840744\n",
      "Iteration 403, loss = 0.01377055\n",
      "Iteration 420, loss = 0.01800699\n",
      "Iteration 404, loss = 0.01433917\n",
      "Iteration 421, loss = 0.01913686\n",
      "Iteration 405, loss = 0.01492901\n",
      "Iteration 422, loss = 0.02196118\n",
      "Iteration 406, loss = 0.01276331\n",
      "Iteration 423, loss = 0.01759782\n",
      "Iteration 407, loss = 0.01363987\n",
      "Iteration 424, loss = 0.01911195\n",
      "Iteration 408, loss = 0.01270170\n",
      "Iteration 425, loss = 0.01905338\n",
      "Iteration 409, loss = 0.01298508\n",
      "Iteration 426, loss = 0.02139393\n",
      "Iteration 410, loss = 0.01308089\n",
      "Iteration 427, loss = 0.01727202\n",
      "Iteration 411, loss = 0.01334758\n",
      "Iteration 428, loss = 0.01804222\n",
      "Iteration 412, loss = 0.01548146\n",
      "Iteration 429, loss = 0.01694623\n",
      "Iteration 413, loss = 0.01327168\n",
      "Iteration 430, loss = 0.01755587\n",
      "Iteration 414, loss = 0.01364652\n",
      "Iteration 431, loss = 0.01772533\n",
      "Iteration 415, loss = 0.01329977\n",
      "Iteration 416, loss = 0.01317387\n",
      "Iteration 432, loss = 0.01698551\n",
      "Iteration 417, loss = 0.01231497\n",
      "Iteration 433, loss = 0.01947480\n",
      "Iteration 434, loss = 0.01669471\n",
      "Iteration 418, loss = 0.01239760\n",
      "Iteration 435, loss = 0.01811978\n",
      "Iteration 419, loss = 0.01274678\n",
      "Iteration 436, loss = 0.01598077\n",
      "Iteration 420, loss = 0.01291129\n",
      "Iteration 437, loss = 0.01695146\n",
      "Iteration 421, loss = 0.01191443\n",
      "Iteration 438, loss = 0.01691598\n",
      "Iteration 422, loss = 0.01224168\n",
      "Iteration 439, loss = 0.01739172\n",
      "Iteration 423, loss = 0.01240422\n",
      "Iteration 440, loss = 0.01623630\n",
      "Iteration 424, loss = 0.01169251\n",
      "Iteration 441, loss = 0.01686284\n",
      "Iteration 425, loss = 0.01219899\n",
      "Iteration 442, loss = 0.01593270\n",
      "Iteration 426, loss = 0.01519749\n",
      "Iteration 443, loss = 0.01707063\n",
      "Iteration 427, loss = 0.01428392\n",
      "Iteration 444, loss = 0.01567568\n",
      "Iteration 428, loss = 0.01340963\n",
      "Iteration 445, loss = 0.01638748\n",
      "Iteration 429, loss = 0.01138638\n",
      "Iteration 446, loss = 0.01789910\n",
      "Iteration 430, loss = 0.01540655\n",
      "Iteration 447, loss = 0.01593021\n",
      "Iteration 431, loss = 0.01215269\n",
      "Iteration 448, loss = 0.01726821\n",
      "Iteration 432, loss = 0.01271718\n",
      "Iteration 449, loss = 0.01608962\n",
      "Iteration 433, loss = 0.01126275\n",
      "Iteration 450, loss = 0.01591161\n",
      "Iteration 434, loss = 0.01148880\n",
      "Iteration 451, loss = 0.01573904\n",
      "Iteration 435, loss = 0.01164022\n",
      "Iteration 452, loss = 0.01538340\n",
      "Iteration 436, loss = 0.01125332\n",
      "Iteration 437, loss = 0.01109577\n",
      "Iteration 453, loss = 0.01495548\n",
      "Iteration 438, loss = 0.01271160\n",
      "Iteration 454, loss = 0.01541708\n",
      "Iteration 439, loss = 0.01365157\n",
      "Iteration 455, loss = 0.01484103\n",
      "Iteration 440, loss = 0.01180270\n",
      "Iteration 456, loss = 0.01513484\n",
      "Iteration 441, loss = 0.01228160\n",
      "Iteration 457, loss = 0.01557727\n",
      "Iteration 442, loss = 0.01210084\n",
      "Iteration 458, loss = 0.01437876\n",
      "Iteration 443, loss = 0.01260650\n",
      "Iteration 459, loss = 0.01498773\n",
      "Iteration 444, loss = 0.01267808\n",
      "Iteration 460, loss = 0.01415742\n",
      "Iteration 461, loss = 0.01471819\n",
      "Iteration 445, loss = 0.01065337\n",
      "Iteration 462, loss = 0.01482428\n",
      "Iteration 446, loss = 0.01052071\n",
      "Iteration 463, loss = 0.01422971\n",
      "Iteration 447, loss = 0.01070238\n",
      "Iteration 464, loss = 0.01403651\n",
      "Iteration 448, loss = 0.01122588\n",
      "Iteration 449, loss = 0.01048733\n",
      "Iteration 465, loss = 0.01408627\n",
      "Iteration 466, loss = 0.01386193Iteration 450, loss = 0.01156763\n",
      "\n",
      "Iteration 451, loss = 0.01155190\n",
      "Iteration 467, loss = 0.01422271\n",
      "Iteration 468, loss = 0.01457333\n",
      "Iteration 452, loss = 0.01059823\n",
      "Iteration 469, loss = 0.01567381\n",
      "Iteration 453, loss = 0.01038099\n",
      "Iteration 470, loss = 0.01478134\n",
      "Iteration 454, loss = 0.01016570\n",
      "Iteration 471, loss = 0.01401662\n",
      "Iteration 455, loss = 0.00986973\n",
      "Iteration 472, loss = 0.01368423\n",
      "Iteration 456, loss = 0.01020433\n",
      "Iteration 473, loss = 0.01419879\n",
      "Iteration 457, loss = 0.01083739\n",
      "Iteration 474, loss = 0.01350427\n",
      "Iteration 458, loss = 0.01199825\n",
      "Iteration 475, loss = 0.01368568\n",
      "Iteration 459, loss = 0.01043998\n",
      "Iteration 476, loss = 0.01429868\n",
      "Iteration 460, loss = 0.01117303\n",
      "Iteration 477, loss = 0.01332761\n",
      "Iteration 461, loss = 0.01026050\n",
      "Iteration 478, loss = 0.01322797\n",
      "Iteration 462, loss = 0.01095020\n",
      "Iteration 479, loss = 0.01348570\n",
      "Iteration 463, loss = 0.01010743\n",
      "Iteration 480, loss = 0.01326120\n",
      "Iteration 464, loss = 0.00972323\n",
      "Iteration 481, loss = 0.01311764\n",
      "Iteration 465, loss = 0.01012429\n",
      "Iteration 482, loss = 0.01286655\n",
      "Iteration 466, loss = 0.01080968\n",
      "Iteration 483, loss = 0.01298584\n",
      "Iteration 467, loss = 0.00912091\n",
      "Iteration 484, loss = 0.01280913\n",
      "Iteration 468, loss = 0.00959333\n",
      "Iteration 485, loss = 0.01267506\n",
      "Iteration 469, loss = 0.00930818\n",
      "Iteration 486, loss = 0.01400926\n",
      "Iteration 470, loss = 0.00922497\n",
      "Iteration 487, loss = 0.01338782\n",
      "Iteration 471, loss = 0.00914906\n",
      "Iteration 488, loss = 0.01388316\n",
      "Iteration 472, loss = 0.00924535\n",
      "Iteration 489, loss = 0.01420199\n",
      "Iteration 473, loss = 0.00933370\n",
      "Iteration 490, loss = 0.01257965\n",
      "Iteration 474, loss = 0.00915445\n",
      "Iteration 491, loss = 0.01254437\n",
      "Iteration 475, loss = 0.00934178\n",
      "Iteration 492, loss = 0.01309968\n",
      "Iteration 476, loss = 0.00905488\n",
      "Iteration 493, loss = 0.01213946\n",
      "Iteration 477, loss = 0.00939622\n",
      "Iteration 494, loss = 0.01323166\n",
      "Iteration 478, loss = 0.01222777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 495, loss = 0.01368527\n",
      "Iteration 496, loss = 0.01296478\n",
      "Iteration 497, loss = 0.01482129\n",
      "Iteration 498, loss = 0.02025739\n",
      "Iteration 499, loss = 0.02204887\n",
      "Iteration 500, loss = 0.01209681\n",
      "Iteration 501, loss = 0.01173500\n",
      "Iteration 502, loss = 0.01212636\n",
      "Iteration 503, loss = 0.01192412\n",
      "Iteration 504, loss = 0.01116280\n",
      "Iteration 505, loss = 0.01113307\n",
      "Iteration 506, loss = 0.01099881\n",
      "Iteration 507, loss = 0.01125789\n",
      "Iteration 508, loss = 0.01103488\n",
      "Iteration 509, loss = 0.01139758\n",
      "Iteration 510, loss = 0.01183620\n",
      "Iteration 511, loss = 0.01194175\n",
      "Iteration 512, loss = 0.01290448\n",
      "Iteration 1, loss = 1.35469307\n",
      "Iteration 513, loss = 0.01184130\n",
      "Iteration 2, loss = 1.08814006\n",
      "Iteration 514, loss = 0.01069026\n",
      "Iteration 3, loss = 0.91339923\n",
      "Iteration 515, loss = 0.01121664\n",
      "Iteration 4, loss = 0.79701245\n",
      "Iteration 516, loss = 0.01054475\n",
      "Iteration 5, loss = 0.71969709\n",
      "Iteration 517, loss = 0.01057247\n",
      "Iteration 6, loss = 0.65866529\n",
      "Iteration 518, loss = 0.01102921\n",
      "Iteration 7, loss = 0.61324295\n",
      "Iteration 519, loss = 0.01049199\n",
      "Iteration 8, loss = 0.57914813\n",
      "Iteration 520, loss = 0.01051297\n",
      "Iteration 9, loss = 0.54786965\n",
      "Iteration 521, loss = 0.01022495\n",
      "Iteration 10, loss = 0.52382099\n",
      "Iteration 522, loss = 0.01068471\n",
      "Iteration 11, loss = 0.50200036\n",
      "Iteration 523, loss = 0.01042097\n",
      "Iteration 12, loss = 0.48272725\n",
      "Iteration 524, loss = 0.01017936\n",
      "Iteration 13, loss = 0.46496966\n",
      "Iteration 525, loss = 0.01025294\n",
      "Iteration 14, loss = 0.44992522\n",
      "Iteration 526, loss = 0.01043529\n",
      "Iteration 15, loss = 0.43565722\n",
      "Iteration 527, loss = 0.01048612\n",
      "Iteration 16, loss = 0.42310220\n",
      "Iteration 528, loss = 0.01065425\n",
      "Iteration 17, loss = 0.41283105\n",
      "Iteration 529, loss = 0.01036616\n",
      "Iteration 18, loss = 0.40268231\n",
      "Iteration 530, loss = 0.01075168\n",
      "Iteration 19, loss = 0.39220501\n",
      "Iteration 531, loss = 0.01018712\n",
      "Iteration 20, loss = 0.38336734\n",
      "Iteration 532, loss = 0.00994260\n",
      "Iteration 21, loss = 0.37491091\n",
      "Iteration 533, loss = 0.01020284\n",
      "Iteration 22, loss = 0.36800207\n",
      "Iteration 534, loss = 0.00958394\n",
      "Iteration 23, loss = 0.36018966\n",
      "Iteration 535, loss = 0.00970718\n",
      "Iteration 24, loss = 0.35349421\n",
      "Iteration 536, loss = 0.01033196\n",
      "Iteration 25, loss = 0.34690920\n",
      "Iteration 537, loss = 0.00947690\n",
      "Iteration 26, loss = 0.34026631\n",
      "Iteration 538, loss = 0.00917718\n",
      "Iteration 27, loss = 0.33426460\n",
      "Iteration 539, loss = 0.00920284\n",
      "Iteration 28, loss = 0.32863722\n",
      "Iteration 540, loss = 0.00907077\n",
      "Iteration 29, loss = 0.32344148\n",
      "Iteration 541, loss = 0.00904061\n",
      "Iteration 30, loss = 0.31791470\n",
      "Iteration 542, loss = 0.00921937\n",
      "Iteration 31, loss = 0.31298960\n",
      "Iteration 543, loss = 0.00927117\n",
      "Iteration 32, loss = 0.30681836\n",
      "Iteration 544, loss = 0.00888322\n",
      "Iteration 33, loss = 0.30203908\n",
      "Iteration 545, loss = 0.00882716\n",
      "Iteration 34, loss = 0.29751070\n",
      "Iteration 546, loss = 0.00924620\n",
      "Iteration 35, loss = 0.29346674\n",
      "Iteration 547, loss = 0.00977708\n",
      "Iteration 36, loss = 0.28688491\n",
      "Iteration 548, loss = 0.00989779\n",
      "Iteration 37, loss = 0.28242314\n",
      "Iteration 38, loss = 0.27751445\n",
      "Iteration 549, loss = 0.00924786\n",
      "Iteration 39, loss = 0.27407175\n",
      "Iteration 550, loss = 0.00883698\n",
      "Iteration 40, loss = 0.27060090\n",
      "Iteration 551, loss = 0.00849982\n",
      "Iteration 41, loss = 0.26426596\n",
      "Iteration 552, loss = 0.00852675\n",
      "Iteration 42, loss = 0.25999524\n",
      "Iteration 553, loss = 0.00857880\n",
      "Iteration 43, loss = 0.25829609\n",
      "Iteration 554, loss = 0.00841503\n",
      "Iteration 44, loss = 0.25321061\n",
      "Iteration 555, loss = 0.00907260\n",
      "Iteration 45, loss = 0.24983348\n",
      "Iteration 556, loss = 0.00859427\n",
      "Iteration 46, loss = 0.24640525\n",
      "Iteration 557, loss = 0.00832372\n",
      "Iteration 47, loss = 0.24262445\n",
      "Iteration 558, loss = 0.00824026\n",
      "Iteration 48, loss = 0.23844690\n",
      "Iteration 559, loss = 0.00851788\n",
      "Iteration 49, loss = 0.23478698\n",
      "Iteration 560, loss = 0.00846314\n",
      "Iteration 50, loss = 0.23156361\n",
      "Iteration 561, loss = 0.00836672\n",
      "Iteration 51, loss = 0.22842698\n",
      "Iteration 562, loss = 0.00903647\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.22504676\n",
      "Iteration 53, loss = 0.22283589\n",
      "Iteration 54, loss = 0.21922800\n",
      "Iteration 55, loss = 0.21656210\n",
      "Iteration 56, loss = 0.21411496\n",
      "Iteration 57, loss = 0.21096219\n",
      "Iteration 58, loss = 0.20800675\n",
      "Iteration 59, loss = 0.20529085\n",
      "Iteration 60, loss = 0.20255757\n",
      "Iteration 61, loss = 0.19973440\n",
      "Iteration 62, loss = 0.19741969\n",
      "Iteration 63, loss = 0.19479316\n",
      "Iteration 64, loss = 0.19303888\n",
      "Iteration 65, loss = 0.18983327\n",
      "Iteration 66, loss = 0.18804387\n",
      "Iteration 67, loss = 0.18553096\n",
      "Iteration 68, loss = 0.18289612\n",
      "Iteration 69, loss = 0.18164730\n",
      "Iteration 70, loss = 0.17901423\n",
      "Iteration 1, loss = 1.22435149\n",
      "Iteration 71, loss = 0.17654335\n",
      "Iteration 2, loss = 0.91212990\n",
      "Iteration 72, loss = 0.17451900\n",
      "Iteration 3, loss = 0.75869844\n",
      "Iteration 4, loss = 0.68371791\n",
      "Iteration 73, loss = 0.17266742\n",
      "Iteration 5, loss = 0.63285736\n",
      "Iteration 74, loss = 0.17041190\n",
      "Iteration 6, loss = 0.58835167\n",
      "Iteration 75, loss = 0.16889850\n",
      "Iteration 7, loss = 0.55277563\n",
      "Iteration 76, loss = 0.16796088\n",
      "Iteration 8, loss = 0.52407654\n",
      "Iteration 77, loss = 0.16541700\n",
      "Iteration 9, loss = 0.50191844\n",
      "Iteration 78, loss = 0.16382072\n",
      "Iteration 10, loss = 0.48428695\n",
      "Iteration 79, loss = 0.16156631\n",
      "Iteration 11, loss = 0.46839413\n",
      "Iteration 80, loss = 0.16002318\n",
      "Iteration 12, loss = 0.45480366\n",
      "Iteration 81, loss = 0.15748486\n",
      "Iteration 13, loss = 0.44299763\n",
      "Iteration 82, loss = 0.15574129\n",
      "Iteration 83, loss = 0.15342499\n",
      "Iteration 14, loss = 0.43255688\n",
      "Iteration 84, loss = 0.15253475\n",
      "Iteration 15, loss = 0.42222277\n",
      "Iteration 85, loss = 0.15104677\n",
      "Iteration 16, loss = 0.41308112\n",
      "Iteration 86, loss = 0.14868256\n",
      "Iteration 17, loss = 0.40487810\n",
      "Iteration 87, loss = 0.14917085\n",
      "Iteration 18, loss = 0.39746982\n",
      "Iteration 19, loss = 0.38931547\n",
      "Iteration 88, loss = 0.14653089\n",
      "Iteration 20, loss = 0.38176116\n",
      "Iteration 89, loss = 0.14524449\n",
      "Iteration 90, loss = 0.14411158\n",
      "Iteration 21, loss = 0.37539553\n",
      "Iteration 91, loss = 0.14219119\n",
      "Iteration 22, loss = 0.36899070\n",
      "Iteration 92, loss = 0.14018846\n",
      "Iteration 23, loss = 0.36280379\n",
      "Iteration 93, loss = 0.13789863\n",
      "Iteration 24, loss = 0.35701165\n",
      "Iteration 94, loss = 0.13758573\n",
      "Iteration 25, loss = 0.35143611\n",
      "Iteration 95, loss = 0.13617342\n",
      "Iteration 26, loss = 0.34600270\n",
      "Iteration 96, loss = 0.13396273\n",
      "Iteration 27, loss = 0.34121221\n",
      "Iteration 28, loss = 0.33575778\n",
      "Iteration 97, loss = 0.13363405\n",
      "Iteration 29, loss = 0.33200371\n",
      "Iteration 98, loss = 0.13113408\n",
      "Iteration 99, loss = 0.12996896Iteration 30, loss = 0.32693971\n",
      "\n",
      "Iteration 100, loss = 0.12877673\n",
      "Iteration 31, loss = 0.32199181\n",
      "Iteration 32, loss = 0.31771418\n",
      "Iteration 101, loss = 0.12778637\n",
      "Iteration 33, loss = 0.31371355\n",
      "Iteration 102, loss = 0.12580372\n",
      "Iteration 34, loss = 0.30956686\n",
      "Iteration 103, loss = 0.12660945\n",
      "Iteration 35, loss = 0.30562417\n",
      "Iteration 104, loss = 0.12335479\n",
      "Iteration 36, loss = 0.30167610\n",
      "Iteration 105, loss = 0.12326244\n",
      "Iteration 37, loss = 0.29726063\n",
      "Iteration 106, loss = 0.11960565\n",
      "Iteration 38, loss = 0.29338144\n",
      "Iteration 107, loss = 0.12063092\n",
      "Iteration 39, loss = 0.28899319\n",
      "Iteration 108, loss = 0.11807989\n",
      "Iteration 40, loss = 0.28587588\n",
      "Iteration 109, loss = 0.11767605\n",
      "Iteration 41, loss = 0.28161150\n",
      "Iteration 110, loss = 0.11763846\n",
      "Iteration 42, loss = 0.27847621\n",
      "Iteration 111, loss = 0.11492692\n",
      "Iteration 43, loss = 0.27505633\n",
      "Iteration 112, loss = 0.11501069\n",
      "Iteration 44, loss = 0.27062701\n",
      "Iteration 113, loss = 0.11355383\n",
      "Iteration 45, loss = 0.26830240\n",
      "Iteration 114, loss = 0.11388649\n",
      "Iteration 46, loss = 0.26464022\n",
      "Iteration 115, loss = 0.11262900\n",
      "Iteration 47, loss = 0.26037875\n",
      "Iteration 116, loss = 0.10977491\n",
      "Iteration 48, loss = 0.25692413\n",
      "Iteration 117, loss = 0.10937216\n",
      "Iteration 49, loss = 0.25397597\n",
      "Iteration 118, loss = 0.10729300\n",
      "Iteration 50, loss = 0.25079352\n",
      "Iteration 119, loss = 0.10620772\n",
      "Iteration 51, loss = 0.24723191\n",
      "Iteration 120, loss = 0.10484724\n",
      "Iteration 52, loss = 0.24445245\n",
      "Iteration 121, loss = 0.10427367\n",
      "Iteration 53, loss = 0.24066030\n",
      "Iteration 122, loss = 0.10319880\n",
      "Iteration 54, loss = 0.23747115\n",
      "Iteration 123, loss = 0.10257234\n",
      "Iteration 55, loss = 0.23488256\n",
      "Iteration 124, loss = 0.10117756\n",
      "Iteration 56, loss = 0.23168501\n",
      "Iteration 125, loss = 0.10088959\n",
      "Iteration 57, loss = 0.22900566\n",
      "Iteration 126, loss = 0.09956418\n",
      "Iteration 58, loss = 0.22558697\n",
      "Iteration 127, loss = 0.09828224\n",
      "Iteration 59, loss = 0.22255046\n",
      "Iteration 128, loss = 0.09740600\n",
      "Iteration 60, loss = 0.22083341\n",
      "Iteration 129, loss = 0.09646408\n",
      "Iteration 61, loss = 0.21747798\n",
      "Iteration 130, loss = 0.09572090\n",
      "Iteration 62, loss = 0.21444779\n",
      "Iteration 131, loss = 0.09606718\n",
      "Iteration 63, loss = 0.21250051\n",
      "Iteration 132, loss = 0.09510801\n",
      "Iteration 64, loss = 0.20949603\n",
      "Iteration 133, loss = 0.09588777\n",
      "Iteration 65, loss = 0.20697219\n",
      "Iteration 134, loss = 0.09519052\n",
      "Iteration 66, loss = 0.20419855\n",
      "Iteration 135, loss = 0.09316604\n",
      "Iteration 67, loss = 0.20161763\n",
      "Iteration 136, loss = 0.09384977\n",
      "Iteration 68, loss = 0.19908126\n",
      "Iteration 137, loss = 0.09029696\n",
      "Iteration 69, loss = 0.19671865\n",
      "Iteration 138, loss = 0.09090099\n",
      "Iteration 70, loss = 0.19448752\n",
      "Iteration 139, loss = 0.08779808\n",
      "Iteration 71, loss = 0.19218592\n",
      "Iteration 140, loss = 0.08817125\n",
      "Iteration 72, loss = 0.18998979\n",
      "Iteration 141, loss = 0.08644517\n",
      "Iteration 73, loss = 0.18748857\n",
      "Iteration 142, loss = 0.08610243\n",
      "Iteration 74, loss = 0.18504997\n",
      "Iteration 143, loss = 0.08562333\n",
      "Iteration 75, loss = 0.18269089\n",
      "Iteration 144, loss = 0.08558166\n",
      "Iteration 76, loss = 0.18101717\n",
      "Iteration 145, loss = 0.08370549\n",
      "Iteration 77, loss = 0.17779084\n",
      "Iteration 146, loss = 0.08423884\n",
      "Iteration 78, loss = 0.17740144\n",
      "Iteration 147, loss = 0.08277492\n",
      "Iteration 79, loss = 0.17546757\n",
      "Iteration 148, loss = 0.08198126\n",
      "Iteration 80, loss = 0.17313061\n",
      "Iteration 149, loss = 0.08073677\n",
      "Iteration 81, loss = 0.17090481\n",
      "Iteration 150, loss = 0.08059579\n",
      "Iteration 82, loss = 0.16915889\n",
      "Iteration 151, loss = 0.08028075\n",
      "Iteration 83, loss = 0.16699757\n",
      "Iteration 152, loss = 0.07979038\n",
      "Iteration 84, loss = 0.16546955\n",
      "Iteration 153, loss = 0.08339428\n",
      "Iteration 85, loss = 0.16284170\n",
      "Iteration 154, loss = 0.07805288\n",
      "Iteration 86, loss = 0.16147232\n",
      "Iteration 155, loss = 0.07855342\n",
      "Iteration 87, loss = 0.15966934\n",
      "Iteration 156, loss = 0.08055199\n",
      "Iteration 88, loss = 0.15809855\n",
      "Iteration 157, loss = 0.07732251\n",
      "Iteration 89, loss = 0.15762702\n",
      "Iteration 158, loss = 0.07666572\n",
      "Iteration 90, loss = 0.15443352\n",
      "Iteration 159, loss = 0.07629316\n",
      "Iteration 91, loss = 0.15516242\n",
      "Iteration 160, loss = 0.07352098\n",
      "Iteration 92, loss = 0.15188269\n",
      "Iteration 161, loss = 0.07412560\n",
      "Iteration 93, loss = 0.15059684\n",
      "Iteration 162, loss = 0.07312065\n",
      "Iteration 94, loss = 0.14879264\n",
      "Iteration 163, loss = 0.07313094\n",
      "Iteration 95, loss = 0.14746254\n",
      "Iteration 164, loss = 0.07266779\n",
      "Iteration 96, loss = 0.14587311\n",
      "Iteration 165, loss = 0.07364554\n",
      "Iteration 97, loss = 0.14547953\n",
      "Iteration 166, loss = 0.07017633\n",
      "Iteration 98, loss = 0.14592291\n",
      "Iteration 167, loss = 0.07198913\n",
      "Iteration 99, loss = 0.14182210\n",
      "Iteration 168, loss = 0.06875591\n",
      "Iteration 100, loss = 0.14069513\n",
      "Iteration 169, loss = 0.06974763\n",
      "Iteration 101, loss = 0.14015733\n",
      "Iteration 170, loss = 0.06830685\n",
      "Iteration 102, loss = 0.13858170\n",
      "Iteration 171, loss = 0.06805868\n",
      "Iteration 103, loss = 0.13649635\n",
      "Iteration 172, loss = 0.06706848\n",
      "Iteration 104, loss = 0.13503880\n",
      "Iteration 173, loss = 0.06599411\n",
      "Iteration 105, loss = 0.13299541\n",
      "Iteration 174, loss = 0.06575745\n",
      "Iteration 106, loss = 0.13233115\n",
      "Iteration 175, loss = 0.06712038\n",
      "Iteration 107, loss = 0.13080934\n",
      "Iteration 176, loss = 0.06776018\n",
      "Iteration 108, loss = 0.13051789\n",
      "Iteration 177, loss = 0.06497641\n",
      "Iteration 109, loss = 0.12923984\n",
      "Iteration 178, loss = 0.06521131\n",
      "Iteration 110, loss = 0.12815085\n",
      "Iteration 179, loss = 0.06475303\n",
      "Iteration 111, loss = 0.12579559\n",
      "Iteration 180, loss = 0.06364498\n",
      "Iteration 112, loss = 0.12606869\n",
      "Iteration 181, loss = 0.06137167\n",
      "Iteration 113, loss = 0.12354682\n",
      "Iteration 182, loss = 0.06317074\n",
      "Iteration 114, loss = 0.12353501\n",
      "Iteration 183, loss = 0.06263406\n",
      "Iteration 115, loss = 0.12137809\n",
      "Iteration 184, loss = 0.06078522\n",
      "Iteration 116, loss = 0.12083359\n",
      "Iteration 185, loss = 0.06033200\n",
      "Iteration 117, loss = 0.11985037\n",
      "Iteration 186, loss = 0.06450738\n",
      "Iteration 118, loss = 0.11882284\n",
      "Iteration 187, loss = 0.05815850\n",
      "Iteration 119, loss = 0.11946915\n",
      "Iteration 188, loss = 0.06010760\n",
      "Iteration 120, loss = 0.11680563\n",
      "Iteration 189, loss = 0.05918250\n",
      "Iteration 121, loss = 0.11673736\n",
      "Iteration 190, loss = 0.05736616\n",
      "Iteration 122, loss = 0.11547397\n",
      "Iteration 191, loss = 0.05842146\n",
      "Iteration 123, loss = 0.11432624\n",
      "Iteration 192, loss = 0.05678890\n",
      "Iteration 124, loss = 0.11446662\n",
      "Iteration 193, loss = 0.05662318\n",
      "Iteration 125, loss = 0.11094729\n",
      "Iteration 194, loss = 0.05717531\n",
      "Iteration 126, loss = 0.11021447\n",
      "Iteration 195, loss = 0.05751344\n",
      "Iteration 127, loss = 0.10987146\n",
      "Iteration 196, loss = 0.05634189\n",
      "Iteration 128, loss = 0.11086157\n",
      "Iteration 197, loss = 0.05449957\n",
      "Iteration 129, loss = 0.10649510\n",
      "Iteration 198, loss = 0.05357728\n",
      "Iteration 130, loss = 0.10726486\n",
      "Iteration 199, loss = 0.05487890\n",
      "Iteration 131, loss = 0.10675035\n",
      "Iteration 200, loss = 0.05329777\n",
      "Iteration 132, loss = 0.10509232\n",
      "Iteration 201, loss = 0.05261165\n",
      "Iteration 133, loss = 0.10377646\n",
      "Iteration 202, loss = 0.05292876\n",
      "Iteration 134, loss = 0.10398389\n",
      "Iteration 203, loss = 0.05255710\n",
      "Iteration 135, loss = 0.10177284\n",
      "Iteration 204, loss = 0.05167485\n",
      "Iteration 136, loss = 0.10143840\n",
      "Iteration 205, loss = 0.05127790\n",
      "Iteration 137, loss = 0.09994016\n",
      "Iteration 206, loss = 0.05236827\n",
      "Iteration 138, loss = 0.10001264\n",
      "Iteration 207, loss = 0.05028647\n",
      "Iteration 139, loss = 0.09940321\n",
      "Iteration 208, loss = 0.05153243\n",
      "Iteration 140, loss = 0.09772988\n",
      "Iteration 209, loss = 0.04971330\n",
      "Iteration 141, loss = 0.09777005\n",
      "Iteration 210, loss = 0.04956741\n",
      "Iteration 142, loss = 0.09590983\n",
      "Iteration 211, loss = 0.04883342\n",
      "Iteration 143, loss = 0.09736687\n",
      "Iteration 212, loss = 0.04846803\n",
      "Iteration 144, loss = 0.09423258\n",
      "Iteration 213, loss = 0.04814993\n",
      "Iteration 145, loss = 0.09393639\n",
      "Iteration 214, loss = 0.04839050\n",
      "Iteration 146, loss = 0.09317732\n",
      "Iteration 215, loss = 0.04942382\n",
      "Iteration 147, loss = 0.09224642\n",
      "Iteration 216, loss = 0.04835015\n",
      "Iteration 148, loss = 0.09294354\n",
      "Iteration 217, loss = 0.04715233\n",
      "Iteration 149, loss = 0.09044843\n",
      "Iteration 218, loss = 0.04673588\n",
      "Iteration 150, loss = 0.08958655\n",
      "Iteration 219, loss = 0.04713060\n",
      "Iteration 151, loss = 0.08991341\n",
      "Iteration 220, loss = 0.04607498\n",
      "Iteration 152, loss = 0.09010070\n",
      "Iteration 221, loss = 0.04693807\n",
      "Iteration 153, loss = 0.08829481\n",
      "Iteration 222, loss = 0.04549227\n",
      "Iteration 154, loss = 0.08865221\n",
      "Iteration 223, loss = 0.04572579\n",
      "Iteration 155, loss = 0.08562957\n",
      "Iteration 224, loss = 0.04508732\n",
      "Iteration 156, loss = 0.08545514\n",
      "Iteration 225, loss = 0.04413558\n",
      "Iteration 157, loss = 0.08429399\n",
      "Iteration 226, loss = 0.04501284\n",
      "Iteration 158, loss = 0.08389851\n",
      "Iteration 227, loss = 0.04474541\n",
      "Iteration 159, loss = 0.08371121\n",
      "Iteration 228, loss = 0.04498658\n",
      "Iteration 160, loss = 0.08185935\n",
      "Iteration 229, loss = 0.04470879\n",
      "Iteration 161, loss = 0.08171434\n",
      "Iteration 230, loss = 0.04559634\n",
      "Iteration 162, loss = 0.08106732\n",
      "Iteration 231, loss = 0.04343287\n",
      "Iteration 163, loss = 0.07992279\n",
      "Iteration 232, loss = 0.04397726\n",
      "Iteration 164, loss = 0.07902055\n",
      "Iteration 233, loss = 0.04181064\n",
      "Iteration 165, loss = 0.07890887\n",
      "Iteration 234, loss = 0.04430235\n",
      "Iteration 166, loss = 0.07941965\n",
      "Iteration 235, loss = 0.04932691\n",
      "Iteration 167, loss = 0.07740790\n",
      "Iteration 236, loss = 0.04915953\n",
      "Iteration 168, loss = 0.07833906\n",
      "Iteration 237, loss = 0.04590674\n",
      "Iteration 169, loss = 0.07659682\n",
      "Iteration 238, loss = 0.04322683\n",
      "Iteration 170, loss = 0.07582579\n",
      "Iteration 239, loss = 0.04404858\n",
      "Iteration 171, loss = 0.07880282\n",
      "Iteration 240, loss = 0.04095502\n",
      "Iteration 172, loss = 0.07367774\n",
      "Iteration 241, loss = 0.04073328\n",
      "Iteration 173, loss = 0.07488329\n",
      "Iteration 242, loss = 0.04017559\n",
      "Iteration 174, loss = 0.07520372\n",
      "Iteration 243, loss = 0.04027579\n",
      "Iteration 175, loss = 0.07271276\n",
      "Iteration 244, loss = 0.04017991\n",
      "Iteration 176, loss = 0.07417701\n",
      "Iteration 245, loss = 0.03917767\n",
      "Iteration 177, loss = 0.07195892\n",
      "Iteration 246, loss = 0.03972291\n",
      "Iteration 178, loss = 0.07075492\n",
      "Iteration 247, loss = 0.04026837\n",
      "Iteration 179, loss = 0.07077503\n",
      "Iteration 248, loss = 0.03935024\n",
      "Iteration 180, loss = 0.06980577\n",
      "Iteration 249, loss = 0.03879040\n",
      "Iteration 181, loss = 0.06988001\n",
      "Iteration 250, loss = 0.03805710\n",
      "Iteration 182, loss = 0.06883920\n",
      "Iteration 251, loss = 0.03847244\n",
      "Iteration 183, loss = 0.06831172\n",
      "Iteration 252, loss = 0.03768242\n",
      "Iteration 184, loss = 0.06808347\n",
      "Iteration 253, loss = 0.03753764\n",
      "Iteration 185, loss = 0.06756965\n",
      "Iteration 254, loss = 0.03741983\n",
      "Iteration 186, loss = 0.06591705\n",
      "Iteration 255, loss = 0.03798493\n",
      "Iteration 187, loss = 0.06551383\n",
      "Iteration 256, loss = 0.03717693\n",
      "Iteration 188, loss = 0.06508289\n",
      "Iteration 257, loss = 0.03749101\n",
      "Iteration 189, loss = 0.06544544\n",
      "Iteration 258, loss = 0.03679030\n",
      "Iteration 190, loss = 0.06561034\n",
      "Iteration 259, loss = 0.03638844\n",
      "Iteration 191, loss = 0.06401135\n",
      "Iteration 260, loss = 0.03638886\n",
      "Iteration 192, loss = 0.06356689\n",
      "Iteration 261, loss = 0.03628077\n",
      "Iteration 193, loss = 0.06356965\n",
      "Iteration 262, loss = 0.03629499\n",
      "Iteration 194, loss = 0.06378040\n",
      "Iteration 263, loss = 0.03621417\n",
      "Iteration 195, loss = 0.06108077\n",
      "Iteration 264, loss = 0.03546579\n",
      "Iteration 196, loss = 0.06051135\n",
      "Iteration 265, loss = 0.03519197\n",
      "Iteration 197, loss = 0.06097228\n",
      "Iteration 266, loss = 0.03489524\n",
      "Iteration 198, loss = 0.06132961\n",
      "Iteration 267, loss = 0.03458469\n",
      "Iteration 199, loss = 0.05945795\n",
      "Iteration 268, loss = 0.03477628\n",
      "Iteration 200, loss = 0.05921367\n",
      "Iteration 269, loss = 0.03452724\n",
      "Iteration 201, loss = 0.06132847\n",
      "Iteration 270, loss = 0.03390496\n",
      "Iteration 202, loss = 0.05874537\n",
      "Iteration 271, loss = 0.03490477\n",
      "Iteration 203, loss = 0.05893103\n",
      "Iteration 272, loss = 0.03536074\n",
      "Iteration 204, loss = 0.05755672\n",
      "Iteration 273, loss = 0.03613373\n",
      "Iteration 205, loss = 0.05719384\n",
      "Iteration 274, loss = 0.03266460\n",
      "Iteration 206, loss = 0.05617137\n",
      "Iteration 275, loss = 0.03521954\n",
      "Iteration 207, loss = 0.05749348\n",
      "Iteration 276, loss = 0.03520247\n",
      "Iteration 208, loss = 0.05647618\n",
      "Iteration 277, loss = 0.03665872\n",
      "Iteration 209, loss = 0.05747708\n",
      "Iteration 278, loss = 0.03441555\n",
      "Iteration 210, loss = 0.05630541\n",
      "Iteration 279, loss = 0.03330400\n",
      "Iteration 211, loss = 0.05630316\n",
      "Iteration 280, loss = 0.03453931\n",
      "Iteration 212, loss = 0.05580172\n",
      "Iteration 281, loss = 0.03471649\n",
      "Iteration 213, loss = 0.05314374\n",
      "Iteration 282, loss = 0.03402202\n",
      "Iteration 214, loss = 0.05426873\n",
      "Iteration 283, loss = 0.03237477\n",
      "Iteration 215, loss = 0.05311450\n",
      "Iteration 284, loss = 0.03268233\n",
      "Iteration 216, loss = 0.05270650\n",
      "Iteration 285, loss = 0.03202370\n",
      "Iteration 217, loss = 0.05216460\n",
      "Iteration 286, loss = 0.03143660\n",
      "Iteration 218, loss = 0.05215974\n",
      "Iteration 287, loss = 0.03210337\n",
      "Iteration 219, loss = 0.05188194\n",
      "Iteration 288, loss = 0.03112142\n",
      "Iteration 220, loss = 0.05125092\n",
      "Iteration 289, loss = 0.03137397\n",
      "Iteration 221, loss = 0.05081561\n",
      "Iteration 290, loss = 0.03104864\n",
      "Iteration 222, loss = 0.05059323\n",
      "Iteration 223, loss = 0.05049682\n",
      "Iteration 291, loss = 0.03060978\n",
      "Iteration 224, loss = 0.04963178\n",
      "Iteration 292, loss = 0.03175120\n",
      "Iteration 293, loss = 0.03093511\n",
      "Iteration 225, loss = 0.05125288\n",
      "Iteration 294, loss = 0.03050323\n",
      "Iteration 226, loss = 0.04982114\n",
      "Iteration 295, loss = 0.03005405\n",
      "Iteration 227, loss = 0.04830465\n",
      "Iteration 296, loss = 0.02969548\n",
      "Iteration 228, loss = 0.05074618\n",
      "Iteration 297, loss = 0.02939275\n",
      "Iteration 229, loss = 0.04920091\n",
      "Iteration 298, loss = 0.02962944\n",
      "Iteration 230, loss = 0.04927797\n",
      "Iteration 299, loss = 0.02990433\n",
      "Iteration 231, loss = 0.04836182\n",
      "Iteration 300, loss = 0.03017222\n",
      "Iteration 232, loss = 0.04746457\n",
      "Iteration 233, loss = 0.04852380\n",
      "Iteration 301, loss = 0.02979640\n",
      "Iteration 234, loss = 0.04808514\n",
      "Iteration 302, loss = 0.02872969\n",
      "Iteration 235, loss = 0.04693073\n",
      "Iteration 303, loss = 0.02980343\n",
      "Iteration 236, loss = 0.04669324\n",
      "Iteration 304, loss = 0.02861885\n",
      "Iteration 237, loss = 0.04601057\n",
      "Iteration 305, loss = 0.02876802\n",
      "Iteration 238, loss = 0.04667542\n",
      "Iteration 306, loss = 0.02931017\n",
      "Iteration 239, loss = 0.04494553\n",
      "Iteration 307, loss = 0.02892027\n",
      "Iteration 240, loss = 0.04516659\n",
      "Iteration 308, loss = 0.02808110\n",
      "Iteration 241, loss = 0.04499622\n",
      "Iteration 309, loss = 0.02791719\n",
      "Iteration 242, loss = 0.04490303\n",
      "Iteration 310, loss = 0.02906323\n",
      "Iteration 243, loss = 0.04585766\n",
      "Iteration 311, loss = 0.02840177\n",
      "Iteration 244, loss = 0.04474962\n",
      "Iteration 312, loss = 0.02737386\n",
      "Iteration 245, loss = 0.04423126\n",
      "Iteration 313, loss = 0.02746941\n",
      "Iteration 246, loss = 0.04386206\n",
      "Iteration 314, loss = 0.02852590\n",
      "Iteration 247, loss = 0.04397818\n",
      "Iteration 315, loss = 0.02696096\n",
      "Iteration 248, loss = 0.04593313\n",
      "Iteration 316, loss = 0.02738280\n",
      "Iteration 249, loss = 0.04452231\n",
      "Iteration 317, loss = 0.02782831\n",
      "Iteration 318, loss = 0.02667158\n",
      "Iteration 250, loss = 0.04260405\n",
      "Iteration 251, loss = 0.04291773Iteration 319, loss = 0.02644188\n",
      "\n",
      "Iteration 252, loss = 0.04289701\n",
      "Iteration 320, loss = 0.02715084\n",
      "Iteration 321, loss = 0.02654044\n",
      "Iteration 253, loss = 0.04300603\n",
      "Iteration 322, loss = 0.02632958\n",
      "Iteration 254, loss = 0.04094702\n",
      "Iteration 323, loss = 0.02694683\n",
      "Iteration 255, loss = 0.04295602\n",
      "Iteration 256, loss = 0.04074681\n",
      "Iteration 324, loss = 0.02662167\n",
      "Iteration 325, loss = 0.02627254\n",
      "Iteration 257, loss = 0.04104849\n",
      "Iteration 326, loss = 0.02552657\n",
      "Iteration 258, loss = 0.04058563\n",
      "Iteration 327, loss = 0.02748293\n",
      "Iteration 259, loss = 0.04012446\n",
      "Iteration 260, loss = 0.04009298Iteration 328, loss = 0.02561957\n",
      "\n",
      "Iteration 329, loss = 0.02566081\n",
      "Iteration 261, loss = 0.04118631\n",
      "Iteration 330, loss = 0.02575622\n",
      "Iteration 262, loss = 0.04136381\n",
      "Iteration 331, loss = 0.02544913\n",
      "Iteration 263, loss = 0.04150892\n",
      "Iteration 332, loss = 0.02531613\n",
      "Iteration 264, loss = 0.04034645\n",
      "Iteration 333, loss = 0.02527000\n",
      "Iteration 265, loss = 0.04099165\n",
      "Iteration 334, loss = 0.02690376\n",
      "Iteration 266, loss = 0.03894958\n",
      "Iteration 335, loss = 0.02467819\n",
      "Iteration 267, loss = 0.03900797\n",
      "Iteration 336, loss = 0.02469924\n",
      "Iteration 268, loss = 0.03802090\n",
      "Iteration 337, loss = 0.02445748\n",
      "Iteration 269, loss = 0.03828927\n",
      "Iteration 338, loss = 0.02440491\n",
      "Iteration 270, loss = 0.03769848\n",
      "Iteration 339, loss = 0.02451131Iteration 271, loss = 0.03809200\n",
      "\n",
      "Iteration 272, loss = 0.03785128Iteration 340, loss = 0.02474042\n",
      "\n",
      "Iteration 273, loss = 0.03761824\n",
      "Iteration 341, loss = 0.02487007\n",
      "Iteration 274, loss = 0.03734541\n",
      "Iteration 342, loss = 0.02421257\n",
      "Iteration 275, loss = 0.03851203\n",
      "Iteration 343, loss = 0.02503163\n",
      "Iteration 276, loss = 0.03764028\n",
      "Iteration 344, loss = 0.02356859\n",
      "Iteration 277, loss = 0.03724509\n",
      "Iteration 345, loss = 0.02456081\n",
      "Iteration 278, loss = 0.03700166\n",
      "Iteration 346, loss = 0.02417880\n",
      "Iteration 279, loss = 0.03655288\n",
      "Iteration 347, loss = 0.02667746\n",
      "Iteration 280, loss = 0.03644831\n",
      "Iteration 348, loss = 0.02501090\n",
      "Iteration 281, loss = 0.03626821\n",
      "Iteration 349, loss = 0.02250038\n",
      "Iteration 282, loss = 0.03553813\n",
      "Iteration 350, loss = 0.02520458\n",
      "Iteration 351, loss = 0.02535482\n",
      "Iteration 283, loss = 0.03554913\n",
      "Iteration 352, loss = 0.02467154\n",
      "Iteration 284, loss = 0.03566065\n",
      "Iteration 353, loss = 0.02450542\n",
      "Iteration 285, loss = 0.03642945\n",
      "Iteration 354, loss = 0.02293678\n",
      "Iteration 286, loss = 0.03555299\n",
      "Iteration 287, loss = 0.03524128\n",
      "Iteration 355, loss = 0.02399697\n",
      "Iteration 288, loss = 0.03556042\n",
      "Iteration 356, loss = 0.02667509\n",
      "Iteration 289, loss = 0.03510916\n",
      "Iteration 357, loss = 0.02298201\n",
      "Iteration 290, loss = 0.03443151\n",
      "Iteration 358, loss = 0.02275446\n",
      "Iteration 291, loss = 0.03432870\n",
      "Iteration 359, loss = 0.02408022\n",
      "Iteration 292, loss = 0.03503164\n",
      "Iteration 360, loss = 0.02251570\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 293, loss = 0.03489729\n",
      "Iteration 294, loss = 0.03347486\n",
      "Iteration 295, loss = 0.03410801\n",
      "Iteration 296, loss = 0.03472089\n",
      "Iteration 297, loss = 0.03650130\n",
      "Iteration 298, loss = 0.03251908\n",
      "Iteration 299, loss = 0.03414690\n",
      "Iteration 300, loss = 0.03437617\n",
      "Iteration 301, loss = 0.03256803\n",
      "Iteration 302, loss = 0.03314728\n",
      "Iteration 303, loss = 0.03229657\n",
      "Iteration 304, loss = 0.03288945\n",
      "Iteration 305, loss = 0.03206709\n",
      "Iteration 306, loss = 0.03301048\n",
      "Iteration 307, loss = 0.03292038\n",
      "Iteration 308, loss = 0.03284252\n",
      "Iteration 309, loss = 0.03087371\n",
      "Iteration 310, loss = 0.03571983\n",
      "Iteration 1, loss = 1.34157058\n",
      "Iteration 311, loss = 0.03784845\n",
      "Iteration 2, loss = 1.07597054\n",
      "Iteration 312, loss = 0.03221840\n",
      "Iteration 3, loss = 0.91940465\n",
      "Iteration 313, loss = 0.03306921\n",
      "Iteration 4, loss = 0.81299538\n",
      "Iteration 314, loss = 0.03275766\n",
      "Iteration 5, loss = 0.73638436\n",
      "Iteration 315, loss = 0.03328149\n",
      "Iteration 6, loss = 0.67760170\n",
      "Iteration 316, loss = 0.03231429\n",
      "Iteration 7, loss = 0.63232858\n",
      "Iteration 317, loss = 0.03000362\n",
      "Iteration 8, loss = 0.59527896\n",
      "Iteration 318, loss = 0.03227287\n",
      "Iteration 9, loss = 0.56496690\n",
      "Iteration 319, loss = 0.03079878\n",
      "Iteration 10, loss = 0.53880495\n",
      "Iteration 320, loss = 0.02989629\n",
      "Iteration 11, loss = 0.51699197\n",
      "Iteration 321, loss = 0.03032004\n",
      "Iteration 12, loss = 0.49713440\n",
      "Iteration 322, loss = 0.03042125\n",
      "Iteration 13, loss = 0.48009433\n",
      "Iteration 323, loss = 0.02931147\n",
      "Iteration 14, loss = 0.46425226\n",
      "Iteration 324, loss = 0.03044314\n",
      "Iteration 15, loss = 0.45114152\n",
      "Iteration 325, loss = 0.03043134\n",
      "Iteration 16, loss = 0.43928653\n",
      "Iteration 326, loss = 0.02947351\n",
      "Iteration 17, loss = 0.42743390\n",
      "Iteration 327, loss = 0.02991585\n",
      "Iteration 18, loss = 0.41847019\n",
      "Iteration 328, loss = 0.03033132\n",
      "Iteration 19, loss = 0.40874203\n",
      "Iteration 329, loss = 0.02887122\n",
      "Iteration 20, loss = 0.39942052\n",
      "Iteration 330, loss = 0.02898218\n",
      "Iteration 21, loss = 0.39177302\n",
      "Iteration 331, loss = 0.02879036\n",
      "Iteration 22, loss = 0.38359164\n",
      "Iteration 332, loss = 0.02871180\n",
      "Iteration 23, loss = 0.37664646\n",
      "Iteration 333, loss = 0.02868684\n",
      "Iteration 24, loss = 0.36948611\n",
      "Iteration 334, loss = 0.02963287\n",
      "Iteration 25, loss = 0.36331011\n",
      "Iteration 335, loss = 0.02826179\n",
      "Iteration 26, loss = 0.35662971\n",
      "Iteration 336, loss = 0.02860804\n",
      "Iteration 27, loss = 0.35058079\n",
      "Iteration 337, loss = 0.03277983\n",
      "Iteration 28, loss = 0.34489363\n",
      "Iteration 338, loss = 0.03469960\n",
      "Iteration 29, loss = 0.33876267\n",
      "Iteration 339, loss = 0.02822593\n",
      "Iteration 30, loss = 0.33336855\n",
      "Iteration 31, loss = 0.32904832\n",
      "Iteration 340, loss = 0.02946249\n",
      "Iteration 341, loss = 0.02964296\n",
      "Iteration 32, loss = 0.32252618\n",
      "Iteration 33, loss = 0.31843357\n",
      "Iteration 342, loss = 0.03035449\n",
      "Iteration 34, loss = 0.31300973\n",
      "Iteration 343, loss = 0.02785622\n",
      "Iteration 35, loss = 0.30836991\n",
      "Iteration 344, loss = 0.02882347\n",
      "Iteration 36, loss = 0.30403042\n",
      "Iteration 345, loss = 0.02774116\n",
      "Iteration 37, loss = 0.29916984\n",
      "Iteration 346, loss = 0.02716685\n",
      "Iteration 38, loss = 0.29475716\n",
      "Iteration 347, loss = 0.02746840\n",
      "Iteration 39, loss = 0.28971453\n",
      "Iteration 348, loss = 0.02717635\n",
      "Iteration 40, loss = 0.28585763\n",
      "Iteration 349, loss = 0.02720439\n",
      "Iteration 41, loss = 0.28220022\n",
      "Iteration 350, loss = 0.02702078\n",
      "Iteration 42, loss = 0.27752344\n",
      "Iteration 351, loss = 0.02664622\n",
      "Iteration 43, loss = 0.27335954\n",
      "Iteration 352, loss = 0.02722573\n",
      "Iteration 44, loss = 0.26951957\n",
      "Iteration 353, loss = 0.02650834\n",
      "Iteration 45, loss = 0.26535572\n",
      "Iteration 354, loss = 0.02592687\n",
      "Iteration 46, loss = 0.26166389\n",
      "Iteration 355, loss = 0.02757194\n",
      "Iteration 47, loss = 0.25784846\n",
      "Iteration 356, loss = 0.02899860\n",
      "Iteration 48, loss = 0.25398215\n",
      "Iteration 357, loss = 0.03231298\n",
      "Iteration 49, loss = 0.25006905\n",
      "Iteration 358, loss = 0.02765225\n",
      "Iteration 50, loss = 0.24681255\n",
      "Iteration 359, loss = 0.03155040\n",
      "Iteration 51, loss = 0.24285411\n",
      "Iteration 360, loss = 0.02643468\n",
      "Iteration 52, loss = 0.24049248\n",
      "Iteration 361, loss = 0.02840316\n",
      "Iteration 53, loss = 0.23690420\n",
      "Iteration 362, loss = 0.02759226\n",
      "Iteration 54, loss = 0.23464749\n",
      "Iteration 363, loss = 0.02810636\n",
      "Iteration 55, loss = 0.22898957\n",
      "Iteration 364, loss = 0.02705243\n",
      "Iteration 56, loss = 0.22739324\n",
      "Iteration 365, loss = 0.02543866\n",
      "Iteration 57, loss = 0.22360011\n",
      "Iteration 366, loss = 0.02876193\n",
      "Iteration 58, loss = 0.22057831\n",
      "Iteration 367, loss = 0.02782510\n",
      "Iteration 59, loss = 0.21723717\n",
      "Iteration 368, loss = 0.02552987\n",
      "Iteration 60, loss = 0.21516230\n",
      "Iteration 369, loss = 0.02653680\n",
      "Iteration 61, loss = 0.21167497\n",
      "Iteration 370, loss = 0.02692639\n",
      "Iteration 62, loss = 0.20937932\n",
      "Iteration 371, loss = 0.02461277\n",
      "Iteration 63, loss = 0.20745237\n",
      "Iteration 372, loss = 0.02634244\n",
      "Iteration 64, loss = 0.20570949\n",
      "Iteration 373, loss = 0.02480604\n",
      "Iteration 65, loss = 0.20119449\n",
      "Iteration 374, loss = 0.02464368\n",
      "Iteration 66, loss = 0.19885840\n",
      "Iteration 375, loss = 0.02467892\n",
      "Iteration 67, loss = 0.19625629\n",
      "Iteration 376, loss = 0.02443830\n",
      "Iteration 68, loss = 0.19412247\n",
      "Iteration 377, loss = 0.02568368\n",
      "Iteration 69, loss = 0.19164862\n",
      "Iteration 378, loss = 0.02486588\n",
      "Iteration 70, loss = 0.18964969\n",
      "Iteration 379, loss = 0.02488811\n",
      "Iteration 71, loss = 0.18789733\n",
      "Iteration 380, loss = 0.02510403\n",
      "Iteration 72, loss = 0.18464202\n",
      "Iteration 381, loss = 0.02540276\n",
      "Iteration 73, loss = 0.18306913\n",
      "Iteration 382, loss = 0.02504745\n",
      "Iteration 74, loss = 0.18149470\n",
      "Iteration 383, loss = 0.02582405\n",
      "Iteration 75, loss = 0.17922253\n",
      "Iteration 384, loss = 0.02403157\n",
      "Iteration 76, loss = 0.17730579\n",
      "Iteration 385, loss = 0.02358260\n",
      "Iteration 77, loss = 0.17435866\n",
      "Iteration 386, loss = 0.02420475\n",
      "Iteration 78, loss = 0.17330745\n",
      "Iteration 387, loss = 0.02354416\n",
      "Iteration 79, loss = 0.17219467\n",
      "Iteration 388, loss = 0.02427643\n",
      "Iteration 80, loss = 0.16919322\n",
      "Iteration 389, loss = 0.02373315\n",
      "Iteration 81, loss = 0.16654513\n",
      "Iteration 390, loss = 0.02328398\n",
      "Iteration 82, loss = 0.16472787\n",
      "Iteration 391, loss = 0.02339474\n",
      "Iteration 83, loss = 0.16336953\n",
      "Iteration 392, loss = 0.02335226\n",
      "Iteration 84, loss = 0.16148623\n",
      "Iteration 393, loss = 0.02357344\n",
      "Iteration 85, loss = 0.15928006\n",
      "Iteration 394, loss = 0.02260841\n",
      "Iteration 86, loss = 0.15960425\n",
      "Iteration 395, loss = 0.02431656\n",
      "Iteration 87, loss = 0.15777804\n",
      "Iteration 396, loss = 0.02383945\n",
      "Iteration 88, loss = 0.15476288\n",
      "Iteration 397, loss = 0.02270925\n",
      "Iteration 89, loss = 0.15375888\n",
      "Iteration 398, loss = 0.02286598\n",
      "Iteration 90, loss = 0.15101795\n",
      "Iteration 399, loss = 0.02249968\n",
      "Iteration 91, loss = 0.15054432\n",
      "Iteration 400, loss = 0.02241273\n",
      "Iteration 92, loss = 0.14890278\n",
      "Iteration 401, loss = 0.02222029\n",
      "Iteration 93, loss = 0.14566875\n",
      "Iteration 402, loss = 0.02589791\n",
      "Iteration 94, loss = 0.14590081\n",
      "Iteration 403, loss = 0.02511053\n",
      "Iteration 95, loss = 0.14241505\n",
      "Iteration 404, loss = 0.02131831\n",
      "Iteration 96, loss = 0.14124687\n",
      "Iteration 405, loss = 0.02535761\n",
      "Iteration 97, loss = 0.14105841\n",
      "Iteration 406, loss = 0.02667551\n",
      "Iteration 98, loss = 0.13834717\n",
      "Iteration 407, loss = 0.02313919\n",
      "Iteration 99, loss = 0.13649337\n",
      "Iteration 408, loss = 0.02425696\n",
      "Iteration 100, loss = 0.13551839\n",
      "Iteration 409, loss = 0.02178680\n",
      "Iteration 101, loss = 0.13460045\n",
      "Iteration 410, loss = 0.02375731\n",
      "Iteration 102, loss = 0.13639777\n",
      "Iteration 411, loss = 0.02133173\n",
      "Iteration 103, loss = 0.13419066\n",
      "Iteration 412, loss = 0.02229230\n",
      "Iteration 104, loss = 0.13043045\n",
      "Iteration 413, loss = 0.02297979\n",
      "Iteration 105, loss = 0.13013304\n",
      "Iteration 414, loss = 0.02313732\n",
      "Iteration 106, loss = 0.12665736\n",
      "Iteration 415, loss = 0.02283024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 107, loss = 0.12817521\n",
      "Iteration 108, loss = 0.12595056\n",
      "Iteration 109, loss = 0.12584247\n",
      "Iteration 110, loss = 0.12166755\n",
      "Iteration 111, loss = 0.12142798\n",
      "Iteration 112, loss = 0.11923327\n",
      "Iteration 113, loss = 0.11846254\n",
      "Iteration 114, loss = 0.11708030\n",
      "Iteration 115, loss = 0.11772359\n",
      "Iteration 116, loss = 0.11654678\n",
      "Iteration 117, loss = 0.11422079\n",
      "Iteration 118, loss = 0.11448596\n",
      "Iteration 119, loss = 0.11359109\n",
      "Iteration 120, loss = 0.11002170\n",
      "Iteration 121, loss = 0.11062838\n",
      "Iteration 122, loss = 0.10879048\n",
      "Iteration 123, loss = 0.10859004\n",
      "Iteration 124, loss = 0.10657901\n",
      "Iteration 1, loss = 1.48182509\n",
      "Iteration 125, loss = 0.10507095\n",
      "Iteration 2, loss = 1.23299727\n",
      "Iteration 126, loss = 0.10411224\n",
      "Iteration 3, loss = 1.05696909\n",
      "Iteration 127, loss = 0.10409113\n",
      "Iteration 4, loss = 0.93378631\n",
      "Iteration 128, loss = 0.10376691\n",
      "Iteration 5, loss = 0.83400309\n",
      "Iteration 129, loss = 0.10304218\n",
      "Iteration 6, loss = 0.75553870\n",
      "Iteration 130, loss = 0.10016756\n",
      "Iteration 7, loss = 0.69297162\n",
      "Iteration 131, loss = 0.09915335\n",
      "Iteration 8, loss = 0.64203173\n",
      "Iteration 132, loss = 0.09852114\n",
      "Iteration 9, loss = 0.59989288\n",
      "Iteration 133, loss = 0.09994944\n",
      "Iteration 10, loss = 0.56535917\n",
      "Iteration 134, loss = 0.09605712\n",
      "Iteration 11, loss = 0.53760259\n",
      "Iteration 135, loss = 0.09712263\n",
      "Iteration 12, loss = 0.51285834\n",
      "Iteration 136, loss = 0.09432002\n",
      "Iteration 13, loss = 0.49217613\n",
      "Iteration 137, loss = 0.09328150\n",
      "Iteration 14, loss = 0.47375642\n",
      "Iteration 138, loss = 0.09286944\n",
      "Iteration 15, loss = 0.45797318\n",
      "Iteration 139, loss = 0.09186363\n",
      "Iteration 16, loss = 0.44422232\n",
      "Iteration 140, loss = 0.09032255\n",
      "Iteration 17, loss = 0.43197128\n",
      "Iteration 141, loss = 0.08985104\n",
      "Iteration 18, loss = 0.42018754\n",
      "Iteration 142, loss = 0.08859028\n",
      "Iteration 19, loss = 0.40962754\n",
      "Iteration 143, loss = 0.08798776\n",
      "Iteration 20, loss = 0.40047166\n",
      "Iteration 144, loss = 0.08780415\n",
      "Iteration 21, loss = 0.39232680\n",
      "Iteration 145, loss = 0.08527360\n",
      "Iteration 22, loss = 0.38416231\n",
      "Iteration 146, loss = 0.08467317\n",
      "Iteration 23, loss = 0.37665683\n",
      "Iteration 147, loss = 0.08393827\n",
      "Iteration 24, loss = 0.36882374\n",
      "Iteration 148, loss = 0.08354895\n",
      "Iteration 25, loss = 0.36273651\n",
      "Iteration 149, loss = 0.08371709\n",
      "Iteration 26, loss = 0.35588741\n",
      "Iteration 150, loss = 0.08150175\n",
      "Iteration 27, loss = 0.35034166\n",
      "Iteration 151, loss = 0.08120249\n",
      "Iteration 28, loss = 0.34400602\n",
      "Iteration 152, loss = 0.08401894\n",
      "Iteration 29, loss = 0.33850103\n",
      "Iteration 153, loss = 0.08136603\n",
      "Iteration 30, loss = 0.33313938\n",
      "Iteration 154, loss = 0.07874655\n",
      "Iteration 31, loss = 0.32794218\n",
      "Iteration 155, loss = 0.08018247\n",
      "Iteration 32, loss = 0.32288344\n",
      "Iteration 156, loss = 0.07751660\n",
      "Iteration 33, loss = 0.31753116\n",
      "Iteration 157, loss = 0.07743166\n",
      "Iteration 34, loss = 0.31368723\n",
      "Iteration 158, loss = 0.07585340\n",
      "Iteration 35, loss = 0.30886989\n",
      "Iteration 159, loss = 0.07419635\n",
      "Iteration 36, loss = 0.30445554\n",
      "Iteration 160, loss = 0.07293270\n",
      "Iteration 37, loss = 0.30000392\n",
      "Iteration 161, loss = 0.07278022\n",
      "Iteration 38, loss = 0.29519514\n",
      "Iteration 162, loss = 0.07257399\n",
      "Iteration 39, loss = 0.29060287\n",
      "Iteration 163, loss = 0.07084852\n",
      "Iteration 40, loss = 0.28607682\n",
      "Iteration 164, loss = 0.07107392\n",
      "Iteration 41, loss = 0.28228055\n",
      "Iteration 165, loss = 0.07167863\n",
      "Iteration 42, loss = 0.27840349\n",
      "Iteration 166, loss = 0.07075944\n",
      "Iteration 43, loss = 0.27450558\n",
      "Iteration 167, loss = 0.06942588\n",
      "Iteration 44, loss = 0.27027478\n",
      "Iteration 168, loss = 0.06920304\n",
      "Iteration 45, loss = 0.26728599\n",
      "Iteration 169, loss = 0.06692008\n",
      "Iteration 46, loss = 0.26500695\n",
      "Iteration 170, loss = 0.06811063\n",
      "Iteration 47, loss = 0.26005722\n",
      "Iteration 171, loss = 0.06539780\n",
      "Iteration 48, loss = 0.25761208\n",
      "Iteration 172, loss = 0.06455212\n",
      "Iteration 49, loss = 0.25230687\n",
      "Iteration 173, loss = 0.06454200\n",
      "Iteration 50, loss = 0.25047005\n",
      "Iteration 174, loss = 0.06333217\n",
      "Iteration 51, loss = 0.24564275\n",
      "Iteration 175, loss = 0.06326874\n",
      "Iteration 52, loss = 0.24258343\n",
      "Iteration 176, loss = 0.06270423\n",
      "Iteration 53, loss = 0.24051235\n",
      "Iteration 177, loss = 0.06130031\n",
      "Iteration 54, loss = 0.23566917\n",
      "Iteration 178, loss = 0.06192541\n",
      "Iteration 55, loss = 0.23283675\n",
      "Iteration 179, loss = 0.06094426\n",
      "Iteration 56, loss = 0.23052122\n",
      "Iteration 180, loss = 0.06143090\n",
      "Iteration 57, loss = 0.22760453\n",
      "Iteration 181, loss = 0.05948232\n",
      "Iteration 58, loss = 0.22426135\n",
      "Iteration 182, loss = 0.05823233\n",
      "Iteration 59, loss = 0.22105614\n",
      "Iteration 183, loss = 0.05860867\n",
      "Iteration 60, loss = 0.21859772\n",
      "Iteration 184, loss = 0.05755436\n",
      "Iteration 61, loss = 0.21584202\n",
      "Iteration 185, loss = 0.05819024\n",
      "Iteration 62, loss = 0.21332163\n",
      "Iteration 186, loss = 0.05814112\n",
      "Iteration 63, loss = 0.21056847\n",
      "Iteration 187, loss = 0.05601616\n",
      "Iteration 64, loss = 0.20781574\n",
      "Iteration 188, loss = 0.05657499\n",
      "Iteration 65, loss = 0.20551938\n",
      "Iteration 189, loss = 0.05482388\n",
      "Iteration 66, loss = 0.20334100\n",
      "Iteration 190, loss = 0.05426556\n",
      "Iteration 67, loss = 0.20046442\n",
      "Iteration 191, loss = 0.05400166\n",
      "Iteration 68, loss = 0.19800959\n",
      "Iteration 192, loss = 0.05306664\n",
      "Iteration 69, loss = 0.19704107\n",
      "Iteration 193, loss = 0.05314926\n",
      "Iteration 70, loss = 0.19282408\n",
      "Iteration 194, loss = 0.05308689\n",
      "Iteration 71, loss = 0.19141087\n",
      "Iteration 195, loss = 0.05155461\n",
      "Iteration 72, loss = 0.19024114\n",
      "Iteration 196, loss = 0.05258914\n",
      "Iteration 73, loss = 0.18650530\n",
      "Iteration 197, loss = 0.05129272\n",
      "Iteration 74, loss = 0.18538823\n",
      "Iteration 198, loss = 0.05033662\n",
      "Iteration 75, loss = 0.18242485\n",
      "Iteration 199, loss = 0.05067357\n",
      "Iteration 76, loss = 0.18049127\n",
      "Iteration 200, loss = 0.04913774\n",
      "Iteration 77, loss = 0.17830621\n",
      "Iteration 201, loss = 0.04930988\n",
      "Iteration 78, loss = 0.17592525\n",
      "Iteration 202, loss = 0.04950347\n",
      "Iteration 79, loss = 0.17489279\n",
      "Iteration 203, loss = 0.04895651\n",
      "Iteration 80, loss = 0.17240561\n",
      "Iteration 204, loss = 0.04737529\n",
      "Iteration 81, loss = 0.17186395\n",
      "Iteration 205, loss = 0.04723986\n",
      "Iteration 82, loss = 0.16877206\n",
      "Iteration 206, loss = 0.04777132\n",
      "Iteration 83, loss = 0.16747197\n",
      "Iteration 207, loss = 0.04783678\n",
      "Iteration 84, loss = 0.16513224\n",
      "Iteration 208, loss = 0.04615747\n",
      "Iteration 85, loss = 0.16431562\n",
      "Iteration 209, loss = 0.04473350\n",
      "Iteration 86, loss = 0.16242357\n",
      "Iteration 210, loss = 0.04559322\n",
      "Iteration 87, loss = 0.16057229\n",
      "Iteration 211, loss = 0.04422275\n",
      "Iteration 88, loss = 0.15949131\n",
      "Iteration 212, loss = 0.04552386\n",
      "Iteration 89, loss = 0.15824753\n",
      "Iteration 213, loss = 0.04417175\n",
      "Iteration 90, loss = 0.15602857\n",
      "Iteration 214, loss = 0.04353263\n",
      "Iteration 91, loss = 0.15559551\n",
      "Iteration 215, loss = 0.04278963\n",
      "Iteration 92, loss = 0.15364390\n",
      "Iteration 216, loss = 0.04257004\n",
      "Iteration 93, loss = 0.15197095\n",
      "Iteration 217, loss = 0.04191711\n",
      "Iteration 94, loss = 0.14975893\n",
      "Iteration 218, loss = 0.04242323\n",
      "Iteration 95, loss = 0.14876359\n",
      "Iteration 219, loss = 0.04288614\n",
      "Iteration 96, loss = 0.14796365\n",
      "Iteration 220, loss = 0.04016113\n",
      "Iteration 97, loss = 0.14682414\n",
      "Iteration 221, loss = 0.04141539\n",
      "Iteration 98, loss = 0.14643973\n",
      "Iteration 222, loss = 0.04022254\n",
      "Iteration 99, loss = 0.14423368\n",
      "Iteration 223, loss = 0.03962400\n",
      "Iteration 100, loss = 0.14335350\n",
      "Iteration 224, loss = 0.04123544\n",
      "Iteration 101, loss = 0.14047864\n",
      "Iteration 225, loss = 0.03936190\n",
      "Iteration 102, loss = 0.14022415\n",
      "Iteration 226, loss = 0.03945824\n",
      "Iteration 103, loss = 0.13971242\n",
      "Iteration 227, loss = 0.03810442\n",
      "Iteration 104, loss = 0.13719560\n",
      "Iteration 228, loss = 0.03805518\n",
      "Iteration 105, loss = 0.13583937\n",
      "Iteration 229, loss = 0.03754303\n",
      "Iteration 106, loss = 0.13574205\n",
      "Iteration 230, loss = 0.03740265\n",
      "Iteration 107, loss = 0.13505233\n",
      "Iteration 231, loss = 0.03746739\n",
      "Iteration 108, loss = 0.13238034\n",
      "Iteration 232, loss = 0.03769806\n",
      "Iteration 109, loss = 0.13162824\n",
      "Iteration 233, loss = 0.03711699\n",
      "Iteration 110, loss = 0.13047157\n",
      "Iteration 234, loss = 0.03666721\n",
      "Iteration 111, loss = 0.12965983\n",
      "Iteration 235, loss = 0.03696243\n",
      "Iteration 112, loss = 0.12846745\n",
      "Iteration 236, loss = 0.03771129\n",
      "Iteration 113, loss = 0.12882963\n",
      "Iteration 237, loss = 0.03636290\n",
      "Iteration 114, loss = 0.12762402\n",
      "Iteration 238, loss = 0.03496181\n",
      "Iteration 115, loss = 0.12638337\n",
      "Iteration 239, loss = 0.03462402\n",
      "Iteration 116, loss = 0.12603163\n",
      "Iteration 240, loss = 0.03432415\n",
      "Iteration 117, loss = 0.12278476\n",
      "Iteration 241, loss = 0.03387428\n",
      "Iteration 118, loss = 0.12295205\n",
      "Iteration 242, loss = 0.03461290\n",
      "Iteration 119, loss = 0.12134616\n",
      "Iteration 243, loss = 0.03474843\n",
      "Iteration 120, loss = 0.12099876\n",
      "Iteration 244, loss = 0.03360239\n",
      "Iteration 121, loss = 0.12023716\n",
      "Iteration 245, loss = 0.03264336\n",
      "Iteration 122, loss = 0.11857749\n",
      "Iteration 246, loss = 0.03316103\n",
      "Iteration 123, loss = 0.12028013\n",
      "Iteration 247, loss = 0.03316008\n",
      "Iteration 124, loss = 0.11811904\n",
      "Iteration 248, loss = 0.03183827\n",
      "Iteration 125, loss = 0.11609290\n",
      "Iteration 249, loss = 0.03226151\n",
      "Iteration 126, loss = 0.11635769\n",
      "Iteration 250, loss = 0.03118661\n",
      "Iteration 127, loss = 0.11581547\n",
      "Iteration 251, loss = 0.03125300\n",
      "Iteration 128, loss = 0.11565208\n",
      "Iteration 252, loss = 0.03107577\n",
      "Iteration 129, loss = 0.11286218\n",
      "Iteration 253, loss = 0.03023200\n",
      "Iteration 130, loss = 0.11277877\n",
      "Iteration 254, loss = 0.03079829\n",
      "Iteration 131, loss = 0.11131322\n",
      "Iteration 255, loss = 0.03056847\n",
      "Iteration 132, loss = 0.11087032\n",
      "Iteration 256, loss = 0.02935968\n",
      "Iteration 133, loss = 0.11018235\n",
      "Iteration 257, loss = 0.03130082\n",
      "Iteration 134, loss = 0.11180397\n",
      "Iteration 258, loss = 0.03142274\n",
      "Iteration 135, loss = 0.10871303\n",
      "Iteration 259, loss = 0.03175910\n",
      "Iteration 136, loss = 0.10820850\n",
      "Iteration 260, loss = 0.02942802\n",
      "Iteration 137, loss = 0.10736745\n",
      "Iteration 261, loss = 0.02910400\n",
      "Iteration 138, loss = 0.10642363\n",
      "Iteration 262, loss = 0.03053750\n",
      "Iteration 139, loss = 0.10643256\n",
      "Iteration 263, loss = 0.02857766\n",
      "Iteration 140, loss = 0.10445121\n",
      "Iteration 264, loss = 0.02854215\n",
      "Iteration 141, loss = 0.10442483\n",
      "Iteration 265, loss = 0.02784961\n",
      "Iteration 142, loss = 0.10334108\n",
      "Iteration 266, loss = 0.02733884\n",
      "Iteration 143, loss = 0.10361193\n",
      "Iteration 267, loss = 0.02718017\n",
      "Iteration 144, loss = 0.10222671\n",
      "Iteration 268, loss = 0.02700191\n",
      "Iteration 145, loss = 0.10104646\n",
      "Iteration 269, loss = 0.02663654\n",
      "Iteration 146, loss = 0.10068918\n",
      "Iteration 270, loss = 0.02689247\n",
      "Iteration 147, loss = 0.10014743\n",
      "Iteration 271, loss = 0.02625965\n",
      "Iteration 148, loss = 0.10011453\n",
      "Iteration 272, loss = 0.02654829\n",
      "Iteration 149, loss = 0.10005589\n",
      "Iteration 273, loss = 0.02685467\n",
      "Iteration 150, loss = 0.09861441\n",
      "Iteration 274, loss = 0.02568810\n",
      "Iteration 151, loss = 0.09914631\n",
      "Iteration 275, loss = 0.02584724\n",
      "Iteration 152, loss = 0.09668243\n",
      "Iteration 276, loss = 0.02493278\n",
      "Iteration 153, loss = 0.09726553\n",
      "Iteration 277, loss = 0.02518685\n",
      "Iteration 154, loss = 0.09543669\n",
      "Iteration 278, loss = 0.02490401\n",
      "Iteration 155, loss = 0.09671077\n",
      "Iteration 279, loss = 0.02498463\n",
      "Iteration 156, loss = 0.09504343\n",
      "Iteration 280, loss = 0.02461212\n",
      "Iteration 157, loss = 0.09330656\n",
      "Iteration 281, loss = 0.02397277\n",
      "Iteration 158, loss = 0.09546778\n",
      "Iteration 282, loss = 0.02408968\n",
      "Iteration 159, loss = 0.09291526\n",
      "Iteration 283, loss = 0.02423351\n",
      "Iteration 160, loss = 0.09310980\n",
      "Iteration 284, loss = 0.02373810\n",
      "Iteration 161, loss = 0.09537987\n",
      "Iteration 285, loss = 0.02418384\n",
      "Iteration 162, loss = 0.09006522\n",
      "Iteration 286, loss = 0.02372549\n",
      "Iteration 163, loss = 0.08988410\n",
      "Iteration 287, loss = 0.02329954\n",
      "Iteration 164, loss = 0.09078977\n",
      "Iteration 288, loss = 0.02319459\n",
      "Iteration 165, loss = 0.08854255\n",
      "Iteration 289, loss = 0.02287102\n",
      "Iteration 166, loss = 0.08923622\n",
      "Iteration 290, loss = 0.02299548\n",
      "Iteration 167, loss = 0.08743725\n",
      "Iteration 291, loss = 0.02296109\n",
      "Iteration 168, loss = 0.08710504\n",
      "Iteration 292, loss = 0.02209804\n",
      "Iteration 169, loss = 0.08642229\n",
      "Iteration 293, loss = 0.02231040\n",
      "Iteration 170, loss = 0.08614409\n",
      "Iteration 294, loss = 0.02184008\n",
      "Iteration 171, loss = 0.08748173\n",
      "Iteration 295, loss = 0.02177199\n",
      "Iteration 172, loss = 0.08429389\n",
      "Iteration 296, loss = 0.02176781\n",
      "Iteration 173, loss = 0.08494298\n",
      "Iteration 297, loss = 0.02179059\n",
      "Iteration 174, loss = 0.08514946\n",
      "Iteration 298, loss = 0.02174060\n",
      "Iteration 175, loss = 0.08645145\n",
      "Iteration 299, loss = 0.02194948\n",
      "Iteration 176, loss = 0.08360572\n",
      "Iteration 300, loss = 0.02092310\n",
      "Iteration 177, loss = 0.08161924\n",
      "Iteration 301, loss = 0.02103560\n",
      "Iteration 178, loss = 0.08293911\n",
      "Iteration 302, loss = 0.02102085\n",
      "Iteration 179, loss = 0.08339070\n",
      "Iteration 303, loss = 0.02054502\n",
      "Iteration 180, loss = 0.08220196\n",
      "Iteration 304, loss = 0.02021332\n",
      "Iteration 181, loss = 0.07969794\n",
      "Iteration 305, loss = 0.02035869\n",
      "Iteration 182, loss = 0.07967207\n",
      "Iteration 306, loss = 0.02008077\n",
      "Iteration 183, loss = 0.07829039\n",
      "Iteration 307, loss = 0.01993621\n",
      "Iteration 184, loss = 0.07852591\n",
      "Iteration 308, loss = 0.01977514\n",
      "Iteration 185, loss = 0.07674063\n",
      "Iteration 309, loss = 0.01965568\n",
      "Iteration 186, loss = 0.07704365\n",
      "Iteration 310, loss = 0.01997480\n",
      "Iteration 187, loss = 0.07626854\n",
      "Iteration 311, loss = 0.02043319\n",
      "Iteration 188, loss = 0.07600515\n",
      "Iteration 312, loss = 0.01877287\n",
      "Iteration 189, loss = 0.07495773\n",
      "Iteration 313, loss = 0.01986957\n",
      "Iteration 190, loss = 0.07465323\n",
      "Iteration 314, loss = 0.02058177\n",
      "Iteration 191, loss = 0.07381217\n",
      "Iteration 315, loss = 0.01927440\n",
      "Iteration 192, loss = 0.07431854\n",
      "Iteration 316, loss = 0.01892574\n",
      "Iteration 193, loss = 0.07389757\n",
      "Iteration 317, loss = 0.01935192\n",
      "Iteration 194, loss = 0.07194403\n",
      "Iteration 318, loss = 0.01958627\n",
      "Iteration 195, loss = 0.07326855\n",
      "Iteration 319, loss = 0.01874519\n",
      "Iteration 196, loss = 0.07331400\n",
      "Iteration 320, loss = 0.01916414\n",
      "Iteration 197, loss = 0.07135923\n",
      "Iteration 321, loss = 0.01875474\n",
      "Iteration 198, loss = 0.07082034\n",
      "Iteration 322, loss = 0.01882079\n",
      "Iteration 199, loss = 0.07028035\n",
      "Iteration 323, loss = 0.01896617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 200, loss = 0.06904074\n",
      "Iteration 201, loss = 0.06897235\n",
      "Iteration 202, loss = 0.06951822\n",
      "Iteration 203, loss = 0.06945299\n",
      "Iteration 204, loss = 0.07007559\n",
      "Iteration 205, loss = 0.06805480\n",
      "Iteration 206, loss = 0.06928636\n",
      "Iteration 207, loss = 0.06637129\n",
      "Iteration 208, loss = 0.06702635\n",
      "Iteration 209, loss = 0.06697068\n",
      "Iteration 210, loss = 0.07210176\n",
      "Iteration 211, loss = 0.07067873\n",
      "Iteration 212, loss = 0.07223716\n",
      "Iteration 213, loss = 0.06441231\n",
      "Iteration 214, loss = 0.06917940\n",
      "Iteration 215, loss = 0.06899682\n",
      "Iteration 216, loss = 0.06771753\n",
      "Iteration 217, loss = 0.06332101\n",
      "Iteration 1, loss = 1.40870002\n",
      "Iteration 218, loss = 0.06636201\n",
      "Iteration 2, loss = 1.09756399\n",
      "Iteration 219, loss = 0.06932284\n",
      "Iteration 3, loss = 0.91616575\n",
      "Iteration 220, loss = 0.06503117\n",
      "Iteration 4, loss = 0.80837949\n",
      "Iteration 221, loss = 0.06565083\n",
      "Iteration 5, loss = 0.73492446\n",
      "Iteration 222, loss = 0.06300020\n",
      "Iteration 6, loss = 0.67231631\n",
      "Iteration 223, loss = 0.06288687\n",
      "Iteration 7, loss = 0.62417595\n",
      "Iteration 224, loss = 0.06139810\n",
      "Iteration 8, loss = 0.58834901\n",
      "Iteration 225, loss = 0.05906402\n",
      "Iteration 9, loss = 0.56008787\n",
      "Iteration 226, loss = 0.06119351\n",
      "Iteration 10, loss = 0.53410118\n",
      "Iteration 227, loss = 0.05957472\n",
      "Iteration 11, loss = 0.51222095\n",
      "Iteration 228, loss = 0.05868007\n",
      "Iteration 12, loss = 0.49326415\n",
      "Iteration 229, loss = 0.05978780\n",
      "Iteration 13, loss = 0.47583895\n",
      "Iteration 230, loss = 0.05966659\n",
      "Iteration 14, loss = 0.46080988\n",
      "Iteration 231, loss = 0.05824948\n",
      "Iteration 15, loss = 0.44722479\n",
      "Iteration 232, loss = 0.05665775\n",
      "Iteration 16, loss = 0.43487279\n",
      "Iteration 233, loss = 0.05797971\n",
      "Iteration 17, loss = 0.42373263\n",
      "Iteration 234, loss = 0.05543861\n",
      "Iteration 18, loss = 0.41346376\n",
      "Iteration 235, loss = 0.05553787\n",
      "Iteration 19, loss = 0.40324564\n",
      "Iteration 236, loss = 0.05480905\n",
      "Iteration 20, loss = 0.39429358\n",
      "Iteration 237, loss = 0.05606232\n",
      "Iteration 21, loss = 0.38580354\n",
      "Iteration 238, loss = 0.05402806\n",
      "Iteration 22, loss = 0.37802981\n",
      "Iteration 239, loss = 0.05373445\n",
      "Iteration 23, loss = 0.37110760\n",
      "Iteration 240, loss = 0.05316104\n",
      "Iteration 24, loss = 0.36410480\n",
      "Iteration 241, loss = 0.05377400\n",
      "Iteration 25, loss = 0.35791932\n",
      "Iteration 242, loss = 0.05326016\n",
      "Iteration 26, loss = 0.35081340\n",
      "Iteration 243, loss = 0.05464516\n",
      "Iteration 27, loss = 0.34500895\n",
      "Iteration 244, loss = 0.05502880\n",
      "Iteration 28, loss = 0.33959705\n",
      "Iteration 245, loss = 0.05369225\n",
      "Iteration 29, loss = 0.33354087\n",
      "Iteration 246, loss = 0.05173147\n",
      "Iteration 30, loss = 0.32875388\n",
      "Iteration 247, loss = 0.05136579\n",
      "Iteration 31, loss = 0.32334948\n",
      "Iteration 248, loss = 0.05327972\n",
      "Iteration 32, loss = 0.31838975\n",
      "Iteration 249, loss = 0.05121053\n",
      "Iteration 33, loss = 0.31342107\n",
      "Iteration 250, loss = 0.05053539\n",
      "Iteration 34, loss = 0.30923089\n",
      "Iteration 251, loss = 0.04997470\n",
      "Iteration 35, loss = 0.30400857\n",
      "Iteration 252, loss = 0.05044455\n",
      "Iteration 36, loss = 0.30048088\n",
      "Iteration 253, loss = 0.05043894\n",
      "Iteration 37, loss = 0.29556569\n",
      "Iteration 254, loss = 0.04902038\n",
      "Iteration 38, loss = 0.29076231\n",
      "Iteration 255, loss = 0.04966700\n",
      "Iteration 39, loss = 0.28691005\n",
      "Iteration 256, loss = 0.04871917\n",
      "Iteration 40, loss = 0.28236803\n",
      "Iteration 257, loss = 0.04885393\n",
      "Iteration 41, loss = 0.27836762\n",
      "Iteration 258, loss = 0.04925871\n",
      "Iteration 42, loss = 0.27489263\n",
      "Iteration 259, loss = 0.04795101\n",
      "Iteration 43, loss = 0.27138520\n",
      "Iteration 260, loss = 0.04781367\n",
      "Iteration 44, loss = 0.26628298\n",
      "Iteration 261, loss = 0.04757880\n",
      "Iteration 45, loss = 0.26372906\n",
      "Iteration 262, loss = 0.04684802\n",
      "Iteration 46, loss = 0.25980243\n",
      "Iteration 47, loss = 0.25723513\n",
      "Iteration 263, loss = 0.04780992\n",
      "Iteration 48, loss = 0.25308733\n",
      "Iteration 264, loss = 0.04748931\n",
      "Iteration 49, loss = 0.24989732\n",
      "Iteration 265, loss = 0.04669434\n",
      "Iteration 50, loss = 0.24531369\n",
      "Iteration 266, loss = 0.04791904\n",
      "Iteration 51, loss = 0.24310606\n",
      "Iteration 267, loss = 0.04705576\n",
      "Iteration 52, loss = 0.23859611\n",
      "Iteration 268, loss = 0.04579108\n",
      "Iteration 53, loss = 0.23666932\n",
      "Iteration 269, loss = 0.04514239\n",
      "Iteration 54, loss = 0.23228421\n",
      "Iteration 270, loss = 0.04641868\n",
      "Iteration 55, loss = 0.23093670\n",
      "Iteration 271, loss = 0.04561317\n",
      "Iteration 56, loss = 0.22647067\n",
      "Iteration 272, loss = 0.04388044\n",
      "Iteration 57, loss = 0.22454062\n",
      "Iteration 273, loss = 0.04343758\n",
      "Iteration 58, loss = 0.22100314\n",
      "Iteration 274, loss = 0.04415493\n",
      "Iteration 275, loss = 0.04315547\n",
      "Iteration 59, loss = 0.21806746\n",
      "Iteration 276, loss = 0.04343466\n",
      "Iteration 60, loss = 0.21503044\n",
      "Iteration 277, loss = 0.04255542\n",
      "Iteration 61, loss = 0.21342374\n",
      "Iteration 278, loss = 0.04281989\n",
      "Iteration 62, loss = 0.21095837\n",
      "Iteration 63, loss = 0.20850918\n",
      "Iteration 279, loss = 0.04374867\n",
      "Iteration 64, loss = 0.20540384\n",
      "Iteration 280, loss = 0.04277503\n",
      "Iteration 65, loss = 0.20344153\n",
      "Iteration 281, loss = 0.04200062\n",
      "Iteration 66, loss = 0.20140445\n",
      "Iteration 282, loss = 0.04308271\n",
      "Iteration 67, loss = 0.19857585\n",
      "Iteration 283, loss = 0.04498234\n",
      "Iteration 68, loss = 0.19693775\n",
      "Iteration 284, loss = 0.04312171\n",
      "Iteration 69, loss = 0.19366371\n",
      "Iteration 285, loss = 0.04805148\n",
      "Iteration 70, loss = 0.19091222\n",
      "Iteration 286, loss = 0.04627840\n",
      "Iteration 71, loss = 0.18916319\n",
      "Iteration 287, loss = 0.04533341\n",
      "Iteration 72, loss = 0.18668803\n",
      "Iteration 288, loss = 0.04482664\n",
      "Iteration 73, loss = 0.18464431\n",
      "Iteration 289, loss = 0.04302970\n",
      "Iteration 74, loss = 0.18278996\n",
      "Iteration 290, loss = 0.04020191\n",
      "Iteration 75, loss = 0.18040950\n",
      "Iteration 291, loss = 0.04086247\n",
      "Iteration 76, loss = 0.17915964\n",
      "Iteration 292, loss = 0.04111944\n",
      "Iteration 77, loss = 0.17677128\n",
      "Iteration 293, loss = 0.04051645\n",
      "Iteration 78, loss = 0.17553707\n",
      "Iteration 294, loss = 0.03875882\n",
      "Iteration 79, loss = 0.17278115\n",
      "Iteration 295, loss = 0.03970666\n",
      "Iteration 80, loss = 0.17194529\n",
      "Iteration 296, loss = 0.04226209\n",
      "Iteration 81, loss = 0.16812649\n",
      "Iteration 297, loss = 0.03843479\n",
      "Iteration 82, loss = 0.17029355\n",
      "Iteration 298, loss = 0.04091783\n",
      "Iteration 83, loss = 0.16534188\n",
      "Iteration 299, loss = 0.03849119\n",
      "Iteration 84, loss = 0.16405984\n",
      "Iteration 300, loss = 0.03919699\n",
      "Iteration 85, loss = 0.16267775\n",
      "Iteration 301, loss = 0.04204839\n",
      "Iteration 86, loss = 0.16003412\n",
      "Iteration 302, loss = 0.04119221\n",
      "Iteration 87, loss = 0.15902126\n",
      "Iteration 303, loss = 0.03733452\n",
      "Iteration 88, loss = 0.15818854\n",
      "Iteration 304, loss = 0.04461127\n",
      "Iteration 89, loss = 0.15597032\n",
      "Iteration 305, loss = 0.04292325\n",
      "Iteration 90, loss = 0.15503355\n",
      "Iteration 306, loss = 0.03938059\n",
      "Iteration 91, loss = 0.15260277\n",
      "Iteration 307, loss = 0.03941160\n",
      "Iteration 92, loss = 0.15055707\n",
      "Iteration 308, loss = 0.03908461\n",
      "Iteration 93, loss = 0.14940316\n",
      "Iteration 309, loss = 0.03765136\n",
      "Iteration 94, loss = 0.14798112\n",
      "Iteration 310, loss = 0.03558638\n",
      "Iteration 95, loss = 0.14572358\n",
      "Iteration 311, loss = 0.03596252\n",
      "Iteration 96, loss = 0.14522494\n",
      "Iteration 312, loss = 0.03634892\n",
      "Iteration 97, loss = 0.14351423\n",
      "Iteration 313, loss = 0.03580319\n",
      "Iteration 98, loss = 0.14238039\n",
      "Iteration 314, loss = 0.03674721\n",
      "Iteration 99, loss = 0.14085158\n",
      "Iteration 315, loss = 0.03519573\n",
      "Iteration 100, loss = 0.14000036\n",
      "Iteration 316, loss = 0.03494909\n",
      "Iteration 101, loss = 0.13754744\n",
      "Iteration 317, loss = 0.03504136\n",
      "Iteration 102, loss = 0.13705652\n",
      "Iteration 318, loss = 0.03421834\n",
      "Iteration 103, loss = 0.13485475\n",
      "Iteration 319, loss = 0.03657030\n",
      "Iteration 104, loss = 0.13438347\n",
      "Iteration 320, loss = 0.03817215\n",
      "Iteration 105, loss = 0.13259969\n",
      "Iteration 321, loss = 0.03684810\n",
      "Iteration 106, loss = 0.13131921\n",
      "Iteration 322, loss = 0.03378544\n",
      "Iteration 107, loss = 0.13024699\n",
      "Iteration 323, loss = 0.03570079\n",
      "Iteration 108, loss = 0.12907866\n",
      "Iteration 324, loss = 0.03358946\n",
      "Iteration 109, loss = 0.12910699\n",
      "Iteration 325, loss = 0.03448172\n",
      "Iteration 110, loss = 0.12615282\n",
      "Iteration 326, loss = 0.03332768\n",
      "Iteration 111, loss = 0.12678727\n",
      "Iteration 327, loss = 0.03288892\n",
      "Iteration 112, loss = 0.12399184\n",
      "Iteration 328, loss = 0.03313776\n",
      "Iteration 113, loss = 0.12410084\n",
      "Iteration 329, loss = 0.03300574\n",
      "Iteration 114, loss = 0.12157461\n",
      "Iteration 330, loss = 0.03313581\n",
      "Iteration 115, loss = 0.12135738\n",
      "Iteration 331, loss = 0.03211276\n",
      "Iteration 116, loss = 0.12006914\n",
      "Iteration 332, loss = 0.03311459\n",
      "Iteration 117, loss = 0.11890583\n",
      "Iteration 333, loss = 0.03403256\n",
      "Iteration 118, loss = 0.11813758\n",
      "Iteration 334, loss = 0.03337990\n",
      "Iteration 119, loss = 0.11633204\n",
      "Iteration 335, loss = 0.03461204\n",
      "Iteration 120, loss = 0.11699104\n",
      "Iteration 336, loss = 0.03286745\n",
      "Iteration 121, loss = 0.11458379\n",
      "Iteration 337, loss = 0.03204312\n",
      "Iteration 122, loss = 0.11413464\n",
      "Iteration 338, loss = 0.03303520\n",
      "Iteration 123, loss = 0.11283516\n",
      "Iteration 339, loss = 0.03534798\n",
      "Iteration 124, loss = 0.11237741\n",
      "Iteration 340, loss = 0.03218548\n",
      "Iteration 125, loss = 0.11073512\n",
      "Iteration 341, loss = 0.03102139\n",
      "Iteration 126, loss = 0.10994071\n",
      "Iteration 342, loss = 0.03258126\n",
      "Iteration 127, loss = 0.10906792\n",
      "Iteration 343, loss = 0.03264642\n",
      "Iteration 128, loss = 0.10792885\n",
      "Iteration 344, loss = 0.03278450\n",
      "Iteration 129, loss = 0.10831414\n",
      "Iteration 345, loss = 0.03089502\n",
      "Iteration 130, loss = 0.10637117\n",
      "Iteration 346, loss = 0.03030654\n",
      "Iteration 131, loss = 0.10571492\n",
      "Iteration 347, loss = 0.03130937\n",
      "Iteration 132, loss = 0.10517082\n",
      "Iteration 348, loss = 0.03219960\n",
      "Iteration 133, loss = 0.10433461\n",
      "Iteration 349, loss = 0.03060592\n",
      "Iteration 134, loss = 0.10244483\n",
      "Iteration 350, loss = 0.03005970\n",
      "Iteration 135, loss = 0.10355029\n",
      "Iteration 351, loss = 0.02970804\n",
      "Iteration 136, loss = 0.10204560\n",
      "Iteration 352, loss = 0.02992465\n",
      "Iteration 137, loss = 0.10063629\n",
      "Iteration 353, loss = 0.02986871\n",
      "Iteration 138, loss = 0.10059848\n",
      "Iteration 354, loss = 0.03018998\n",
      "Iteration 139, loss = 0.09892985\n",
      "Iteration 355, loss = 0.02894669\n",
      "Iteration 140, loss = 0.09814847\n",
      "Iteration 356, loss = 0.02917690\n",
      "Iteration 141, loss = 0.09806652\n",
      "Iteration 357, loss = 0.02867949\n",
      "Iteration 142, loss = 0.09649690\n",
      "Iteration 358, loss = 0.02895042\n",
      "Iteration 143, loss = 0.09655219\n",
      "Iteration 359, loss = 0.02829729\n",
      "Iteration 144, loss = 0.09547818\n",
      "Iteration 360, loss = 0.02857873\n",
      "Iteration 145, loss = 0.09373091\n",
      "Iteration 361, loss = 0.02833159\n",
      "Iteration 146, loss = 0.09427285\n",
      "Iteration 362, loss = 0.03005372\n",
      "Iteration 147, loss = 0.09433497\n",
      "Iteration 363, loss = 0.02839277\n",
      "Iteration 148, loss = 0.09357716\n",
      "Iteration 364, loss = 0.02821395\n",
      "Iteration 149, loss = 0.09239937\n",
      "Iteration 365, loss = 0.02861183\n",
      "Iteration 150, loss = 0.09277070\n",
      "Iteration 366, loss = 0.02806605\n",
      "Iteration 151, loss = 0.09069829\n",
      "Iteration 367, loss = 0.02898831\n",
      "Iteration 152, loss = 0.08912727\n",
      "Iteration 368, loss = 0.03125593\n",
      "Iteration 153, loss = 0.08892837\n",
      "Iteration 369, loss = 0.02943255\n",
      "Iteration 154, loss = 0.08900603\n",
      "Iteration 370, loss = 0.02886882\n",
      "Iteration 155, loss = 0.08721135\n",
      "Iteration 371, loss = 0.02784557\n",
      "Iteration 156, loss = 0.08882883\n",
      "Iteration 372, loss = 0.02939520\n",
      "Iteration 157, loss = 0.08703803\n",
      "Iteration 373, loss = 0.02836213\n",
      "Iteration 158, loss = 0.08724796\n",
      "Iteration 374, loss = 0.02781128\n",
      "Iteration 159, loss = 0.08470375\n",
      "Iteration 375, loss = 0.02708927\n",
      "Iteration 160, loss = 0.08373208\n",
      "Iteration 376, loss = 0.02804637\n",
      "Iteration 161, loss = 0.08362315\n",
      "Iteration 377, loss = 0.02830777\n",
      "Iteration 162, loss = 0.08266552\n",
      "Iteration 378, loss = 0.02843712\n",
      "Iteration 163, loss = 0.08166959\n",
      "Iteration 379, loss = 0.02860135\n",
      "Iteration 164, loss = 0.08205041\n",
      "Iteration 380, loss = 0.02594990\n",
      "Iteration 165, loss = 0.08203814\n",
      "Iteration 381, loss = 0.02917769\n",
      "Iteration 166, loss = 0.07972735\n",
      "Iteration 382, loss = 0.02792552\n",
      "Iteration 167, loss = 0.07947199\n",
      "Iteration 168, loss = 0.07978281\n",
      "Iteration 383, loss = 0.02623482\n",
      "Iteration 169, loss = 0.07862102\n",
      "Iteration 384, loss = 0.02823108\n",
      "Iteration 170, loss = 0.07736958\n",
      "Iteration 385, loss = 0.02517591\n",
      "Iteration 171, loss = 0.07672711\n",
      "Iteration 386, loss = 0.02649757\n",
      "Iteration 172, loss = 0.07693668\n",
      "Iteration 387, loss = 0.02679160\n",
      "Iteration 173, loss = 0.07585380\n",
      "Iteration 388, loss = 0.02579994\n",
      "Iteration 174, loss = 0.07521258\n",
      "Iteration 389, loss = 0.02489622\n",
      "Iteration 175, loss = 0.07491974\n",
      "Iteration 390, loss = 0.02599191\n",
      "Iteration 176, loss = 0.07440089\n",
      "Iteration 391, loss = 0.02544518\n",
      "Iteration 177, loss = 0.07397702\n",
      "Iteration 392, loss = 0.02513598\n",
      "Iteration 178, loss = 0.07341967\n",
      "Iteration 393, loss = 0.02629815\n",
      "Iteration 179, loss = 0.07299973\n",
      "Iteration 394, loss = 0.02468344\n",
      "Iteration 180, loss = 0.07137267\n",
      "Iteration 395, loss = 0.02454875\n",
      "Iteration 181, loss = 0.07121680\n",
      "Iteration 396, loss = 0.02459512\n",
      "Iteration 182, loss = 0.06975952\n",
      "Iteration 397, loss = 0.02512266\n",
      "Iteration 183, loss = 0.06995501\n",
      "Iteration 398, loss = 0.02415487\n",
      "Iteration 184, loss = 0.07062121\n",
      "Iteration 399, loss = 0.02404788\n",
      "Iteration 185, loss = 0.06855727\n",
      "Iteration 400, loss = 0.02410312\n",
      "Iteration 186, loss = 0.06842434\n",
      "Iteration 401, loss = 0.02466069\n",
      "Iteration 187, loss = 0.06854599\n",
      "Iteration 402, loss = 0.02434483\n",
      "Iteration 188, loss = 0.06698828\n",
      "Iteration 403, loss = 0.02352134\n",
      "Iteration 189, loss = 0.06692543\n",
      "Iteration 404, loss = 0.02426967\n",
      "Iteration 190, loss = 0.06639287\n",
      "Iteration 405, loss = 0.02360984\n",
      "Iteration 191, loss = 0.06602346\n",
      "Iteration 406, loss = 0.02371103\n",
      "Iteration 192, loss = 0.06567084\n",
      "Iteration 407, loss = 0.02327737\n",
      "Iteration 193, loss = 0.06551601\n",
      "Iteration 408, loss = 0.02425774\n",
      "Iteration 194, loss = 0.06680970\n",
      "Iteration 409, loss = 0.02438541\n",
      "Iteration 195, loss = 0.06582999\n",
      "Iteration 410, loss = 0.02469154\n",
      "Iteration 196, loss = 0.06453752\n",
      "Iteration 411, loss = 0.02276225\n",
      "Iteration 197, loss = 0.06470956\n",
      "Iteration 412, loss = 0.02384579\n",
      "Iteration 198, loss = 0.06292962\n",
      "Iteration 413, loss = 0.02359443\n",
      "Iteration 199, loss = 0.06394253\n",
      "Iteration 414, loss = 0.02305653\n",
      "Iteration 200, loss = 0.06182319\n",
      "Iteration 415, loss = 0.02203289\n",
      "Iteration 201, loss = 0.06153719\n",
      "Iteration 416, loss = 0.02278669\n",
      "Iteration 202, loss = 0.06081158\n",
      "Iteration 417, loss = 0.02226205\n",
      "Iteration 203, loss = 0.06073946\n",
      "Iteration 418, loss = 0.02240435\n",
      "Iteration 204, loss = 0.06053419\n",
      "Iteration 419, loss = 0.02233095\n",
      "Iteration 205, loss = 0.06095582\n",
      "Iteration 420, loss = 0.02237813\n",
      "Iteration 206, loss = 0.05891239\n",
      "Iteration 421, loss = 0.02224798\n",
      "Iteration 207, loss = 0.05971774\n",
      "Iteration 422, loss = 0.02222467\n",
      "Iteration 208, loss = 0.05819442\n",
      "Iteration 423, loss = 0.02222286\n",
      "Iteration 209, loss = 0.05773080\n",
      "Iteration 424, loss = 0.02165209\n",
      "Iteration 210, loss = 0.05714928\n",
      "Iteration 425, loss = 0.02126452\n",
      "Iteration 211, loss = 0.05737113\n",
      "Iteration 426, loss = 0.02172349\n",
      "Iteration 212, loss = 0.05626328\n",
      "Iteration 427, loss = 0.02154585\n",
      "Iteration 213, loss = 0.05614649\n",
      "Iteration 428, loss = 0.02246194\n",
      "Iteration 214, loss = 0.05579511\n",
      "Iteration 429, loss = 0.02326418\n",
      "Iteration 215, loss = 0.05526896\n",
      "Iteration 430, loss = 0.02602285\n",
      "Iteration 216, loss = 0.05545256\n",
      "Iteration 431, loss = 0.02211737\n",
      "Iteration 217, loss = 0.05563746\n",
      "Iteration 432, loss = 0.02202013\n",
      "Iteration 218, loss = 0.05357118\n",
      "Iteration 433, loss = 0.02115198\n",
      "Iteration 219, loss = 0.05507402\n",
      "Iteration 434, loss = 0.02165976\n",
      "Iteration 220, loss = 0.05344169\n",
      "Iteration 435, loss = 0.02053422\n",
      "Iteration 221, loss = 0.05275194\n",
      "Iteration 436, loss = 0.02179452\n",
      "Iteration 222, loss = 0.05249013\n",
      "Iteration 437, loss = 0.02129363\n",
      "Iteration 223, loss = 0.05193269\n",
      "Iteration 438, loss = 0.02117298\n",
      "Iteration 224, loss = 0.05196426\n",
      "Iteration 439, loss = 0.02254525\n",
      "Iteration 225, loss = 0.05175342\n",
      "Iteration 440, loss = 0.01984629\n",
      "Iteration 226, loss = 0.05118866\n",
      "Iteration 441, loss = 0.02013809\n",
      "Iteration 227, loss = 0.05170274\n",
      "Iteration 442, loss = 0.02175224\n",
      "Iteration 228, loss = 0.04996606\n",
      "Iteration 443, loss = 0.02017640\n",
      "Iteration 229, loss = 0.05047941\n",
      "Iteration 444, loss = 0.02023603\n",
      "Iteration 230, loss = 0.05141251\n",
      "Iteration 445, loss = 0.01988723\n",
      "Iteration 231, loss = 0.05062506\n",
      "Iteration 446, loss = 0.02014905\n",
      "Iteration 232, loss = 0.05212661\n",
      "Iteration 447, loss = 0.02536897\n",
      "Iteration 233, loss = 0.05140894\n",
      "Iteration 448, loss = 0.02512246\n",
      "Iteration 234, loss = 0.05008173\n",
      "Iteration 449, loss = 0.02034164\n",
      "Iteration 235, loss = 0.04794763\n",
      "Iteration 450, loss = 0.02003791\n",
      "Iteration 236, loss = 0.04830503\n",
      "Iteration 451, loss = 0.02195021\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 237, loss = 0.04794681\n",
      "Iteration 238, loss = 0.05131973\n",
      "Iteration 239, loss = 0.05612845\n",
      "Iteration 240, loss = 0.05521519\n",
      "Iteration 241, loss = 0.05303643\n",
      "Iteration 242, loss = 0.04787463\n",
      "Iteration 243, loss = 0.04894596\n",
      "Iteration 244, loss = 0.04673241\n",
      "Iteration 245, loss = 0.04815966\n",
      "Iteration 246, loss = 0.04631766\n",
      "Iteration 247, loss = 0.04585422\n",
      "Iteration 248, loss = 0.04497312\n",
      "Iteration 249, loss = 0.04592835\n",
      "Iteration 250, loss = 0.04679126\n",
      "Iteration 251, loss = 0.04326806\n",
      "Iteration 252, loss = 0.04397000\n",
      "Iteration 253, loss = 0.04586573\n",
      "Iteration 254, loss = 0.04198369\n",
      "Iteration 255, loss = 0.04285973\n",
      "Iteration 1, loss = 17.28262482\n",
      "Iteration 256, loss = 0.04839823\n",
      "Iteration 2, loss = 20.53552488\n",
      "Iteration 257, loss = 0.04851871\n",
      "Iteration 3, loss = 9.98698942\n",
      "Iteration 258, loss = 0.04701710\n",
      "Iteration 4, loss = 9.84324819\n",
      "Iteration 259, loss = 0.04537179\n",
      "Iteration 5, loss = 6.48045579\n",
      "Iteration 260, loss = 0.04253399\n",
      "Iteration 6, loss = 9.48108227\n",
      "Iteration 261, loss = 0.04186566\n",
      "Iteration 7, loss = 8.60663693\n",
      "Iteration 262, loss = 0.04108517\n",
      "Iteration 8, loss = 8.09274992\n",
      "Iteration 263, loss = 0.04062756\n",
      "Iteration 9, loss = 5.85118485\n",
      "Iteration 264, loss = 0.04102406\n",
      "Iteration 10, loss = 5.71939445\n",
      "Iteration 265, loss = 0.03968821\n",
      "Iteration 11, loss = 6.59110984\n",
      "Iteration 266, loss = 0.03992806\n",
      "Iteration 12, loss = 4.88927437\n",
      "Iteration 267, loss = 0.03923167\n",
      "Iteration 13, loss = 5.34242011\n",
      "Iteration 268, loss = 0.03904402\n",
      "Iteration 14, loss = 4.87405035\n",
      "Iteration 269, loss = 0.03899561\n",
      "Iteration 15, loss = 5.60249160\n",
      "Iteration 270, loss = 0.03823976\n",
      "Iteration 16, loss = 4.11739355\n",
      "Iteration 271, loss = 0.03808330\n",
      "Iteration 17, loss = 3.06957695\n",
      "Iteration 272, loss = 0.03833600\n",
      "Iteration 18, loss = 3.06914493\n",
      "Iteration 19, loss = 3.31198691\n",
      "Iteration 273, loss = 0.03867009\n",
      "Iteration 274, loss = 0.03725693\n",
      "Iteration 20, loss = 2.12924670\n",
      "Iteration 21, loss = 1.66133007\n",
      "Iteration 275, loss = 0.03811004\n",
      "Iteration 22, loss = 2.65327580\n",
      "Iteration 276, loss = 0.03790951\n",
      "Iteration 23, loss = 3.42897818\n",
      "Iteration 277, loss = 0.03866861\n",
      "Iteration 24, loss = 2.33052819\n",
      "Iteration 278, loss = 0.03776395\n",
      "Iteration 279, loss = 0.03608247\n",
      "Iteration 25, loss = 1.88838178\n",
      "Iteration 280, loss = 0.03796779\n",
      "Iteration 26, loss = 2.32161325\n",
      "Iteration 281, loss = 0.03620032\n",
      "Iteration 27, loss = 1.96019979\n",
      "Iteration 282, loss = 0.03550147\n",
      "Iteration 28, loss = 2.03497876\n",
      "Iteration 283, loss = 0.03514872\n",
      "Iteration 29, loss = 1.28578440\n",
      "Iteration 284, loss = 0.03596878\n",
      "Iteration 30, loss = 1.53465883\n",
      "Iteration 285, loss = 0.03558626\n",
      "Iteration 31, loss = 1.18625624\n",
      "Iteration 286, loss = 0.03549555\n",
      "Iteration 32, loss = 0.77304399\n",
      "Iteration 287, loss = 0.03481614\n",
      "Iteration 33, loss = 0.71159811\n",
      "Iteration 288, loss = 0.03455598\n",
      "Iteration 34, loss = 0.90358861\n",
      "Iteration 289, loss = 0.03394297\n",
      "Iteration 35, loss = 2.29417722\n",
      "Iteration 290, loss = 0.03473193\n",
      "Iteration 36, loss = 1.81415631\n",
      "Iteration 291, loss = 0.03405207\n",
      "Iteration 37, loss = 0.89719040\n",
      "Iteration 292, loss = 0.03423027\n",
      "Iteration 38, loss = 1.27387907\n",
      "Iteration 293, loss = 0.03547219\n",
      "Iteration 39, loss = 0.72789594\n",
      "Iteration 294, loss = 0.03421516\n",
      "Iteration 40, loss = 0.82128341\n",
      "Iteration 41, loss = 1.34033067\n",
      "Iteration 295, loss = 0.03416992\n",
      "Iteration 42, loss = 1.59429709\n",
      "Iteration 296, loss = 0.03254333\n",
      "Iteration 43, loss = 1.08944971\n",
      "Iteration 297, loss = 0.03355117\n",
      "Iteration 44, loss = 0.92932455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 298, loss = 0.03404885\n",
      "Iteration 299, loss = 0.03171474\n",
      "Iteration 300, loss = 0.03278548\n",
      "Iteration 301, loss = 0.03195365\n",
      "Iteration 302, loss = 0.03140420\n",
      "Iteration 303, loss = 0.03133732\n",
      "Iteration 304, loss = 0.03153661\n",
      "Iteration 305, loss = 0.03136015\n",
      "Iteration 306, loss = 0.03103027\n",
      "Iteration 307, loss = 0.03076814\n",
      "Iteration 308, loss = 0.03362408\n",
      "Iteration 309, loss = 0.03204544\n",
      "Iteration 310, loss = 0.03177882\n",
      "Iteration 311, loss = 0.03164446\n",
      "Iteration 312, loss = 0.02988686\n",
      "Iteration 313, loss = 0.02972664\n",
      "Iteration 314, loss = 0.03022521\n",
      "Iteration 315, loss = 0.03094858\n",
      "Iteration 316, loss = 0.02920557\n",
      "Iteration 1, loss = 15.99474433\n",
      "Iteration 317, loss = 0.02897108\n",
      "Iteration 2, loss = 14.94434964\n",
      "Iteration 318, loss = 0.02829015\n",
      "Iteration 3, loss = 11.18876974\n",
      "Iteration 319, loss = 0.02906740\n",
      "Iteration 4, loss = 9.06722616\n",
      "Iteration 320, loss = 0.02844517\n",
      "Iteration 5, loss = 7.42143636\n",
      "Iteration 321, loss = 0.02940830\n",
      "Iteration 6, loss = 9.10582356\n",
      "Iteration 322, loss = 0.02784882\n",
      "Iteration 7, loss = 10.83813525\n",
      "Iteration 323, loss = 0.02825116\n",
      "Iteration 8, loss = 7.05215811\n",
      "Iteration 324, loss = 0.02921897\n",
      "Iteration 9, loss = 6.67609243\n",
      "Iteration 325, loss = 0.02911164\n",
      "Iteration 10, loss = 5.23659833\n",
      "Iteration 326, loss = 0.03130137\n",
      "Iteration 11, loss = 8.56291857\n",
      "Iteration 327, loss = 0.03099951\n",
      "Iteration 12, loss = 6.90323264\n",
      "Iteration 328, loss = 0.02636223\n",
      "Iteration 13, loss = 7.45932488\n",
      "Iteration 329, loss = 0.02889417\n",
      "Iteration 14, loss = 5.88361905\n",
      "Iteration 330, loss = 0.02900487\n",
      "Iteration 15, loss = 4.53431493\n",
      "Iteration 331, loss = 0.02792420\n",
      "Iteration 16, loss = 3.78714027\n",
      "Iteration 17, loss = 3.11975221\n",
      "Iteration 332, loss = 0.02792743\n",
      "Iteration 18, loss = 2.41330531\n",
      "Iteration 333, loss = 0.02544024\n",
      "Iteration 19, loss = 2.83611781\n",
      "Iteration 334, loss = 0.02786181\n",
      "Iteration 20, loss = 3.17024710\n",
      "Iteration 335, loss = 0.02700237\n",
      "Iteration 21, loss = 3.31264102\n",
      "Iteration 336, loss = 0.02567536\n",
      "Iteration 22, loss = 3.43884623\n",
      "Iteration 337, loss = 0.02634020\n",
      "Iteration 23, loss = 2.92540505\n",
      "Iteration 338, loss = 0.02600507\n",
      "Iteration 24, loss = 2.03284839\n",
      "Iteration 339, loss = 0.02508020\n",
      "Iteration 25, loss = 1.95478832\n",
      "Iteration 340, loss = 0.02582054\n",
      "Iteration 26, loss = 1.95925886\n",
      "Iteration 341, loss = 0.02622897\n",
      "Iteration 27, loss = 2.21075435\n",
      "Iteration 342, loss = 0.02486249\n",
      "Iteration 28, loss = 2.16157018\n",
      "Iteration 343, loss = 0.02509041\n",
      "Iteration 29, loss = 2.80978623\n",
      "Iteration 344, loss = 0.02570258\n",
      "Iteration 30, loss = 2.52864953\n",
      "Iteration 345, loss = 0.02482744\n",
      "Iteration 31, loss = 2.65561743\n",
      "Iteration 346, loss = 0.02466572\n",
      "Iteration 32, loss = 2.13491644\n",
      "Iteration 347, loss = 0.02465711\n",
      "Iteration 33, loss = 1.54253772\n",
      "Iteration 348, loss = 0.02439649\n",
      "Iteration 34, loss = 1.87993200\n",
      "Iteration 349, loss = 0.02486476\n",
      "Iteration 35, loss = 2.07826871\n",
      "Iteration 350, loss = 0.02444626\n",
      "Iteration 36, loss = 1.51105153\n",
      "Iteration 351, loss = 0.02361130\n",
      "Iteration 37, loss = 1.84230459\n",
      "Iteration 352, loss = 0.02440221\n",
      "Iteration 38, loss = 1.48709826\n",
      "Iteration 353, loss = 0.02366171\n",
      "Iteration 39, loss = 1.35879479\n",
      "Iteration 354, loss = 0.02353783\n",
      "Iteration 40, loss = 1.19217851\n",
      "Iteration 355, loss = 0.02402974\n",
      "Iteration 41, loss = 1.82826589\n",
      "Iteration 356, loss = 0.02530152\n",
      "Iteration 42, loss = 3.25078777\n",
      "Iteration 357, loss = 0.02307245\n",
      "Iteration 43, loss = 2.90478309\n",
      "Iteration 358, loss = 0.02508909\n",
      "Iteration 44, loss = 2.53319674\n",
      "Iteration 359, loss = 0.02276691\n",
      "Iteration 45, loss = 2.18502207\n",
      "Iteration 360, loss = 0.02393388\n",
      "Iteration 46, loss = 2.17734309\n",
      "Iteration 361, loss = 0.02224764\n",
      "Iteration 47, loss = 2.07733269\n",
      "Iteration 362, loss = 0.02292541\n",
      "Iteration 48, loss = 1.92502095\n",
      "Iteration 363, loss = 0.02212811\n",
      "Iteration 49, loss = 1.09382567\n",
      "Iteration 364, loss = 0.02260392\n",
      "Iteration 50, loss = 1.37216282\n",
      "Iteration 365, loss = 0.02239131\n",
      "Iteration 51, loss = 1.43270492\n",
      "Iteration 366, loss = 0.02192925\n",
      "Iteration 52, loss = 1.06926091\n",
      "Iteration 367, loss = 0.02311913\n",
      "Iteration 53, loss = 1.18455820\n",
      "Iteration 368, loss = 0.02268570\n",
      "Iteration 54, loss = 0.75847367\n",
      "Iteration 369, loss = 0.02152304\n",
      "Iteration 55, loss = 0.65954827\n",
      "Iteration 370, loss = 0.02363656\n",
      "Iteration 56, loss = 0.62150066\n",
      "Iteration 371, loss = 0.02191672\n",
      "Iteration 57, loss = 0.65903952\n",
      "Iteration 372, loss = 0.02147454\n",
      "Iteration 58, loss = 0.55942175\n",
      "Iteration 373, loss = 0.02072956\n",
      "Iteration 59, loss = 0.76212901\n",
      "Iteration 374, loss = 0.02124533\n",
      "Iteration 60, loss = 1.95144370\n",
      "Iteration 375, loss = 0.02121525\n",
      "Iteration 61, loss = 2.08834584\n",
      "Iteration 376, loss = 0.02065055\n",
      "Iteration 62, loss = 1.85709352\n",
      "Iteration 377, loss = 0.02054980\n",
      "Iteration 63, loss = 1.29706188\n",
      "Iteration 378, loss = 0.02036889\n",
      "Iteration 64, loss = 1.31519097\n",
      "Iteration 379, loss = 0.02098286\n",
      "Iteration 65, loss = 1.37200443\n",
      "Iteration 380, loss = 0.02064046\n",
      "Iteration 66, loss = 1.30068408\n",
      "Iteration 381, loss = 0.02223591\n",
      "Iteration 67, loss = 0.91309043\n",
      "Iteration 382, loss = 0.01967628\n",
      "Iteration 68, loss = 0.68841005\n",
      "Iteration 383, loss = 0.02070772\n",
      "Iteration 69, loss = 0.94016980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 384, loss = 0.02115956\n",
      "Iteration 385, loss = 0.02185503\n",
      "Iteration 386, loss = 0.02326046\n",
      "Iteration 387, loss = 0.02055029\n",
      "Iteration 388, loss = 0.01913096\n",
      "Iteration 389, loss = 0.01966989\n",
      "Iteration 390, loss = 0.02021008\n",
      "Iteration 391, loss = 0.01910823\n",
      "Iteration 392, loss = 0.01913755\n",
      "Iteration 393, loss = 0.01891942\n",
      "Iteration 394, loss = 0.01962512\n",
      "Iteration 395, loss = 0.01983136\n",
      "Iteration 396, loss = 0.01904197\n",
      "Iteration 397, loss = 0.01846081\n",
      "Iteration 398, loss = 0.01862376\n",
      "Iteration 399, loss = 0.02029386\n",
      "Iteration 400, loss = 0.01911063\n",
      "Iteration 401, loss = 0.02146645\n",
      "Iteration 1, loss = 16.39658348\n",
      "Iteration 402, loss = 0.01947500\n",
      "Iteration 2, loss = 12.87844498\n",
      "Iteration 403, loss = 0.01897737\n",
      "Iteration 3, loss = 8.85840119\n",
      "Iteration 404, loss = 0.01765158\n",
      "Iteration 4, loss = 8.92947380\n",
      "Iteration 405, loss = 0.01944574\n",
      "Iteration 5, loss = 9.20692924\n",
      "Iteration 406, loss = 0.01872473\n",
      "Iteration 6, loss = 8.59959743\n",
      "Iteration 407, loss = 0.01821735\n",
      "Iteration 7, loss = 8.54395893\n",
      "Iteration 408, loss = 0.01925986\n",
      "Iteration 8, loss = 5.65727191\n",
      "Iteration 409, loss = 0.01791144\n",
      "Iteration 9, loss = 6.32191012\n",
      "Iteration 410, loss = 0.01755802\n",
      "Iteration 10, loss = 6.41670143\n",
      "Iteration 411, loss = 0.01734258\n",
      "Iteration 11, loss = 5.23789949\n",
      "Iteration 412, loss = 0.01730691\n",
      "Iteration 12, loss = 5.24785452\n",
      "Iteration 413, loss = 0.01681515\n",
      "Iteration 13, loss = 4.58852980\n",
      "Iteration 414, loss = 0.01804916\n",
      "Iteration 14, loss = 4.44877145\n",
      "Iteration 415, loss = 0.01947962\n",
      "Iteration 15, loss = 5.02471421\n",
      "Iteration 416, loss = 0.02113006\n",
      "Iteration 16, loss = 4.09711263\n",
      "Iteration 417, loss = 0.01891402\n",
      "Iteration 17, loss = 4.63668635\n",
      "Iteration 418, loss = 0.01696260\n",
      "Iteration 18, loss = 3.97921637\n",
      "Iteration 419, loss = 0.01620872\n",
      "Iteration 19, loss = 3.95901153\n",
      "Iteration 420, loss = 0.01673196\n",
      "Iteration 20, loss = 3.00823491\n",
      "Iteration 421, loss = 0.01631147\n",
      "Iteration 21, loss = 2.61392781\n",
      "Iteration 422, loss = 0.01649997\n",
      "Iteration 22, loss = 2.73675430\n",
      "Iteration 423, loss = 0.01601977\n",
      "Iteration 23, loss = 2.29014239\n",
      "Iteration 424, loss = 0.01637094\n",
      "Iteration 24, loss = 2.28351921\n",
      "Iteration 425, loss = 0.01587859\n",
      "Iteration 25, loss = 1.99455458\n",
      "Iteration 426, loss = 0.01588605\n",
      "Iteration 26, loss = 1.79810216\n",
      "Iteration 427, loss = 0.01586066\n",
      "Iteration 27, loss = 1.78501369\n",
      "Iteration 428, loss = 0.01567705\n",
      "Iteration 28, loss = 1.86676457\n",
      "Iteration 429, loss = 0.01597776\n",
      "Iteration 29, loss = 1.69122426\n",
      "Iteration 430, loss = 0.01516951\n",
      "Iteration 30, loss = 1.36898880\n",
      "Iteration 431, loss = 0.01680418\n",
      "Iteration 31, loss = 1.03993367\n",
      "Iteration 432, loss = 0.01840520\n",
      "Iteration 32, loss = 0.63971126\n",
      "Iteration 433, loss = 0.01856975\n",
      "Iteration 33, loss = 0.85698666\n",
      "Iteration 434, loss = 0.01830029\n",
      "Iteration 34, loss = 1.68544716\n",
      "Iteration 435, loss = 0.01776873\n",
      "Iteration 35, loss = 1.57788893\n",
      "Iteration 436, loss = 0.01842912\n",
      "Iteration 36, loss = 1.06880459\n",
      "Iteration 437, loss = 0.01617870\n",
      "Iteration 37, loss = 0.84659769\n",
      "Iteration 438, loss = 0.01479638\n",
      "Iteration 38, loss = 1.05863100\n",
      "Iteration 439, loss = 0.01494982\n",
      "Iteration 39, loss = 1.25429830\n",
      "Iteration 440, loss = 0.01583374\n",
      "Iteration 40, loss = 0.88237652\n",
      "Iteration 441, loss = 0.01586170\n",
      "Iteration 41, loss = 0.87566443\n",
      "Iteration 442, loss = 0.01759297\n",
      "Iteration 42, loss = 1.11531397\n",
      "Iteration 443, loss = 0.01648748\n",
      "Iteration 43, loss = 0.74031577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 444, loss = 0.01491755\n",
      "Iteration 445, loss = 0.01561973\n",
      "Iteration 446, loss = 0.01992778\n",
      "Iteration 447, loss = 0.02029657\n",
      "Iteration 448, loss = 0.01557984\n",
      "Iteration 449, loss = 0.01509571\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.15360443\n",
      "Iteration 2, loss = 9.65311736\n",
      "Iteration 3, loss = 8.76881475\n",
      "Iteration 4, loss = 10.03512055\n",
      "Iteration 5, loss = 8.77457355\n",
      "Iteration 6, loss = 6.92497449\n",
      "Iteration 1, loss = 16.06685402\n",
      "Iteration 7, loss = 5.82559198\n",
      "Iteration 2, loss = 11.03340028\n",
      "Iteration 8, loss = 5.13529130\n",
      "Iteration 3, loss = 9.74136139\n",
      "Iteration 9, loss = 9.61784782\n",
      "Iteration 4, loss = 10.43901637\n",
      "Iteration 10, loss = 4.77462396\n",
      "Iteration 5, loss = 6.55663808\n",
      "Iteration 11, loss = 4.02657175\n",
      "Iteration 6, loss = 6.04678302\n",
      "Iteration 12, loss = 4.73613886\n",
      "Iteration 7, loss = 5.35222336\n",
      "Iteration 13, loss = 3.36653114\n",
      "Iteration 8, loss = 7.67279414\n",
      "Iteration 14, loss = 7.20817350\n",
      "Iteration 9, loss = 8.41718123\n",
      "Iteration 15, loss = 4.86953532\n",
      "Iteration 10, loss = 4.96813060\n",
      "Iteration 16, loss = 4.97439110\n",
      "Iteration 11, loss = 4.20819877\n",
      "Iteration 17, loss = 4.87385194\n",
      "Iteration 12, loss = 4.25094409\n",
      "Iteration 18, loss = 3.09941015\n",
      "Iteration 13, loss = 4.74540186\n",
      "Iteration 19, loss = 3.76629254\n",
      "Iteration 14, loss = 3.84619094\n",
      "Iteration 20, loss = 3.44956374\n",
      "Iteration 15, loss = 3.91656222\n",
      "Iteration 21, loss = 3.73974757\n",
      "Iteration 16, loss = 3.72526947\n",
      "Iteration 22, loss = 3.78278074\n",
      "Iteration 17, loss = 2.35351402\n",
      "Iteration 23, loss = 3.97430184\n",
      "Iteration 18, loss = 3.45061597\n",
      "Iteration 24, loss = 3.55148780\n",
      "Iteration 19, loss = 4.67761433\n",
      "Iteration 25, loss = 3.90652249\n",
      "Iteration 20, loss = 3.66780983\n",
      "Iteration 26, loss = 2.13225289\n",
      "Iteration 21, loss = 2.01452277\n",
      "Iteration 27, loss = 2.36856353\n",
      "Iteration 22, loss = 2.33700069\n",
      "Iteration 28, loss = 1.92211827\n",
      "Iteration 23, loss = 2.66590638\n",
      "Iteration 29, loss = 1.97941716\n",
      "Iteration 24, loss = 2.99287415\n",
      "Iteration 30, loss = 1.78049610\n",
      "Iteration 25, loss = 3.05409353\n",
      "Iteration 31, loss = 2.50191199\n",
      "Iteration 26, loss = 2.32992809\n",
      "Iteration 32, loss = 1.87171652\n",
      "Iteration 27, loss = 2.20384120\n",
      "Iteration 33, loss = 2.01795756\n",
      "Iteration 28, loss = 1.84598261\n",
      "Iteration 34, loss = 1.92987721\n",
      "Iteration 29, loss = 2.07153606\n",
      "Iteration 35, loss = 1.42202579\n",
      "Iteration 30, loss = 3.06847466\n",
      "Iteration 36, loss = 1.06454783\n",
      "Iteration 31, loss = 1.90109724\n",
      "Iteration 37, loss = 1.18364178\n",
      "Iteration 32, loss = 1.57639372\n",
      "Iteration 38, loss = 1.45779881\n",
      "Iteration 33, loss = 1.33658580\n",
      "Iteration 39, loss = 1.60871477\n",
      "Iteration 34, loss = 1.10708115\n",
      "Iteration 40, loss = 1.34280360\n",
      "Iteration 35, loss = 1.35455555\n",
      "Iteration 41, loss = 0.73262105\n",
      "Iteration 36, loss = 0.93553047\n",
      "Iteration 42, loss = 1.40297349\n",
      "Iteration 37, loss = 0.96331201\n",
      "Iteration 43, loss = 1.22438875\n",
      "Iteration 38, loss = 1.06519254\n",
      "Iteration 44, loss = 1.36596825\n",
      "Iteration 39, loss = 0.70906311\n",
      "Iteration 45, loss = 1.03794149\n",
      "Iteration 40, loss = 1.25978102\n",
      "Iteration 46, loss = 0.76157674\n",
      "Iteration 41, loss = 1.18030942\n",
      "Iteration 47, loss = 0.99310397\n",
      "Iteration 42, loss = 1.03938104\n",
      "Iteration 48, loss = 0.82419954\n",
      "Iteration 43, loss = 1.03486564\n",
      "Iteration 49, loss = 1.04663351\n",
      "Iteration 44, loss = 0.56390623\n",
      "Iteration 50, loss = 1.43004619\n",
      "Iteration 45, loss = 0.80536020\n",
      "Iteration 51, loss = 1.20900629\n",
      "Iteration 46, loss = 0.45455188\n",
      "Iteration 52, loss = 1.47235445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.65545217\n",
      "Iteration 48, loss = 0.79384090\n",
      "Iteration 49, loss = 0.73282346\n",
      "Iteration 50, loss = 0.40991890\n",
      "Iteration 51, loss = 1.31454506\n",
      "Iteration 52, loss = 1.09584933\n",
      "Iteration 53, loss = 0.67065348\n",
      "Iteration 54, loss = 0.86264599\n",
      "Iteration 55, loss = 0.96310655\n",
      "Iteration 56, loss = 0.93437727\n",
      "Iteration 57, loss = 0.41892173\n",
      "Iteration 58, loss = 0.37927226\n",
      "Iteration 59, loss = 0.28021987\n",
      "Iteration 60, loss = 0.28703165\n",
      "Iteration 61, loss = 0.17648476\n",
      "Iteration 62, loss = 0.22991684\n",
      "Iteration 63, loss = 0.33814812\n",
      "Iteration 64, loss = 0.41095404\n",
      "Iteration 1, loss = 17.22162502\n",
      "Iteration 65, loss = 0.29907264\n",
      "Iteration 2, loss = 18.69964079\n",
      "Iteration 66, loss = 0.31492958\n",
      "Iteration 3, loss = 9.79629780\n",
      "Iteration 67, loss = 0.38207736\n",
      "Iteration 4, loss = 10.40137529\n",
      "Iteration 68, loss = 0.30373049\n",
      "Iteration 5, loss = 9.46079785\n",
      "Iteration 69, loss = 0.22746879\n",
      "Iteration 6, loss = 5.36686965\n",
      "Iteration 70, loss = 0.22630151\n",
      "Iteration 7, loss = 5.66939076\n",
      "Iteration 71, loss = 0.21070540\n",
      "Iteration 8, loss = 4.80212239\n",
      "Iteration 72, loss = 0.19083863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 9, loss = 5.30436198\n",
      "Iteration 10, loss = 5.43657262\n",
      "Iteration 11, loss = 6.14068568\n",
      "Iteration 12, loss = 4.89897344\n",
      "Iteration 13, loss = 3.21888007\n",
      "Iteration 14, loss = 3.33956882\n",
      "Iteration 15, loss = 2.91497399\n",
      "Iteration 16, loss = 3.42979450\n",
      "Iteration 17, loss = 3.79972263\n",
      "Iteration 18, loss = 3.10613664\n",
      "Iteration 19, loss = 3.79857347\n",
      "Iteration 20, loss = 2.65279854\n",
      "Iteration 21, loss = 3.24310787\n",
      "Iteration 22, loss = 2.38815756\n",
      "Iteration 23, loss = 2.40121050\n",
      "Iteration 24, loss = 2.24953780\n",
      "Iteration 25, loss = 1.71386431\n",
      "Iteration 26, loss = 2.81918743\n",
      "Iteration 27, loss = 1.51988524\n",
      "Iteration 1, loss = 15.77741455\n",
      "Iteration 28, loss = 1.21300395\n",
      "Iteration 2, loss = 11.36254600\n",
      "Iteration 29, loss = 1.50492523\n",
      "Iteration 3, loss = 7.26868702\n",
      "Iteration 30, loss = 1.83858225\n",
      "Iteration 4, loss = 11.64054928\n",
      "Iteration 31, loss = 1.59180915\n",
      "Iteration 5, loss = 9.19306410\n",
      "Iteration 32, loss = 1.65131397\n",
      "Iteration 6, loss = 9.52184935\n",
      "Iteration 33, loss = 1.66480626\n",
      "Iteration 7, loss = 7.04575691\n",
      "Iteration 34, loss = 1.09612134\n",
      "Iteration 8, loss = 7.60568977\n",
      "Iteration 35, loss = 0.84772552\n",
      "Iteration 9, loss = 7.91357481\n",
      "Iteration 36, loss = 1.44816974\n",
      "Iteration 10, loss = 5.13861277\n",
      "Iteration 37, loss = 1.14267048\n",
      "Iteration 11, loss = 5.86230433\n",
      "Iteration 38, loss = 2.46124290\n",
      "Iteration 12, loss = 4.23212876\n",
      "Iteration 39, loss = 1.86681964\n",
      "Iteration 13, loss = 3.60948840\n",
      "Iteration 40, loss = 1.49949444\n",
      "Iteration 14, loss = 4.87034580\n",
      "Iteration 15, loss = 3.89131416\n",
      "Iteration 41, loss = 1.26631645\n",
      "Iteration 16, loss = 3.59164174\n",
      "Iteration 42, loss = 1.56611089\n",
      "Iteration 17, loss = 4.07461937\n",
      "Iteration 43, loss = 1.62735966\n",
      "Iteration 18, loss = 5.18648722\n",
      "Iteration 44, loss = 1.64699961\n",
      "Iteration 19, loss = 3.08704654\n",
      "Iteration 45, loss = 1.86992744\n",
      "Iteration 20, loss = 2.64468874\n",
      "Iteration 46, loss = 1.31162893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 2.95062371\n",
      "Iteration 22, loss = 2.31336848\n",
      "Iteration 23, loss = 1.94398277\n",
      "Iteration 24, loss = 2.29575376\n",
      "Iteration 25, loss = 2.58545320\n",
      "Iteration 26, loss = 1.91244348\n",
      "Iteration 27, loss = 2.12933865\n",
      "Iteration 28, loss = 1.37239175\n",
      "Iteration 29, loss = 1.17140788\n",
      "Iteration 30, loss = 0.93502958\n",
      "Iteration 31, loss = 0.96869974\n",
      "Iteration 32, loss = 1.08408644\n",
      "Iteration 33, loss = 1.25064927\n",
      "Iteration 34, loss = 1.31699684\n",
      "Iteration 35, loss = 1.46860945\n",
      "Iteration 36, loss = 0.88276922\n",
      "Iteration 37, loss = 1.15871054\n",
      "Iteration 38, loss = 1.17608560\n",
      "Iteration 1, loss = 18.67307251\n",
      "Iteration 39, loss = 0.97317755\n",
      "Iteration 2, loss = 15.03050919\n",
      "Iteration 40, loss = 1.11732128\n",
      "Iteration 3, loss = 14.28304039\n",
      "Iteration 41, loss = 1.00476721\n",
      "Iteration 4, loss = 9.93484871\n",
      "Iteration 42, loss = 1.39499960\n",
      "Iteration 5, loss = 7.45877940\n",
      "Iteration 43, loss = 2.28502559\n",
      "Iteration 6, loss = 7.40654257\n",
      "Iteration 44, loss = 1.92484365\n",
      "Iteration 7, loss = 6.72094956\n",
      "Iteration 45, loss = 1.40964504\n",
      "Iteration 8, loss = 7.47271493\n",
      "Iteration 46, loss = 1.16683529\n",
      "Iteration 9, loss = 5.27850771\n",
      "Iteration 47, loss = 1.43026431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 7.76980479\n",
      "Iteration 11, loss = 7.69946506\n",
      "Iteration 12, loss = 5.60106255\n",
      "Iteration 13, loss = 5.74038412\n",
      "Iteration 14, loss = 5.60807620\n",
      "Iteration 15, loss = 5.92387772\n",
      "Iteration 16, loss = 4.30645771\n",
      "Iteration 17, loss = 4.52278909\n",
      "Iteration 18, loss = 3.99249587\n",
      "Iteration 19, loss = 3.64340239\n",
      "Iteration 20, loss = 4.72797513\n",
      "Iteration 21, loss = 3.65455691\n",
      "Iteration 22, loss = 2.89720103\n",
      "Iteration 23, loss = 2.60765549\n",
      "Iteration 24, loss = 2.70677032\n",
      "Iteration 25, loss = 2.23564992\n",
      "Iteration 26, loss = 2.33755210\n",
      "Iteration 27, loss = 2.01139100\n",
      "Iteration 1, loss = 16.85813307\n",
      "Iteration 28, loss = 1.89358594\n",
      "Iteration 2, loss = 13.43066411\n",
      "Iteration 29, loss = 2.32871363\n",
      "Iteration 3, loss = 14.24722531\n",
      "Iteration 30, loss = 1.51054385\n",
      "Iteration 4, loss = 8.48763296\n",
      "Iteration 31, loss = 1.29648874\n",
      "Iteration 5, loss = 6.46136465\n",
      "Iteration 32, loss = 1.44162971\n",
      "Iteration 6, loss = 7.64183519\n",
      "Iteration 33, loss = 1.39440853\n",
      "Iteration 7, loss = 6.55282665\n",
      "Iteration 34, loss = 1.73903757\n",
      "Iteration 8, loss = 4.68404752\n",
      "Iteration 35, loss = 2.58893870\n",
      "Iteration 9, loss = 3.98546070\n",
      "Iteration 36, loss = 2.24433617\n",
      "Iteration 10, loss = 5.20439827\n",
      "Iteration 37, loss = 1.39424542\n",
      "Iteration 11, loss = 3.76689299\n",
      "Iteration 38, loss = 1.15761515\n",
      "Iteration 12, loss = 4.00529660\n",
      "Iteration 39, loss = 0.87226522\n",
      "Iteration 13, loss = 5.42102722\n",
      "Iteration 40, loss = 0.69674114\n",
      "Iteration 14, loss = 4.03795598\n",
      "Iteration 41, loss = 0.85306398\n",
      "Iteration 15, loss = 3.13170066\n",
      "Iteration 42, loss = 0.89098679\n",
      "Iteration 16, loss = 3.87350503\n",
      "Iteration 43, loss = 0.75476233\n",
      "Iteration 17, loss = 2.77994107\n",
      "Iteration 44, loss = 1.06494157\n",
      "Iteration 18, loss = 3.85745967\n",
      "Iteration 45, loss = 1.08816540\n",
      "Iteration 19, loss = 2.19656785\n",
      "Iteration 46, loss = 0.64981325\n",
      "Iteration 20, loss = 2.85840211\n",
      "Iteration 47, loss = 0.60663981\n",
      "Iteration 21, loss = 4.85167554\n",
      "Iteration 48, loss = 1.17632139\n",
      "Iteration 22, loss = 3.72974799\n",
      "Iteration 49, loss = 0.86578429\n",
      "Iteration 23, loss = 3.47998883\n",
      "Iteration 50, loss = 0.70034581\n",
      "Iteration 24, loss = 3.03442465\n",
      "Iteration 51, loss = 0.59285330\n",
      "Iteration 25, loss = 3.14254417\n",
      "Iteration 52, loss = 0.48142867\n",
      "Iteration 26, loss = 2.76846174\n",
      "Iteration 53, loss = 0.44622196\n",
      "Iteration 27, loss = 2.25462370\n",
      "Iteration 54, loss = 0.69445009\n",
      "Iteration 28, loss = 2.37082086\n",
      "Iteration 55, loss = 0.60047553\n",
      "Iteration 29, loss = 2.34702606\n",
      "Iteration 56, loss = 0.55910890\n",
      "Iteration 30, loss = 3.14671331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.52810752\n",
      "Iteration 58, loss = 0.78931763\n",
      "Iteration 59, loss = 0.46831854\n",
      "Iteration 60, loss = 0.31230388\n",
      "Iteration 61, loss = 0.34254278\n",
      "Iteration 62, loss = 0.31299200\n",
      "Iteration 63, loss = 0.26862537\n",
      "Iteration 64, loss = 0.45386876\n",
      "Iteration 65, loss = 0.43037257\n",
      "Iteration 66, loss = 0.56071054\n",
      "Iteration 67, loss = 0.49448062\n",
      "Iteration 68, loss = 0.40820115\n",
      "Iteration 69, loss = 0.23332208\n",
      "Iteration 70, loss = 0.24800400\n",
      "Iteration 71, loss = 0.18122454\n",
      "Iteration 72, loss = 0.39001388\n",
      "Iteration 73, loss = 0.39291781\n",
      "Iteration 74, loss = 0.25051653\n",
      "Iteration 1, loss = 16.19799050\n",
      "Iteration 75, loss = 0.17176446\n",
      "Iteration 2, loss = 13.97733843\n",
      "Iteration 76, loss = 0.18472432\n",
      "Iteration 3, loss = 11.49508174\n",
      "Iteration 77, loss = 0.11359285\n",
      "Iteration 4, loss = 9.82890628\n",
      "Iteration 78, loss = 0.08087073\n",
      "Iteration 5, loss = 8.75646406\n",
      "Iteration 79, loss = 0.07842812\n",
      "Iteration 6, loss = 7.86840903\n",
      "Iteration 80, loss = 0.13709418\n",
      "Iteration 7, loss = 4.78854572\n",
      "Iteration 81, loss = 0.27251590\n",
      "Iteration 8, loss = 5.44384339\n",
      "Iteration 82, loss = 0.60187871\n",
      "Iteration 9, loss = 5.04941808\n",
      "Iteration 83, loss = 0.38974255\n",
      "Iteration 10, loss = 5.51528586\n",
      "Iteration 84, loss = 0.30572170\n",
      "Iteration 11, loss = 4.46855838\n",
      "Iteration 85, loss = 0.42950362\n",
      "Iteration 12, loss = 4.78973708\n",
      "Iteration 86, loss = 0.52983575\n",
      "Iteration 13, loss = 5.54189095\n",
      "Iteration 87, loss = 0.50050683\n",
      "Iteration 14, loss = 4.31891851\n",
      "Iteration 88, loss = 0.48789009\n",
      "Iteration 15, loss = 4.00831079\n",
      "Iteration 89, loss = 0.28365677\n",
      "Iteration 16, loss = 2.86015921\n",
      "Iteration 90, loss = 0.38069282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 2.81972426\n",
      "Iteration 18, loss = 3.25325509\n",
      "Iteration 19, loss = 3.54491023\n",
      "Iteration 20, loss = 3.39596940\n",
      "Iteration 21, loss = 2.83361457\n",
      "Iteration 22, loss = 2.75753379\n",
      "Iteration 23, loss = 2.66082304\n",
      "Iteration 24, loss = 2.81664560\n",
      "Iteration 25, loss = 1.84248059\n",
      "Iteration 26, loss = 1.99599969\n",
      "Iteration 27, loss = 2.70713351\n",
      "Iteration 28, loss = 2.23215063\n",
      "Iteration 29, loss = 1.76068952\n",
      "Iteration 30, loss = 1.93304249\n",
      "Iteration 31, loss = 2.75399223\n",
      "Iteration 32, loss = 1.94832190\n",
      "Iteration 33, loss = 1.66650243\n",
      "Iteration 34, loss = 1.42944734\n",
      "Iteration 1, loss = 19.75836451\n",
      "Iteration 35, loss = 2.03941033\n",
      "Iteration 2, loss = 24.18939025\n",
      "Iteration 36, loss = 1.12980307\n",
      "Iteration 3, loss = 12.71169192\n",
      "Iteration 37, loss = 1.69003084\n",
      "Iteration 4, loss = 7.71771118\n",
      "Iteration 38, loss = 1.46410430\n",
      "Iteration 5, loss = 7.52144226\n",
      "Iteration 39, loss = 1.14851529\n",
      "Iteration 6, loss = 7.36888256\n",
      "Iteration 40, loss = 1.15920401\n",
      "Iteration 7, loss = 7.67654959\n",
      "Iteration 41, loss = 0.88940199\n",
      "Iteration 8, loss = 6.39623510\n",
      "Iteration 42, loss = 0.88517733\n",
      "Iteration 9, loss = 5.44976088\n",
      "Iteration 43, loss = 0.74021617\n",
      "Iteration 10, loss = 5.42298704\n",
      "Iteration 44, loss = 0.50579211\n",
      "Iteration 11, loss = 10.06214381\n",
      "Iteration 45, loss = 0.56481138\n",
      "Iteration 12, loss = 7.06345992\n",
      "Iteration 46, loss = 0.71902597\n",
      "Iteration 13, loss = 5.54862261\n",
      "Iteration 47, loss = 0.76726729\n",
      "Iteration 14, loss = 4.74325570\n",
      "Iteration 48, loss = 0.91291124\n",
      "Iteration 15, loss = 4.01728911\n",
      "Iteration 49, loss = 1.11624039\n",
      "Iteration 16, loss = 3.86667673\n",
      "Iteration 50, loss = 1.00725122\n",
      "Iteration 17, loss = 2.76649521\n",
      "Iteration 51, loss = 0.79512785\n",
      "Iteration 18, loss = 2.56506740\n",
      "Iteration 52, loss = 0.68495371\n",
      "Iteration 19, loss = 2.75627459\n",
      "Iteration 53, loss = 0.59438466\n",
      "Iteration 20, loss = 2.50677645\n",
      "Iteration 54, loss = 0.65946253\n",
      "Iteration 21, loss = 2.39723503\n",
      "Iteration 55, loss = 0.50197640\n",
      "Iteration 22, loss = 2.13182057\n",
      "Iteration 56, loss = 0.48185517\n",
      "Iteration 23, loss = 2.65699739\n",
      "Iteration 57, loss = 0.52346945\n",
      "Iteration 24, loss = 2.29035927\n",
      "Iteration 58, loss = 0.72918478\n",
      "Iteration 25, loss = 2.84651807\n",
      "Iteration 59, loss = 0.74505252\n",
      "Iteration 26, loss = 2.24405596\n",
      "Iteration 60, loss = 1.26085597\n",
      "Iteration 27, loss = 2.56752621\n",
      "Iteration 61, loss = 0.82737273\n",
      "Iteration 28, loss = 2.29994509\n",
      "Iteration 62, loss = 1.05686698\n",
      "Iteration 29, loss = 1.25386196\n",
      "Iteration 63, loss = 1.50980053\n",
      "Iteration 30, loss = 2.24055517\n",
      "Iteration 64, loss = 1.04974399\n",
      "Iteration 31, loss = 1.26731903\n",
      "Iteration 65, loss = 1.14275362\n",
      "Iteration 32, loss = 1.07619944\n",
      "Iteration 66, loss = 0.85216337\n",
      "Iteration 33, loss = 1.04396217\n",
      "Iteration 67, loss = 1.14541687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.91922951\n",
      "Iteration 35, loss = 0.81948252\n",
      "Iteration 36, loss = 0.67555216\n",
      "Iteration 37, loss = 0.66732365\n",
      "Iteration 38, loss = 1.55190799\n",
      "Iteration 39, loss = 1.16388988\n",
      "Iteration 40, loss = 1.18034323\n",
      "Iteration 41, loss = 0.80842144\n",
      "Iteration 42, loss = 1.13042216\n",
      "Iteration 43, loss = 1.47527946\n",
      "Iteration 44, loss = 0.84264231\n",
      "Iteration 45, loss = 0.71810796\n",
      "Iteration 46, loss = 0.74851675\n",
      "Iteration 47, loss = 0.53989723\n",
      "Iteration 48, loss = 0.46548369\n",
      "Iteration 49, loss = 0.64028159\n",
      "Iteration 50, loss = 0.85047644\n",
      "Iteration 51, loss = 0.51624583\n",
      "Iteration 52, loss = 0.42553981\n",
      "Iteration 1, loss = 14.17462794\n",
      "Iteration 53, loss = 0.47087338\n",
      "Iteration 2, loss = 13.78026549\n",
      "Iteration 54, loss = 0.46230758\n",
      "Iteration 3, loss = 13.31286798\n",
      "Iteration 4, loss = 12.34469539\n",
      "Iteration 55, loss = 0.69345738\n",
      "Iteration 5, loss = 10.43790612\n",
      "Iteration 56, loss = 0.73145457\n",
      "Iteration 6, loss = 10.56962881\n",
      "Iteration 57, loss = 0.68329976\n",
      "Iteration 7, loss = 7.94618105\n",
      "Iteration 58, loss = 0.75524382\n",
      "Iteration 8, loss = 7.48208819\n",
      "Iteration 59, loss = 0.89802745\n",
      "Iteration 9, loss = 6.58109211\n",
      "Iteration 60, loss = 0.56599553\n",
      "Iteration 10, loss = 5.47368783\n",
      "Iteration 61, loss = 0.44253939\n",
      "Iteration 11, loss = 6.33898181\n",
      "Iteration 62, loss = 0.56393293\n",
      "Iteration 12, loss = 8.80605460\n",
      "Iteration 63, loss = 0.59119999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 5.48758559\n",
      "Iteration 14, loss = 6.19667793\n",
      "Iteration 15, loss = 4.53474875\n",
      "Iteration 16, loss = 4.70655493\n",
      "Iteration 17, loss = 3.94913863\n",
      "Iteration 18, loss = 3.14803794\n",
      "Iteration 19, loss = 3.53029604\n",
      "Iteration 20, loss = 2.95126157\n",
      "Iteration 21, loss = 2.73665863\n",
      "Iteration 22, loss = 2.72518545\n",
      "Iteration 23, loss = 2.44326162\n",
      "Iteration 24, loss = 2.76012070\n",
      "Iteration 25, loss = 2.02406707\n",
      "Iteration 26, loss = 1.95169037\n",
      "Iteration 27, loss = 1.94878208\n",
      "Iteration 28, loss = 1.51752835\n",
      "Iteration 29, loss = 1.30770426\n",
      "Iteration 30, loss = 0.96433157\n",
      "Iteration 1, loss = 18.27484992\n",
      "Iteration 31, loss = 0.75854490\n",
      "Iteration 2, loss = 11.80888734\n",
      "Iteration 32, loss = 0.73450611\n",
      "Iteration 3, loss = 12.23412157\n",
      "Iteration 33, loss = 0.82423145\n",
      "Iteration 4, loss = 14.43362837\n",
      "Iteration 34, loss = 0.79645879\n",
      "Iteration 5, loss = 12.41709318\n",
      "Iteration 35, loss = 0.71210905\n",
      "Iteration 6, loss = 10.46549348\n",
      "Iteration 36, loss = 0.70988289\n",
      "Iteration 7, loss = 11.93995547\n",
      "Iteration 37, loss = 0.55957693\n",
      "Iteration 8, loss = 8.37663869\n",
      "Iteration 38, loss = 0.72401764\n",
      "Iteration 9, loss = 9.36424718\n",
      "Iteration 39, loss = 0.51925509\n",
      "Iteration 10, loss = 5.67144542\n",
      "Iteration 40, loss = 0.64386574\n",
      "Iteration 11, loss = 4.21001943\n",
      "Iteration 41, loss = 0.76431609\n",
      "Iteration 12, loss = 5.14181667\n",
      "Iteration 42, loss = 0.55219075\n",
      "Iteration 13, loss = 4.68856444\n",
      "Iteration 43, loss = 0.62209451\n",
      "Iteration 14, loss = 3.47998410\n",
      "Iteration 44, loss = 0.59462575\n",
      "Iteration 15, loss = 2.98098619\n",
      "Iteration 45, loss = 0.69837878\n",
      "Iteration 16, loss = 2.95542650\n",
      "Iteration 46, loss = 0.65145296\n",
      "Iteration 17, loss = 3.89433275\n",
      "Iteration 47, loss = 0.39964916\n",
      "Iteration 18, loss = 5.18223672\n",
      "Iteration 48, loss = 0.36916847\n",
      "Iteration 19, loss = 3.00001309\n",
      "Iteration 49, loss = 0.32633937\n",
      "Iteration 20, loss = 2.50536988\n",
      "Iteration 50, loss = 0.43969148\n",
      "Iteration 21, loss = 2.99464361\n",
      "Iteration 51, loss = 0.46055176\n",
      "Iteration 22, loss = 2.35803376\n",
      "Iteration 52, loss = 0.34548337\n",
      "Iteration 23, loss = 1.85270077\n",
      "Iteration 53, loss = 0.38195139\n",
      "Iteration 24, loss = 2.40021026\n",
      "Iteration 54, loss = 0.35530139\n",
      "Iteration 25, loss = 2.07240692\n",
      "Iteration 55, loss = 0.46170681\n",
      "Iteration 26, loss = 1.56380150\n",
      "Iteration 56, loss = 0.36843955\n",
      "Iteration 27, loss = 1.61973765\n",
      "Iteration 57, loss = 0.26277667\n",
      "Iteration 28, loss = 3.03813639\n",
      "Iteration 58, loss = 0.30095973\n",
      "Iteration 29, loss = 1.95471736\n",
      "Iteration 59, loss = 0.48105467\n",
      "Iteration 30, loss = 1.78111389\n",
      "Iteration 60, loss = 0.43848351\n",
      "Iteration 31, loss = 1.44200396\n",
      "Iteration 61, loss = 0.44153099\n",
      "Iteration 32, loss = 1.46928988\n",
      "Iteration 62, loss = 0.51257597\n",
      "Iteration 33, loss = 1.26302067\n",
      "Iteration 63, loss = 0.47315734\n",
      "Iteration 34, loss = 1.43205628\n",
      "Iteration 64, loss = 0.35179409\n",
      "Iteration 35, loss = 1.67599873\n",
      "Iteration 65, loss = 0.35263412\n",
      "Iteration 36, loss = 1.37731298\n",
      "Iteration 66, loss = 0.23837800\n",
      "Iteration 37, loss = 1.09738048\n",
      "Iteration 67, loss = 0.31537844\n",
      "Iteration 38, loss = 1.22952943\n",
      "Iteration 68, loss = 0.30786157\n",
      "Iteration 39, loss = 1.08046909\n",
      "Iteration 69, loss = 0.33367135\n",
      "Iteration 40, loss = 0.98002804\n",
      "Iteration 70, loss = 0.34429651\n",
      "Iteration 41, loss = 0.99058984\n",
      "Iteration 71, loss = 0.28244784\n",
      "Iteration 42, loss = 1.50458307\n",
      "Iteration 72, loss = 0.25122252\n",
      "Iteration 43, loss = 1.08551804\n",
      "Iteration 73, loss = 0.28985760\n",
      "Iteration 44, loss = 0.88800669\n",
      "Iteration 74, loss = 0.19494959\n",
      "Iteration 45, loss = 0.87320644\n",
      "Iteration 75, loss = 0.25210789\n",
      "Iteration 46, loss = 1.26442747\n",
      "Iteration 76, loss = 0.24617962\n",
      "Iteration 47, loss = 1.13962185\n",
      "Iteration 77, loss = 0.18912934\n",
      "Iteration 48, loss = 1.13618793\n",
      "Iteration 78, loss = 0.15141829\n",
      "Iteration 49, loss = 1.87328876\n",
      "Iteration 79, loss = 0.17401776\n",
      "Iteration 50, loss = 1.63813308\n",
      "Iteration 80, loss = 0.17752359\n",
      "Iteration 51, loss = 1.46111280\n",
      "Iteration 81, loss = 0.13947864\n",
      "Iteration 52, loss = 1.05565189\n",
      "Iteration 82, loss = 0.12972023\n",
      "Iteration 53, loss = 1.14217720\n",
      "Iteration 83, loss = 0.14748987\n",
      "Iteration 54, loss = 0.86432223\n",
      "Iteration 84, loss = 0.13271363\n",
      "Iteration 55, loss = 2.11847607\n",
      "Iteration 85, loss = 0.13111815\n",
      "Iteration 56, loss = 1.13161690\n",
      "Iteration 86, loss = 0.13774955\n",
      "Iteration 57, loss = 0.99025725\n",
      "Iteration 87, loss = 0.13138108\n",
      "Iteration 58, loss = 0.99369777\n",
      "Iteration 88, loss = 0.19491517\n",
      "Iteration 59, loss = 0.87083201\n",
      "Iteration 89, loss = 0.17626667\n",
      "Iteration 60, loss = 1.26326509\n",
      "Iteration 90, loss = 0.18757454\n",
      "Iteration 61, loss = 1.30511514\n",
      "Iteration 91, loss = 0.14571687\n",
      "Iteration 62, loss = 0.66300454\n",
      "Iteration 92, loss = 0.11706039\n",
      "Iteration 63, loss = 0.74013879\n",
      "Iteration 93, loss = 0.12431839\n",
      "Iteration 64, loss = 0.45854670\n",
      "Iteration 94, loss = 0.12265254\n",
      "Iteration 65, loss = 0.65596168\n",
      "Iteration 95, loss = 0.12296834\n",
      "Iteration 66, loss = 1.46756859\n",
      "Iteration 96, loss = 0.10576496\n",
      "Iteration 67, loss = 1.27081510\n",
      "Iteration 97, loss = 0.10825643\n",
      "Iteration 68, loss = 0.69264983\n",
      "Iteration 98, loss = 0.10618248\n",
      "Iteration 69, loss = 1.00525970\n",
      "Iteration 99, loss = 0.09591604\n",
      "Iteration 70, loss = 0.93384008\n",
      "Iteration 100, loss = 0.09803943\n",
      "Iteration 71, loss = 1.04736238\n",
      "Iteration 101, loss = 0.09274128\n",
      "Iteration 72, loss = 1.64215152\n",
      "Iteration 102, loss = 0.10997787\n",
      "Iteration 73, loss = 1.92838691\n",
      "Iteration 103, loss = 0.10746184\n",
      "Iteration 74, loss = 0.98109640\n",
      "Iteration 104, loss = 0.09633179\n",
      "Iteration 75, loss = 0.92401912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 105, loss = 0.09640673\n",
      "Iteration 106, loss = 0.12708847\n",
      "Iteration 107, loss = 0.16589900\n",
      "Iteration 108, loss = 0.18731869\n",
      "Iteration 109, loss = 0.20035723\n",
      "Iteration 110, loss = 0.35021146\n",
      "Iteration 111, loss = 0.31757951\n",
      "Iteration 112, loss = 0.26918117\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.29597433\n",
      "Iteration 2, loss = 15.28055239\n",
      "Iteration 3, loss = 13.64528509\n",
      "Iteration 4, loss = 13.57173328\n",
      "Iteration 5, loss = 7.19754207\n",
      "Iteration 6, loss = 6.50662173\n",
      "Iteration 7, loss = 9.29111706\n",
      "Iteration 8, loss = 6.80718805\n",
      "Iteration 1, loss = 16.34149421\n",
      "Iteration 9, loss = 6.93848129\n",
      "Iteration 2, loss = 13.60134464\n",
      "Iteration 10, loss = 10.83904822\n",
      "Iteration 3, loss = 13.26247636\n",
      "Iteration 11, loss = 8.94020486\n",
      "Iteration 4, loss = 10.99220897\n",
      "Iteration 12, loss = 6.65812237\n",
      "Iteration 5, loss = 5.36147908\n",
      "Iteration 13, loss = 5.22063892\n",
      "Iteration 6, loss = 6.71242149\n",
      "Iteration 14, loss = 5.22932196\n",
      "Iteration 7, loss = 5.49350639\n",
      "Iteration 15, loss = 5.75382273\n",
      "Iteration 8, loss = 4.75448214\n",
      "Iteration 16, loss = 6.02766121\n",
      "Iteration 9, loss = 4.71201159\n",
      "Iteration 17, loss = 4.91198124\n",
      "Iteration 10, loss = 4.02746626\n",
      "Iteration 18, loss = 4.41793143\n",
      "Iteration 11, loss = 3.63731174\n",
      "Iteration 19, loss = 3.56709981\n",
      "Iteration 12, loss = 4.27686077\n",
      "Iteration 20, loss = 3.26146161\n",
      "Iteration 13, loss = 3.78001406\n",
      "Iteration 21, loss = 3.80987907\n",
      "Iteration 14, loss = 3.15572511\n",
      "Iteration 22, loss = 2.82346645\n",
      "Iteration 15, loss = 3.81560303\n",
      "Iteration 23, loss = 2.45835638\n",
      "Iteration 16, loss = 3.53542878\n",
      "Iteration 24, loss = 3.33271382\n",
      "Iteration 17, loss = 2.09390116\n",
      "Iteration 25, loss = 2.53967213\n",
      "Iteration 18, loss = 2.55441496\n",
      "Iteration 26, loss = 1.80941887\n",
      "Iteration 19, loss = 2.38365879\n",
      "Iteration 27, loss = 2.18212918\n",
      "Iteration 20, loss = 2.16794050\n",
      "Iteration 28, loss = 1.61210951\n",
      "Iteration 21, loss = 3.04031656\n",
      "Iteration 29, loss = 1.73602674\n",
      "Iteration 22, loss = 2.85357530\n",
      "Iteration 30, loss = 1.57895699\n",
      "Iteration 23, loss = 2.42962803\n",
      "Iteration 31, loss = 2.17809386\n",
      "Iteration 24, loss = 1.43438601\n",
      "Iteration 32, loss = 1.97010148\n",
      "Iteration 25, loss = 1.49722343\n",
      "Iteration 33, loss = 1.38337207\n",
      "Iteration 26, loss = 1.20004583\n",
      "Iteration 34, loss = 1.77317419\n",
      "Iteration 27, loss = 1.07688929\n",
      "Iteration 35, loss = 1.52955822\n",
      "Iteration 28, loss = 1.32578389\n",
      "Iteration 36, loss = 1.99091183\n",
      "Iteration 29, loss = 1.18442990\n",
      "Iteration 37, loss = 1.39538684\n",
      "Iteration 30, loss = 1.05450446\n",
      "Iteration 38, loss = 2.18448054\n",
      "Iteration 31, loss = 0.77665678\n",
      "Iteration 39, loss = 1.55936866\n",
      "Iteration 32, loss = 0.77401716\n",
      "Iteration 40, loss = 1.53565541\n",
      "Iteration 33, loss = 0.55849530\n",
      "Iteration 41, loss = 1.38238422\n",
      "Iteration 34, loss = 0.85805667\n",
      "Iteration 42, loss = 1.60553642\n",
      "Iteration 35, loss = 0.56888350\n",
      "Iteration 43, loss = 1.26305186\n",
      "Iteration 36, loss = 0.70102762\n",
      "Iteration 44, loss = 0.92131876\n",
      "Iteration 37, loss = 0.61553465\n",
      "Iteration 45, loss = 0.82158851\n",
      "Iteration 38, loss = 1.00057353\n",
      "Iteration 46, loss = 1.01264398\n",
      "Iteration 39, loss = 1.28344959\n",
      "Iteration 47, loss = 0.91807530\n",
      "Iteration 40, loss = 1.05812933\n",
      "Iteration 48, loss = 1.16808402\n",
      "Iteration 41, loss = 0.97540694\n",
      "Iteration 49, loss = 1.31164203\n",
      "Iteration 42, loss = 1.19859658\n",
      "Iteration 50, loss = 1.06339306\n",
      "Iteration 43, loss = 0.99392525\n",
      "Iteration 51, loss = 0.86909224\n",
      "Iteration 44, loss = 0.78290252\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.68105006\n",
      "Iteration 53, loss = 0.86532705\n",
      "Iteration 54, loss = 1.19798350\n",
      "Iteration 55, loss = 1.16298436\n",
      "Iteration 56, loss = 1.15775717\n",
      "Iteration 57, loss = 0.90535199\n",
      "Iteration 58, loss = 0.81959548\n",
      "Iteration 59, loss = 0.84964126\n",
      "Iteration 60, loss = 1.11112119\n",
      "Iteration 61, loss = 0.96772264\n",
      "Iteration 62, loss = 0.69318050\n",
      "Iteration 63, loss = 0.84853478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.89430733\n",
      "Iteration 2, loss = 13.23319705\n",
      "Iteration 3, loss = 10.46970457\n",
      "Iteration 4, loss = 7.30227359\n",
      "Iteration 5, loss = 6.65422547\n",
      "Iteration 6, loss = 6.81993598\n",
      "Iteration 7, loss = 6.50048431\n",
      "Iteration 8, loss = 5.46320918\n",
      "Iteration 9, loss = 5.78222719\n",
      "Iteration 10, loss = 6.13043179\n",
      "Iteration 11, loss = 5.02959598\n",
      "Iteration 12, loss = 4.78892861\n",
      "Iteration 1, loss = 18.99775220\n",
      "Iteration 13, loss = 4.55009902\n",
      "Iteration 2, loss = 14.63823241\n",
      "Iteration 14, loss = 4.84217864\n",
      "Iteration 3, loss = 10.69174146\n",
      "Iteration 15, loss = 5.06087120\n",
      "Iteration 4, loss = 11.49863813\n",
      "Iteration 16, loss = 3.17169905\n",
      "Iteration 5, loss = 8.26376650\n",
      "Iteration 17, loss = 3.29078596\n",
      "Iteration 6, loss = 9.89653135\n",
      "Iteration 18, loss = 3.15275921\n",
      "Iteration 7, loss = 7.97850566\n",
      "Iteration 19, loss = 3.13017842\n",
      "Iteration 8, loss = 7.53369602\n",
      "Iteration 20, loss = 3.42073647\n",
      "Iteration 9, loss = 7.89838484\n",
      "Iteration 21, loss = 4.33072796\n",
      "Iteration 10, loss = 6.42793374\n",
      "Iteration 22, loss = 3.72290816\n",
      "Iteration 11, loss = 8.03076799\n",
      "Iteration 23, loss = 2.75672653\n",
      "Iteration 12, loss = 5.16300714\n",
      "Iteration 24, loss = 2.11337831\n",
      "Iteration 13, loss = 3.99559001\n",
      "Iteration 25, loss = 2.66305177\n",
      "Iteration 14, loss = 3.74482605\n",
      "Iteration 26, loss = 2.49453215\n",
      "Iteration 15, loss = 4.31067960\n",
      "Iteration 27, loss = 2.89500940\n",
      "Iteration 16, loss = 5.42830081\n",
      "Iteration 28, loss = 2.27407574\n",
      "Iteration 17, loss = 4.95505257\n",
      "Iteration 29, loss = 1.65590025\n",
      "Iteration 18, loss = 3.85359000\n",
      "Iteration 30, loss = 1.73334638\n",
      "Iteration 19, loss = 4.04758313\n",
      "Iteration 31, loss = 1.72954947\n",
      "Iteration 20, loss = 4.06179634\n",
      "Iteration 32, loss = 1.65905033\n",
      "Iteration 21, loss = 2.95920872\n",
      "Iteration 33, loss = 1.49826367\n",
      "Iteration 22, loss = 3.03180850\n",
      "Iteration 34, loss = 1.25082414\n",
      "Iteration 23, loss = 3.18121607\n",
      "Iteration 35, loss = 1.08793022\n",
      "Iteration 24, loss = 2.56555356\n",
      "Iteration 36, loss = 1.06545570\n",
      "Iteration 25, loss = 1.95187613\n",
      "Iteration 37, loss = 0.99559418\n",
      "Iteration 26, loss = 2.89855769\n",
      "Iteration 38, loss = 0.77107544\n",
      "Iteration 27, loss = 1.81010536\n",
      "Iteration 39, loss = 2.23798269\n",
      "Iteration 28, loss = 2.51212836\n",
      "Iteration 40, loss = 1.37257033\n",
      "Iteration 29, loss = 2.22686619\n",
      "Iteration 41, loss = 0.72066696\n",
      "Iteration 30, loss = 2.17784598\n",
      "Iteration 42, loss = 0.99155546\n",
      "Iteration 31, loss = 1.79220664\n",
      "Iteration 43, loss = 0.91486161\n",
      "Iteration 32, loss = 1.97446656\n",
      "Iteration 44, loss = 0.89660038\n",
      "Iteration 33, loss = 2.72150854\n",
      "Iteration 45, loss = 0.83103473\n",
      "Iteration 34, loss = 2.65366708\n",
      "Iteration 46, loss = 0.66592922\n",
      "Iteration 35, loss = 2.29281469\n",
      "Iteration 47, loss = 0.67892159\n",
      "Iteration 36, loss = 2.93847162\n",
      "Iteration 48, loss = 0.79253646\n",
      "Iteration 37, loss = 3.62110558\n",
      "Iteration 49, loss = 0.82678942\n",
      "Iteration 38, loss = 2.92598568\n",
      "Iteration 50, loss = 0.59395085\n",
      "Iteration 39, loss = 3.04964992\n",
      "Iteration 51, loss = 0.84642474\n",
      "Iteration 40, loss = 2.47595215\n",
      "Iteration 52, loss = 0.73312465\n",
      "Iteration 41, loss = 2.20240274\n",
      "Iteration 53, loss = 0.81268248\n",
      "Iteration 42, loss = 1.94506246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.84282198\n",
      "Iteration 55, loss = 0.54948772\n",
      "Iteration 56, loss = 0.46740064\n",
      "Iteration 57, loss = 0.66405877\n",
      "Iteration 58, loss = 0.89340797\n",
      "Iteration 59, loss = 0.76333367\n",
      "Iteration 60, loss = 1.83928372\n",
      "Iteration 61, loss = 1.63211129\n",
      "Iteration 62, loss = 1.59703006\n",
      "Iteration 63, loss = 1.73676064\n",
      "Iteration 64, loss = 2.29849589\n",
      "Iteration 65, loss = 2.26717969\n",
      "Iteration 66, loss = 2.22670827\n",
      "Iteration 67, loss = 1.18748807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 14.50536924\n",
      "Iteration 2, loss = 14.84249752\n",
      "Iteration 3, loss = 13.73000903\n",
      "Iteration 4, loss = 9.95158844\n",
      "Iteration 5, loss = 7.80133773\n",
      "Iteration 6, loss = 9.90117740\n",
      "Iteration 7, loss = 8.98386873\n",
      "Iteration 8, loss = 9.81443589\n",
      "Iteration 9, loss = 9.99399330\n",
      "Iteration 10, loss = 6.29689083\n",
      "Iteration 11, loss = 6.02678421\n",
      "Iteration 12, loss = 6.36826387\n",
      "Iteration 13, loss = 6.37497486\n",
      "Iteration 14, loss = 6.95522298\n",
      "Iteration 1, loss = 15.11260078\n",
      "Iteration 15, loss = 7.70583804\n",
      "Iteration 2, loss = 13.18171169\n",
      "Iteration 16, loss = 5.02382337\n",
      "Iteration 3, loss = 15.58653434\n",
      "Iteration 17, loss = 3.42953524\n",
      "Iteration 4, loss = 13.38610539\n",
      "Iteration 18, loss = 3.51509572\n",
      "Iteration 5, loss = 10.47655905\n",
      "Iteration 19, loss = 2.60201208\n",
      "Iteration 6, loss = 8.77113152\n",
      "Iteration 20, loss = 2.86287160\n",
      "Iteration 7, loss = 7.19664161\n",
      "Iteration 21, loss = 2.98245874\n",
      "Iteration 8, loss = 8.77969934\n",
      "Iteration 22, loss = 2.44443363\n",
      "Iteration 9, loss = 8.17138693\n",
      "Iteration 23, loss = 1.59401052\n",
      "Iteration 10, loss = 6.11362559\n",
      "Iteration 24, loss = 1.94493806\n",
      "Iteration 11, loss = 5.15532113\n",
      "Iteration 25, loss = 1.68067008\n",
      "Iteration 12, loss = 5.17488462\n",
      "Iteration 26, loss = 1.50811664\n",
      "Iteration 13, loss = 5.57135004\n",
      "Iteration 27, loss = 1.33938365\n",
      "Iteration 14, loss = 4.58796094\n",
      "Iteration 28, loss = 1.08036159\n",
      "Iteration 15, loss = 3.95182987\n",
      "Iteration 29, loss = 0.97695770\n",
      "Iteration 16, loss = 3.80104864\n",
      "Iteration 30, loss = 0.82607122\n",
      "Iteration 17, loss = 3.56872196\n",
      "Iteration 31, loss = 0.84273998\n",
      "Iteration 18, loss = 2.85183181\n",
      "Iteration 32, loss = 0.94457437\n",
      "Iteration 19, loss = 3.77652230\n",
      "Iteration 33, loss = 1.10399672\n",
      "Iteration 20, loss = 3.74193255\n",
      "Iteration 34, loss = 1.18127139\n",
      "Iteration 21, loss = 3.14927575\n",
      "Iteration 35, loss = 0.90892560\n",
      "Iteration 22, loss = 3.03880703\n",
      "Iteration 36, loss = 1.16575132\n",
      "Iteration 23, loss = 4.16205744\n",
      "Iteration 37, loss = 1.51464401\n",
      "Iteration 24, loss = 3.65959222\n",
      "Iteration 38, loss = 0.99102783\n",
      "Iteration 25, loss = 3.99592870\n",
      "Iteration 39, loss = 1.03023054\n",
      "Iteration 26, loss = 2.82542523\n",
      "Iteration 40, loss = 0.96487378\n",
      "Iteration 27, loss = 2.48975250\n",
      "Iteration 41, loss = 0.99204534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 2.73367629\n",
      "Iteration 29, loss = 2.21258712\n",
      "Iteration 30, loss = 1.63573553\n",
      "Iteration 31, loss = 1.63345644\n",
      "Iteration 32, loss = 1.14237371\n",
      "Iteration 33, loss = 1.23468721\n",
      "Iteration 34, loss = 1.50344611\n",
      "Iteration 35, loss = 1.41918315\n",
      "Iteration 36, loss = 1.94708208\n",
      "Iteration 37, loss = 1.20162631\n",
      "Iteration 38, loss = 1.03000620\n",
      "Iteration 39, loss = 0.89011396\n",
      "Iteration 40, loss = 0.94957303\n",
      "Iteration 41, loss = 1.06310068\n",
      "Iteration 42, loss = 0.71728632\n",
      "Iteration 43, loss = 0.93219531\n",
      "Iteration 44, loss = 0.79366517\n",
      "Iteration 45, loss = 0.90039014\n",
      "Iteration 46, loss = 0.80895090\n",
      "Iteration 1, loss = 17.20976808\n",
      "Iteration 47, loss = 1.30132818\n",
      "Iteration 2, loss = 13.29273678\n",
      "Iteration 48, loss = 1.06141129\n",
      "Iteration 3, loss = 11.03335563\n",
      "Iteration 49, loss = 1.07133316\n",
      "Iteration 4, loss = 13.52499475\n",
      "Iteration 50, loss = 0.75038387\n",
      "Iteration 5, loss = 8.11420302\n",
      "Iteration 51, loss = 1.01670028\n",
      "Iteration 6, loss = 5.17625001\n",
      "Iteration 52, loss = 1.13511004\n",
      "Iteration 7, loss = 8.14774763\n",
      "Iteration 53, loss = 0.75545279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 8.28714112\n",
      "Iteration 9, loss = 6.68532070\n",
      "Iteration 10, loss = 4.95090717\n",
      "Iteration 11, loss = 4.80996680\n",
      "Iteration 12, loss = 4.51427422\n",
      "Iteration 13, loss = 5.89775803\n",
      "Iteration 14, loss = 3.36454871\n",
      "Iteration 15, loss = 3.55908816\n",
      "Iteration 16, loss = 3.49283430\n",
      "Iteration 17, loss = 2.69822174\n",
      "Iteration 18, loss = 3.01108686\n",
      "Iteration 19, loss = 3.49260724\n",
      "Iteration 20, loss = 3.11936194\n",
      "Iteration 21, loss = 2.82117551\n",
      "Iteration 22, loss = 2.61381516\n",
      "Iteration 23, loss = 2.65654438\n",
      "Iteration 24, loss = 1.94333148\n",
      "Iteration 25, loss = 2.84755218\n",
      "Iteration 26, loss = 2.22079345\n",
      "Iteration 1, loss = 20.12432066\n",
      "Iteration 27, loss = 2.25559477\n",
      "Iteration 2, loss = 16.41227286\n",
      "Iteration 3, loss = 9.83032963\n",
      "Iteration 28, loss = 3.41272119\n",
      "Iteration 4, loss = 8.67040873\n",
      "Iteration 29, loss = 5.03141910\n",
      "Iteration 5, loss = 10.59677530\n",
      "Iteration 30, loss = 2.70930919\n",
      "Iteration 6, loss = 5.36430381\n",
      "Iteration 31, loss = 2.80399296\n",
      "Iteration 7, loss = 5.97849987\n",
      "Iteration 32, loss = 2.24827445\n",
      "Iteration 8, loss = 8.47681880\n",
      "Iteration 33, loss = 3.08685219\n",
      "Iteration 9, loss = 6.20440678\n",
      "Iteration 34, loss = 2.01716627\n",
      "Iteration 10, loss = 4.66530550\n",
      "Iteration 35, loss = 2.06255155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 4.50344645\n",
      "Iteration 12, loss = 3.51998456\n",
      "Iteration 13, loss = 4.75458332\n",
      "Iteration 14, loss = 5.42384167\n",
      "Iteration 15, loss = 3.60579696\n",
      "Iteration 16, loss = 2.85878909\n",
      "Iteration 17, loss = 2.81453134\n",
      "Iteration 18, loss = 2.53587315\n",
      "Iteration 19, loss = 2.00365530\n",
      "Iteration 20, loss = 2.80566100\n",
      "Iteration 21, loss = 2.67767059\n",
      "Iteration 22, loss = 2.63621714\n",
      "Iteration 23, loss = 2.32155613\n",
      "Iteration 24, loss = 1.72014623\n",
      "Iteration 25, loss = 1.77976014\n",
      "Iteration 26, loss = 1.39908616\n",
      "Iteration 27, loss = 1.39614249\n",
      "Iteration 28, loss = 1.10430087\n",
      "Iteration 1, loss = 16.00402688\n",
      "Iteration 29, loss = 1.06865728\n",
      "Iteration 2, loss = 14.39771966\n",
      "Iteration 30, loss = 1.03667577\n",
      "Iteration 3, loss = 10.35389440\n",
      "Iteration 31, loss = 1.11237370\n",
      "Iteration 4, loss = 8.34121690\n",
      "Iteration 32, loss = 1.19997864\n",
      "Iteration 5, loss = 9.14373882\n",
      "Iteration 33, loss = 1.47039475\n",
      "Iteration 6, loss = 9.60155802\n",
      "Iteration 34, loss = 1.08773545\n",
      "Iteration 7, loss = 9.88902727\n",
      "Iteration 35, loss = 0.93309312\n",
      "Iteration 8, loss = 7.27006082\n",
      "Iteration 36, loss = 1.01437194\n",
      "Iteration 9, loss = 6.63629683\n",
      "Iteration 37, loss = 1.10213994\n",
      "Iteration 10, loss = 7.15336651\n",
      "Iteration 38, loss = 0.90163523\n",
      "Iteration 11, loss = 5.10953958\n",
      "Iteration 39, loss = 1.10109101\n",
      "Iteration 12, loss = 5.66053635\n",
      "Iteration 40, loss = 0.72070365\n",
      "Iteration 13, loss = 4.46517126\n",
      "Iteration 41, loss = 0.64470653\n",
      "Iteration 14, loss = 3.64605748\n",
      "Iteration 42, loss = 0.64199584\n",
      "Iteration 15, loss = 4.46606901\n",
      "Iteration 43, loss = 0.72036177\n",
      "Iteration 16, loss = 5.12304591\n",
      "Iteration 44, loss = 0.70761571\n",
      "Iteration 17, loss = 3.01597201\n",
      "Iteration 45, loss = 0.55931097\n",
      "Iteration 18, loss = 3.40263028\n",
      "Iteration 46, loss = 0.60237949\n",
      "Iteration 19, loss = 3.06653627\n",
      "Iteration 47, loss = 0.48369737\n",
      "Iteration 20, loss = 2.56700597\n",
      "Iteration 48, loss = 0.60942618\n",
      "Iteration 21, loss = 2.65886686\n",
      "Iteration 49, loss = 0.53958026\n",
      "Iteration 22, loss = 2.88749227\n",
      "Iteration 50, loss = 0.83895452\n",
      "Iteration 23, loss = 2.21506273\n",
      "Iteration 51, loss = 0.86688586\n",
      "Iteration 24, loss = 2.32483687\n",
      "Iteration 52, loss = 0.94803725\n",
      "Iteration 25, loss = 3.14022199\n",
      "Iteration 53, loss = 0.87298671\n",
      "Iteration 26, loss = 2.81628210\n",
      "Iteration 54, loss = 0.70457634\n",
      "Iteration 27, loss = 2.29564625\n",
      "Iteration 55, loss = 0.60935680\n",
      "Iteration 28, loss = 1.99484780\n",
      "Iteration 56, loss = 0.52374371\n",
      "Iteration 29, loss = 2.02471432\n",
      "Iteration 57, loss = 0.61198640\n",
      "Iteration 30, loss = 2.04893770\n",
      "Iteration 58, loss = 0.47776788\n",
      "Iteration 31, loss = 2.30924390\n",
      "Iteration 59, loss = 0.43252147\n",
      "Iteration 32, loss = 1.45570582\n",
      "Iteration 60, loss = 0.71819445\n",
      "Iteration 33, loss = 2.29799762\n",
      "Iteration 61, loss = 0.43137370\n",
      "Iteration 34, loss = 1.88834256\n",
      "Iteration 62, loss = 0.32800897\n",
      "Iteration 35, loss = 1.29607240\n",
      "Iteration 63, loss = 0.31908096\n",
      "Iteration 36, loss = 1.69407342\n",
      "Iteration 64, loss = 0.28631240\n",
      "Iteration 37, loss = 1.24967588\n",
      "Iteration 65, loss = 0.34523676\n",
      "Iteration 38, loss = 1.19519606\n",
      "Iteration 66, loss = 0.44650683\n",
      "Iteration 39, loss = 1.04351786\n",
      "Iteration 40, loss = 0.70823005\n",
      "Iteration 67, loss = 0.45894474\n",
      "Iteration 41, loss = 0.80588248\n",
      "Iteration 68, loss = 0.31659865\n",
      "Iteration 42, loss = 0.71094949\n",
      "Iteration 69, loss = 0.41884289\n",
      "Iteration 43, loss = 0.65823275\n",
      "Iteration 70, loss = 0.31716276\n",
      "Iteration 44, loss = 0.62938813\n",
      "Iteration 71, loss = 0.33106620\n",
      "Iteration 45, loss = 1.05641891\n",
      "Iteration 72, loss = 0.49172219\n",
      "Iteration 46, loss = 0.71284801\n",
      "Iteration 73, loss = 0.47433163\n",
      "Iteration 47, loss = 0.65871929\n",
      "Iteration 74, loss = 0.60797491\n",
      "Iteration 48, loss = 0.56362522\n",
      "Iteration 75, loss = 0.42861517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.47950606\n",
      "Iteration 50, loss = 0.57393682\n",
      "Iteration 51, loss = 0.96129963\n",
      "Iteration 52, loss = 1.13888685\n",
      "Iteration 53, loss = 1.04986220\n",
      "Iteration 54, loss = 0.52928989\n",
      "Iteration 55, loss = 0.50447698\n",
      "Iteration 56, loss = 0.49843736\n",
      "Iteration 57, loss = 0.49665349\n",
      "Iteration 58, loss = 0.55463984\n",
      "Iteration 59, loss = 0.54430992\n",
      "Iteration 60, loss = 0.45817983\n",
      "Iteration 61, loss = 0.43842506\n",
      "Iteration 62, loss = 0.63238560\n",
      "Iteration 63, loss = 0.69731722\n",
      "Iteration 64, loss = 0.63818426\n",
      "Iteration 65, loss = 0.50379152\n",
      "Iteration 66, loss = 0.57365346\n",
      "Iteration 1, loss = 15.18058077\n",
      "Iteration 67, loss = 0.60905980\n",
      "Iteration 2, loss = 14.72194319\n",
      "Iteration 68, loss = 0.47361752\n",
      "Iteration 3, loss = 9.72757783\n",
      "Iteration 69, loss = 0.53371169\n",
      "Iteration 4, loss = 8.45129889\n",
      "Iteration 70, loss = 0.45790526\n",
      "Iteration 5, loss = 10.48493260\n",
      "Iteration 71, loss = 0.60490071\n",
      "Iteration 6, loss = 9.53851308\n",
      "Iteration 72, loss = 0.52511529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 11.22534989\n",
      "Iteration 8, loss = 7.88738893\n",
      "Iteration 9, loss = 5.98837328\n",
      "Iteration 10, loss = 6.56668773\n",
      "Iteration 11, loss = 4.33259399\n",
      "Iteration 12, loss = 4.72023581\n",
      "Iteration 13, loss = 3.88155018\n",
      "Iteration 14, loss = 3.18192470\n",
      "Iteration 15, loss = 3.08475360\n",
      "Iteration 16, loss = 3.02432026\n",
      "Iteration 17, loss = 2.90503248\n",
      "Iteration 18, loss = 3.01258812\n",
      "Iteration 19, loss = 3.27072511\n",
      "Iteration 20, loss = 2.67838847\n",
      "Iteration 21, loss = 2.95999261\n",
      "Iteration 22, loss = 1.79064741\n",
      "Iteration 23, loss = 2.14243592\n",
      "Iteration 24, loss = 1.96375065\n",
      "Iteration 1, loss = 14.94552832\n",
      "Iteration 25, loss = 1.59266129\n",
      "Iteration 2, loss = 13.38679314\n",
      "Iteration 26, loss = 1.55448930\n",
      "Iteration 3, loss = 12.32516341\n",
      "Iteration 27, loss = 1.79551798\n",
      "Iteration 4, loss = 11.04263905\n",
      "Iteration 28, loss = 1.69809751\n",
      "Iteration 5, loss = 10.53595908\n",
      "Iteration 29, loss = 1.49940680\n",
      "Iteration 6, loss = 6.78626512\n",
      "Iteration 30, loss = 2.47304274\n",
      "Iteration 7, loss = 7.74422812\n",
      "Iteration 31, loss = 1.33339385\n",
      "Iteration 8, loss = 8.74720214\n",
      "Iteration 32, loss = 1.88146047\n",
      "Iteration 9, loss = 7.70444180\n",
      "Iteration 33, loss = 2.10462749\n",
      "Iteration 10, loss = 6.04186338\n",
      "Iteration 34, loss = 1.43323633\n",
      "Iteration 11, loss = 6.25539510\n",
      "Iteration 35, loss = 1.54526138\n",
      "Iteration 12, loss = 7.03525161\n",
      "Iteration 36, loss = 1.03866939\n",
      "Iteration 13, loss = 5.18187560\n",
      "Iteration 37, loss = 1.12958787\n",
      "Iteration 14, loss = 5.36914942\n",
      "Iteration 38, loss = 0.84408142\n",
      "Iteration 15, loss = 5.47227967\n",
      "Iteration 39, loss = 0.87534349\n",
      "Iteration 16, loss = 4.32103888\n",
      "Iteration 40, loss = 1.00432596\n",
      "Iteration 17, loss = 3.90462863\n",
      "Iteration 41, loss = 1.09648579\n",
      "Iteration 18, loss = 4.05620261\n",
      "Iteration 42, loss = 1.62919646\n",
      "Iteration 19, loss = 2.98813830\n",
      "Iteration 43, loss = 2.24638642\n",
      "Iteration 20, loss = 3.08917531\n",
      "Iteration 44, loss = 1.91116383\n",
      "Iteration 21, loss = 2.98694783\n",
      "Iteration 45, loss = 1.91545116\n",
      "Iteration 22, loss = 3.51092403\n",
      "Iteration 46, loss = 1.92300329\n",
      "Iteration 23, loss = 3.57517837\n",
      "Iteration 47, loss = 1.58994512\n",
      "Iteration 24, loss = 2.81304714\n",
      "Iteration 48, loss = 1.69110335\n",
      "Iteration 25, loss = 2.03741549\n",
      "Iteration 49, loss = 1.45424490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 1.83619092\n",
      "Iteration 27, loss = 1.21077654\n",
      "Iteration 28, loss = 1.13343271\n",
      "Iteration 29, loss = 1.18441184\n",
      "Iteration 30, loss = 1.30755458\n",
      "Iteration 31, loss = 1.35120084\n",
      "Iteration 32, loss = 1.18532653\n",
      "Iteration 33, loss = 1.31864800\n",
      "Iteration 34, loss = 1.39020852\n",
      "Iteration 35, loss = 1.52935031\n",
      "Iteration 36, loss = 1.92180678\n",
      "Iteration 37, loss = 1.94069561\n",
      "Iteration 38, loss = 1.44122562\n",
      "Iteration 39, loss = 2.35838439\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.59147028\n",
      "Iteration 2, loss = 13.66690512\n",
      "Iteration 3, loss = 11.77309779\n",
      "Iteration 4, loss = 11.60899557\n",
      "Iteration 5, loss = 8.33887430\n",
      "Iteration 6, loss = 5.71036356\n",
      "Iteration 7, loss = 6.77069830\n",
      "Iteration 8, loss = 6.21640301\n",
      "Iteration 9, loss = 5.21370566\n",
      "Iteration 10, loss = 4.82705230\n",
      "Iteration 11, loss = 4.77223823\n",
      "Iteration 12, loss = 5.12220194\n",
      "Iteration 13, loss = 4.72107990\n",
      "Iteration 14, loss = 3.88461472\n",
      "Iteration 1, loss = 15.67988854\n",
      "Iteration 15, loss = 3.08999875\n",
      "Iteration 2, loss = 13.26447468\n",
      "Iteration 16, loss = 2.86519192\n",
      "Iteration 3, loss = 12.74592717\n",
      "Iteration 17, loss = 3.87229591\n",
      "Iteration 4, loss = 8.84693462\n",
      "Iteration 18, loss = 3.82879264\n",
      "Iteration 5, loss = 12.57901865\n",
      "Iteration 19, loss = 3.05953034\n",
      "Iteration 6, loss = 9.77878751\n",
      "Iteration 20, loss = 3.30275047\n",
      "Iteration 7, loss = 8.17942491\n",
      "Iteration 21, loss = 2.70967133\n",
      "Iteration 8, loss = 6.20780611\n",
      "Iteration 22, loss = 2.88192053\n",
      "Iteration 9, loss = 9.42546146\n",
      "Iteration 23, loss = 2.54636047\n",
      "Iteration 10, loss = 6.32878495\n",
      "Iteration 24, loss = 2.22277420\n",
      "Iteration 11, loss = 4.42224442\n",
      "Iteration 25, loss = 2.15185872\n",
      "Iteration 12, loss = 4.03788235\n",
      "Iteration 26, loss = 1.54874256\n",
      "Iteration 13, loss = 3.96905088\n",
      "Iteration 27, loss = 1.39892519\n",
      "Iteration 14, loss = 4.24689987\n",
      "Iteration 28, loss = 1.02177414\n",
      "Iteration 15, loss = 3.67785364\n",
      "Iteration 29, loss = 0.93242531\n",
      "Iteration 16, loss = 3.29084612\n",
      "Iteration 30, loss = 0.94674812\n",
      "Iteration 17, loss = 3.66370234\n",
      "Iteration 31, loss = 1.19383615\n",
      "Iteration 18, loss = 2.96083737\n",
      "Iteration 32, loss = 1.34810896\n",
      "Iteration 19, loss = 2.28876672\n",
      "Iteration 33, loss = 1.21707177\n",
      "Iteration 20, loss = 2.23423325\n",
      "Iteration 34, loss = 1.10922985\n",
      "Iteration 21, loss = 2.30348568\n",
      "Iteration 35, loss = 1.04633414\n",
      "Iteration 22, loss = 1.77700349\n",
      "Iteration 36, loss = 0.87900117\n",
      "Iteration 23, loss = 1.52338933\n",
      "Iteration 37, loss = 1.09894079\n",
      "Iteration 24, loss = 1.88865955\n",
      "Iteration 38, loss = 0.86111947\n",
      "Iteration 25, loss = 1.63088177\n",
      "Iteration 39, loss = 0.89861071\n",
      "Iteration 26, loss = 1.54516927\n",
      "Iteration 40, loss = 1.16802364\n",
      "Iteration 27, loss = 1.32076450\n",
      "Iteration 41, loss = 1.66171490\n",
      "Iteration 28, loss = 1.18516887\n",
      "Iteration 42, loss = 0.89609858\n",
      "Iteration 29, loss = 2.16238067\n",
      "Iteration 43, loss = 0.81629424\n",
      "Iteration 30, loss = 1.50982510\n",
      "Iteration 44, loss = 0.99785005\n",
      "Iteration 31, loss = 1.59291832\n",
      "Iteration 45, loss = 0.90762833\n",
      "Iteration 32, loss = 1.83351109\n",
      "Iteration 46, loss = 1.00211026\n",
      "Iteration 33, loss = 1.64710000\n",
      "Iteration 47, loss = 0.60379726\n",
      "Iteration 34, loss = 1.89236124\n",
      "Iteration 48, loss = 0.87787426\n",
      "Iteration 35, loss = 1.44164064\n",
      "Iteration 49, loss = 0.64679932\n",
      "Iteration 36, loss = 1.35421232\n",
      "Iteration 50, loss = 1.14362868\n",
      "Iteration 37, loss = 1.71501567\n",
      "Iteration 51, loss = 0.90223925\n",
      "Iteration 38, loss = 1.24734167\n",
      "Iteration 52, loss = 0.64966338\n",
      "Iteration 39, loss = 1.32140203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 1.01676917\n",
      "Iteration 54, loss = 1.20553847\n",
      "Iteration 55, loss = 1.08807746\n",
      "Iteration 56, loss = 1.08368096\n",
      "Iteration 57, loss = 0.82366905\n",
      "Iteration 58, loss = 0.70577212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.86253675\n",
      "Iteration 2, loss = 16.20281250\n",
      "Iteration 3, loss = 12.82774100\n",
      "Iteration 4, loss = 10.45347482\n",
      "Iteration 5, loss = 10.51976803\n",
      "Iteration 6, loss = 8.86861516\n",
      "Iteration 1, loss = 16.25831650\n",
      "Iteration 7, loss = 6.24919464\n",
      "Iteration 2, loss = 16.39715371\n",
      "Iteration 8, loss = 7.48633862\n",
      "Iteration 3, loss = 8.45950039\n",
      "Iteration 9, loss = 6.78697391\n",
      "Iteration 4, loss = 9.44019153\n",
      "Iteration 10, loss = 6.81909770\n",
      "Iteration 5, loss = 7.45245974\n",
      "Iteration 11, loss = 5.41509267\n",
      "Iteration 6, loss = 6.96652445\n",
      "Iteration 12, loss = 5.21802125\n",
      "Iteration 7, loss = 5.58056830\n",
      "Iteration 13, loss = 5.06720037\n",
      "Iteration 8, loss = 8.96971344\n",
      "Iteration 14, loss = 5.02518596\n",
      "Iteration 9, loss = 6.51803757\n",
      "Iteration 15, loss = 3.63855712\n",
      "Iteration 10, loss = 5.36701429\n",
      "Iteration 16, loss = 3.08106044\n",
      "Iteration 11, loss = 5.66204129\n",
      "Iteration 17, loss = 3.02873953\n",
      "Iteration 12, loss = 5.00086763\n",
      "Iteration 18, loss = 4.17029675\n",
      "Iteration 13, loss = 5.43687400\n",
      "Iteration 19, loss = 3.43355145\n",
      "Iteration 14, loss = 4.96747853\n",
      "Iteration 20, loss = 3.16390539\n",
      "Iteration 15, loss = 4.15518636\n",
      "Iteration 21, loss = 3.22915417\n",
      "Iteration 16, loss = 4.61945385\n",
      "Iteration 22, loss = 3.46481566\n",
      "Iteration 17, loss = 4.41082876\n",
      "Iteration 23, loss = 3.39921710\n",
      "Iteration 18, loss = 4.42531859\n",
      "Iteration 24, loss = 2.79057346\n",
      "Iteration 19, loss = 3.64952463\n",
      "Iteration 25, loss = 2.63508656\n",
      "Iteration 20, loss = 3.14836060\n",
      "Iteration 26, loss = 2.03295010\n",
      "Iteration 21, loss = 3.41369798\n",
      "Iteration 27, loss = 2.20295111\n",
      "Iteration 22, loss = 2.74320748\n",
      "Iteration 28, loss = 2.77802063\n",
      "Iteration 23, loss = 2.24879778\n",
      "Iteration 29, loss = 2.67405289\n",
      "Iteration 24, loss = 2.17036184\n",
      "Iteration 30, loss = 2.51224308\n",
      "Iteration 25, loss = 2.03025653\n",
      "Iteration 31, loss = 2.46796212\n",
      "Iteration 26, loss = 2.19745150\n",
      "Iteration 32, loss = 1.73246082\n",
      "Iteration 27, loss = 2.26934312\n",
      "Iteration 33, loss = 1.89209145\n",
      "Iteration 28, loss = 1.94444200\n",
      "Iteration 34, loss = 1.38240904\n",
      "Iteration 29, loss = 2.08743127\n",
      "Iteration 35, loss = 1.72948757\n",
      "Iteration 30, loss = 2.03171663\n",
      "Iteration 36, loss = 1.63656535\n",
      "Iteration 31, loss = 1.89272744\n",
      "Iteration 37, loss = 1.08336539\n",
      "Iteration 32, loss = 1.26115690\n",
      "Iteration 38, loss = 1.08169882\n",
      "Iteration 33, loss = 1.11543474\n",
      "Iteration 39, loss = 1.13638414\n",
      "Iteration 34, loss = 1.00539847\n",
      "Iteration 40, loss = 1.06936245\n",
      "Iteration 41, loss = 0.78646141\n",
      "Iteration 35, loss = 1.44313906\n",
      "Iteration 42, loss = 1.02329803\n",
      "Iteration 36, loss = 1.07176540\n",
      "Iteration 43, loss = 0.86762617\n",
      "Iteration 37, loss = 1.08273542\n",
      "Iteration 44, loss = 1.10557114\n",
      "Iteration 38, loss = 1.06325592\n",
      "Iteration 45, loss = 1.49449956\n",
      "Iteration 39, loss = 0.61991764\n",
      "Iteration 46, loss = 1.41775878\n",
      "Iteration 40, loss = 0.82598058\n",
      "Iteration 47, loss = 1.47670008\n",
      "Iteration 41, loss = 0.95431382\n",
      "Iteration 48, loss = 1.23002429\n",
      "Iteration 42, loss = 0.81879581\n",
      "Iteration 49, loss = 1.37500710\n",
      "Iteration 43, loss = 0.43094429\n",
      "Iteration 50, loss = 1.70073621\n",
      "Iteration 44, loss = 0.75240634\n",
      "Iteration 51, loss = 1.40910086\n",
      "Iteration 45, loss = 1.31515542\n",
      "Iteration 52, loss = 1.34448836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.78895578\n",
      "Iteration 47, loss = 0.72790703\n",
      "Iteration 48, loss = 0.55669586\n",
      "Iteration 49, loss = 0.79381367\n",
      "Iteration 50, loss = 1.35831508\n",
      "Iteration 51, loss = 1.25582194\n",
      "Iteration 52, loss = 0.66490855\n",
      "Iteration 53, loss = 0.54032553\n",
      "Iteration 54, loss = 0.86821089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.20022343\n",
      "Iteration 2, loss = 15.28590169\n",
      "Iteration 3, loss = 7.03920154\n",
      "Iteration 4, loss = 7.07621834\n",
      "Iteration 5, loss = 10.70132272\n",
      "Iteration 6, loss = 8.83860997\n",
      "Iteration 7, loss = 6.06747059\n",
      "Iteration 8, loss = 8.68027549\n",
      "Iteration 9, loss = 8.30488024\n",
      "Iteration 1, loss = 16.92978837\n",
      "Iteration 10, loss = 8.19053831\n",
      "Iteration 2, loss = 20.01502374\n",
      "Iteration 11, loss = 8.63207444\n",
      "Iteration 3, loss = 11.18935419\n",
      "Iteration 12, loss = 7.11644174\n",
      "Iteration 4, loss = 10.35311498\n",
      "Iteration 13, loss = 6.58715534\n",
      "Iteration 5, loss = 9.67637808\n",
      "Iteration 14, loss = 5.82922284\n",
      "Iteration 6, loss = 7.38638892\n",
      "Iteration 15, loss = 5.05241087\n",
      "Iteration 7, loss = 6.86746932\n",
      "Iteration 16, loss = 5.15359739\n",
      "Iteration 8, loss = 7.27353885\n",
      "Iteration 17, loss = 3.79004854\n",
      "Iteration 9, loss = 7.86592968\n",
      "Iteration 18, loss = 4.25160824\n",
      "Iteration 10, loss = 6.20271355\n",
      "Iteration 19, loss = 4.47956111\n",
      "Iteration 11, loss = 6.99621833\n",
      "Iteration 20, loss = 3.97856467\n",
      "Iteration 12, loss = 4.57327600\n",
      "Iteration 21, loss = 3.77496245\n",
      "Iteration 13, loss = 5.12758793\n",
      "Iteration 22, loss = 4.22066926\n",
      "Iteration 14, loss = 3.89259811\n",
      "Iteration 23, loss = 3.13294292\n",
      "Iteration 15, loss = 4.98045415\n",
      "Iteration 24, loss = 3.01980704\n",
      "Iteration 16, loss = 4.50107177\n",
      "Iteration 25, loss = 2.55984005\n",
      "Iteration 17, loss = 4.37405634\n",
      "Iteration 26, loss = 2.29090449\n",
      "Iteration 18, loss = 3.38083581\n",
      "Iteration 27, loss = 2.81735889\n",
      "Iteration 19, loss = 3.47816681\n",
      "Iteration 28, loss = 1.97777344\n",
      "Iteration 20, loss = 3.86207368\n",
      "Iteration 29, loss = 2.00348539\n",
      "Iteration 21, loss = 4.64253294\n",
      "Iteration 30, loss = 1.67550444\n",
      "Iteration 22, loss = 2.61072573\n",
      "Iteration 31, loss = 2.03824639\n",
      "Iteration 23, loss = 2.93969681\n",
      "Iteration 32, loss = 1.93995412\n",
      "Iteration 24, loss = 2.78616067\n",
      "Iteration 33, loss = 1.68700114\n",
      "Iteration 25, loss = 2.21435453\n",
      "Iteration 34, loss = 1.08531788\n",
      "Iteration 26, loss = 1.88687710\n",
      "Iteration 35, loss = 1.17685115\n",
      "Iteration 27, loss = 1.72745151\n",
      "Iteration 36, loss = 0.77258118\n",
      "Iteration 28, loss = 1.79125014\n",
      "Iteration 37, loss = 0.97755632\n",
      "Iteration 29, loss = 1.76071898\n",
      "Iteration 38, loss = 0.97466181\n",
      "Iteration 30, loss = 1.67019454\n",
      "Iteration 39, loss = 1.14451755\n",
      "Iteration 31, loss = 2.26557566\n",
      "Iteration 40, loss = 1.48381978\n",
      "Iteration 32, loss = 2.16885563\n",
      "Iteration 41, loss = 1.21493383\n",
      "Iteration 33, loss = 1.82876565\n",
      "Iteration 42, loss = 1.27806799\n",
      "Iteration 34, loss = 1.67611065\n",
      "Iteration 43, loss = 0.74545514\n",
      "Iteration 35, loss = 1.21587056\n",
      "Iteration 44, loss = 1.47465764\n",
      "Iteration 36, loss = 1.66490760\n",
      "Iteration 45, loss = 1.36678152\n",
      "Iteration 37, loss = 1.15998162\n",
      "Iteration 46, loss = 0.79023864\n",
      "Iteration 38, loss = 0.98185941\n",
      "Iteration 47, loss = 0.81290960\n",
      "Iteration 39, loss = 1.09600110\n",
      "Iteration 48, loss = 0.60482781\n",
      "Iteration 40, loss = 0.82276168\n",
      "Iteration 49, loss = 0.56213254\n",
      "Iteration 41, loss = 0.90827045\n",
      "Iteration 50, loss = 0.56517299\n",
      "Iteration 42, loss = 0.49035291\n",
      "Iteration 51, loss = 0.62472562\n",
      "Iteration 43, loss = 0.84792709\n",
      "Iteration 52, loss = 0.60174508\n",
      "Iteration 44, loss = 1.65277829\n",
      "Iteration 53, loss = 0.97375543\n",
      "Iteration 45, loss = 2.52599378\n",
      "Iteration 54, loss = 1.18322892\n",
      "Iteration 46, loss = 1.69226316\n",
      "Iteration 55, loss = 2.03619284\n",
      "Iteration 47, loss = 1.68682760\n",
      "Iteration 56, loss = 1.97229128\n",
      "Iteration 48, loss = 2.00950205\n",
      "Iteration 57, loss = 1.71247112\n",
      "Iteration 49, loss = 1.16305315\n",
      "Iteration 58, loss = 1.36627269\n",
      "Iteration 50, loss = 1.06510482\n",
      "Iteration 59, loss = 1.70811334\n",
      "Iteration 51, loss = 1.00471992\n",
      "Iteration 60, loss = 1.22590321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.67331329\n",
      "Iteration 53, loss = 1.17654097\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.48519278\n",
      "Iteration 2, loss = 15.55753691\n",
      "Iteration 1, loss = 18.12234337\n",
      "Iteration 3, loss = 9.02560043\n",
      "Iteration 2, loss = 20.13217267\n",
      "Iteration 4, loss = 7.40931816\n",
      "Iteration 3, loss = 11.46788244\n",
      "Iteration 5, loss = 7.66700822\n",
      "Iteration 4, loss = 8.43992765\n",
      "Iteration 6, loss = 8.29300563\n",
      "Iteration 5, loss = 7.56840435\n",
      "Iteration 7, loss = 6.71791106\n",
      "Iteration 6, loss = 5.58168213\n",
      "Iteration 8, loss = 5.34209859\n",
      "Iteration 7, loss = 7.79545710\n",
      "Iteration 9, loss = 4.87125731\n",
      "Iteration 8, loss = 7.76561182\n",
      "Iteration 10, loss = 5.81484914\n",
      "Iteration 9, loss = 6.78153548\n",
      "Iteration 11, loss = 5.34524677\n",
      "Iteration 10, loss = 5.74595653\n",
      "Iteration 12, loss = 3.92364629\n",
      "Iteration 11, loss = 7.81592479\n",
      "Iteration 13, loss = 3.34903404\n",
      "Iteration 12, loss = 6.52508409\n",
      "Iteration 14, loss = 4.78474611\n",
      "Iteration 13, loss = 5.31882530\n",
      "Iteration 15, loss = 3.71778106\n",
      "Iteration 14, loss = 4.85373190\n",
      "Iteration 16, loss = 3.57687353\n",
      "Iteration 15, loss = 3.31170693\n",
      "Iteration 17, loss = 2.74333656\n",
      "Iteration 16, loss = 3.69530490\n",
      "Iteration 18, loss = 2.45936901\n",
      "Iteration 17, loss = 3.04487147\n",
      "Iteration 19, loss = 2.58975208\n",
      "Iteration 18, loss = 2.79769023\n",
      "Iteration 20, loss = 3.60553938\n",
      "Iteration 19, loss = 2.20271601\n",
      "Iteration 21, loss = 3.07363036\n",
      "Iteration 20, loss = 2.18002998\n",
      "Iteration 22, loss = 2.40166075\n",
      "Iteration 21, loss = 1.94591471\n",
      "Iteration 23, loss = 2.10261004\n",
      "Iteration 22, loss = 2.09956520\n",
      "Iteration 24, loss = 2.93595776\n",
      "Iteration 23, loss = 1.70844365\n",
      "Iteration 25, loss = 1.44747001\n",
      "Iteration 24, loss = 1.37653668\n",
      "Iteration 26, loss = 2.22525610\n",
      "Iteration 25, loss = 1.81158413\n",
      "Iteration 27, loss = 1.68548169\n",
      "Iteration 26, loss = 1.79967928\n",
      "Iteration 28, loss = 1.72710043\n",
      "Iteration 27, loss = 1.78476819\n",
      "Iteration 29, loss = 1.86545032\n",
      "Iteration 28, loss = 1.94421514\n",
      "Iteration 30, loss = 1.33511209\n",
      "Iteration 29, loss = 1.59299710\n",
      "Iteration 31, loss = 1.24683058\n",
      "Iteration 30, loss = 1.44413404\n",
      "Iteration 32, loss = 1.24885826\n",
      "Iteration 31, loss = 1.26659290\n",
      "Iteration 33, loss = 1.35400754\n",
      "Iteration 32, loss = 1.20727140\n",
      "Iteration 34, loss = 1.09802696\n",
      "Iteration 33, loss = 1.39785618\n",
      "Iteration 35, loss = 1.13361441\n",
      "Iteration 34, loss = 1.50167354\n",
      "Iteration 36, loss = 0.82167631\n",
      "Iteration 35, loss = 1.31769295\n",
      "Iteration 37, loss = 1.26671324\n",
      "Iteration 36, loss = 0.84934312\n",
      "Iteration 38, loss = 1.40121593\n",
      "Iteration 37, loss = 0.93027997\n",
      "Iteration 39, loss = 1.15505480\n",
      "Iteration 38, loss = 0.73184014\n",
      "Iteration 40, loss = 0.93002470\n",
      "Iteration 39, loss = 0.80917629\n",
      "Iteration 41, loss = 0.79112250\n",
      "Iteration 40, loss = 0.62475567\n",
      "Iteration 42, loss = 0.74555313\n",
      "Iteration 41, loss = 0.63020364\n",
      "Iteration 43, loss = 1.12054515\n",
      "Iteration 42, loss = 0.53778840\n",
      "Iteration 44, loss = 0.94881896\n",
      "Iteration 43, loss = 0.51707250\n",
      "Iteration 45, loss = 1.07906066\n",
      "Iteration 44, loss = 0.49767941\n",
      "Iteration 46, loss = 0.65739060\n",
      "Iteration 45, loss = 0.40117525\n",
      "Iteration 47, loss = 0.53861317\n",
      "Iteration 46, loss = 0.42048679\n",
      "Iteration 48, loss = 0.55874233\n",
      "Iteration 47, loss = 0.37985940\n",
      "Iteration 49, loss = 0.54721481\n",
      "Iteration 48, loss = 0.61861534\n",
      "Iteration 50, loss = 0.68244164\n",
      "Iteration 49, loss = 0.57056737\n",
      "Iteration 51, loss = 0.85360688\n",
      "Iteration 50, loss = 0.48072324\n",
      "Iteration 52, loss = 0.69951661\n",
      "Iteration 51, loss = 0.51756854\n",
      "Iteration 53, loss = 0.83660816\n",
      "Iteration 52, loss = 0.58248065\n",
      "Iteration 54, loss = 0.85899287\n",
      "Iteration 53, loss = 0.63699345\n",
      "Iteration 55, loss = 1.10162940\n",
      "Iteration 54, loss = 0.82687068\n",
      "Iteration 56, loss = 1.00703318\n",
      "Iteration 55, loss = 0.69747066\n",
      "Iteration 57, loss = 1.05450332\n",
      "Iteration 56, loss = 0.62039395\n",
      "Iteration 58, loss = 1.11771067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.63689745\n",
      "Iteration 58, loss = 0.42947653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.20551723\n",
      "Iteration 2, loss = 17.71784765\n",
      "Iteration 1, loss = 13.99341248\n",
      "Iteration 3, loss = 12.75182038\n",
      "Iteration 2, loss = 23.06098788\n",
      "Iteration 4, loss = 10.63929217\n",
      "Iteration 3, loss = 13.40594866\n",
      "Iteration 5, loss = 10.19941212\n",
      "Iteration 4, loss = 7.16586579\n",
      "Iteration 6, loss = 7.78136149\n",
      "Iteration 5, loss = 7.86126418\n",
      "Iteration 7, loss = 6.54999017\n",
      "Iteration 6, loss = 7.39097265\n",
      "Iteration 8, loss = 6.54541456\n",
      "Iteration 7, loss = 11.94496018\n",
      "Iteration 9, loss = 6.49780690\n",
      "Iteration 8, loss = 6.96237455\n",
      "Iteration 10, loss = 4.96638444\n",
      "Iteration 9, loss = 4.93305775\n",
      "Iteration 11, loss = 4.75898802\n",
      "Iteration 10, loss = 4.47651953\n",
      "Iteration 12, loss = 3.98917760\n",
      "Iteration 11, loss = 4.56773894\n",
      "Iteration 13, loss = 4.49752526\n",
      "Iteration 12, loss = 4.84928206\n",
      "Iteration 14, loss = 4.73036892\n",
      "Iteration 13, loss = 5.54029261\n",
      "Iteration 15, loss = 3.46187397\n",
      "Iteration 14, loss = 5.66632773\n",
      "Iteration 16, loss = 3.94513281\n",
      "Iteration 15, loss = 3.86290809\n",
      "Iteration 17, loss = 3.80540219\n",
      "Iteration 16, loss = 3.54957821\n",
      "Iteration 18, loss = 3.92934017\n",
      "Iteration 17, loss = 3.28897834\n",
      "Iteration 19, loss = 3.08883172\n",
      "Iteration 18, loss = 3.08408599\n",
      "Iteration 20, loss = 2.80137160\n",
      "Iteration 19, loss = 2.75026994\n",
      "Iteration 21, loss = 2.32692264\n",
      "Iteration 20, loss = 2.80250177\n",
      "Iteration 22, loss = 2.13898857\n",
      "Iteration 21, loss = 3.30677331\n",
      "Iteration 23, loss = 2.58935136\n",
      "Iteration 22, loss = 2.86008278\n",
      "Iteration 24, loss = 1.80214785\n",
      "Iteration 23, loss = 2.44422885\n",
      "Iteration 25, loss = 2.00136900\n",
      "Iteration 24, loss = 2.00264999\n",
      "Iteration 26, loss = 1.68900610\n",
      "Iteration 25, loss = 1.91982477\n",
      "Iteration 27, loss = 1.39231159\n",
      "Iteration 26, loss = 1.98814527\n",
      "Iteration 28, loss = 1.59818816\n",
      "Iteration 27, loss = 1.44280166\n",
      "Iteration 29, loss = 1.07532807\n",
      "Iteration 28, loss = 1.45905553\n",
      "Iteration 30, loss = 1.34110670\n",
      "Iteration 29, loss = 1.63705532\n",
      "Iteration 31, loss = 1.20940508\n",
      "Iteration 30, loss = 1.94092637\n",
      "Iteration 32, loss = 1.52605983\n",
      "Iteration 31, loss = 2.04418205\n",
      "Iteration 33, loss = 1.34192217\n",
      "Iteration 32, loss = 1.99895136\n",
      "Iteration 34, loss = 1.35594401\n",
      "Iteration 33, loss = 1.65563720\n",
      "Iteration 35, loss = 1.21267031\n",
      "Iteration 34, loss = 2.18031569\n",
      "Iteration 36, loss = 0.95055309\n",
      "Iteration 35, loss = 2.48643361\n",
      "Iteration 37, loss = 0.86280236\n",
      "Iteration 36, loss = 1.93868373\n",
      "Iteration 38, loss = 1.58683522\n",
      "Iteration 37, loss = 2.15617865\n",
      "Iteration 39, loss = 1.44401701\n",
      "Iteration 38, loss = 2.23886762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.95348479\n",
      "Iteration 41, loss = 0.91805106\n",
      "Iteration 42, loss = 0.91970582\n",
      "Iteration 43, loss = 1.15642131\n",
      "Iteration 44, loss = 1.17096602\n",
      "Iteration 45, loss = 1.62950372\n",
      "Iteration 46, loss = 1.35252056\n",
      "Iteration 47, loss = 0.89884879\n",
      "Iteration 48, loss = 0.66028243\n",
      "Iteration 49, loss = 0.63840114\n",
      "Iteration 50, loss = 1.06242350\n",
      "Iteration 51, loss = 0.86158440\n",
      "Iteration 52, loss = 0.85387565\n",
      "Iteration 53, loss = 0.72324994\n",
      "Iteration 54, loss = 0.74676866\n",
      "Iteration 55, loss = 0.64267325\n",
      "Iteration 56, loss = 0.88484481\n",
      "Iteration 57, loss = 0.62285758\n",
      "Iteration 1, loss = 19.46881783\n",
      "Iteration 58, loss = 0.38350489\n",
      "Iteration 2, loss = 14.86813186\n",
      "Iteration 59, loss = 0.49525752\n",
      "Iteration 3, loss = 10.82940667\n",
      "Iteration 60, loss = 0.41934734\n",
      "Iteration 4, loss = 13.45074645\n",
      "Iteration 61, loss = 0.64232456\n",
      "Iteration 5, loss = 9.70152631\n",
      "Iteration 62, loss = 0.42470373\n",
      "Iteration 6, loss = 7.39116293\n",
      "Iteration 63, loss = 0.49324029\n",
      "Iteration 7, loss = 7.17198583\n",
      "Iteration 64, loss = 0.39012653\n",
      "Iteration 8, loss = 6.46667349\n",
      "Iteration 65, loss = 0.58821087\n",
      "Iteration 9, loss = 6.37053528\n",
      "Iteration 66, loss = 0.56948735\n",
      "Iteration 10, loss = 6.40783894\n",
      "Iteration 67, loss = 0.50600309\n",
      "Iteration 11, loss = 6.73628783\n",
      "Iteration 68, loss = 0.88534555\n",
      "Iteration 12, loss = 5.68444002\n",
      "Iteration 69, loss = 0.50283963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 4.64882742\n",
      "Iteration 14, loss = 5.22044037\n",
      "Iteration 15, loss = 6.25548180\n",
      "Iteration 16, loss = 5.42676781\n",
      "Iteration 17, loss = 4.10942317\n",
      "Iteration 18, loss = 3.17353331\n",
      "Iteration 19, loss = 4.05512222\n",
      "Iteration 20, loss = 4.06530802\n",
      "Iteration 21, loss = 3.74839713\n",
      "Iteration 22, loss = 3.09079472\n",
      "Iteration 23, loss = 2.62130743\n",
      "Iteration 24, loss = 2.68632725\n",
      "Iteration 25, loss = 3.23827900\n",
      "Iteration 26, loss = 2.48576868\n",
      "Iteration 27, loss = 2.64976331\n",
      "Iteration 28, loss = 2.02941776\n",
      "Iteration 29, loss = 2.44933569\n",
      "Iteration 30, loss = 1.93006693\n",
      "Iteration 1, loss = 17.20916467\n",
      "Iteration 31, loss = 1.96857252\n",
      "Iteration 32, loss = 2.01519482Iteration 2, loss = 13.67419308\n",
      "\n",
      "Iteration 3, loss = 9.31310310\n",
      "Iteration 33, loss = 1.51232065\n",
      "Iteration 4, loss = 7.97056242\n",
      "Iteration 34, loss = 1.59734474\n",
      "Iteration 5, loss = 5.41231530\n",
      "Iteration 35, loss = 1.34043918\n",
      "Iteration 6, loss = 6.41760671\n",
      "Iteration 36, loss = 0.97995696\n",
      "Iteration 7, loss = 8.68353857\n",
      "Iteration 37, loss = 1.36812750\n",
      "Iteration 8, loss = 6.06170536\n",
      "Iteration 38, loss = 1.64534240\n",
      "Iteration 9, loss = 10.39346222\n",
      "Iteration 39, loss = 1.47103773\n",
      "Iteration 10, loss = 8.09717479\n",
      "Iteration 40, loss = 1.44348963\n",
      "Iteration 11, loss = 5.82466827\n",
      "Iteration 41, loss = 1.58326645\n",
      "Iteration 12, loss = 5.55088855\n",
      "Iteration 42, loss = 1.65297804\n",
      "Iteration 13, loss = 4.20031267\n",
      "Iteration 43, loss = 1.18061601\n",
      "Iteration 14, loss = 3.95180832\n",
      "Iteration 44, loss = 1.20269274\n",
      "Iteration 15, loss = 3.83695068\n",
      "Iteration 45, loss = 1.01088066\n",
      "Iteration 16, loss = 3.21716288\n",
      "Iteration 46, loss = 1.36437716\n",
      "Iteration 17, loss = 2.68723109\n",
      "Iteration 47, loss = 1.46490238\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 2.43216505\n",
      "Iteration 19, loss = 2.36342831\n",
      "Iteration 20, loss = 2.24375687\n",
      "Iteration 21, loss = 3.11669183\n",
      "Iteration 22, loss = 2.38641621\n",
      "Iteration 23, loss = 2.54447491\n",
      "Iteration 24, loss = 2.29600405\n",
      "Iteration 25, loss = 1.69363213\n",
      "Iteration 26, loss = 1.91847719\n",
      "Iteration 27, loss = 1.61846466\n",
      "Iteration 28, loss = 1.72160034\n",
      "Iteration 29, loss = 1.46996313\n",
      "Iteration 30, loss = 1.96169904\n",
      "Iteration 31, loss = 1.27483246\n",
      "Iteration 32, loss = 1.15505120\n",
      "Iteration 33, loss = 1.22817448\n",
      "Iteration 34, loss = 1.01619302\n",
      "Iteration 35, loss = 1.21177728\n",
      "Iteration 1, loss = 14.14514281\n",
      "Iteration 36, loss = 0.77315381\n",
      "Iteration 2, loss = 12.24146885\n",
      "Iteration 37, loss = 1.24548411\n",
      "Iteration 3, loss = 11.03428733\n",
      "Iteration 38, loss = 1.09875166\n",
      "Iteration 4, loss = 7.64029415\n",
      "Iteration 39, loss = 0.85790742\n",
      "Iteration 5, loss = 6.34841441\n",
      "Iteration 40, loss = 0.99663307\n",
      "Iteration 6, loss = 8.50511674\n",
      "Iteration 41, loss = 1.23184054\n",
      "Iteration 7, loss = 7.76774619\n",
      "Iteration 42, loss = 1.66555642\n",
      "Iteration 8, loss = 6.91236439\n",
      "Iteration 43, loss = 0.98268923\n",
      "Iteration 9, loss = 6.63926539\n",
      "Iteration 44, loss = 0.92333083\n",
      "Iteration 10, loss = 5.37897170\n",
      "Iteration 45, loss = 1.03586681\n",
      "Iteration 11, loss = 5.16070419\n",
      "Iteration 46, loss = 1.22173298\n",
      "Iteration 12, loss = 6.82598547\n",
      "Iteration 47, loss = 1.07872708\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 4.74738394\n",
      "Iteration 14, loss = 3.94347223\n",
      "Iteration 15, loss = 3.21941545\n",
      "Iteration 16, loss = 3.28854015\n",
      "Iteration 17, loss = 3.04595285\n",
      "Iteration 18, loss = 2.63958644\n",
      "Iteration 19, loss = 2.39731317\n",
      "Iteration 20, loss = 2.47621384\n",
      "Iteration 21, loss = 2.44815946\n",
      "Iteration 22, loss = 2.54554814\n",
      "Iteration 23, loss = 2.71272999\n",
      "Iteration 24, loss = 2.35432418\n",
      "Iteration 25, loss = 2.04441001\n",
      "Iteration 26, loss = 1.60631554\n",
      "Iteration 27, loss = 1.68444677\n",
      "Iteration 28, loss = 1.90661200\n",
      "Iteration 29, loss = 1.88478299\n",
      "Iteration 30, loss = 1.89045668\n",
      "Iteration 31, loss = 2.60769203\n",
      "Iteration 1, loss = 15.77526600\n",
      "Iteration 32, loss = 2.24525907\n",
      "Iteration 2, loss = 12.14073901\n",
      "Iteration 33, loss = 1.43545515\n",
      "Iteration 3, loss = 9.35091257\n",
      "Iteration 34, loss = 1.22760300\n",
      "Iteration 4, loss = 11.55125462\n",
      "Iteration 35, loss = 1.23423388\n",
      "Iteration 5, loss = 9.94871106\n",
      "Iteration 36, loss = 1.84303365\n",
      "Iteration 6, loss = 10.80125716\n",
      "Iteration 37, loss = 2.27434975\n",
      "Iteration 7, loss = 6.01552905\n",
      "Iteration 38, loss = 1.94022596\n",
      "Iteration 8, loss = 5.41086170\n",
      "Iteration 39, loss = 1.30580827\n",
      "Iteration 9, loss = 5.05843399\n",
      "Iteration 40, loss = 1.03795581\n",
      "Iteration 10, loss = 4.52211963\n",
      "Iteration 41, loss = 0.96258559\n",
      "Iteration 11, loss = 5.59622941\n",
      "Iteration 42, loss = 1.20817575\n",
      "Iteration 12, loss = 5.64003006\n",
      "Iteration 43, loss = 0.74601548\n",
      "Iteration 13, loss = 5.16204814\n",
      "Iteration 44, loss = 0.62793370\n",
      "Iteration 14, loss = 5.07036387\n",
      "Iteration 45, loss = 0.68274268\n",
      "Iteration 15, loss = 4.44773426\n",
      "Iteration 46, loss = 0.98429170\n",
      "Iteration 16, loss = 3.90491645\n",
      "Iteration 47, loss = 1.26450454\n",
      "Iteration 17, loss = 2.73273885\n",
      "Iteration 48, loss = 0.73022674\n",
      "Iteration 18, loss = 2.67341192\n",
      "Iteration 49, loss = 0.87011182\n",
      "Iteration 19, loss = 2.33726891\n",
      "Iteration 50, loss = 1.09527897\n",
      "Iteration 20, loss = 2.25913124\n",
      "Iteration 51, loss = 1.28294323\n",
      "Iteration 21, loss = 1.54912852\n",
      "Iteration 52, loss = 1.28557783\n",
      "Iteration 22, loss = 2.51361860\n",
      "Iteration 53, loss = 1.06165806\n",
      "Iteration 23, loss = 2.83389772\n",
      "Iteration 54, loss = 0.86604608\n",
      "Iteration 24, loss = 2.52607175\n",
      "Iteration 55, loss = 1.20670572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 25, loss = 2.83561952\n",
      "Iteration 26, loss = 2.28956875\n",
      "Iteration 27, loss = 2.56954213\n",
      "Iteration 28, loss = 2.98234004\n",
      "Iteration 29, loss = 1.80959551\n",
      "Iteration 30, loss = 1.53525051\n",
      "Iteration 31, loss = 1.53743315\n",
      "Iteration 32, loss = 1.45747141\n",
      "Iteration 33, loss = 1.39212213\n",
      "Iteration 34, loss = 1.64022404\n",
      "Iteration 35, loss = 1.20884556\n",
      "Iteration 36, loss = 0.88146867\n",
      "Iteration 37, loss = 0.83797103\n",
      "Iteration 38, loss = 0.91408913\n",
      "Iteration 39, loss = 1.11545525\n",
      "Iteration 40, loss = 2.09745273\n",
      "Iteration 41, loss = 1.14622733\n",
      "Iteration 42, loss = 0.96194817\n",
      "Iteration 43, loss = 1.28444974\n",
      "Iteration 1, loss = 16.84123011\n",
      "Iteration 44, loss = 1.36449454\n",
      "Iteration 2, loss = 14.04131644\n",
      "Iteration 45, loss = 1.78006432\n",
      "Iteration 3, loss = 11.61581979\n",
      "Iteration 46, loss = 1.22029456\n",
      "Iteration 4, loss = 8.84962889\n",
      "Iteration 47, loss = 1.34102002\n",
      "Iteration 5, loss = 9.38836875\n",
      "Iteration 48, loss = 1.44568526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 9.03616128\n",
      "Iteration 7, loss = 6.29978204\n",
      "Iteration 8, loss = 5.44751387\n",
      "Iteration 9, loss = 6.21191078\n",
      "Iteration 10, loss = 5.43413390\n",
      "Iteration 11, loss = 5.44277080\n",
      "Iteration 12, loss = 4.44790474\n",
      "Iteration 13, loss = 4.40169308\n",
      "Iteration 14, loss = 3.12513291\n",
      "Iteration 15, loss = 3.06629214\n",
      "Iteration 16, loss = 2.94154425\n",
      "Iteration 17, loss = 3.70857180\n",
      "Iteration 18, loss = 3.20704476\n",
      "Iteration 19, loss = 3.08580226\n",
      "Iteration 20, loss = 2.45505249\n",
      "Iteration 21, loss = 2.30504338\n",
      "Iteration 22, loss = 2.26610480\n",
      "Iteration 23, loss = 2.23390421\n",
      "Iteration 24, loss = 2.09735124\n",
      "Iteration 1, loss = 19.73209847\n",
      "Iteration 25, loss = 2.48436238\n",
      "Iteration 2, loss = 15.24320481\n",
      "Iteration 26, loss = 1.35754474\n",
      "Iteration 3, loss = 14.27823469\n",
      "Iteration 27, loss = 1.50780532\n",
      "Iteration 4, loss = 9.93927205\n",
      "Iteration 28, loss = 1.27332124\n",
      "Iteration 5, loss = 6.59937577\n",
      "Iteration 29, loss = 1.83388002\n",
      "Iteration 6, loss = 7.07817798\n",
      "Iteration 30, loss = 1.67284108\n",
      "Iteration 7, loss = 6.49312750\n",
      "Iteration 31, loss = 2.00273722\n",
      "Iteration 8, loss = 9.20557920\n",
      "Iteration 32, loss = 1.64668000\n",
      "Iteration 9, loss = 6.76890726\n",
      "Iteration 10, loss = 6.56612056\n",
      "Iteration 33, loss = 1.33509358\n",
      "Iteration 11, loss = 4.70771280\n",
      "Iteration 34, loss = 1.50720764\n",
      "Iteration 12, loss = 5.34532223\n",
      "Iteration 35, loss = 1.52335344\n",
      "Iteration 13, loss = 4.49321547\n",
      "Iteration 36, loss = 1.53578457\n",
      "Iteration 14, loss = 4.35728248\n",
      "Iteration 37, loss = 1.54078219\n",
      "Iteration 15, loss = 3.69898553\n",
      "Iteration 38, loss = 1.11437469\n",
      "Iteration 16, loss = 4.03502010\n",
      "Iteration 39, loss = 0.84509586\n",
      "Iteration 17, loss = 3.66755933\n",
      "Iteration 40, loss = 0.87480378\n",
      "Iteration 18, loss = 2.80378377\n",
      "Iteration 41, loss = 0.85879414\n",
      "Iteration 19, loss = 3.49338952\n",
      "Iteration 42, loss = 0.68953451\n",
      "Iteration 20, loss = 2.79866700\n",
      "Iteration 43, loss = 0.86312408\n",
      "Iteration 21, loss = 4.42336134\n",
      "Iteration 44, loss = 0.53730068\n",
      "Iteration 22, loss = 4.11172532\n",
      "Iteration 45, loss = 0.95116892\n",
      "Iteration 23, loss = 3.11336282\n",
      "Iteration 46, loss = 0.77818705\n",
      "Iteration 24, loss = 3.68960252\n",
      "Iteration 47, loss = 1.78281591\n",
      "Iteration 25, loss = 2.76262014\n",
      "Iteration 48, loss = 1.75255609\n",
      "Iteration 26, loss = 2.49614162\n",
      "Iteration 49, loss = 1.77421307\n",
      "Iteration 27, loss = 2.72495669\n",
      "Iteration 50, loss = 0.90892449\n",
      "Iteration 28, loss = 2.77567180\n",
      "Iteration 51, loss = 0.81288024\n",
      "Iteration 29, loss = 2.27202482\n",
      "Iteration 52, loss = 0.88489404\n",
      "Iteration 30, loss = 1.49178531\n",
      "Iteration 53, loss = 1.48527066\n",
      "Iteration 31, loss = 2.03682291\n",
      "Iteration 54, loss = 1.77115279\n",
      "Iteration 32, loss = 1.91514704\n",
      "Iteration 55, loss = 1.79920607\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 1.73374198\n",
      "Iteration 34, loss = 1.85122736\n",
      "Iteration 35, loss = 1.71008632\n",
      "Iteration 36, loss = 0.99212755\n",
      "Iteration 37, loss = 1.33682930\n",
      "Iteration 38, loss = 1.83006929\n",
      "Iteration 39, loss = 1.73656067\n",
      "Iteration 40, loss = 1.41561591\n",
      "Iteration 41, loss = 1.77884764\n",
      "Iteration 42, loss = 1.80306288\n",
      "Iteration 43, loss = 1.78190185\n",
      "Iteration 44, loss = 1.38668727\n",
      "Iteration 45, loss = 0.98890811\n",
      "Iteration 46, loss = 0.85373325\n",
      "Iteration 47, loss = 0.91079933\n",
      "Iteration 48, loss = 0.91590809\n",
      "Iteration 49, loss = 0.97498486\n",
      "Iteration 50, loss = 0.75852042\n",
      "Iteration 1, loss = 18.65863417\n",
      "Iteration 51, loss = 1.55433857\n",
      "Iteration 2, loss = 17.53785770\n",
      "Iteration 52, loss = 1.69545546\n",
      "Iteration 3, loss = 12.50300809\n",
      "Iteration 53, loss = 1.37139179\n",
      "Iteration 4, loss = 8.24219511\n",
      "Iteration 54, loss = 1.18466091\n",
      "Iteration 5, loss = 7.76113499\n",
      "Iteration 55, loss = 0.84292530\n",
      "Iteration 6, loss = 7.91341514\n",
      "Iteration 56, loss = 0.72146253\n",
      "Iteration 7, loss = 10.25500603\n",
      "Iteration 57, loss = 0.81331765\n",
      "Iteration 8, loss = 8.82639726\n",
      "Iteration 58, loss = 1.27585558\n",
      "Iteration 9, loss = 7.08444386\n",
      "Iteration 59, loss = 1.15153286\n",
      "Iteration 10, loss = 7.00603753\n",
      "Iteration 60, loss = 1.68674773\n",
      "Iteration 11, loss = 9.96503552\n",
      "Iteration 61, loss = 1.20303121\n",
      "Iteration 12, loss = 7.67065787\n",
      "Iteration 62, loss = 1.23845844\n",
      "Iteration 13, loss = 8.93178319\n",
      "Iteration 63, loss = 0.94780223\n",
      "Iteration 14, loss = 5.85293180\n",
      "Iteration 64, loss = 0.75824437\n",
      "Iteration 15, loss = 7.01226407\n",
      "Iteration 65, loss = 1.05932534\n",
      "Iteration 16, loss = 5.50884853\n",
      "Iteration 66, loss = 0.75702167\n",
      "Iteration 17, loss = 5.89074202\n",
      "Iteration 67, loss = 0.87548788\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 5.87055143\n",
      "Iteration 19, loss = 6.27460834\n",
      "Iteration 20, loss = 5.92897187\n",
      "Iteration 21, loss = 6.01371214\n",
      "Iteration 22, loss = 5.46812144\n",
      "Iteration 23, loss = 4.92114126\n",
      "Iteration 24, loss = 5.43399289\n",
      "Iteration 25, loss = 5.04343377\n",
      "Iteration 26, loss = 5.75088825\n",
      "Iteration 27, loss = 5.39929552\n",
      "Iteration 28, loss = 4.18383546\n",
      "Iteration 29, loss = 4.10627349\n",
      "Iteration 30, loss = 5.20597156\n",
      "Iteration 31, loss = 5.95203617\n",
      "Iteration 32, loss = 4.85265512\n",
      "Iteration 33, loss = 5.24535813\n",
      "Iteration 34, loss = 4.81357831\n",
      "Iteration 35, loss = 4.38156985\n",
      "Iteration 1, loss = 17.63923035\n",
      "Iteration 36, loss = 4.30305780\n",
      "Iteration 2, loss = 11.99345575\n",
      "Iteration 37, loss = 6.14813238\n",
      "Iteration 3, loss = 10.92710819\n",
      "Iteration 38, loss = 4.89099793\n",
      "Iteration 4, loss = 10.06604815\n",
      "Iteration 39, loss = 4.57612435\n",
      "Iteration 5, loss = 10.04237560\n",
      "Iteration 40, loss = 4.81295702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 10.64891381\n",
      "Iteration 7, loss = 8.40177303\n",
      "Iteration 8, loss = 12.71525749\n",
      "Iteration 9, loss = 9.23762463\n",
      "Iteration 10, loss = 10.00393373\n",
      "Iteration 11, loss = 9.22479001\n",
      "Iteration 12, loss = 8.25182824\n",
      "Iteration 13, loss = 7.12040246\n",
      "Iteration 14, loss = 6.96125540\n",
      "Iteration 15, loss = 7.54006986\n",
      "Iteration 16, loss = 8.22592795\n",
      "Iteration 17, loss = 5.92234523\n",
      "Iteration 18, loss = 6.24263163\n",
      "Iteration 19, loss = 6.01153073\n",
      "Iteration 20, loss = 7.62223799\n",
      "Iteration 21, loss = 6.76154383\n",
      "Iteration 22, loss = 6.52830378\n",
      "Iteration 23, loss = 6.88365836\n",
      "Iteration 1, loss = 16.27615960\n",
      "Iteration 24, loss = 6.80867550\n",
      "Iteration 2, loss = 15.09997616\n",
      "Iteration 25, loss = 6.77156017\n",
      "Iteration 3, loss = 12.18702076\n",
      "Iteration 26, loss = 5.71445679\n",
      "Iteration 4, loss = 10.12341998\n",
      "Iteration 27, loss = 6.54146460\n",
      "Iteration 5, loss = 11.06639066\n",
      "Iteration 28, loss = 5.56316853\n",
      "Iteration 6, loss = 8.83045706\n",
      "Iteration 29, loss = 5.36857176\n",
      "Iteration 7, loss = 8.62937877\n",
      "Iteration 30, loss = 6.11500404\n",
      "Iteration 8, loss = 7.60871051\n",
      "Iteration 31, loss = 5.56736278\n",
      "Iteration 9, loss = 6.64831669\n",
      "Iteration 32, loss = 7.05001562\n",
      "Iteration 10, loss = 5.67381557\n",
      "Iteration 33, loss = 7.64719671\n",
      "Iteration 11, loss = 6.39920244\n",
      "Iteration 34, loss = 5.80338519\n",
      "Iteration 12, loss = 6.08999537\n",
      "Iteration 35, loss = 6.27420480\n",
      "Iteration 13, loss = 6.04892387\n",
      "Iteration 36, loss = 5.45198963\n",
      "Iteration 14, loss = 5.60907309\n",
      "Iteration 37, loss = 4.55188701\n",
      "Iteration 15, loss = 6.57332414\n",
      "Iteration 38, loss = 5.76997792\n",
      "Iteration 16, loss = 6.11385308\n",
      "Iteration 39, loss = 6.43936461\n",
      "Iteration 17, loss = 5.14166625\n",
      "Iteration 40, loss = 4.63741225\n",
      "Iteration 18, loss = 6.56098199\n",
      "Iteration 41, loss = 5.38390332\n",
      "Iteration 19, loss = 6.25419988\n",
      "Iteration 42, loss = 5.10999539\n",
      "Iteration 20, loss = 4.22704075\n",
      "Iteration 43, loss = 4.75713048\n",
      "Iteration 21, loss = 5.84643652\n",
      "Iteration 44, loss = 5.28770125\n",
      "Iteration 22, loss = 4.52058279\n",
      "Iteration 45, loss = 5.54230066\n",
      "Iteration 23, loss = 5.27524192\n",
      "Iteration 46, loss = 5.50477814\n",
      "Iteration 24, loss = 4.89096006\n",
      "Iteration 47, loss = 5.48473827\n",
      "Iteration 25, loss = 4.46386447\n",
      "Iteration 48, loss = 5.46886525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 26, loss = 4.97875452\n",
      "Iteration 27, loss = 4.86577973\n",
      "Iteration 28, loss = 4.71542540\n",
      "Iteration 29, loss = 5.35033634\n",
      "Iteration 30, loss = 4.88465300\n",
      "Iteration 31, loss = 4.41778818\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.26145640\n",
      "Iteration 2, loss = 16.87570074\n",
      "Iteration 3, loss = 12.55939479\n",
      "Iteration 4, loss = 9.97740191\n",
      "Iteration 5, loss = 12.28197642\n",
      "Iteration 6, loss = 11.16578296\n",
      "Iteration 1, loss = 17.05611481\n",
      "Iteration 7, loss = 10.00253127\n",
      "Iteration 2, loss = 13.69367476\n",
      "Iteration 8, loss = 13.36836066\n",
      "Iteration 3, loss = 13.28369018\n",
      "Iteration 9, loss = 11.39873052\n",
      "Iteration 4, loss = 13.14597302\n",
      "Iteration 10, loss = 7.97787828\n",
      "Iteration 5, loss = 9.50185616\n",
      "Iteration 11, loss = 9.01268433\n",
      "Iteration 6, loss = 10.03163861\n",
      "Iteration 12, loss = 8.61870236\n",
      "Iteration 7, loss = 8.74235464\n",
      "Iteration 13, loss = 8.70894918\n",
      "Iteration 8, loss = 8.39519662\n",
      "Iteration 14, loss = 7.68147731\n",
      "Iteration 9, loss = 7.53892702\n",
      "Iteration 15, loss = 6.87296989\n",
      "Iteration 10, loss = 9.72973427\n",
      "Iteration 16, loss = 7.61758322\n",
      "Iteration 11, loss = 10.53926868\n",
      "Iteration 17, loss = 7.37768723\n",
      "Iteration 12, loss = 9.74608151\n",
      "Iteration 18, loss = 6.81989726\n",
      "Iteration 13, loss = 7.75539832\n",
      "Iteration 19, loss = 6.64841172\n",
      "Iteration 14, loss = 7.44089853\n",
      "Iteration 20, loss = 6.12038252\n",
      "Iteration 15, loss = 6.33128752\n",
      "Iteration 21, loss = 5.54804594\n",
      "Iteration 16, loss = 7.13971444\n",
      "Iteration 22, loss = 6.14775803\n",
      "Iteration 17, loss = 6.76996692\n",
      "Iteration 23, loss = 5.76509434\n",
      "Iteration 18, loss = 6.51139705\n",
      "Iteration 24, loss = 6.36120462\n",
      "Iteration 19, loss = 5.97492028\n",
      "Iteration 25, loss = 5.97562022\n",
      "Iteration 20, loss = 5.35812078\n",
      "Iteration 26, loss = 5.58933242\n",
      "Iteration 21, loss = 5.79988675\n",
      "Iteration 27, loss = 6.33894693\n",
      "Iteration 22, loss = 5.25986898\n",
      "Iteration 28, loss = 5.59705432\n",
      "Iteration 23, loss = 5.54565398\n",
      "Iteration 29, loss = 5.99138159\n",
      "Iteration 24, loss = 4.74401446\n",
      "Iteration 30, loss = 5.99226342\n",
      "Iteration 25, loss = 4.72667214\n",
      "Iteration 31, loss = 5.40524702\n",
      "Iteration 26, loss = 6.31117163\n",
      "Iteration 32, loss = 5.95659218\n",
      "Iteration 27, loss = 5.14483407\n",
      "Iteration 33, loss = 6.27161335\n",
      "Iteration 28, loss = 5.62694112\n",
      "Iteration 34, loss = 5.60579232\n",
      "Iteration 29, loss = 4.93592781\n",
      "Iteration 35, loss = 6.39163100\n",
      "Iteration 30, loss = 5.14131289\n",
      "Iteration 36, loss = 6.23673475\n",
      "Iteration 31, loss = 4.83176172\n",
      "Iteration 37, loss = 6.12107019\n",
      "Iteration 32, loss = 4.32394077\n",
      "Iteration 38, loss = 5.69140939\n",
      "Iteration 33, loss = 4.67793458\n",
      "Iteration 39, loss = 5.61457708\n",
      "Iteration 34, loss = 4.40332559\n",
      "Iteration 40, loss = 5.69614125\n",
      "Iteration 35, loss = 4.40256820\n",
      "Iteration 41, loss = 6.91512480\n",
      "Iteration 36, loss = 4.44118461\n",
      "Iteration 42, loss = 6.33016029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 3.97026698\n",
      "Iteration 38, loss = 4.87241789\n",
      "Iteration 39, loss = 4.28464800\n",
      "Iteration 40, loss = 4.44420581\n",
      "Iteration 41, loss = 5.08138198\n",
      "Iteration 42, loss = 4.97635510\n",
      "Iteration 43, loss = 4.91021714\n",
      "Iteration 44, loss = 4.92906108\n",
      "Iteration 45, loss = 4.82407497\n",
      "Iteration 46, loss = 4.36072015\n",
      "Iteration 47, loss = 4.71799596\n",
      "Iteration 48, loss = 4.49199246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.55528621\n",
      "Iteration 2, loss = 15.81326597\n",
      "Iteration 3, loss = 12.07395972\n",
      "Iteration 4, loss = 10.49143382\n",
      "Iteration 5, loss = 7.02075007\n",
      "Iteration 6, loss = 6.79549375\n",
      "Iteration 7, loss = 7.47744363\n",
      "Iteration 8, loss = 7.75584191\n",
      "Iteration 9, loss = 6.97118836\n",
      "Iteration 10, loss = 7.37007526\n",
      "Iteration 11, loss = 7.23706395\n",
      "Iteration 12, loss = 6.26659121\n",
      "Iteration 1, loss = 14.30752969\n",
      "Iteration 13, loss = 5.56037751\n",
      "Iteration 2, loss = 14.09325445\n",
      "Iteration 14, loss = 5.12187064\n",
      "Iteration 3, loss = 14.50342682\n",
      "Iteration 15, loss = 5.61556962\n",
      "Iteration 4, loss = 14.17906488\n",
      "Iteration 16, loss = 5.20207217\n",
      "Iteration 5, loss = 13.81745706\n",
      "Iteration 17, loss = 5.49014475\n",
      "Iteration 6, loss = 10.50688102\n",
      "Iteration 18, loss = 5.69603704\n",
      "Iteration 7, loss = 11.55111377\n",
      "Iteration 19, loss = 5.50695153\n",
      "Iteration 8, loss = 10.78413859\n",
      "Iteration 20, loss = 5.15886739\n",
      "Iteration 9, loss = 9.76027978\n",
      "Iteration 21, loss = 5.71202089\n",
      "Iteration 10, loss = 8.89044479\n",
      "Iteration 22, loss = 5.55820296\n",
      "Iteration 11, loss = 6.60468383\n",
      "Iteration 23, loss = 5.32535256\n",
      "Iteration 12, loss = 6.97162919\n",
      "Iteration 24, loss = 5.28784670\n",
      "Iteration 13, loss = 7.17197080\n",
      "Iteration 25, loss = 5.52371906\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 6.26264348\n",
      "Iteration 15, loss = 6.51549498\n",
      "Iteration 16, loss = 6.27981620\n",
      "Iteration 17, loss = 5.91599770\n",
      "Iteration 18, loss = 6.70935547\n",
      "Iteration 19, loss = 6.38165227\n",
      "Iteration 20, loss = 6.82181987\n",
      "Iteration 21, loss = 5.45682010\n",
      "Iteration 22, loss = 5.69783967\n",
      "Iteration 23, loss = 6.60309426\n",
      "Iteration 24, loss = 6.63551169\n",
      "Iteration 25, loss = 6.28823479\n",
      "Iteration 26, loss = 6.24680142\n",
      "Iteration 27, loss = 5.89214663\n",
      "Iteration 28, loss = 6.08198587\n",
      "Iteration 29, loss = 5.60567801\n",
      "Iteration 30, loss = 5.67951232\n",
      "Iteration 31, loss = 5.75417753\n",
      "Iteration 32, loss = 6.10650947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.52297242\n",
      "Iteration 2, loss = 13.14790944\n",
      "Iteration 3, loss = 12.28526349\n",
      "Iteration 4, loss = 11.42269665\n",
      "Iteration 5, loss = 9.28269085\n",
      "Iteration 6, loss = 8.17180405\n",
      "Iteration 7, loss = 7.10472387\n",
      "Iteration 8, loss = 8.40177681\n",
      "Iteration 9, loss = 7.79212486\n",
      "Iteration 10, loss = 7.63446125\n",
      "Iteration 11, loss = 8.28637418\n",
      "Iteration 12, loss = 5.93916778\n",
      "Iteration 13, loss = 6.09500308\n",
      "Iteration 14, loss = 6.13364198\n",
      "Iteration 15, loss = 6.36568163\n",
      "Iteration 16, loss = 5.37641546\n",
      "Iteration 17, loss = 4.85289103\n",
      "Iteration 18, loss = 4.71711844\n",
      "Iteration 19, loss = 6.14569465\n",
      "Iteration 1, loss = 17.52985245\n",
      "Iteration 2, loss = 13.65473286\n",
      "Iteration 20, loss = 6.43460777\n",
      "Iteration 3, loss = 12.39059123\n",
      "Iteration 21, loss = 6.13429076\n",
      "Iteration 22, loss = 5.67330800\n",
      "Iteration 4, loss = 9.16436727\n",
      "Iteration 23, loss = 5.56237663\n",
      "Iteration 5, loss = 7.56826941\n",
      "Iteration 24, loss = 4.88994631\n",
      "Iteration 6, loss = 9.47400083\n",
      "Iteration 25, loss = 4.86755718\n",
      "Iteration 7, loss = 10.49104312\n",
      "Iteration 26, loss = 4.08972321\n",
      "Iteration 8, loss = 8.14609076\n",
      "Iteration 27, loss = 4.76298906\n",
      "Iteration 9, loss = 8.39151746\n",
      "Iteration 28, loss = 4.88795874\n",
      "Iteration 10, loss = 9.73364833\n",
      "Iteration 29, loss = 5.52253570\n",
      "Iteration 11, loss = 9.39906242\n",
      "Iteration 30, loss = 6.82318418\n",
      "Iteration 12, loss = 6.68670727\n",
      "Iteration 31, loss = 4.28915306\n",
      "Iteration 13, loss = 6.78259319\n",
      "Iteration 32, loss = 5.08158512\n",
      "Iteration 14, loss = 7.16936732\n",
      "Iteration 33, loss = 4.74448009\n",
      "Iteration 15, loss = 8.04452110\n",
      "Iteration 34, loss = 4.87390441\n",
      "Iteration 16, loss = 7.21306598\n",
      "Iteration 35, loss = 4.60776887\n",
      "Iteration 17, loss = 6.72069852\n",
      "Iteration 36, loss = 4.37738360\n",
      "Iteration 18, loss = 7.04540336\n",
      "Iteration 37, loss = 4.42208716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 6.31103725\n",
      "Iteration 20, loss = 6.24341043\n",
      "Iteration 21, loss = 5.82054297\n",
      "Iteration 22, loss = 6.21976808\n",
      "Iteration 23, loss = 5.44273253\n",
      "Iteration 24, loss = 5.56729191\n",
      "Iteration 25, loss = 5.37882251\n",
      "Iteration 26, loss = 5.18683291\n",
      "Iteration 27, loss = 5.34518518\n",
      "Iteration 28, loss = 4.79734009\n",
      "Iteration 29, loss = 5.42490514\n",
      "Iteration 30, loss = 8.44550064\n",
      "Iteration 31, loss = 6.09532073\n",
      "Iteration 32, loss = 6.18082861\n",
      "Iteration 33, loss = 6.23363583\n",
      "Iteration 34, loss = 6.05053230\n",
      "Iteration 35, loss = 5.86519222\n",
      "Iteration 36, loss = 5.01106569\n",
      "Iteration 37, loss = 5.01514008\n",
      "Iteration 1, loss = 16.94666022\n",
      "Iteration 38, loss = 5.44947699\n",
      "Iteration 2, loss = 15.53722123\n",
      "Iteration 39, loss = 5.01938463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 14.34494820\n",
      "Iteration 4, loss = 8.88096313\n",
      "Iteration 5, loss = 9.16203547\n",
      "Iteration 6, loss = 9.11287096\n",
      "Iteration 7, loss = 12.07759241\n",
      "Iteration 8, loss = 11.17637484\n",
      "Iteration 9, loss = 9.25410537\n",
      "Iteration 10, loss = 8.35003896\n",
      "Iteration 11, loss = 8.80563393\n",
      "Iteration 12, loss = 7.09123479\n",
      "Iteration 13, loss = 7.47865039\n",
      "Iteration 14, loss = 6.42770240\n",
      "Iteration 15, loss = 7.55176621\n",
      "Iteration 16, loss = 6.88600085\n",
      "Iteration 17, loss = 7.30754833\n",
      "Iteration 18, loss = 7.36543131\n",
      "Iteration 19, loss = 6.75239588\n",
      "Iteration 1, loss = 19.54718007\n",
      "Iteration 20, loss = 8.76771399\n",
      "Iteration 2, loss = 18.79602027\n",
      "Iteration 21, loss = 6.81414035\n",
      "Iteration 3, loss = 14.95125445\n",
      "Iteration 22, loss = 6.03974041\n",
      "Iteration 4, loss = 13.39807114\n",
      "Iteration 23, loss = 6.17231790\n",
      "Iteration 5, loss = 13.11794669\n",
      "Iteration 24, loss = 9.36917197\n",
      "Iteration 6, loss = 9.94484821\n",
      "Iteration 25, loss = 6.84428130\n",
      "Iteration 7, loss = 11.10054860\n",
      "Iteration 26, loss = 5.48618149\n",
      "Iteration 8, loss = 10.07422744\n",
      "Iteration 27, loss = 5.62719300\n",
      "Iteration 9, loss = 9.36459553\n",
      "Iteration 28, loss = 5.97062596\n",
      "Iteration 10, loss = 10.48504193\n",
      "Iteration 29, loss = 5.64486763\n",
      "Iteration 11, loss = 8.68785389\n",
      "Iteration 30, loss = 5.27301417\n",
      "Iteration 12, loss = 6.87385352\n",
      "Iteration 31, loss = 6.10818276\n",
      "Iteration 13, loss = 7.94056074\n",
      "Iteration 32, loss = 5.76011387\n",
      "Iteration 14, loss = 7.97400494\n",
      "Iteration 33, loss = 5.33345567\n",
      "Iteration 15, loss = 7.17720022\n",
      "Iteration 34, loss = 5.37627255\n",
      "Iteration 16, loss = 7.43881978\n",
      "Iteration 35, loss = 4.78870540\n",
      "Iteration 17, loss = 7.07023219\n",
      "Iteration 36, loss = 4.75137962\n",
      "Iteration 18, loss = 9.44557132\n",
      "Iteration 37, loss = 4.71167401\n",
      "Iteration 19, loss = 8.48600683\n",
      "Iteration 38, loss = 5.14501637\n",
      "Iteration 20, loss = 6.66459774\n",
      "Iteration 39, loss = 4.44035807\n",
      "Iteration 21, loss = 7.11881005\n",
      "Iteration 40, loss = 5.07090238\n",
      "Iteration 22, loss = 5.53434844\n",
      "Iteration 41, loss = 5.42974107\n",
      "Iteration 23, loss = 6.03585862\n",
      "Iteration 42, loss = 5.19632003\n",
      "Iteration 24, loss = 7.83779973\n",
      "Iteration 43, loss = 4.88210489\n",
      "Iteration 25, loss = 7.08843176\n",
      "Iteration 44, loss = 4.84279682\n",
      "Iteration 26, loss = 6.29197210\n",
      "Iteration 45, loss = 5.31611935\n",
      "Iteration 27, loss = 6.28539086\n",
      "Iteration 46, loss = 4.80569577\n",
      "Iteration 28, loss = 7.22033751\n",
      "Iteration 47, loss = 4.84299231\n",
      "Iteration 29, loss = 6.69446407\n",
      "Iteration 48, loss = 5.07507750\n",
      "Iteration 30, loss = 6.59806968\n",
      "Iteration 49, loss = 5.78245031\n",
      "Iteration 31, loss = 5.71696899\n",
      "Iteration 50, loss = 5.82549179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 6.20851365\n",
      "Iteration 33, loss = 5.47572615\n",
      "Iteration 34, loss = 5.16986226\n",
      "Iteration 35, loss = 5.25501202\n",
      "Iteration 36, loss = 4.75179164\n",
      "Iteration 37, loss = 5.26678147\n",
      "Iteration 38, loss = 5.15248870\n",
      "Iteration 39, loss = 4.72280424\n",
      "Iteration 40, loss = 4.60605935\n",
      "Iteration 41, loss = 5.32220164\n",
      "Iteration 42, loss = 5.79260930\n",
      "Iteration 43, loss = 4.92083497\n",
      "Iteration 44, loss = 4.17542405\n",
      "Iteration 45, loss = 4.56798358\n",
      "Iteration 46, loss = 3.98042852\n",
      "Iteration 47, loss = 4.64735163\n",
      "Iteration 48, loss = 4.80397738\n",
      "Iteration 49, loss = 4.45201230\n",
      "Iteration 1, loss = 16.16087716\n",
      "Iteration 50, loss = 5.16005991\n",
      "Iteration 2, loss = 13.32364112\n",
      "Iteration 51, loss = 4.33729801\n",
      "Iteration 3, loss = 11.54045868\n",
      "Iteration 52, loss = 4.80855224\n",
      "Iteration 4, loss = 13.01706884\n",
      "Iteration 53, loss = 4.65312019\n",
      "Iteration 5, loss = 10.63682858\n",
      "Iteration 54, loss = 3.94807370\n",
      "Iteration 6, loss = 7.48160236\n",
      "Iteration 55, loss = 4.38404699\n",
      "Iteration 7, loss = 7.50086170\n",
      "Iteration 56, loss = 4.14351797\n",
      "Iteration 8, loss = 8.50154682\n",
      "Iteration 57, loss = 4.69210173\n",
      "Iteration 9, loss = 8.26932897\n",
      "Iteration 58, loss = 4.18206686\n",
      "Iteration 10, loss = 6.84381268\n",
      "Iteration 59, loss = 4.33375345\n",
      "Iteration 11, loss = 6.15841022\n",
      "Iteration 60, loss = 4.37763252\n",
      "Iteration 12, loss = 6.60323621\n",
      "Iteration 61, loss = 3.90722664\n",
      "Iteration 13, loss = 6.72136855\n",
      "Iteration 14, loss = 5.96767206\n",
      "Iteration 62, loss = 4.25978263\n",
      "Iteration 15, loss = 6.30308784\n",
      "Iteration 63, loss = 4.80835849\n",
      "Iteration 16, loss = 5.84473703Iteration 64, loss = 4.29525264\n",
      "\n",
      "Iteration 17, loss = 5.53919272\n",
      "Iteration 65, loss = 3.62688672\n",
      "Iteration 18, loss = 5.42847558\n",
      "Iteration 66, loss = 4.02479489\n",
      "Iteration 19, loss = 5.35824347\n",
      "Iteration 67, loss = 5.74683688\n",
      "Iteration 20, loss = 10.46257174\n",
      "Iteration 68, loss = 4.33502844\n",
      "Iteration 21, loss = 6.39069905\n",
      "Iteration 69, loss = 3.94172810\n",
      "Iteration 22, loss = 6.63935204\n",
      "Iteration 70, loss = 4.48974961\n",
      "Iteration 23, loss = 5.36085394\n",
      "Iteration 71, loss = 4.72787080\n",
      "Iteration 24, loss = 4.86761727\n",
      "Iteration 72, loss = 4.29776506\n",
      "Iteration 25, loss = 5.00117908\n",
      "Iteration 73, loss = 4.02357708\n",
      "Iteration 26, loss = 5.29081059\n",
      "Iteration 74, loss = 3.74918372\n",
      "Iteration 27, loss = 4.40334974\n",
      "Iteration 75, loss = 4.21973592\n",
      "Iteration 28, loss = 4.06399522\n",
      "Iteration 76, loss = 4.71283247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 4.19332033\n",
      "Iteration 30, loss = 4.55391595\n",
      "Iteration 31, loss = 4.67642798\n",
      "Iteration 32, loss = 3.62062922\n",
      "Iteration 33, loss = 4.44610808\n",
      "Iteration 34, loss = 5.30958141\n",
      "Iteration 35, loss = 4.44656012\n",
      "Iteration 36, loss = 4.36795342\n",
      "Iteration 37, loss = 3.81905657\n",
      "Iteration 38, loss = 3.77991846\n",
      "Iteration 39, loss = 4.05428467\n",
      "Iteration 40, loss = 4.28918716\n",
      "Iteration 41, loss = 3.59509490\n",
      "Iteration 42, loss = 4.05344780\n",
      "Iteration 43, loss = 4.01425391\n",
      "Iteration 44, loss = 4.09246063\n",
      "Iteration 45, loss = 4.60168288\n",
      "Iteration 46, loss = 3.69769935\n",
      "Iteration 1, loss = 15.26947896\n",
      "Iteration 47, loss = 4.63901205\n",
      "Iteration 2, loss = 13.39301392\n",
      "Iteration 48, loss = 4.40326019\n",
      "Iteration 3, loss = 11.87084836\n",
      "Iteration 49, loss = 3.74663406\n",
      "Iteration 4, loss = 7.58023820\n",
      "Iteration 50, loss = 3.81465442\n",
      "Iteration 5, loss = 10.85752466\n",
      "Iteration 51, loss = 3.85334463\n",
      "Iteration 6, loss = 9.38245336\n",
      "Iteration 52, loss = 4.55865940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 10.96835070\n",
      "Iteration 8, loss = 9.70057238\n",
      "Iteration 9, loss = 12.60114753\n",
      "Iteration 10, loss = 8.05788580\n",
      "Iteration 11, loss = 6.34854737\n",
      "Iteration 12, loss = 6.47465290\n",
      "Iteration 13, loss = 5.79435081\n",
      "Iteration 14, loss = 8.71112942\n",
      "Iteration 15, loss = 6.08985391\n",
      "Iteration 16, loss = 5.80830798\n",
      "Iteration 17, loss = 5.64157786\n",
      "Iteration 18, loss = 5.90690516\n",
      "Iteration 19, loss = 5.65744016\n",
      "Iteration 20, loss = 6.22365179\n",
      "Iteration 21, loss = 6.90513017\n",
      "Iteration 22, loss = 7.11327473\n",
      "Iteration 23, loss = 6.21568310\n",
      "Iteration 24, loss = 6.84937232\n",
      "Iteration 1, loss = 19.83592204Iteration 25, loss = 5.52406202\n",
      "\n",
      "Iteration 26, loss = 4.91210177\n",
      "Iteration 2, loss = 23.64395546\n",
      "Iteration 27, loss = 4.76951714\n",
      "Iteration 3, loss = 13.54955770\n",
      "Iteration 28, loss = 5.26921667\n",
      "Iteration 4, loss = 13.28799812\n",
      "Iteration 29, loss = 4.70886888\n",
      "Iteration 5, loss = 10.58395368\n",
      "Iteration 30, loss = 4.63531422\n",
      "Iteration 6, loss = 9.04549485\n",
      "Iteration 31, loss = 4.63973399\n",
      "Iteration 7, loss = 7.43679891\n",
      "Iteration 32, loss = 5.58484047Iteration 8, loss = 8.29884373\n",
      "\n",
      "Iteration 9, loss = 6.74265617\n",
      "Iteration 33, loss = 4.52866626\n",
      "Iteration 10, loss = 7.31720232\n",
      "Iteration 34, loss = 4.80882874\n",
      "Iteration 11, loss = 5.98660769\n",
      "Iteration 35, loss = 4.14933793\n",
      "Iteration 12, loss = 6.36196109\n",
      "Iteration 36, loss = 4.86276948\n",
      "Iteration 37, loss = 5.22388478\n",
      "Iteration 38, loss = 4.95540012\n",
      "Iteration 13, loss = 6.99586285\n",
      "Iteration 39, loss = 4.88222451\n",
      "Iteration 14, loss = 6.05134009\n",
      "Iteration 40, loss = 4.64880630\n",
      "Iteration 15, loss = 4.62877434\n",
      "Iteration 41, loss = 5.27649818\n",
      "Iteration 16, loss = 5.43791203\n",
      "Iteration 42, loss = 4.76697816\n",
      "Iteration 17, loss = 4.74881359\n",
      "Iteration 43, loss = 4.37383248\n",
      "Iteration 18, loss = 4.52956429\n",
      "Iteration 44, loss = 4.76471672\n",
      "Iteration 19, loss = 5.48756666\n",
      "Iteration 45, loss = 5.00079525\n",
      "Iteration 20, loss = 6.40304104\n",
      "Iteration 21, loss = 5.11882782\n",
      "Iteration 46, loss = 5.12024873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 22, loss = 4.62288490\n",
      "Iteration 23, loss = 4.28611804\n",
      "Iteration 24, loss = 4.06403851\n",
      "Iteration 25, loss = 4.62280377\n",
      "Iteration 26, loss = 4.51380998\n",
      "Iteration 27, loss = 5.14623459\n",
      "Iteration 28, loss = 4.83356129\n",
      "Iteration 29, loss = 4.48504971\n",
      "Iteration 30, loss = 4.57359107\n",
      "Iteration 31, loss = 4.03621426\n",
      "Iteration 32, loss = 4.20255744\n",
      "Iteration 33, loss = 3.77656699\n",
      "Iteration 34, loss = 3.35227473\n",
      "Iteration 35, loss = 3.27910576\n",
      "Iteration 36, loss = 2.96824331\n",
      "Iteration 37, loss = 3.28376705\n",
      "Iteration 38, loss = 3.01148684\n",
      "Iteration 1, loss = 20.48786951\n",
      "Iteration 39, loss = 2.89545521\n",
      "Iteration 2, loss = 18.31246166\n",
      "Iteration 40, loss = 3.56347122\n",
      "Iteration 3, loss = 10.77625260\n",
      "Iteration 41, loss = 3.60383717\n",
      "Iteration 4, loss = 8.88815027\n",
      "Iteration 42, loss = 3.01537878\n",
      "Iteration 5, loss = 11.37463329\n",
      "Iteration 43, loss = 3.36733732\n",
      "Iteration 6, loss = 11.15483320\n",
      "Iteration 44, loss = 3.13139293\n",
      "Iteration 7, loss = 8.74755913\n",
      "Iteration 45, loss = 3.19826971\n",
      "Iteration 8, loss = 7.12438786\n",
      "Iteration 46, loss = 3.36647261\n",
      "Iteration 9, loss = 7.50456867\n",
      "Iteration 10, loss = 7.54847236\n",
      "Iteration 47, loss = 3.64117298\n",
      "Iteration 11, loss = 8.61932326\n",
      "Iteration 48, loss = 3.64113429\n",
      "Iteration 12, loss = 9.84185341\n",
      "Iteration 49, loss = 3.95597824\n",
      "Iteration 13, loss = 7.90125440\n",
      "Iteration 50, loss = 3.99586321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 8.64897791\n",
      "Iteration 15, loss = 7.86811478\n",
      "Iteration 16, loss = 7.51947114\n",
      "Iteration 17, loss = 6.81250008\n",
      "Iteration 18, loss = 6.92387921\n",
      "Iteration 19, loss = 6.28565711\n",
      "Iteration 20, loss = 7.05326755\n",
      "Iteration 21, loss = 7.30205943\n",
      "Iteration 22, loss = 6.25153599\n",
      "Iteration 23, loss = 5.98869975\n",
      "Iteration 24, loss = 6.43210854\n",
      "Iteration 25, loss = 6.83281073\n",
      "Iteration 26, loss = 5.66173412\n",
      "Iteration 27, loss = 5.85858046\n",
      "Iteration 28, loss = 6.28957485\n",
      "Iteration 29, loss = 5.50543473\n",
      "Iteration 30, loss = 5.81682647\n",
      "Iteration 1, loss = 16.91209991\n",
      "Iteration 31, loss = 6.13131910\n",
      "Iteration 2, loss = 14.03937093\n",
      "Iteration 32, loss = 4.99608118\n",
      "Iteration 3, loss = 10.22369956\n",
      "Iteration 33, loss = 4.64519819\n",
      "Iteration 4, loss = 10.06139605\n",
      "Iteration 34, loss = 4.56974380\n",
      "Iteration 5, loss = 7.79530992\n",
      "Iteration 35, loss = 5.00463897\n",
      "Iteration 6, loss = 7.27204842\n",
      "Iteration 36, loss = 4.61401866\n",
      "Iteration 7, loss = 7.90646575\n",
      "Iteration 37, loss = 4.61449948\n",
      "Iteration 8, loss = 12.50589685\n",
      "Iteration 38, loss = 4.49761586\n",
      "Iteration 9, loss = 14.18202723\n",
      "Iteration 39, loss = 4.38085011\n",
      "Iteration 10, loss = 10.24855757\n",
      "Iteration 40, loss = 4.38218992\n",
      "Iteration 11, loss = 10.71334738\n",
      "Iteration 41, loss = 4.54090718\n",
      "Iteration 12, loss = 10.64858219\n",
      "Iteration 42, loss = 4.65955824\n",
      "Iteration 13, loss = 10.47340293\n",
      "Iteration 43, loss = 4.50323552\n",
      "Iteration 14, loss = 8.35852914\n",
      "Iteration 44, loss = 4.58196617\n",
      "Iteration 15, loss = 7.36363236\n",
      "Iteration 45, loss = 4.07333656\n",
      "Iteration 16, loss = 6.19017747\n",
      "Iteration 46, loss = 4.23045558\n",
      "Iteration 17, loss = 6.05540575\n",
      "Iteration 47, loss = 4.34786020\n",
      "Iteration 18, loss = 5.85782242\n",
      "Iteration 48, loss = 4.85757205\n",
      "Iteration 19, loss = 5.76396966\n",
      "Iteration 49, loss = 5.09278776\n",
      "Iteration 20, loss = 5.07316461\n",
      "Iteration 50, loss = 5.52447244\n",
      "Iteration 21, loss = 7.51366877\n",
      "Iteration 51, loss = 6.34779034\n",
      "Iteration 22, loss = 8.34332542\n",
      "Iteration 52, loss = 5.56036827\n",
      "Iteration 23, loss = 6.97840947\n",
      "Iteration 53, loss = 5.36265104\n",
      "Iteration 24, loss = 6.59874399\n",
      "Iteration 54, loss = 5.32430650\n",
      "Iteration 25, loss = 6.73010603\n",
      "Iteration 55, loss = 5.32622095\n",
      "Iteration 26, loss = 6.34543104\n",
      "Iteration 56, loss = 5.01214544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 7.05675787\n",
      "Iteration 28, loss = 6.19789533\n",
      "Iteration 29, loss = 6.00550550\n",
      "Iteration 30, loss = 5.57558497\n",
      "Iteration 31, loss = 5.57148704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.15260714\n",
      "Iteration 2, loss = 19.25081552\n",
      "Iteration 3, loss = 11.74519444\n",
      "Iteration 4, loss = 9.43042512\n",
      "Iteration 5, loss = 10.60316476\n",
      "Iteration 1, loss = 19.85729459\n",
      "Iteration 6, loss = 11.07225830\n",
      "Iteration 2, loss = 22.36310324\n",
      "Iteration 7, loss = 8.83422431\n",
      "Iteration 3, loss = 13.35462379\n",
      "Iteration 8, loss = 8.16662161\n",
      "Iteration 4, loss = 11.24944728\n",
      "Iteration 9, loss = 9.08288916\n",
      "Iteration 5, loss = 9.94537086\n",
      "Iteration 10, loss = 9.33654228\n",
      "Iteration 6, loss = 11.31952528\n",
      "Iteration 11, loss = 6.96983016\n",
      "Iteration 7, loss = 7.98889371\n",
      "Iteration 12, loss = 7.44807067\n",
      "Iteration 8, loss = 7.82856150\n",
      "Iteration 13, loss = 6.22245696\n",
      "Iteration 9, loss = 9.92437259\n",
      "Iteration 14, loss = 6.31724608\n",
      "Iteration 10, loss = 7.18113603\n",
      "Iteration 15, loss = 6.16294504\n",
      "Iteration 11, loss = 6.66171199\n",
      "Iteration 16, loss = 7.88428853\n",
      "Iteration 12, loss = 5.87721141\n",
      "Iteration 17, loss = 6.49804813\n",
      "Iteration 13, loss = 6.29669807\n",
      "Iteration 18, loss = 5.37775552\n",
      "Iteration 14, loss = 7.17943801\n",
      "Iteration 19, loss = 5.63817439\n",
      "Iteration 15, loss = 8.24562937\n",
      "Iteration 20, loss = 5.70887380\n",
      "Iteration 16, loss = 6.23999802\n",
      "Iteration 21, loss = 5.69379899\n",
      "Iteration 17, loss = 6.03169369\n",
      "Iteration 22, loss = 5.00313685\n",
      "Iteration 18, loss = 5.61926934\n",
      "Iteration 23, loss = 5.13240698\n",
      "Iteration 19, loss = 5.91214863\n",
      "Iteration 24, loss = 4.86510555\n",
      "Iteration 20, loss = 8.12181511\n",
      "Iteration 25, loss = 4.94814363\n",
      "Iteration 21, loss = 6.56145285\n",
      "Iteration 26, loss = 4.79537736\n",
      "Iteration 22, loss = 5.62482908\n",
      "Iteration 27, loss = 4.95513537\n",
      "Iteration 23, loss = 5.62699944\n",
      "Iteration 28, loss = 5.46662539\n",
      "Iteration 24, loss = 5.27378284\n",
      "Iteration 29, loss = 4.95715263\n",
      "Iteration 25, loss = 4.92735309\n",
      "Iteration 30, loss = 4.40803366\n",
      "Iteration 26, loss = 4.15232304\n",
      "Iteration 31, loss = 4.21339369\n",
      "Iteration 27, loss = 4.47584031\n",
      "Iteration 32, loss = 4.99868712\n",
      "Iteration 28, loss = 4.44241694\n",
      "Iteration 33, loss = 4.21438150\n",
      "Iteration 29, loss = 3.93547769\n",
      "Iteration 34, loss = 4.09694155\n",
      "Iteration 30, loss = 4.68256382\n",
      "Iteration 35, loss = 4.52855130\n",
      "Iteration 31, loss = 4.88292608\n",
      "Iteration 36, loss = 6.13724446\n",
      "Iteration 32, loss = 5.70433544\n",
      "Iteration 37, loss = 4.84465778\n",
      "Iteration 33, loss = 5.00473046\n",
      "Iteration 38, loss = 4.76644788\n",
      "Iteration 34, loss = 4.29961374\n",
      "Iteration 39, loss = 5.08082143\n",
      "Iteration 35, loss = 4.69276335\n",
      "Iteration 40, loss = 4.72993231\n",
      "Iteration 36, loss = 4.88870024\n",
      "Iteration 41, loss = 5.20234259\n",
      "Iteration 37, loss = 4.92746476\n",
      "Iteration 42, loss = 4.88891529\n",
      "Iteration 38, loss = 4.90745080\n",
      "Iteration 43, loss = 5.63317872\n",
      "Iteration 39, loss = 4.49806577\n",
      "Iteration 44, loss = 4.65160144\n",
      "Iteration 40, loss = 4.53792100\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 3.86697642\n",
      "Iteration 46, loss = 4.10472832\n",
      "Iteration 47, loss = 3.99054806\n",
      "Iteration 48, loss = 3.87579323\n",
      "Iteration 49, loss = 3.64117822\n",
      "Iteration 50, loss = 3.91650509\n",
      "Iteration 51, loss = 3.74878312\n",
      "Iteration 52, loss = 4.54174858\n",
      "Iteration 53, loss = 4.69753460\n",
      "Iteration 54, loss = 5.12773657\n",
      "Iteration 55, loss = 4.38039485\n",
      "Iteration 56, loss = 4.02479811\n",
      "Iteration 57, loss = 3.55262031\n",
      "Iteration 58, loss = 4.13958726\n",
      "Iteration 59, loss = 4.06029845\n",
      "Iteration 60, loss = 4.45317487\n",
      "Iteration 61, loss = 4.26402018\n",
      "Iteration 62, loss = 3.99877750\n",
      "Iteration 1, loss = 15.49266025\n",
      "Iteration 63, loss = 3.76832049\n",
      "Iteration 2, loss = 15.14935768\n",
      "Iteration 64, loss = 4.04514850\n",
      "Iteration 3, loss = 9.65665957\n",
      "Iteration 65, loss = 3.96999512\n",
      "Iteration 4, loss = 11.63533650\n",
      "Iteration 66, loss = 4.63977508\n",
      "Iteration 5, loss = 9.44456888\n",
      "Iteration 67, loss = 3.62228866\n",
      "Iteration 6, loss = 7.11011177\n",
      "Iteration 68, loss = 3.38852272\n",
      "Iteration 7, loss = 8.60272146\n",
      "Iteration 69, loss = 3.31070742\n",
      "Iteration 8, loss = 11.63160732\n",
      "Iteration 70, loss = 3.58583320\n",
      "Iteration 9, loss = 9.01257098\n",
      "Iteration 71, loss = 3.38903524\n",
      "Iteration 10, loss = 9.90268545\n",
      "Iteration 72, loss = 4.32909979\n",
      "Iteration 11, loss = 9.41727208\n",
      "Iteration 73, loss = 4.40723377\n",
      "Iteration 12, loss = 8.06101434\n",
      "Iteration 74, loss = 4.92077163\n",
      "Iteration 13, loss = 6.82198090\n",
      "Iteration 75, loss = 3.91096849\n",
      "Iteration 14, loss = 6.43929288\n",
      "Iteration 76, loss = 3.92275754\n",
      "Iteration 15, loss = 7.06603441\n",
      "Iteration 77, loss = 3.85279263\n",
      "Iteration 16, loss = 7.10549006\n",
      "Iteration 78, loss = 4.21168303\n",
      "Iteration 17, loss = 6.54950122\n",
      "Iteration 79, loss = 4.64348835\n",
      "Iteration 18, loss = 6.36527480\n",
      "Iteration 80, loss = 4.91836395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 6.00858155\n",
      "Iteration 20, loss = 5.64331322\n",
      "Iteration 21, loss = 5.07584666\n",
      "Iteration 22, loss = 6.07187167\n",
      "Iteration 23, loss = 5.57903967\n",
      "Iteration 24, loss = 4.88817227\n",
      "Iteration 25, loss = 5.48860989\n",
      "Iteration 26, loss = 5.49909751\n",
      "Iteration 27, loss = 5.66918896\n",
      "Iteration 28, loss = 5.24792323\n",
      "Iteration 29, loss = 5.68320252\n",
      "Iteration 30, loss = 5.09578819\n",
      "Iteration 31, loss = 6.04044222\n",
      "Iteration 32, loss = 6.81511373\n",
      "Iteration 33, loss = 5.62819697\n",
      "Iteration 34, loss = 5.73002979\n",
      "Iteration 35, loss = 5.02238720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.45724575\n",
      "Iteration 2, loss = 15.37508380\n",
      "Iteration 3, loss = 13.22212271\n",
      "Iteration 4, loss = 13.36103414\n",
      "Iteration 5, loss = 8.51229353\n",
      "Iteration 6, loss = 7.39455154\n",
      "Iteration 7, loss = 7.75362976\n",
      "Iteration 8, loss = 13.48491270\n",
      "Iteration 9, loss = 13.81610295\n",
      "Iteration 10, loss = 9.77725799\n",
      "Iteration 11, loss = 8.61760622\n",
      "Iteration 12, loss = 8.42835094\n",
      "Iteration 13, loss = 7.66780867\n",
      "Iteration 14, loss = 5.78324421\n",
      "Iteration 15, loss = 6.78723588\n",
      "Iteration 16, loss = 5.79101025\n",
      "Iteration 17, loss = 5.49366542\n",
      "Iteration 18, loss = 5.57100449\n",
      "Iteration 1, loss = 18.96399765\n",
      "Iteration 19, loss = 5.04754260\n",
      "Iteration 2, loss = 15.26306678\n",
      "Iteration 20, loss = 5.42009971\n",
      "Iteration 3, loss = 11.32752018\n",
      "Iteration 21, loss = 5.08035659\n",
      "Iteration 4, loss = 10.27872951\n",
      "Iteration 22, loss = 5.04918696\n",
      "Iteration 5, loss = 6.19150312\n",
      "Iteration 23, loss = 6.03676964\n",
      "Iteration 6, loss = 6.89148355\n",
      "Iteration 24, loss = 4.98494428\n",
      "Iteration 7, loss = 7.49005226\n",
      "Iteration 25, loss = 4.64016529\n",
      "Iteration 8, loss = 9.81212713\n",
      "Iteration 26, loss = 4.72381597\n",
      "Iteration 9, loss = 5.90160645\n",
      "Iteration 27, loss = 4.21768775\n",
      "Iteration 10, loss = 6.14016887\n",
      "Iteration 28, loss = 4.69142818\n",
      "Iteration 11, loss = 6.20931266\n",
      "Iteration 29, loss = 4.92875561\n",
      "Iteration 12, loss = 5.67389944\n",
      "Iteration 30, loss = 4.34015862\n",
      "Iteration 13, loss = 5.48419463\n",
      "Iteration 31, loss = 4.29894446\n",
      "Iteration 14, loss = 6.34778448\n",
      "Iteration 32, loss = 4.45531946\n",
      "Iteration 15, loss = 7.16646217\n",
      "Iteration 33, loss = 4.14096922\n",
      "Iteration 16, loss = 6.91785364\n",
      "Iteration 34, loss = 4.14098246\n",
      "Iteration 17, loss = 5.09417383\n",
      "Iteration 35, loss = 3.82545983\n",
      "Iteration 18, loss = 4.64140378\n",
      "Iteration 36, loss = 3.90319796\n",
      "Iteration 19, loss = 5.20840778\n",
      "Iteration 37, loss = 3.70788557\n",
      "Iteration 20, loss = 5.81003508\n",
      "Iteration 38, loss = 4.06097419\n",
      "Iteration 21, loss = 6.09432262\n",
      "Iteration 39, loss = 3.78667393\n",
      "Iteration 22, loss = 6.53488243\n",
      "Iteration 40, loss = 3.94396215\n",
      "Iteration 23, loss = 6.42468046\n",
      "Iteration 41, loss = 3.63035219\n",
      "Iteration 24, loss = 5.21293710\n",
      "Iteration 42, loss = 4.21659358\n",
      "Iteration 25, loss = 5.33874652\n",
      "Iteration 43, loss = 4.45079454\n",
      "Iteration 26, loss = 4.89218211\n",
      "Iteration 44, loss = 4.25548312\n",
      "Iteration 27, loss = 5.70169708\n",
      "Iteration 45, loss = 3.70913499\n",
      "Iteration 28, loss = 5.60494935\n",
      "Iteration 46, loss = 3.82898503\n",
      "Iteration 29, loss = 4.68134745\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 3.71362926\n",
      "Iteration 48, loss = 4.46120667\n",
      "Iteration 49, loss = 4.07195537\n",
      "Iteration 50, loss = 3.28856164\n",
      "Iteration 51, loss = 3.60297576\n",
      "Iteration 52, loss = 3.40740099\n",
      "Iteration 53, loss = 3.01366358\n",
      "Iteration 54, loss = 3.52119572\n",
      "Iteration 55, loss = 3.32542874\n",
      "Iteration 56, loss = 3.87448796\n",
      "Iteration 57, loss = 3.44224814\n",
      "Iteration 58, loss = 3.60038998\n",
      "Iteration 59, loss = 3.83751087\n",
      "Iteration 60, loss = 4.58495374\n",
      "Iteration 61, loss = 4.31242877\n",
      "Iteration 62, loss = 3.88666342\n",
      "Iteration 63, loss = 3.69400408\n",
      "Iteration 1, loss = 19.44095317\n",
      "Iteration 64, loss = 3.38087646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 2, loss = 15.59624041\n",
      "Iteration 3, loss = 13.07460328\n",
      "Iteration 4, loss = 10.95994728\n",
      "Iteration 5, loss = 10.47017105\n",
      "Iteration 6, loss = 12.24611591\n",
      "Iteration 7, loss = 14.22508730\n",
      "Iteration 8, loss = 10.06221681\n",
      "Iteration 9, loss = 8.32715017\n",
      "Iteration 10, loss = 7.17674121\n",
      "Iteration 11, loss = 7.38131744\n",
      "Iteration 12, loss = 6.69981118\n",
      "Iteration 13, loss = 6.86455567\n",
      "Iteration 14, loss = 6.27306085\n",
      "Iteration 15, loss = 5.90554828\n",
      "Iteration 16, loss = 10.78596529\n",
      "Iteration 17, loss = 8.56570629\n",
      "Iteration 18, loss = 6.70049218\n",
      "Iteration 19, loss = 6.20969212\n",
      "Iteration 1, loss = 17.93960479\n",
      "Iteration 20, loss = 6.54176827\n",
      "Iteration 2, loss = 16.06022526\n",
      "Iteration 21, loss = 6.51855436\n",
      "Iteration 3, loss = 13.07538515\n",
      "Iteration 22, loss = 5.23875535\n",
      "Iteration 4, loss = 11.73616607\n",
      "Iteration 23, loss = 5.80901055\n",
      "Iteration 5, loss = 8.13288387\n",
      "Iteration 24, loss = 4.23607127\n",
      "Iteration 6, loss = 6.93178244\n",
      "Iteration 25, loss = 4.67320319\n",
      "Iteration 7, loss = 6.74038051\n",
      "Iteration 26, loss = 5.27758850\n",
      "Iteration 8, loss = 6.69728557\n",
      "Iteration 27, loss = 5.26820593\n",
      "Iteration 9, loss = 8.01315727\n",
      "Iteration 28, loss = 6.10690947\n",
      "Iteration 10, loss = 11.54193117\n",
      "Iteration 29, loss = 5.00174628\n",
      "Iteration 11, loss = 11.03491026\n",
      "Iteration 30, loss = 4.42309889\n",
      "Iteration 12, loss = 8.50406520\n",
      "Iteration 31, loss = 5.84603689\n",
      "Iteration 13, loss = 7.55274577\n",
      "Iteration 32, loss = 4.31644861\n",
      "Iteration 14, loss = 8.88215257\n",
      "Iteration 33, loss = 4.25020983\n",
      "Iteration 15, loss = 7.30064105\n",
      "Iteration 34, loss = 4.52578355\n",
      "Iteration 16, loss = 7.66179333\n",
      "Iteration 35, loss = 5.11445334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 10.51192343\n",
      "Iteration 18, loss = 7.38907227\n",
      "Iteration 19, loss = 8.38204699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.76312469\n",
      "Iteration 2, loss = 22.44321387\n",
      "Iteration 1, loss = 20.88180064\n",
      "Iteration 3, loss = 9.41183365\n",
      "Iteration 2, loss = 15.52919509\n",
      "Iteration 4, loss = 8.66604770\n",
      "Iteration 3, loss = 13.01574508\n",
      "Iteration 5, loss = 13.48357220\n",
      "Iteration 4, loss = 15.20310508\n",
      "Iteration 6, loss = 10.21779354\n",
      "Iteration 5, loss = 11.83645445\n",
      "Iteration 7, loss = 8.59506268\n",
      "Iteration 6, loss = 9.31068743\n",
      "Iteration 8, loss = 10.30310650\n",
      "Iteration 7, loss = 8.18038410\n",
      "Iteration 9, loss = 6.19685277\n",
      "Iteration 8, loss = 10.05364284\n",
      "Iteration 10, loss = 6.81696062\n",
      "Iteration 9, loss = 13.05394395\n",
      "Iteration 11, loss = 7.44676311\n",
      "Iteration 10, loss = 10.31520792\n",
      "Iteration 12, loss = 6.91825919\n",
      "Iteration 11, loss = 9.04900922\n",
      "Iteration 13, loss = 6.36820494\n",
      "Iteration 12, loss = 8.98308598\n",
      "Iteration 14, loss = 6.46442879\n",
      "Iteration 13, loss = 7.17548287\n",
      "Iteration 15, loss = 7.05857942\n",
      "Iteration 14, loss = 6.41202007\n",
      "Iteration 16, loss = 6.03224257\n",
      "Iteration 15, loss = 11.01155758\n",
      "Iteration 17, loss = 4.67563782\n",
      "Iteration 16, loss = 9.65449562\n",
      "Iteration 17, loss = 8.04990456\n",
      "Iteration 18, loss = 4.99518052\n",
      "Iteration 18, loss = 7.13410646\n",
      "Iteration 19, loss = 4.91886390\n",
      "Iteration 20, loss = 4.88686769\n",
      "Iteration 19, loss = 6.40853466\n",
      "Iteration 21, loss = 4.54615167\n",
      "Iteration 20, loss = 5.55405657\n",
      "Iteration 22, loss = 3.85592420\n",
      "Iteration 21, loss = 6.72488844\n",
      "Iteration 23, loss = 4.14579339\n",
      "Iteration 22, loss = 5.80774772\n",
      "Iteration 24, loss = 4.08070483\n",
      "Iteration 23, loss = 5.50777164\n",
      "Iteration 25, loss = 4.04959561\n",
      "Iteration 24, loss = 5.78906054\n",
      "Iteration 26, loss = 4.13232014\n",
      "Iteration 25, loss = 5.16578325\n",
      "Iteration 27, loss = 3.82104062\n",
      "Iteration 26, loss = 4.44620406\n",
      "Iteration 28, loss = 3.97890124\n",
      "Iteration 27, loss = 5.87595471\n",
      "Iteration 29, loss = 4.01775780\n",
      "Iteration 28, loss = 5.87800521\n",
      "Iteration 30, loss = 3.50867295\n",
      "Iteration 29, loss = 5.36972065\n",
      "Iteration 31, loss = 3.98098166\n",
      "Iteration 30, loss = 5.21652682\n",
      "Iteration 32, loss = 4.45311984\n",
      "Iteration 31, loss = 5.58765970\n",
      "Iteration 33, loss = 4.13937452\n",
      "Iteration 32, loss = 5.14232048\n",
      "Iteration 34, loss = 4.41636302\n",
      "Iteration 33, loss = 5.57466210\n",
      "Iteration 35, loss = 4.14897756\n",
      "Iteration 34, loss = 5.02672674\n",
      "Iteration 36, loss = 3.80571023\n",
      "Iteration 35, loss = 5.30188650\n",
      "Iteration 37, loss = 3.61651192\n",
      "Iteration 36, loss = 4.75266029\n",
      "Iteration 38, loss = 4.01426894\n",
      "Iteration 37, loss = 5.06628108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 3.86585490\n",
      "Iteration 40, loss = 3.55852102\n",
      "Iteration 41, loss = 3.75774764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.14210204\n",
      "Iteration 2, loss = 24.07779405\n",
      "Iteration 1, loss = 19.21645361\n",
      "Iteration 3, loss = 12.37105524\n",
      "Iteration 2, loss = 18.84113039\n",
      "Iteration 4, loss = 12.42031048\n",
      "Iteration 3, loss = 10.64652966\n",
      "Iteration 5, loss = 10.74469177\n",
      "Iteration 4, loss = 10.85437946\n",
      "Iteration 6, loss = 7.58961908\n",
      "Iteration 5, loss = 9.94457743\n",
      "Iteration 7, loss = 9.87883693\n",
      "Iteration 6, loss = 7.98266902\n",
      "Iteration 8, loss = 11.83256166\n",
      "Iteration 7, loss = 8.72613787\n",
      "Iteration 9, loss = 9.45008773\n",
      "Iteration 8, loss = 7.48971739\n",
      "Iteration 10, loss = 6.49584489\n",
      "Iteration 9, loss = 5.81614013\n",
      "Iteration 11, loss = 7.35588111\n",
      "Iteration 10, loss = 6.87344926\n",
      "Iteration 12, loss = 7.03945915\n",
      "Iteration 11, loss = 5.98889854\n",
      "Iteration 13, loss = 9.45456455\n",
      "Iteration 12, loss = 7.51991169\n",
      "Iteration 14, loss = 7.21685220\n",
      "Iteration 13, loss = 6.99573972\n",
      "Iteration 15, loss = 6.60417523\n",
      "Iteration 14, loss = 6.38218712\n",
      "Iteration 16, loss = 7.11921162\n",
      "Iteration 15, loss = 6.03793199\n",
      "Iteration 17, loss = 6.41986225\n",
      "Iteration 16, loss = 5.80605549\n",
      "Iteration 18, loss = 7.16415336\n",
      "Iteration 17, loss = 5.45850861\n",
      "Iteration 19, loss = 7.11766509\n",
      "Iteration 18, loss = 5.58064664\n",
      "Iteration 20, loss = 6.48332792\n",
      "Iteration 19, loss = 4.44559984\n",
      "Iteration 21, loss = 6.15348115\n",
      "Iteration 20, loss = 5.82338693\n",
      "Iteration 22, loss = 5.89209449\n",
      "Iteration 21, loss = 5.94634441\n",
      "Iteration 23, loss = 5.27466463\n",
      "Iteration 22, loss = 6.46125817\n",
      "Iteration 24, loss = 5.79333252\n",
      "Iteration 23, loss = 5.09808881\n",
      "Iteration 25, loss = 7.56310721\n",
      "Iteration 24, loss = 5.38329069\n",
      "Iteration 26, loss = 5.60194813\n",
      "Iteration 25, loss = 5.35442519\n",
      "Iteration 27, loss = 5.24736109\n",
      "Iteration 26, loss = 5.20474210\n",
      "Iteration 28, loss = 5.67798219\n",
      "Iteration 27, loss = 6.19274844\n",
      "Iteration 29, loss = 6.10698603\n",
      "Iteration 28, loss = 4.94269666\n",
      "Iteration 30, loss = 5.86976012\n",
      "Iteration 29, loss = 5.41824070\n",
      "Iteration 31, loss = 5.60238494\n",
      "Iteration 30, loss = 5.69613807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 6.19376227\n",
      "Iteration 33, loss = 4.90096542\n",
      "Iteration 34, loss = 5.29470110\n",
      "Iteration 35, loss = 4.62862008\n",
      "Iteration 36, loss = 4.55223994\n",
      "Iteration 37, loss = 4.19937768\n",
      "Iteration 38, loss = 5.88527856\n",
      "Iteration 39, loss = 5.80616920\n",
      "Iteration 40, loss = 4.74603757\n",
      "Iteration 41, loss = 4.94133927\n",
      "Iteration 42, loss = 4.54756562\n",
      "Iteration 43, loss = 4.54614580\n",
      "Iteration 44, loss = 4.78012145\n",
      "Iteration 45, loss = 5.05377771\n",
      "Iteration 46, loss = 5.01189950\n",
      "Iteration 47, loss = 5.00910957\n",
      "Iteration 48, loss = 5.71264294\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.34164232\n",
      "Iteration 2, loss = 15.57234849\n",
      "Iteration 3, loss = 11.47040510\n",
      "Iteration 4, loss = 12.99754234\n",
      "Iteration 5, loss = 12.35010918\n",
      "Iteration 6, loss = 11.71391492\n",
      "Iteration 7, loss = 10.52686824\n",
      "Iteration 8, loss = 8.98465787\n",
      "Iteration 9, loss = 7.17521907\n",
      "Iteration 10, loss = 6.22986616\n",
      "Iteration 11, loss = 8.99441679\n",
      "Iteration 12, loss = 7.74388113\n",
      "Iteration 13, loss = 6.56955661\n",
      "Iteration 14, loss = 6.35315926\n",
      "Iteration 15, loss = 7.34248934\n",
      "Iteration 16, loss = 5.55922699\n",
      "Iteration 1, loss = 16.03016501\n",
      "Iteration 17, loss = 4.86562908\n",
      "Iteration 2, loss = 14.05489557\n",
      "Iteration 18, loss = 4.79531843\n",
      "Iteration 3, loss = 11.11121378\n",
      "Iteration 19, loss = 5.11557155\n",
      "Iteration 4, loss = 8.75379549\n",
      "Iteration 20, loss = 5.12225884\n",
      "Iteration 5, loss = 10.19351930\n",
      "Iteration 21, loss = 4.77714401\n",
      "Iteration 6, loss = 10.00205943\n",
      "Iteration 22, loss = 6.50993836\n",
      "Iteration 7, loss = 9.85146436\n",
      "Iteration 23, loss = 5.53406050\n",
      "Iteration 8, loss = 7.07688995\n",
      "Iteration 24, loss = 5.30336867\n",
      "Iteration 9, loss = 7.14530100\n",
      "Iteration 25, loss = 5.54432736\n",
      "Iteration 10, loss = 7.54736469\n",
      "Iteration 26, loss = 5.82439004\n",
      "Iteration 11, loss = 6.48923413\n",
      "Iteration 27, loss = 4.76877072\n",
      "Iteration 12, loss = 6.04958987\n",
      "Iteration 28, loss = 4.62745838\n",
      "Iteration 13, loss = 5.83622892\n",
      "Iteration 29, loss = 4.34299383\n",
      "Iteration 14, loss = 6.10052453\n",
      "Iteration 30, loss = 4.26797640\n",
      "Iteration 15, loss = 6.05435220\n",
      "Iteration 31, loss = 4.42710340\n",
      "Iteration 16, loss = 6.02108439\n",
      "Iteration 32, loss = 4.70319718\n",
      "Iteration 17, loss = 6.49564700\n",
      "Iteration 33, loss = 4.16088401\n",
      "Iteration 18, loss = 5.13558647\n",
      "Iteration 34, loss = 4.54029717\n",
      "Iteration 19, loss = 5.36151253\n",
      "Iteration 35, loss = 4.31015623\n",
      "Iteration 20, loss = 5.40157609\n",
      "Iteration 36, loss = 4.50524843\n",
      "Iteration 21, loss = 4.85299411\n",
      "Iteration 37, loss = 5.18897121\n",
      "Iteration 22, loss = 5.24650573\n",
      "Iteration 38, loss = 5.09292786\n",
      "Iteration 23, loss = 6.34640938\n",
      "Iteration 39, loss = 4.85860201\n",
      "Iteration 24, loss = 4.89711276\n",
      "Iteration 40, loss = 5.44976369\n",
      "Iteration 25, loss = 5.20455471\n",
      "Iteration 41, loss = 4.78492634\n",
      "Iteration 26, loss = 4.72952436\n",
      "Iteration 42, loss = 4.79037100\n",
      "Iteration 27, loss = 4.42703206\n",
      "Iteration 43, loss = 4.05847711\n",
      "Iteration 28, loss = 4.50503330\n",
      "Iteration 44, loss = 4.46425577\n",
      "Iteration 29, loss = 4.07468494\n",
      "Iteration 45, loss = 4.23887866\n",
      "Iteration 30, loss = 3.64445134\n",
      "Iteration 46, loss = 4.44197390\n",
      "Iteration 31, loss = 3.99811731\n",
      "Iteration 47, loss = 4.56417699\n",
      "Iteration 32, loss = 4.90029243\n",
      "Iteration 48, loss = 4.29250267\n",
      "Iteration 33, loss = 4.54772601\n",
      "Iteration 49, loss = 4.45085886\n",
      "Iteration 34, loss = 5.90622699\n",
      "Iteration 50, loss = 4.60845679\n",
      "Iteration 35, loss = 5.31146079\n",
      "Iteration 51, loss = 3.66838983\n",
      "Iteration 36, loss = 4.94105040\n",
      "Iteration 52, loss = 3.94376659\n",
      "Iteration 37, loss = 4.70284999\n",
      "Iteration 53, loss = 4.02299951\n",
      "Iteration 38, loss = 4.43224359\n",
      "Iteration 54, loss = 4.21980364\n",
      "Iteration 39, loss = 4.39982999\n",
      "Iteration 55, loss = 4.10226490\n",
      "Iteration 40, loss = 4.51329341\n",
      "Iteration 56, loss = 4.41108052\n",
      "Iteration 41, loss = 4.16052772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 4.92440707\n",
      "Iteration 58, loss = 4.33738758\n",
      "Iteration 59, loss = 4.49410348\n",
      "Iteration 60, loss = 4.29767999\n",
      "Iteration 61, loss = 3.52138002\n",
      "Iteration 62, loss = 3.39550142\n",
      "Iteration 63, loss = 3.70933199\n",
      "Iteration 64, loss = 3.94496091\n",
      "Iteration 65, loss = 3.74883487\n",
      "Iteration 66, loss = 3.98386147\n",
      "Iteration 67, loss = 3.73953986\n",
      "Iteration 68, loss = 4.41434240\n",
      "Iteration 69, loss = 4.92293605\n",
      "Iteration 70, loss = 4.25534633\n",
      "Iteration 71, loss = 4.56877094\n",
      "Iteration 72, loss = 3.94068633\n",
      "Iteration 73, loss = 3.78345921\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.87261877\n",
      "Iteration 2, loss = 17.56066679\n",
      "Iteration 3, loss = 13.81212624\n",
      "Iteration 4, loss = 11.84628823\n",
      "Iteration 5, loss = 8.15549862\n",
      "Iteration 6, loss = 7.46791790\n",
      "Iteration 7, loss = 8.98300247\n",
      "Iteration 8, loss = 7.94150196\n",
      "Iteration 9, loss = 6.46295938\n",
      "Iteration 10, loss = 8.67877057\n",
      "Iteration 11, loss = 10.31224998\n",
      "Iteration 12, loss = 8.43164654\n",
      "Iteration 13, loss = 7.92421604\n",
      "Iteration 14, loss = 7.68320573\n",
      "Iteration 15, loss = 6.33834056\n",
      "Iteration 16, loss = 6.36020361\n",
      "Iteration 1, loss = 16.26664678\n",
      "Iteration 17, loss = 6.64870220\n",
      "Iteration 2, loss = 17.01979203\n",
      "Iteration 18, loss = 6.81549488\n",
      "Iteration 3, loss = 13.90043642\n",
      "Iteration 19, loss = 6.69030937\n",
      "Iteration 4, loss = 11.21769498\n",
      "Iteration 20, loss = 6.12336041\n",
      "Iteration 5, loss = 9.78307904\n",
      "Iteration 21, loss = 5.89000975\n",
      "Iteration 6, loss = 8.20024253\n",
      "Iteration 22, loss = 5.54117718\n",
      "Iteration 7, loss = 10.94197644\n",
      "Iteration 23, loss = 5.97770244\n",
      "Iteration 8, loss = 7.86116035\n",
      "Iteration 24, loss = 5.27277208\n",
      "Iteration 9, loss = 8.10136542\n",
      "Iteration 25, loss = 4.52789185\n",
      "Iteration 10, loss = 7.86621582\n",
      "Iteration 26, loss = 4.64637545\n",
      "Iteration 11, loss = 8.27036947\n",
      "Iteration 27, loss = 4.48901625\n",
      "Iteration 12, loss = 7.77863162\n",
      "Iteration 28, loss = 5.94199782\n",
      "Iteration 13, loss = 8.39380172\n",
      "Iteration 29, loss = 5.04050341\n",
      "Iteration 14, loss = 7.14504849\n",
      "Iteration 30, loss = 4.81704494\n",
      "Iteration 15, loss = 6.23065467\n",
      "Iteration 31, loss = 4.65436364\n",
      "Iteration 16, loss = 5.81271019\n",
      "Iteration 32, loss = 4.49951339\n",
      "Iteration 17, loss = 5.55720379\n",
      "Iteration 33, loss = 4.69671675\n",
      "Iteration 18, loss = 5.80655504\n",
      "Iteration 34, loss = 4.06905112\n",
      "Iteration 19, loss = 5.50133049\n",
      "Iteration 35, loss = 4.24525578\n",
      "Iteration 20, loss = 7.97974712\n",
      "Iteration 36, loss = 4.14687950\n",
      "Iteration 21, loss = 7.12640979\n",
      "Iteration 37, loss = 4.34286171\n",
      "Iteration 22, loss = 6.19811321\n",
      "Iteration 38, loss = 4.02837876\n",
      "Iteration 23, loss = 6.09010139\n",
      "Iteration 39, loss = 4.53852304\n",
      "Iteration 24, loss = 5.27255663\n",
      "Iteration 40, loss = 4.32205540\n",
      "Iteration 25, loss = 5.74786418\n",
      "Iteration 41, loss = 5.11018743\n",
      "Iteration 26, loss = 5.20131391\n",
      "Iteration 42, loss = 3.91065438\n",
      "Iteration 27, loss = 6.37949198\n",
      "Iteration 43, loss = 5.08832835\n",
      "Iteration 28, loss = 5.04752694\n",
      "Iteration 44, loss = 4.38163543\n",
      "Iteration 29, loss = 5.16823193\n",
      "Iteration 45, loss = 4.57851331\n",
      "Iteration 30, loss = 5.60174231\n",
      "Iteration 46, loss = 3.91102971\n",
      "Iteration 31, loss = 5.49042249\n",
      "Iteration 47, loss = 3.87117604\n",
      "Iteration 32, loss = 5.26594106\n",
      "Iteration 48, loss = 3.94891689\n",
      "Iteration 33, loss = 4.72578729\n",
      "Iteration 49, loss = 4.38013196\n",
      "Iteration 34, loss = 4.73075996\n",
      "Iteration 50, loss = 4.53666630\n",
      "Iteration 35, loss = 4.53740127\n",
      "Iteration 51, loss = 5.36091436\n",
      "Iteration 36, loss = 4.73586558\n",
      "Iteration 52, loss = 4.84972919\n",
      "Iteration 37, loss = 4.69892416\n",
      "Iteration 53, loss = 4.33754716\n",
      "Iteration 38, loss = 4.69858581\n",
      "Iteration 54, loss = 4.61078641\n",
      "Iteration 39, loss = 4.22552449\n",
      "Iteration 55, loss = 4.03838484\n",
      "Iteration 40, loss = 4.37920681\n",
      "Iteration 56, loss = 4.60908432\n",
      "Iteration 41, loss = 4.84779262\n",
      "Iteration 57, loss = 6.80487915\n",
      "Iteration 42, loss = 4.65290902\n",
      "Iteration 58, loss = 5.35346647\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 5.36361913\n",
      "Iteration 44, loss = 5.30372068\n",
      "Iteration 45, loss = 5.28995783\n",
      "Iteration 46, loss = 4.70192067\n",
      "Iteration 47, loss = 4.74084646\n",
      "Iteration 48, loss = 4.15155261\n",
      "Iteration 49, loss = 4.34720255\n",
      "Iteration 50, loss = 4.22762167\n",
      "Iteration 51, loss = 4.53955324\n",
      "Iteration 52, loss = 4.22529915\n",
      "Iteration 53, loss = 3.75378135\n",
      "Iteration 54, loss = 4.61714236\n",
      "Iteration 55, loss = 4.85356144\n",
      "Iteration 56, loss = 4.10872049\n",
      "Iteration 57, loss = 4.09840530\n",
      "Iteration 58, loss = 4.46591900\n",
      "Iteration 59, loss = 4.57978132\n",
      "Iteration 1, loss = 13.50138493\n",
      "Iteration 60, loss = 3.75465330\n",
      "Iteration 2, loss = 11.53105344\n",
      "Iteration 61, loss = 4.49749170\n",
      "Iteration 3, loss = 13.60738571\n",
      "Iteration 62, loss = 4.69140217\n",
      "Iteration 4, loss = 9.41641796\n",
      "Iteration 63, loss = 5.04411899\n",
      "Iteration 5, loss = 9.66340803\n",
      "Iteration 64, loss = 5.00511924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 6, loss = 10.44892063\n",
      "Iteration 7, loss = 8.81643386\n",
      "Iteration 8, loss = 8.40888005\n",
      "Iteration 9, loss = 8.13282411\n",
      "Iteration 10, loss = 8.32652590\n",
      "Iteration 11, loss = 7.27090522\n",
      "Iteration 12, loss = 8.57398048\n",
      "Iteration 13, loss = 7.18180589\n",
      "Iteration 14, loss = 6.35714212\n",
      "Iteration 15, loss = 7.20446817\n",
      "Iteration 16, loss = 7.10154936\n",
      "Iteration 17, loss = 6.56019525\n",
      "Iteration 18, loss = 6.05596905\n",
      "Iteration 19, loss = 5.82612478\n",
      "Iteration 20, loss = 5.98673079\n",
      "Iteration 21, loss = 5.83221834\n",
      "Iteration 22, loss = 8.18625687\n",
      "Iteration 23, loss = 6.34211258\n",
      "Iteration 1, loss = 18.65653106\n",
      "Iteration 24, loss = 5.44205480\n",
      "Iteration 2, loss = 11.90718545\n",
      "Iteration 25, loss = 6.07360068\n",
      "Iteration 3, loss = 10.18522081\n",
      "Iteration 26, loss = 5.23640337\n",
      "Iteration 4, loss = 8.44790961\n",
      "Iteration 27, loss = 5.60918132\n",
      "Iteration 5, loss = 11.80509974\n",
      "Iteration 28, loss = 5.65235272\n",
      "Iteration 6, loss = 8.37048795\n",
      "Iteration 29, loss = 6.20830096\n",
      "Iteration 7, loss = 8.68480967\n",
      "Iteration 30, loss = 6.05778429\n",
      "Iteration 8, loss = 7.89314721\n",
      "Iteration 31, loss = 5.43425462\n",
      "Iteration 9, loss = 6.97625161\n",
      "Iteration 32, loss = 6.47346261\n",
      "Iteration 10, loss = 7.06664676\n",
      "Iteration 33, loss = 6.30204268\n",
      "Iteration 11, loss = 6.59056669\n",
      "Iteration 34, loss = 5.20531173\n",
      "Iteration 12, loss = 11.89637791\n",
      "Iteration 35, loss = 5.79503381\n",
      "Iteration 13, loss = 11.83208393\n",
      "Iteration 36, loss = 5.75637398\n",
      "Iteration 14, loss = 9.79816245\n",
      "Iteration 37, loss = 5.30492389\n",
      "Iteration 15, loss = 7.22800043\n",
      "Iteration 38, loss = 5.67816211\n",
      "Iteration 16, loss = 6.10785467\n",
      "Iteration 39, loss = 5.37825177\n",
      "Iteration 17, loss = 6.64768696\n",
      "Iteration 40, loss = 5.51857858\n",
      "Iteration 18, loss = 6.58401238\n",
      "Iteration 41, loss = 5.26250305\n",
      "Iteration 19, loss = 6.48647003\n",
      "Iteration 42, loss = 5.66757219\n",
      "Iteration 20, loss = 5.57980392\n",
      "Iteration 43, loss = 5.83196955\n",
      "Iteration 21, loss = 6.73349171\n",
      "Iteration 44, loss = 6.38496622\n",
      "Iteration 22, loss = 5.36627608\n",
      "Iteration 45, loss = 5.68029372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 5.91678745\n",
      "Iteration 24, loss = 5.16087285\n",
      "Iteration 25, loss = 4.90578470\n",
      "Iteration 26, loss = 4.44831266\n",
      "Iteration 27, loss = 4.81066543\n",
      "Iteration 28, loss = 4.54149858\n",
      "Iteration 29, loss = 4.89924141\n",
      "Iteration 30, loss = 5.17953876\n",
      "Iteration 31, loss = 4.90856926\n",
      "Iteration 32, loss = 5.42033017\n",
      "Iteration 33, loss = 4.52088586\n",
      "Iteration 34, loss = 4.28774562\n",
      "Iteration 35, loss = 4.09178879\n",
      "Iteration 36, loss = 4.01303861\n",
      "Iteration 37, loss = 4.01309104\n",
      "Iteration 38, loss = 4.26509436\n",
      "Iteration 39, loss = 3.97560999\n",
      "Iteration 40, loss = 4.17352556\n",
      "Iteration 1, loss = 18.66314754\n",
      "Iteration 41, loss = 4.48785811\n",
      "Iteration 2, loss = 16.89806397\n",
      "Iteration 42, loss = 4.64650504\n",
      "Iteration 3, loss = 12.16802932\n",
      "Iteration 43, loss = 4.25517369\n",
      "Iteration 4, loss = 13.26164653\n",
      "Iteration 44, loss = 4.05968702\n",
      "Iteration 5, loss = 13.23366866\n",
      "Iteration 45, loss = 4.09914692\n",
      "Iteration 6, loss = 10.69738061\n",
      "Iteration 46, loss = 4.09957618\n",
      "Iteration 7, loss = 9.84954889\n",
      "Iteration 47, loss = 3.66924688\n",
      "Iteration 8, loss = 9.04548929\n",
      "Iteration 48, loss = 3.86594935\n",
      "Iteration 9, loss = 8.56286429\n",
      "Iteration 49, loss = 4.53298911\n",
      "Iteration 10, loss = 6.13364614\n",
      "Iteration 50, loss = 4.65185685\n",
      "Iteration 11, loss = 7.95452188\n",
      "Iteration 51, loss = 5.63471805\n",
      "Iteration 12, loss = 8.02883933\n",
      "Iteration 52, loss = 5.12643514\n",
      "Iteration 13, loss = 7.11241937\n",
      "Iteration 53, loss = 4.61814648\n",
      "Iteration 14, loss = 6.18852343\n",
      "Iteration 54, loss = 4.42301759\n",
      "Iteration 15, loss = 6.07598069\n",
      "Iteration 55, loss = 4.03212539\n",
      "Iteration 16, loss = 6.18851402\n",
      "Iteration 56, loss = 4.81927010\n",
      "Iteration 17, loss = 5.58457668\n",
      "Iteration 57, loss = 4.70374202\n",
      "Iteration 18, loss = 8.07002705\n",
      "Iteration 58, loss = 4.82606478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 6.40086254\n",
      "Iteration 20, loss = 6.56097978\n",
      "Iteration 21, loss = 6.34022681\n",
      "Iteration 22, loss = 6.02718243\n",
      "Iteration 23, loss = 6.76283820\n",
      "Iteration 24, loss = 5.72332897\n",
      "Iteration 25, loss = 5.53855234\n",
      "Iteration 26, loss = 5.43099513\n",
      "Iteration 27, loss = 5.28675263\n",
      "Iteration 28, loss = 6.32108232\n",
      "Iteration 29, loss = 4.69297867\n",
      "Iteration 30, loss = 4.55256440\n",
      "Iteration 31, loss = 4.21109233\n",
      "Iteration 32, loss = 4.29839545\n",
      "Iteration 33, loss = 3.59810917\n",
      "Iteration 34, loss = 4.07341246\n",
      "Iteration 35, loss = 3.84176972\n",
      "Iteration 1, loss = 22.53205955\n",
      "Iteration 36, loss = 3.96255158\n",
      "Iteration 2, loss = 24.18320866\n",
      "Iteration 37, loss = 3.92507395\n",
      "Iteration 3, loss = 10.12135804\n",
      "Iteration 38, loss = 4.08211763\n",
      "Iteration 4, loss = 8.60906344\n",
      "Iteration 39, loss = 4.19895614\n",
      "Iteration 5, loss = 7.01645443\n",
      "Iteration 40, loss = 4.00304696\n",
      "Iteration 6, loss = 9.88031710\n",
      "Iteration 41, loss = 4.55417863\n",
      "Iteration 7, loss = 11.44183438\n",
      "Iteration 42, loss = 4.00670702\n",
      "Iteration 8, loss = 11.84860394\n",
      "Iteration 43, loss = 4.67423459\n",
      "Iteration 9, loss = 9.74333513\n",
      "Iteration 44, loss = 3.92858028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 11.25857331\n",
      "Iteration 11, loss = 10.64797162\n",
      "Iteration 12, loss = 7.58506765\n",
      "Iteration 13, loss = 8.07341470\n",
      "Iteration 14, loss = 8.14618765\n",
      "Iteration 15, loss = 7.57839183\n",
      "Iteration 16, loss = 8.36742794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.23878518\n",
      "Iteration 2, loss = 14.30781952\n",
      "Iteration 3, loss = 7.82633125\n",
      "Iteration 4, loss = 9.56729074\n",
      "Iteration 5, loss = 10.50115941\n",
      "Iteration 6, loss = 10.23145525\n",
      "Iteration 7, loss = 9.27592349\n",
      "Iteration 8, loss = 9.58387835\n",
      "Iteration 1, loss = 15.87327285\n",
      "Iteration 9, loss = 6.42060462\n",
      "Iteration 2, loss = 16.75916183\n",
      "Iteration 10, loss = 7.71826492\n",
      "Iteration 3, loss = 9.27958045\n",
      "Iteration 11, loss = 6.37808348\n",
      "Iteration 4, loss = 9.84474868\n",
      "Iteration 12, loss = 6.16726083\n",
      "Iteration 5, loss = 8.42145746\n",
      "Iteration 13, loss = 6.44007115\n",
      "Iteration 6, loss = 7.55670945\n",
      "Iteration 14, loss = 7.73170919\n",
      "Iteration 7, loss = 8.44044195\n",
      "Iteration 15, loss = 8.62928190\n",
      "Iteration 8, loss = 15.05907149\n",
      "Iteration 16, loss = 6.73925260\n",
      "Iteration 9, loss = 12.91417234\n",
      "Iteration 17, loss = 6.42578112\n",
      "Iteration 10, loss = 7.91263497\n",
      "Iteration 18, loss = 6.02932540\n",
      "Iteration 11, loss = 8.01698414\n",
      "Iteration 19, loss = 5.51262179\n",
      "Iteration 12, loss = 7.47501519\n",
      "Iteration 20, loss = 6.48218573\n",
      "Iteration 13, loss = 7.46213111\n",
      "Iteration 21, loss = 5.72170259\n",
      "Iteration 14, loss = 6.93151877\n",
      "Iteration 22, loss = 7.11159932\n",
      "Iteration 15, loss = 6.65875336\n",
      "Iteration 23, loss = 5.79022502\n",
      "Iteration 16, loss = 6.99399695\n",
      "Iteration 24, loss = 5.24877498\n",
      "Iteration 17, loss = 5.86420461\n",
      "Iteration 25, loss = 5.29454947\n",
      "Iteration 18, loss = 7.58547335\n",
      "Iteration 26, loss = 6.08696043\n",
      "Iteration 19, loss = 6.94520466\n",
      "Iteration 27, loss = 5.97247122\n",
      "Iteration 20, loss = 5.59282877\n",
      "Iteration 28, loss = 5.26664961\n",
      "Iteration 21, loss = 5.33002123\n",
      "Iteration 29, loss = 4.87558098\n",
      "Iteration 22, loss = 5.68872718\n",
      "Iteration 30, loss = 4.75867592\n",
      "Iteration 23, loss = 5.34113027\n",
      "Iteration 31, loss = 4.72021926\n",
      "Iteration 24, loss = 5.30653630\n",
      "Iteration 32, loss = 4.64004097\n",
      "Iteration 25, loss = 4.75888889\n",
      "Iteration 33, loss = 4.67668558\n",
      "Iteration 26, loss = 5.34692714\n",
      "Iteration 34, loss = 5.26466410\n",
      "Iteration 27, loss = 5.68302715\n",
      "Iteration 35, loss = 5.14491548\n",
      "Iteration 28, loss = 5.78986354\n",
      "Iteration 36, loss = 4.98552881\n",
      "Iteration 29, loss = 6.31700174\n",
      "Iteration 37, loss = 5.33507490\n",
      "Iteration 30, loss = 5.46796416\n",
      "Iteration 38, loss = 5.99959570\n",
      "Iteration 31, loss = 6.26183334\n",
      "Iteration 39, loss = 4.68071869\n",
      "Iteration 32, loss = 5.13052548\n",
      "Iteration 40, loss = 4.73947688\n",
      "Iteration 33, loss = 4.97813976\n",
      "Iteration 41, loss = 4.73801138\n",
      "Iteration 34, loss = 4.58965352\n",
      "Iteration 42, loss = 4.85583805\n",
      "Iteration 35, loss = 4.98485084\n",
      "Iteration 43, loss = 4.97314285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 5.41710327\n",
      "Iteration 37, loss = 5.14315115\n",
      "Iteration 38, loss = 5.26203405\n",
      "Iteration 39, loss = 4.73270081\n",
      "Iteration 40, loss = 5.14516464\n",
      "Iteration 41, loss = 5.61559518\n",
      "Iteration 42, loss = 5.69093968\n",
      "Iteration 43, loss = 5.61117319\n",
      "Iteration 44, loss = 4.84529107\n",
      "Iteration 45, loss = 4.62890395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.81613112\n",
      "Iteration 2, loss = 14.31684340\n",
      "Iteration 3, loss = 11.64048702\n",
      "Iteration 4, loss = 11.55289602\n",
      "Iteration 5, loss = 10.47232177\n",
      "Iteration 6, loss = 8.40159500\n",
      "Iteration 7, loss = 7.67955291\n",
      "Iteration 8, loss = 7.36367830\n",
      "Iteration 9, loss = 12.41105630\n",
      "Iteration 1, loss = 16.44467621\n",
      "Iteration 10, loss = 8.69697564\n",
      "Iteration 2, loss = 15.71844012\n",
      "Iteration 11, loss = 7.30716280\n",
      "Iteration 3, loss = 9.79199004\n",
      "Iteration 12, loss = 6.58319300\n",
      "Iteration 4, loss = 10.02194411\n",
      "Iteration 13, loss = 7.14880355\n",
      "Iteration 5, loss = 12.27441616\n",
      "Iteration 14, loss = 6.29128331\n",
      "Iteration 6, loss = 14.79477537\n",
      "Iteration 15, loss = 6.12949478\n",
      "Iteration 7, loss = 12.85595538\n",
      "Iteration 16, loss = 6.15871442\n",
      "Iteration 8, loss = 8.98809229\n",
      "Iteration 17, loss = 6.57676782\n",
      "Iteration 9, loss = 8.98317581\n",
      "Iteration 18, loss = 6.86970004\n",
      "Iteration 10, loss = 10.56910872\n",
      "Iteration 19, loss = 6.29079292\n",
      "Iteration 11, loss = 7.87990559\n",
      "Iteration 20, loss = 5.90378363\n",
      "Iteration 12, loss = 8.75631791\n",
      "Iteration 21, loss = 6.06442367\n",
      "Iteration 13, loss = 6.56115575\n",
      "Iteration 22, loss = 5.36219400\n",
      "Iteration 14, loss = 6.40167843\n",
      "Iteration 23, loss = 5.32712823\n",
      "Iteration 15, loss = 6.27637984\n",
      "Iteration 24, loss = 5.84167081\n",
      "Iteration 16, loss = 6.03067117\n",
      "Iteration 25, loss = 5.06083805\n",
      "Iteration 17, loss = 5.34436633\n",
      "Iteration 26, loss = 5.88885968\n",
      "Iteration 18, loss = 5.12677359\n",
      "Iteration 27, loss = 4.91252685\n",
      "Iteration 19, loss = 5.10088942\n",
      "Iteration 28, loss = 5.07243892\n",
      "Iteration 20, loss = 5.34544341\n",
      "Iteration 29, loss = 5.35020093\n",
      "Iteration 21, loss = 5.27417082\n",
      "Iteration 30, loss = 6.29278186\n",
      "Iteration 22, loss = 4.78533818\n",
      "Iteration 31, loss = 5.42991319\n",
      "Iteration 23, loss = 4.50731768\n",
      "Iteration 32, loss = 4.80206663\n",
      "Iteration 24, loss = 5.41808340\n",
      "Iteration 33, loss = 5.46910895\n",
      "Iteration 25, loss = 5.58092176\n",
      "Iteration 34, loss = 6.05713088\n",
      "Iteration 26, loss = 6.56483095\n",
      "Iteration 35, loss = 6.13440880\n",
      "Iteration 27, loss = 5.65532858\n",
      "Iteration 36, loss = 5.46731130\n",
      "Iteration 28, loss = 5.30183510\n",
      "Iteration 37, loss = 5.11423284\n",
      "Iteration 29, loss = 3.90414714\n",
      "Iteration 38, loss = 5.50696191\n",
      "Iteration 30, loss = 4.18358183\n",
      "Iteration 39, loss = 5.38932141\n",
      "Iteration 31, loss = 4.06882193\n",
      "Iteration 40, loss = 5.38847002\n",
      "Iteration 32, loss = 4.14934309\n",
      "Iteration 41, loss = 5.42712740\n",
      "Iteration 33, loss = 4.03304873\n",
      "Iteration 42, loss = 5.30898255\n",
      "Iteration 34, loss = 3.56234016\n",
      "Iteration 43, loss = 5.07342028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 3.60079948\n",
      "Iteration 36, loss = 3.75758582\n",
      "Iteration 37, loss = 3.44351323\n",
      "Iteration 38, loss = 4.14924695\n",
      "Iteration 39, loss = 4.42438986\n",
      "Iteration 40, loss = 4.11015271\n",
      "Iteration 41, loss = 4.07070898\n",
      "Iteration 42, loss = 4.65914010\n",
      "Iteration 43, loss = 5.28837391\n",
      "Iteration 44, loss = 4.82051544\n",
      "Iteration 45, loss = 4.07789112\n",
      "Iteration 46, loss = 4.31509459\n",
      "Iteration 47, loss = 3.76599727\n",
      "Iteration 48, loss = 3.64816861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.54107692\n",
      "Iteration 2, loss = 14.04428180\n",
      "Iteration 3, loss = 9.88726169\n",
      "Iteration 4, loss = 11.33281455\n",
      "Iteration 5, loss = 9.44525665\n",
      "Iteration 6, loss = 11.70344723\n",
      "Iteration 7, loss = 13.58285179\n",
      "Iteration 8, loss = 8.59890954\n",
      "Iteration 9, loss = 8.35201223\n",
      "Iteration 10, loss = 7.39310932\n",
      "Iteration 11, loss = 7.08189012\n",
      "Iteration 12, loss = 9.25774423\n",
      "Iteration 13, loss = 9.40059281\n",
      "Iteration 14, loss = 8.76519645\n",
      "Iteration 15, loss = 7.73548470\n",
      "Iteration 16, loss = 7.60956852\n",
      "Iteration 17, loss = 6.58123411\n",
      "Iteration 18, loss = 7.04273776\n",
      "Iteration 19, loss = 6.19210187\n",
      "Iteration 20, loss = 8.15309725\n",
      "Iteration 21, loss = 8.29626621\n",
      "Iteration 22, loss = 7.68943944\n",
      "Iteration 23, loss = 5.78562341\n",
      "Iteration 24, loss = 7.21774777\n",
      "Iteration 25, loss = 6.37094464\n",
      "Iteration 26, loss = 5.28892429\n",
      "Iteration 27, loss = 6.40159589\n",
      "Iteration 28, loss = 5.19631293\n",
      "Iteration 29, loss = 5.20517747\n",
      "Iteration 30, loss = 4.97543237\n",
      "Iteration 31, loss = 5.33171441\n",
      "Iteration 32, loss = 5.56934195\n",
      "Iteration 33, loss = 5.17861781\n",
      "Iteration 34, loss = 5.29569602\n",
      "Iteration 35, loss = 5.96495138\n",
      "Iteration 36, loss = 5.69108441\n",
      "Iteration 37, loss = 5.73299604\n",
      "Iteration 38, loss = 4.71719988\n",
      "Iteration 39, loss = 5.30894518\n",
      "Iteration 40, loss = 5.42849045\n",
      "Iteration 41, loss = 4.93287691\n",
      "Iteration 42, loss = 4.76404356\n",
      "Iteration 43, loss = 4.60694374\n",
      "Iteration 44, loss = 5.42295104\n",
      "Iteration 45, loss = 5.11319871\n",
      "Iteration 46, loss = 5.66696418\n",
      "Iteration 47, loss = 5.47870738\n",
      "Iteration 48, loss = 6.25783109\n",
      "Iteration 49, loss = 4.71544702\n",
      "Iteration 50, loss = 4.30620349\n",
      "Iteration 51, loss = 4.93608667\n",
      "Iteration 52, loss = 6.37662528\n",
      "Iteration 53, loss = 5.49058297\n",
      "Iteration 54, loss = 5.01796053\n",
      "Iteration 55, loss = 4.89917916\n",
      "Iteration 56, loss = 5.12576770\n",
      "Iteration 57, loss = 4.58800555\n",
      "Iteration 58, loss = 4.31458306\n",
      "Iteration 59, loss = 4.86464629\n",
      "Iteration 60, loss = 4.70780850\n",
      "Iteration 61, loss = 5.04463940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.32202393\n",
      "Iteration 2, loss = 1.06303840\n",
      "Iteration 3, loss = 0.91398348\n",
      "Iteration 4, loss = 0.81120925\n",
      "Iteration 5, loss = 0.72676976\n",
      "Iteration 6, loss = 0.66139515\n",
      "Iteration 7, loss = 0.60945056\n",
      "Iteration 8, loss = 0.56845652\n",
      "Iteration 9, loss = 0.53496663\n",
      "Iteration 10, loss = 0.50820583\n",
      "Iteration 11, loss = 0.48644959\n",
      "Iteration 12, loss = 0.46940506\n",
      "Iteration 13, loss = 0.45258272\n",
      "Iteration 14, loss = 0.43943668\n",
      "Iteration 15, loss = 0.42924365\n",
      "Iteration 16, loss = 0.41743662\n",
      "Iteration 17, loss = 0.40715613\n",
      "Iteration 18, loss = 0.39895578\n",
      "Iteration 19, loss = 0.39070732\n",
      "Iteration 20, loss = 0.38103962\n",
      "Iteration 21, loss = 0.37349189\n",
      "Iteration 22, loss = 0.36557343\n",
      "Iteration 23, loss = 0.36122518\n",
      "Iteration 24, loss = 0.35572401\n",
      "Iteration 25, loss = 0.34841750\n",
      "Iteration 26, loss = 0.34343336\n",
      "Iteration 27, loss = 0.33772211\n",
      "Iteration 28, loss = 0.33294891\n",
      "Iteration 29, loss = 0.32612646\n",
      "Iteration 30, loss = 0.32227293\n",
      "Iteration 31, loss = 0.31699880\n",
      "Iteration 32, loss = 0.31316553\n",
      "Iteration 33, loss = 0.30886152\n",
      "Iteration 34, loss = 0.30581887\n",
      "Iteration 35, loss = 0.30093538\n",
      "Iteration 36, loss = 0.29880370\n",
      "Iteration 37, loss = 0.29407972\n",
      "Iteration 38, loss = 0.29307914\n",
      "Iteration 39, loss = 0.28768520\n",
      "Iteration 40, loss = 0.28757006\n",
      "Iteration 41, loss = 0.27898011\n",
      "Iteration 42, loss = 0.27872406\n",
      "Iteration 43, loss = 0.27441330\n",
      "Iteration 44, loss = 0.26894138\n",
      "Iteration 45, loss = 0.26923392\n",
      "Iteration 46, loss = 0.26574781\n",
      "Iteration 47, loss = 0.25988240\n",
      "Iteration 48, loss = 0.25747160\n",
      "Iteration 49, loss = 0.25303676\n",
      "Iteration 50, loss = 0.25484550\n",
      "Iteration 51, loss = 0.24754536\n",
      "Iteration 52, loss = 0.25258669\n",
      "Iteration 53, loss = 0.24524806\n",
      "Iteration 54, loss = 0.24221870\n",
      "Iteration 55, loss = 0.24208934\n",
      "Iteration 56, loss = 0.24388171\n",
      "Iteration 57, loss = 0.23144003\n",
      "Iteration 58, loss = 0.23178409\n",
      "Iteration 59, loss = 0.23068079\n",
      "Iteration 60, loss = 0.23469147\n",
      "Iteration 61, loss = 0.22495016\n",
      "Iteration 62, loss = 0.22480363\n",
      "Iteration 63, loss = 0.22187647\n",
      "Iteration 64, loss = 0.21885347\n",
      "Iteration 65, loss = 0.21719498\n",
      "Iteration 66, loss = 0.21340678\n",
      "Iteration 67, loss = 0.21170335\n",
      "Iteration 68, loss = 0.21417928\n",
      "Iteration 69, loss = 0.20775848\n",
      "Iteration 70, loss = 0.20365128\n",
      "Iteration 71, loss = 0.20229214\n",
      "Iteration 72, loss = 0.20048466\n",
      "Iteration 73, loss = 0.19764091\n",
      "Iteration 74, loss = 0.19775217\n",
      "Iteration 75, loss = 0.19424064\n",
      "Iteration 76, loss = 0.19455751\n",
      "Iteration 77, loss = 0.19694241\n",
      "Iteration 78, loss = 0.19328894\n",
      "Iteration 79, loss = 0.19194810\n",
      "Iteration 80, loss = 0.19165396\n",
      "Iteration 81, loss = 0.18558546\n",
      "Iteration 82, loss = 0.19025894\n",
      "Iteration 83, loss = 0.18522652\n",
      "Iteration 84, loss = 0.18191053\n",
      "Iteration 85, loss = 0.17982611\n",
      "Iteration 86, loss = 0.18050289\n",
      "Iteration 87, loss = 0.17717089\n",
      "Iteration 88, loss = 0.17465756\n",
      "Iteration 89, loss = 0.17326528\n",
      "Iteration 90, loss = 0.17157168\n",
      "Iteration 91, loss = 0.17489707\n",
      "Iteration 92, loss = 0.17214197\n",
      "Iteration 93, loss = 0.17116779\n",
      "Iteration 94, loss = 0.17205420\n",
      "Iteration 95, loss = 0.16860800\n",
      "Iteration 96, loss = 0.16913204\n",
      "Iteration 97, loss = 0.16442893\n",
      "Iteration 98, loss = 0.16376143\n",
      "Iteration 99, loss = 0.15912128\n",
      "Iteration 100, loss = 0.16406446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                                        ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                          transformers=[(&#x27;Variables &#x27;\n",
       "                                                                         &#x27;Categóricas&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                         [&#x27;MSZoning&#x27;,\n",
       "                                                                          &#x27;Street&#x27;,\n",
       "                                                                          &#x27;Alley&#x27;,\n",
       "                                                                          &#x27;LotShape&#x27;,\n",
       "                                                                          &#x27;LandContour&#x27;,\n",
       "                                                                          &#x27;Utilities&#x27;,\n",
       "                                                                          &#x27;LotConfig&#x27;,\n",
       "                                                                          &#x27;LandSlope&#x27;,\n",
       "                                                                          &#x27;Neighborhood&#x27;,\n",
       "                                                                          &#x27;Condition1&#x27;,\n",
       "                                                                          &#x27;Condition2&#x27;,\n",
       "                                                                          &#x27;BldgType&#x27;,\n",
       "                                                                          &#x27;HouseStyle&#x27;,\n",
       "                                                                          &#x27;RoofSty...\n",
       "                                                                          &#x27;GarageArea&#x27;,\n",
       "                                                                          &#x27;WoodDeckSF&#x27;,\n",
       "                                                                          &#x27;OpenPorchSF&#x27;,\n",
       "                                                                          &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                                       (&#x27;mlpclassifier&#x27;,\n",
       "                                        MLPClassifier(activation=&#x27;identity&#x27;,\n",
       "                                                      hidden_layer_sizes=(),\n",
       "                                                      max_iter=300,\n",
       "                                                      verbose=True))]),\n",
       "             n_jobs=2,\n",
       "             param_grid={&#x27;mlpclassifier__hidden_layer_sizes&#x27;: ((30, 20),\n",
       "                                                               (50, 30),\n",
       "                                                               (10, 20)),\n",
       "                         &#x27;mlpclassifier__learning_rate_init&#x27;: (0.01, 0.001, 1,\n",
       "                                                               10),\n",
       "                         &#x27;mlpclassifier__max_iter&#x27;: (100, 200, 500, 1000)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                                        ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                                          transformers=[(&#x27;Variables &#x27;\n",
       "                                                                         &#x27;Categóricas&#x27;,\n",
       "                                                                         OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                                         [&#x27;MSZoning&#x27;,\n",
       "                                                                          &#x27;Street&#x27;,\n",
       "                                                                          &#x27;Alley&#x27;,\n",
       "                                                                          &#x27;LotShape&#x27;,\n",
       "                                                                          &#x27;LandContour&#x27;,\n",
       "                                                                          &#x27;Utilities&#x27;,\n",
       "                                                                          &#x27;LotConfig&#x27;,\n",
       "                                                                          &#x27;LandSlope&#x27;,\n",
       "                                                                          &#x27;Neighborhood&#x27;,\n",
       "                                                                          &#x27;Condition1&#x27;,\n",
       "                                                                          &#x27;Condition2&#x27;,\n",
       "                                                                          &#x27;BldgType&#x27;,\n",
       "                                                                          &#x27;HouseStyle&#x27;,\n",
       "                                                                          &#x27;RoofSty...\n",
       "                                                                          &#x27;GarageArea&#x27;,\n",
       "                                                                          &#x27;WoodDeckSF&#x27;,\n",
       "                                                                          &#x27;OpenPorchSF&#x27;,\n",
       "                                                                          &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                                       (&#x27;mlpclassifier&#x27;,\n",
       "                                        MLPClassifier(activation=&#x27;identity&#x27;,\n",
       "                                                      hidden_layer_sizes=(),\n",
       "                                                      max_iter=300,\n",
       "                                                      verbose=True))]),\n",
       "             n_jobs=2,\n",
       "             param_grid={&#x27;mlpclassifier__hidden_layer_sizes&#x27;: ((30, 20),\n",
       "                                                               (50, 30),\n",
       "                                                               (10, 20)),\n",
       "                         &#x27;mlpclassifier__learning_rate_init&#x27;: (0.01, 0.001, 1,\n",
       "                                                               10),\n",
       "                         &#x27;mlpclassifier__max_iter&#x27;: (100, 200, 500, 1000)})</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;Condition2&#x27;, &#x27;BldgType&#x27;,\n",
       "                                                   &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;,\n",
       "                                                   &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                                   &#x27;E...\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpclassifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(),\n",
       "                               max_iter=300, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;,\n",
       "                                  &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;,...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;identity&#x27;, hidden_layer_sizes=(), max_iter=300,\n",
       "              verbose=True)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          transformers=[('Variables '\n",
       "                                                                         'Categóricas',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         ['MSZoning',\n",
       "                                                                          'Street',\n",
       "                                                                          'Alley',\n",
       "                                                                          'LotShape',\n",
       "                                                                          'LandContour',\n",
       "                                                                          'Utilities',\n",
       "                                                                          'LotConfig',\n",
       "                                                                          'LandSlope',\n",
       "                                                                          'Neighborhood',\n",
       "                                                                          'Condition1',\n",
       "                                                                          'Condition2',\n",
       "                                                                          'BldgType',\n",
       "                                                                          'HouseStyle',\n",
       "                                                                          'RoofSty...\n",
       "                                                                          'GarageArea',\n",
       "                                                                          'WoodDeckSF',\n",
       "                                                                          'OpenPorchSF',\n",
       "                                                                          'EnclosedPorch', ...])])),\n",
       "                                       ('mlpclassifier',\n",
       "                                        MLPClassifier(activation='identity',\n",
       "                                                      hidden_layer_sizes=(),\n",
       "                                                      max_iter=300,\n",
       "                                                      verbose=True))]),\n",
       "             n_jobs=2,\n",
       "             param_grid={'mlpclassifier__hidden_layer_sizes': ((30, 20),\n",
       "                                                               (50, 30),\n",
       "                                                               (10, 20)),\n",
       "                         'mlpclassifier__learning_rate_init': (0.01, 0.001, 1,\n",
       "                                                               10),\n",
       "                         'mlpclassifier__max_iter': (100, 200, 500, 1000)})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametros_tun  = param_grid = {\n",
    "    'mlpclassifier__max_iter': (100,200,500,1000),\n",
    "    'mlpclassifier__learning_rate_init': (0.01, 0.001, 1,10),\n",
    "    'mlpclassifier__hidden_layer_sizes':((30,20),(50,30),(10,20))}\n",
    "model_grid_search = GridSearchCV(modelo1, param_grid=parametros_tun,\n",
    "                                 n_jobs=2, cv=10) #Vamos a usar dos procesadores(n_jobs), y 10 k-folds\n",
    "model_grid_search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlpclassifier__hidden_layer_sizes': (10, 20),\n",
       " 'mlpclassifier__learning_rate_init': 0.001,\n",
       " 'mlpclassifier__max_iter': 100}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model_grid_search.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      " [[125   0  27]\n",
      " [  0 114  23]\n",
      " [ 18  20 111]]\n",
      "Accuracy:  0.7990867579908676\n",
      "Precision:  0.7990867579908676\n",
      "recall:  0.7990867579908676\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(target_test,pred)\n",
    "accuracy=accuracy_score(target_test,pred)\n",
    "precision =precision_score(target_test,pred,average='micro')\n",
    "recall =  recall_score(target_test,pred,average='micro')\n",
    "f1 = f1_score(target_test,pred,average='micro')\n",
    "print('Matriz de confusión\\n',cm)\n",
    "print('Accuracy: ',accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('recall: ',recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo la variable respuesta SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500\n",
       "1       181500\n",
       "2       223500\n",
       "3       140000\n",
       "4       250000\n",
       "         ...  \n",
       "1455    175000\n",
       "1456    210000\n",
       "1457    266500\n",
       "1458    142125\n",
       "1459    147500\n",
       "Name: SalePrice, Length: 1460, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_df = pd.read_csv('train.csv')\n",
    "y = houses_df.pop(\"SalePrice\")\n",
    "X = houses_df\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador_categorico = Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")), (\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "preprocesador_numerico = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocesador = ColumnTransformer([\n",
    "    ('Variables Categóricas',preprocesador_categorico, categorical_columns),\n",
    "    ('Variables Numéricas',preprocesador_numerico, numerical_columns)\n",
    "], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888     268000\n",
       "939     244400\n",
       "285     164700\n",
       "927     176000\n",
       "565     128000\n",
       "         ...  \n",
       "1116    184100\n",
       "165     127500\n",
       "1279     68400\n",
       "1217    229456\n",
       "729     103000\n",
       "Name: SalePrice, Length: 1021, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-6 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-6 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-6 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-6 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-6 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-6 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-6 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-6 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-6 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></label><div class=\"sk-toggleable__content \"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-54\" type=\"checkbox\" ><label for=\"sk-estimator-id-54\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Categóricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-55\" type=\"checkbox\" ><label for=\"sk-estimator-id-55\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-56\" type=\"checkbox\" ><label for=\"sk-estimator-id-56\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-57\" type=\"checkbox\" ><label for=\"sk-estimator-id-57\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">Variables Numéricas</label><div class=\"sk-toggleable__content \"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-58\" type=\"checkbox\" ><label for=\"sk-estimator-id-58\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content \"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">remainder</label><div class=\"sk-toggleable__content \"><pre></pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">passthrough</label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label  sk-toggleable__label-arrow \">&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a></label><div class=\"sk-toggleable__content \"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   '...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpregressor',\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = make_pipeline(preprocesador, MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, max_iter=350, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-63\" type=\"checkbox\" ><label for=\"sk-estimator-id-63\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                                   &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;,\n",
       "                                                   &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;,\n",
       "                                                   &#x27;BsmtFullBath&#x27;,\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\" ><label for=\"sk-estimator-id-64\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\" ><label for=\"sk-estimator-id-65\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;Id&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-72\" type=\"checkbox\" ><label for=\"sk-estimator-id-72\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   '...\n",
       "                                                   'BsmtUnfSF', 'TotalBsmtSF',\n",
       "                                                   '1stFlrSF', '2ndFlrSF',\n",
       "                                                   'LowQualFinSF', 'GrLivArea',\n",
       "                                                   'BsmtFullBath',\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpregressor',\n",
       "                 MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=350))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustar el modelo\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (test): 1289490692.2606401\n",
      "MAE (test): 23742.27102880617\n",
      "RMSE (test): 35909.47914215187\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, pred1)\n",
    "print(\"MSE (test):\", mse)\n",
    "mae_test = mean_absolute_error(y_test, pred1)\n",
    "print(\"MAE (test):\", mae_test)\n",
    "rmse_test = np.sqrt(mse)\n",
    "print(\"RMSE (test):\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBetter = make_pipeline(preprocesador, MLPRegressor(hidden_layer_sizes=(150, 70), activation='relu', solver='adam', alpha=0.000001, batch_size='auto', learning_rate='constant', learning_rate_init=0.01, max_iter=350, shuffle=True, random_state=None, tol=0.0001, verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.5, beta_2=0.999, epsilon=1e-12, n_iter_no_change=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19600451544.78639984\n",
      "Iteration 2, loss = 18517273899.59008789\n",
      "Iteration 3, loss = 13850616208.85337830\n",
      "Iteration 4, loss = 7899354928.69705105\n",
      "Iteration 5, loss = 6850795261.13539982\n",
      "Iteration 6, loss = 6747533820.44307137\n",
      "Iteration 7, loss = 6621108281.39086533\n",
      "Iteration 8, loss = 6497209088.95252895\n",
      "Iteration 9, loss = 6384091152.68044949\n",
      "Iteration 10, loss = 6235164006.55463982\n",
      "Iteration 11, loss = 6070945466.80849552\n",
      "Iteration 12, loss = 5862603516.00449657\n",
      "Iteration 13, loss = 5640253866.83744526\n",
      "Iteration 14, loss = 5395098303.41687775\n",
      "Iteration 15, loss = 5204385403.24533176\n",
      "Iteration 16, loss = 4669099855.21670532\n",
      "Iteration 17, loss = 4206910575.99617052\n",
      "Iteration 18, loss = 3742998179.46111774\n",
      "Iteration 19, loss = 3173199461.00178003\n",
      "Iteration 20, loss = 2731469320.70029259\n",
      "Iteration 21, loss = 2090722427.49244785\n",
      "Iteration 22, loss = 1635308438.58852410\n",
      "Iteration 23, loss = 1328258197.55928469\n",
      "Iteration 24, loss = 1338832875.97600174\n",
      "Iteration 25, loss = 1031722584.75459528\n",
      "Iteration 26, loss = 906882917.42140150\n",
      "Iteration 27, loss = 852503184.39559531\n",
      "Iteration 28, loss = 803216278.92359996\n",
      "Iteration 29, loss = 808706758.94127202\n",
      "Iteration 30, loss = 814195290.83454478\n",
      "Iteration 31, loss = 900609525.49868894\n",
      "Iteration 32, loss = 730736052.97918832\n",
      "Iteration 33, loss = 718549899.70360518\n",
      "Iteration 34, loss = 784745294.27864599\n",
      "Iteration 35, loss = 688512849.91092706\n",
      "Iteration 36, loss = 871394586.32102680\n",
      "Iteration 37, loss = 702851463.46275103\n",
      "Iteration 38, loss = 750685231.26145411\n",
      "Iteration 39, loss = 712676922.88137007\n",
      "Iteration 40, loss = 786178267.25680172\n",
      "Iteration 41, loss = 1006146590.76893592\n",
      "Iteration 42, loss = 681089708.07962310\n",
      "Iteration 43, loss = 666856957.07814837\n",
      "Iteration 44, loss = 630340242.26678956\n",
      "Iteration 45, loss = 624055694.34898019\n",
      "Iteration 46, loss = 624742906.13555372\n",
      "Iteration 47, loss = 654497407.49192774\n",
      "Iteration 48, loss = 624282367.96684742\n",
      "Iteration 49, loss = 624297753.07452118\n",
      "Iteration 50, loss = 608371455.06895185\n",
      "Iteration 51, loss = 603969258.16087806\n",
      "Iteration 52, loss = 612251652.30486846\n",
      "Iteration 53, loss = 588679123.82570601\n",
      "Iteration 54, loss = 589784112.90039933\n",
      "Iteration 55, loss = 585090194.00752199\n",
      "Iteration 56, loss = 589655343.65993011\n",
      "Iteration 57, loss = 578636851.61148632\n",
      "Iteration 58, loss = 578424509.14038944\n",
      "Iteration 59, loss = 563154852.21575737\n",
      "Iteration 60, loss = 570757540.37323153\n",
      "Iteration 61, loss = 555386853.72477782\n",
      "Iteration 62, loss = 606421216.28016615\n",
      "Iteration 63, loss = 605828569.12313068\n",
      "Iteration 64, loss = 550382319.81344640\n",
      "Iteration 65, loss = 556830378.03208065\n",
      "Iteration 66, loss = 561066435.27516305\n",
      "Iteration 67, loss = 535465647.46512312\n",
      "Iteration 68, loss = 551790763.58319330\n",
      "Iteration 69, loss = 531441787.29558808\n",
      "Iteration 70, loss = 601458728.20754373\n",
      "Iteration 71, loss = 554565656.09028089\n",
      "Iteration 72, loss = 538439788.29995894\n",
      "Iteration 73, loss = 534911996.84725475\n",
      "Iteration 74, loss = 533764992.27618241\n",
      "Iteration 75, loss = 566730149.81460369\n",
      "Iteration 76, loss = 528445681.99773675\n",
      "Iteration 77, loss = 705504222.80496025\n",
      "Iteration 78, loss = 523705658.77552372\n",
      "Iteration 79, loss = 523476681.69757831\n",
      "Iteration 80, loss = 507998806.71263105\n",
      "Iteration 81, loss = 523038819.31766105\n",
      "Iteration 82, loss = 505986670.15116298\n",
      "Iteration 83, loss = 518052800.94612205\n",
      "Iteration 84, loss = 507755779.63488245\n",
      "Iteration 85, loss = 873528129.59989893\n",
      "Iteration 86, loss = 521949362.89785594\n",
      "Iteration 87, loss = 508125307.22781712\n",
      "Iteration 88, loss = 504596177.34013271\n",
      "Iteration 89, loss = 494220489.48657817\n",
      "Iteration 90, loss = 503053553.33181059\n",
      "Iteration 91, loss = 660770028.05876577\n",
      "Iteration 92, loss = 481757088.70728880\n",
      "Iteration 93, loss = 479741391.42268491\n",
      "Iteration 94, loss = 511953643.82248586\n",
      "Iteration 95, loss = 527818122.88166082\n",
      "Iteration 96, loss = 524861657.49876344\n",
      "Iteration 97, loss = 480962501.08285749\n",
      "Iteration 98, loss = 472749483.98320234\n",
      "Iteration 99, loss = 511955321.43612516\n",
      "Iteration 100, loss = 563613130.58545065\n",
      "Iteration 101, loss = 484830753.90830100\n",
      "Iteration 102, loss = 472812978.55729324\n",
      "Iteration 103, loss = 483552089.76882261\n",
      "Iteration 104, loss = 477793037.16280341\n",
      "Iteration 105, loss = 467486875.59776616\n",
      "Iteration 106, loss = 464803141.45248336\n",
      "Iteration 107, loss = 488434056.76090473\n",
      "Iteration 108, loss = 472052457.85489625\n",
      "Iteration 109, loss = 497860407.79577780\n",
      "Iteration 110, loss = 511121535.54919726\n",
      "Iteration 111, loss = 462130701.41421103\n",
      "Iteration 112, loss = 452370928.15929979\n",
      "Iteration 113, loss = 466836392.41415918\n",
      "Iteration 114, loss = 464638186.69012988\n",
      "Iteration 115, loss = 467861687.30477560\n",
      "Iteration 116, loss = 454861314.13782132\n",
      "Iteration 117, loss = 495751083.11796898\n",
      "Iteration 118, loss = 449959587.37674981\n",
      "Iteration 119, loss = 470184787.73452330\n",
      "Iteration 120, loss = 459654503.93721944\n",
      "Iteration 121, loss = 445400957.39792234\n",
      "Iteration 122, loss = 631403552.54711652\n",
      "Iteration 123, loss = 444752358.69071370\n",
      "Iteration 124, loss = 456400300.97496444\n",
      "Iteration 125, loss = 490850583.24877316\n",
      "Iteration 126, loss = 442444978.24492192\n",
      "Iteration 127, loss = 442393991.45822567\n",
      "Iteration 128, loss = 512365638.41240561\n",
      "Iteration 129, loss = 440234315.38181102\n",
      "Iteration 130, loss = 508673373.16443247\n",
      "Iteration 131, loss = 794108282.38120556\n",
      "Iteration 132, loss = 472222688.88846451\n",
      "Iteration 133, loss = 429267423.13830251\n",
      "Iteration 134, loss = 425195243.49548542\n",
      "Iteration 135, loss = 433979589.64936513\n",
      "Iteration 136, loss = 439855779.80792838\n",
      "Iteration 137, loss = 419639260.67368817\n",
      "Iteration 138, loss = 449451115.71769422\n",
      "Iteration 139, loss = 418306254.24384135\n",
      "Iteration 140, loss = 417768825.30674809\n",
      "Iteration 141, loss = 426543867.62726843\n",
      "Iteration 142, loss = 415787941.74275768\n",
      "Iteration 143, loss = 417807371.08233106\n",
      "Iteration 144, loss = 432931252.17864543\n",
      "Iteration 145, loss = 426787167.49918228\n",
      "Iteration 146, loss = 456023297.33942878\n",
      "Iteration 147, loss = 423641286.07716858\n",
      "Iteration 148, loss = 441479013.29669797\n",
      "Iteration 149, loss = 430702518.15369630\n",
      "Iteration 150, loss = 411169843.13146025\n",
      "Iteration 151, loss = 448343877.77344108\n",
      "Iteration 152, loss = 426283952.18799460\n",
      "Iteration 153, loss = 408488523.04789811\n",
      "Iteration 154, loss = 424866542.66240323\n",
      "Iteration 155, loss = 405073654.04889756\n",
      "Iteration 156, loss = 610620725.20949054\n",
      "Iteration 157, loss = 443623579.63758576\n",
      "Iteration 158, loss = 409416739.41515201\n",
      "Iteration 159, loss = 656662771.42057741\n",
      "Iteration 160, loss = 416220447.20033753\n",
      "Iteration 161, loss = 395940738.06523323\n",
      "Iteration 162, loss = 412308223.13020754\n",
      "Iteration 163, loss = 410229997.55737662\n",
      "Iteration 164, loss = 454632707.41428965\n",
      "Iteration 165, loss = 706792464.42859983\n",
      "Iteration 166, loss = 404194107.15188420\n",
      "Iteration 167, loss = 409563182.17734081\n",
      "Iteration 168, loss = 399304399.73688656\n",
      "Iteration 169, loss = 400342003.66354984\n",
      "Iteration 170, loss = 404664268.33175528\n",
      "Iteration 171, loss = 393826970.28074837\n",
      "Iteration 172, loss = 392200282.60917556\n",
      "Iteration 173, loss = 816589251.14684212\n",
      "Iteration 174, loss = 402824143.74259388\n",
      "Iteration 175, loss = 402894348.67142671\n",
      "Iteration 176, loss = 384499781.68273914\n",
      "Iteration 177, loss = 396128386.94285524\n",
      "Iteration 178, loss = 449362814.49617052\n",
      "Iteration 179, loss = 386012003.42893058\n",
      "Iteration 180, loss = 481167667.16738576\n",
      "Iteration 181, loss = 384089087.62423491\n",
      "Iteration 182, loss = 384611728.96285164\n",
      "Iteration 183, loss = 395725089.91002929\n",
      "Iteration 184, loss = 391888582.20123422\n",
      "Iteration 185, loss = 391121125.20438433\n",
      "Iteration 186, loss = 393367557.37069100\n",
      "Iteration 187, loss = 394416749.97468120\n",
      "Iteration 188, loss = 382492978.08128732\n",
      "Iteration 189, loss = 373823113.92324704\n",
      "Iteration 190, loss = 433787268.16891247\n",
      "Iteration 191, loss = 435775954.64402503\n",
      "Iteration 192, loss = 370649693.34069961\n",
      "Iteration 193, loss = 382431416.28718561\n",
      "Iteration 194, loss = 401048947.98042804\n",
      "Iteration 195, loss = 370006267.34664017\n",
      "Iteration 196, loss = 370851470.82536614\n",
      "Iteration 197, loss = 367907274.86316723\n",
      "Iteration 198, loss = 384977640.21384209\n",
      "Iteration 199, loss = 428062305.64210784\n",
      "Iteration 200, loss = 404881902.22892129\n",
      "Iteration 201, loss = 383489381.01792812\n",
      "Iteration 202, loss = 389598832.21937132\n",
      "Iteration 203, loss = 372437673.26070166\n",
      "Iteration 204, loss = 373676720.99096352\n",
      "Iteration 205, loss = 362366361.20196211\n",
      "Iteration 206, loss = 366508931.70554775\n",
      "Iteration 207, loss = 375005229.00856352\n",
      "Iteration 208, loss = 365885746.04862243\n",
      "Iteration 209, loss = 358802164.18140358\n",
      "Iteration 210, loss = 375487537.79885095\n",
      "Iteration 211, loss = 412026701.00978559\n",
      "Iteration 212, loss = 422494061.68740141\n",
      "Iteration 213, loss = 418423523.60887879\n",
      "Iteration 214, loss = 367016733.81183213\n",
      "Iteration 215, loss = 414578455.19633895\n",
      "Iteration 216, loss = 361390135.16128808\n",
      "Iteration 217, loss = 353883478.82913595\n",
      "Iteration 218, loss = 400132079.74641448\n",
      "Iteration 219, loss = 409084175.39134657\n",
      "Iteration 220, loss = 350964629.13016635\n",
      "Iteration 221, loss = 365940374.51635456\n",
      "Iteration 222, loss = 379601852.38818151\n",
      "Iteration 223, loss = 359396269.04514819\n",
      "Iteration 224, loss = 366564850.59922963\n",
      "Iteration 225, loss = 359282762.70133483\n",
      "Iteration 226, loss = 398873565.62757349\n",
      "Iteration 227, loss = 355427929.93570101\n",
      "Iteration 228, loss = 360160490.90283364\n",
      "Iteration 229, loss = 734664704.88995743\n",
      "Iteration 230, loss = 354873199.30603456\n",
      "Iteration 231, loss = 386432117.43682450\n",
      "Iteration 232, loss = 354006812.41726190\n",
      "Iteration 233, loss = 371205303.16345996\n",
      "Iteration 234, loss = 348186015.06653035\n",
      "Iteration 235, loss = 368667081.95491856\n",
      "Iteration 236, loss = 375306454.06810790\n",
      "Iteration 237, loss = 351327603.23468882\n",
      "Iteration 238, loss = 344789377.51137179\n",
      "Iteration 239, loss = 352471426.90512311\n",
      "Iteration 240, loss = 358147102.34023535\n",
      "Iteration 241, loss = 337615954.01897538\n",
      "Iteration 242, loss = 348350123.00023985\n",
      "Iteration 243, loss = 359465193.43276209\n",
      "Iteration 244, loss = 343153126.88739836\n",
      "Iteration 245, loss = 340763522.08244872\n",
      "Iteration 246, loss = 351492676.14132696\n",
      "Iteration 247, loss = 425962636.86112469\n",
      "Iteration 248, loss = 353470290.98192501\n",
      "Iteration 249, loss = 352905949.55336243\n",
      "Iteration 250, loss = 341417781.51622182\n",
      "Iteration 251, loss = 333463649.81808752\n",
      "Iteration 252, loss = 331648842.02069002\n",
      "Iteration 253, loss = 341527728.71404481\n",
      "Iteration 254, loss = 332406974.67228460\n",
      "Iteration 255, loss = 336534084.73949552\n",
      "Iteration 256, loss = 337331852.08247775\n",
      "Iteration 257, loss = 337508474.73518908\n",
      "Iteration 258, loss = 365145317.20451301\n",
      "Iteration 259, loss = 373925580.37882680\n",
      "Iteration 260, loss = 372554317.52010280\n",
      "Iteration 261, loss = 343985886.09581727\n",
      "Iteration 262, loss = 377600935.01395762\n",
      "Iteration 263, loss = 339273150.73465681\n",
      "Iteration 264, loss = 333461974.54087210\n",
      "Iteration 265, loss = 337060403.88252795\n",
      "Iteration 266, loss = 375981105.46091896\n",
      "Iteration 267, loss = 341088908.22091210\n",
      "Iteration 268, loss = 359702249.72949904\n",
      "Iteration 269, loss = 331326214.54097337\n",
      "Iteration 270, loss = 352811930.22423190\n",
      "Iteration 271, loss = 325324888.55460882\n",
      "Iteration 272, loss = 336610247.48738390\n",
      "Iteration 273, loss = 336044371.78069079\n",
      "Iteration 274, loss = 378847253.44808346\n",
      "Iteration 275, loss = 709629308.06129038\n",
      "Iteration 276, loss = 327302260.79200435\n",
      "Iteration 277, loss = 340335852.29516214\n",
      "Iteration 278, loss = 332521891.86615378\n",
      "Iteration 279, loss = 332626356.99529028\n",
      "Iteration 280, loss = 335365711.48821253\n",
      "Iteration 281, loss = 319601379.05029851\n",
      "Iteration 282, loss = 325206351.90545976\n",
      "Iteration 283, loss = 394946690.65011829\n",
      "Iteration 284, loss = 321573236.55501235\n",
      "Iteration 285, loss = 340225478.90650201\n",
      "Iteration 286, loss = 318406186.38941932\n",
      "Iteration 287, loss = 322673096.76731122\n",
      "Iteration 288, loss = 335024407.77029067\n",
      "Iteration 289, loss = 347357485.93450785\n",
      "Iteration 290, loss = 323191159.71256340\n",
      "Iteration 291, loss = 460395382.75139785\n",
      "Iteration 292, loss = 315314731.18959969\n",
      "Iteration 293, loss = 320000323.62559527\n",
      "Iteration 294, loss = 323562412.50816858\n",
      "Iteration 295, loss = 333910989.37686139\n",
      "Iteration 296, loss = 333881650.32224554\n",
      "Iteration 297, loss = 316149192.65475750\n",
      "Iteration 298, loss = 349327443.67916638\n",
      "Iteration 299, loss = 320332184.53782213\n",
      "Iteration 300, loss = 323681457.41917694\n",
      "Iteration 301, loss = 313785951.95659506\n",
      "Iteration 302, loss = 350382357.29637086\n",
      "Iteration 303, loss = 309059161.38946784\n",
      "Iteration 304, loss = 332600126.93660551\n",
      "Iteration 305, loss = 553841225.44283116\n",
      "Iteration 306, loss = 317006556.19673657\n",
      "Iteration 307, loss = 308521004.09952343\n",
      "Iteration 308, loss = 324658036.89098763\n",
      "Iteration 309, loss = 302256507.11806852\n",
      "Iteration 310, loss = 347296947.58588547\n",
      "Iteration 311, loss = 379453569.66004974\n",
      "Iteration 312, loss = 356985786.46257997\n",
      "Iteration 313, loss = 305913927.52465808\n",
      "Iteration 314, loss = 303538681.58822435\n",
      "Iteration 315, loss = 368295081.87037915\n",
      "Iteration 316, loss = 312564300.85827535\n",
      "Iteration 317, loss = 308355659.40414041\n",
      "Iteration 318, loss = 298614450.33707422\n",
      "Iteration 319, loss = 369264773.43854970\n",
      "Iteration 320, loss = 340072361.57824111\n",
      "Iteration 321, loss = 370697075.29331917\n",
      "Iteration 322, loss = 338759145.75365061\n",
      "Iteration 323, loss = 313940108.41530657\n",
      "Iteration 324, loss = 300619485.51424921\n",
      "Iteration 325, loss = 310184676.64913434\n",
      "Iteration 326, loss = 632892834.91068769\n",
      "Iteration 327, loss = 309546461.80901921\n",
      "Iteration 328, loss = 317419647.87450862\n",
      "Iteration 329, loss = 396592097.75775892\n",
      "Iteration 330, loss = 301488638.54199207\n",
      "Iteration 331, loss = 304957527.46896738\n",
      "Iteration 332, loss = 299550831.71820587\n",
      "Iteration 333, loss = 288897295.50025189\n",
      "Iteration 334, loss = 299043308.60359210\n",
      "Iteration 335, loss = 320183654.84924865\n",
      "Iteration 336, loss = 302224564.48639846\n",
      "Iteration 337, loss = 292786869.67607498\n",
      "Iteration 338, loss = 288914490.81400996\n",
      "Iteration 339, loss = 474337571.13714397\n",
      "Iteration 340, loss = 289223991.81669599\n",
      "Iteration 341, loss = 315658870.54432195\n",
      "Iteration 342, loss = 286743243.68621790\n",
      "Iteration 343, loss = 305741701.15486747\n",
      "Iteration 344, loss = 384141501.51758617\n",
      "Iteration 345, loss = 281429284.41238654\n",
      "Iteration 346, loss = 334916539.24514908\n",
      "Iteration 347, loss = 301792787.85538787\n",
      "Iteration 348, loss = 280034793.29040205\n",
      "Iteration 349, loss = 275877943.94048315\n",
      "Iteration 350, loss = 288672464.67944658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(alpha=1e-06, beta_1=0.5, epsilon=1e-12,\n",
       "                              hidden_layer_sizes=(150, 70),\n",
       "                              learning_rate_init=0.01, max_iter=350,\n",
       "                              n_iter_no_change=20, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                                  [&#x27;MSZoning&#x27;, &#x27;Street&#x27;,\n",
       "                                                   &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                                   &#x27;LandContour&#x27;, &#x27;Utilities&#x27;,\n",
       "                                                   &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;,\n",
       "                                                   &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                                   &#x27;...\n",
       "                                                   &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;,\n",
       "                                                   &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                                   &#x27;KitchenAbvGr&#x27;,\n",
       "                                                   &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                                   &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;,\n",
       "                                                   &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;,\n",
       "                                                   &#x27;OpenPorchSF&#x27;,\n",
       "                                                   &#x27;EnclosedPorch&#x27;, ...])])),\n",
       "                (&#x27;mlpregressor&#x27;,\n",
       "                 MLPRegressor(alpha=1e-06, beta_1=0.5, epsilon=1e-12,\n",
       "                              hidden_layer_sizes=(150, 70),\n",
       "                              learning_rate_init=0.01, max_iter=350,\n",
       "                              n_iter_no_change=20, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;Variables Categóricas&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;median&#x27;))]),\n",
       "                                 [&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;,\n",
       "                                  &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;,\n",
       "                                  &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;,\n",
       "                                  &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;,\n",
       "                                  &#x27;...\n",
       "                                  &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;,\n",
       "                                  &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;,\n",
       "                                  &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;,\n",
       "                                  &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;,\n",
       "                                  &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;,\n",
       "                                  &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;,\n",
       "                                  &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;,\n",
       "                                  &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;,\n",
       "                                  &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, ...])])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-76\" type=\"checkbox\" ><label for=\"sk-estimator-id-76\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Categóricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSZoning&#x27;, &#x27;Street&#x27;, &#x27;Alley&#x27;, &#x27;LotShape&#x27;, &#x27;LandContour&#x27;, &#x27;Utilities&#x27;, &#x27;LotConfig&#x27;, &#x27;LandSlope&#x27;, &#x27;Neighborhood&#x27;, &#x27;Condition1&#x27;, &#x27;Condition2&#x27;, &#x27;BldgType&#x27;, &#x27;HouseStyle&#x27;, &#x27;RoofStyle&#x27;, &#x27;RoofMatl&#x27;, &#x27;Exterior1st&#x27;, &#x27;Exterior2nd&#x27;, &#x27;MasVnrType&#x27;, &#x27;ExterQual&#x27;, &#x27;ExterCond&#x27;, &#x27;Foundation&#x27;, &#x27;BsmtQual&#x27;, &#x27;BsmtCond&#x27;, &#x27;BsmtExposure&#x27;, &#x27;BsmtFinType1&#x27;, &#x27;BsmtFinType2&#x27;, &#x27;Heating&#x27;, &#x27;HeatingQC&#x27;, &#x27;CentralAir&#x27;, &#x27;Electrical&#x27;, &#x27;KitchenQual&#x27;, &#x27;Functional&#x27;, &#x27;FireplaceQu&#x27;, &#x27;GarageType&#x27;, &#x27;GarageFinish&#x27;, &#x27;GarageQual&#x27;, &#x27;GarageCond&#x27;, &#x27;PavedDrive&#x27;, &#x27;PoolQC&#x27;, &#x27;Fence&#x27;, &#x27;MiscFeature&#x27;, &#x27;SaleType&#x27;, &#x27;SaleCondition&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-77\" type=\"checkbox\" ><label for=\"sk-estimator-id-77\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-78\" type=\"checkbox\" ><label for=\"sk-estimator-id-78\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-79\" type=\"checkbox\" ><label for=\"sk-estimator-id-79\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Variables Numéricas</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;MSSubClass&#x27;, &#x27;LotFrontage&#x27;, &#x27;LotArea&#x27;, &#x27;OverallQual&#x27;, &#x27;OverallCond&#x27;, &#x27;YearBuilt&#x27;, &#x27;YearRemodAdd&#x27;, &#x27;MasVnrArea&#x27;, &#x27;BsmtFinSF1&#x27;, &#x27;BsmtFinSF2&#x27;, &#x27;BsmtUnfSF&#x27;, &#x27;TotalBsmtSF&#x27;, &#x27;1stFlrSF&#x27;, &#x27;2ndFlrSF&#x27;, &#x27;LowQualFinSF&#x27;, &#x27;GrLivArea&#x27;, &#x27;BsmtFullBath&#x27;, &#x27;BsmtHalfBath&#x27;, &#x27;FullBath&#x27;, &#x27;HalfBath&#x27;, &#x27;BedroomAbvGr&#x27;, &#x27;KitchenAbvGr&#x27;, &#x27;TotRmsAbvGrd&#x27;, &#x27;Fireplaces&#x27;, &#x27;GarageYrBlt&#x27;, &#x27;GarageCars&#x27;, &#x27;GarageArea&#x27;, &#x27;WoodDeckSF&#x27;, &#x27;OpenPorchSF&#x27;, &#x27;EnclosedPorch&#x27;, &#x27;3SsnPorch&#x27;, &#x27;ScreenPorch&#x27;, &#x27;PoolArea&#x27;, &#x27;MiscVal&#x27;, &#x27;MoSold&#x27;, &#x27;YrSold&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-80\" type=\"checkbox\" ><label for=\"sk-estimator-id-80\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SimpleImputer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-81\" type=\"checkbox\" ><label for=\"sk-estimator-id-81\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-82\" type=\"checkbox\" ><label for=\"sk-estimator-id-82\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">remainder</label><div class=\"sk-toggleable__content fitted\"><pre>[&#x27;Id&#x27;]</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-83\" type=\"checkbox\" ><label for=\"sk-estimator-id-83\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">passthrough</label><div class=\"sk-toggleable__content fitted\"><pre>passthrough</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-84\" type=\"checkbox\" ><label for=\"sk-estimator-id-84\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPRegressor.html\">?<span>Documentation for MLPRegressor</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPRegressor(alpha=1e-06, beta_1=0.5, epsilon=1e-12,\n",
       "             hidden_layer_sizes=(150, 70), learning_rate_init=0.01,\n",
       "             max_iter=350, n_iter_no_change=20, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('Variables Categóricas',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                  ('imputer',\n",
       "                                                                   SimpleImputer(strategy='median'))]),\n",
       "                                                  ['MSZoning', 'Street',\n",
       "                                                   'Alley', 'LotShape',\n",
       "                                                   'LandContour', 'Utilities',\n",
       "                                                   'LotConfig', 'LandSlope',\n",
       "                                                   'Neighborhood', 'Condition1',\n",
       "                                                   '...\n",
       "                                                   'BsmtHalfBath', 'FullBath',\n",
       "                                                   'HalfBath', 'BedroomAbvGr',\n",
       "                                                   'KitchenAbvGr',\n",
       "                                                   'TotRmsAbvGrd', 'Fireplaces',\n",
       "                                                   'GarageYrBlt', 'GarageCars',\n",
       "                                                   'GarageArea', 'WoodDeckSF',\n",
       "                                                   'OpenPorchSF',\n",
       "                                                   'EnclosedPorch', ...])])),\n",
       "                ('mlpregressor',\n",
       "                 MLPRegressor(alpha=1e-06, beta_1=0.5, epsilon=1e-12,\n",
       "                              hidden_layer_sizes=(150, 70),\n",
       "                              learning_rate_init=0.01, max_iter=350,\n",
       "                              n_iter_no_change=20, verbose=True))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelBetter.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predBetter = modelBetter.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (test): 745381554.6307167\n",
      "MAE (test): 19602.020482953114\n",
      "RMSE (test): 27301.67677324447\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, predBetter)\n",
    "print(\"MSE (test):\", mse)\n",
    "mae_test = mean_absolute_error(y_test, predBetter)\n",
    "print(\"MAE (test):\", mae_test)\n",
    "rmse_test = np.sqrt(mse)\n",
    "print(\"RMSE (test):\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Create a scorer\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, train_sizes=train_sizes, cv=5, scoring=rmse_scorer)\n",
    "\n",
    "# Calculate the means and standard deviations of the training and test scores\n",
    "train_mean = np.mean(-train_scores, axis=1)\n",
    "train_std = np.std(-train_scores, axis=1)\n",
    "test_mean = np.mean(-test_scores, axis=1)\n",
    "test_std = np.std(-test_scores, axis=1)\n",
    "\n",
    "# Draw the learning curves\n",
    "plt.plot(train_sizes, train_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='#DDDDDD')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='#DDDDDD')\n",
    "\n",
    "# Create the legend and titles\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 2 SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = make_pipeline(preprocesador, MLPRegressor(hidden_layer_sizes=(100, 50), activation='logistic', solver='adam', alpha=0.001, batch_size='auto', learning_rate='constant', learning_rate_init=0.0002, max_iter=500, shuffle=True, random_state=None, tol=0.0001, verbose=True, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-20, n_iter_no_change=10))\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, pred2)\n",
    "print(\"MSE (test):\", mse)\n",
    "mae_test = mean_absolute_error(y_test, pred2)\n",
    "print(\"MAE (test):\", mae_test)\n",
    "rmse_test = np.sqrt(mse)\n",
    "print(\"RMSE (test):\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a function to calculate RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Create a scorer\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=False)\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Calculate the learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(model1, X, y, train_sizes=train_sizes, cv=5, scoring=rmse_scorer)\n",
    "\n",
    "# Calculate the means and standard deviations of the training and test scores\n",
    "train_mean = np.mean(-train_scores, axis=1)\n",
    "train_std = np.std(-train_scores, axis=1)\n",
    "test_mean = np.mean(-test_scores, axis=1)\n",
    "test_std = np.std(-test_scores, axis=1)\n",
    "\n",
    "# Draw the learning curves\n",
    "plt.plot(train_sizes, train_mean, label='Training score')\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='#DDDDDD')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='#DDDDDD')\n",
    "\n",
    "# Create the legend and titles\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
